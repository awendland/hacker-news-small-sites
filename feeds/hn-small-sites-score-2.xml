<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 2]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 2. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Thu, 01 Oct 2020 08:26:57 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-2.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Thu, 01 Oct 2020 08:26:57 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[What is the C++ standard library?]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24625526">thread link</a>) | @bvaldivielso
<br/>
September 29, 2020 | https://cor3ntin.github.io/posts/std/ | <a href="https://web.archive.org/web/*/https://cor3ntin.github.io/posts/std/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <p><a href="https://xkcd.com/2347/">
<img src="https://imgs.xkcd.com/comics/dependency.png">
</a>
</p>
<hr>
<p><strong>DISCLAIMER</strong></p>
<p>The following represent my opinions, not that of the C++ committee (WG21), any of its members or any other person mentioned in this article.</p>
<hr>
<p>I think the most fundamental work done by WG21 is trying to answer meta-questions about itself.
What is C++, what is its essence, what should we focus on? How to evolve a language with a growing community,
a growing committee?
A language that is deployed on billions of devices, with an estimated 50 billions actively maintained lines of code.
During a CppCon panel last week I was asked about stability VS evolution. This is a hot topic,
one that may never stop being on people’s mind.</p>
<p>There are a lot of interesting meta-questions worth asking about the standard library too.
A question that comes over and over again, is what does it mean to deprecate something, why and when.
Another is what to put in there to begin with.</p>
<p>I wrote a few times on the subject <a href="https://cor3ntin.github.io/posts/what_should_go_in_stl/">before</a>,
hopefully, I will be self-consistent. Not promising anything!</p>
<p>Bryce Adelstein Lelbach, then chair of LEWGI coined the phrase</p>
<blockquote>
<p>Knowing that our resources are scarce and our time is limited, do we want to give more time to this proposal?</p>
</blockquote>
<p>This has become somewhat of a meme in the committee.
Since then, we shipped C++20, Bryce became chair of LEWG (which is a super difficult job that he does brilliantly),
and oh. There is this pandemic you might have heard about.</p>
<p>Never have the scarcity of our resources and the limitedness of our time be more apparent.</p>
<p>We try to make the best of the situation, but I think it’s fair to say that WG21’s output is
reduced. And frankly, we cannot ask of anyone to prioritize C++ standardization with all that’s going on right now.
But even at the best of times, C++ library design is a costly, lengthy process involving a lot of people.
Which is good, <a href="https://en.wikipedia.org/wiki/Linus%27s_law">Linus’s law</a>, plays a huge role in the quality of the standard library.</p>
<p>I don’t know that this used to be a question anyone asked. For a very long time, there were few enough proposals
that they virtually could accept all of those they liked.
At the beginning of the committee, there even was a single pipeline for both language and library.
We can argue whether the separation of rooms we have today is always sensible.
Most of the time library features can be added without new language proposal, but at the same time ADL, customization points,
overload sets etc have been growing pain point for the library, and no organization can escape Conway’s law.</p>
<p>Anyway, the influx of new proposal has grown enough in the past few years that LEWG has now the luxury
and the burden to chose which direction to go in and how to use its far too limited time.</p>
<h2 id="standardization-is-expensive">Standardization is expensive</h2>
<p>I think the life cycle of a library feature goes a bit like this:</p>
<ul>
<li>Someone floats an idea and write a paper</li>
<li>The paper is matured over 1-10 years</li>
<li>There is some latency for implementations (6 months - 5 years) - at least 3 or 4 implementations</li>
<li>There is a ton of latency in deploying toolchains where people can use them (this might be a story for another day)</li>
<li>There is a literal ton of people writing blog posts / textbooks / conference talks about that one feature</li>
<li>Then there is a slow adoption and debate about whether adopting the feature is good or not</li>
</ul>
<p>And every step is resources constrained. Deprecation and removal is also very slow.
People are still using components that were deprecated 10, 20 years ago.</p>
<p>Critical flight software at NASA is estimated at 1000$ per line.
I wouldn’t be surprised if the standard library costs more.
And so, one of the way to answer
“Should that piece of code be in the standard” can maybe be reformulated as “Would this benefit from the standardization process?”</p>
<p>Fundamentally, that makes the standard library a bad package manager.
If the only motivation for something to be in the standard is to palliate to the lack of good package managers,
then it’s probably not a good motivation.
(Especially as the standard library is super bad at availability. it will be years until <code>&lt;ranges&gt;</code> is everywhere.)</p>
<p>Of course, that argument falls flat if you consider <code>std::vector</code>. It doesn’t <em>need</em> to be there, but we are all sure glad it is.
So there is an <em>universality</em> argument to be made too.
If something is universally useful (for example, 90% of programs would use it), then it starts to be a very compelling
feature for the standard library.</p>
<p>Some features can’t live anywhere but in the STL:</p>
<p>Type traits, and everything that needs or benefits from compiler magic and intrinsics.
<a href="http://eel.is/c++draft/support.srcloc">source_location</a>, <a href="https://wg21.link/p0881r6">std::stacktrace</a>, <a href="https://wg21.link/p1885r2">encoding detection</a>,
Reflection support library and other introspection capabilities, <a href="https://wg21.link/p0627r3">std::unreachable</a>, <a href="https://wg21.link/p1773r0">std::assume</a>,
<a href="https://wg21.link/p1040r6">std::embed</a>.
All of these are magic and rely on the compiler. In other words they cannot be implemented portably outside of the standard library.
These are necessary for communicating between user code and compiler, and are the basis of higher-level components.
A logger would use <code>std::source_location</code> for example.</p>
<p>This is especially true of reflection: until C++ gets reflection, an entire class of program cannot be written.
Pattern matching make it possible to write cleaner code. Reflection make it possible to write… code.
Code that you cannot otherwise emulate in standard C++, regardless how much you try.
And that can be pretty expensive across the industry.</p>
<p>So library components that make new things possible are high on the list of the things I think should go in the standard library.</p>
<p>Then, even more obvious, come amelioration to what’s already there.
As standard types get deployed, the committee has to improve them. Both in response to usage experience and to increase synergy between types
or add obvious features that were missing, bug fix, support for new language features.
As such, this <a href="https://wg21.link/p1679r3">std::string::contains</a> proposal might not be the most exciting that will land in 23,
but it might be one of the most useful for many people.</p>
<p>This is the rationale for my own  <a href="https://wg21.link/p2019r0">thread name proposal</a>.
It is not possible to name threads created by <code>std::thread</code>,
and people who rely on that (for ex. the game industry) need to write an entire thread class to replace <code>std::thread</code>, just for this extra piece of functionality.
Other people might give a name to their threads if it’s easy, but might not bother if it implies reimplementing <code>std::thread</code> themselves.
The cost/benefit of using a feature decreases if that feature is present in the standard library. But that is mostly true for small quality-of-life features,
not larger features that are application critical.</p>
<p>There are also vocabulary types: types that are designed to be the glue between libraries, a universal language for interface boundaries.
These get used everywhere. We spend a lot of time getting them right because of this by-design pervasiveness. <code>std::span</code> might simultaneously
be the simplest type of C++20 and also the one that took the most work getting right.
One example of glaringly missing vocabulary type is <a href="https://wg21.link/p1059r0"><code>std::expected</code></a>:
A type that is a bit stuck in another super important meta-conversation: What are the error handlings mechanism that should be used and promoted in C++?
We still have to answer that question.</p>
<p>But… I am not sure types are the right kind of entities at interfaces boundary.
Concepts make for a better vocabulary because they prescribe interfaces, not implementations. (<code>span</code> is nothing if not ““template erasure”” over any <code>contiguous_range</code>).
So maybe the standard library should mainly provide concepts to tie all 3rd parties together? The <code>&lt;concept&gt;</code> header is a good start here.</p>
<p>There is however an issue with just trying to add concepts in the standard.
Concepts are informed by concrete types and how they are used, they cannot be pulled out of thin air. Not without making mistakes anyway, and <a href="https://www.youtube.com/watch?v=v_yzLe-wnfk">concepts are not amendable to mistakes</a>.</p>
<p>So, it would seem that if the STL is to have concepts, it need algorithms.</p>
<blockquote><p lang="en" dir="ltr">Generic Programming pro tip: Although Concepts are constraints on types, you don't find them by looking at the types in your system. You find them by studying the algorithms.<a href="https://twitter.com/hashtag/cpp?src=hash&amp;ref_src=twsrc%5Etfw">#cpp</a></p>— Eric Niebler (@ericniebler) <a href="https://twitter.com/ericniebler/status/990390059579789312?ref_src=twsrc%5Etfw">April 29, 2018</a></blockquote>


<p>Good algorithms are agnostic of domain-specific knowledge and can be used in the widest variety of situations.
Algorithms are not useful to people who write games, or people who make microcontrollers, they are useful to people who write C++.
I’d write better code if I was able to recognise more algorithms.</p>
<p>A focus of C++20 was concepts and ranges, and I hope this remains the case in future versions of C++. views in particular are one of these things people might not actively looked for if available by default.
I sure think <code>views::product</code> is more maintainable than nested loops, but I might not try to find a library that has it, if it’s not in the standard.</p>
<p>So, magic types, vocabulary types, concepts and improvements of existing facilities. A good list of what might be LEWG priorities.</p>
<p>But what about networking, Unicode, processes, 15D graphics, audio, a web engine, ML facilities, JSON parsing, crypto, blockchains, Http, event handling, regexes that don’t suck, etc?</p>
<p>These sure make a great front page cover.
But here is the thing:</p>
<p>WG21 is… kinda bad at design?
Not because we are inherently inept, but because library design is fundamentally hard.
And what we understand to be good library design is a somewhat fast-moving target.</p>
<p>The STL is the foundation of the entire C++ ecosystem. And often used to demonstrate how to use new features and write good code. And we try to cover as many use cases as possible.
Design is finding a path in the maze of the design space, and at each crossing, we go in the direction that we think is the most widely useful.
Problem is, the numbers of crossings grows exponentially with the number of abstraction layers or the complexity of the domain.
It soon becomes impossible to please everybody, or even a majority of people.</p>
<p><code>vector</code> has a few knobs: allocation strategies, growth strategies, error handling etc. It’s a manageable number of knobs that we can tweak …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://cor3ntin.github.io/posts/std/">https://cor3ntin.github.io/posts/std/</a></em></p>]]>
            </description>
            <link>https://cor3ntin.github.io/posts/std/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24625526</guid>
            <pubDate>Tue, 29 Sep 2020 09:18:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Fix PostgreSQL Performance Issues with PG Extras]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24625471">thread link</a>) | @todsacerdoti
<br/>
September 29, 2020 | https://pawelurbanek.com/postgresql-fix-performance | <a href="https://web.archive.org/web/*/https://pawelurbanek.com/postgresql-fix-performance">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
  
    <p><img title="PostgreSQL performance checklist is represented by a notebook Photo by Aaron Burden on Unsplash" alt="PostgreSQL performance checklist is represented by a notebook Photo by Aaron Burden on Unsplash" data-src="https://pawelurbanek.com/assets/postgres-performance-notebook-223d1a2313851e07a649511f2ec7d7d7bbcf49c50cff6fbccf133f7a50a164f1.jpg" src="https://pawelurbanek.com/assets/postgres-performance-notebook-thumb-d32ee718054bdb4e0941997a0fef3c2040885ba2197dfc125668a82f6dea1a0b.jpg">
    </p>
  

  

  <p>PostgreSQL database queries are a common performance bottleneck for web apps. Before you resort to more complex optimization techniques like caching or read replicas, you should double-check if your database engine is correctly tuned and queries are not underperforming.</p>

<p>PG Extras is a tool that allows you to spot common PostgreSQL pitfalls. <a href="https://github.com/pawurb/ruby-pg-extras" target="_blank">Ruby</a>, <a href="https://github.com/pawurb/rails-pg-extras" target="_blank">Rails</a>, <a href="https://github.com/pawurb/ecto_psql_extras" target="_blank">Elixir</a>, and <a href="https://github.com/pawurb/node-postgres-extras" target="_blank">NodeJS</a> implementations are currently available.</p>

<p>In this blog post, I present a step by step guide on using PG Extras library to spot and resolve common PostgreSQL database performance issues.</p>



<p>Please refer to READMEs of respective implementations for installation details.  API is almost identical for all the versions. Let’s compare the invocation of the <code>cache_hit</code> method:</p>

<p><code>Ruby</code></p>

<figure><pre><code data-lang="ruby"><span>RubyPGExtras</span><span>.</span><span>cache_hit</span></code></pre></figure>

<p><code>Rails</code></p>

<figure><pre><code data-lang="ruby"><span>RailsPGExtras</span><span>.</span><span>cache_hit</span></code></pre></figure>

<p><code>Elixir</code></p>

<figure><pre><code data-lang="elixir"><span>EctoPSQLExtras</span><span>.</span><span>query</span><span>(</span><span>:cache_hit</span><span>,</span> <span>YourApp</span><span>.</span><span>Repo</span><span>)</span></code></pre></figure>

<p><code>NodeJS</code></p>

<figure><pre><code data-lang="javascript"><span>PostgresExtras</span><span>.</span><span>cache_hit</span><span>()</span></code></pre></figure>

<p>In this blog post, I’ll be using examples from the pure Ruby version.</p>

<h3 id="enable-pg_stat_statements-extension">Enable <code>pg_stat_statements</code> extension</h3>

<p>Some of the PG Extras methods depend on the <code>pg_stat_statements</code> extension. If you are using a default Heroku PostgreSQL plugin or <a href="https://aws.amazon.com/rds/" target="_blank">AWS RDS</a>, you should be good to go without making any changes.</p>

<p>To check if the extension is already enabled you can use PG Extras itself:</p>

<figure><pre><code data-lang="ruby"><span>RubyPGExtras</span><span>.</span><span>extensions</span>

<span>...</span>
<span>|</span> <span>pg_stat_statements</span> <span>|</span> <span>1.7</span> <span>|</span> <span>1.7</span> <span>|</span> <span>track</span> <span>execution</span> <span>statistics</span> <span>of</span> <span>all</span> <span>SQL</span> <span>statements</span> <span>executed</span>
<span>...</span></code></pre></figure>

<p>If <code>pg_stat_statements</code> is not listed you should check out <a href="https://www.postgresql.org/docs/current/pgstatstatements.html" target="_blank">these docs</a> for info on installing it.</p>

<p>Now that you’ve set up the library in the language of your choice let’s start checking our database’s health.</p>

<p><strong>[Important]</strong> <em>Make certain to run all the checks on a warmed up production database. Under the hood, PG Extras performs a lightweight queries on PostgreSQL metadata tables. It will not impact your production workload.</em></p>

<h2 id="1-validate-your-database-specs-with-cache-hit-ratios">1) Validate your database specs with cache hit ratios</h2>

<p>In theory, the simplest solution to optimize the underperforming database is to scale it up vertically. Before you start throwing money at your performance issues, it’s good to check if it will actually help.</p>

<p>PostgreSQL tracks access patterns of your data and keeps frequently read chunks in a memory cache. A reliable indicator that a database should be scaled up is an invalid cache hit ratio.</p>

<p>You can check index and table cache hit ratios using the following code:</p>

<figure><pre><code data-lang="ruby"><span>RubyPGExtras</span><span>.</span><span>cache_hit</span>

      <span>name</span>      <span>|</span>         <span>ratio</span>
<span>----------------+------------------------</span>
 <span>index</span> <span>hit</span> <span>rate</span> <span>|</span>        <span>0.999577</span>
 <span>table</span> <span>hit</span> <span>rate</span> <span>|</span>        <span>0.988721</span></code></pre></figure>

<p>If you want to drill down into each individual’s table and index cache hit ratios, you can use <code>table_cache_hit</code> and <code>index_cache_hit</code> methods.</p>

<p>The rule of the thumb is that values should be above 99%. If your database cache hit ratios are lower, it’s either not correctly configured or should be scaled up to increase the performance.</p>

<p>Heroku PostgreSQL ships with already optimized settings and does not allow you to change them. If you see low cache hit ratios, your best bet is to provision a more powerful database instance.</p>

<p>Amazon RDS is notorious for shipping the database instances with incorrect default settings. If you’re using it, make sure to tweak them before deciding to scale up the instance. <a href="https://pgtune.leopard.in.ua/" target="_blank">PGTune</a> is the best tool to help you tweak the most important Postgres buttons and dials to the correct values.</p>

<h2 id="2-remove-unused-indexes">2) Remove unused indexes</h2>

<p>Overusing indexes is a recipe for a sluggish web app.</p>

<p>The more indexes you add, the more write operations have to be performed on each data update. Misconfigured indexes also tend to unecessarily bloat the size of a database, slowing down the backup/restore/upgrade operations.</p>

<p>It’s entirely possible that some of your indexes and not used and can be safely removed.</p>

<p>PG Extras <code>unused_indexes</code> method can help you spot them:</p>

<figure><pre><code data-lang="ruby"><span>RubyPGExtras</span><span>.</span><span>unused_indexes</span>

          <span>table</span>      <span>|</span>                       <span>index</span>                <span>|</span> <span>index_size</span> <span>|</span> <span>index_scans</span>
<span>---------------------+--------------------------------------------+------------+-------------</span>
 <span>public</span><span>.</span><span>grade_levels</span> <span>|</span> <span>index_placement_attempts_on_grade_level_id</span> <span>|</span> <span>97</span> <span>MB</span>      <span>|</span>           <span>0</span>
 <span>public</span><span>.</span><span>observations</span> <span>|</span> <span>observations_attrs_grade_resources</span>         <span>|</span> <span>33</span> <span>MB</span>      <span>|</span>           <span>0</span>
 <span>public</span><span>.</span><span>messages</span>     <span>|</span> <span>user_resource_id_idx</span>                       <span>|</span> <span>12</span> <span>MB</span>      <span>|</span>           <span>0</span></code></pre></figure>

<p>Few <code>index_scans</code> on an index that has been around for a while means that it should be removed. If the index is large, remember to use the <a href="https://www.postgresql.org/docs/current/sql-dropindex.html" target="_blank"><code>CONCURRENTLY</code></a> option when dropping it, to avoid exclusively blocking the whole related table.</p>

<p><code>index_size</code> method can give you a quick overview of how much space your database indexes are taking:</p>

<figure><pre><code data-lang="ruby"><span>RubyPGExtras</span><span>.</span><span>index_size</span>

     <span>name</span>            <span>|</span>  <span>size</span>
<span>-----------------------------------------------</span>
 <span>index_a_name</span>        <span>|</span> <span>5196</span> <span>MB</span>
 <span>index_b_name</span>        <span>|</span> <span>4045</span> <span>MB</span>
 <span>index_b_name</span>        <span>|</span> <span>2611</span> <span>MB</span></code></pre></figure>

<h2 id="3-add-missing-indexes">3) Add missing indexes</h2>

<p>Now that we’ve removed unused indexes, let’s add some new ones. We don’t want them to share the fate of their recently deprovisioned cousins. Let’s look at PG Extras <code>seq_scans</code> and <code>calls</code> methods before deciding on what should be indexed.</p>

<p>A <em>sequential scan</em> is an action that Postgres performs if it cannot find an index necessary to fulfill the query condition. For the following query:</p>

<figure><pre><code data-lang="sql"><span>SELECT</span> <span>*</span> <span>FROM</span> <span>USERS</span> <span>WHERE</span> <span>AGE</span> <span>=</span> <span>18</span><span>;</span></code></pre></figure>

<p>the related <code>EXPLAIN ANALYZE</code> query output will show <code>Seq scan on users Filter: AGE = 18</code> or <code>Index Scan using users_age_index Index Cond: AGE = 18</code> depending on whether the index on <code>age</code> column is present or not.</p>

<p><code>seq_scans</code> method displays the number of <code>Seq Scan</code> operations for each table:</p>

<figure><pre><code data-lang="ruby"><span>RubyPGExtras</span><span>.</span><span>seq_scans</span>

               <span>name</span>                <span>|</span>  <span>count</span>
<span>-----------------------------------+----------</span>
 <span>learning_coaches</span>                  <span>|</span> <span>44820063</span>
 <span>states</span>                            <span>|</span> <span>36794975</span>
 <span>grade_levels</span>                      <span>|</span> <span>13972293</span>
 <span>charities_customers</span>               <span>|</span>  <span>8615277</span></code></pre></figure>

<p>Now that we know which tables are often read inefficiently, we can use <code>calls</code> and <code>outliers</code> methods to list the most often executed and most time-consuming queries.</p>

<p>Both of those methods let you extract the raw query string. You can use it to perform <code>EXPLAIN ANALYZE</code> and check if the query planner does <code>Seq scan</code> on one of the tables.</p>

<p>By correlating all those sources of data, you should be able to spot queries that are consuming a lot of your database resources and are potentially missing an index.</p>

<p>Watch out to avoid premature optimization by adding unnecessary indexes. PostgreSQL will often fallback to <code>Seq Scan</code> instead of <code>Index Scan</code> on small tables, for which using the index would be less efficient than reading the whole table row by row.</p>

<h2 id="4-identify-deadlocks">4) Identify deadlocks</h2>

<p>PostgreSQL uses locks to ensure data consistency in multithreaded environments. There are different kinds of locks, but we’ll focus on <code>ExclusiveLock</code> and <code>RowExclusiveLock</code>. A healthy web app should never lock for more than a couple of hundred of miliseconds.</p>

<p>Deadlock is two more or database locks blocking each other and not able to continue execution. An implementation error that results in a deadlock might have disastrous consequences for your application. The queue of requests not able to proceed could start piling up and eventually crash your servers.</p>

<p>Common reasons for deadlocks and locks that are granted for too long:</p>

<ul>
  <li>too broad database transaction scope</li>
  <li>adding or removing index without using the <code>CONCURRENTLY</code> option</li>
  <li>updating lots of rows at once</li>
  <li>adding a new column with the default value (before PostgreSQL 12)</li>
</ul>

<h3 id="how-to-detect-locks-and-deadlocks">How to detect locks and deadlocks</h3>

<p>You can use <code>locks</code> method to see all the currently obtained locks together with the source query:</p>

<figure><pre><code data-lang="ruby"><span>RubyPGExtras</span><span>.</span><span>locks</span>

 <span>procpid</span> <span>|</span> <span>relname</span> <span>|</span> <span>transactionid</span> <span>|</span> <span>granted</span> <span>|</span>     <span>query_snippet</span>     <span>|</span> <span>mode</span>             <span>|</span>       <span>age</span>
<span>---------+---------+---------------+---------+-----------------------+-------------------------------------</span>
   <span>31776</span> <span>|</span>         <span>|</span>               <span>|</span> <span>t</span>       <span>|</span> <span>&lt;</span><span>IDLE</span><span>&gt;</span> <span>in</span> <span>transaction</span> <span>|</span> <span>ExclusiveLock</span>    <span>|</span>  <span>00</span><span>:</span><span>19</span><span>:</span><span>29.837898</span>
   <span>31776</span> <span>|</span>         <span>|</span>          <span>1294</span> <span>|</span> <span>t</span>       <span>|</span> <span>&lt;</span><span>IDLE</span><span>&gt;</span> <span>in</span> <span>transaction</span> <span>|</span> <span>RowExclusiveLock</span> <span>|</span>  <span>00</span><span>:</span><span>19</span><span>:</span><span>29.837898</span>
   <span>31912</span> <span>|</span>         <span>|</span>               <span>|</span> <span>t</span>       <span>|</span> <span>select</span> <span>*</span> <span>from</span> <span>hello</span><span>;</span>  <span>|</span> <span>ExclusiveLock</span>    <span>|</span>  <span>00</span><span>:</span><span>19</span><span>:</span><span>17.94259</span>
    <span>3443</span> <span>|</span>         <span>|</span>               <span>|</span> <span>t</span>       <span>|</span>                      <span>+|</span> <span>ExclusiveLock</span>    <span>|</span>  <span>00</span><span>:</span><span>00</span><span>:</span><span>00</span></code></pre></figure>

<p>The mere presence of locks does not mean that something is wrong with your database. Only locks that are granted for too long are potentially problematic. You can use the following Ruby snippet integrated into the background job to alert you if this happens:</p>

<figure><pre><code data-lang="ruby"><span>TRESHOLD_SECONDS</span> <span>=</span> <span>1</span>

<span>long_locks</span> <span>=</span> <span>RubyPGExtras</span><span>.</span><span>locks</span><span>(</span><span>in_format: :hash</span><span>).</span><span>select</span> <span>do</span> <span>|</span><span>lock</span><span>|</span>
  <span>Time</span><span>.</span><span>parse</span><span>(</span><span>lock</span><span>.</span><span>fetch</span><span>(</span><span>"age"</span><span>)).</span><span>seconds_since_midnight</span> <span>&gt;</span> <span>TRESHOLD_SECONDS</span>
<span>end</span>

<span>raise</span> <span>"Long running locks: </span><span>#{</span><span>long_locks</span><span>}</span><span>"</span> <span>if</span> <span>long_locks</span><span>.</span><span>present?</span></code></pre></figure>

<p>If you notice extended locks, you can use the <code>blocking</code> method to check which SQL statements cannot continue execution because of a lock:</p>

<figure><pre><code data-lang="ruby"><span>RubyPGExtras</span><span>.</span><span>blocking</span>

 <span>blocked_pid</span> <span>|</span>    <span>blocking_statement</span>    <span>|</span> <span>blocking_duration</span> <span>|</span> <span>blocking_pid</span> <span>|</span>                                        <span>blocked_statement</span>                           <span>|</span> <span>blocked_duration</span>
<span>-------------+--------------------------+-------------------+--------------+------------------------------------------------------------------------------------+------------------</span>
         <span>461</span> <span>|</span> <span>select</span> <span>count</span><span>(</span><span>*</span><span>)</span> <span>from</span> <span>app</span> <span>|</span> <span>00</span><span>:</span><span>00</span><span>:</span><span>03</span><span>.</span><span>838314</span>   <span>|</span>        <span>15682</span> <span>|</span> <span>UPDATE</span> <span>"app"</span> <span>SET</span> <span>"updated_at"</span> <span>=</span> <span>'2013-03-04 15:07:04.746688'</span> <span>WHERE</span> <span>"id"</span> <span>=</span> <span>12823149</span> <span>|</span> <span>00</span><span>:</span><span>00</span><span>:</span><span>03</span><span>.</span><span>821826</span></code></pre></figure>

<p>If your app is crashing because of deadlocks, you can use the <code>kill_all</code> to terminate all the database processes before you manage to resolve the underlying cause.</p>

<h2 id="5-get-rid-of-unnecessary-bloat">5) Get rid of unnecessary bloat</h2>

<p>The way PostgreSQL works is that it never updates or removes the data in place but instead marks each row as visible or not for transactions using two meta columns <code>xmin</code> and <code>xmax</code>. Rows no longer visible for any of the currently active transactions are called <em>dead rows</em> or <em>bloat</em>.</p>

<p>Dead rows are regularly <em>garbage collected</em> by a process called <em>AUTOVACUUM</em>, …</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://pawelurbanek.com/postgresql-fix-performance">https://pawelurbanek.com/postgresql-fix-performance</a></em></p>]]>
            </description>
            <link>https://pawelurbanek.com/postgresql-fix-performance</link>
            <guid isPermaLink="false">hacker-news-small-sites-24625471</guid>
            <pubDate>Tue, 29 Sep 2020 09:06:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[When the Impact of Digital Tech on Our Mental Health Begins to Matter]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24625172">thread link</a>) | @Lima_Writes
<br/>
September 29, 2020 | https://lifebeyond.one/blogs/tech-impact/when-the-impact-of-digital-tech-on-our-mental-health-begins-to-matter | <a href="https://web.archive.org/web/*/https://lifebeyond.one/blogs/tech-impact/when-the-impact-of-digital-tech-on-our-mental-health-begins-to-matter">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
  <div>
    <div>
      <div id="shopify-section-article-template">

<div>
  <section name="9b04">

<div>
<div>
<h3 name="d56f">Seeing ourselves through the eyes of our children and our&nbsp;tech</h3>
<figure name="c560"><img data-image-id="1*vr4Rb1HI9B3nHj0E1snGoA.jpeg" data-width="1161" data-height="728" src="https://cdn-images-1.medium.com/max/1200/1*vr4Rb1HI9B3nHj0E1snGoA.jpeg"></figure>
<p name="2ab1"><strong>The above photo is from the </strong><a href="https://weconomics.org/events/congres2019/congresprogramma/" data-href="https://weconomics.org/events/congres2019/congresprogramma/" rel="noopener noreferrer" target="_blank"><strong>Weconomics convention</strong></a><strong> last year, where I had the opportunity to pre-present my book </strong><a href="https://lifebeyond.one/" data-href="https://lifebeyond.one/" rel="noopener noreferrer" target="_blank"><strong>Life Beyond the Touch Screen</strong></a><strong> before its official launch in 2020. Something really interesting happened during my session.</strong></p>
<p name="da59">I never presented my book as much as I simply told a story about my personal experience with technology and mental health.&nbsp;</p>
<p name="f245">In my presentation and workshop I mostly just shared the story about <a href="https://lifebeyond.one/blogs/tech-impact/smartphone-addiction-burnout-and-balance?_pos=2&amp;_sid=0e6e4570f&amp;_ss=r" data-href="https://lifebeyond.one/blogs/tech-impact/smartphone-addiction-burnout-and-balance?_pos=2&amp;_sid=0e6e4570f&amp;_ss=r" rel="noopener noreferrer" target="_blank">my own experience with digital dependency and with burnout</a>.&nbsp;</p>
<p name="5820">In that story I recount how for about two years I was simply asking far too much from myself; my body and my brain. And how looking back, I could easily retrace the steps that led to my inevitable collapse, more or less.</p>
<p name="f412">The fun part is that — though I do mention here and there that <a href="https://lifebeyond.one/blogs/tech-impact/smartphone-addiction-technology-dependency-science-and-statistics?_pos=1&amp;_sid=76458b6a6&amp;_ss=r" data-href="https://lifebeyond.one/blogs/tech-impact/smartphone-addiction-technology-dependency-science-and-statistics?_pos=1&amp;_sid=76458b6a6&amp;_ss=r" rel="noopener noreferrer" target="_blank">study after study is showing the possible heinous effects overuse of digital technology</a> can and probably does have on our mental health — </p>
<blockquote name="302e">Digital technology is not the root cause in my story.</blockquote>
<h4 name="dbf8">Digital tech is your black&nbsp;mirror</h4>
<p name="a727">Digital technology, in my personal experience, is simply a (black) mirror that shows us what we’re <em>actually</em> giving our focus and attention to — as compared to what we <em>think</em> our priorities and values are. If we let it, it can show us what’s really going on under the surface.</p>
<p name="2187">My talk at the Weconomics convention was by far the best received ‘performance’ I have ever given. The reactions from listeners were easily 10x those of my greatest and best performances in my time as a musician.</p>
<p name="969a">I think it had to do with relevance, vulnerability and authenticity.</p>
<p name="753a">But there was something else that absolutely stood out to me as well.</p>
<h4 name="c19b">When do things start to matter to&nbsp;humans?</h4>
<p name="e8ee">I saw faces in the crowd change, and I noticed the tone of the discussion become much, much more serious when we started talking about the impacts of tech on our younger generations. Our kids.</p>
<blockquote name="530f">What if our children are already, inevitably headed to digitally-induced mental health disaster, and there’s not much we can do now to stop the train?</blockquote>
<p name="7f01">Touching as it may have felt, it also had me worried. Why can we suddenly see how serious something can really be impacting our focus, energy, creativity, productivity — generally; our growth and wellbeing — when it’s about our children? Why can’t we see that when it’s about <em>ourselves</em>?</p>
<p name="e05c">You want my blunt, honest opinion? I think it’s a combination of an underdeveloped capacity to seriously self-reflect as human adults, with an underdeveloped degree of self-love.&nbsp;</p>
<p name="3499">And the funny thing is: if we want a better world and a better future for our children — it starts exactly there: with ourselves, our self-love and self-awareness.&nbsp;</p>
<p name="ff4e">Namasté, peoples.</p>
</div>
</div>
</section>
<section name="79ac">


</section>
</div>


  <!-- /snippets/social-sharing.liquid -->







</div>
    </div>
  </div>
</article></div>]]>
            </description>
            <link>https://lifebeyond.one/blogs/tech-impact/when-the-impact-of-digital-tech-on-our-mental-health-begins-to-matter</link>
            <guid isPermaLink="false">hacker-news-small-sites-24625172</guid>
            <pubDate>Tue, 29 Sep 2020 07:57:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Keep your GitHub forks up to date]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24625051">thread link</a>) | @reconquestio
<br/>
September 29, 2020 | https://samizdat.dev/keep-your-github-forks-up-to-date/ | <a href="https://web.archive.org/web/*/https://samizdat.dev/keep-your-github-forks-up-to-date/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">
<article>
    <p>Sometimes I do fork repositories and do some tweaks here and there for my personal needs which ain’t
really going to be merged into the upstream repository.</p>
<p>One thing that used to concern me is that I needed to manually rebase my changes onto the upstream
to have new goods but keep my changes on top of them.</p>
<p>Fortunately, GitHub Actions supports the <code>schedule</code> trigger for workflows and this is what we are
going to use.</p>
<p>The CI plan is simple and straightforward:</p>
<ul>
<li>Tigger by a schedule or by a manual run.</li>
<li>Fetch &amp; checkout repository.</li>
<li>Specify an upstream Git URL. Unfortunately, GitHub doesn’t expose an environment variable of a
upstream repository (or I didn’t find it).</li>
<li>Rebase onto upstream.</li>
<li>Push changes.</li>
</ul>
<div><pre><code data-lang="yaml"><span>name</span><span>:</span><span> </span><span>'Rebase'</span><span>
</span><span>
</span><span></span><span>on</span><span>:</span><span>
</span><span>  </span><span>schedule</span><span>:</span><span>
</span><span>    </span>- <span>cron</span><span>:</span><span> </span><span>'*/15 * * * *'</span><span>
</span><span>  </span><span>workflow_dispatch</span><span>:</span><span>
</span><span>
</span><span>
</span><span></span><span>jobs</span><span>:</span><span>
</span><span>  </span><span>rebase</span><span>:</span><span>
</span><span>    </span><span>runs-on</span><span>:</span><span> </span>ubuntu-latest<span>
</span><span>    </span><span>steps</span><span>:</span><span>
</span><span>    </span>- <span>uses</span><span>:</span><span> </span>actions/checkout@v2<span>
</span><span>    </span>- <span>uses</span><span>:</span><span> </span>shitiomatic/forkbacon@master<span>
</span><span>      </span><span>with</span><span>:</span><span>
</span><span>        </span><span>upstream_url</span><span>:</span><span> </span><span>"upstream url here"</span><span>
</span><span>        </span><span>upstream_branch</span><span>:</span><span> </span><span>"master"</span><span>
</span><span>        </span><span>branch</span><span>:</span><span> </span><span>"master"</span><span>
</span><span>        </span><span>method</span><span>:</span><span> </span><span>"rebase"</span><span>
</span></code></pre></div><p>Almost there. GitHub doesn’t sync &amp; install schedules for forked repositoies without a manual run or
an additional push. Go to Actions, find Rebase workflow and click on the Run workflow button.</p>
<p>That’s it, really. If the rebase fails, GitHub will send you an email letting you know about it.</p>
</article>

        </div></div>]]>
            </description>
            <link>https://samizdat.dev/keep-your-github-forks-up-to-date/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24625051</guid>
            <pubDate>Tue, 29 Sep 2020 07:33:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Designing an accessible color palette with magic numbers]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 5 (<a href="https://news.ycombinator.com/item?id=24624914">thread link</a>) | @darekkay
<br/>
September 29, 2020 | https://darekkay.com/blog/accessible-color-palette/ | <a href="https://web.archive.org/web/*/https://darekkay.com/blog/accessible-color-palette/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>A low text contrast is the <a href="https://webaim.org/projects/million/" target="_blank" rel="noopener">most common accessibility issue</a>. 86% of the top 1,000,000 websites have at least one contrast ratio violation, which may lead to a bad user experience. Our favorite orange website isn’t leading by example, either. Some comments are almost unreadable:</p><figure><picture><img src="https://darekkay.com/blog/accessible-color-palette/hacker-news.png" srcset="https://darekkay.com/blog/accessible-color-palette/hacker-news.png, https://darekkay.com/blog/accessible-color-palette/hacker-news-2x.png 2x" alt="Insufficient contrast ratio on Hacker News"></picture><figcaption>Insufficient contrast ratio on Hacker News</figcaption></figure><p>There are various tools that help us <em>identify</em> and <em>fix</em> contrast ratio issues on our websites. This post presents an approach for designing and structuring color palettes so that we can prevent such issues <em>before</em> they arise: “magic numbers”.</p><h2 id="Which-contrast-ratio-is-accessible"><a href="#Which-contrast-ratio-is-accessible" title="Which contrast ratio is accessible?"></a>Which contrast ratio is accessible?</h2><p>How do we know whether the contrast ratio between a text and its background is sufficient? The <em>Web Content Accessibility Guidelines</em> (WCAG) <a href="https://www.w3.org/TR/WCAG20-TECHS/G18.html" target="_blank" rel="noopener">define</a> minimum required contrast ratios, based on colors, text properties (size, boldness) and conformance levels (AA vs. AAA):</p><figure><table><thead><tr><th></th><th>Level AA</th><th>Level AAA</th></tr></thead><tbody><tr><td><strong>small text</strong></td><td>4.5+</td><td>7+</td></tr><tr><td><strong>large text</strong></td><td>3+</td><td>4.5+</td></tr></tbody></table><figcaption>Minimum required contrast ratio values</figcaption></figure><p>Note: <em>large text</em> is defined as <em>19px+ bold</em> or <em>24px+ normal</em>.</p><p>As a rule of thumb, try to go for a <strong>4.5:1</strong> minimum contrast ratio, which will pass <strong>WCAG AA</strong> independent of the text size. This provides a good cost-value trade-off and is often the legal accessibility requirement.</p><p>Calculating the accessibility conformance manually is tedious. Instead, I suggest using a tool like <a href="https://contrast-ratio.com/" target="_blank" rel="noopener">contrast-ratio.com</a>. DevTools in <a href="https://developer.mozilla.org/en-US/docs/Tools/Accessibility_inspector#Color_contrast" target="_blank" rel="noopener">Firefox</a> and <a href="https://umaar.com/dev-tips/236-accessible-colour-suggestions/" target="_blank" rel="noopener">Chrome</a> provide great built-in browser support. Finally, <a href="https://github.com/dequelabs/axe-cli" target="_blank" rel="noopener">axe-core</a> will scan a website for all kinds of accessibility violations.</p><p>Those tools are a great way to find contrast ratio issues, but let me describe a technique to prevent them in the first place.</p><h2 id="Magic-numbers"><a href="#Magic-numbers" title="Magic numbers"></a>Magic numbers</h2><p>Most color palettes divide their colors into grades (e.g. <code>pink-10</code> … <code>pink-90</code>).</p><p>Let’s define a <em>difference between two color grades</em> as <strong>magic number</strong>, e.g.:</p><ul><li>Colors: <code>blue-80</code> and <code>orange-30</code></li><li>Magic number: <code>80 - 30</code> = <code>50</code></li></ul><p>What if we could find a magic number for the whole palette that ensures a sufficient color contrast between two colors? The first time I’ve heard about this concept was in a <a href="https://pspeter3.com/blog/2020/02/19/accessible-contrast-shades/" target="_blank" rel="noopener">blog post</a> from Phips Peter. I’ve learned about the <a href="https://designsystem.digital.gov/design-tokens/color/overview/" target="_blank" rel="noopener">U.S. Web Design System</a> and was immediately hooked. The color system provides the following magic numbers:</p><ul><li>A magic number of <strong>40+</strong> ensures a contrast ratio of <strong>3+</strong></li><li>A magic number of <strong>50+</strong> ensures a contrast ratio of <strong>4.5+</strong></li><li>A magic number of <strong>70+</strong> ensures a contrast ratio of <strong>7+</strong></li></ul><p>By looking at the color names, I know that <code>red-40</code> and <code>gray-90</code> (= <code>50</code>) will definitely pass <em>WCAG AA</em> (required contrast ratio <code>4.5+</code>), while <code>red-60</code> and <code>gray-90</code> (= <code>30</code>) will not. This leads to a <em>fantastic</em> designer/developer experience. I don’t have to use a contrast checker or look anything up to ensure a sufficient contrast ratio. A difference of 50 or more is all I care about.</p><h2 id="Calculating-magic-numbers-for-an-existing-color-palette"><a href="#Calculating-magic-numbers-for-an-existing-color-palette" title="Calculating magic numbers for an existing color palette"></a>Calculating magic numbers for an existing color palette</h2><p>Some libraries define their magic numbers in the documentation, but what about others?</p><p>I have written <a href="https://github.com/darekkay/a11y-contrast" target="_blank" rel="noopener">a11y-contrast</a> to calculate the magic numbers for any color palette that follows a grade naming pattern (e.g. <code>red-20</code>). This tool also finds all violations for any given magic number. This way you can prevent regression issues after adding or adjusting a color value.</p><figure><picture><img src="https://darekkay.com/blog/accessible-color-palette/a11y-contrast.png" srcset="https://darekkay.com/blog/accessible-color-palette/a11y-contrast.png, https://darekkay.com/blog/accessible-color-palette/a11y-contrast-2x.png 2x" alt="CLI output with magic numbers and a list of violations"></picture><figcaption>CLI output with magic numbers and a list of violations</figcaption></figure><p>I’ve calculated the magic numbers for some common color palettes:</p><table><thead><tr><th></th><th>3+</th><th>4.5+</th><th>7+</th></tr></thead><tbody><tr><td><a href="https://designsystem.digital.gov/design-tokens/color/system-tokens/" target="_blank" rel="noopener">USWDS</a></td><td>40</td><td>50</td><td>70</td></tr><tr><td><a href="https://www.ibm.com/design/v1/language/resources/color-library/" target="_blank" rel="noopener">IBM v1</a></td><td>50</td><td>60</td><td>70</td></tr><tr><td><a href="https://github.com/carbon-design-system/carbon/tree/master/packages/colors" target="_blank" rel="noopener">IBM Carbon v2.1</a></td><td>50</td><td>50</td><td>70</td></tr><tr><td><a href="https://tailwindcss.com/docs/customizing-colors/#default-color-palette" target="_blank" rel="noopener">Tailwind v1</a></td><td>60</td><td>70</td><td>80</td></tr><tr><td><a href="https://github.com/tailwindlabs/tailwindcss/pull/2132" target="_blank" rel="noopener">Tailwind v2</a> (proposal)</td><td>50</td><td>60</td><td>80</td></tr><tr><td><a href="https://yeun.github.io/open-color/" target="_blank" rel="noopener">Open Color</a></td><td>-</td><td>-</td><td>-</td></tr></tbody></table><p>Here are my takeaways:</p><ul><li><strong>USWDS</strong> defines <em>by far</em> the most colors (461!), and yet it uses the smallest magic numbers. This leads to a much wider spectrum of allowed color combinations than any of the other color palettes I’ve checked.</li><li><strong>Open color</strong> doesn’t have <em>any</em> magic numbers. This means I cannot reliably derive the contrast ratio from the color naming (e.g., <code>gray-90/red-20</code> is fine, but <code>red-90/red-20</code> is not).</li><li>Both <a href="https://github.com/uswds/uswds/issues/3329" target="_blank" rel="noopener">USWDS</a> and <a href="https://github.com/carbon-design-system/carbon/issues/6130" target="_blank" rel="noopener">IBM Carbon</a> previously contained minor violations, showing the importance of automatic tests.</li></ul><h2 id="Defining-luminance-bounds-with-fixed-magic-numbers"><a href="#Defining-luminance-bounds-with-fixed-magic-numbers" title="Defining luminance bounds with fixed magic numbers"></a>Defining luminance bounds with fixed magic numbers</h2><p>Predefined color palettes are great, but what if we want to change or add a color? What grade does a certain color map to?</p><p>Because the contrast ratio between two colors depends only on their <a href="https://www.w3.org/TR/WCAG20-TECHS/G17.html" target="_blank" rel="noopener">luminance values</a>, it is possible to create a mapping from a color to a grade between 0 and 100. For the USWDS color palette, here are the <a href="https://github.com/uswds/uswds/issues/3329#issuecomment-594762982" target="_blank" rel="noopener">luminance bounds</a>:</p><table><thead><tr><th>grade</th><th>min luminance (%)</th><th>max luminance (%)</th></tr></thead><tbody><tr><td>0</td><td>100</td><td>100</td></tr><tr><td>5</td><td>85</td><td>93</td></tr><tr><td>10</td><td>75</td><td>82</td></tr><tr><td>20</td><td>50</td><td>65</td></tr><tr><td>30</td><td>35</td><td>45</td></tr><tr><td>40</td><td>25</td><td>30</td></tr><tr><td>50</td><td>17.5</td><td>18.3</td></tr><tr><td>60</td><td>10</td><td>12.5</td></tr><tr><td>70</td><td>5</td><td>7</td></tr><tr><td>80</td><td>2</td><td>4</td></tr><tr><td>90</td><td>0.05</td><td>1.5</td></tr><tr><td>100</td><td>0</td><td>0</td></tr></tbody></table><p><a href="https://github.com/thisisdano" target="_blank" rel="noopener">Dan O. Williams</a> — a USWDS maintainer — optimized those values for <em>consistency</em> instead of <em>coverage</em>. This means there are more colors that don’t fit into <em>any</em> bound, even though they would technically pass the WCAG contrast ratio. While this setup is more constraining, I agree with the consistency benefits.</p><blockquote><p>It’s important that our color system be consistent and predictable, that users know what they’re getting when they choose a color of a certain grade. This makes us inclined to favor smaller, more equal ranges, and consistent spacing between ranges.</p></blockquote><p>If you want to calculate the luminance value and the potential USWDS grade of any color, check out my <a href="https://darekkay.com/dev/color-tools.html">Color Tools</a>.</p><picture><source type="image/webp" srcset="https://darekkay.com/blog/accessible-color-palette/color-palette.webp, https://darekkay.com/blog/accessible-color-palette/color-palette-2x.webp 2x"><img src="https://darekkay.com/blog/accessible-color-palette/color-palette.jpg" srcset="https://darekkay.com/blog/accessible-color-palette/color-palette.jpg, https://darekkay.com/blog/accessible-color-palette/color-palette-2x.jpg 2x" alt=""></picture><h2 id="Conclusion"><a href="#Conclusion" title="Conclusion"></a>Conclusion</h2><p>Let me summarize all the theory into some <strong>actionable advice</strong>. There are two ways to create an accessible color palette with <em>magic numbers</em>:</p><ol><li>Create a color palette first and calculate the magic numbers with <a href="https://github.com/darekkay/a11y-contrast" target="_blank" rel="noopener">a11y-contrast</a>. Depending on the colors, the magic numbers might be rather high or even non-existent (see Open Color).</li><li>Define luminance bounds with fixed magic numbers and use only colors that can be mapped. This approach is more constraining and requires more work, but the result is a future-proof and consistent color palette.</li></ol><p>If you don’t want to go through all the work, I suggest you try out the <a href="https://designsystem.digital.gov/design-tokens/color/system-tokens/" target="_blank" rel="noopener">USWDS color palette</a>.</p></div></div></div>]]>
            </description>
            <link>https://darekkay.com/blog/accessible-color-palette/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24624914</guid>
            <pubDate>Tue, 29 Sep 2020 07:09:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Fun Yet Effective Meetings]]>
            </title>
            <description>
<![CDATA[
Score 27 | Comments 15 (<a href="https://news.ycombinator.com/item?id=24624819">thread link</a>) | @thesnide
<br/>
September 28, 2020 | https://blog.pwkf.org/2020/09/27/we-should-only-have-fun-meetings.html | <a href="https://web.archive.org/web/*/https://blog.pwkf.org/2020/09/27/we-should-only-have-fun-meetings.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <blockquote>
  <p>⚠️ This article is extreme &amp; satirical on the purpose of being thought-provoking. Therefore, please, do take it with some grain of salt.</p>

  <p>This is a followup of my <a href="https://blog.pwkf.org/2020/09/26/remote-working-is-a-paradigm-shift.html">Remote-mostly working is a paradigm shift</a>.</p>
</blockquote>



<p>My #1 advice about distributed teams is actually even the case in colocated teams:
<strong>Try as hard as possible to avoid meetings, use textual IM instead</strong> as <a href="https://blog.codinghorror.com/meetings-where-work-goes-to-die/">meetings are where work goes to die</a>.
I know it sounds pretty dull, but if you really think about it you’ll see that it’s usually wasted time.
The usual pattern of decision taking is:</p>

<div><div><pre><code>1. if you don’t have a clue, ask someone else.
2. to ask him, schedule a meeting
3. to schedule a meeting, you have to find a slot
4. that usually postpones the decision
</code></pre></div></div>

<p>Postponing the decision is usually what one really wants, even unknowingly: “<em>to be able to have an excuse not to decide at once</em>”.</p>

<p>It’s a very very fair humane reaction, I also fell myself into that trap.</p>

<p><img src="https://blog.pwkf.org/assets/images/out-of-window.jpg" alt="Thrown out of the fence for proposing an textual chat"></p>



<p>I end up having the following workflow:</p>

<ul>
  <li>Avoid email loops. Those have too much overhead, and will divide your audience pretty quickly.</li>
  <li>Create a slack channel instead of the email loop. This will retain the whole conversation in 1 central place.</li>
  <li>Systematically push back on any meetings. Accept them only if you have a clear statement of who will drive it.</li>
  <li>
    <p>Meetings should be a broadcast mode. All the Q&amp;A should go back to the slack channel.</p>

    <ul>
      <li>Having everything searchable makes it super easy for
        <ul>
          <li>newcomers that can simply scroll back</li>
          <li>old timers such as me that forget and can also simply scroll back</li>
        </ul>
      </li>
      <li>
        <p>Even recorded meetings are a vast of time if you need to find an information, as it’s not indexed.</p>
      </li>
      <li>If not recorded, sending the minutes afterwards are usually a loss of information.
        <ul>
          <li>It’s still very important to extract “executive summaries” from those meetings, even only textual. As can be posted on a public place for massive &amp; broad sharing.</li>
          <li>Yet, the real context on those summaries will be kept inside the whole meeting</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<blockquote>
  <p><strong>Context effectively fights Cargo Cult</strong></p>

  <p>Usually the why of decisions are not provided in a “<a href="https://english.stackexchange.com/questions/120739/a-peek-into-the-sausage-factory">build the sausage</a>” manner. But after a while, the hypothesis of the decision are not true anymore, and therefore another decision has to be taken.</p>

  <p>This is not possible when loss of information occurs and that’s when <a href="https://en.wikipedia.org/wiki/Cargo_cult_programming">Cargo cult</a> begins to creep in.</p>
</blockquote>

<p>In a record-everything scenario, context switch is just a matter of reading back some previous messages. Otherwise you’ll spend numerous times “explaining the context” in a meeting to everyone. And then everyone will just move to something else, and you’ll have pitiful engagement, while still spending time on it.</p>

<p><strong>A company that use its time effectively, is a company that moves much faster than its competitors</strong>, no matter the skillsets in that company. Even if you have the brightest minds, if you waste their talents in boring activities, you’ll dry them up. And either they’ll leave, or worse, they <a href="https://www.sbnonline.com/article/the-quit-and-stay-syndrome-a-business-epidemic-thats-a-silent-profit-killer/">quit &amp; stay</a>.</p>

<blockquote>
  <p>💡 This enabled me to scale to a reasonable multitasking ability while curbing its overhead to a manageable level</p>
</blockquote>

<p><strong>That’s why “startups” are so effective</strong> : they are small and therefore very fast. Being fast brings capacity to try &amp; fail, and therefore agility.</p>

<blockquote>
  <p>💡 <strong>time is the only really scarse resource in a company</strong> : you can’t buy it back, no matter the price.</p>
</blockquote>

<h2 id="why-you-should-only-have-fun-ones">Why you should only have fun ones</h2>

<p>That said, I do agree that travelling is necessary. But we should name the real usecase of travelling : having <em>fun</em> <strong>together</strong>.</p>

<p>The nicest part of that purpose is :</p>

<ul>
  <li>you can plan it far in advance</li>
  <li>you can also budget it in advance</li>
  <li>you can mentally map yourself in those fun times, and focus on actual work right now.</li>
</ul>



<p>The most used reason for meetings is “alignments”, but it is very usually wasted as :</p>

<ul>
  <li>There’s always someone missing for that meeting</li>
  <li>No-one really wants to write the minutes out of it</li>
  <li>If written nonetheless, those minutes are only seldom capturing the <em>why</em> the decision is reached. Which is the most important part!</li>
</ul>

<p>The “<em>there’s always someone missing from that meeting</em>” is the worst part, as usually, that is the one that says “what you decided cannot be done”. And you’ll end up with yet another round of thinking at best. At worst you’ll try to shoehorn your precious alignment (that did sink a huge travel budget) into the needs of that missing person.</p>

<blockquote>
  <p>💡 We can draw a parallel to the paradigm shift I mentioned earlier:</p>
</blockquote>

<ul>
  <li><strong>F2F</strong> Meetings are the <strong>monolith</strong> way.</li>
  <li>Informal, dedicated &amp; <strong>textual meetings</strong> are the <strong>micro-services</strong> way.</li>
</ul>

<p>Now, one immediately notices the following:</p>

<ul>
  <li><strong>People are very much at ease with monoliths</strong>. It takes a huge mental leap to go micro-services.</li>
  <li><strong>Micro-services have an overhead</strong>. But no-one will argue that they can scale way better.</li>
  <li>Monoliths can seldomly be distributed the way micro-services can : you’ll quickly end up with that infamous distributed monolith pattern.</li>
</ul>

<blockquote>
  <p>💡 Now, let’s travel only for “team building” purposes.</p>

  <p>You can plan them in advance, and you won’t have over-budget ones. As their outcome is predictable, and if there’s someone missing, it won’t jeopardy the whole travelling.</p>
</blockquote>

<p>A final word of caution, the reference to scaling is <strong>not about runtime performance</strong>. It’s more about <strong>organisation size</strong>. As Martin Fowler said : <em>don’t even consider microservices unless you have a system that’s too complex to manage as a monolith</em>.</p>

<p>So, it usually makes very much sense to start with a small, colocated team, that has traditional meetings. The trick is to recognize the need to change the paradigm when the team grows, as it always done smoothly over the months. Just don’t forget to have good meeting hygiene (written inputs &amp; outputs), as <strong>even monoliths should be nicely modular</strong>.</p>

<blockquote>
  <p>I posted those 2 articles (<a href="https://blog.pwkf.org/2020/09/26/remote-working-is-a-paradigm-shift.html">Remote-mostly working</a>, <a href="https://blog.pwkf.org/2020/09/27/we-should-only-have-fun-meetings.html">Effective Meetings</a>) internally to my company some years ago, but I’m republishing those publicly to help others the same way it helped us.</p>
</blockquote>

  </div>
</article>

      </div>
    </div></div>]]>
            </description>
            <link>https://blog.pwkf.org/2020/09/27/we-should-only-have-fun-meetings.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24624819</guid>
            <pubDate>Tue, 29 Sep 2020 06:54:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Blur attacks bypass Bluetooth Classic and BLE security mechanisms CVE-2020-15802]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24624206">thread link</a>) | @a5withtrrs
<br/>
September 28, 2020 | https://hexhive.epfl.ch/BLURtooth/ | <a href="https://web.archive.org/web/*/https://hexhive.epfl.ch/BLURtooth/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      
      
      

      <h2 id="blur-attacks">BLUR attacks</h2>

<p>BLURtooth (the BLUR attacks) exploits the lack of cross-transport key
validation, allowing an attacker to bypass Bluetooth Classic and Bluetooth Low
Energy security mechanisms.</p>

<p>Bluetooth’s cross-transport key derivation (CTKD) is vulnerable to attacks
enabling to attack Bluetooth Classic from Bluetooth Low Energy and vice versa.
A remote attacker in Bluetooth range may impersonate, man-in-the-middle, and
establish malicious sessions with arbitrary devices.</p>

<ul>
  <li><strong><em>Security Impact:</em></strong> device impersonation, man-in-the-middle, malicious
session establishment with arbitrary devices</li>
  <li><strong><em>Affected Devices:</em></strong> the attack is standard compliant, so all BT/BLE
devices supporting CTKD are likely vulnerable; all our tested devices are
vulnerable</li>
  <li>BLURtooth is tracked under <a href="https://kb.cert.org/vuls/id/589825">CVE-2020-15802</a></li>
  <li><strong><em>Credit:</em></strong> Daniele Antonioli and Mathias Payer
from École Polytechnique Fédérale de Lausanne (EPFL),
Nils Ole Tippenhauer from Helmholtz Center for Information Security (CISPA),
and Kasper Rasmussen from University of Oxford.</li>
  <li><strong><em>Contacts at EPFL:</em></strong>
<a href="mailto:daniele.antonioli@epfl.ch,mathias.payer@nebelwelt.net">Daniele Antonioli and Mathias Payer</a></li>
</ul>

<h2 id="summary">Summary</h2>

<p>Here, we provide more details about a set of novel and standard-compliant
Bluetooth vulnerabilities affecting both Bluetooth Classic (BT) and Bluetooth
Low Energy (BLE).  The uncovered vulnerabilities affect a security mechanism
called cross-transport key derivation (CTKD). CTKD is used to improve the
usability of Bluetooth pairing by allowing to generate BT and BLE pairing keys
just by pairing two devices either on BT or BLE (rather than pairing them two
times).</p>

<p>However, we find that CTKD introduces cross-transport security issues and that
an attacker can abuse those issues to attack BT from BLE and vice versa.  In
particular, our attacks enable to impersonate, man-in-the-middle, and establish
malicious sessions with arbitrary devices by abusing CTKD, while defeating all
the security mechanisms put in place by BT and BLE.  Our work is named BLURtooth
and the related attacks are called BLUR attacks as they blur the security
boundary between BT and BLE.</p>

<p>The team behind this work consists of
<a href="https://francozappa.github.io/">Daniele Antonioli</a>
and
<a href="https://nebelwelt.net/">Mathias Payer</a>
from the <a href="https://hexhive.epfl.ch/">HexHive group</a> at
École Polytechnique Fédérale de Lausanne (EPFL),
<a href="https://tippenhauer.de/">Nils Ole Tippenhauer</a>
from Helmholtz Center for Information Security (CISPA), and
<a href="https://www.cs.ox.ac.uk/people/kasper.rasmussen/">Kasper Rasmussen</a>
from the University of Oxford.</p>

<p>In the remainder of this document, we provide information on
technical details, disclosure, impact, our proposed mitigation, the response
from the Bluetooth SIG.</p>

<h2 id="technical-details">Technical Details</h2>

<p>The Bluetooth standard includes two technologies <em>Bluetooth Classic (BT)</em> (also
known as Bluetooth BR/EDR) and <em>Bluetooth Low Energy (BLE)</em>. The majority of
mobile devices, including laptops, smartphones, tablets, headphones, and
smartwatches, support both and are defined as <em>dual-mode</em> Bluetooth devices. To
securely use dual-mode devices over BT and BLE a user has to pair her devices
two times, once for BT and once for BLE. As pairing the same device is
considered <em>user-unfriendly</em>, in 2014, with the release of Bluetooth version
4.2, the Bluetooth standard introduced a security mechanism that allows a user
to pair dual-mode Bluetooth devices once (either over BT or BLE) and then
securely use them both over BT and BLE. This security mechanism is called
<em>cross-transport key derivation (CTKD)</em>, and, as the name implies, it enables
deriving pairing keys across different transports (i.e.  derive a BT pairing key
from BLE and vice versa).</p>

<p>Despite being a security-critical mechanism, CTKD is not part of the Bluetooth
threat model and there are no security evaluations of CTKD. Those reasons
motivated us to perform a security analysis of CTKD, resulting in our findings.
In particular, CTKD is affected by 5 major issues (i.e.  vulnerabilities)
enabling an attacker to abuse Bluetooth roles, association, security modes,
keys, and pairing states across BT and BLE. Such issues derive from the <em>lack of
a cross-transport threat model</em> in the Bluetooth standard. The standard
considers BT and BLE with separate threat models and security architectures
while, through CTKD, opens avenues for cross-transport attacks (i.e., attacks
that exploit BT by taking advantage of a vulnerability in BLE or vice versa).</p>

<p>We demonstrate that the identified CTKD issues can be exploited by a remote
attacker in Bluetooth range with the victims. In particular, the attacker can
perform impersonation, man-in-the-middle, and malicious session establishment
attacks while bypassing all the security mechanisms provided by BT and BLE
(including Secure Connections or strong association).  Those are very serious
attacks that violate the security guarantees promised by Bluetooth.  We
confirmed the feasibility of our attacks by testing them on 13 common Bluetooth
devices using 10 unique Bluetooth chips. All of them were vulnerable.</p>

<p>You will find technical details about CTKD, our security analysis, a detailed
discussion of the threads, a discussion, and potential mitigations in our
<a href="https://arxiv.org/abs/2009.11776">BLURtooth preprint</a>.</p>

<h2 id="disclosure">Disclosure</h2>

<p>We discovered the vulnerability in March 2020 and responsibly disclosed our
findings along with suggested countermeasures to the Bluetooth SIG in May 2020.
We kept our findings private and the Bluetooth SIG publicly disclosed them,
without informing us, on the 10th of September of 2020.  Our work is assigned
<em><a href="https://kb.cert.org/vuls/id/589825">CVE-2020-15802</a></em>.</p>

<h2 id="impact">Impact</h2>

<p>The BLUR attacks are a <em>significant threat for all Bluetooth users and
the related vulnerabilities remain 0-days</em>. Our claim
is backed up by our experimental results where we successfully conducted
impersonation, man-in-the-middle, and malicious sessions establishment attacks
on 13 different devices. Our device sample include manufacturers such as
Dell, Google, Lenovo, Samsung, and Sony, operating systems, such as Windows
10, Linux, and Android, and Bluetooth chip manufacturers such as Cypress,
Qualcomm, Intel, Broadcom, and Cambridge Silicon Radio (CSR).</p>

<h2 id="our-mitigations">Our Mitigations</h2>

<p>As part of our disclosure, we provided <em>concrete fixes to combat the BLUR
attacks</em>. In particular, we recommended disabling the capability to overwrite
keys via CTKD in certain circumstances, enforce strong association and Secure
Connections and roles across BT and BLE, disable pairing over BT and/or BLE when
not needed, and add user notifications in case of odd behaviors. Our fixes can
be implemented at the standard level and do not require vendor-specific
features.</p>

<h2 id="response-from-the-bluetooth-sig">Response from the Bluetooth SIG</h2>

<p>At the time of writing, there are <em>no deployed patches</em> to address the BLUR
attacks on actual devices.  The Bluetooth SIG suggested that version 5.1 of the
standard will contain guidelines to mitigate the BLUR attacks (e.g., disable key
overwrites in certain circumstances as proposed in our countermeasures), but
such guidelines are not (yet) public and we cannot comment on them.  The
Bluetooth SIG provides a <a href="https://www.bluetooth.com/learn-about-bluetooth/bluetooth-technology/bluetooth-security/blurtooth/">public statement about BLURtooth and the BLUR
attacks</a>.</p>


      
    </div></div>]]>
            </description>
            <link>https://hexhive.epfl.ch/BLURtooth/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24624206</guid>
            <pubDate>Tue, 29 Sep 2020 04:43:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Our Vision – Coalition for App Fairness]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24624006">thread link</a>) | @tambourine_man
<br/>
September 28, 2020 | https://appfairness.org/our-vision/ | <a href="https://web.archive.org/web/*/https://appfairness.org/our-vision/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content"><div><div id="primary"><main id="main"><article class="page" id="post-394" itemtype="https://schema.org/CreativeWork" itemscope="itemscope"><div itemprop="text"><div data-elementor-type="wp-page" data-elementor-id="394" data-elementor-settings="[]"><div><div><section data-id="e1785ac" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}"></section><section data-id="29e5348" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}"><div><div><div data-id="df3eaf0" data-element_type="column"><div><div><div data-id="9fdb9b1" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;,&quot;_animation_delay&quot;:300}" data-widget_type="text-editor.default"><div><p>The world’s most popular online platforms and the app stores that govern access to them have become a critical gateway to the consumers of digital products and services worldwide. While they can be beneficial when fairly operated, they can also be used by platform owners to hurt developers and consumers. As enforcers, regulators and legislators around the world seek to address these important issues, we, the Coalition for App Fairness, urge them to recognize that every app developer, regardless of size or the nature of the developer’s business, is entitled to fair treatment by these app stores and the platform owners who operate them, and should be afforded the following rights:</p></div></div></div></div></div></div></div></section><section data-id="e9c6c37" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;gradient&quot;,&quot;animation&quot;:&quot;none&quot;}"><div><div><div data-id="537c477" data-element_type="column" data-settings="{&quot;background_background&quot;:&quot;classic&quot;,&quot;animation&quot;:&quot;fadeInUp&quot;,&quot;animation_delay&quot;:300}"><div><div><section data-id="ec9b1aa" data-element_type="section" data-settings="{&quot;animation&quot;:&quot;none&quot;}"><div><div><div data-id="5b0d45c" data-element_type="column" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}"><div><div><div data-id="a4f6862" data-element_type="widget" data-widget_type="tp-info-box.default"><div><div id="info_box5f759262a460b" data-id="info_box5f759262a460b" data-connection="" data-eventtype=""><div><div><div><div><p><img src="https://appfairness.org/wp-content/uploads/2020/09/icon-number_1.png" alt="one"></p><div><p>No developer should be required to use an app store exclusively, or to use ancillary services of the app store owner, including payment systems, or to accept other supplementary obligations in order to have access to the app store.</p></div></div></div></div></div></div></div></div></div></div></div><div data-id="5eefa74" data-element_type="column" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}"><div><div><div data-id="143ef27" data-element_type="widget" data-widget_type="tp-info-box.default"><div><div id="info_box5f759262a6f53" data-id="info_box5f759262a6f53" data-connection="" data-eventtype=""><div><div><div><div><p><img src="https://appfairness.org/wp-content/uploads/2020/09/icon-number_2.png" alt="two"></p><div><p>No developer should be blocked from the platform or discriminated against based on a developer’s business model, how it delivers content and services, or whether it competes in any way with the app store owner.</p></div></div></div></div></div></div></div></div></div></div></div></div></div></section><section data-id="a36f301" data-element_type="section" data-settings="{&quot;animation&quot;:&quot;none&quot;}"><div><div><div data-id="c13739d" data-element_type="column" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}"><div><div><div data-id="dd74397" data-element_type="widget" data-widget_type="tp-info-box.default"><div><div id="info_box5f759262aa219" data-id="info_box5f759262aa219" data-connection="" data-eventtype=""><div><div><div><div><p><img src="https://appfairness.org/wp-content/uploads/2020/09/icon-number_3.png" alt="three"></p><div><p>Every developer should have timely access to the same interoperability interfaces and technical information as the app store owner makes available to its own developers.</p></div></div></div></div></div></div></div></div></div></div></div><div data-id="d7acc59" data-element_type="column" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}"><div><div><div data-id="c8f0078" data-element_type="widget" data-widget_type="tp-info-box.default"><div><div id="info_box5f759262aca14" data-id="info_box5f759262aca14" data-connection="" data-eventtype=""><div><div><div><div><p><img src="https://appfairness.org/wp-content/uploads/2020/09/icon-number_4.png" alt="four"></p><div><p>Every developer should always have access to app stores as long as its app meets fair, objective and nondiscriminatory standards for security, privacy, quality, content, and digital safety.</p></div></div></div></div></div></div></div></div></div></div></div></div></div></section><section data-id="c3f38e1" data-element_type="section" data-settings="{&quot;animation&quot;:&quot;none&quot;}"><div><div><div data-id="c29ef73" data-element_type="column" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}"><div><div><div data-id="6d00f0d" data-element_type="widget" data-widget_type="tp-info-box.default"><div><div id="info_box5f759262afa15" data-id="info_box5f759262afa15" data-connection="" data-eventtype=""><div><div><div><div><p><img src="https://appfairness.org/wp-content/uploads/2020/09/icon-number_5.png" alt="five"></p><div><p>A developer’s data should not be used to compete with the developer.</p></div></div></div></div></div></div></div></div></div></div></div><div data-id="7c1173a" data-element_type="column" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}"><div><div><div data-id="aaac757" data-element_type="widget" data-widget_type="tp-info-box.default"><div><div id="info_box5f759262b1f47" data-id="info_box5f759262b1f47" data-connection="" data-eventtype=""><div><div><div><div><p><img src="https://appfairness.org/wp-content/uploads/2020/09/icon-number_6.png" alt="six"></p><div><p>Every developer should always have the right to communicate directly with its users through its app for legitimate business purposes.</p></div></div></div></div></div></div></div></div></div></div></div></div></div></section><section data-id="00e085e" data-element_type="section" data-settings="{&quot;animation&quot;:&quot;none&quot;}"><div><div><div data-id="0931faa" data-element_type="column" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}"><div><div><div data-id="be80356" data-element_type="widget" data-widget_type="tp-info-box.default"><div><div id="info_box5f759262b4f4b" data-id="info_box5f759262b4f4b" data-connection="" data-eventtype=""><div><div><div><div><p><img src="https://appfairness.org/wp-content/uploads/2020/09/icon-number_7.png" alt="seven"></p><div><p>No app store owner or its platform should engage in self-preferencing its own apps or services, or interfere with users’ choice of preferences or defaults.</p></div></div></div></div></div></div></div></div></div></div></div><div data-id="c58956e" data-element_type="column" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}"><div><div><div data-id="e726b73" data-element_type="widget" data-widget_type="tp-info-box.default"><div><div id="info_box5f759262b7667" data-id="info_box5f759262b7667" data-connection="" data-eventtype=""><div><div><div><div><p><img src="https://appfairness.org/wp-content/uploads/2020/09/icon-number_8.png" alt="eight"></p><div><p>No developer should be required to pay unfair, unreasonable or discriminatory fees or revenue shares, nor be required to sell within its app anything it doesn’t wish to sell, as a condition to gain access to the app store.</p></div></div></div></div></div></div></div></div></div></div></div></div></div></section><section data-id="b79a45e" data-element_type="section" data-settings="{&quot;animation&quot;:&quot;none&quot;}"><div><div><div data-id="35127a4" data-element_type="column" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}"><div><div><div data-id="669eafc" data-element_type="widget" data-widget_type="tp-info-box.default"><div><div id="info_box5f759262ba390" data-id="info_box5f759262ba390" data-connection="" data-eventtype=""><div><div><div><div><p><img src="https://appfairness.org/wp-content/uploads/2020/09/icon-number_9.png" alt="nine"></p><div><p>No app store owner should prohibit third parties from offering competing app stores on the app store owner’s platform, or discourage developers or consumers from using them.</p></div></div></div></div></div></div></div></div></div></div></div><div data-id="e891f97" data-element_type="column" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}"><div><div><div data-id="a8cb9a4" data-element_type="widget" data-widget_type="tp-info-box.default"><div><div id="info_box5f759262bc8f3" data-id="info_box5f759262bc8f3" data-connection="" data-eventtype=""><div><div><div><div><p><img src="https://appfairness.org/wp-content/uploads/2020/09/icon-number_10.png" alt="ten"></p><div><p>All app stores will be transparent about their rules and policies and opportunities for promotion and marketing, apply these consistently and objectively, provide notice of changes, and make available a quick, simple and fair process to resolve disputes.</p></div></div></div></div></div></div></div></div></div></div></div></div></div></section></div></div></div></div></div></section><section data-id="d605c57" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}"><div><div><div data-id="1d8304b" data-element_type="column"><div><div><div data-id="15c3e7a" data-element_type="widget" data-settings="{&quot;_animation&quot;:&quot;fadeIn&quot;,&quot;_animation_delay&quot;:400}" data-widget_type="text-editor.default"><div><p>The Coalition for App Fairness was created by industry leading companies who want to see freedom of choice for consumers and a level playing field for businesses. This is an open call to all developers, big and small, to join us – and together we will fight back against the monopolist control of the app ecosystem by Apple.</p></div></div></div></div></div></div></div></section></div></div></div></div></article></main></div></div></div></div>]]>
            </description>
            <link>https://appfairness.org/our-vision/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24624006</guid>
            <pubDate>Tue, 29 Sep 2020 03:50:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Possible reason for crashes of the Nvidia RTX 3080 and RTX 3090]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24623963">thread link</a>) | @g42gregory
<br/>
September 28, 2020 | https://www.igorslab.de/en/what-real-what-can-be-investigative-within-the-crashes-and-instabilities-of-the-force-rtx-3080-andrtx-3090/ | <a href="https://web.archive.org/web/*/https://www.igorslab.de/en/what-real-what-can-be-investigative-within-the-crashes-and-instabilities-of-the-force-rtx-3080-andrtx-3090/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><article id="post-149171"><div><div><div><div><div><p>Not only the editors and testers were surprised by sudden instabilities of the new GeForce RTX 3080 and RTX 3090, but also the first customers who were able to get board partner cards from the first wave. An interesting pattern of behavior emerged that did not affect all cards or manufacturers and the problems only occurred at certain boost clock rates above or just around 2 GHz. To make matters worse, NVIDIA has obviously also slightly undermined the quality management of the board partners (AIC) due to the secrecy – unconsciously, of course, but with plausible consequences. A chain of adverse circumstances? This could well be the case, because this explains the somewhat diffuse error pattern from the most diverse forums.</p><h3><span><strong>Start of production without real function control?</strong></span></h3><p>Let’s start with the latter, before I get lost in the technical analyses. You probably remember when I wrote that the board partners couldn’t use working drivers yet and only work with a very limited driver and NVPunish. Since the driver problem lasted until shortly before the launch, but the first wave of cards had to be produced already, the functional testing of the first models was obviously limited to power-on and thermal stability. Running, not running. However, this does not say much about the chip quality and the possible maximum frequencies that the respective chip can safely handle.</p><p>Thus, it would at least be plausible that cards could have been sold as OC cards, which wouldn’t have passed a real quality test at the manufacturer with the delivered settings. Real binning? Nothing. Subsequent selection of particularly overclocked cards? Impossible, in fact. And so it is by no means impossible that one or the other “Potato” chip could also have gotten lost on such an OC card. We know the consequences from the posts of the buyers in the relevant forums.</p><h3><span><strong>Wrong component selection? Plausible!</strong></span></h3><p>Now let’s come to the fact that even good chips have dropped out now and then. That they are good, you can see for yourself e.g. by the boost cycle and the temperatures. So it is quite easy to find out with a selected card. This brings us now to a point that I was actually very unconsciously haunting the back of my mind at first, and which then solidified into a realization when comparing the boards of different models So let’s go directly to the “reference board” PG132, which can also be understood as a so-called Base Design. Especially the backside and especially the area below the BGA is interesting. What is interesting about such drawings and the so-called BoM (Bill of Materials) is that you are offered different placement alternatives.</p><p><a href="https://www.igorslab.de/wp-content/uploads/2020/09/Bottom-POSCAP-vs-MLCC.jpg"><img loading="lazy" src="https://www.igorslab.de/wp-content/uploads/2020/09/Bottom-POSCAP-vs-MLCC.jpg" alt="" width="980" height="765" srcset="https://www.igorslab.de/wp-content/uploads/2020/09/Bottom-POSCAP-vs-MLCC.jpg 980w, https://www.igorslab.de/wp-content/uploads/2020/09/Bottom-POSCAP-vs-MLCC-300x234.jpg 300w, https://www.igorslab.de/wp-content/uploads/2020/09/Bottom-POSCAP-vs-MLCC-768x600.jpg 768w" sizes="(max-width: 980px) 100vw, 980px"></a></p><p>I will (have to) simplify the following for better understanding. Below the BGA we see the six NECESSARY capacitors for filtering high frequencies on the voltage rails, i.e. NVVDD and MSVDD. Apart from the fact that there is still enough high-frequency “garbage” from the voltage converters, it is mainly the so-called GPU load including all jumps caused by boost, which leads to very broadband frequency mixtures, which become more extreme the higher the boost clock goes. The BoM and the drawing from June leave it open whether large-area POSCAPs (Conductive Polymer Tantalum Solid Capacitors) are used (marked in red), or rather the somewhat more expensive MLCCs (Multilayer Ceramic Chip Capacitor). The latter are smaller and have to be grouped for a higher capacity.</p><p>According to the list and specifications of Nvidia, both are possible. In terms of quality, however, good MLCCs are better able to filter the very high frequency components in particular. In the end, this is simple practical knowledge, which only often enough collides with the world view of a financial controller.&nbsp; If one searches the forums, it seems that the Zotac Trinity is particularly affected when it comes to instabilities starting at certain boost clock rates from around 2010 MHz. A feat, because Zotac is relying on a total of six cheaper POSCAPs.</p><p><a href="https://www.igorslab.de/wp-content/uploads/2020/09/Zotac-Trinity.jpg"><img loading="lazy" src="https://www.igorslab.de/wp-content/uploads/2020/09/Zotac-Trinity.jpg" alt="" width="980" height="390" srcset="https://www.igorslab.de/wp-content/uploads/2020/09/Zotac-Trinity.jpg 980w, https://www.igorslab.de/wp-content/uploads/2020/09/Zotac-Trinity-300x119.jpg 300w, https://www.igorslab.de/wp-content/uploads/2020/09/Zotac-Trinity-768x306.jpg 768w" sizes="(max-width: 980px) 100vw, 980px"></a></p><p>And what does NVIDIA do with its own Founders Editions? One does it obviously better, because I could not reproduce these stability problems with any FE even very clearly beyond 2 GHz (fan to 100%). If something went wrong, it was almost certainly a driver problem. If we take a look at the FE, we see only four SP-CAPs (red) and in the middle two MLCC groups of 10 individual capacitors each (green). This is definitely the better solution and the optimal compromise. because especially the middle areas should best be provided with suitable filters (short circuit of the high-frequency frequency mixtures).</p><p><a href="https://www.igorslab.de/wp-content/uploads/2020/09/Founders-1.jpg"><img loading="lazy" src="https://www.igorslab.de/wp-content/uploads/2020/09/Founders-1.jpg" alt="" width="980" height="406" srcset="https://www.igorslab.de/wp-content/uploads/2020/09/Founders-1.jpg 980w, https://www.igorslab.de/wp-content/uploads/2020/09/Founders-1-300x124.jpg 300w, https://www.igorslab.de/wp-content/uploads/2020/09/Founders-1-768x318.jpg 768w" sizes="(max-width: 980px) 100vw, 980px"></a></p><p>If it is only about NVVDD, a single MLCC block may be sufficient to solve the most serious problems. For example, MSI uses only one on the Gaming X Trio, which is theoretically enough, but could have been better solved if, for example, the 2.1 GHz were to be used with water. Whether this is still enough would of course have to be tested. PC Partner, Zotac’s mother company, seems to have recognised this and is obviously changing its cards. By the way, the following example is from a soldering experiment that was NOT made by Zotac, but which confirmed the effectiveness of MLCC smoothing very impressively. One can almost be envious of these soldering skills.</p><p><a href="https://www.igorslab.de/wp-content/uploads/2020/09/Hand-made-MLCC.jpg"><img loading="lazy" src="https://www.igorslab.de/wp-content/uploads/2020/09/Hand-made-MLCC.jpg" alt="" width="980" height="497" srcset="https://www.igorslab.de/wp-content/uploads/2020/09/Hand-made-MLCC.jpg 980w, https://www.igorslab.de/wp-content/uploads/2020/09/Hand-made-MLCC-300x152.jpg 300w, https://www.igorslab.de/wp-content/uploads/2020/09/Hand-made-MLCC-768x389.jpg 768w" sizes="(max-width: 980px) 100vw, 980px"></a></p><p>By the way, you also have to praise a company here that recognized the whole thing from the start and didn’t even let it touch them, as the Asus TUF RTX 3080 Gaming consequently did without POSCAPs and only used MLCC groups. My compliments, it fits!</p><p><a href="https://www.igorslab.de/wp-content/uploads/2020/09/Asus-TUF-1.jpg"><img loading="lazy" src="https://www.igorslab.de/wp-content/uploads/2020/09/Asus-TUF-1.jpg" alt="" width="980" height="439" srcset="https://www.igorslab.de/wp-content/uploads/2020/09/Asus-TUF-1.jpg 980w, https://www.igorslab.de/wp-content/uploads/2020/09/Asus-TUF-1-300x134.jpg 300w, https://www.igorslab.de/wp-content/uploads/2020/09/Asus-TUF-1-768x344.jpg 768w" sizes="(max-width: 980px) 100vw, 980px"></a></p><p>Interestingly, all board partners are silent on this issue, no matter who you ask. No answer is also an answer, because this behaviour is the absolute exception and almost resembles a muzzle decree. This is because components are normally spoken freely when the launch has already taken place. But here comes nothing but meaningful silence. This also applies to the question of whether the BoM was subsequently changed again to completely exclude the exclusive use of POSCAPs/SP-CAPs.</p><p>Sometimes things are so obvious that you really have to look several times to see them. But once you have understood it, many things suddenly go from nebulous to plausible. NVIDIA, by the way, cannot be blamed directly, because the fact that MLCCs work better than POSCAPs is something that any board designer who hasn’t taken the wrong profession knows. Such a thing can even be simulated if necessary. I will of course stay on it, because my interest is naturally aroused.</p><p>Please read also the latest follow up to that sory:</p><h2><a href="https://www.igorslab.de/en/nvidia-geforce-rtx-3080-und-rtx-3090-and-the-crash-why-the-capacitors-are-so-important-and-what-are-the-object-behind/" target="_blank" rel="noopener noreferrer">NVIDIA GeForce RTX 3080 and RTX 3090 and the crashes – Why capacitors are so important and what’s behind them</a></h2><p><iframe title="Aufgedeckt: Gründe, warum eine NVIDIA GeForce RTX 3080 oder RTX 3090 oberhalb 2 GHz crashen könnten!" width="1320" height="743" src="https://www.youtube.com/embed/7YQ7rNgoqMA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p> </div></div></div></div></div></article></div></div>]]>
            </description>
            <link>https://www.igorslab.de/en/what-real-what-can-be-investigative-within-the-crashes-and-instabilities-of-the-force-rtx-3080-andrtx-3090/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24623963</guid>
            <pubDate>Tue, 29 Sep 2020 03:38:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[OpenStreetMap State of the Map conf 2020 – a few thoughts on the experiment]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24623160">thread link</a>) | @pabs3
<br/>
September 28, 2020 | http://blog.imagico.de/sotm-2020-a-few-thoughts-on-the-experiment/ | <a href="https://web.archive.org/web/*/http://blog.imagico.de/sotm-2020-a-few-thoughts-on-the-experiment/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				<p>Last weekend has been the <a href="https://2020.stateofthemap.org/">2020 State of the Map conference</a> – which did not take place like it was originally planned and as it has been conducted in the past years at a specific physical place (in this case Capetown, South Africa) but was done in a purely virtual distributed form across the internet.</p>
<p>I regard this change – forced by the pandemic situation we all struggle with these days in some form – as in a way a welcome disruption.  Due to an outside event the powers-that-be have been forced to try something they would not have tried probably in many years to come otherwise.</p>
<p>The implementation of the virtual distributed conference as an afterthought on an originally planned physical single place event led of course to some flaws and inconsistencies in the practical setup and to not using the full potential of the virtual setting in all of its aspects.  This is obviously owed a lot to the desire not to throw away work already done.  The most obvious issue resulting from that approach is that the main conference program contained almost exclusively program items submitted by people under the original premise of a physical conference – or in other words:  The chance to hold a talk at the virtual conference still depended on the willingness and ability of people to travel to South Africa and be there for the talk in person.</p>
<p>This means the conference in its program was not even remotely as diverse as it could have been it it had been set up as a distributed remote conference in the first place.  This should IMO be kept in mind by everyone evaluating how SotM 2020 turned out.</p>
<p>I regard the whole event mostly as an experiment to test various techniques and methods and means of communication to have a virtual conference in the OSM context.  This applies both to behind-the-scene infrastructure and the public interfaces.  If the SotM WG documents and shares their findings publicly that could have use far beyond SotM for the OSM community.</p>
<h3>Practical observations from the conference</h3>
<p>The pads for collecting questions and comments on talks worked great.  This is definitely a concept that could play a central role in future distributed conferences.  Initially the questions were asked anonymously which has led in particular in case of Frederik’s talk to quite a lot of people making vile comments under the disguise of anonymity.  It was later established that questions and comments should be signed.  I also think that the use of pads could be extended to non-talk program items like self organized sessions.</p>
<div id="attachment_9119"><p><a href="https://pad.sotm.bitcast.co.za/p/general-feedback"><img src="http://blog.imagico.de/wp-content/uploads/2020/07/sotm2020_pad.png" alt="" width="512" height="429" srcset="http://blog.imagico.de/wp-content/uploads/2020/07/sotm2020_pad.png 512w, http://blog.imagico.de/wp-content/uploads/2020/07/sotm2020_pad-320x268.png 320w" sizes="(max-width: 512px) 100vw, 512px"></a></p><p>general feedback pad of the conference – there was a similar pad for questions and comments on each of the talks</p></div>
<p>The attractiveness of the pads to a large extent comes from the real time capability (which is essential for a real time conference obviously) combined with the non-linear free form structure of the text (which contrasts pleasantly with most other real time communication channels that tend to have a strictly linear structure).</p>
<p>There are quite a few things that could be improved about the audio.  This starts with the levels of the pause music relative to the talk audio levels and continues with reverberations in poorly dampened rooms of some presenters and feedback noise in some people’s audio setup.  That is mostly a matter of sufficient testing and experience with setting up and adjusting equipment in a way that works well.  That takes time from everyone involved obviously.  This is the hardest the first time but gets easier once you gain experience.  And i am confident with the corona virus crisis incentivizing many people to gain more practice in remote communication knowledge and experience in this field is much improving every day.  More communication about how to ensure good audio recording and communication quality within the community, sharing experiences and techniques used, would definitely be helpful.</p>
<p>None the less what also became clear to me during the conference is that the willingness of people to engage in communication was very clearly in the order written conversation &gt; audio communication &gt; video.  I think this is an observation to consider for any audio or video conversation in the OSM context.  Video meetings might be very convenient for heavily engaged extroverted community members with a pre-existing prominence but for many people this can be a source of discomfort.  And cultural and language barriers can be strongly emphasized by use of real time audio and especially video communication.</p>
<h3>Comments on the talks</h3>
<p>I have not watched all the talks of the conference so this is more a list of anecdotal observations than a complete review.  All the talks of the main conference program were pre-recorded while the Q&amp;A after the talks were live.  The pre-recorded talks offered a lot of options for presenters which would not be available in a live conference talk and which were used very differently by the presenters.  Ilya in his talk <em><a href="https://2020.stateofthemap.org/sessions/CKYTVS">Send me a Postcard</a></em> IMO showed the most innovative approach to this.  Watching this talk is recommended to anyone who in the future might be in the position to pre-record a conference talk as a positive example.</p>
<p>Some of the talks i watched so far that i consider particularly interesting:</p>
<div id="attachment_9121"><p><a href="https://2020.stateofthemap.org/sessions/RRVNAM"><img src="http://blog.imagico.de/wp-content/uploads/2020/07/sotm2020_allan2.png" alt="" width="512" height="288" srcset="http://blog.imagico.de/wp-content/uploads/2020/07/sotm2020_allan2.png 512w, http://blog.imagico.de/wp-content/uploads/2020/07/sotm2020_allan2-320x180.png 320w" sizes="(max-width: 512px) 100vw, 512px"></a></p><p>Allan’s American perspective on the political spectrum of OSM</p></div>
<p>Allan’s keynote <em><a href="https://2020.stateofthemap.org/sessions/RRVNAM">Winds of Change in OpenStreetMap</a></em> – While this did not provide much new information of substance to those following OSMF politics in general and who have read past statements from Allan on that subject, it seems to provide a valuable glimpse into the current mentality of the OSMF board regarding their work.  Although Allan had a prominent disclaimer that these are his personal views and do not represent those of the board, it is quite clear from statements and actions of other board members that they see many of these things similarly.  There is quite a lot of accurate analysis in the talk but also quite a few highly questionable selective perceptions, assumptions and conclusions.  I might comment about some of those separately although it is not clear at this time if the board is currently willing to openly discuss the merits of their views and opinions on the OSM community and the future of the OSM project and on the OSMFs role and defend their views and conclusions on these matters in a public setting.</p>
<div id="attachment_9122"><p><a href="https://2020.stateofthemap.org/sessions/DYXWDC"><img src="http://blog.imagico.de/wp-content/uploads/2020/07/sotm2020_fred.png" alt="" width="512" height="288" srcset="http://blog.imagico.de/wp-content/uploads/2020/07/sotm2020_fred.png 512w, http://blog.imagico.de/wp-content/uploads/2020/07/sotm2020_fred-320x180.png 320w" sizes="(max-width: 512px) 100vw, 512px"></a></p><p>Frederik explaining OpenStreetMap</p></div>
<p>Frederik’s talk <em><a href="https://2020.stateofthemap.org/sessions/DYXWDC">There might have been a misunderstanding…</a></em> – As usual Frederik explains in a well understandable way many of the central aspects of the OpenStreetMap project which new contributors as well as data users often struggle with because they differ from what people are used to, either in other internet communities or in the world of geodata.  Naturally, a lot of these frequently misunderstood aspects of OSM are also fairly controversial and this has – as hinted above – led to a lot of critical and in parts insulting comments on the talk by people who would like these things to change and for OSM to become more compatible with their expectations.  What Frederik presents however is for the most part not wishful thinking – presenting how he would like OSM to be – but how OpenStreetMap actually works and functions based on knowledge derived from many years of practical involvement in the project.  Other long term participants will largely be able to confirm that.  So whether you like these aspects of OSM or not and in what direction you might want OSM to develop in the future this is a very useful talk to watch to understand how OpenStreetMap ticks.  </p>
<div id="attachment_9123"><p><a href="https://2020.stateofthemap.org/sessions/RHDUV9"><img src="http://blog.imagico.de/wp-content/uploads/2020/07/sotm2020_mikel.png" alt="" width="512" height="288" srcset="http://blog.imagico.de/wp-content/uploads/2020/07/sotm2020_mikel.png 512w, http://blog.imagico.de/wp-content/uploads/2020/07/sotm2020_mikel-320x180.png 320w" sizes="(max-width: 512px) 100vw, 512px"></a></p><p>Mikel mocking concerns about conflicts of interest of corporate employees in the OSMF</p></div>
<p>Mikel’s talk <em><a href="https://2020.stateofthemap.org/sessions/RHDUV9">An Incomplete History of Companies and Professionals in OpenStreetMap</a></em> – Essentially Mikel is painting corporate activities in OSM and their history in rosy colors while saying: just pay no attention to all the skeletons lying around here.  A lot could be criticized about selective presentations of facts as well as factual and logical errors or about the technique of jokingly dismissing and ridiculing differentiated philosophical critique of the influence of corporate interests in OSM.  Anyway – I think this is a valuable talk to watch to get a glimpse into the mindset of many corporate employees involved in OSM as part of or in relation to their job.  </p>
<div id="attachment_9124"><p><a href="https://2020.stateofthemap.org/sessions/DZ8PWQ"><img src="http://blog.imagico.de/wp-content/uploads/2020/07/sotm2020_janet.png" alt="" width="512" height="288" srcset="http://blog.imagico.de/wp-content/uploads/2020/07/sotm2020_janet.png 512w, http://blog.imagico.de/wp-content/uploads/2020/07/sotm2020_janet-320x180.png 320w" sizes="(max-width: 512px) 100vw, 512px"></a></p><p>Janet explaining aid work in rural Tanzania</p></div>
<p>Janet’s talk <em><a href="https://2020.stateofthemap.org/sessions/DZ8PWQ">Building mapping communities in rural Tanzania – challenges, successes and lessons learnt</a></em> – I found this interesting because of a certain observation.  In the beginning a number of specific non mapping related examples are shown of aid being given to people in rural areas of Tanzania for everyday life problems.  And emphasis is admirably given to helping locals solving these problems themselves in a sustainable and independent fashion using locally available means.  Yet when it comes to mapping and digital technology the same initiative (and from what i know also many other humanitarian mapping projects) critiquelessly rely on commercial services and proprietary tools and encourage locals to use and rely on those services and tools that increase and perpetuate dependence of local people on non-local corporations for their local mapping work instead of educating people in using open source technology and tools they can manage and control themselves.</p>
<p>To be clear, i am not at all saying that this talk in any way constitutes an example for particularly bad practice in that regard, on the contrary the examples shown illustrate a principal awareness of the issue that is missing elsewhere.  But to me it demonstrates quite well how fundamentally different measures are applied to the goal of supplying aid in a way that enables locals to solve serious problems in a sustainable fashion outside the digital world and within it.  </p>
<div id="attachment_9125"><p><a href="https://2020.stateofthemap.org/sessions/CKYTVS"><img src="http://blog.imagico.de/wp-content/uploads/2020/07/sotm2020_ilya.png" alt="" width="512" height="288" srcset="http://blog.imagico.de/wp-content/uploads/2020/07/sotm2020_ilya.png 512w, http://blog.imagico.de/wp-content/uploads/2020/07/sotm2020_ilya-320x180.png 320w" sizes="(max-width: 512px) 100vw, 512px"></a></p><p>Ilya making a case for sending postcards in an innovative style video</p></div>
<p>Ilya’s talk <em><a href="https://2020.stateofthemap.org/sessions/CKYTVS">Send me a Postcard</a></em> – I mentioned his talk already above as an example for making innovative use of the …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://blog.imagico.de/sotm-2020-a-few-thoughts-on-the-experiment/">http://blog.imagico.de/sotm-2020-a-few-thoughts-on-the-experiment/</a></em></p>]]>
            </description>
            <link>http://blog.imagico.de/sotm-2020-a-few-thoughts-on-the-experiment/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24623160</guid>
            <pubDate>Tue, 29 Sep 2020 01:09:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Julia mixed precision GEMM codegen meets and exceeds CUBLAS]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24623153">thread link</a>) | @amkkma
<br/>
September 28, 2020 | https://juliagpu.org/2020-09-28-gemmkernels/ | <a href="https://web.archive.org/web/*/https://juliagpu.org/2020-09-28-gemmkernels/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <main id="main">
        





<i data-feather="calendar"></i> <time datetime="2020-09-28">Sep 28, 2020</time>




  <br>
  <i data-feather="edit-2"></i> Thomas Faignaert, Tim Besard, Bjorn De Sutter


<p>General Matrix Multiplication or GEMM kernels take center place in high performance
computing and machine learning. Recent NVIDIA GPUs include GEMM accelerators, such as
NVIDIA’s Tensor Cores. In this paper we show how it is possible to program these
accelerators from Julia, and present abstractions and interfaces that allow to do so
efficiently without sacrificing performance.</p>
<p>A pre-print of the paper has been published on arXiv:
<a href="https://arxiv.org/abs/2009.12263">arXiv:2009.12263</a>. <br> The source code can be found on
GitHub:
<a href="https://github.com/thomasfaingnaert/GemmKernels.jl">thomasfaingnaert/GemmKernels.jl</a>.</p>
<p>With the APIs from GemmKernels.jl, it is possible to instantiate GEMM kernels that perform
in the same ball park as, and sometimes even outperform state-of-the-art libraries like
CUBLAS and CUTLASS. For example, performing a mixed-precision multiplication of two 16-bit
matrixes into a 32-bit accumulator (on different combinations of layouts):</p>


<figure>
	<img src="https://juliagpu.org/2020-09-28-gemmkernels/mixed_precision.png" alt="Performance of mixed-precision GEMM">
</figure>

<p>The APIs are also highly flexible and allow customization of each step, e.g., to apply the
activation function <code>max(x, 0)</code> for implementing a rectified linear unit (ReLU):</p>
<div><pre><code data-lang="julia">a <span>=</span> CuArray(rand(<span>Float16</span>, (M, K)))
b <span>=</span> CuArray(rand(<span>Float16</span>, (K, N)))
c <span>=</span> CuArray(rand(<span>Float32</span>, (M, N)))
d <span>=</span> similar(c)

conf <span>=</span> GemmKernels<span>.</span>get_config(
    gemm_shape <span>=</span> (M <span>=</span> M, N <span>=</span> N, K <span>=</span> K),
    operator <span>=</span> Operator<span>.</span>WMMAOp{<span>16</span>, <span>16</span>, <span>16</span>},
    global_a_layout <span>=</span> Layout<span>.</span>AlignedColMajor{<span>Float16</span>},
    global_c_layout <span>=</span> Layout<span>.</span>AlignedColMajor{<span>Float32</span>})

GemmKernels<span>.</span>matmul(
    a, b, c, d, conf;
    transform_regs_to_shared_d <span>=</span> Transform<span>.</span>Elementwise(x <span>-&gt;</span> max(x, <span>0</span>)))
</code></pre></div><p>The GemmKernels.jl framework is written entirely in Julia, demonstrating the
high-performance GPU programming capabilities of this language, but at the same time keeping
the research accessible and easy to modify or repurpose by other Julia developers.</p>



      </main>
    </div></div>]]>
            </description>
            <link>https://juliagpu.org/2020-09-28-gemmkernels/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24623153</guid>
            <pubDate>Tue, 29 Sep 2020 01:08:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[OpenBSD on the Desktop (Part I)]]>
            </title>
            <description>
<![CDATA[
Score 61 | Comments 33 (<a href="https://news.ycombinator.com/item?id=24622788">thread link</a>) | @upofadown
<br/>
September 28, 2020 | https://paedubucher.ch/articles/2020-09-05-openbsd-on-the-desktop-part-i.html | <a href="https://web.archive.org/web/*/https://paedubucher.ch/articles/2020-09-05-openbsd-on-the-desktop-part-i.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Let's install OpenBSD on a Lenovo Thinkpad X270. I used this computer for my
computer science studies. It has both Arch Linux and Windows 10 installed as
dual boot. Now that I'm no longer required to run Windows, I can ditch the dual
boot and install an operating system of my choice.</p>

<p>First, I grab my work Thinkpad running Arch Linux and some USB dongle big enough
for the <a href="https://cdn.openbsd.org/pub/OpenBSD/6.7/amd64/miniroot67.fs">amd64 miniroot
image</a> (roughly
five megabytes, that is). This small image does not include the file sets, which
will be downloaded during installation instead. I also download the <a href="https://mirror.ungleich.ch/pub/OpenBSD/6.7/amd64/SHA256">SHA256
checksums</a> from the
Swiss mirror, and verify the downloaded image, before I copy it on my dongle:</p>
<pre><code>$ sha256sum -c --ignore-missing SHA256 
miniroot67.fs: OK
$ sudo dd if=miniroot67.fs of=/dev/sda bs=1M
</code></pre>

<p>The Thinkpad X270 is connected to my network through Ethernet. The WiFi firmware
usually needs to be installed separately, so only Ethernet will work out of the
box. The BIOS has UEFI activated. OpenBSD and UEFI has issues on older hardware
(at least on a 2014 Dell laptop I have), but let's try it on this laptop,
anyway.</p>
<p>I plug in the dongle prepared before, and start the computer. I interrupt
the regular boot with Enter and pick an alternative boot method by pressing F12.
Now I pick my USB dongle. After roughly a minute, the installer has been
started. Now I follow these steps:</p>
<ul>
<li>I choose the option <code>I</code> to install OpenBSD.</li>
<li>For the keyboard layout, I pick <code>sg</code>, for Swiss German.</li>
<li>As a hostname, I simply pick <code>x270</code>, because it's a Thinkpad X270, and I'm not
  very creative when it comes to naming things.</li>
<li>From the available network options (<code>iwm0</code>: WiFi, <code>em0</code>: Ethernet, and
  <code>vlan0</code>: Virtual LAN), I pick <code>em0</code>.</li>
<li>I try to get an IPv4 address over DHCP, which seems to work very quickly.</li>
<li>Next, I type in my very secret root password twice.</li>
<li>I do <em>not</em> start <code>sshd</code> by default, because I don't need to connect to this
  machine through SSH. It's supposed to be a workstation, not a server.</li>
<li>The X Window System should not be started by <code>xnodm(1)</code>, so I leave it to
  <code>no</code>.</li>
<li>Neither do I want to change the default to <code>com0</code>.</li>
<li>I set up my user <code>patrick</code> with my proper name <code>Patrick Bucher</code>, and a decent
  password.</li>
<li>The time zone has been detected properly as <code>Europe/Zurich</code>, which I just
  leave the way it is.</li>
<li>The installer detected two disks: <code>sd0</code> and <code>sd1</code>. Since <code>sd0</code> is the detected
  SSD in my laptop, the UEFI issue from my Dell laptop doesn't exist on this
  computer. I pick <code>sd0</code> for the root disk, since <code>sd1</code> is my USB dongle.</li>
<li>I choose to use the whole disk with a GPT partitioning schema, because it's
  2020.</li>
<li>An auto-allocated layout for <code>sd0</code> is presented. It looks decent to me, so I
  just go with that auto layout.</li>
<li>I don't want to initialize another disk, so I just press Enter (<code>done</code>).</li>
<li>Since the miniroot image does not come with the file sets, I pick <code>http</code> as
  the location for the sets.</li>
<li>I don't use a proxy, and use the mirror <code>mirrog.ungleich.ch</code> and the server
  directory <code>pub/OpenBSD/6.7/amd64</code> as proposed.</li>
<li>Next, I unselect the game sets by entering <code>-game*</code>. (I heard that they're not
  much fun to play.) I leave all the other sets activated, including the <code>x</code>
  sets, which will be required for the GUI later on.</li>
<li>After those sets are installed, I press Enter (<code>done</code>). Now the installer
  performs various tasks, after which I choose to <code>halt</code> the computer. This
  gives me time to remove the USB dongle.</li>
</ul>

<p>I now restart my laptop, and OpenBSD boots. This takes more time than booting
Arch Linux, which uses <code>systemd</code>, whereas OpenBSD uses <code>rc</code>, which performs the
startup tasks sequentially.</p>
<p>There's a message showing up that various firmware (<code>intel-firmware</code>,
<code>iwm-firmware</code>, <code>inteldrm-firmware</code>, <code>uvideo-firmware</code>, and <code>vmm-firmware</code>) has
been installed automatically. Very nice, indeed.</p>
<h2>WiFi Connection</h2>
<p>Now that the <code>iwm-firmware</code> has been installed, I can connect right away to my
WiFi network <code>frzbxpdb5</code>. I create a file called <code>/etc/hostname.iwm0</code>, wich
<code>hostname</code> being a literal string, and <code>iwm0</code> being the WiFi network card. The
connection to my WiFi network consists of a single line:</p>
<pre><code>dhcp nwid frzbxpdb5 wpakey [my-wpakey]
</code></pre>
<p>Whereas <code>frzbxpdb5</code> is my WiFi network's ESSID, and <code>[my-wpakey]</code> needs to be
replaced by the actual WPA key.</p>
<p>Then the networking can be restarted for that device:</p>
<pre><code># sh /etc/netstart iwm0
</code></pre>
<p>This script is kind enough to set the file permissions of <code>/etc/hostname.iwm0</code>
to <code>640</code>, and then connects to my WiFi network.</p>
<p>I unplug the Ethernet cable and <code>ping openbsd.org</code>, which works fine, even after
a restart.</p>

<p>My GUI on Unix-like systems is based on the Dynamic Window Manager (<code>dwm</code>) and a
couple of other tools, such as <code>dmenu</code>, <code>st</code>, <code>slstatus</code>, <code>slock</code>, all created and
maintained by the <a href="http://suckless.org/">Suckless</a> community.</p>
<p>This software doesn't come with configuration facilities, but needs to be
configured in the respective C header file <code>config.h</code>, and then re-compiled.
Even though OpenBSD offers <code>dwm</code> as a package, customizing and configuring that
window manager requires to build it from source.</p>
<h2>Building <code>dwm</code> and Friends</h2>
<p>First, I need to install <code>git</code> to fetch the source code:</p>
<pre><code># pkg_add git
</code></pre>
<p>Then I fetch the source code for <code>dwm</code>, <code>dmenu</code>, <code>st</code>, and <code>slstatus</code> from <a href="http://suckless.org/">Suckless</a>:</p>
<pre><code>$ git clone https://git.suckless.org/dwm
$ git clone https://git.suckless.org/dmenu
$ git clone https://git.suckless.org/st
$ git clone https://git.suckless.org/slstatus
</code></pre>
<h3>Building <code>dwm</code></h3>
<p>Next, I try to build <code>dwm</code>:</p>
<pre><code>$ cd dwm
$ make
</code></pre>
<p>This fails with an error message (<code>'ft2build.h' file not found</code>), which reminds
me of building <code>dwm</code> on FreeBSD roughly a month before. Since I can finde the
header file at another location:</p>
<pre><code># find / -type f -name ft2build.h
/usr/X11R6/include/freetype2/ft2build.h
</code></pre>
<p>I simply can modify the <code>config.mk</code> accordingly by changing</p>
<pre><code>FREETYPEINC = /usr/include/freetype2
</code></pre>
<p>to</p>
<pre><code>FREETYPEINC = $(X11INC}/freetype2
</code></pre>
<p>Actually, I only need to comment the above line, and uncomment the line below</p>
<pre><code># OpenBSD (uncomment)
</code></pre>
<p>The Suckless folks obviously are friendly towards OpenBSD, which is also
noticable in other places (more evidence to be shown further below).</p>
<p>The next compilation attempt succeeds:</p>
<pre><code>$ make
</code></pre>
<p>So let's install <code>dwm</code>, too:</p>
<pre><code># make install
</code></pre>
<p>By default, and as to be seen in <code>config.h</code>, the keyboard combination
<code>[Alt]+[Shift]+[Enter]</code> (deeply engraved into the muscle memories of many <code>dwm</code>
users) starts the <code>st</code> terminal. This will be built in a while. However, I
prefer to use the <em>Super</em> or <em>Windows</em> key instead of <code>Alt</code>, since the former
is of no use in OpenBSD, and the latter still comes in handy when working with
the emacs readline mode. Therefore, I change the <code>MODKEY</code> from</p>
<pre><code>#define MODKEY Mod1Mask
</code></pre>
<p>to</p>
<pre><code>#define MODKEY Mod4Mask
</code></pre>
<p>Then I rebuild and reinstall <code>dwm</code>:</p>
<pre><code># make install
</code></pre>
<h3>Building <code>st</code></h3>
<p>Let's switch over to the <code>st</code> source directory and just try to compile it:</p>
<pre><code>$ cd ../st
$ make
</code></pre>
<p>Here, we get a warning that the function <code>pledge</code> (an OpenBSD mitigation, which
is built into the <code>master</code> branch, but surrounded by an <code>ifdef</code> preprocessor
statement, so that it will only be compiled for OpenBSD) is imported implicitly.
Let's just ignore this warning for now.</p>
<p>What's worse, the compilation fails with the error message:</p>
<pre><code>ld: error: unable to find library -lrt
</code></pre>
<p>Here, the FAQ comes in handy, stating that</p>
<pre><code>If you want to compile st for OpenBSD you have to remove -lrt from
config.mk, ...
</code></pre>
<p>Having done so in <code>config.mk</code>, <code>st</code> compiles without any further issues, and,
thus, can be rebuilt and installed:</p>
<pre><code># make install
</code></pre>
<h3>Building <code>dmenu</code></h3>
<p>Even OpenBSD users with Suckless tools have to open another GUI application than
a terminal emulator once in a while. For this purpose, Suckless offers <code>dmenu</code>.
Let's switch over to it and compile it:</p>
<pre><code>$ cd ../dmenu
$ make
</code></pre>
<p>Again, we have the issue with <code>ft2build.h</code>, which can be resolved as above with
<code>dwm</code>: by using the proper path for <code>FREETYPEINC</code> in <code>config.mk</code>. Afterwards,
the build succeeds, and <code>dmenu</code> can be installed:</p>
<pre><code># make install
</code></pre>
<h3>Building <code>slstatus</code></h3>
<p><code>dwm</code> has a status bar on the top right, which can be used to show various
information. I used to write some shell commands in <code>.xinitrc</code> to compose such a
status line, and then set it by <code>xset -b</code> once every five seconds or so. This
approach generates a multitude of processes every couple of seconds.</p>
<p><code>slstatus</code> is a C programm that is capable of showing various kinds of more or
less useful information. Let's switch over to <code>slstatus</code> and see, what is
available in <code>config.def.h</code>:</p>
<pre><code>$ cd ../slstatus
$ less config.def.h
</code></pre>
<p>The comments section lists different functions (<code>battery_perc</code> for the battery
percentage, <code>datetime</code> for date and time information, <code>temp</code> for thermal
information, etc.). I usually display the CPU load, the battery percentage, the
memory usage, the current keyboard layout, and the current date and time.</p>
<p>Before configuring those, let's try to compile <code>slstatus</code>:</p>
<pre><code>$ make
</code></pre>
<p>This worked fine, so let's configure the information to be displayed in
<code>config.h</code>:</p>
<pre><code>static const struct arg args[] = {
    /* function    format    argument */
    { datetime,    "%s",     "%F %T" },
};
</code></pre>
<p>This renders the current date as follows:</p>
<pre><code>$ date +"%F %T"
2020-09-05 19:26:38
</code></pre>
<p>I also like to have the weekday included, but not the seconds, so I define a
different argument string:</p>
<pre><code>$ date +"%a %Y-%m-%d %H:%M"
Sat 2020-09-05 19:27
</code></pre>
<p>That's better, so let's use it in <code>config.h</code> (surrounded with some spaces in the
format string):</p>
<pre><code>static const struct arg args[] = {
    /* function    format    argument */
    { datetime,    " %s ",   "%a %Y-%m-%d %H:%M" },
};
</code></pre>
<p>The other settings I like to have do not require any arguments, at least not on
OpenBSD, so I only need to define a decent format string (with <code>|</code> as a
seperator) for those:</p>
<pre><code>static const struct arg args[] = {
    /* function    format           argument */
    { cpu_perc,     " cpu: %s%% |", NULL },
    { battery_perc, " bat: %s%% |", NULL },
    { ram_used,     " mem: %s |",   NULL },
    { keymap,       " %s |"         NULL },
    { datetime,     " %s ",         "%a %Y-%m-%d %H:%M" },
};
</code></pre>
<p>This actually compiles, so let's install it:</p>
<pre><code># make install
</code></pre>
<h2>Configuring X Startup</h2>
<p>Now that all software is compiled and installed, let's run X. To do so, a file
<code>.xinitrc</code> in the user's directory is …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://paedubucher.ch/articles/2020-09-05-openbsd-on-the-desktop-part-i.html">https://paedubucher.ch/articles/2020-09-05-openbsd-on-the-desktop-part-i.html</a></em></p>]]>
            </description>
            <link>https://paedubucher.ch/articles/2020-09-05-openbsd-on-the-desktop-part-i.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24622788</guid>
            <pubDate>Tue, 29 Sep 2020 00:23:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Decorating a home office when you’re a gigantic nerd]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24621390">thread link</a>) | @animationwill
<br/>
September 28, 2020 | https://www.intergalacticprimate.com/decorating-a-home-office-when-youre-a-gigantic-nerd/ | <a href="https://web.archive.org/web/*/https://www.intergalacticprimate.com/decorating-a-home-office-when-youre-a-gigantic-nerd/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://www.intergalacticprimate.com/decorating-a-home-office-when-youre-a-gigantic-nerd/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24621390</guid>
            <pubDate>Mon, 28 Sep 2020 21:25:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Hanging Gardens of Babylon]]>
            </title>
            <description>
<![CDATA[
Score 28 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24621384">thread link</a>) | @quickfox
<br/>
September 28, 2020 | https://analog-antiquarian.net/2020/09/25/chapter-1-the-ancients-bucket-list/ | <a href="https://web.archive.org/web/*/https://analog-antiquarian.net/2020/09/25/chapter-1-the-ancients-bucket-list/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://analog-antiquarian.net/2020/09/25/chapter-1-the-ancients-bucket-list/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24621384</guid>
            <pubDate>Mon, 28 Sep 2020 21:25:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Nix × IPFS – Milestone 1]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24621276">thread link</a>) | @Fnoord
<br/>
September 28, 2020 | https://blog.ipfs.io/2020-09-08-nix-ipfs-milestone-1/ | <a href="https://web.archive.org/web/*/https://blog.ipfs.io/2020-09-08-nix-ipfs-milestone-1/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

    


    <div>
      
      <p>by John Ericson on 2020-09-08</p>

      

      

<p><a href="https://obsidian.systems/">Obsidian Systems</a> is adding support for IPFS to Nix so that build products can be persisted to and fetched from IPFS. This adds resiliency and makes it easier for Nix users to reproduce and distribute their work - by caching and distributing source code (and hopefully in the future intermediate build steps) peer to peer by using IPFS content addresses (CIDs).</p>

<h2 id="what-is-nix">What is Nix?</h2>

<p><a href="https://nixos.org/">Nix</a> is usually used as a package manager, but at its heart is a general-purpose build tool, like Make, Ninja, or Bazel.</p>

<p>What distinguishes Nix is its focus on sandboxing build steps and caching build artifacts.
With these features, neither the plans nor the build artifacts can have hidden dependencies, so builds can be reproduced and build artifacts shared robustly.</p>

<p>This makes Nix an ideal build tool to use with a peer-to-peer system like IPFS.
Indeed, the premier project using Nix is Nixpkgs, a package collection (with associated Linux distro) that is one of the largest most widely-contributed projects on GitHub.</p>

<h2 id="why-we-use-nix">Why we use Nix</h2>

<p>Obsidian Systems is an end-to-end software product consultancy serving everyone from recently funded startups to large institutions.
We have made Nix an integral part of both our production deployments and developer workflows since our founding 2014.</p>

<p>Nix is an indispensable tool for us because we frequently need to switch between projects, and Nix makes setting up and sharing per-project development environments trivial.
It has also made it easy to package software that the end user installs on their own machines, such as blockchain wallets.</p>

<h2 id="challenges-that-inspired-our-use-of-ipfs">Challenges that inspired our use of IPFS</h2>

<p>While Nix build plans are reproducible, one limitation that remains is the availability of the initial data—source code.
Nix plans have what are known as “fixed output derivations”.
These are unsandboxed build steps, with network access to download various sources.
They produce data that must match a pre-fixed hash, so the lack of sandboxing cannot be exploited to result in nondeterministic output.</p>

<p>The big problem with this is that if the URL becomes inaccessible or the data downloaded is non-deterministic (e.g. due to some metadata), this build step will fail —
in other words, we are facing the exact same problems around linkrot IPFS is trying to solve with the web in general!
The IPFS solution is the right one —
we shouldn’t be relying on the <em>location</em> at which some source code was originally uploaded.
And we’re already identifying source code by content addresses, so IPFS solution isn’t even a huge leap from our existing tooling and community practices.</p>

<p>Yes, we can already pin and cache the results of those fixed-output build steps just as we do with regular build steps, but we’re stuck with our own cache of source code completely independent from what upstream offers.
It is tedious and inefficient for users of Nix to each maintain their own uncooperative caches of source code, and even more so for the downstream user who would have to manually configure each of the caches individually, when all they want is some source code that is self-authenticating due to the content-address.</p>

<h2 id="value">Value</h2>

<p>For the Nix community at large, we finally have a chance to leverage our hard work on reproducibility and make it a practical reality.
Rather than relying on our centralized <a href="https://cache.nixos.org/">cache.nixos.org</a> to build artifacts before sources rot away, everyone should feel free to use Nixpkgs as a source or binary distro—as was originally intended.</p>

<p>For users of Obsidian’s open-source software specifically, they finally have an easy and robust way to trust neither our own pre-built binaries or <strong>cache.nixos.org</strong>’s, but build everything from source, which makes auditing security-critical code easier.</p>

<p>Ideally, we want to cache and distribute source code in collaboration with the upstream developers themselves and other downstream distributions.
<a href="https://ipld.io/">IPLD</a>, more than any other schema we’ve seen, understands the value of addressing data with its original intended “native” references, rather than some bespoke 3rd-party format that others cannot understand.</p>

<p>We think this is the key to enable that cooperation.
Upstream devs can simply continue working with git repos (or any other version control system with content-addressing that IPLD supports).
Downstream distros consume that data directly, without any conversion steps that obscure the data’s authenticity.
Neither party is faced with doing chores that the other used to handle.</p>

<h2 id="scope">Scope</h2>

<p>We aim to address these problems in two distinct phases.</p>

<h3 id="milestone-1-distribution-with-ipfs">Milestone 1: Distribution with IPFS</h3>

<p>We wanted Nix to be able to use IPFS as a “substituter” or provider of source/build artifacts alongside the other sorts of substituters that exist today.</p>

<p>As part of this, we taught Nix git tree hashing, so it can content-address git repos in a way IPFS will understand —
which helps IPFS, Nix, upstream collaborators, and other parties with an interest in archiving and disseminating source code find a common way to reference these artifacts.
While the git hashing scheme has its limitations, we think it is the best method for multi-party collaboration on git data.</p>

<p>Looking ahead to using IPFS for build products and deployments, we also added support for a metadata format around git tree hashing for IPFS and Nix to also convey data with run-time dependencies between separately-installed file system trees.
Finally, we provide a way for existing Nix build artifacts to be converted to this new data format.</p>

<h3 id="milestone-2-building-with-ipfs">Milestone 2: Building with IPFS</h3>

<p>Nix doesn’t actually content-address data produced by regular build steps (as opposed to the “fixed output” build steps described above).
Instead, it addresses them based on the plan from which they were made.</p>

<p>Situations exist — such as when someone edits a comment — where the plan changes but the results don’t.
Besides causing extra rebuilding downstream, this muddles the separation between the raw data and its provenance.
With peer-to-peer systems, it doesn’t matter who is <em>providing</em> the data (and we want to take advantage of that not mattering), but it absolutely does matter who is <em>claiming</em> what the data represents.</p>

<p>With this core improvement, we can make new improved versions of build plans in IPLD and produce our newly supported IPFS-compatible formats directly from each build step, no manual conversions from legacy input-addressed data needed. This final step brings everything from both milestones together.</p>

<p>For a complete breakdown, visit our <a href="https://github.com/ipfs/devgrants/blob/master/open-grants/open-proposal-nix-ipfs.md">open grant proposal</a>.</p>

<h2 id="what-was-accomplished">What was accomplished</h2>

<p>We’re happy to announce completion of Milestone 1! In response to community feedback, we were also able to do some extra work to get the Nix community started on using IPFS before migrating to the ideal git tree hashing.</p>

<p>This neatly lays the groundwork for some of our Milestone 2 objectives.
We hope this step can help everyone transition to using IPFS more gracefully.</p>

<p>Get started using our <a href="https://github.com/obsidiansystems/ipfs-nix-guide/">guide repo</a> and, in particular, our <a href="https://github.com/obsidiansystems/ipfs-nix-guide/blob/master/tutorial.md">tutorial</a>.</p>

<p>Finally, we recently did an interview on the <a href="https://zimbatm.com/NixFriday/">Nix Friday</a> stream going over all our work, and also discussing more broadly how we see the IPFS and Nix ecosystems fitting together.
You can watch a recording <a href="https://www.youtube.com/watch?v=FievtzvDbs82">here</a>:</p>


<p>
  <iframe src="//www.youtube.com/embed/FievtzvDbs8" allowfullscreen="" title="YouTube Video"></iframe>
</p>


<h2 id="what-s-next">What’s Next?</h2>

<p>We’ve begun implementing Milestone 2, including the improved build steps that produce content-addressed data.
We expect this to be the bulk of the work, with the final IPFS integration being relatively smooth, as by that point the concepts of Nix and IPFS will align so neatly!</p>

<p>We’ve been fastidious about juggling many branches to separate feature work from general improvements of the internals, and thus have been able to upstream many of those improvements.</p>

<p>We like this approach because it allows us to continuously engage with the community, and leaves much more readable diffs for the features themselves.</p>

<p>We hope you can give the demo a spin and like what you see.
Stay tuned for milestone 2!</p>


      
          
          
          
      
    </div>
  </div></div>]]>
            </description>
            <link>https://blog.ipfs.io/2020-09-08-nix-ipfs-milestone-1/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24621276</guid>
            <pubDate>Mon, 28 Sep 2020 21:13:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The SaaS website content you need to close sales]]>
            </title>
            <description>
<![CDATA[
Score 17 | Comments 3 (<a href="https://news.ycombinator.com/item?id=24621132">thread link</a>) | @franciscassel
<br/>
September 28, 2020 | https://www.mikesonders.com/saas-website-content/ | <a href="https://web.archive.org/web/*/https://www.mikesonders.com/saas-website-content/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="text">
			
<p>The beauty of using keyword research to do market research is that we can literally uncover and quantify <em>exactly</em>&nbsp;what information a specific audience is looking for.</p>



<p>In this case, <strong>I’ve analyzed the online searches of thousands of B2B SaaS buyers to uncover exactly&nbsp;what information they want when considering a SaaS purchase.</strong></p>



<p>Based on the results, I can confidently say your SaaS website probably isn’t delivering everything SaaS buyers need. </p>



<p>I’m going to share the results of my analysis, and then show you how to translate the results into SaaS website content that answers prospect questions, addresses their objections, and gives them the confidence to purchase.</p>



<p>So if you’re ready to pave the way for more leads and revenue, let’s get started.</p>



<h3><strong>Contents</strong></h3>



<ul><li><a href="#quick">A quick note on methodology</a></li><li><a href="#results">The results: What SaaS buyers are searching for</a></li><li><a href="#best">Best practices: Creating the website content SaaS buyers want</a></li><li><a href="#method">Methodology &amp; raw data</a></li></ul>



<p>The recommendations in this post assume you’ve found product-market fit. Otherwise, building out your website content perhaps shouldn’t be at the top of your list of concerns.</p>



<h2 id="quick">A quick note on methodology</h2>



<p>If you’d like to see my full research methodology, you can <a href="#method">jump to the Methodology section</a> at the end of the post.</p>



<p>In short: I selected ten well-known B2B SaaS brands from among the <a href="https://www.mikesonders.com/largest-saas-companies/">largest SaaS companies</a> in the world. I then identified the ~16K branded searches (e.g., “hubspot pricing”, “servicenow login”, “zendesk support”, etc.) that people enter into Google to search for information on these brands.</p>



<div><figure><img src="https://www.mikesonders.com/wp-content/uploads/2020/09/image1-1024x248.png" alt="Branded keyword modifiers" srcset="https://www.mikesonders.com/wp-content/uploads/2020/09/image1.png 1024w, https://www.mikesonders.com/wp-content/uploads/2020/09/image1-300x73.png 300w, https://www.mikesonders.com/wp-content/uploads/2020/09/image1-768x186.png 768w" sizes="(max-width: 1024px) 100vw, 1024px"></figure></div>



<p>Among those ~16K branded search terms, I analyzed their modifiers to find the ones common to most of the ten SaaS brands. Of those common modifiers, I identified the ones that searchers use when considering a potential SaaS purchase.</p>



<p>E.g., “hubspot pricing” is a search that a potential buyer would use when considering a purchase; “hubspot investor relations” and “hubspot login” are not.</p>



<p>That is, <strong>I uncovered the search terms SaaS buyers use most commonly in the “consideration” phase of the buyer journey</strong>&nbsp;— when they’re gathering the information they need to decide (1) whether to make a purchase (i.e., convert) and (2) which vendor will get their money.</p>



<div><figure><img src="https://www.mikesonders.com/wp-content/uploads/2020/09/image11.png" alt="SaaS conversion funnel" width="550" srcset="https://www.mikesonders.com/wp-content/uploads/2020/09/image11.png 788w, https://www.mikesonders.com/wp-content/uploads/2020/09/image11-300x236.png 300w, https://www.mikesonders.com/wp-content/uploads/2020/09/image11-768x604.png 768w" sizes="(max-width: 788px) 100vw, 788px"></figure></div>



<p>Using median search volumes, I indexed the relative demand for the information implied by each “consideration” modifier.</p>



<div><figure><img src="https://www.mikesonders.com/wp-content/uploads/2020/09/image6.png" alt="Branded search terms for SaaS reviews" width="550" srcset="https://www.mikesonders.com/wp-content/uploads/2020/09/image6.png 804w, https://www.mikesonders.com/wp-content/uploads/2020/09/image6-300x193.png 300w, https://www.mikesonders.com/wp-content/uploads/2020/09/image6-768x495.png 768w" sizes="(max-width: 804px) 100vw, 804px"><figcaption><em>The median search volume for “reviews” keywords is 225.</em></figcaption></figure></div>



<h2 id="results">The results: What SaaS buyers are searching for </h2>



<h3>Pre-sale</h3>



<p>Search volumes for SaaS pricing information are so high–more than nine times higher than searches for “alternatives”–that including&nbsp;pricing data would have blown out the scale of a consolidated chart.</p>



<p>So, I show here the results across two charts, indexing to the demand for “alternatives” information in both:</p>



<figure><img src="https://www.mikesonders.com/wp-content/uploads/2020/09/saas-pricing-info.png" alt="" srcset="https://www.mikesonders.com/wp-content/uploads/2020/09/saas-pricing-info.png 994w, https://www.mikesonders.com/wp-content/uploads/2020/09/saas-pricing-info-300x93.png 300w, https://www.mikesonders.com/wp-content/uploads/2020/09/saas-pricing-info-768x238.png 768w" sizes="(max-width: 994px) 100vw, 994px"></figure>



<div><figure><img src="https://www.mikesonders.com/wp-content/uploads/2020/09/saas-consideration-info-1.png" alt="" srcset="https://www.mikesonders.com/wp-content/uploads/2020/09/saas-consideration-info-1.png 1020w, https://www.mikesonders.com/wp-content/uploads/2020/09/saas-consideration-info-1-300x296.png 300w, https://www.mikesonders.com/wp-content/uploads/2020/09/saas-consideration-info-1-768x757.png 768w" sizes="(max-width: 1020px) 100vw, 1020px"></figure></div>



<p>It’s no coincidence that this chart looks like a bisected funnel.</p>



<p>Generally speaking, most buyers start their journey at the top of this list (with pricing) and proceed downward. As buyers fail to find satisfactory information that matches up with their needs, they fall out of the funnel.</p>



<p>For example:</p>



<ul><li>If the price is out of budget range, they’re going to be much less interested in getting a demo.</li><li>If the demo goes poorly (or simply shows that the solution doesn’t their your requirements), there’s not much reason to explore integrations.</li><li>If the solution doesn’t offer the specific integrations they need, getting a free trial is likely moot.</li></ul>



<p>The path of the B2B buyer journey certainly varies. Still, this chart paints a clear picture regarding the relative importance of different features, resources, and information SaaS buyers are seeking.</p>



<p>Correspondingly, <strong>you can use these data to prioritize the content (and features!) you create to keep buyers in your funnel and convince them that you’re the best solution</strong>.</p>



<p>Please note: it’s not that lower relative demand for certain information (e.g. “free trial) means that the information isn’t important. (Offering a free trial, if possible, is very important!) To some degree, it simply means there are fewer prospects left in the funnel at that stage seeking that information.</p>











<h3 id="post">Pre- and post-sale</h3>



<p>There’s a set of your resources–like your API documentation or support site–that your existing customers will Google instead of trying to find on your site.</p>



<p>So, it’s not <em>just</em>&nbsp;potential buyers creating the search demand for the following terms.</p>



<p>But I know from <a href="https://www.mikesonders.com/about/">my experience</a>&nbsp;that SaaS buyers oftentimes want to make sure these resources are available before they’ll pull the trigger on any purchase. &nbsp;&nbsp;</p>



<p>And (robustly) <strong>providing these resources is not only an excellent way to close more sales — it sets your customers up to succeed with your product, thereby improving your retention rates</strong>.</p>



<figure><img src="https://www.mikesonders.com/wp-content/uploads/2020/09/saas-content-pre-post.png" alt="" srcset="https://www.mikesonders.com/wp-content/uploads/2020/09/saas-content-pre-post.png 876w, https://www.mikesonders.com/wp-content/uploads/2020/09/saas-content-pre-post-300x292.png 300w, https://www.mikesonders.com/wp-content/uploads/2020/09/saas-content-pre-post-768x747.png 768w" sizes="(max-width: 876px) 100vw, 876px"></figure>



<p>In the next sections, I’ll show you best practices for putting these data into action as compelling content on your website.</p>



<h2 id="best">Best practices: Creating the website content SaaS buyers want</h2>



<h3 id="h.eaif2yj0bxlo">Pricing</h3>



<div><figure><img src="https://www.mikesonders.com/wp-content/uploads/2020/09/image17-1024x647.png" alt="Help Scout pricing page" srcset="https://www.mikesonders.com/wp-content/uploads/2020/09/image17.png 1024w, https://www.mikesonders.com/wp-content/uploads/2020/09/image17-300x190.png 300w, https://www.mikesonders.com/wp-content/uploads/2020/09/image17-768x486.png 768w, https://www.mikesonders.com/wp-content/uploads/2020/09/image17-1536x971.png 1536w" sizes="(max-width: 1024px) 100vw, 1024px"></figure></div>



<p>It makes sense that there’s a steep drop-off in demand for pricing information versus any other purchase-consideration information.</p>



<p>Pricing information is among the easiest to find on a SaaS website, and it’s a very straightforward litmus test that kicks off (or puts an abrupt stop to) the buying process:</p>



<p><em>Is the price low enough for me to consider, or is it so expensive that I shouldn’t even bother continuing to look into this solution?</em></p>



<p>Pricing can also put off potential buyers when it’s too confusing or when it’s simply missing from the website.</p>



<h4 id="h.fv29gmntzagy">Confusing pricing</h4>



<p>Do your pricing plans <a href="https://twitter.com/mikesonders/status/1285643715554549760" target="_blank" rel="noopener noreferrer">compare apples to apples</a>? Your mileage will vary, but I’ve helped clients <a href="https://twitter.com/mikesonders/status/1257751045729595394" target="_blank" rel="noopener noreferrer">double conversion rates</a>&nbsp;by simply clarifying their pricing pages. (Those links point to threads of mine on Twitter where I provide more details.)</p>



<h4 id="h.tgobcgqit9vl">Missing pricing</h4>



<p>You might (or might not) have good reasons for not providing pricing information on your website.</p>



<p>To avoid unnecessarily driving away prospective customers, <a href="https://businesscasualcopywriting.com/show-pricing-on-website/" target="_blank" rel="noopener noreferrer">consider these alternative approaches</a>&nbsp;formulated by <a href="https://twitter.com/JoelKlettke" target="_blank" rel="noopener noreferrer">Joel Klettke</a>.</p>



<h3 id="h.xvesd4pfueqe">Alternatives</h3>



<p>It makes logical sense that someone considering a meaningful purchase would want to uncover and evaluate all the appropriate alternatives.</p>



<p>That’s why a lot of people in the beginning of the SaaS buyer journey Google things like “[brand] alternative”, “[brand] competitors”, and “alternatives to [brand]”.</p>



<p>In the SaaS world, the search results for these queries tend to be dominated by listicles from the software comparison sites like Capterra, G2, and TrustRadius.</p>



<p>So your most important step is to <strong>make sure your app has a profile on the comparison sites that appear when someone searches on “[your_brand] alternatives”</strong>. And, of course, populate those profiles with positive reviews from customers — which I’ll discuss in a section below.</p>



<p>You should also consider or experiment with paying for better surfacing of your app on those comparison sites. It’s not uncommon for paid campaigns like these to be a top source of leads for SaaS startups.</p>



<p><strong>You can (and should) rank a page from your own site for these “alternatives” queries</strong>, <strong>whether someone is searching for alternatives to your brand, or to one of your competitors.</strong></p>



<p>I recommend following both of these methods:</p>



<h4 id="h.x05igpe9c7lt">The Best [Competitor] Alternative</h4>



<div><figure><img src="https://www.mikesonders.com/wp-content/uploads/2020/09/image22-1024x512.png" alt="Plivo best alternative page" srcset="https://www.mikesonders.com/wp-content/uploads/2020/09/image22.png 1024w, https://www.mikesonders.com/wp-content/uploads/2020/09/image22-300x150.png 300w, https://www.mikesonders.com/wp-content/uploads/2020/09/image22-768x384.png 768w, https://www.mikesonders.com/wp-content/uploads/2020/09/image22-1536x768.png 1536w" sizes="(max-width: 1024px) 100vw, 1024px"></figure></div>



<p>Create a page on your marketing site or blog that positions your solution as the “Best [Competitor] Alternative”, where [Competitor] is a well-known brand in your vertical against whom you can position yourself effectively. &nbsp;</p>



<p>Start with the most well-known of your competitors, as more people will be searching for alternatives for them.</p>



<p>Here are a few examples of pages that follow this method and rank among the top search results for their respective “[competitor] alternatives” searches:</p>



<ul><li><a href="https://www.plivo.com/twilio-alternative/" target="_blank" rel="noopener noreferrer">Twilio Alternative | The best alternative to Twilio API | Plivo</a></li><li><a href="https://www.salesmate.io/pipedrive-crm-alternative/" target="_blank" rel="noopener noreferrer">One Of The Best Pipedrive Alternatives | Salesmate CRM</a></li><li><a href="https://supportbee.com/zendesk-alternative" target="_blank" rel="noopener noreferrer">A simpler, and smarter Zendesk alternative – SupportBee</a></li></ul>



<h4 id="h.z4bgyoba3kj">[Your_brand] Alternatives</h4>



<div><figure><img src="https://www.mikesonders.com/wp-content/uploads/2020/09/image26-1024x742.png" alt="Jira alternatives page" srcset="https://www.mikesonders.com/wp-content/uploads/2020/09/image26.png 1024w, https://www.mikesonders.com/wp-content/uploads/2020/09/image26-300x218.png 300w, https://www.mikesonders.com/wp-content/uploads/2020/09/image26-768x557.png 768w, https://www.mikesonders.com/wp-content/uploads/2020/09/image26-1536x1114.png 1536w" sizes="(max-width: 1024px) 100vw, 1024px"></figure></div>



<p>Yes, <a href="https://www.atlassian.com/software/jira/comparison" target="_blank" rel="noopener noreferrer">like JIRA</a>, you can have a page on your site that lists your competitors.</p>



<p>Why?</p>



<p>One, prospective buyers are going to discover your competitors, anyway.</p>



<p>Two, when someone is searching for alternatives to your solution, appearing in the search results at least gives you a chance&nbsp;to position yourself relative to your competition. To present your relative strengths and clarify your best-fit customer.</p>



<p>Otherwise, you’re letting your competitors or a 3rd-party site like Capterra define your positioning for you.</p>



<div><figure><img src="https://www.mikesonders.com/wp-content/uploads/2020/09/image4-1024x433.png" alt="JIRA alternatives search results" srcset="https://www.mikesonders.com/wp-content/uploads/2020/09/image4.png 1024w, https://www.mikesonders.com/wp-content/uploads/2020/09/image4-300x127.png 300w, https://www.mikesonders.com/wp-content/uploads/2020/09/image4-768x325.png 768w, https://www.mikesonders.com/wp-content/uploads/2020/09/image4-1536x650.png 1536w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><em>Atlassian has the fourth organic result when people search for alternatives to their product, JIRA.</em></figcaption></figure></div>



<p>Surprisingly, not many of the big SaaS brands–other than JIRA and <a href="https://www.salesforce.com/in/hub/crm/viable-salesforce-alternatives/" target="_blank" rel="noopener noreferrer">Salesforce</a>–have implemented this defensive strategy. &nbsp;</p>



<h3 id="h.r797chgho5xp">Demo</h3>



<p>Prospects want to see your solution in action. There’s a reason “Schedule a Demo” and similar are such common calls to action (CTAs) on SaaS websites–especially for sales-driven (vs. self-service) products.</p>



<div><figure><img src="https://www.mikesonders.com/wp-content/uploads/2020/09/image5-1024x609.png" alt="Churn Buster request a demo page" srcset="https://www.mikesonders.com/wp-content/uploads/2020/09/image5.png 1024w, https://www.mikesonders.com/wp-content/uploads/2020/09/image5-300x178.png 300w, https://www.mikesonders.com/wp-content/uploads/2020/09/image5-768x457.png 768w, https://www.mikesonders.com/wp-content/uploads/2020/09/image5-1536x914.png 1536w" sizes="(max-width: 1024px) 100vw, 1024px"></figure></div>



<p>But maybe you’ve got a relatively low-priced solution and it doesn’t make sense to schedule live demos with every prospect.</p>



<p>In that case, consider recording a demo and posting the video to a “demo” page on your website.</p>



<p>And I’d recommend uploading that demo to YouTube and giving the video a straightforward title.</p>



<p>Search results for “[brand] demo” typically feature video packs, and those video packs sometimes appear even before the brand’s own, relevant website content.</p>



<p><strong>Having demo content on your website <em>and</em>&nbsp;on YouTube gives you a chance to own more real estate in search results</strong>, thereby giving you a better chance to own the narrative.</p>



<div><figure><img src="https://www.mikesonders.com/wp-content/uploads/2020/09/image23-1024x832.png" alt="Shartsheet demo in search results" srcset="https://www.mikesonders.com/wp-content/uploads/2020/09/image23.png 1024w, https://www.mikesonders.com/wp-content/uploads/2020/09/image23-300x244.png 300w, https://www.mikesonders.com/wp-content/uploads/2020/09/image23-768x624.png 768w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><em>Smartsheet created demo content on both its website and on YouTube, allowing it to dominate the first search results for “smartsheet demo”.</em></figcaption></figure></div>



<h3 id="h.toefavd2jjr">Reviews</h3>



<p>In competitive industries, search results for “[brand] reviews” are dominated by the big review-aggregation sites like G2, Capterra, and …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.mikesonders.com/saas-website-content/">https://www.mikesonders.com/saas-website-content/</a></em></p>]]>
            </description>
            <link>https://www.mikesonders.com/saas-website-content/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24621132</guid>
            <pubDate>Mon, 28 Sep 2020 20:56:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Software metrics: a guide for modern dev leaders]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24620919">thread link</a>) | @necco908
<br/>
September 28, 2020 | https://linearb.io/blog/17-software-metrics-for-modern-dev-leaders/ | <a href="https://web.archive.org/web/*/https://linearb.io/blog/17-software-metrics-for-modern-dev-leaders/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-id="670d84f7" data-element_type="widget" data-widget_type="theme-post-content.default">
				<div>
			
<figure><picture>
<source type="image/webp" srcset="https://linearb.io/wp-content/uploads/2020/06/17-Metrics-Blog-Cover.png.webp 1200w" sizes="(max-width: 1200px) 100vw, 1200px">
<img src="https://linearb.io/wp-content/uploads/2020/06/17-Metrics-Blog-Cover.png" alt="Software metrics for modern dev leaders" srcset="https://linearb.io/wp-content/uploads/2020/06/17-Metrics-Blog-Cover.png 1200w, https://linearb.io/wp-content/uploads/2020/06/17-Metrics-Blog-Cover-300x157.png 300w, https://linearb.io/wp-content/uploads/2020/06/17-Metrics-Blog-Cover-1024x536.png 1024w, https://linearb.io/wp-content/uploads/2020/06/17-Metrics-Blog-Cover-768x402.png 768w" sizes="(max-width: 1200px) 100vw, 1200px">
</picture>
</figure>



<h2><strong>Software metrics that help accelerate delivery, maintain a positive culture, and translate dev activity to business value</strong></h2>



<h3><strong>Motivation to be data-driven</strong></h3>



<p>As dev leaders, we spend most of our time translating between two groups of people in two parallel universes. Most CEOs and board members come from the business side (sales, marketing, finance) and while they enjoy the outcomes of engineering, they don’t fully understand how we work. At the same time, many engineers don’t fully understand the business side of the organization. This is the background of most dev leaders. The right software metrics can help provide a common language between dev leaders and business executives. </p>



<figure><blockquote><p>Save or share “17 Software Metrics for Modern Dev Leaders”. </p><p><a href="https://linearb.io/metrics-modern-dev-leaders-purple/" target="_blank" rel="noreferrer noopener">Click here to download</a></p></blockquote></figure>



<p>Driven to help our teams succeed and bridge the gap between engineering and the business, many of us have turned to data and metrics. But what software metrics truly help us accelerate delivery and correlate to KPIs the business cares about (revenue, leads, retention)?</p>



<p>We would all agree that measuring the right things is important. But some software development teams still use legacy measurements that actually stop them from getting better. We call these metrics anti-KPIs. Some of the most common software development anti-KPIs include <a href="https://linearb.io/blog/why-agile-velocity-is-the-most-dangerous-metric-for-software-development-teams/" target="_blank" rel="noreferrer noopener">velocity (story points completed)</a>, lines of code, code commits, Jira tickets completed.</p>



<div><figure><img src="https://linearb.io/wp-content/uploads/2020/06/Anti-KPI-1024x1024.png" alt="" width="256" height="256" srcset="https://linearb.io/wp-content/uploads/2020/06/Anti-KPI-1024x1024.png 1024w, https://linearb.io/wp-content/uploads/2020/06/Anti-KPI-300x300.png 300w, https://linearb.io/wp-content/uploads/2020/06/Anti-KPI-150x150.png 150w, https://linearb.io/wp-content/uploads/2020/06/Anti-KPI-768x768.png 768w, https://linearb.io/wp-content/uploads/2020/06/Anti-KPI.png 1394w" sizes="(max-width: 256px) 100vw, 256px"></figure></div>



<p>What do these anti-KPIs all have in common? First, they may on the surface indicate that your contributors are busy but they don’t actually help you figure out if you’re providing value to customers.</p>



<p>Second, they encourage the wrong behavior. Is writing more code good? Maybe. Or maybe it is a sign of inefficiency. Do completed Jira tasks show productivity? Most developers would argue that what happens in Jira has little to do with the “real work” of writing code. Third, and worst of all, these anti-KPIs can easily be gamed.&nbsp;</p>



<p>Your contributors that are struggling might be tempted to inflate these anti-KPI numbers (which they can do easily) which actually blocks you from giving them the help they need.&nbsp;</p>



<p>So what should modern dev leaders measure? Keep reading to discover 17 software metrics that matter for dev leaders and how to use them.</p>







<h2><strong>Which Software Metrics to Measure?</strong></h2>



<p>At LinearB we think of operational excellence in software engineering as the pursuit of predictably delivering projects, with high quality, maintaining efficient working hours, with happy contributors and teams, continuously improving. That’s a lot 🙂 But we think it’s possible.</p>



<p>It can seem overwhelming – especially if you’re not sure how you’re doing against all of those dimensions and what steps can be taken to improve. We worked backwards from the goal of delighting our customers and being operationally excellent and came up with 12 key performance indicators. Some you’re probably already measuring. Some may be new to you.</p>



<p>We look at three categories of software metrics:</p>



<ul><li>Delivery Pipeline</li><li>Investment Profile</li><li>Quality</li></ul>



<p>And we measure those categories across 2 dimensions</p>



<ul><li>Iteration</li><li>Team</li></ul>







<figure><blockquote><p>Looking for key metrics about your team, updated in real-time?</p><p><a href="https://linearb.io/get-started-linearb/" target="_blank" rel="noreferrer noopener">Give LinearB a try. Free for 21 days</a>.</p></blockquote></figure>



<h3><strong>Measure Your Software Delivery Pipeline</strong></h3>



<p>Your software delivery pipeline is what enables your team to deliver code to production. It includes all of the phases from “work requested” all the way through release to production and validation. Some of the common phases include development work beginning, pull request open, pull request merge, and release to prod.</p>



<p>Efficient delivery pipelines lead to predictable value delivery, happy developers, happy product owners, and happy customers. Frustrations arise with inefficient pipelines. These situations can happen when code is merged and ready to be released but “the release is not until next week” or when a developer has opened a pull request but it takes days to receive a review or when a story has been sitting in the backlog for weeks.</p>



<p>Measuring the stages of your delivery pipeline allows for bottleneck detection. This provides a high leverage point to increase your delivery performance because it impacts all teams and contributors.</p>



<figure><picture>
<source type="image/webp" srcset="https://linearb.io/wp-content/uploads/2020/05/cycle.png.webp 1800w" sizes="(max-width: 1800px) 100vw, 1800px">
<img src="https://linearb.io/wp-content/uploads/2020/05/cycle.png" alt="Cycle time is a key software metric for modern dev leaders." srcset="https://linearb.io/wp-content/uploads/2020/05/cycle.png 1800w, https://linearb.io/wp-content/uploads/2020/05/cycle-300x210.png 300w, https://linearb.io/wp-content/uploads/2020/05/cycle-1024x717.png 1024w, https://linearb.io/wp-content/uploads/2020/05/cycle-768x538.png 768w, https://linearb.io/wp-content/uploads/2020/05/cycle-1536x1075.png 1536w" sizes="(max-width: 1800px) 100vw, 1800px">
</picture>
</figure>



<h4><strong>ESSENTIAL SOFTWARE METRICS FOR DELIVERY PIPELINE</strong></h4>



<p><strong><a href="https://linearb.io/cycle-time/" target="_blank" rel="noreferrer noopener"><em><span>Cycle Time:</span></em> </a></strong>The amount of time from work started until work finished.</p>



<p>Why it matters: Cycle time is the #1 indicator of your speed to value and efficiency ratio.</p>



<p>Note: Many of you are already measuring (or trying to measure) cycle time. But what do you do when you think your cycle time is too high? The next four software metrics help you diagnose where to spend your time to drive cycle time down.</p>



<p><span><strong><em>Deployment Frequency:</em> </strong></span>The number of releases per day.</p>



<p>Why it matters: This is a strong indicator of how much value your team is capable of getting into the hands of customers. Even if you have an efficient pipeline, if your deployment frequency is low, you may not be delivering enough value.</p>



<p><em><strong><span>Lead time:</span></strong> </em>The amount of time from work requested until release.</p>



<p>Why it matters: Start time + Cycle time = Lead time. Once you understand your cycle time, looking at start time can tell you how long it takes on average to get through your product management process and backlog. If you have high start times it could be an indication you need to hire more contributors or change the way you set expectations with customers and your sales team.</p>



<p><em><strong><span>Time to release:</span></strong></em> The amount of time from Pull Request Merged to Production Release</p>



<p>Why it matters: Your developers may have finished their job but your customers may not be getting the value. If time to release is high, it could mean you need to invest more in continuous deployment (CD) or that you have an opportunity to move to a micro-service architecture.&nbsp;</p>



<p><strong><span><em>Time to merge</em>:</span></strong> The amount of time from first Commit to PR Merged</p>



<p>Why it matters: This is a key indicator of your cycle time. It can show the efficiency of your pipeline and it affects all members of your team. If time to merge is high it could be an indication that you’re lacking automation or your team needs additional coaching or process or an indicator your developers are not getting enough detail from product management.</p>



<p><strong><span><em>Review request waiting time or pickup time</em>:</span></strong> The amount of time it takes from the pull request submitted until review begins.</p>



<p>Why it matters: Efficient teams have a low pickup time. The less time PRs spend waiting for review, the faster they are released. This metric is important for all dev teams but is even more critical for remote dev teams.&nbsp;</p>



<iframe width="560" height="400" src="https://www.youtube.com/embed/NKSqXi2q4f0?rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><p>Long-living PRs increase pickup time and can derail cycle time.



</p>



<h3><strong>Measure Your Investment Profile</strong></h3>



<figure><picture>
<source type="image/webp" srcset="https://linearb.io/wp-content/uploads/2020/06/Group-547.png.webp 1238w" sizes="(max-width: 1238px) 100vw, 1238px">
<img src="https://linearb.io/wp-content/uploads/2020/06/Group-547.png" alt="Software metrics about the investment profile help dev leaders see how teams allocate resources." srcset="https://linearb.io/wp-content/uploads/2020/06/Group-547.png 1238w, https://linearb.io/wp-content/uploads/2020/06/Group-547-300x130.png 300w, https://linearb.io/wp-content/uploads/2020/06/Group-547-1024x445.png 1024w, https://linearb.io/wp-content/uploads/2020/06/Group-547-768x334.png 768w" sizes="(max-width: 1238px) 100vw, 1238px">
</picture>
</figure>



<p>The most valuable asset that your organization possesses is your people’s time. It is a scarce and limited resource. There are many forces pulling at your team’s time. Your CEO wants to deliver new value to customers, your engineers want non functional investment, the support team wants to fix bugs, and your sales team brings customer commitments. Lacking visibility into where your team is actually spending time makes balancing all of these forces very difficult.</p>



<p>Your investment profile is a data-driven representation of the types of work in which your team is spending effort. The work types typically include, but are not limited to, stories, non-functional tasks, and bugs.</p>



<p>Measuring and tracking your investment profile puts you back in control. It allows you to determine if your actual investment areas match your expected investment areas. It also allows you to be in the driver’s seat when interacting with stakeholders like your CEO or Product lead.</p>



<h4>ESSENTIAL SOFTWARE METRICS FOR INVESTMENT PROFILE</h4>



<p><strong><span><em>Story to Bug Ratio:</em> </span></strong>The ratio of completed stories to completed bug.</p>



<p>Why it matters: You probably know how many production bugs you have but do you know the effect it has on your customer-facing work? Your contributors certainly know when they feel like they are spending too much time on bugs. Quantifying the impact can help you decide if you have a bigger issue to investigate.</p>



<p><em><strong><span>Support &amp; Sales Issues:</span></strong> </em>The percentage of work dedicated to one-off requests coming from the support or sales team.</p>



<p>Why it matters: If you build software for large enterprises, developing based on support and sales team requirements may be a great use of resources. If not, if this ratio of work is high, it could be an indication of planning issues.</p>







<h3><strong>Measure Your Work Quality</strong></h3>



<figure><picture>
<source type="image/webp" srcset="https://linearb.io/wp-content/uploads/2020/06/Group-548.png.webp 1200w" sizes="(max-width: 1200px) 100vw, 1200px">
<img src="https://linearb.io/wp-content/uploads/2020/06/Group-548.png" alt="Software metrics like work breakdown can help predict quality issues." srcset="https://linearb.io/wp-content/uploads/2020/06/Group-548.png 1200w, https://linearb.io/wp-content/uploads/2020/06/Group-548-300x258.png 300w, https://linearb.io/wp-content/uploads/2020/06/Group-548-1024x881.png 1024w, https://linearb.io/wp-content/uploads/2020/06/Group-548-768x660.png 768w" sizes="(max-width: 1200px) 100vw, 1200px">
</picture>
</figure>



<p>Most teams have experienced the situation where low quality leads to missed delivery dates, iteration interruptions, long hours, unhappy customers, and a frustrated engineering organization. On the flip side, high quality leads to predictable delivery, efficient work hours, happy customers, and a happy engineering organization.</p>



<p>There are many different metrics that you could measure as an engineering leader. Some of the classics range from test coverage to service uptime. While these metrics are great, we have found that there are a few metrics that really help to measure delivery predictability.</p>



<h4>ESSENTIAL SOFTWARE METRICS FOR QUALITY</h4>



<p><strong><span><em><a href="https://video.drift.com/v/ab9M6hmJei6/" target="_blank" rel="noreferrer noopener">High-risk work:</a></em> </span></strong>Branches with large change and high rework or refactored work.</p>



<p>Why it matters: The general rule on most dev teams is that the larger the change, the higher the risk (i.e. branches with 300 lines of code are riskier than small branches). Branches with a high percentage of rework and refactored work are also riskier. High-risk work is a leading indicator of quality.</p>



<p><span><strong><em>Code Rework</em>: </strong></span>Percentage of recently delivered code your team is already rewriting.</p>



<p>Why it matters: A high rework percentage could mean you have a training issue, you’re rushing the process, your review process is lacking or you have a breakdown in communication with product management.</p>



<p><em><span><strong>Bugs Found in Prod:</strong></span></em> The number of bugs found in production per time period.</p>



<p>Why it matters: …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://linearb.io/blog/17-software-metrics-for-modern-dev-leaders/">https://linearb.io/blog/17-software-metrics-for-modern-dev-leaders/</a></em></p>]]>
            </description>
            <link>https://linearb.io/blog/17-software-metrics-for-modern-dev-leaders/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24620919</guid>
            <pubDate>Mon, 28 Sep 2020 20:30:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Field Guide to Genetic Programming (2008) [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 73 | Comments 33 (<a href="https://news.ycombinator.com/item?id=24620614">thread link</a>) | @optimalsolver
<br/>
September 28, 2020 | http://libros.metabiblioteca.org:8080/bitstream/001/184/4/978-1-4092-0073-4.pdf | <a href="https://web.archive.org/web/*/http://libros.metabiblioteca.org:8080/bitstream/001/184/4/978-1-4092-0073-4.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://libros.metabiblioteca.org:8080/bitstream/001/184/4/978-1-4092-0073-4.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-24620614</guid>
            <pubDate>Mon, 28 Sep 2020 19:53:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Negotiating the developer-to-tester ratio. 3:1 is just the beginning]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24620351">thread link</a>) | @ohjeez
<br/>
September 28, 2020 | https://www.functionize.com/blog/how-many-developers-does-it-take-to-test-a-product/ | <a href="https://web.archive.org/web/*/https://www.functionize.com/blog/how-many-developers-does-it-take-to-test-a-product/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          <p><img width="1080" height="634" src="https://3laqvw22wekb3ykm8z4dbnq8-wpengine.netdna-ssl.com/wp-content/uploads/2020/09/ft-devs-to-qa-ratio.jpg" alt="How many developers does it take to test a product?" srcset="https://www.functionize.com/wp-content/uploads/2020/09/ft-devs-to-qa-ratio.jpg 1080w, https://www.functionize.com/wp-content/uploads/2020/09/ft-devs-to-qa-ratio-300x176.jpg 300w, https://www.functionize.com/wp-content/uploads/2020/09/ft-devs-to-qa-ratio-1024x601.jpg 1024w, https://www.functionize.com/wp-content/uploads/2020/09/ft-devs-to-qa-ratio-768x451.jpg 768w" sizes="(max-width: 1080px) 100vw, 1080px">      </p>	  
    
        <blockquote><p>Most development shops want QA testers involved in creating their new software. But what’s the proper ratio of testers to developers? Yes, “it depends,” but let’s look at the answers to, “…on what?”</p></blockquote>
<p>Development shops generally agree that quality assurance testing is a good thing, but that attitude is by no means universal. Even when development teams want QA testing, they may disagree about how many team members should devote themselves to testing full time rather than writing code. In a few organizations, the policy seems to be that developers should do their own testing, eliminating any need for separate testers.</p>
<p>Adding to the lack of agreement is the fact that not all types of development are the same. For example, teams developing interactive, informational webpages require different testing procedures than do teams creating airliner control systems. Likewise, some approaches, such as Agile development, have a built-in tester-to-developer ratio in which QA testers are an essential part of the team.</p>
<p>Still, in general there must be an ideal ratio of testers to developers, right? Well, maybe. As with everything else, the most you can say is that “it depends.” There are lots of <a href="https://www.functionize.com/blog/8-ways-to-know-that-its-time-to-hire-a-new-qa-tester/">reasons to say, “It’s time to hire a new QA professional,”</a> after all.</p>
<p>So if you realistically need a QA team, how big should it be?</p>
<h3>Start with one tester for two-to-five developers, and adjust accordingly</h3>
<p>Again and again, I was told about that popular ratio: one tester for every three to four developers. But use that as a starting point, not a <a href="https://amzn.to/32XAB1A" target="_blank" rel="noopener noreferrer">formal design pattern</a>, urge experienced developers and testers.</p>
<p>“There is no set-in-stone ratio that works perfectly for every scenario,” said James Boatwright, CEO of&nbsp;<a href="https://www.thecodegalaxy.com/" target="_blank" rel="noopener noreferrer">Code Galaxy</a>, which teaches programming skills to children. The testing scenarios should guide you. “While in most cases, for every two to five developers, one tester would be fine, some circumstances may require one tester for every developer, or maybe even two developers,” he says. This allows testers to focus on specific platforms or use cases.</p>
<p>As Jimmy Kamboj, founder of custom software firm <a href="https://www.imensosoftware.com/" target="_blank" rel="noopener noreferrer">Imenso&nbsp;Software</a>, says, many variables come into play, ranging from <a href="https://www.functionize.com/blog/5-rules-for-successful-test-automation/">the availability of automation tools in testing</a> to the income reliability of the project, to how much of the testing can be automated. “While one good tester is enough for a set of three, perhaps even four developers, a lot of things – like what is the scale of the project and what is being built – is extremely crucial to determine the ideal ratio,” he explains.</p>
<p>If your project uses Agile methodology, <a href="https://www.functionize.com/blog/lets-make-testing-agile-they-said-uh-what-did-they-mean-by-that/">expect to hire more testers</a>, because its premise includes the expectation that the testers are actively involved in improving the product. &nbsp;At custom web development company <a href="https://sprout.co.id/" target="_blank" rel="noopener noreferrer">Sprout</a>, squads have four developers, one user interface/user experience specialist, one product person, and one QA tester, says the company’s director, Arnold Sebastian Egg. After a sprint is complete, an automated QA engineer automates the previous test scripts of the previous test journey to make sure quality is maintained, Egg adds.</p>
<p>While a 3:1 ratio is the popular answer, it’s certainly not the <em>only</em> answer. For Reuben Yonatan, founder and CEO of&nbsp;<a href="https://getvoip.com/">GetVoIP</a>, development team size is usually five or six developers. “An average team will therefore have two testers and five developers to make a total of seven. I am comfortable with that ratio, and from experience, more developers might hinder the product instead of enhancing it.”</p>
<h3>Adjust developer:tester ratio based on expertise requirements – and tester experience</h3>
<p>Complex projects require more people to connect the dots, or specialists who know <a href="https://www.functionize.com/blog/what-testers-should-know-about-domain-knowledge/">how to find bugs in particular knowledge domains</a> (such as security testing or mobile applications). That may justify hiring someone with particular knowledge. That boosts the “standard” ratio to one QA tester for every two developers.</p>
<p>“The more complex a project, the more developers and testers it requires,” says Michal Kowalkowski, co-founder and CEO of <a href="https://www.nospoilers.ai/" target="_blank" rel="noopener noreferrer">NoSpoilers.AI</a>, where the developer:tester ratio is currently 3:1. “But writing tests can prove slower than the actual development.”</p>
<p>“Some features we develop take less time to test and troubleshoot, but some features are really complex,” agrees Oleg Donets, CEO at <a href="https://odmsoft.com/" target="_blank" rel="noopener noreferrer">ODMsoft</a>, which currently employs seven developers and two testers. “If we decide to get more developers on board, we’d try to keep the QA people in the same ratio.”</p>
<p>The ratio of two developers to one tester is used by Daniel Florido, lead developer at web development firm <a href="https://pixelstorm.com.au/" target="_blank" rel="noopener noreferrer">Pixelstorm</a>. “Testers are not just responsible for testing software end output.&nbsp;All development tasks should be tested, for technical bugs, device, and browser bugs as well as functional testing and usability testing.” That also enables developers and testers to learn one another’s style and practices. Testers may learn about a developer’s the strengths and weaknesses, and find out how to best provide feedback. Doing so, explains Florido, improves efficiency as well as team morale.</p>
<p>“We’ve spent years working out&nbsp;the right ratio for both profitability and accurate product delivery,” Florido continues.&nbsp;“For a tester to do what they need to with complete focus, this is the ideal outcome.”</p>
<h3>Look at other ratios, too</h3>
<p>But you should look at more than the number comparison for “developer head count.” Jon Quigley, principal of <a href="https://www.valuetransform.com/" target="_blank" rel="noopener noreferrer">Value Transformation</a>, allocates testers in a development environment based on financial number. “It should be 30% to 50% of the hours or money. For every two people that’s one tester or testing activity.”</p>
<p>Depending on project size, that testing activity could involve a lot of people. Quigley’s specialty is mission critical applications, including automotive, heavy truck, and mining equipment product development. A single failure could lead to death or disaster.</p>
<h3>Can you afford to hire another tester? Can you afford <em>not</em> to?</h3>
<p>Whatever your team’s need for QA testing, and its ability to hire good people, the need for QA testers can only grow. David Moise, president of <a href="https://decideconsulting.com/" target="_blank" rel="noopener noreferrer">Decide Consulting</a>, says that this could be a challenge. “There is a 9:1 ratio of developers to testers in the job market,” Moise says, noting that this has improved from an earlier ratio of 11:1. “To include front end people, it’s more like 13:1.”</p>
<p>That means it’s harder to hire QA testers that it used to be. “The skills are changing and there are more demands on what people need to have,” Moise says. “Before, you needed just to have experience testing. Now you need experience using automated platforms. They’re more expensive.”</p>
<p>What this means to the tester-to-developer ratio: It’s going to cost companies more to hire the testing staff they need. (On the other hand, if you’re a tester looking for a new job, this means you can ask for more money! Yay you! Just be sure to <a href="https://www.functionize.com/blog/what-should-be-on-a-qa-testers-resume/">level-up your QA tester’s resume</a>.)</p>
<p>And before you can accomplish that, you need buy-in from the people who hold the purse strings – who think that the developers can do it all. According to Sami Ullah, spokesperson for <a href="https://www.kualitatem.com/" target="_blank" rel="noopener noreferrer">Kulitatem</a>, a software QA testing company, some companies simply don’t want to spend the money. “They think QA as overhead for the company,” Ullah explains. That might happen, he posits, when “the company is quite small so they can’t afford hiring separate QA staff; the company has not defined departments and roles; they don’t realize the importance of QA (which they will eventually realize, after rolling out buggy applications in production).”</p>
<p>“Management has to see the value in QA,” Moise says. The considerations need to include the cost to fix a bug versus finding in during development. The amount of risk are you willing to tolerate, and the potential damage to your product’s reputation if your software looks bad. “Some companies don’t get it,” he says.</p>
<blockquote><p>Who does that hiring? Who makes this decision? As our white paper argues, perhaps it should be <a href="https://www.functionize.com/project/why-your-enterprise-needs-a-cqo-chief-quality-officer/">the chief quality officer</a>.</p></blockquote>
<div>
<div>
<p><img src="https://3laqvw22wekb3ykm8z4dbnq8-wpengine.netdna-ssl.com/wp-content/uploads/2020/02/WR-NBC-Photo-72DPI.jpg" alt="Wayne Rash"></p>
<div>
<p><span>by</span> Wayne Rash</p>
<p>Wayne Rash is based in Washington and has been writing about science and technology for nearly 40 years. He is a contributor to Forbes.com and a columnist for eWEEK. He is a frequent speaker on technology and has been a guest on NPR, NBC, PBS, CNN and Fox News.</p>
</div>

</div>
</div>
  </div></div>]]>
            </description>
            <link>https://www.functionize.com/blog/how-many-developers-does-it-take-to-test-a-product/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24620351</guid>
            <pubDate>Mon, 28 Sep 2020 19:27:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Reflections on running spaCy: commercial open-source NLP]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24620337">thread link</a>) | @dsr12
<br/>
September 28, 2020 | https://ines.io/blog/spacy-commercial-open-source-nlp | <a href="https://web.archive.org/web/*/https://ines.io/blog/spacy-commercial-open-source-nlp">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>As more and more people and companies are getting involved with open-source
software, balancing the expectations of an open community and a traditional
provider vs. consumer relationship is becoming increasingly difficult. Are
maintainers becoming too authoritarian? Are users becoming too demanding? Are
large companies selling out open-source?</p>
<p>In this post I’ll share some lessons we’ve learned from running
<a href="https://spacy.io/" target="_blank" rel="noopener nofollow noreferrer">spaCy</a>, the popular and fast-growing library for Natural
Language Processing in Python. I’ll also give you my perspective on how to make
commercial open-source work for both users and developers.</p>
<h2>Unpacking open-source</h2>
<p>Looking at the open-source ecosystem as one giant space with a fixed set of
rules and best practices can be problematic and frustrating. It’s frustrating
for maintainers of private projects, who end up overwhelmed by the flood of bug
reports and often demanding support requests. It’s frustrating for companies who
open-source their tools and are suddenly expected to get as many contributors
involved as possible. And it’s frustrating for users, who keep getting told to
“submit a PR or shut up”, and are struggling to decide which project to adopt
and trust.</p>
<p>Most open-source projects roughly fall into one of three categories:</p>





























<table><thead><tr><th></th><th><h4>Private</h4><em>“I made a thing and it’s over here.”</em></th><th><h4>Community</h4><em>“Let’s make a thing together.”</em></th><th><h4>Commercial</h4><em>“We made a product and it’s free.”</em></th></tr></thead><tbody><tr><td><strong>Maintainer Control</strong></td><td>high</td><td>low</td><td>high</td></tr><tr><td><strong>Maintainer Responsibility</strong></td><td>low</td><td>medium</td><td>high</td></tr><tr><td><strong>User Responsibility</strong></td><td>high</td><td>high</td><td>low</td></tr></tbody></table>
<p>Private projects are usually maintained by one person making most of the
decisions, but bearing little responsibility. After all, it’s just a person
sharing their code or showcasing their work, hoping it might be useful to
others. Community projects tend to make decisions collectively: maintainers take
responsibility for the software, but ultimately, users are expected to get
involved, instead of only making demands. In return, they get a say in the
project direction.</p>
<p>Commercial projects on the other hand generally stay more centralised:
maintainers often run a business related to their software and retain more
control over their project, while investing more resources and expecting less of
their users. Even projects by very large companies like Google and Facebook with
sizable developer communities roughly follow the same line of thought.</p>
<h2>A case for centralised, commercial open-source</h2>
<p>In many ways, <a href="https://spacy.io/" target="_blank" rel="noopener nofollow noreferrer">spaCy</a> is a pretty typical commercial
open-source project. It’s developed and maintained by mostly two people – Matt
and me. spaCy puts our work in front of many developers, which has allowed us to
bootstrap our company <a href="https://explosion.ai/" target="_blank" rel="noopener nofollow noreferrer">Explosion AI</a> independently through
consulting work while keeping our software free.</p>
<p>spaCy’s strength is that it’s easy to use, fast and opinionated. There’s only
one implementation of each component, and we’re trying to make it the best
possible one. At the same time, the core of spaCy is inherently hard to
contribute to. It’s fast because it’s written in <a href="http://cython.org/" target="_blank" rel="noopener nofollow noreferrer">Cython</a>, a
relatively niche language. The API is easy to use because it’s cohesive and was
mostly written by a single author.</p>
<p>All of this makes spaCy a good fit for production use, and we’re excited to see
more and more companies using it to power great products. But growth also comes
with responsibility. By making the choice to adopt our open-source software, our
users are offering us a large amount of trust upfront. <strong>We’re asking for
that trust, so we need to keep up our end of the bargain</strong>. If something is
broken, <em>we</em> need to fix it. If we rely on users to report problems, <em>we</em> better
make their experience pleasant. If we’re encouraging people to use spaCy in
production, <em>we</em> are responsible for making it work. And if we want to keep
spaCy cohesive and maintain attention to detail, <em>we</em> need to take the lead.</p>
<p>As we’re moving into a phase with more options for contributions, we want to
encourage them where they make the biggest difference: language data,
interoperation, tests and documentation. After we provided more docs and
refactored our website’s markup language, we saw a big increase in small pull
requests – from fixing minor typos in the docs (I admire everyone who goes out
of their way to do this!) to adding tokenizer exceptions for Bengali or Hebrew.</p>
<p>Our community consists of people with very different backgrounds and motivations
– developers who’ve been working in computational linguistics since before it
was cool, deep learning engineers training models with text input, data
scientists and digital humanities researchers, mobile app developers working on
their first bot and computer science students looking to get started. (For the
record, my background is front-end development, marketing and linguistics.) <strong>AI
is not a field of homogenous skills and experiences</strong>, and if we want to build
great software, need to adapt the way we think about community-driven
development.</p>
<h2>Challenges for open-source NLP</h2>
<p>One of the biggest challenges for Natural Language Processing is dealing with
fast-moving and unpredictable technologies. Most open-source development follows
a basic assumption: There’s a bug, and there’s a fix. There’s a feature, and
there’s an implementation. The quality of the code may vary and there are always
trade-offs. But ultimately, there’s a path, and there’s a goal. This is a lot
less true in AI or NLP.</p>
<p>Since spaCy was released, the best practices for NLP have changed considerably.
This also means that the library has had to change a lot. For instance,
dependency labels used to be much more relevant – now, our biggest focus is
getting spaCy up to speed with deep learning. However, new features and
enhancements are still based on very subjective assumptions about how people are
going to do NLP in the future. And it’s not only about code. There’s another
component that’s just as important: <strong>statistical models</strong>.</p>
<p>In the past, spaCy’s models had to be downloaded via a server maintained by us.
Although they played a huge part in spaCy’s performance, they were mostly hidden
away from the user. This was problematic – black-boxing technology is pretty
much the opposite of what we want to stand for. But how do you “open-source”
large binary data?</p>
<p>In spaCy v1.7, we finally introduced a
<a href="https://spacy.io/docs/usage/saving-loading" target="_blank" rel="noopener nofollow noreferrer">new way of loading models</a>, by
wrapping them as Python packages that can be installed via pip. All files are
also available attached to individual
<a href="https://github.com/explosion/spacy-models/releases" target="_blank" rel="noopener nofollow noreferrer">GitHub releases</a>,
containing more information on the model’s capabilities, license and data. We’ve
also included a model packaging tool in spaCy’s CLI so users can package their
own models.</p>
<p>Aside from the obvious advantages, like native versioning and pip installation,
model as packages send a much more <strong>reasonable message</strong>: A model is a
component of your application, just like any other dependency. In reality,
there’s not one “the model”. There can be many different ones with different
capabilities, that will produce very different results depending on what you do
with them. You can train your own models from scratch, or update existing ones.
And you can package and share your models with the community, just like we do.</p>
<h2>Giving projects a voice</h2>
<p>Whether you want it or not, your project will have a voice. Yours. This includes
everything you say and do – from documentation you write to issues you answer.
If a project sends mixed messaging, it causes confusion and conflicts. The
website says “Use our software!”, while the maintainers say “PR or GTFO”. The
community guidelines say that “there are no stupid questions, just stupid
answers”, while the maintainers mock their user’s issues on Twitter. Being rude
is not quirky, and it doesn’t save you any time or money either. People will not
appreciate your work more if you put them down.</p>
<p>This is also important to keep in mind when talking about diversity in
open-source and building an inclusive community. It’s not enough to simply adopt
community standards and state that “everybody’s welcome”. If this is what you
believe in, it also needs to be reflected in the overall messaging of your
project. (GitHub’s
<a href="https://opensource.guide/building-community/" target="_blank" rel="noopener nofollow noreferrer">Open Source Guide</a> has a nice
summary on this topic.) There’s also a difference between giving detailed
guidance, and enforcing strict rules. People are less likely to invest time and
contribute to a project if there’s a high potential of “doing things wrong” —
either due to lack of clarity, or arbitrary and overly rigid rules where every
deviation will be scrutinised.</p>
<p>Another root cause of mixed messaging is a lack of predictability. Users and
contributors should know where the project is going – even if ultimately, the
maintainers are going to be the ones making the decisions.
<a href="https://medium.com/swlh/the-unreliable-startup-69461f629383" target="_blank" rel="noopener nofollow noreferrer">Unreliable startups</a>
and their amazing journeys have made people wary of being lured in with big
promises, only to be let down.</p>
<p>One of our goals for spaCy is to focus on communicating our plans and ideas more
openly. This is one of the downsides of being such a small team: a lot of
decisions happen in one or two heads, which deprives the community of insights
into the process. Some of our decisions have been
<a href="https://github.com/explosion/spaCy/issues/962" target="_blank" rel="noopener nofollow noreferrer">more controversial</a> than others,
but no matter how much thought we put into them, it becomes irrelevant if we
don’t talk about it publicly. (Docker’s
<a href="https://github.com/moby/moby/pull/32691" target="_blank" rel="noopener nofollow noreferrer">recent fiasco</a> is an example of how
this can go very wrong and cause a lot of frustration.)</p>
<h2>Avoiding issue tracker bankruptcy</h2>
<p>By running spaCy in a centralised way, we accept that we have to be the main
source of support for now. This is hard, and we’ve not always been doing a great
job at this. It’s easy to get get stressed out when seeing the issue count go up
and falling behind on maintenance. We’ve all seen it before in other projects:
issues keep piling up and the maintainers, unable to keep up, eventually declare
issue tracker bankruptcy.</p>
<p><span>
      <a href="https://ines.io/static/e01c7326e8deb5bf061bea74e956066e/02884/open-source_bankruptcy.jpg">
    <span></span>
  <picture>
        <source srcset="https://ines.io/static/e01c7326e8deb5bf061bea74e956066e/3f012/open-source_bankruptcy.webp 198w, https://ines.io/static/e01c7326e8deb5bf061bea74e956066e/e5ab5/open-source_bankruptcy.webp 395w, https://ines.io/static/e01c7326e8deb5bf061bea74e956066e/c6f09/open-source_bankruptcy.webp 590w" sizes="(max-width: 590px) 100vw, 590px" type="image/webp">
        <source srcset="https://ines.io/static/e01c7326e8deb5bf061bea74e956066e/fbe57/open-source_bankruptcy.jpg 198w, https://ines.io/static/e01c7326e8deb5bf061bea74e956066e/e4247/open-source_bankruptcy.jpg 395w, https://ines.io/static/e01c7326e8deb5bf061bea74e956066e/02884/open-source_bankruptcy.jpg 590w" sizes="(max-width: 590px) 100vw, 590px" type="image/jpeg">
        <img src="https://d33wubrfki0l68.cloudfront.net/d8ba0c2197bf3af4bd78d213599ff9f1083b7e18/44328/static/e01c7326e8deb5bf061bea74e956066e/02884/open-source_bankruptcy.jpg" alt="Screenshot of GitHub tab bar with lots of issues and pull requests" title="Screenshot of GitHub tab bar with lots of issues and pull requests" loading="lazy">
      </picture>
  </a>
    </span></p><p>We try to label issues as they come in, even if we don’t have time to reply or
get to the bottom of them. Of course, reorganising the reports won’t actually
fix any bugs – but it makes it …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ines.io/blog/spacy-commercial-open-source-nlp">https://ines.io/blog/spacy-commercial-open-source-nlp</a></em></p>]]>
            </description>
            <link>https://ines.io/blog/spacy-commercial-open-source-nlp</link>
            <guid isPermaLink="false">hacker-news-small-sites-24620337</guid>
            <pubDate>Mon, 28 Sep 2020 19:25:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[We are shutting down Covid-19 projections using Machine Learning]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24620310">thread link</a>) | @dsr12
<br/>
September 28, 2020 | https://youyanggu.com/blog/six-months-later | <a href="https://web.archive.org/web/*/https://youyanggu.com/blog/six-months-later">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div class="page">

		<div data-page-title="Six Months Later – Youyang Gu">

			<section>

	<div>

		
		<p>September 28th, 2020</p>

		<p>It’s been six months since I made my first COVID-19 projections. What started as a small side project became a months long endeavor. 180 days and 180 forecasts later, the pandemic shows no signs of abating as we head towards winter. When I started <a href="https://covid19-projections.com/">covid19-projections.com</a>, there were only a handful of existing models, and very few of them were accurate. Now, there are over 30 models on the <a href="https://www.cdc.gov/coronavirus/2019-ncov/covid-data/forecasting-us.html">CDC Forecasting</a> page. A lot of great progress has been made in the modeling space over the past six months, and I hope others will get to better know these other models in the months to come. After much consideration, I have come to the difficult decision to not extend my projections beyond November 1, 2020. I plan to make the last forecast update to <i>covid19-projections.com</i> on Monday, October 5. This was undoubtedly a tough choice for me, and I hope to convey my thoughts in this post.</p>

<h3 id="winding-down">Winding Down</h3>

<p>There are several reasons that went into my decision, which I describe below:</p>

<ul>
  <li>Back in March and April, I was concerned by the lack of high-quality models being cited in the news and media. The numbers being referenced ranged widely from <a href="http://www.healthdata.org/news-release/ihme-hold-media-briefing-4-pm-eastern-today-details-below">60,000 deaths</a> to <a href="https://www.imperial.ac.uk/mrc-global-infectious-disease-analysis/covid-19/report-9-impact-of-npis-on-covid-19/">2,200,000 deaths</a> in the US by August. My goal was to create a more realistic and accurate model, and hence <em>covid19-projections.com</em> was born. Looking back now, I believe I was able to achieve what I had set out to accomplish. In the months since, several other reliable models have emerged. Hence, I believe this is an appropriate time for me to wind down.</li>
  <li>In the beginning, the majority of my time was spent on building a model from scratch: learning about infectious diseases, incorporating a machine learning layer, and iteratively learning the various epidemiolgical parameters. As time went on and my model became more mature, the focus of the work changed. Lately, a lot of my time is being spent making minor adjustments and tweaks to refine the model’s performance. I feel that I am now spending more time on maintaining the model rather than making new advancements, which is something I hope to change.</li>
  <li>As one can imagine, building and maintaining a COVID-19 model takes a lot of time and effort. Many models have entire groups dedicated to the project, as well as the funding and resources necessary to continue this project for the foreseeable future. Unfortunately, I do not fall under this category. Since Day 1, I have been the sole author of the model and have not relied on any external funding. The only things I used to build this model was a laptop, <a href="https://twitter.com/youyanggu">a Twitter account</a>, and $20 to buy the domain name <em>covid19-projections.com</em>. As much as I would like to continue working on this project, my current setup is not sustainable or scalable in the long run.</li>
  <li>Looking at COVID-19 data on a daily basis for the past six months can be exhausting. Taking a step back would allow me to explore new ideas.</li>
</ul>

<p>With that said, I firmly believe that the modeling community is in good hands. Below, I will present a few models that I have found to be the most reliable.</p>

<h3 id="model-alternatives">Model Alternatives</h3>

<p>I know this news is disappointing for the many people who have been closely following my model over the past few months, so I want to provide a few reputable alternatives. It’s important that we focus on models which have a proven track record and not just those that have generated the biggest headlines. No single model is perfect, hence this is why I believe it is important to look at different models and understand the assumptions of each one in order to interpret the forecasts. Due to this reasoning, I recommend the <a href="https://covid19forecasthub.org/">COVID-19 Forecast Hub</a>, which aggregates forecasts from over 30 models and sends them to the <a href="https://www.cdc.gov/coronavirus/2019-ncov/covid-data/forecasting-us.html">CDC</a> each week to help inform public health decision making.</p>

<p>From among the Forecast Hub, below are a list of models that I have found to be the most reliable over the past few weeks and months. You can find a visualization of all the models listed below <a href="https://viz.covid19forecasthub.org/">here</a>. In addition to forecasting reported deaths, the below models also have forecasts for <a href="https://www.cdc.gov/coronavirus/2019-ncov/cases-updates/forecasts-cases.html">confirmed cases</a>. The USC, COVIDAnalytics, and LANL models also have forecasts for international countries.</p>

<ul>
  <li><a href="https://viz.covid19forecasthub.org/">COVIDhub Ensemble</a> - An aggregation of the forecasts of ~30 models submitted to the <a href="https://covid19forecasthub.org/">COVID-19 Forecast Hub</a>. The combined forecast is then published on the <a href="https://www.cdc.gov/coronavirus/2019-ncov/covid-data/forecasting-us.html">CDC website</a>. You can find the pre-print <a href="https://www.medrxiv.org/content/10.1101/2020.08.19.20177493v1">here</a>. Because it is able to combine the forecasts of so many models, it is more accurate than any single model alone. Hence, if one were to only use one model, this would be the one to use.</li>
  <li>UMass Amherst - An early model that has consistently performed well since its release in May. It is made by the <a href="https://reichlab.io/">Reich Lab</a>, the same group that runs the COVID-19 Forecast Hub. The downside is that it only forecasts 4 weeks out and has no visualizations (other than on the Forecast Hub).</li>
  <li><a href="https://covid19.uclaml.org/">UCLA</a> - Another early model that has consistently performed well. It also has forecasts for the reproduction number (Rt). The visualizations are very well-done.</li>
  <li><a href="https://pandemicnavigator.oliverwyman.com/forecast?mode=country&amp;region=United%20States&amp;panel=mortality">Oliver Wyman</a> - A new model released in June that instantly became one of the top-performing models since its release. It only has public forecasts 4 weeks into the future.</li>
  <li><a href="https://www.covidanalytics.io/">COVIDAnalytics (MIT DELPHI)</a> - A top-performing model for US nationwide forecasts.</li>
  <li><a href="https://scc-usc.github.io/ReCOVER-COVID-19/#/">USC</a> - A new model released in July that has made great improvements over the past few weeks. It is one of the only models (along with my own) to make daily updates.</li>
  <li><a href="https://covid-19.bsvgateway.org/">Los Alamos National Lab (LANL)</a> - One of the top-performing models from April-July, but has been under-forecasting recently.</li>
  <li>Other up-and-coming Forecast Hub models that have performed well thus far: CMU, LNQ, JCB</li>
</ul>

<p>I highly recommend those who have been following my work to take some time studying the aforementioned models. I have personally spoken to most of the groups and have listened to their presentations. I can attest to their proven track record and hope they can continue to provide reliable forecasts in the weeks and months to come. When viewed in tandem, these models can help provide a clearer picture of what will most likely happen in the upcoming weeks. While not crystal balls, I believe these forecasts can be very useful tools for researchers and policy makers.</p>

<p>The above list is not necessarily an exhaustive list of reliable models. You can learn more about my weekly evaluations of the different models <a href="https://covid19-projections.com/about/#historical-performance">here</a>. I hope to continue updating these model evaluations in the near future.</p>

<h3 id="whats-next">What’s Next</h3>

<p>Ending my model forecasts does not mean that my work in COVID-19 is over. This decision will allow me to dedicate my freed up time to other areas of COVID-19 data analysis. In this day and age, misinterpretation of data (both intentional and unintentional) is pervasive. Anyone can cherry-pick data to support his or her narrative. My goal is to continue presenting COVID-19 data in a rigorous, unbiased manner. Follow me on Twitter at <a href="https://twitter.com/youyanggu">@youyanggu</a> to stay up to date with my latest analysis.</p>

<p>I am forever grateful to have the support of so many people from across the US and around the world. I want to thank everyone who believed in my work from the early days, especially Nicholas Reich, his group at UMass Amherst, and the scientists at the CDC. I also want to thank all the scientists, researchers, and everyone else with whom I’ve had the pleasure of interacting with online; at a time where in-person contact has been limited, these interactions have been tremendously helpful. I feel honored to be able to contribute to the scientific community in improving our understanding of the disease. This was certainly not something that I, a data scientist with no background in infectious diseases, expected just a year ago. I haven’t always been right, but I’m thankful to be part of a community that is constantly helping me learn.</p>

<p>I am currently working on a piece that outlines the things I have learned over the past six months. I hope to post it here in the next week or two. Stay tuned!</p>

<h3 id="get-in-touch">Get in Touch</h3>

<p>While I no longer will be making public forecasts, I hope to continue to be involved in the forecasting space in some shape or form. If you are interested in hearing more about my work, feel free to send me a message.</p>

<p>I still don’t know what the future holds. I am always open to new challenges and projects, especially those involving the use of data-oriented modeling to tackle public health problems. If you have any suggestions or ideas, please don’t hesitate to reach out to me using the contact button below. I would love to get in touch.</p>

<p><a href="https://youyanggu.com/contact">Contact Me</a></p>

<p>In the meantime, let’s all work together to continue fighting this pandemic. Each one of us <em>can</em> make a difference.</p>

<h4 id="--youyang"><em>- Youyang</em></h4>

<p><br>
<a href="#top">Back to Top</a></p>


	</div>

</section>

		</div>

	</div></div>]]>
            </description>
            <link>https://youyanggu.com/blog/six-months-later</link>
            <guid isPermaLink="false">hacker-news-small-sites-24620310</guid>
            <pubDate>Mon, 28 Sep 2020 19:22:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[MySQL's Gotchas]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24619777">thread link</a>) | @awmarthur
<br/>
September 28, 2020 | https://www.dolthub.com/blog/2020-09-28-mysql-gotchas/ | <a href="https://web.archive.org/web/*/https://www.dolthub.com/blog/2020-09-28-mysql-gotchas/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-cy="blog-post-text"><p><a href="https://github.com/liquidata-inc/dolt">Dolt</a> is Git for data. Git
versions files, Dolt versions tables. Dolt comes with a SQL engine
built in, which lets you run SQL queries against any version of the
data you've committed. Our goal is to become fully SQL compliant and
compatible with MySQL's dialect. To this end, we've contributed
to <a href="https://github.com/dolthub/sqllogictest">sqllogictest</a> and
<a href="https://www.dolthub.com/blog/2020-05-04-adopting-go-mysql-server/">adopted</a>
the go-mysql-server project.
We've made significant progress on <a href="https://www.dolthub.com/blog/2019-12-17-one-nine-of-sql-correctness/">our journey</a>
to full SQL correctness.
As of publishing time, we're 92% correct according to sqllogictest.
Along the way we've learned a lot about MySQL's idiosyncrasies and
found some pretty inexplicable behavior.
Today we're going to share some of our favorites.</p>
<h2>MySQL: the Javascript of Databases</h2>
<p>People love the hate. I've watched endless software talks
on brilliant technologies given by top industry experts.
My favorite software talk?
<a href="https://www.destroyallsoftware.com/talks/wat">"Wat"</a>,
the JS-roasting classic by Gary Bernhardt.
Sure, self-driving cars and planet-scale data systems are cool,
but still not as interesting as </p>
<div data-language="javascript"><pre><code><span>%</span> node
<span>&gt;</span> <span>{</span>is<span>:</span> <span>"this"</span><span>}</span> <span>+</span> <span>[</span><span>"a"</span><span>,</span> <span>"number?"</span><span>]</span>    
<span>NaN</span> </code></pre></div>
<p>In the words of one of Dolthub's founders, "MySQL is the Javascript of databases".
It's got its rough spots, but it's a known entity.
Let's take a look at some of those rough spots. </p>
<h2>GOTO FAIL</h2>
<p>MySQL has no problem letting you shoot yourself in the foot.</p>
<div data-language="sql"><pre><code><span>CREATE</span> <span>TABLE</span> default_fail <span>(</span>
    pk <span>int</span> <span>NOT</span> <span>NULL</span> <span>PRIMARY</span> <span>KEY</span> <span>DEFAULT</span> <span>(</span><span>NULL</span><span>)</span>
<span>)</span><span>;</span></code></pre></div>
<p>Much like <code>{} + []</code> this example is something you're not likely to run into,
on the other hand why doesn't this just error?</p>
<p>Foreign Keys, like all database constraints, are crucial to ensuring data integrity.
That's why you probably should't be able to link these two columns with disparate data domains:</p>
<div data-language="sql"><pre><code><span>CREATE</span> <span>TABLE</span> parent <span>(</span>
    id <span>ENUM</span><span>(</span><span>'a'</span><span>,</span><span>'b'</span><span>,</span><span>'c'</span><span>)</span> <span>PRIMARY</span> <span>KEY</span>
<span>)</span><span>;</span>
<span>CREATE</span> <span>TABLE</span> child <span>(</span>
    parent_id <span>ENUM</span><span>(</span><span>'x'</span><span>,</span><span>'y'</span><span>,</span><span>'z'</span><span>)</span> <span>PRIMARY</span> <span>KEY</span><span>,</span>
    <span>FOREIGN</span> <span>KEY</span> <span>(</span>parent_id<span>)</span> <span>REFERENCES</span> parent<span>(</span>id<span>)</span>
<span>)</span><span>;</span></code></pre></div>
<p>Things are complicated even further by MySQL's implementation of enums:</p>
<div data-language="sql"><pre><code><span>INSERT</span> <span>INTO</span> parent <span>(</span>id<span>)</span> <span>VALUES</span> <span>(</span><span>'a'</span><span>)</span><span>;</span>
<span>INSERT</span> <span>INTO</span> child <span>(</span>parent_id<span>)</span> <span>VALUES</span> <span>(</span><span>'x'</span><span>)</span><span>;</span></code></pre></div>
<p>This actually <em>doesn't</em> fail because enums are translated to integers
and in this instance both 'a' and 'x' evaluate to 1. Gotcha.</p>
<p>The first two examples can really be attributed to user error.
If you're going to mess with the schema of a database,
you should know enough not to make those mistakes.
But it turns out there are even more interesting ways to break relations
between MySQL objects. Consider this table, view, and insert trigger:</p>
<div data-language="sql"><pre><code>mysql<span>&gt;</span> <span>CREATE</span> <span>TABLE</span> users <span>(</span>
    id <span>int</span> <span>PRIMARY</span> <span>KEY</span><span>,</span>
    <span>level</span> <span>int</span>
<span>)</span><span>;</span>
mysql<span>&gt;</span> <span>CREATE</span> <span>VIEW</span> cohorts <span>AS</span> 
    <span>SELECT</span> <span>level</span><span>,</span> <span>count</span><span>(</span><span>level</span><span>)</span> <span>FROM</span> users 
        <span>GROUP</span> <span>BY</span> <span>level</span> <span>ORDER</span> <span>BY</span> <span>level</span> <span>ASC</span><span>;</span>
mysql<span>&gt;</span> <span>CREATE</span> <span>TRIGGER</span> start_level_zero
    BEFORE <span>INSERT</span> <span>ON</span> users
    <span>FOR EACH ROW</span> 
        <span>SET</span> NEW<span>.</span><span>level</span> <span>=</span> <span>0</span><span>;</span></code></pre></div>
<p>Now say we want to change the name of the column <code>level</code>...</p>
<div data-language="sql"><pre><code>mysql<span>&gt;</span> <span>ALTER</span> <span>TABLE</span> users <span>RENAME</span> <span>COLUMN</span> <span>level</span> <span>TO</span> <span>status</span><span>;</span>
    Query OK<span>,</span> <span>0</span> <span>rows</span> affected <span>(</span><span>0.01</span> sec<span>)</span>
    Records: <span>0</span>  Duplicates: <span>0</span>  <span>Warnings</span>: <span>0</span>

mysql<span>&gt;</span> <span>INSERT</span> <span>INTO</span> users <span>VALUES</span> <span>(</span><span>4</span><span>,</span><span>0</span><span>)</span><span>;</span>
    ERROR <span>1054</span> <span>(</span><span>42</span>S22<span>)</span>: Unknown <span>column</span> <span>'level'</span> <span>in</span> <span>'NEW'</span>
mysql<span>&gt;</span> <span>SELECT</span>  <span>*</span> <span>FROM</span> cohorts<span>;</span>
    ERROR <span>1356</span> <span>(</span>HY000<span>)</span>: <span>View</span> <span>'cohorts'</span> <span>references</span> invalid <span>table</span><span>(</span>s<span>)</span> <span>or</span> <span>column</span><span>(</span>s<span>)</span> <span>or</span> <span>function</span><span>(</span>s<span>)</span> <span>or</span> <span>definer</span><span>/</span><span>invoker</span> <span>of</span> <span>view</span> lack rights <span>to</span> <span>use</span> them</code></pre></div>
<p>Broken! The column rename left the view and the trigger statement referencing the old column name.
Everything that depends on that column name must be updated.
<code>Unknown column 'level' in 'NEW'</code> doesn't clue you in that your trigger is broken,
and until you figure it out you won't be able to make any inserts to <code>users</code>.</p>
<p>Now let's take a look at MySQL's type system, which I have to say is <em>especially</em> Javascripty.
Our table definition is: </p>
<div data-language="sql"><pre><code><span>CREATE</span> <span>TABLE</span> types_table <span>(</span>
    pk <span>int</span> <span>primary</span> <span>key</span><span>,</span> s <span>varchar</span><span>(</span><span>20</span><span>)</span><span>,</span> b <span>bool</span>
<span>)</span><span>;</span>
<span>INSERT</span> <span>INTO</span> types_table <span>VALUES</span>
    <span>(</span><span>0</span><span>,</span> <span>"abc"</span><span>,</span> <span>true</span><span>)</span><span>,</span>
    <span>(</span><span>1</span><span>,</span> <span>"xyz"</span><span>,</span> <span>false</span><span>)</span><span>;</span></code></pre></div>
<p>Suppose we made wanted to find rows with <code>s = "abc"</code>, but we made a mistake and typed <code>b</code> instead of <code>s</code>. </p>
<div data-language="sql"><pre><code>mysql<span>&gt;</span> <span>SELECT</span> <span>*</span> <span>FROM</span> types_table <span>WHERE</span> b <span>=</span> <span>"abc"</span>  
<span>+</span>
<span>|</span> pk <span>|</span> s    <span>|</span> b    <span>|</span>
<span>+</span>
<span>|</span>  <span>1</span> <span>|</span> xyz  <span>|</span>    <span>0</span> <span>|</span>
<span>+</span>
<span>1</span> <span>row</span> <span>in</span> <span>set</span><span>,</span> <span>1</span> warning <span>(</span><span>0.00</span> sec<span>)</span></code></pre></div>
<p>MySQL is happy to perform type coercion, interpret <code>"abc"</code> as <code>false</code>, and execute the query.
It even spits out <code>1 warning</code> at the end, which is mea culpa if I've ever seen it.</p>

<p>Understanding and modeling MySQL's behaviors is important because it lets
us leverage the massive MySQL ecosystem.
Our SQL parser and server are based on Vitess and support the standard MySQL wire protocol.
This means we can support a wide range of sql <a href="https://www.dolthub.com/docs/integrations/programmatic-clients/">integrations</a>
with minimal effort.
However, as some of the examples above showed, following along bit for bit isn't necessarily a good idea.
Datetime conversion is one example of how we choose to deviate from the MySQL standard.
The following query casts a datetime to an integer. </p>
<div data-language="sql"><pre><code>
<span>SELECT</span> <span>CONVERT</span><span>(</span><span>CONVERT</span><span>(</span><span>"2020-08-07 06:05:04"</span><span>,</span> <span>DATETIME</span><span>)</span><span>,</span> SIGNED<span>)</span><span>;</span> </code></pre></div>
<p>MySQL chooses to use ISO8601 which have the format <code>YYYYMMDDHHMMSS</code>.
Dolt, on the other hand, uses standard Unix timestamps for this cast.</p>
<div data-language="sql"><pre><code>
<span>SELECT</span> <span>CONVERT</span><span>(</span><span>CONVERT</span><span>(</span><span>"2020-08-07 06:05:04"</span><span>,</span> <span>DATETIME</span><span>)</span><span>,</span> SIGNED<span>)</span><span>;</span> </code></pre></div>
<p>Choosing where and how to deviate from the standard isn't an exact science,
it involves some estimation and a lot of listening to your users.
Generally we seek to maximize compatibility while also createing intuitive behaviors.</p>

<p>Building a relational database is a lot of work.
Reaching 100% compatibility with an industry standard like MySQL adds another layer of difficulty.
SQL is the lingua franca of data analysis and Git is the same for version control.
We believe that your data deserves the power of both, and that migrating to Dolt should be a seamless transition.
Our journey is ongoing, but we hope you'll come along for the ride.
As always, <a href="https://www.dolthub.com/contact">let us know</a> what you think about Dolt and what we should build next. </p></div></div>]]>
            </description>
            <link>https://www.dolthub.com/blog/2020-09-28-mysql-gotchas/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24619777</guid>
            <pubDate>Mon, 28 Sep 2020 18:25:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Useful and Beneficial (2012)]]>
            </title>
            <description>
<![CDATA[
Score 15 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24619745">thread link</a>) | @panic
<br/>
September 28, 2020 | http://mikekchar.github.io/portfolio//UsefulAndBeneficial | <a href="https://web.archive.org/web/*/http://mikekchar.github.io/portfolio//UsefulAndBeneficial">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		    <p>Long ago a master woodcarver and his assistant were walking along a
road when they happened upon a huge tree.  It was at least 1000 years
old and was so large that it stretched from horizon to horizon.  At
its base, a shrine dedicated to the spirit of the tree was doing a
brisk business catering to the many pilgrims who had traveled to see
it.</p>

<p>The assistant stopped and looked astonished at the magnificent tree.
As a man in love with his trade he could not help but to be moved by
such an amazing tree.  However, the master woodcarver continued along
his way, scarcely giving the tree a second glance.</p>

<p>“How can you, a master woodcarver who has spent his long life working
in wood, not be struck by such a sight?” the assistant inquired.</p>

<p>“That tree is not fit for carving.  Its limbs are twisted and its
trunk full of knots.  A cabinet made from it would warp.  A boat made
from it would sink.  It is useless!”</p>

<p>That night, the assistant dreamed of the tree.  In his dream the tree
spoke to him, “Humans are so blind.  They look at me and don’t see the
secret of my success.  I am useless, but it is precisely because I am
useless that I have survived to become great.  Had my wood been useful
for any purpose, I would long ago have succumbed to the axe and the
saw.  I would have died to satisfy the whims of others.”</p>

<p>The assistant awoke from his dream with new understanding, but one
question still plagued him.  Later that day he told the master
woodcarver of his dream and asked, “The tree said that it could thrive
because it could not be used.  But doesn’t the shrine use the tree?
Is the tree not being useful to the shrine.”</p>

<p>“What does a tree care if a shrine worships it?  It is true that the
tree is beneficial to the shrine, but it is not being used.  It is
free to be a tree and to live as it chooses.  In fact, by being
beneficial to the shrine, it receives benefit in return.  The shrine,
not wanting to lose the benefit of the tree, protects it from storms
and waters it during droughts.  In this way, the tree has learned to
achieve ultimate success.”</p>

<p>I read a similar story many years ago as a struggling computer
programmer in Canada.  It is not a new story. It is a compilation of a
few similar stories from taoist literature.  It is at least 2500 years
old.</p>

<p>It is very easy to fall into the trap of trying to be useful.  We do
what we are told to do and suffer through the many things which we
don’t enjoy.  But we risk becoming the woodcarver’s wood rather than
the tree.  We struggle to be useful but then resent the fact that we
are used.</p>

<p>In every job there are countless ways to be beneficial.  We can choose
things that we, ourselves, value and do them independently.  If we
choose wisely these will be things that the people around us benefit
from.  The more we choose to benefit others by being
ourselves, the more others will come to rely on our unique strengths.  Instead
of finding ways to use us, they will simply be delighted by our
presence.</p>

	    </div><p><strong>Note:</strong> This website uses Google Analytics to 
    collect statistics.  If you wish to avoid being tracked, please 
    disable Javascript for this website (using the NoScript plugin, 
    for instance).
</p></div>]]>
            </description>
            <link>http://mikekchar.github.io/portfolio//UsefulAndBeneficial</link>
            <guid isPermaLink="false">hacker-news-small-sites-24619745</guid>
            <pubDate>Mon, 28 Sep 2020 18:23:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Head of MS Estonia investigation: Estonia sank on collision with submarine]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24619728">thread link</a>) | @mpweiher
<br/>
September 28, 2020 | https://news.err.ee/1140442/head-of-ms-estonia-investigation-estonia-sank-on-collision-with-submarine | <a href="https://web.archive.org/web/*/https://news.err.ee/1140442/head-of-ms-estonia-investigation-estonia-sank-on-collision-with-submarine">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>Margus Kurm, former state prosecutor and head of the government's investigative committee looking into the sinking of ferry MS <em>Estonia </em>in 2005-2009, said in an interview with ETV's "Pealtnägija" that new scenes of the shipwreck show the ship most likely sank after a collision with a submarine.</p><div><p><strong>You have seen these clips that have reached the media repeatedly and before anyone else. What was your first reaction and emotion seeing scenes of the dive?</strong></p><p>The first reaction was shocking. Not because the hole (in the ship's hull - ed.) was visible but rather because it was discovered so simply.</p><p><strong>Explain, what is the location of this hole and what is the meaning of it?</strong></p><p>The meaning is [MS] <em>Estonia</em> did not sink because of a bow visor breaking, it was a collision with something large enough to create a four-meter long hole in the ship's hull.</p><p><strong>This is an unbelievable twist!</strong></p><p>It is not unbelievable in that sense. That there could be a hole on the ship has been mentioned in previous evidence and analyses. It is evidence for something that has long been speculated.</p><p><strong>But a collision? With what?</strong></p><p>Considering that the tear is below the water line and considering noone has ever mentioned that another ship could have sunk with <em>Estonia </em>and none of the survivors have said they saw a ship close to <em>Estonia - </em>the most likely cause is <em>Estonia </em>collided with a submarine.</p><p><strong>That means there should be a damaged submarine somewhere?</strong></p><p>Yes, it means there should be a damaged submarine somewhere. But I will specify a bit. If one says a collision with a submarine, the first thought is the submarine ran into <em>Estonia </em>from its side. It might not have been so simple. It was more likely a intrusion. That <em>Estonia </em>and a submarine went in the same direction. And we can not rule out that <em>Estonia </em>might have hit the submarine, grazed the submarine. The question is what was a submarine doing on <em>Estonia's</em> route.</p><p><strong>There has been mentions of an explanation that perhaps the hole developed after the ship had sunk. There is a theory that it bumped into a large rock or cliff while sinking and that caused the hole.</strong></p><p>I do not consider that likely. The part, the section where the damage was found has never touched the seabed. The position that <em>Estonia </em>is in post-accident was documented during dives conducted in 1994. There have been figures drawn, graphics made on how <em>Estonia </em>lies on the seabed. The entire bottom of the ship, including the vehicle deck, on both sides, is out of water. It is a simple thing, everyone can check it on paper at home. We know, according to the report that the ship is under a 211-degree angle. Meaning, if we draw a straight line vertically on <em>Estonia</em>'s hull, from funnel to keel, and compare it to the seabed, the angle is 211 degrees.</p><p>In addition, from the footage provided, we know that <em>Estonia</em>'s so-called hotel part is partly under mud but a large part still sticks out. A part of the captain's deck is also out. You can draw a two-dimensional picture of the position <em>Estonia </em>is in underwater. It clearly shows that the entire bottom, including vehicle deck, is away from the seabed.</p><p>And therefore, a statement has been made that the location of the damages was not visible earlier. It absolutely was. The entire bottom, including the vehicle deck, was away from the seabed and could have been filmed in 1994.</p><p><strong>Which in turn leads to the point that in 1994, when the official investigative dives took place, the holes we see today were not spotted?</strong></p><p>Yes. And that has been a large problem. Two options, it either was not filmed or was filmed and not made public. The second option is most likely, because the footage was lost.</p><p><strong>The fact we have not been made aware of the hole for 26 years or it has been covered up is scandalous enough?</strong></p><p>Well, I think so.</p><p><strong>Once again. The hole, as it has been filmed by the documentary crew, would explain all those questions to you, but to anyone else still bothered by it?</strong></p><p>This will sound funny but the hole fits <em>Estonia's </em>hull well. It explains all questions up in the air for 26 years. Firstly, it explains how water was able to get under the vehicle deck. Secondly, it explains why <em>Estonia </em>did not keel over in a few minutes.</p><p>If a large amount of water reaches the vehicle deck or a ferry loses stability in another way, it keels over in minutes, sinks, windows go under water and the ship's so-called superstructure breaks, it's larger, it gets heavier, sinks and the bottom, where the air is, rises to the top and since the air can not escape, the ship remains on the surface for hours or even days. This did not happen with <em>Estonia. </em><em>Estonia </em>did not keel over in minutes, it stood up for half an hour. And once it finally tipped over, it sunk in ten minutes.</p><p>This hole solves all contradictory evidence as well. Since it is no longer necessary to bend the statements made by three young seamen, who saw from the machine room that the ramp was closed.</p><p>And most importantly - one of the men said in addition that he had seen from a camera pointed at the ramp that it was closed and water was coming in from the sides, he saw the side of the ship, from a camera pointed at the maritime pilot's door that there was water on the vehicle deck. Since the ship was angled, there was water on the starboard decks and there was enough to reach the front lights of the cars on board. Estimatedly, 500-700 tons of it.</p><p>All experts are in agreement that there is too much water for it to have come in from the sides of the ramp. But there is not enough water there for it to have come in from the opened ramp. If the ramp is open, 2000 tons would flow in a minute. There was 500-700 tons. Therefore, there was water on the vehicle deck. But it must have come in from someone else, not the ramp. And as we can see from the new footage, the tear reaches both the bottom and top of the vehicle deck. Meaning the hole - considerably smaller that an opened ramp - was where the water came from.</p><p><strong>What should be done now with this information?</strong></p><p>The wise don't rush (an Estonian idiom - ed.). I think the government should firstly sit down calmly and think deeply about the situation. And ask who we can trust at a time like this. And also think about how to win back the public's trust.</p><p>Let's go back in time. Everyone, who spoke of any kind of weapon smuggling, were ridiculed. It turned out in 2005 that there indeed were weapons transported. People speaking of a hole in the ship's hull all these years have been ridiculed. It was considered impossible. Now, 26 years later, we have footage of there being a hole.</p><p>If we add to it that less than a month after the sinking, there was an idea to cover the ship in concrete. A gravesite peace was agreed to in 1995. An international investigative committee had not yet been able to start their investigation when the Swedish government came out with an idea to cover the ship with concrete. And before the committee finished, the shipwreck was locked down. It is unheard of that the most critical piece of evidence is locked.</p><p><strong>From the first reactions of the government of Estonia and other countries, I gather that there is a new investigation to be done, including a robot investigation. Would that satisfy you?</strong></p><p>Yes and no. Like I said - the question is whether there is enough trust to conduct such research today. I think, considering all that has been mentioned, the Swedes should no longer be trusted on this topic. Does the Estonian government have enough trust credit to conduct an investigation, trusted by its people and the public as a whole? I do not think so. Meaning, if another investigation is conducted, it must be completely transparent. It would be good if the investigation would be headed by someone outside the government sector.</p><p>Secondly, the investigation committee must have representatives from Sweden and Finland as well. And thirdly, to me, there must be press involved. This investigation should be completely transparent. It should be followable online, in essence. If there is a dive, ETV must be there with cameras and film it. So everyone could see what is done, how it is done and what are the results. If we do not achieve such transparency, it is difficult to trust the final results of the committee.</p><p>But before all, I would recommend the Estonian government to sit down with the Swedes and ask if they have any documents archived where it says what actually happened. And if there are any people in Sweden who know what happened and are prepared to talk about it.</p><p><em><strong>The further details and circumstances of MS Estonia's doom will be discussed at length on "Pealtnägija" on Wednesday.</strong></em></p><p><strong>Background</strong></p><p>The ferry Estonia sank on the night of September 28, 1994, sailing from Tallinn to Stockholm.&nbsp;The sinking of Estonia is the largest maritime disaster in peacetime in the Baltic Sea, killing 852 people from 17 countries.</p><p>The shipwreck was investigated by a joint committee formed by the governments of Estonia, Finland and Sweden between 1994 and 1997 and by a government commission headed by the Public Prosecutor's Office in 2005-2009.&nbsp;</p><p>In 1995, Estonia, Finland and Sweden signed an agreement to protect the shipwreck, which prohibits diving to the wreck.</p><p>The disaster is commemorated by the "Broken Line" monument in Tallinn.</p><figure content-photo-template2="" data-photo-id="688654"><img data-photo-id="688654" src="https://s.err.ee/photo/crop/2019/09/23/688654h12eet24.jpg"><figcaption>The 'Broken Line' monument in Tallinn.<span> Source: </span>Jan Pohunek/Creative Commons</figcaption></figure><p>--</p><p><em>Follow ERR News on <a href="https://www.facebook.com/ERRNews/">Facebook</a> and <a href="https://twitter.com/errnews">Twitter</a> and never miss an update!</em></p>									</div></div>]]>
            </description>
            <link>https://news.err.ee/1140442/head-of-ms-estonia-investigation-estonia-sank-on-collision-with-submarine</link>
            <guid isPermaLink="false">hacker-news-small-sites-24619728</guid>
            <pubDate>Mon, 28 Sep 2020 18:22:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Promising computer simulations for stellarator plasmas]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24619682">thread link</a>) | @sizzle
<br/>
September 28, 2020 | https://www.ipp.mpg.de/4928395/05_20 | <a href="https://web.archive.org/web/*/https://www.ipp.mpg.de/4928395/05_20">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  
  
  <p>Path to higher thermal insulation of the plasma / Reduction of plasma turbulence</p>
  

  

  <p>The turbulence code GENE (Gyrokinetic Electromagnetic Numerical Experiment), developed at Max Planck Institute for Plasma Physics (IPP) at Garching, Germany, has proven to be very useful for the theoretical description of turbulence in the plasma of tokamak-type fusion devices. Extended for the more complex geometry of stellarator-type devices, computer simulations with GENE now indicate a new method to reduce plasma turbulence in stellarator plasmas. This could significantly increase the efficiency of a future fusion power plant.</p>
  
  
<figure data-description="Turbulence calculation with the GENE-3D code for Wendelstein 7-X: The snapshot shows the turbulent variation of the plasma density over different cross-sections of the plasma ring. In contrast to tokamaks, whose ring-shaped plasma has the same cross-section everywhere, the cross-section changes its shape in stellarators." data-picture="base64;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">
      
    

    
    <figcaption>
        <p>
          Turbulence calculation with the GENE-3D code for Wendelstein 7-X: The snapshot shows the turbulent variation of the plasma density over different cross-sections of the plasma ring. In contrast to tokamaks, whose ring-shaped plasma has the same cross-section everywhere, the cross-section changes its shape in stellarators.
        </p>
        <p>
           Illustration: IPP, A. Bañón Navarro
        </p>
    </figcaption>
</figure>



<p>For the fusion researchers at IPP, who want to develop a power plant based on the model of the sun, the turbulence formation in its fuel – a hydrogen plasma – is a central research topic. The small eddies carry particles and heat out of the hot plasma centre and thus reduce the thermal insulation of the magnetically confined plasma. Because the size and thus the price of electricity of a future fusion power plant depends on it, one of the most important goals is to understand, predict and influence this “turbulent transport”.</p>
<p>Since the exact computational description of plasma turbulence would require the solution of highly complex systems of equations and the execution of countless computational steps, the code development process is aimed at achieving reasonable simplifications. The GENE code developed at IPP is based on a set of simplified, so-called gyrokinetic equations. They disregard all phenomena in the plasma which do not play a major role in turbulent transport. Although the computational effort can be reduced by many orders of magnitude in this way, the world’s fastest and most powerful supercomputers have always been needed to further develop the code. In the meantime, GENE is able to describe the formation and propagation of small low-frequency plasma eddies in the plasma interior well and to reproduce and explain the experimental results – but originally only for the simply constructed, because axisymmetric fusion devices of the tokamak type.</p>
<p>For example, calculations with GENE showed that fast ions can greatly reduce turbulent transport in tokamak plasmas. Experiments at the ASDEX Upgrade tokamak at Garching confirmed this result. The required fast ions were provided by plasma heating using radio waves of the ion cyclotron frequency.</p>






<p><strong>A tokamak code for stellarators</strong><br>In stellarators, this turbulence suppression by fast ions had not been observed experimentally so far. However, the latest calculations with GENE now suggest that this effect should also exist in stellarator plasmas: In the Wendelstein 7-X stellarator at IPP at Greifswald, it could theoretically reduce turbulence by more than half. As IPP scientists Alessandro Di Siena, Alejandro Bañón Navarro and Frank Jenko show in the journal Physical Review Letters, the optimal ion temperature depends strongly on the shape of the magnetic field. Professor Frank Jenko, head of the Tokamak Theory department at IPP in Garching: “If this calculated result is confirmed in future experiments with Wendelstein 7-X in Greifswald, this could open up a path to interesting high-performance plasmas”.</p>
<p>In order to use GENE for turbulence calculation in the more complicated shaped plasmas of stellarators, major code adjustments were necessary. Without the axial symmetry of the tokamaks, one has to cope with a much more complex geometry for stellarators.</p>
<p>For Professor Per Helander, head of the Stellarator Theory department at IPP in Greifswald, the stellarator simulations performed with GENE are “very exciting physics”. He hopes that the results can be verified in the Wendelstein 7-X stellarator at Greifswald. “Whether the plasma values in Wendelstein 7-X are suitable for such experiments can be investigated when, in the coming experimental period, the radio wave heating system will be put into operation in addition to the current microwave and particle heating,” says Professor Robert Wolf, whose department is responsible for plasma heating.</p>


<figure data-description="Turbulence in the plasma of the Wendelstein 7-X stellarator, calculated with the GENE-3D code: The magnetic cage generated by the magnetic coils (grey) shapes and encloses the plasma. The turbulent variation of the plasma density is to be seen in the plasma cross-section." data-picture="base64;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">
      
    

    
    <figcaption>
        <p>
          Turbulence in the plasma of the Wendelstein 7-X stellarator, calculated with the GENE-3D code: The magnetic cage generated by the magnetic coils (grey) shapes and encloses the plasma. The turbulent variation of the plasma density is to be seen in the plasma cross-section.
        </p>
        <p>
           Illustration: IPP, A. Bañón Navarro
        </p>
    </figcaption>
</figure>



<div><p><strong>GENE becomes GENE-3D</strong><br>According to Frank Jenko, it was another “enormous step” to make GENE not only approximately, but completely fit for the complex, three-dimensional shape of stellarators. After almost five years of development work, the code GENE-3D, now presented in the “Journal of Computational Physics” by Maurice Maurer and co-authors, provides a “fast and yet realistic turbulence calculation also for stellarators”, says Frank Jenko. In contrast to other stellarator turbulence codes, GENE-3D describes the full dynamics of the system, i.e. the turbulent motion of the ions and also of the electrons over the entire inner volume of the plasma, including the resulting fluctuations of the magnetic field.</p></div>
<p>Isabella Milch</p>


<p><strong>Publications</strong></p>

<div>
  <p>
  1.
</p>

  

  <div>
    
    <p>Turbulence suppression by energetic particle effects in modern optimized stellarators</p>
    <p>Physical Review Letters, 2020</p>
  </div>
</div>


<div>
  <p>
  2.
</p>

  

  <div>
    <p>M. Maurer et al.</p>
    <p>GENE-3D – A global gyrokinetic turbulence code for stellarators</p>
    <p>Journal of Computational Physics, 2020</p>
  </div>
</div>

  
</div></div>]]>
            </description>
            <link>https://www.ipp.mpg.de/4928395/05_20</link>
            <guid isPermaLink="false">hacker-news-small-sites-24619682</guid>
            <pubDate>Mon, 28 Sep 2020 18:17:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Brief History of Neural Nets and Deep Learning]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 3 (<a href="https://news.ycombinator.com/item?id=24619560">thread link</a>) | @andreyk
<br/>
September 28, 2020 | https://www.skynettoday.com/overviews/neural-net-history | <a href="https://web.archive.org/web/*/https://www.skynettoday.com/overviews/neural-net-history">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <h2 id="prologue-the-deep-learning-tsunami">Prologue: The Deep Learning Tsunami</h2>

<blockquote>
  <p>“Deep Learning waves have lapped at the shores of computational linguistics for several years now, but 2015 seems like the year when the full force of the tsunami hit the major Natural Language Processing (NLP) conferences.” -<a href="http://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00239">Dr. Christopher D. Manning, Dec 2015</a> <sup id="fnref:part1_1" role="doc-noteref"><a href="#fn:part1_1">1</a></sup></p>
</blockquote>

<p>This may sound hyperbolic - to say the established methods of an entire field of research are quickly being superseded by a new discovery, as if hit by a research ‘tsunami’. But, this catastrophic language is appropriate for describing the meteoric rise of Deep Learning over the last several years - a rise characterized by drastic improvements over reigning approaches towards the hardest problems in AI, massive investments from industry giants such as Google, and exponential growth in research publications (and Machine Learning graduate students). Having taken several classes on Machine Learning, and even used it in undergraduate research, I could not help but wonder if this new ‘Deep Learning’ was anything fancy or just a scaled up version of the ‘artificial neural nets’ that were already developed by the late 80s. And let me tell you, the answer is quite a story - the story of not just neural nets, not just of a sequence of research breakthroughs that make Deep Learning somewhat more interesting than ‘big neural nets’  (that I will attempt to explain in a way that just about anyone can understand), but most of all of how several unyielding researchers made it through dark decades of banishment to finally redeem neural nets and achieve the dream of Deep Learning.</p>


<blockquote><p id="sources">
I am certainly not a foremost expert on this topic. In depth technical overviews with long lists of references written by those who actually made the field what it is include Yoshua Bengio's <a href="http://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf">"Learning Deep Architectures for AI"</a>, Jürgen Schmidhuber's <a href="http://arxiv.org/pdf/1404.7828v4.pdf">"Deep Learning in Neural Networks: An Overview"</a> and LeCun et al.s' <a href="http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf">"Deep learning"</a>. In particular, this is mostly a history of research in the US/Canada AI community, and even there will not mention many researchers; a particularly in depth history of the field that covers these omissions is Jürgen Schmidhuber's <a href="http://people.idsia.ch/~juergen/deep-learning-overview.html">"Deep Learning in Neural Networks: An Overview"</a>. I am also most certainly not a professional writer, and will cop to there being shorter and much less technical overviews written by professional writers such as Paul Voosen's <a href="http://chronicle.com/article/The-Believers/190147">"The Believers"</a>, John Markoff's <a href="http://www.nytimes.com/2012/11/24/science/scientists-see-advances-in-deep-learning-a-part-of-artificial-intelligence.html">"Scientists See Promise in Deep-Learning Programs"</a> and Gary Marcus's <a href="http://www.newyorker.com/news/news-desk/is-deep-learning-a-revolution-in-artificial-intelligence">"Is “Deep Learning” a Revolution in Artificial Intelligence?"</a>. I also will stay away from getting too technical here, but there is a plethora of tutorials on the internet on all the major topics covered in brief by me.
<br>
Any corrections would be appreciated, though I will note some ommisions are intentional since I want to try and keep this 'brief' and a good mix of simple technical explanations and storytelling. 
<br>
This piece is an updated and expanded version of blog posts originally released in 2015 on www.andreykurenkov.com. 
</p></blockquote>

<ul id="markdown-toc">
  <li><a href="#prologue-the-deep-learning-tsunami" id="markdown-toc-prologue-the-deep-learning-tsunami">Prologue: The Deep Learning Tsunami</a></li>
  <li><a href="#part-1-the-beginnings-1950s-1980s" id="markdown-toc-part-1-the-beginnings-1950s-1980s">Part 1: The Beginnings (1950s-1980s)</a>    <ul>
      <li><a href="#the-centuries-old-machine-learning-algorithm" id="markdown-toc-the-centuries-old-machine-learning-algorithm">The Centuries Old Machine Learning Algorithm</a></li>
      <li><a href="#the-folly-of-false-promises" id="markdown-toc-the-folly-of-false-promises">The Folly of False Promises</a></li>
      <li><a href="#the-thaw-of-the-ai-winter" id="markdown-toc-the-thaw-of-the-ai-winter">The Thaw of the AI Winter</a></li>
    </ul>
  </li>
  <li><a href="#part-2-neural-nets-blossom-1980s-2000s" id="markdown-toc-part-2-neural-nets-blossom-1980s-2000s">Part 2: Neural Nets Blossom (1980s-2000s)</a>    <ul>
      <li><a href="#neural-nets-gain-vision" id="markdown-toc-neural-nets-gain-vision">Neural Nets Gain Vision</a></li>
      <li><a href="#neural-nets-go-unsupervised" id="markdown-toc-neural-nets-go-unsupervised">Neural Nets Go Unsupervised</a></li>
      <li><a href="#neural-nets-gain-beliefs" id="markdown-toc-neural-nets-gain-beliefs">Neural Nets Gain Beliefs</a></li>
      <li><a href="#neural-nets-make-decisions" id="markdown-toc-neural-nets-make-decisions">Neural Nets Make Decisions</a></li>
      <li><a href="#neural-nets-get-loopy" id="markdown-toc-neural-nets-get-loopy">Neural Nets Get Loopy</a></li>
      <li><a href="#neural-nets-start-to-speak" id="markdown-toc-neural-nets-start-to-speak">Neural Nets Start to Speak</a></li>
      <li><a href="#a-new-winter-dawns" id="markdown-toc-a-new-winter-dawns">A New Winter Dawns</a></li>
    </ul>
  </li>
  <li><a href="#part-3-deep-learning-2000s-2010s" id="markdown-toc-part-3-deep-learning-2000s-2010s">Part 3: Deep Learning (2000s-2010s)</a>    <ul>
      <li><a href="#the-funding-of-more-layers" id="markdown-toc-the-funding-of-more-layers">The Funding of More Layers</a></li>
      <li><a href="#the-development-of-big-data" id="markdown-toc-the-development-of-big-data">The Development of Big Data</a></li>
      <li><a href="#the-importance-of-brute-force" id="markdown-toc-the-importance-of-brute-force">The Importance of Brute Force</a></li>
      <li><a href="#the-deep-learning-equation" id="markdown-toc-the-deep-learning-equation">The Deep Learning Equation</a></li>
    </ul>
  </li>
  <li><a href="#epilogue-the-decade-of-deep-learning" id="markdown-toc-epilogue-the-decade-of-deep-learning">Epilogue: The Decade of Deep Learning</a></li>
</ul>


<p>The beginning of a story spanning half a century, about how we learned to make computers learn. In this part, we shall cover the birth of neural nets with the Perceptron in 1958, the AI Winter of the 70s, and neural nets’ return to popularity with backpropagation in 1986.</p>

<h2 id="the-centuries-old-machine-learning-algorithm">The Centuries Old Machine Learning Algorithm</h2>
<figure>
    <img src="https://www.skynettoday.com/assets/img/overviews/2020-09-27-a-brief-history-of-neural-nets-and-deep-learning/Linear_regression.svg" alt="Linear Regression">     
    <figcaption>Linear regression <a href="https://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg">(Source)</a></figcaption>
</figure>

<p>Let’s start with a brief primer on what Machine Learning is. Take some points on a 2D graph, and draw a line that fits them as well as possible. What you have just done is generalized from a few example of pairs of input values (x) and output values (y) to a general function that can map any input value to an output value. This is known as linear regression, and it is a wonderful little <a href="https://en.wikipedia.org/wiki/Linear_regression#cite_note-4">200 year old</a> technique for extrapolating a general function from some set of input-output pairs. And here’s why having such a technique is wonderful: there is an incalculable number of functions that are hard to develop equations for directly, but are easy to collect examples of input and output pairs for in the real world - for instance, the function mapping an input of recorded audio of a spoken word to an output of what that spoken word is.</p>

<p>Linear regression is a bit too wimpy a technique to solve the problem of speech recognition, but what it does is essentially what <strong>supervised Machine Learning</strong> is all about: ‘learning’ a function given a <strong>training set</strong> of <strong>examples</strong>, where each example is a pair of an input and output from the function (we shall touch on the unsupervised flavor in a little while). In particular, machine learning methods should derive a function that can generalize well to inputs not in the training set, since then we can actually apply it to inputs for which we do not have an output. For instance, Google’s current speech recognition technology is powered by Machine Learning with a massive training set, but not nearly as big a training set as all the possible speech inputs you might task your phone with understanding.</p>

<p>This generalization principle is so important that there is almost always a <strong>test set</strong> of data (more examples of inputs and outputs) that is not part of the training set. The separate set can be used to evaluate the effectiveness of the machine learning technique by seeing how many of the examples the method correctly computes outputs for given the inputs. The nemesis of generalization is <strong>overfitting</strong> - learning a function that works really well for the training set but badly on the test set. Since machine learning researchers needed means to compare the effectiveness of their methods, over time there appeared standard <strong>datasets</strong> of training and testing sets that could be used to evaluate machine learning algorithms.</p>

<p>Okay okay, enough definitions. Point is - our line drawing exercise is a very simple example of supervised machine learning: the points are the training set (X is input and Y is output), the line is the approximated function, and we can use the line to find Y values for X values that don’t match any of the points we started with. Don’t worry, the rest of this history will not be nearly so dry as all this. Here we go.</p>

<h2 id="the-folly-of-false-promises">The Folly of False Promises</h2>

<p>Why have all this prologue with linear regression, since the topic here is ostensibly neural nets? Well, in fact linear regression bears some resemblance to the first idea conceived specifically as a method to make machines learn: <a href="http://psycnet.apa.org/index.cfm?fa=buy.optionToBuy&amp;id=1959-09865-001">Frank Rosenblatt’s <strong>Perceptron</strong></a><sup id="fnref:part1_2" role="doc-noteref"><a href="#fn:part1_2">2</a></sup>.</p>
<figure>
    <img src="https://www.skynettoday.com/assets/img/overviews/2020-09-27-a-brief-history-of-neural-nets-and-deep-learning/34998.jpg" alt="Perceptron">
    <figcaption>A diagram showing how the Perceptron works. <a href="http://cse-wiki.unl.edu/wiki/images/0/0f/Perceptron.jpg">(Source)</a></figcaption>    
</figure>

<p>A psychologist, Rosenblatt conceived of the Percetron as a simplified mathematical model of how the neurons in our brains operate: it takes a set of binary inputs (nearby neurons), multiplies each input by a continuous valued weight (the synapse strength to each nearby neuron), and thresholds the sum of these weighted inputs to output a 1 if the sum is big enough and otherwise a 0 (in the same way neurons either fire or do not). Most of the inputs to a Perceptron are either some data or the output of another Perceptron, but an extra detail is that Perceptrons also have one special ‘bias’ input, which just has a value of 1 and basically ensures that more functions are computable with the same input by being able to offset the summed value. This model of the neuron built on the work of Warren McCulloch and Walter Pitts <a href="http://www.minicomplexity.org/pubs/1943-mcculloch-pitts-bmb.pdf">Mcculoch-Pitts</a><sup id="fnref:part1_3" role="doc-noteref"><a href="#fn:part1_3">3</a></sup>, who showed that a neuron model that sums binary inputs and outputs a 1 if the sum exceeds a certain threshold value, and otherwise outputs a 0, can model the basic OR/AND/NOT functions. This, in the early days of AI, was a big deal - the predominant thought at the time was that making computers able to perform formal logical reasoning would essentially solve AI.</p>

<figure>
    <img src="https://www.skynettoday.com/assets/img/overviews/2020-09-27-a-brief-history-of-neural-nets-and-deep-learning/34832.jpg" alt="Perceptron 2"> 
    <figcaption>Another diagram, showing the biological inspiration. The <b>activation function</b> is what people now call the non-linear function applied to the weighted input sum to produce the output of the artificial neuron - in the case of Rosenblatt's Perceptron, the function just a thresholding operation.  <a href="http://cs231n.github.io/neural-networks-1/">(Source)</a> </figcaption>    
</figure>

<p>However, the Mcculoch-Pitts model lacked a mechanism for learning, which was crucial for it to be usable for AI. This is where the Perceptron excelled - Rosenblatt came up with a way to make such artificial neurons learn, inspired by the <a href="http://onlinelibrary.wiley.com/doi/10.1002/cne.900930310/abstract">foundational work</a><sup id="fnref:part1_4" role="doc-noteref"><a href="#fn:part1_4">4</a></sup> of Donald Hebb. Hebb put forth the unexpected and hugely influential idea that knowledge and learning occurs in the brain primarily through the formation and change of synapses between neurons - concisely stated as Hebb’s Rule:</p>

<blockquote>
  <p>“When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased.”</p>
</blockquote>

<p>The Perceptron did not follow this idea exactly, but having weights on the inputs allowed for a very simple and intuitive learning scheme: given a <strong>training set</strong> of input-output examples the Perceptron should ‘learn’ a function from, for each example increase the weights if the Perceptron output for that example’s input is too low compared to the example, and otherwise decrease the weights if the output is too high. Stated ever so slightly more formally, the algorithm is as follows:</p>

<ol>
  <li>Start off with a Perceptron having random weights and a training set</li>
  <li>For the inputs of an example in the …</li></ol></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.skynettoday.com/overviews/neural-net-history">https://www.skynettoday.com/overviews/neural-net-history</a></em></p>]]>
            </description>
            <link>https://www.skynettoday.com/overviews/neural-net-history</link>
            <guid isPermaLink="false">hacker-news-small-sites-24619560</guid>
            <pubDate>Mon, 28 Sep 2020 18:08:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I No Longer Hate America]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24618682">thread link</a>) | @djsumdog
<br/>
September 28, 2020 | https://battlepenguin.com/politics/why-i-no-longer-hate-america/ | <a href="https://web.archive.org/web/*/https://battlepenguin.com/politics/why-i-no-longer-hate-america/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <section>

  

  <article>
    

    <figure>
  
  <img src="https://battlepenguin.com/images/politics/america/us-flag.jpg" alt="Red, White &amp; Blue by Luke Michael">
  
  
  <figcaption>
      
         <a href="https://instagram.com/farcivilian">
      

      Red, White &amp; Blue by Luke Michael

      
         </a>
      
  </figcaption>
  
</figure>

<p>Years ago, a friend told me he was going to visit his sister in Queensland, Australia. I was <a href="https://battlepenguin.com/tech/tech-culture-shock-from-america-to-the-south-pacific-and-seattle-to-chicago/#wellington-new-zealand">living in New Zealand</a> at the time, and we made plans to meet up. We ended up <a href="https://battlepenguin.com/philosophy/perspective/exploration/">hiking</a> through some <a href="https://journeyofkhan.us/photo/rock-face-at-alligator-falls/">spectacular parks</a> and <a href="https://journeyofkhan.us/photo/base-of-alligator-falls/">amazing waterfalls</a>. On one of our hikes, my friend asked me, “Why do you hate America?” It’s one of those lines often parodied from early 2000s Fox News broadcasts, during the height of outrage over the Iraq war.</p>

<p>At the time, I did hate a lot of American foreign policy, and found there <a href="https://battlepenguin.com/politics/video/does-voting-make-a-difference/">was little difference between US political parties</a>. I had my reasons for leaving, yet I eventually came back to America. Having traveled through and lived in different places around the world, I saw the trade-offs in the different values held by nationals of various countries. I still take issue with many policies in the United States, but I believe there are many aspects to American law and ideology, that respect individual rights and freedoms in fundamentally unique ways. America may run afoul of letting people slide through the cracks, but it is also a nation suited to help people succeed at their dreams.</p>

<!--more-->

<p>My father had a front vanity plate on his car, displaying an American Flag, with a Bald Eagle and the words <em>In God We Trust.</em> His family survived the partition after India gained independence from the British Raj. My grandfather almost lost his life to three men who wanted to take the house he was squatting. Both he and my mother’s family fled war, and struggled against poverty. In a Sociology class I took in my undergraduate years, I had a professor who told us that climbing up the economic ladder to a different social class is not very common. George Carlin famously said, <em>“That’s why they call it the American Dream, because you have to be asleep to believe it.<sup id="fnref:1"><a href="#fn:1">1</a></sup>“</em>. I internalized that message as reality, while ignoring that my father, perhaps against all odds, had literally achieved the American Dream.</p>

<h3 id="problems">Problems</h3>

<figure>
  
  <img src="https://battlepenguin.com/images/politics/america/doctor-sign.jpg" alt="Sign stating: Please Be Advised That Staff Are Not Able To Discuss Lab Results &amp; Other Reports Over The Phone. You Will Be Notified Only If There are Significant Abnormal Findings. Test Results Are Usually Available Within 3-5 Days. Thank You. Clayton Park Medical Clinic">
  
  
</figure>

<p>A Canadian friend sent me an image from her doctor’s office. I thought she was complaining about the notice that test results wouldn’t be discussed over the phone, but she was actually concerned that results would take several days, even for tests that were done in a doctors office and available immediately in other countries. I worked for a health insurance company <a href="https://battlepenguin.com/politics/universalhealthcare/">during the 2008 health care debates</a>, and took issue with the sad state of the <a href="https://battlepenguin.com/politics/returning-to-america-and-the-unaffordable-care-act/">Affordable Care Act</a> upon my return to the US. Canada’s national health system does work fairly well for most common conditions, but surgeries can have long wait times. A number of Canadians, who can afford it, will travel to the United States for medical procedures<sup id="fnref:9"><a href="#fn:9">2</a></sup>. Canadians do pay less for many prescription drugs, but that may change with Trump’s recent executive orders around pharmaceutical pricing<sup id="fnref:10"><a href="#fn:10">3</a></sup>.</p>

<p>In Australia, voting is ranked with instant runoff. Citizens put down numbers indicating their preferences in order, making it impossible to throw a vote away, and voting is mandatory. New Zealand uses Mixed-member Proportional Representation (MMP) allowing each voter two votes. This system is somewhat complicated, but it basically allows for a representative distribution of parliament seats based upon the proportion of votes given to each political party<sup id="fnref:11"><a href="#fn:11">4</a></sup>. America has a first past the post system, and while this system falls behind in proportional representation of the population, it attempts to favor representation of individual states in blocks by means of a complex and controversial electoral college system.</p>

<p>While I was living in New Zealand, one of my friends was wrestled to the ground unjustly by a police officer. When he went to file a complaint, the officer’s partner attempted to intimidate him. While I was in Croatia, I was illegally searched by a police officer at a bus station. <a href="https://battlepenguin.com/politics/why-i-no-longer-hate-the-police/">Police misconduct</a> is on the forefront of news and politics in America today, but the US is not exceptional when it comes to police corruption. It can be found in police forces around the world, but so can good, honest constables who simply want to do the right thing.</p>

<p>I could go on comparing Native Americas and Aboriginals, transportation, drug policy or any number of areas where America might either excel or fall short. America has a lot of problems, but it’s also the third largest country by population. The second largest is where my family is from. India has a functioning democracy, with an impoverished population. China, the largest country by population, is ruled by a Communist government that arrests dissidents and places members of their ethnic minority in reeducation camps. Every nation has problems, and every country’s population lives with the trade-offs that come from complex socioeconomic and geopolitical policies.</p>

<h3 id="the-freedom-to-speak">The Freedom to Speak</h3>

<p>On March 15th, 2019, an Australian man shot up two mosques in Christchurch, New Zealand<sup id="fnref:2"><a href="#fn:2">5</a></sup>. He live-streamed the atrocity. Although the video was taken down from Facebook, there were copies people had archived. Possession of the video, as well as the shooter’s manifesto, are illegal in New Zealand without special permission from the government. The video is horrific, and in the United States, it would likely be removed from many platforms. However, it wouldn’t be illegal to possess or distribute. In New Zealand, having a copy of the video or manifesto can carry sentences of up to 14 years, and several people were arrested and prosecuted for possession of the material<sup id="fnref:3"><a href="#fn:3">6</a></sup>.</p>

<p>The majority of my friends in New Zealand, fully support this form of government censorship<sup id="fnref:4"><a href="#fn:4">7</a></sup>. Even Americans I knew who had immigrated, told me they fully understood and accepted this limitation when they became citizens. If this level of censorship happened in the US, citizens would likely be outraged. From the time we are young, we are taught that <em>freedom of speech</em> is an essential part of our civil liberties, and our identities as Americans. It’s the first amendment made to the United States Constitution; the first in our <em>Bill of Rights</em>.</p>

<p>New Zealand doesn’t have a constitution, and Australia does not have a bill of rights. Victoria, Australia passed a Charter of Human Rights in 2006, but its provisions are limited to the state of Victoria<sup id="fnref:5"><a href="#fn:5">8</a></sup>. China has the freedom of speech written into their constitution<sup id="fnref:6"><a href="#fn:6">9</a></sup>, which is a complete and total joke as their only political party exerts Orwellian style censorship over all 1.5 billion Chinese people.</p>

<blockquote>
  <p><em>“Congress shall make no law respecting an establishment of religion, or prohibiting the free exercise thereof; or abridging the freedom of speech, or of the press; or the right of the people peaceably to assemble, and to petition the government for a redress of grievances.”</em> -Amendment I, Constitution of the United States</p>
</blockquote>

<p>Mike Ward is a French Canadian comedian who once made a joke about a child who was chronically ill, and was fined $80,000 CAD by the Quebec’s Human Rights Tribunal for his joke<sup id="fnref:13"><a href="#fn:13">10</a></sup>. The Scottish comedian Mark Meechan trained his dog to raise his paw when he heard the phrase “gas the Jews” for a video he used on a comedy routine. In 2018, Ward was fined £800 in the United Kingdom for violating a communication act for offensive speech. In many nations, jokes can result in fines or prison.</p>

<p>The freedom to speak does not guarantee the freedom to be heard. You can ignore those you disagree with. If you run a forum or social media platform in America, and you have a user who says something you disagree with, you are well within your rights to <a href="https://battlepenguin.com/politics/twitter-is-trying-to-erase-the-past/">delete their content</a> or ban their account. Except for a few states, employers are free to fire people based on their political views as well. The goal of the First Amendment is to protect individuals against persecution from their government. It prevents individuals from being arrested for voicing their opinions. Contrary to what Chris Cuomo says on CNN, the constitution does require that those who assemble must do so peaceably<sup id="fnref:16"><a href="#fn:16">11</a></sup>.</p>

<p>Very few high income nations have true freedom of speech, to the extent we have in the United States. There are limits to that freedom, of course. Although you may advocate for the use of force, immediate and immanent threats of directed violence are typically not protected. Obscenities, images of child abuse, direct threats to individuals, and lying materially to officers of the Federal Government are also not protected by the freedom of speech.</p>

<p>Still, Americas laws and protections are far that above other developed nations. The ability to speak ones mind, without fear of prosecution from the government, is a civil liberty vital to ensuring a free State and protecting against authoritarianism. Although the debate about this basic freedom may be more contentious in the age of <a href="https://battlepenguin.com/philosophy/society/how-social-media-destroyed-my-generation/">social media</a>, the America I grew up in is one where we do not fear ideas.</p>

<h3 id="the-pandemic">The Pandemic</h3>

<p>In Melbourne, Australia, a woman was choked by a police officer and arrested for not wearing a mask. She had a medical exemption, but she probably should have just said so instead of flipping off the police officer<sup id="fnref:17"><a href="#fn:17">12</a></sup>. Another woman in Melbourne was arrested for a Facebook post promoting a lockdown protest<sup id="fnref:12"><a href="#fn:12">13</a></sup>, which shows the limitations of Victoria’s Charter of Human Rights<sup id="fnref:5:1"><a href="#fn:5">8</a></sup>. When I bought this up with friends I knew in Melbourne, all of them supported the lockdown measures<sup id="fnref:20"><a href="#fn:20">14</a></sup>.</p>

<p>From the beginning of the pandemic, I was <a href="https://battlepenguin.com/philosophy/covid-19-is-two-diseases/">concerned about overreactions</a>. Many of those concerns <a href="https://battlepenguin.com/politics/secondary-effects/">turned out to sadly be true</a>. In late August, nearly 40,000 people in Germany<sup id="fnref:14"><a href="#fn:14">15</a></sup> and thousands of people in London<sup id="fnref:15"><a href="#fn:15">16</a></sup>, gathered in protests against the lockdowns. Just like <a href="https://battlepenguin.com/politics/this-is-not-a-time-of-honor/#protest">similar protests in the United States</a>, these groups are often ridiculed by the main stream media, but at least they have the right to gather. In Australia, BLM protests were ruled unlawful in Sydney. While I agree with Sky News’s evaluation of the protests being for a “Marxist movement<sup id="fnref:18"><a href="#fn:18">17</a></sup>,” such …</p></article></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://battlepenguin.com/politics/why-i-no-longer-hate-america/">https://battlepenguin.com/politics/why-i-no-longer-hate-america/</a></em></p>]]>
            </description>
            <link>https://battlepenguin.com/politics/why-i-no-longer-hate-america/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24618682</guid>
            <pubDate>Mon, 28 Sep 2020 16:53:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[All DuckDuckGo bang operators on one page]]>
            </title>
            <description>
<![CDATA[
Score 341 | Comments 116 (<a href="https://news.ycombinator.com/item?id=24618447">thread link</a>) | @MichaelMoser123
<br/>
September 28, 2020 | https://mosermichael.github.io/duckduckbang/html/main.html | <a href="https://web.archive.org/web/*/https://mosermichael.github.io/duckduckbang/html/main.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://mosermichael.github.io/duckduckbang/html/main.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24618447</guid>
            <pubDate>Mon, 28 Sep 2020 16:34:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Coding for Kids Is Not Just Another Fad]]>
            </title>
            <description>
<![CDATA[
Score 35 | Comments 62 (<a href="https://news.ycombinator.com/item?id=24618292">thread link</a>) | @teragoodness
<br/>
September 28, 2020 | https://livetechnoid.com/8-reasons-coding-for-kids-is-not-just-another-fad/ | <a href="https://web.archive.org/web/*/https://livetechnoid.com/8-reasons-coding-for-kids-is-not-just-another-fad/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-22192"><div><div itemprop="text"><p><img src="https://i2.wp.com/livetechnoid.com/wp-content/uploads/2020/09/Coding-for-Kids.png?resize=560%2C315&amp;ssl=1" alt="8 Reasons Coding for Kids is Not Just Another Fad" width="560" height="315" srcset="https://i2.wp.com/livetechnoid.com/wp-content/uploads/2020/09/Coding-for-Kids.png?w=560&amp;ssl=1 560w, https://i2.wp.com/livetechnoid.com/wp-content/uploads/2020/09/Coding-for-Kids.png?resize=300%2C169&amp;ssl=1 300w" sizes="(max-width: 560px) 100vw, 560px" data-recalc-dims="1" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20560%20315'%3E%3C/svg%3E" data-lazy-srcset="https://i2.wp.com/livetechnoid.com/wp-content/uploads/2020/09/Coding-for-Kids.png?w=560&amp;ssl=1 560w, https://i2.wp.com/livetechnoid.com/wp-content/uploads/2020/09/Coding-for-Kids.png?resize=300%2C169&amp;ssl=1 300w" data-lazy-src="https://i2.wp.com/livetechnoid.com/wp-content/uploads/2020/09/Coding-for-Kids.png?resize=560%2C315&amp;ssl=1"></p><h2><span id="Is_Coding_a_Fad">Is Coding a Fad?</span></h2><p>In short, no. Coding is a field that is growing and changing quickly; there is a world of opportunity for young students looking to get involved in the tech world. Opponents to this position might argue that today’s popular languages, like Python, won’t be used ten or fifteen years down the line. So what’s the use in learning these programming languages?</p><h2><span id="8_Reasons_Coding_for_Kids_is_Not_Just_Another_Fad">8 Reasons Coding for Kids is Not Just Another Fad</span></h2><p>While it is true that programming will continue to advance, it is important to remember that there is so much more to learning a coding language like Python than memorizing syntax and applications. When we teach coding, we teach concepts beyond one particular language. Students gain knowledge about how computers process and “think” about a given problem, regardless of which language they are learning. They gain important intuition about fundamental concepts, like basic control structures (e.g. loops and conditional statements) and algorithms, that will undoubtedly be useful in our highly technological future. Below are 8 of the many reasons why coding for kids is not just another trendy or silly fad.</p><p><em><strong>Also, See:</strong></em></p><ul><li><a href="https://livetechnoid.com/how-to-encourage-stem-in-early-education/" rel="noopener follow" data-wpel-link="internal" target="_self"><em><strong>How to Encourage STEM in Early Education</strong></em></a></li><li><em><strong><a href="https://livetechnoid.com/beginner-python-draw-a-harry-potter-symbol/" rel="noopener follow" data-wpel-link="internal" target="_self">Beginner Python: Draw a Harry Potter Symbol</a></strong></em></li><li><em><strong><a href="https://livetechnoid.com/5-reasons-why-having-multiple-mentors-prepares-kids-for-the-future/" rel="noopener follow" data-wpel-link="internal" target="_self">5 Reasons Why Having Multiple Mentors Prepares Kids for the Future</a></strong></em></li></ul><h3><span id="Our_world_is_growing_more_technological">Our world is growing more technological</span></h3><p>In the 21st century, we have seen enormous developments in the technology industry. The world is moving in a direction that is both fast-paced and information-centred. Data is quickly becoming one of the most valuable resources in the global economy, and individuals with digital literacy are becoming more valued in the labour market. With all of this in mind, it’s hard to ignore the massive role that fields like data science and software engineering, both largely centred around coding, are going to have. Figures estimate seemingly outrageous numbers – <a href="https://hostingtribunal.com/blog/how-fast-is-technology-growing/" data-wpel-link="external" target="_blank" rel="nofollow external noopener">an estimated fifty billion smart devices are projected to be in use by 2020</a> – illustrating the abundance of technology in our lives.</p><p>With such statistics in our near futures, it is clear to see that industries are changing. The internet is a hub of information, applications, e-commerce, social networking, and communication platforms, all of which are designed to increase efficiency, performance, and convenience. As industries become digitized, new jobs are simultaneously emerging. Coding is an asset that will only continue to grow as we move forward as an increasingly connected society.</p><h3><span id="It_teaches_kids_to_strategize_for_unique_solutions">It teaches kids to strategize for unique solutions</span></h3><p>Computer science is a highly useful tool beyond its direct applications. As there is rarely one right answer to a given problem, computer science encourages students to create solutions that are unique. For computer scientists, this means that coders have a great amount of leeway to create their own intuitive solutions.</p><p>Teaching kids how to code <a href="https://news.harvard.edu/gazette/story/2014/11/coding-and-creativity/" data-wpel-link="external" target="_blank" rel="nofollow external noopener">pushes young students to actually learn</a>, not just memorize facts and regurgitate them. It challenges students to critically think about a problem, what they want to accomplish, and how to get it done. There are plenty of kid-friendly coding languages, such as <a href="https://junilearning.com/blog/scratch-beginning-coding-language-for-kids" data-wpel-link="external" target="_blank" rel="nofollow external noopener">Scratch</a>, that give young students the opportunity to build unique projects using coding techniques. Scratch also allows coders to share their projects with the world. This connectivity can inspire new and aspiring coders to learn from their peers and guide them through more complicated projects and concepts. This collaborative environment also allows students to work together and learn from one another. Encouraging kids to utilize these programs not only teaches them useful skills for computer science, but it also allows them to exercise their creativity.</p><h3><span id="It_teaches_students_to_not_give_up">It teaches students to not give up</span></h3><p>Most coders will advise, while there is plenty of room for creativity in coding, there is also plenty of room for error. Learning to code teaches students how not to give up on their solutions, but rather to debug their code, even when repeated error messages appear. Coders tease out solutions by using their understanding of computational thinking (see point 6) to isolate any errors and achieve the goal at hand.</p><p>That code can always be improved upon motivates coding students to go back and better their solutions, an incredibly valuable skill set to have in general. Code also instils resilience, yet another invaluable tool in the workplace. While programming can feel tedious, frustrating, and difficult at times, the process of problem-solving inherently makes for better coders and better students.</p><h3><span id="It_can_be_applied_to_almost_everything_that_students_are_learning">It can be applied to almost everything that students are learning</span></h3><p>Computer science is expanding into every corner of the world, with applications for anything one can imagine and automated services that are permeating thousands of business models worldwide. On top of that, programming incorporates many mathematical concepts, and it can complement the concepts students are learning in their math classes.</p><p>Coding also <a href="https://blog.mindresearch.org/blog/coding-mathematical-practices" data-wpel-link="external" target="_blank" rel="nofollow external noopener">promotes problem-solving</a>, an important skill for any student. Beyond math, students can use code to support other interests they may have. With coding, young professionals’ career options extend across industries. By teaching kids how to code, students will find the confidence and tools to explore aspects of software engineering, data analysis, video game development, and mobile app development – in just about any industry that they know and love.</p><h3><span id="It_is_challenging_and_collaborative">It is challenging and collaborative</span></h3><p>Coding challenges young students to explore new fields. Computer science is a collaborative field, allowing individuals to work together to complement one another’s skill sets, and write code that is efficient and intuitive. Students can and often do learn to code in a group setting, so that they can discuss their ideas and learn from each other’s successes. Learning to code encourages students to work together when they reach a roadblock. It promotes collaboration via idea exchange and allows students’ to compare and contrast solutions. Teaching kids about computer programming can be a catalyst for inspiring teamwork and leadership among students.</p><h3><span id="It_teaches_students_about_computational_thinking">It teaches students about computational thinking</span></h3><p>Whichever languages a student decides to pursue – Python, Java, or any of the long list of popular coding languages today – will help them develop computational thinking. This means that students will learn how to effectively break down problems into manageable parts, observe patterns in data, identify how these patterns are generated, and develop the step-by-step instructions for solving those problems.</p><p>Computational thinking and digital literacy are arguably the most important aspect to learning to code at any age due to the huge technological shift in nearly every industry on the global market (see point 1). Establishing a foundation in computational thinking will pave the way for future success in and outside CS related fields. All students can benefit from understanding computational thinking, as it applies to the technological aspects of daily life.</p><h3><span id="Its_good_for_the_brain">It’s good for the brain</span></h3><p>Like any good challenge, coding is an excellent way to strengthen young, developing brains. It encourages students to combine their knowledge of computational learning and out-of-the-box thinking to strategize unique solutions.</p><p><a href="https://medium.com/datadriveninvestor/coding-is-good-for-your-brain-e067063a493e" data-wpel-link="external" target="_blank" rel="nofollow external noopener">Multiple studies</a> support the hypothesis that learning to code has real, long term benefits on young children. Researchers have found that individuals who code tend to have reduced odds of getting neurodegenerative diseases in older age. Coders also tend to do better in cognitive tasks, because coding activates areas of the brain that are associated with memory, attention, and logic. Learning to code at an early age supports neural connections in these regions, leading to high performance in other fields, as well.</p><h3><span id="Its_fun">It’s fun!</span></h3><p>While we may be slightly biased, computer science is an educational way to bring fun into your children’s lives! Computer science is one of the few fields of study in which students can see the immediate outcome of their work as they build out their projects – by running their code, they get instant, live feedback.</p><p>Programs like <a href="https://scratch.mit.edu/" data-wpel-link="external" target="_blank" rel="nofollow external noopener">Scratch</a> and <a href="https://repl.it/" data-wpel-link="external" target="_blank" rel="nofollow external noopener">Repl.it</a> utilize software that is kid-friendly and permits students to visualize their projects as they come to life! They can build code that creates a videogame or a website and share their work with friends. Developing kids’ proficiency with computer science now will lead to a future generation of coders who have the skill set to advance our world.</p><p><em><strong>Also, See:</strong></em></p><ul><li><em><strong><a href="https://livetechnoid.com/5-best-ways-to-learn-internet-marketing/" rel="noopener follow" data-wpel-link="internal" target="_self">5 Best Ways to Learn Internet Marketing</a></strong></em></li><li><em><strong><a href="https://livetechnoid.com/how-to-get-into-a-top-tier-computer-science-program/" rel="noopener follow" data-wpel-link="internal" target="_self">How to Get into a Top-Tier Computer Science Program</a></strong></em></li><li><em><strong><a href="https://livetechnoid.com/computer-maintenance-upgrade/" rel="noopener follow" data-wpel-link="internal" target="_self">Free Tutorial on Computer Maintenance and Upgrade</a></strong></em></li></ul><p><i>This </i><a href="https://junilearning.com/blog/guide/8-reasons-coding-for-kids-not-a-fad/?utm_source=gf&amp;utm_medium=reasons_coding_for_kids_not_another_fad&amp;utm_campaign=outreach" data-wpel-link="external" target="_blank" rel="nofollow external noopener"><i>article</i></a><i> originally appeared on </i><a href="https://junilearning.com/?utm_source=gf&amp;utm_medium=reasons_coding_for_kids_not_another_fad&amp;utm_campaign=outreach" data-wpel-link="external" target="_blank" rel="nofollow external noopener"><i>junilearning.com</i></a><i>&nbsp;</i></p></div></div></article></div>]]>
            </description>
            <link>https://livetechnoid.com/8-reasons-coding-for-kids-is-not-just-another-fad/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24618292</guid>
            <pubDate>Mon, 28 Sep 2020 16:19:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Better Than JSON]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24618121">thread link</a>) | @susam
<br/>
September 28, 2020 | https://wiki.alopex.li/BetterThanJson | <a href="https://web.archive.org/web/*/https://wiki.alopex.li/BetterThanJson">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="wikipage">

<p>I want to take a brief look at various data serialization formats and compare them. Basically the goal is to answer the question, “can we find something better than JSON?” However, note that we are looking at these things for DATA SERIALIZATION, not for config files and stuff, so that’s the goal by which these will be judged.</p>
<p>There’s two orthogonal axes to look at these things under:</p>
<ul>
<li>Self-describing vs.&nbsp;schema-defined formats</li>
<li>Human readable vs.&nbsp;machine-readable formats</li>
</ul>
<p>That is, whether the type information for a structure is defined in a separate file (a schema) that a receiving program checks against, or whether the message itself contains type information. It’s almost exactly the difference between statically and dynamically typed programming languages. Like programming languages, both have pros and cons, neither of them are always better than the other. The goal of this is to compare apples to apples, so we’re gonna note which category these things fall into but not make value judgements based on them. There’s also fuzzy edges; many self-describing formats optionally have a schema layer too. Similarly, we will not really compare tooling quality; the goal is to look at the intrinsic properties of the formats. The culture surrounding them may be considered though.</p>
<p>This is also important not to conflate with an RPC protocol, though many of these things are used IN RPC protocols. Keep in mind that HTTP/REST interfaces are often just a type of RPC protocol, whether realized that way or not.</p>
<p>Up to date as of September 2019. Doesn’t try to include myriad minor things, ’cause there’s only so much time in the world.</p>

<h2 id="json">JSON</h2>
<p><a href="http://json.org/">http://json.org/</a></p>
<p>What everything gets currently compared against. We all know JSON, we all agree it’s Sorta Good Enough but really is kinda crap.</p>
<p><strong>Category:</strong> Human-readable, self-describing. (<a href="https://json-schema.org/">https://json-schema.org/</a> exists but does not seem very widely used.) Has an <a href="https://en.wikipedia.org/wiki/JSON-RPC">RPC protocol</a> but it also seems lightly used, <a href="https://jsonapi.org/">this</a> might be more general.</p>
<p><strong>Users:</strong> Everyone</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Similar to major programming languages – Easy to understand and debug</li>
<li>Simple – Easy to read, write, and understand… at least for simple things. <a href="http://seriot.ch/parsing_json.php">Turns out there’s a lot of gotcha’s though.</a></li>
<li>Pretty compact if minified</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Type system is pretty shit – no date/time, no real integers, no real structs, no unions/tuples/etc</li>
<li>Tends to discourage schema’s – “So simple it doesn’t need it”, until it becomes less simple.</li>
<li>No normalized form – fields may be reordered, <em>duplicated</em>, etc. Makes hashing it hard, gotta read whole message to begin verifying it, etc.</li>
<li>No comments – harder to write well than you might think!</li>
<li>No good way to contain binary data</li>
</ul>
<h2 id="yaml">YAML</h2>
<p><a href="https://yaml.org/">https://yaml.org/</a></p>
<p>Started out as a simpler alternative to XML.</p>
<p><strong>Category:</strong> Human-readable, self-describing.</p>
<p><strong>Users:</strong> Lots of people</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Vaguely simple to read and write, in its basic form</li>
<li>Low visual noise</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Way too complicated – they made it a strict superset of JSON for some damn reason, and nobody uses that form, so it’s just a pile of wasted effort</li>
<li>Reference impl incomplete, other impl’s disagree with each other and the spec</li>
</ul>
<h2 id="xml">XML</h2>
<p><a href="https://en.wikipedia.org/wiki/XML">https://en.wikipedia.org/wiki/XML</a></p>
<p>Not sure anyone really knows how XML happened. It’s basically the W3C’s fault, I think? It’s okay for some things but in the end I’m not sure it’s something anyone actually <em>wants</em> to use, it’s just going to be one more of those mistakes of the past.</p>
<p><strong>Category:</strong> Human-readable, self-describing with common schema usage. Has an <a href="https://en.wikipedia.org/wiki/XML-RPC">RPC protocol</a> and many other complicated things.</p>
<p><strong>Users:</strong> Everyone who can’t avoid it.</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Promotes schemas and validation</li>
<li>Simple to use for simple things</li>
<li>Actually pretty decent for documents</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>I’ve never gotten schemas and validation to actually work in practice</li>
<li>Everything is string-ly typed</li>
<li>No real arrays</li>
<li>Complicated as frig</li>
<li>Very verbose</li>
<li>There’s like 3-4 different ways to do everything</li>
<li>Still no good way to contain binary data</li>
</ul>

<h2 id="protobuf">Protobuf</h2>
<p><a href="https://developers.google.com/protocol-buffers/">https://developers.google.com/protocol-buffers/</a></p>
<p>aka Protocol Buffers, but that’s a pretty dumb name. Google’s common, fast on-the-wire serialization format.</p>
<p><strong>Category:</strong> Machine-readable, schema-defined. Has an <a href="https://grpc.io/">RPC protocol</a> built around it.</p>
<p><strong>Users:</strong> Google, basically everyone</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Backed by Google, so it’s going to be good at the things Google values</li>
<li>Basically reasonable</li>
<li>Now has some support for versioning schemas, though it’s a hard problem in general</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Backed by Google, so it’s going to be good at the things Google values</li>
<li>Not particularly simple</li>
<li>Wire protocol may be more work than it needs to be</li>
<li>Its type system <a href="http://reasonablypolymorphic.com/blog/protos-are-wrong/index.html">could maybe be better</a></li>
</ul>
<h2 id="capn-proto">Cap’n Proto</h2>
<p><a href="https://capnproto.org/">https://capnproto.org/</a></p>
<p>The Other Binary Serialization Protocol.</p>
<p><strong>Category:</strong> Machine-readable, schema-defined. Designed primarily for RPC, which is built in to the reference implementation.</p>
<p><strong>Users:</strong> sandstorm.io, various other people but it doesn’t seem like that many</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Designed to be fast</li>
<li>Made by one of the people who worked heavily on Protobuf at Google, so <a href="https://capnproto.org/faq.html#how-do-i-make-a-field-required-like-in-protocol-buffers">there’s lots of experience behind it</a>. That said, doesn’t mean this cat’s always <em>right</em>, but there’s certainly opinions that are trying to be expressed.</li>
<li>Sophisticated RPC comes as part of the standard package</li>
<li>Designed for zero-copy deserialization</li>
<li>Designed for schema to evolve</li>
<li>Adorable name</li>
<li>Very explicit about correctness and conformance things such as field ordering and layout</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Very explicit about correctness and conformance things such as field ordering and layout</li>
<li>Lots of the docs and concepts are pretty low level, you usually ain’t gonna need it</li>
<li>Seems more complicated than protobuf – this might be one reason there’s fewer 3rd-party implementations</li>
</ul>
<h2 id="thrift">Thrift</h2>
<p><a href="https://thrift.apache.org/">https://thrift.apache.org/</a></p>
<p>Apache’s version of Protobuf. Does anyone actually use this? Facebook, apparently, since they invented it and then gave it to Apache. Anyone else?</p>
<p><strong>Category:</strong> Machine-readable, schema-defined. Designed primarily for RPC.</p>
<p><strong>Users:</strong> Basically just Facebook?</p>
<p><strong>Pros:</strong></p>
<ul>
<li>It works?</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Docs suck</li>
<li>Apache is the tragic junkyard of open source projects</li>
<li>Apparently still not as good as flatbuffers, see below</li>
</ul>
<h2 id="flatbuffers">Flatbuffers</h2>
<p><a href="https://google.github.io/flatbuffers/">https://google.github.io/flatbuffers/</a></p>
<p>Feels a little like Google’s answer to Cap’n Proto, as it has some of the same design goals – zero-copy serialization and layouts that are more amenable to versioning.</p>
<p><strong>Category:</strong> Machine-readable, schema-defined. Includes RPC protocol.</p>
<p><strong>Users:</strong> Google, Cocos2D, Facebook’s mobile client</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Designed for zero-copy deserialization</li>
<li>Designed for schema to evolve</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Kinda feels like the problem is already solved by capnp</li>
<li>Includes a JSON parser for some reason?</li>
<li>Type system is kinda anemic with regards to unions</li>
</ul>
<h2 id="cbor">CBOR</h2>
<p><a href="https://cbor.io/">https://cbor.io/</a></p>
<p>Basically a binary re-imagining of JSON.</p>
<p><strong>Category:</strong> Machine-readable, self-describing.</p>
<p><strong>Users:</strong> ???</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Pretty good type system – there’s things like fixnum’s, datetime’s, blobs, etc</li>
<li>Compact</li>
<li>Built-in extensibility</li>
<li>Designed to be a drop-in replacement for JSON</li>
<li>IETF standard</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Kinda more complicated than it needs to be, though this is for the sake of compactness and comprehensive types. Numbers are densely packed into fewer bits when possible, for example.</li>
<li>Doesn’t actually seem that widely adopted for some reason?</li>
</ul>
<h2 id="msgpack">Msgpack</h2>
<p><a href="https://msgpack.org/">https://msgpack.org/</a></p>
<p>The Other CBOR, or rather, <a href="https://news.ycombinator.com/item?id=14067747">CBOR is derived from this</a>. Designed to be simple and compact. Kinda a <em>lot</em> like a slightly chopped down CBOR, actually, their integer specification stuff looks nearly identical.</p>
<p><strong>Category:</strong> Machine-readable, self-describing.</p>
<p><strong>Users:</strong> Redis, a few others?</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Simple</li>
<li>Compact</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Specification is kinda weak</li>
<li>No real tuple or enum types</li>
<li>Why not just CBOR?</li>
</ul>
<h2 id="bson">BSON</h2>
<p><a href="http://bsonspec.org/">http://bsonspec.org/</a></p>
<p>As the name implies, a binary-ifcation of JSON. Created by MongoDB as its internal data format.</p>
<p><strong>Category:</strong> Machine-readable, self-describing.</p>
<p><strong>Users:</strong> MongoDB</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Type system is full of deprecated and MongoDB-specific shit but is reasonably pragmatic</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Type system is reasonably pragmatic but is full of deprecated and MongoDB-specific shit</li>
<li><strong>C strings</strong> – though there’s random non-C strings in places as well.</li>
<li>Its arrays are a travesty against serializarion</li>
<li>Basically an implementation detail of MongoDB, and it looks like it</li>
</ul>

<p>Things that are interesting but not actually in the scope of serialization languages, or are otherwise irrelevant.</p>
<h2 id="toml">TOML</h2>
<p><a href="https://github.com/toml-lang/toml">https://github.com/toml-lang/toml</a></p>
<p>Invalid, it’s designed as a config language, not a serialization format. It’s basically an attempt to make something as simple and ubiquitous as windows .INI files that is an actual specification rather than a fashion.</p>
<p><strong>Category:</strong> Human-readable, sorta self-describing though usually you have a specific data structure you’re trying to fit it into.</p>
<p><strong>Users:</strong> Various, notably Cargo (Rust’s build tool)</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Work well as a config language without deeply nested structures</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Works poorly when you try to make deeply nested structures</li>
</ul>
<h2 id="ron">RON</h2>
<p><a href="https://github.com/ron-rs/ron">https://github.com/ron-rs/ron</a></p>
<p>Rusty Object Notation. Because shoehorning Rust’s ML-y type systeminto JSON isn’t very much fun. Works startlingly well for this purpose but is basically untried elsewhere.</p>
<p><strong>Category:</strong> Human-readable, sorta self-describing though usually you have a specific data structure you’re trying to fit it into.</p>
<p><strong>Users:</strong> A few, notably <a href="https://amethyst.rs/">Amethyst</a>.</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Good type system for sophisticated functional-style languages</li>
<li>Simple and reasonably compact</li>
<li>Actually very good at what it does</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Young, underspecified, Rust-centric</li>
</ul>
<h2 id="bincode">Bincode</h2>
<p><a href="https://github.com/servo/bincode">https://github.com/servo/bincode</a></p>
<p>Included mainly for completeness. It’s not standardized outside of a single particular implementation which doesn’t promise stability, so not intended for general-purpose use. It’s intended as a fast and easy RPC/IPC format for Servo, and the actual format is basically an implementation detail of that goal.</p>
<p><strong>Users:</strong> Servo, programs written by introverts who don’t care about being able to talk to each other. (Turns out this is a useful niche though, who knew.)</p>
<p><strong>Pros:</strong></p>
<ul>
<li>Compact, fast, simple.</li>
<li>Works basically transparently for IPC with Rust code.</li>
</ul>
<p><strong>Cons:</strong></p>
<ul>
<li>Anything other than that specific version of that specific library is undefined. If you’re OK with that though, it’s great.</li>
</ul>
<h2 id="asn.1">ASN.1</h2>
<p><a href="https://en.wikipedia.org/wiki/Abstract_Syntax_Notation_One">https://en.wikipedia.org/wiki/…</a></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://wiki.alopex.li/BetterThanJson">https://wiki.alopex.li/BetterThanJson</a></em></p>]]>
            </description>
            <link>https://wiki.alopex.li/BetterThanJson</link>
            <guid isPermaLink="false">hacker-news-small-sites-24618121</guid>
            <pubDate>Mon, 28 Sep 2020 16:05:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Daniel Vassallo has made $200k with a Twitter course]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24618058">thread link</a>) | @Pete-Codes
<br/>
September 28, 2020 | https://www.petecodes.io/daniel-vassallo-200k-revenue-report/ | <a href="https://web.archive.org/web/*/https://www.petecodes.io/daniel-vassallo-200k-revenue-report/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://www.petecodes.io/content/images/size/w300/2020/09/Screenshot-2020-09-28-at-13.30.39.png 300w,
                            https://www.petecodes.io/content/images/size/w600/2020/09/Screenshot-2020-09-28-at-13.30.39.png 600w,
                            https://www.petecodes.io/content/images/size/w1000/2020/09/Screenshot-2020-09-28-at-13.30.39.png 1000w,
                            https://www.petecodes.io/content/images/size/w2000/2020/09/Screenshot-2020-09-28-at-13.30.39.png 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://www.petecodes.io/content/images/size/w2000/2020/09/Screenshot-2020-09-28-at-13.30.39.png" alt="Daniel Vassallo has made $200k by teaching how to get a Twitter following">
            </figure>

            <section>
                <div>
                    <p>Daniel Vassallo is a former Software Engineer at AWS who <a href="https://danielvassallo.com/only-intrinsic-motivation-lasts/">quit his job last year</a> in order to work on his products. Along the way he started to pick up a following on Twitter and made a video course on <a href="https://gumroad.com/a/466973811/PBkrO">how to build an audience.</a> I'm excited about chatting to him on Zoom today - <a href="https://gum.co/ICuKS">you're welcome to ask him questions too</a>! </p><!--kg-card-begin: markdown--><p><a href="https://gum.co/ICuKS"><img src="https://www.petecodes.io/content/images/2020/09/Screenshot-2020-09-28-at-13.01.37.png" alt="Screenshot-2020-09-28-at-13.01.37"></a></p>
<!--kg-card-end: markdown--><p>In a very short space of time <a href="https://twitter.com/dvassallo/status/1281263000792346627?s=20">he has earned over $200,000 from just this course</a>. It's pretty incredible given this was just a product he made while he was working on his next big project, <a href="https://userbase.com/">Userbase, which lets you make serverless web apps</a>. </p><p>I don't think Daniel had any idea his course would blow up the way it did. He helped himself by writing a post about quiting his $500k job at Amazon. That definitely helped him get some initial followers. </p><p>Of course with an existing audience he was able to have a big advantage over other content producers. And as word spread of his course, he got more followers! And so his credentials improved meaning he sold more courses! Because if you see someone selling a course on getting more Twitter followers and they themselves have an audience which keeps growing, naturally you are attracted to their product. And so the cycle continues. </p><p>If you are interested in the specifics of the course, <a href="https://www.petecodes.io/review-daniel-vassallo-twitter-audience-course/">I wrote a review here</a>. </p><p>Of course there are already lots of Twitter courses available online. So what made Daniel's course stand out? </p><p>Firstly, he used social proof and early buyers generated a lot of positive reviews. I should know as I bought a copy after seeing that Randall Kanna had seen her following shoot up after taking the course. Word spread quickly in tech circles that it was a quality course. </p><figure><blockquote data-width="550"><p lang="en" dir="ltr">An unexpected $1K day today, came out of nowhere. This month will likely close at ~$12K, and I did no direct promotion. No tweets, no paid ads — just word of mouth. Very satisfied, and thank you for all the support! 🙏 <a href="https://t.co/LQ1pnLyR05">pic.twitter.com/LQ1pnLyR05</a></p>— Daniel Vassallo (@dvassallo) <a href="https://twitter.com/dvassallo/status/1287954138634977281?ref_src=twsrc%5Etfw">July 28, 2020</a></blockquote>

</figure><h2 id="using-affiliates-to-generate-income-passively">Using affiliates to generate income passively </h2><p>Secondly, he has been clever in using the Gumroad affiliates system. Gumroad is awesome as it has a really flexible affiliates feature open to anyone on the platform. You are able to set what % of a sale each affiliate is going to get. Of course, Daniel only has to pay when someone makes a sale. So with affiliates he has people like me speading the word about his courses and making money for him passively. It also helps that he gives 50% to affiliates which is a good incentive to share his course. </p><figure><img src="https://www.petecodes.io/content/images/2020/09/Screenshot-2020-09-28-at-12.51.07.png" alt=""></figure><h2 id="a-high-price-point-communicates-quality">A high price point communicates quality</h2><p>He also set a high price point which conveys value. It can be paradoxical but if people pay a high price it can be better as no-one wants to believe they paid too much. So they justify it to themselves. It's also more likely they will complete the course. </p><p>Like most people, I have a graveyard of free and very cheap courses in my Udemy course. I also have a tonne of free ebooks I've never read. In contrast, I just ordered Steph Smith's Doing Content Right ebook which cost me $30. As it cost me more I'm more motivated to read it as I want to get my money's worth. With free and very cheap books, I value them less. </p><p>Stella Artois was able to craft a better image in the UK in the 90s by a clever slogan - "reassuringly expensive". This is still one of my favourite marketing campaigns of all time! Instead of having people question the price you make the higher price a signal of quality. </p><figure><img src="https://www.petecodes.io/content/images/2020/09/iu-54.jpeg" alt=""></figure><h2 id="ask-daniel-your-questions">Ask Daniel your questions </h2><p>I'm looking forward to chatting with <a href="https://gum.co/ICuKS">Daniel today on Zoom</a> at 10am Seattle time. For 30 minutes I'm going to be asking him questions and then I'm passing the mic, so to speak, to anyone who wants it! I've organised two chats like this already; one with Anne-Laure Le Cunff and last week I spoke to Ben Tossell. </p><p>I've reduced prices by 50% to $15. This gives you access to the Zoom call with Danial as well as next Monday's chat with Scott Keyes, the co-founder of Scott's Cheap Flights. </p><!--kg-card-begin: markdown--><p><a href="https://gum.co/ICuKS"><img src="https://www.petecodes.io/content/images/2020/09/Screenshot-2020-09-28-at-13.01.37.png" alt="Screenshot-2020-09-28-at-13.01.37"></a></p>
<!--kg-card-end: markdown-->
                </div>
            </section>

                <section>
    <h3>Subscribe to Pete Codes </h3>
    <p>Get the latest posts delivered right to your inbox</p>
    <form data-members-form="subscribe">
        
        <p><strong>Great!</strong> Check your inbox and click the link to confirm your subscription.
        </p>
        <p>
            Please enter a valid email address!
        </p>
    </form>
</section>

        </article>

    </div>
</div></div>]]>
            </description>
            <link>https://www.petecodes.io/daniel-vassallo-200k-revenue-report/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24618058</guid>
            <pubDate>Mon, 28 Sep 2020 15:59:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Talk to Customers During an Incident]]>
            </title>
            <description>
<![CDATA[
Score 16 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24617918">thread link</a>) | @redRanger72
<br/>
September 28, 2020 | https://iamevan.me/categories/sre/customer-communication-during-incidents-the-how-to-of-status-page-updates/ | <a href="https://web.archive.org/web/*/https://iamevan.me/categories/sre/customer-communication-during-incidents-the-how-to-of-status-page-updates/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          <p><img src="https://iamevan.me/images/posts/2020-09-27-Customer-Communication-During-Incidents-The-How-To-Of-Status-Page-Updates.jpg" alt=""></p>
<p>Something often overlooked during an incident is how we communicate with our customers and reassure them of the situation. How you convey an incident to the people paying for your service can make all the difference when it comes to contract renewal period. At a past company, our sales team regularly reported back from client meetings that they consistently mentioned how helpful and reassuring status page updates from the SRE team were, even when our service was fully down. We need to stop treating our customers like they’re non-technical children and give them information that will help them make their usage of us more reliable in the long-term.</p>

<p>The most likely and easiest point of contact for a technical team to customers is the status page. It’s the first port of call when you suspect a third-party is causing issues or not behaving itself. It doesn’t really matter what yours looks like as long as it provides the functionality for people to view timestamped updates posted by your oncall team during an incident.</p>
<p>A good status page helps build trust with our users, and show a glimpse into how we behave during incidents and how committed we are not only to reduce the number and impact of incidents, but to be transparent about them (sometimes sharing your mistakes is a good way of shaming yourself into fixing them!).</p>
<p>This will be an ongoing record of of past and present incidents so our customers can have a reasonable and current idea of: 1) how well our services are performing; and 2) if an issue they are experiencing is related to a wider problem or just isolated to them.</p>
<p><img src="https://media.giphy.com/media/citBl9yPwnUOs/giphy.gif" alt="Homer  Simpson hovering over a computer that says “to start, press any key” and homer is thinking aloud “Where’s the ‘any’ key?" "=""></p>
<h2 id="who-writes-the-message">Who Writes The Message</h2>
<p>Assuming you follow something like the <a href="https://en.wikipedia.org/wiki/Incident_Command_System">Incident Command System</a>, this task can either be done by the Incident Commander or delegated to someone else. It’s upto the person leading the incident to decide who updates and how frequently they update.</p>
<h2 id="how-quickly-should-an-update-go-out">How Quickly Should An Update Go Out?</h2>
<p>Simple answer: <strong>as soon as possible</strong>.</p>
<p>Until we’ve posted something, our users won’t know that we know something is wrong and are working to mitigate it. Generally, if you work for a sizeable company, you’ll have experienced the floods of emails to support with “my shit’s broken, yo! WHERE’S YOUR STATUS PAGE?” and while it’s true you can never eliminate all of those, putting up something as soon as you are alerted to an incident is definitely going to help minimise them a lot (and your customers will appreciate you more for it).</p>
<p>With that said though, there is a trade-off to how quick an update goes out. You need to find a balance between providing accurate and relevant information (what the exact impact is and the actual start time of an incident) versus having a fast response. There’s no definitive answer here, it’s mostly something you’ll work out with experience but remember: You can always update the status page to be more specific later, it’s good to err on the side of quickness than waiting too long.</p>
<h2 id="how-often-should-we-update">How Often Should We Update?</h2>
<p>The team should sit down and decide on an appropriate interval to use for incidents going forward. In the past, I’ve found 20 minute intervals works well enough as a starting point.</p>
<p>In that case, you should aim to have an update out every 20 minutes with relevant information - <strong>do not copy and paste the same message</strong>. If we are at a point where there is no new information to be added, it might be time to consider increasing the update interval.</p>
<p>Increasing the interval should be explicitly stated in an update. If we’re moving from 20 minutes to 1 hour, we should state the current impact and say that our customers should expect an update in an hour or less if more information becomes relevant.</p>
<p>Increasing the interval can also be a good way to free up an extra person from the communication role if there aren’t enough hands on deck.</p>
<p>Giving people a reasonable timeframe for more information eases stress and allows them to plan their own mitigation strategy if they rely on our service. We also don’t want someone furiously refreshing our page for hours waiting for an update that isn’t coming.</p>

<p>I would wager you aren’t lucky to have a staff of technical writers on hand to pen your updates, so these messages fall to regular ol' engineers. That also means we don’t expect you to write the next literary masterpiece. What we do want to accomplish is:</p>
<ul>
<li>Clear communication</li>
<li>The right amount of details</li>
<li>A good representation of the company in the public eye</li>
</ul>
<p>Take some time to consider the right tone of your communication, maybe write a doc outlining some common examples for the team.</p>
<h2 id="the-title">The Title</h2>
<p>The title is a very important part of an update. Sometimes it’s the only thing the user sees before making the decision of clicking to see the rest or not, so we need to help the user answer the following question: “Do I need to care about this?”. How do we do this? By stating how it affects <strong>them</strong> as clearly as possible.</p>
<p><img src="https://media.giphy.com/media/3o6UBiAQ9Ws8UWdmqA/giphy.gif" alt="Miss Hoover from the Simpsons sitting in a classroom, reaching under the desk to push a big red button labelled “Independent Thought Alarm”"></p>
<blockquote>
<p>“Website Issues”</p>
</blockquote>
<p>Is quite vague and very poor at indicating what’s actual affected.</p>
<blockquote>
<p>“Increased API Response Times”</p>
</blockquote>
<p>Is much better and clearly identifies the service affected, what symptoms to look for and how that may affect the customer.</p>
<h2 id="the-message">The Message</h2>
<p>How you write this is going to come down to a mixture of experience and empathy. You’re going to need to put yourself in your customer’s shoes and think “what would <em>I</em> care about if I saw this incident?”</p>
<p>You’re going to need a couple elements for certain though:</p>
<ol>
<li>The exact time <em>the problem started</em>. Be sure to include a timezone to avoid an ambiguity. Also important to note that this is <em>not</em> necessarily the same time you opened the status page or were alerted to the problem.</li>
<li>A clear description of what <strong>is</strong> and <strong>isn’t</strong> impacted. This should include information a customer can use to diagnose if their particular issue is related to the incident. It’s also a great time to state what isn’t affected by the incident and relieve some stress. For example: if you ingest user data but are just having delays processing it, it can be good to state that ingestion is unaffected and no user data has been lost.</li>
<li>The exact time of the update and when the issue has been resolved. Again, with timezones, these will help customers build a timeline to compare to their own logs.</li>
<li>A technical description of the problem. Your customers are smarter than you give them credit for and deserve to be treated like adults. Be open in your communication of what the problem is because it will also build respect for the next element.</li>
<li>What are we doing to resolve the problem? Don’t just say “we are dealing with the situation” or “we’re applying a fix” because that isn’t fair to your customers. This is also going to be very useful to you when you create a postmortem for the incident.</li>
</ol>
<h2 id="some-general-tips">Some General Tips</h2>
<ul>
<li>Don’t apologise or use “sorry for the inconvenience” <a href="https://signalvnoise.com/posts/1528-the-bullshit-of-outage-language">bullshit language</a></li>
<li>Be clear, be concise and state the impact up front</li>
<li>Be as specific as possible, but only when it helps</li>
<li>24hr UTC is the crowned king of timestamps</li>
<li>Generic maintenance pages frustrate/scare people</li>
<li>Please, for the love of god do not copy and paste the same variation of the same message <a href="https://status.datadoghq.com/incidents/6s5xxxjh33lh">over</a> and <a href="https://www.githubstatus.com/incidents/pbz6fh7mz86w">over</a> and <a href="https://www.hetzner-status.de/en.html">over</a> again</li>
</ul>
<p>For more info:</p>
<ul>
<li><a href="https://www.hostedgraphite.com/blog/how-to-write-a-status-page-update">“How To Write A Status Page Update”</a> by Fran Garcia</li>
<li><a href="https://signalvnoise.com/posts/1528-the-bullshit-of-outage-language">“The Bullshit Of Outage Language”</a> by David H. Hanson</li>
<li><a href="https://www.atlassian.com/blog/statuspage/how-to-write-a-good-status-update">“How To Write A Good Status Update”</a> by Blake Thorne</li>
</ul>
<p><img src="https://media.giphy.com/media/3o8doT9BL7dgtolp7O/giphy.gif" alt="Carl from the Simpsons performing the chef’s kiss"></p>

<p>These are all examples of postmortems but you’ll notice that a lot of the language is the same and some include all the status updates as well. They’re useful examples as both status updates and postmortems and I would highly recommend reading all the different styles.</p>
<ul>
<li><a href="https://status.hostedgraphite.com/incidents/xdrrb9g3lxm7">The Short all-rounder</a></li>
<li><a href="https://www.traviscistatus.com/incidents/khzk8bg4p9sy">Sometimes, there is no “What Went Well”</a></li>
<li><a href="https://aws.amazon.com/message/4372T8/">The Essay Alternative</a></li>
<li><a href="https://blog.cloudflare.com/how-and-why-the-leap-second-affected-cloudflare-dns/">Sometimes, it’s more of a story</a></li>
<li><a href="https://circleci.statuspage.io/incidents/hr0mm9xmm3x6">A Very Technical Timeline</a></li>
<li><a href="https://sb.status.hostedgraphite.com/incidents/f51fnwjvxx0k">A Lot Of Timelines (and some internal tracking)</a></li>
</ul>

        </div></div>]]>
            </description>
            <link>https://iamevan.me/categories/sre/customer-communication-during-incidents-the-how-to-of-status-page-updates/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24617918</guid>
            <pubDate>Mon, 28 Sep 2020 15:47:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Interactive GCC (igcc) – a REPL for C/C++]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24617861">thread link</a>) | @pmoriarty
<br/>
September 28, 2020 | http://www.artificialworlds.net/wiki/IGCC/IGCC | <a href="https://web.archive.org/web/*/http://www.artificialworlds.net/wiki/IGCC/IGCC">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="wikitext">

<p><a href="#home"><span>Introduction</span></a> | <a href="#download"><span>Downloading and using</span></a> | <a href="#developing"><span>Developing</span></a> | <a href="#links"><span>Links</span></a> | <a href="#copyright"><span>Copyright</span></a>
</p>

<p>Interactive GCC (igcc) is a real-eval-print loop (REPL) for C/C++ programmers.
</p>
<p>It can be used like this:
</p><pre> $ ./igcc 
 g++&gt; int a = 5;
 g++&gt; a += 2;
 g++&gt; cout &lt;&lt; a &lt;&lt; endl;
 7
 g++&gt; --a;
 g++&gt; cout &lt;&lt; a &lt;&lt; endl;
 6
 g++&gt; 
</pre><p>It is possible to include header files you need like this:
</p><pre> $ ./igcc 
 g++&gt; #include &lt;vector&gt;
 g++&gt; vector&lt;int&gt; myvec;
 g++&gt; myvec.push_back( 17 );
 g++&gt; printf( "%d\n", myvec.size() );
 1
 g++&gt; myvec.push_back( 21 );
 g++&gt; printf( "%d\n", myvec.size() );
 2
 g++&gt; 
</pre><p>Compile errors can be tolerated until the code works:
</p><pre> $ ./igcc
 g++&gt; #include &lt;map&gt;
 g++&gt; map&lt;string,int&gt; hits;
 g++&gt; hits["foo"] = 12;
 g++&gt; hits["bar"] = 15;
 g++&gt; for( map&lt;string,int&gt;::iterator it = hits.begin(); it != hits.end(); ++it )
 [Compile error - type .e to see it.]
 g++&gt; {
 [Compile error - type .e to see it.]
 g++&gt; 	cout &lt;&lt; it-&gt;first &lt;&lt; " " &lt;&lt; it-&gt;second &lt;&lt; endl;
 [Compile error - type .e to see it.]
 g++&gt; }
 bar 15
 foo 12
 g++&gt; 
</pre><p>Extra include directories can be supplied:
</p><pre> $ ./igcc -Itest/cpp -Itest/cpp2
 g++&gt; #include "hello.h"
 g++&gt; hello();
 Hello, 
 g++&gt; #include "world.h"
 g++&gt; world();
 world!
 g++&gt; 
</pre><p>Libs can be linked:
</p><pre> $ ./igcc -lm
 g++&gt; #include "math.h"
 g++&gt; cout &lt;&lt; pow( 3, 3 ) &lt;&lt; endl; // Actually a bad example since libm.a is already linked in C++
 27
 g++&gt; 
</pre><p>Your own libs can be linked too:
</p><pre> $ ./igcc -Itest/cpp -Ltest/cpp -lmylib
 g++&gt; #include "mylib.h"
 g++&gt; defined_in_cpp();
 defined_in_cpp saying hello.
 g++&gt; 
</pre><p>The <code>cstdio</code>, <code>iostream</code> and <code>string</code> headers are automatically included, and the <code>std</code> namespace is automatically in scope.
</p>
<h2>Downloading and using</h2>
<p>Download the IGCC tarball from the <a href="https://sourceforge.net/projects/igcc/files/" rel="nofollow">Sourceforge download area</a>.
</p>
<p>Untar it like so:
</p><pre> tar -xjf igcc-0.1.tar.bz2
</pre><p>And then start the program like this:
</p><pre> cd igcc-0.1
 ./igcc
</pre><p>Then type the C++ code you want to execute. It will be compiled with GCC and the results (if any) will be displayed.
</p>
<p>Type <code>.h</code> to see some (minimal) help.
</p>
<h2>Developing</h2>
<p>IGCC is a small python wrapper around GCC.
</p>
<p>Check out the code from git like this:
</p>
<pre> git clone git://igcc.git.sourceforge.net/gitroot/igcc/igcc
</pre><p>Or browse the source on <a href="http://igcc.git.sourceforge.net/git/gitweb.cgi?p=igcc/igcc;a=tree" rel="nofollow">IGCC Gitweb</a>.
</p>
<h2>Links</h2>
<ul><li><a href="http://sourceforge.net/projects/igcc/" rel="nofollow">IGCC Sourceforge page</a>
</li><li><a href="http://www.artificialworlds.net/" rel="nofollow">Andy Balaam's home page</a>
</li><li><a href="http://www.artificialworlds.net/blog" rel="nofollow">Andy Balaam's blog</a>
</li></ul><h2>Copyright</h2>
<p>IGCC is Copyright (C) 2009 Andy Balaam
</p>
<p>IGCC is Free Software released under the terms of the GNU General Public License version 2 or later.
</p>
<p>IGCC comes with NO WARRANTY.
</p>
<p>See the file <a href="http://igcc.git.sourceforge.net/git/gitweb.cgi?p=igcc/igcc;a=blob_plain;f=COPYING.txt;hb=HEAD" rel="nofollow">COPYING</a> for more information.
</p>
</div></div>]]>
            </description>
            <link>http://www.artificialworlds.net/wiki/IGCC/IGCC</link>
            <guid isPermaLink="false">hacker-news-small-sites-24617861</guid>
            <pubDate>Mon, 28 Sep 2020 15:43:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[C standard function 'tdelete()' has a memory bug since its inception]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 8 (<a href="https://news.ycombinator.com/item?id=24617723">thread link</a>) | @cee-studio
<br/>
September 28, 2020 | https://www.cee.studio/tdelete.html | <a href="https://web.archive.org/web/*/https://www.cee.studio/tdelete.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="markdown-content">
  
  <p><strong>tdelete()</strong>, a POSIX function included in Libc, can return dangling pointers.
      In <strong>tdelete()</strong>'s man page<sup>1</sup>, the <strong>RETURN
      VALUE</strong> specifies that "tdelete() returns a pointer to the parent of the
      item deleted, or NULL if the item was not found.".  In the
      <a href="https://github.com/cee-studio/tdelete" target="_blank">testing</a> of both musl libc and
      glibc, <strong>tdelete()</strong> returns dangling pointers. The output of the test (as seen below)
      shows that two dangling pointers are returned.

    </p><pre><code>Testing glibc's implementation of tdelete for a returning dangling address bug

  allocate 0x7fd9f70b0324
  allocate 0x7fd9f70b0388
  allocate 0x7fd9f70b03ec
  allocate 0x7fd9f70b0450
  free 0x7fd9f70b0324
  tdelete returns 0x7fd9f70b0388
  free 0x7fd9f70b0388
  tdelete returns 0x7fd9f70b03ec
  free 0x7fd9f70b03ec
  tdelete returns 0x7fd9f70b03ec &lt;- dangling pointer
  free 0x7fd9f70b0450
  tdelete returns 0x7fd9f70b0450 &lt;- dangling pointer

Testing musl libc's implementation of tdelete for a returning dangling address bug

  allocate 0x7f2ecf539324
  allocate 0x7f2ecf539388
  allocate 0x7f2ecf5393ec
  allocate 0x7f2ecf539450
  free 0x7f2ecf539324
  tdelete returns 0x7f2ecf539388
  free 0x7f2ecf539388
  tdelete returns 0x7f2ecf5393ec
  free 0x7f2ecf5393ec
  tdelete returns 0x7f2ecf5393ec &lt;- dangling pointer
  free 0x7f2ecf539450
  tdelete returns 0x7f2ecf539450 &lt;- dangling pointer
</code>
</pre>

    <p>Returning a dangling pointer is dangerous, especially for library
        functions, and dereferencing the dangling pointer can cause memory
        errors. This bug was reported by Cee.Studio when we implemented a map
        container as a thin wrapper of the <strong>tsearch()</strong> family
        functions. Here is the
        <a href="https://cee.studio/?bucket=200928-vhV&amp;name=tsearch" target="_blank">live demo</a> to see how
        the problem is reported. The following is the report:</p>
    <pre><code>    Warning: returning a dangling address.
    # The address-to-be-returned is of a memory-block (start:0x922a070, size:16 bytes) allocated at
    #    file:/tsearch.c::81, 6
    #    file:/prog.c::28, 8
    #    [libc-start-main]
    # The memory-block has been freed at
    #    file:/tdelete.c::45, 2
    #    file:/prog.c::32, 14
    #    [libc-start-main]
    # The memory-block has been freed, and its allocation location could have
    # been distorted by subsequent reuses.
    #
    # Stack trace (most recent call first)
    # [0]  file:/prog.c::32, 14
    # [1]  [libc-start-main]
    https://cee.studio/explanation
</code></pre><p>
    Even if the latest <strong>tdelete()</strong> man page<sup>2</sup> warns
    about the risk of dangling pointers, this still caught us off guard.
    Our man page does not have the warning and the page returned by Google
    does not have it. <strong>Some automatic mechanisms need to be
    employed to make sure the uses of C functions are correct.</strong></p><p>Stensal SDK re-implements C/C++ like a <strong>memory-safe</strong>
        language. It can help uncover subtle memory bugs like never before.</p>
    

</div></div>]]>
            </description>
            <link>https://www.cee.studio/tdelete.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24617723</guid>
            <pubDate>Mon, 28 Sep 2020 15:33:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Repurposing my old Chromebook to a low power home server]]>
            </title>
            <description>
<![CDATA[
Score 102 | Comments 78 (<a href="https://news.ycombinator.com/item?id=24617661">thread link</a>) | @boros2me
<br/>
September 28, 2020 | https://tamas.dev/repurpose/asus/c300/chromebook/home/server/chromeos/linux/docker/containers/2020/09/28/repurpose-chromebook-low-power-home-server.html | <a href="https://web.archive.org/web/*/https://tamas.dev/repurpose/asus/c300/chromebook/home/server/chromeos/linux/docker/containers/2020/09/28/repurpose-chromebook-low-power-home-server.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <h2 id="cant-sell-it-repurpose-it">Can’t sell it? Repurpose it!</h2>
<p>It was around 4 years ago when I bought a used, slightly beat up Asus C300 Chromebook from a friend of mine for £50. It served me well, I installed Linux on it and used it to work on some side projects while commuting. However as time passed by, I slowly forgot about it and it ended up in a box, being put away. I always thought it’s a shame to just let it sit there, but I don’t wanted to sell it for pennies and neither I wanted to throw it out as it is a perfectly fine machine for some lightweight tasks. I always had the idea to convert it to a home server (yes, with a 7W TDP Celeron N2830 CPU), but couldn’t really find a guide online that explains how to do it and neither I had the urge to do the experimenting myself, up until now.</p>

<h2 id="the-setup">The setup</h2>

<p><a href="https://tamas.dev/static/2020-09-28/chromebook.jpg" target="_blank"><img src="https://tamas.dev/static/2020-09-28/chromebook.jpg" alt="Chromebook home server"></a></p>

<ul>
  <li>1x Asus C300 Chromebook</li>
  <li>1x 128GB USB drive</li>
  <li>1x vertical stand</li>
  <li>1x GalliumOS installed</li>
</ul>

<p>In order to run a generic home server, I knew ChromeOS will just not cut it, it has to be proper Linux. The distribution didn’t really matter as I was going to use Docker anyways (and docker-compose) to run all server applications in containers. Fortunately I already had experience installing GalliumOS, therefore it became my distribution of choice. GalliumOS is an Ubuntu based distro, specifically created to be running on ChromeOS devices. I had to update the firmware on my Chromebook (it’s a very simple, less than 10 steps process) then install the OS using a tool called chrx (another 2-3 steps). Once done, I was able to dual boot my Chromebook and got a fully operational Linux laptop.</p>

<h2 id="software-configuration">Software configuration</h2>

<p>Once I had the OS up and running, I wanted to configure it so I can use it in closed-display (clamshell) mode as leaving it running with the display open wouldn’t be too practical. I knew MacBooks had these option, but I also knew you had to have an external display and keyboard/mouse connected to it in order to make it work. Luckily with Linux, the only limit is your imagination (and your coding/Google-ing skills)! Since GalliumOS is Ubuntu based as mentioned before, I was certain there are people using Ubuntu who want their system to stay awake even after they shut the display. And I was not wrong. Simply editing <code>logind.conf</code> and turning off all power savings and auto-sleep functions through the GUI did the job.</p>

<p>The next thing I had to install was sshd, but thanks to this Reddit post I found out it was as simple as <code>apt install</code>-ing it (again, it’s basically a skinny version of Ubuntu). Left all settings to default, then I SSH-ed from another box. All went smooth, I logged in from the remote machine, started <code>htop</code> and closed the lid to see that the configuration actually works! I left the connection open for about an hour, just to be 100% sure no sleep mode will kick in, then I considered the project a success. Was much smoother than I anticipated.</p>

<h2 id="servers-n-containers">Servers ‘n containers</h2>

<p>Although I would <strong><em>not recommend</em></strong> running any production workload on a <em>Chromebook</em> via <em>Wifi</em> connection, it’ll serve perfectly fine as my home cron/CI server, especially knowing it can run on batteries for 10 hours (allegedly), without having a UPS. Following the documentation I installed Docker with no problem whatsoever and docker-compose as well. I also plugged in a tiny 128GB USB thumb drive I found laying around on my desk, as the built in eMMC is not the most spacious storage ever made. I re-formatted the drive to ext4, then mounted and finally updated Docker to use the partition on the USB device for data storage, which includes all containers, volumes, etc.
In case you’re curious, please see the list of the containerised servers I used with some explanation to the whys below.</p>

<h3 id="portainer">Portainer</h3>

<p>Website: <a href="https://www.portainer.io/">https://www.portainer.io/</a></p>

<p>Portainer provides a very user-friendly way to manage containers on remote boxes. Via a web interface, it provides information about resource utilisation, logs, etc. for each container.</p>

<h3 id="grafana--influxdb--telegraph">Grafana + InfluxDB + Telegraph</h3>
<p>Grafana website: <a href="https://grafana.com/">https://grafana.com/</a></p>

<p>InfluxDB website: <a href="https://www.influxdata.com/">https://www.influxdata.com/</a></p>

<p>Telegraf website: <a href="https://www.influxdata.com/time-series-platform/telegraf/">https://www.influxdata.com/time-series-platform/telegraf/</a></p>

<p>I use this combo to monitor average packet loss, ping, DNS query time, CPU temperature and CPU/Memory/Disk utilisation.</p>

<h3 id="jenkins-ci">Jenkins CI</h3>
<p>Website: <a href="https://www.jenkins.io/">https://www.jenkins.io/</a></p>

<p>I use Jenkins for all the cron/CI jobs I have, to build/deploy and run automated tests on some of my side projects.</p>

<h3 id="sonatype-nexus">Sonatype Nexus</h3>
<p>Website: <a href="https://www.sonatype.com/nexus/repository-oss">https://www.sonatype.com/nexus/repository-oss</a></p>

<p>To cut back on network utilisation, I’ve set Nexus up as well as a local docker/npm/pypi cache.</p>

<h3 id="future-improvements">Future improvements</h3>
<p>I’ll stop at this point as I reached my goal with this project (as always, the journey was more important than the result itself), this little device has loads of potential with it’s 2x USB ports, a full size HDMI port and even an audio jack. At the end of the day, I feel much better to put this neat little tech in use, and convinced myself I don’t need to buy <em>yet another</em> RaspberryPI for a future project that’ll never come.</p>

<hr>

<p>Resources used:</p>

<p><a href="https://uk.store.asus.com/asus-chromebook-c300ma-13-3-light-weight-laptop-intel-dual-core-2gb-32gb-emmc.html">ASUS Chromebook C300</a></p>

<p><a href="https://ark.intel.com/content/www/us/en/ark/products/81071/intel-celeron-processor-n2830-1m-cache-up-to-2-41-ghz.html">Intel Celeron N2830</a></p>

<p><a href="https://galliumos.org/">GalliumOS</a></p>

<p><a href="https://wiki.galliumos.org/Installing">Installing GalliumOS</a></p>

<p><a href="https://chrx.org/">chrx</a></p>

<p><a href="https://askubuntu.com/a/372616">How can I tell Ubuntu to do nothing when I close my laptop lid?</a></p>

<p><a href="https://www.reddit.com/r/GalliumOS/comments/5b7vwi/does_galliumos_not_come_with_ssh_abilities/d9mx0ie/?utm_source=reddit&amp;utm_medium=web2x&amp;context=3">Does GalliumOS not come with ssh abilities installed?</a></p>

<p><a href="https://stackoverflow.com/a/52018760">How to change the default location for “docker create volume” command?</a></p>


  </div>
</article>

      </div>
    </div></div>]]>
            </description>
            <link>https://tamas.dev/repurpose/asus/c300/chromebook/home/server/chromeos/linux/docker/containers/2020/09/28/repurpose-chromebook-low-power-home-server.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24617661</guid>
            <pubDate>Mon, 28 Sep 2020 15:28:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Flexible Performant GEMM Kernels on GPUs in Native Julia]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24617647">thread link</a>) | @ViralBShah
<br/>
September 28, 2020 | https://juliagpu.org/2020-09-28-gemmkernels/ | <a href="https://web.archive.org/web/*/https://juliagpu.org/2020-09-28-gemmkernels/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <main id="main">
        





<i data-feather="calendar"></i> <time datetime="2020-09-28">Sep 28, 2020</time>




  <br>
  <i data-feather="edit-2"></i> Thomas Faignaert, Tim Besard, Bjorn De Sutter


<p>General Matrix Multiplication or GEMM kernels take center place in high performance
computing and machine learning. Recent NVIDIA GPUs include GEMM accelerators, such as
NVIDIA’s Tensor Cores. In this paper we show how it is possible to program these
accelerators from Julia, and present abstractions and interfaces that allow to do so
efficiently without sacrificing performance.</p>
<p>A pre-print of the paper has been published on arXiv:
<a href="https://arxiv.org/abs/2009.12263">arXiv:2009.12263</a>. <br> The source code can be found on
GitHub:
<a href="https://github.com/thomasfaingnaert/GemmKernels.jl">thomasfaingnaert/GemmKernels.jl</a>.</p>
<p>With the APIs from GemmKernels.jl, it is possible to instantiate GEMM kernels that perform
in the same ball park as, and sometimes even outperform state-of-the-art libraries like
CUBLAS and CUTLASS. For example, performing a mixed-precision multiplication of two 16-bit
matrixes into a 32-bit accumulator (on different combinations of layouts):</p>


<figure>
	<img src="https://juliagpu.org/2020-09-28-gemmkernels/mixed_precision.png" alt="Performance of mixed-precision GEMM">
</figure>

<p>The APIs are also highly flexible and allow customization of each step, e.g., to apply the
activation function <code>max(x, 0)</code> for implementing a rectified linear unit (ReLU):</p>
<div><pre><code data-lang="julia">a <span>=</span> CuArray(rand(<span>Float16</span>, (M, K)))
b <span>=</span> CuArray(rand(<span>Float16</span>, (K, N)))
c <span>=</span> CuArray(rand(<span>Float32</span>, (M, N)))
d <span>=</span> similar(c)

conf <span>=</span> GemmKernels<span>.</span>get_config(
    gemm_shape <span>=</span> (M <span>=</span> M, N <span>=</span> N, K <span>=</span> K),
    operator <span>=</span> Operator<span>.</span>WMMAOp{<span>16</span>, <span>16</span>, <span>16</span>},
    global_a_layout <span>=</span> Layout<span>.</span>AlignedColMajor{<span>Float16</span>},
    global_c_layout <span>=</span> Layout<span>.</span>AlignedColMajor{<span>Float32</span>})

GemmKernels<span>.</span>matmul(
    a, b, c, d, conf;
    transform_regs_to_shared_d <span>=</span> Transform<span>.</span>Elementwise(x <span>-&gt;</span> max(x, <span>0</span>)))
</code></pre></div><p>The GemmKernels.jl framework is written entirely in Julia, demonstrating the
high-performance GPU programming capabilities of this language, but at the same time keeping
the research accessible and easy to modify or repurpose by other Julia developers.</p>



      </main>
    </div></div>]]>
            </description>
            <link>https://juliagpu.org/2020-09-28-gemmkernels/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24617647</guid>
            <pubDate>Mon, 28 Sep 2020 15:27:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Basic CQRS by refactoring a Go project]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24617237">thread link</a>) | @roblaszczak
<br/>
September 28, 2020 | https://threedots.tech/post/basic-cqrs-in-go/ | <a href="https://web.archive.org/web/*/https://threedots.tech/post/basic-cqrs-in-go/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>It’s highly likely you know at least one service that:</p><ul><li>has one big, unmaintainable model that is hard to understand and change,</li><li>or where work in parallel on new features is limited,</li><li>or can’t be scaled optimally.</li></ul><p>But often, bad things come in threes. It’s not uncommon to see services with all these problems.</p><p>What is an idea that comes to mind first for solving these issues?
Let’s split it into more microservices!</p><p>Unfortunately, without proper research and planning, the situation after blindly refactoring may be actually worse than before:</p><ul><li><strong>business logic and flow may become even harder to understand</strong> – a complex logic is often easier to understand if it’s in one place,</li><li><strong>distributed transactions</strong> – things are sometimes together for a reason; a big transaction in one database is much faster and less complex than distributed transaction across multiple services,</li><li><strong>adding new changes may require extra coordination</strong>, if one of the services is owned by another team.</li></ul><p><img src="https://threedots.tech/media/introducing-cqrs/more-microservices.jpg" alt="MAKE MORE MICROSERVICES!"></p><p>Microservices are useful, but they will not solve all your issues...</p><p>To be totally clear – I’m not an enemy of microservices.
<strong>I’m just against blindly applying microservices in a way that introduces unneeded complexity and mess instead of making our lives easier.</strong></p><p>Another approach is using CQRS (Command Query Responsibility Segregation) with previously described
<a href="https://threedots.tech/post/introducing-clean-architecture/">Clean Architecture</a> and <a href="https://threedots.tech/post/ddd-lite-in-go-introduction/">DDD Lite</a>.
<strong>It can solve the mentioned problems in a much simpler way.</strong></p><h3 id="isnt-cqrs-a-complex-technique">Isn’t CQRS a complex technique?</h3><p>Isn’t CQRS one of these C#/Java/über enterprise patterns that are hard to implement, and make a big mess in the code?
A lot of books, presentations, and articles describe CQRS as a very complicated pattern.
But it is not the case.</p><p><strong>In practice, CQRS is a very simple pattern that doesn’t require a lot of investment.</strong>
<strong>It can be easily extended with more complex techniques like event-driven architecture, event-sourcing, or polyglot persistence.</strong>
But they’re not always needed.
Even without applying any extra patterns, CQRS can offer better decoupling, and code structure that is easier to understand.</p><p>When to not use CQRS in Go?
How to get all benefits from CQRS?
You can learn all that in today’s article. 😉</p><p>Like always, I will do it by refactoring <a href="https://github.com/ThreeDotsLabs/wild-workouts-go-ddd-example" target="_blank">Wild Workouts</a> application,</p><h3 id="how-to-implement-basic-cqrs-in-go">How to implement basic CQRS in Go</h3><p>CQRS (Command Query Responsibility Segregation) was initially <a href="https://cqrs.files.wordpress.com/2010/11/cqrs_documents.pdf" target="_blank">described by Greg Young</a>.
<strong>It has one simple assumption: instead of having one big model for reads and writes, you should have two separate models.
One for writes and one for reads.</strong>
It also introduces concepts of <em>command</em> and <em>query</em>, and leads to splitting application services into two separate types: command and query handlers.</p><p><img src="https://threedots.tech/media/introducing-cqrs/non-cqrs-architecture.jpg" alt="Non-CQRS architecture"></p><p>Standard, non-CQRS architecture</p><p><img src="https://threedots.tech/media/introducing-cqrs/cqrs-architecture.jpg" alt="CQRS architecture"></p><p>CQRS architecture</p><h4 id="command-vs-query">Command vs Query</h4><p>In simplest words: <strong>a Query should not modify anything, just return the data.
A command is the opposite one: it should make changes in the system, but not return any data.</strong>
Thanks to that, our queries can be cached more efficiently, and we lower the complexity of commands.</p><p>It may sound like a serious constraint, but in practice, it is not.
Most of the operations that we execute are reads or writes. Very rarely, both.</p><p>Of course, for a query, we don’t consider side effects like logs, or metrics as modifying anything.
For commands, it is also a perfectly normal thing to return an error.</p><p>How does the most basic implementation look in practice?
In the <a href="https://threedots.tech/post/introducing-clean-architecture/" target="_blank">previous article</a>,
Miłosz introduced an application service that executes application use cases.
Let’s start by cutting this service into separate command and query handlers.</p><h4 id="approvetrainingreschedule-command">ApproveTrainingReschedule command</h4><p>Previously, the training reschedule was approved from the application service <code>TrainingService</code>.</p><div><pre><code data-lang="diff"><span>- func (c TrainingService) ApproveTrainingReschedule(ctx context.Context, user auth.User, trainingUUID string) error {
</span><span>-  return c.repo.ApproveTrainingReschedule(ctx, trainingUUID, func(training Training) (Training, error) {
</span><span>-     if training.ProposedTime == nil {
</span><span>-        return Training{}, errors.New("training has no proposed time")
</span><span>-     }
</span><span>-     if training.MoveProposedBy == nil {
</span><span>-        return Training{}, errors.New("training has no MoveProposedBy")
</span><span>-     }
</span><span>-     if *training.MoveProposedBy == "trainer" &amp;&amp; training.UserUUID != user.UUID {
</span><span>-        return Training{}, errors.Errorf("user '%s' cannot approve reschedule of user '%s'", user.UUID, training.UserUUID)
</span><span>-     }
</span><span>-     if *training.MoveProposedBy == user.Role {
</span><span>-        return Training{}, errors.New("reschedule cannot be accepted by requesting person")
</span><span>-     }
</span><span>-
</span><span>-     training.Time = *training.ProposedTime
</span><span>-     training.ProposedTime = nil
</span><span>-
</span><span>-     return training, nil
</span><span>-  })
</span><span>- }
</span></code></pre></div><p>There were some magic validations there.
They are now done in the domain layer.
I also found out that we forgot to call the external <code>trainer</code> service to move the training. Oops. 😉
Let’s refactor it to the CQRS approach.</p><p>We start the implementation of a <em>command</em> with the command structure definition.
That structure provides all data needed to execute this command.
If a command has only one field, you can skip the structure and just pass it as a parameter.</p><p>It’s a good idea to use types defined by domain in the command, like <code>training.User</code> in that case.
We don’t need to do any casting later, and we have type safety assured.
<strong>It can save us a lot of issues with string parameters passed in wrong order.</strong></p><div><pre><code data-lang="go"><span>package</span> <span>command</span>

<span>// ...
</span><span></span>
<span>type</span> <span>ApproveTrainingReschedule</span> <span>struct</span> {
   <span>TrainingUUID</span> <span>string</span>
   <span>User</span>         <span>training</span>.<span>User</span>
}
</code></pre></div><p>The second part is a <em>command handler</em> that knows how to execute the command.</p><div><pre><code data-lang="go"><span>package</span> <span>command</span>

<span>// ...
</span><span></span>
<span>type</span> <span>ApproveTrainingRescheduleHandler</span> <span>struct</span> {
   <span>repo</span>           <span>training</span>.<span>Repository</span>
   <span>userService</span>    <span>UserService</span>
   <span>trainerService</span> <span>TrainerService</span>
}

<span>// ...
</span><span></span>
<span>func</span> (<span>h</span> <span>ApproveTrainingRescheduleHandler</span>) <span>Handle</span>(<span>ctx</span> <span>context</span>.<span>Context</span>, <span>cmd</span> <span>ApproveTrainingReschedule</span>) (<span>err</span> <span>error</span>) {
	<span>defer</span> <span>func</span>() {
		<span>logs</span>.<span>LogCommandExecution</span>(<span>"ApproveTrainingReschedule"</span>, <span>cmd</span>, <span>err</span>)
	}()

	<span>return</span> <span>h</span>.<span>repo</span>.<span>UpdateTraining</span>(
		<span>ctx</span>,
		<span>cmd</span>.<span>TrainingUUID</span>,
		<span>cmd</span>.<span>User</span>,
		<span>func</span>(<span>ctx</span> <span>context</span>.<span>Context</span>, <span>tr</span> <span>*</span><span>training</span>.<span>Training</span>) (<span>*</span><span>training</span>.<span>Training</span>, <span>error</span>) {
			<span>originalTrainingTime</span> <span>:=</span> <span>tr</span>.<span>Time</span>()

			<span>if</span> <span>err</span> <span>:=</span> <span>tr</span>.<span>ApproveReschedule</span>(<span>cmd</span>.<span>User</span>.<span>Type</span>()); <span>err</span> <span>!=</span> <span>nil</span> {
				<span>return</span> <span>nil</span>, <span>err</span>
			}

			<span>err</span> <span>:=</span> <span>h</span>.<span>trainerService</span>.<span>MoveTraining</span>(<span>ctx</span>, <span>tr</span>.<span>Time</span>(), <span>originalTrainingTime</span>)
			<span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
				<span>return</span> <span>nil</span>, <span>err</span>
			}

			<span>return</span> <span>tr</span>, <span>nil</span>
		},
	)
}
</code></pre></div><p>The flow is much easier to understand now.
You can clearly see that we approve a reschedule of a persisted <code>*training.Training</code>, and if it succeeds, we call the external <code>trainer</code> service.
Thanks to techniques described in the <a href="https://threedots.tech/post/ddd-lite-in-go-introduction/">DDD Lite article</a>, the command handler doesn’t need to know when it can perform this operation.
It’s all handled by our domain layer.</p><p>This clear flow is even more visible in more complex commands.
Fortunately, the current implementation is really straightforward.
That’s good.
<strong>Our goal is not to create complicated, but simple software.</strong></p><p>If CQRS is the standard way of building applications in your team, it also speeds up learning the service by your teammates who don’t know it.
You just need a list of available commands and queries, and to quickly take a look at how their execution works.
Jumping like crazy through random places in code is not needed.</p><p>This is how it looks like in one of my team’s most complex services:</p><p><img src="https://threedots.tech/media/introducing-cqrs/complex-command.jpg" alt="Example commands list of a complex application"></p><p>Example application layer of one service at <a href="https://www.karhoo.com/" rel="noreferrer" target="_blank">Karhoo</a>.</p><p>You may ask - shouldn’t it be cut to multiple services?
<strong>In practice, it would be a terrible idea.</strong> A lot of operations here need to be transitionally consistent.
Splitting it to separate services would involve a couple of distributed transactions (<em>Sagas</em>).
It would make this flow much more complex, harder to maintain, and debug.
It’s not the best deal.</p><p>It’s also worth mentioning that all of these operations are not very complex.
<strong>Complexity is scaling horizontally excellently here.</strong>
We will cover the extremely important topic of splitting microservices more in-depth soon.
Did I already mention that we messed it up in Wild Workouts on purpose? 😉</p><p>But let’s go back to our command. It’s time to use it in our HTTP port.
It’s available in <code>HttpServer</code> via injected <code>Application</code> structure, which contains all of our commands and queries handlers.</p><div><pre><code data-lang="go"><span>package</span> <span>app</span>

<span>import</span> (
   <span>"github.com/ThreeDotsLabs/wild-workouts-go-ddd-example/internal/trainings/app/command"</span>
   <span>"github.com/ThreeDotsLabs/wild-workouts-go-ddd-example/internal/trainings/app/query"</span>
)

<span>type</span> <span>Application</span> <span>struct</span> {
   <span>Commands</span> <span>Commands</span>
   <span>Queries</span>  <span>Queries</span>
}

<span>type</span> <span>Commands</span> <span>struct</span> {
   <span>ApproveTrainingReschedule</span> <span>command</span>.<span>ApproveTrainingRescheduleHandler</span>
   <span>CancelTraining</span>            <span>command</span>.<span>CancelTrainingHandler</span>
   <span>// ...
</span></code></pre></div><div><pre><code data-lang="go"><span>type</span> <span>HttpServer</span> <span>struct</span> {
   <span>app</span> <span>app</span>.<span>Application</span>
}

<span>// ...
</span><span></span>
<span>func</span> (<span>h</span> <span>HttpServer</span>) <span>ApproveRescheduleTraining</span>(<span>w</span> <span>http</span>.<span>ResponseWriter</span>, <span>r</span> <span>*</span><span>http</span>.<span>Request</span>) {
   <span>trainingUUID</span> <span>:=</span> <span>chi</span>.<span>URLParam</span>(<span>r</span>, <span>"trainingUUID"</span>)

   <span>user</span>, <span>err</span> <span>:=</span> <span>newDomainUserFromAuthUser</span>(<span>r</span>.<span>Context</span>())
   <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
      <span>httperr</span>.<span>RespondWithSlugError</span>(<span>err</span>, <span>w</span>, <span>r</span>)
      <span>return</span>
   }

   <span>err</span> = <span>h</span>.<span>app</span>.<span>Commands</span>.<span>ApproveTrainingReschedule</span>.<span>Handle</span>(<span>r</span>.<span>Context</span>(), <span>command</span>.<span>ApproveTrainingReschedule</span>{
      <span>User</span>:         <span>user</span>,
      <span>TrainingUUID</span>: <span>trainingUUID</span>,
   })
   <span>if</span> <span>err</span> <span>!=</span> <span>nil</span> {
      <span>httperr</span>.<span>RespondWithSlugError</span>(<span>err</span>, <span>w</span>, <span>r</span>)
      <span>return</span>
   }
}
</code></pre></div><p>The command handler can be called in that way from any port: HTTP, gRPC, or CLI.
It’s also useful for executing migrations and <a href="https://github.com/ThreeDotsLabs/wild-workouts-go-ddd-example/blob/22c0a25b67c4669d612a2fa4a434ffae8e35e65a/internal/trainer/fixtures.go#L62" target="_blank">loading fixtures</a> (we already do it in Wild Workouts).</p><h4 id="requesttrainingreschedule-command">RequestTrainingReschedule command</h4><p>Some command handlers can be very simple.</p><div><pre><code data-lang="go"><span>func</span> (<span>h</span> <span>RequestTrainingRescheduleHandler</span>) <span>Handle</span>(<span>ctx</span> <span>context</span>.<span>Context</span>, <span>cmd</span> <span>RequestTrainingReschedule</span>) (<span>err</span> <span>error</span>) {
	<span>defer</span> <span>func</span>() {
		<span>logs</span>.<span>LogCommandExecution</span>(<span>"RequestTrainingReschedule"</span>, <span>cmd</span>, <span>err</span>)
	}()

	<span>return</span> <span>h</span>.<span>repo</span>.<span>UpdateTraining</span>(
		<span>ctx</span>,
		<span>cmd</span>.<span>TrainingUUID</span>,
		<span>cmd</span>.<span>User</span>,
		<span>func</span>(<span>ctx</span> <span>context</span>.<span>Context</span>, <span>tr</span> <span>*</span><span>training</span>.<span>Training</span>) (<span>*</span><span>training</span>.<span>Training</span>, <span>error</span>) {
			<span>if</span> <span>err</span> <span>:=</span> <span>tr</span>.<span>UpdateNotes</span>(<span>cmd</span>.<span>NewNotes</span>); <span>err</span> <span>!=</span> <span>nil</span> {
				<span>return</span> <span>nil</span>, <span>err</span>
			}

			<span>tr</span>.<span>ProposeReschedule</span>(<span>cmd</span>.<span>NewTime</span>, <span>cmd</span>.<span>User</span>.<span>Type</span>())

			<span>return</span> <span>tr</span>, <span>nil</span>
		},
	)
}
</code></pre></div><p>It may be tempting to skip this layer …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://threedots.tech/post/basic-cqrs-in-go/">https://threedots.tech/post/basic-cqrs-in-go/</a></em></p>]]>
            </description>
            <link>https://threedots.tech/post/basic-cqrs-in-go/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24617237</guid>
            <pubDate>Mon, 28 Sep 2020 14:51:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why don't aid agencies like to hire local talent?]]>
            </title>
            <description>
<![CDATA[
Score 30 | Comments 13 (<a href="https://news.ycombinator.com/item?id=24617019">thread link</a>) | @filleduchaos
<br/>
September 28, 2020 | https://karmacolonialism.org/why-dont-aid-agencies-like-to-hire-local-talent/ | <a href="https://web.archive.org/web/*/https://karmacolonialism.org/why-dont-aid-agencies-like-to-hire-local-talent/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-carousel-extra="{&quot;blog_id&quot;:1,&quot;permalink&quot;:&quot;https:\/\/karmacolonialism.org\/why-dont-aid-agencies-like-to-hire-local-talent\/&quot;}"><div data-carousel-extra="{&quot;blog_id&quot;:1,&quot;permalink&quot;:&quot;https:\/\/karmacolonialism.org\/why-dont-aid-agencies-like-to-hire-local-talent\/&quot;}">
<p>When UNICEF needed photos from Bolivia, India, Jordan, Malawi, Myanmar and Niger to illustrate their clean-water campaign, did they hire local photographers in each of those countries?</p>



<p>No, they engaged an Australian man living in New York.</p>



<p>The photographer they chose was the award-winning Ashley Gilbertson, at VII Photo Agency, who produced good work for their “<a href="https://www.independent.co.uk/life-style/health-and-families/features/unicef-wateris-ashley-gilbertson-s-photo-essay-reveals-lengths-families-have-go-access-clean-water-a6715511.html">#Wateris: a family affair</a>” campaign. In my opinion, however, that’s no excuse for UNICEF hiring him rather than local photographers in each country.</p>



<p>But that’s how most Western aid agencies work. Three years later, UNICEF published “Crisis in the Central African Republic” (shown at top). Every single photo is tagged “Gilbertson VII Photo”.</p>



<figure><img data-attachment-id="4186" data-permalink="https://karmacolonialism.org/why-dont-aid-agencies-like-to-hire-local-talent/local-talent-v02/" data-orig-file="https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/09/Local-talent-v02.jpg?fit=1900%2C675&amp;ssl=1" data-orig-size="1900,675" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="Local talent v02" data-image-description="" data-medium-file="https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/09/Local-talent-v02.jpg?fit=300%2C107&amp;ssl=1" data-large-file="https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/09/Local-talent-v02.jpg?fit=584%2C208&amp;ssl=1" loading="lazy" width="584" height="208" src="https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/09/Local-talent-v02.jpg?resize=584%2C208&amp;ssl=1" alt="" srcset="https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/09/Local-talent-v02.jpg?resize=1024%2C364&amp;ssl=1 1024w, https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/09/Local-talent-v02.jpg?resize=300%2C107&amp;ssl=1 300w, https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/09/Local-talent-v02.jpg?resize=768%2C273&amp;ssl=1 768w, https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/09/Local-talent-v02.jpg?resize=1536%2C546&amp;ssl=1 1536w, https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/09/Local-talent-v02.jpg?resize=1200%2C426&amp;ssl=1 1200w, https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/09/Local-talent-v02.jpg?resize=500%2C178&amp;ssl=1 500w, https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/09/Local-talent-v02.jpg?w=1900&amp;ssl=1 1900w, https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/09/Local-talent-v02.jpg?w=1752&amp;ssl=1 1752w" sizes="(max-width: 584px) 100vw, 584px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/09/Local-talent-v02.jpg?resize=1024%2C364&amp;ssl=1 1024w, https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/09/Local-talent-v02.jpg?resize=300%2C107&amp;ssl=1 300w, https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/09/Local-talent-v02.jpg?resize=768%2C273&amp;ssl=1 768w, https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/09/Local-talent-v02.jpg?resize=1536%2C546&amp;ssl=1 1536w, https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/09/Local-talent-v02.jpg?resize=1200%2C426&amp;ssl=1 1200w, https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/09/Local-talent-v02.jpg?resize=500%2C178&amp;ssl=1 500w, https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/09/Local-talent-v02.jpg?w=1900&amp;ssl=1 1900w, https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/09/Local-talent-v02.jpg?w=1752&amp;ssl=1 1752w" data-lazy-src="https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/09/Local-talent-v02.jpg?resize=584%2C208&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption>Five reports from U.N. agencies and Western NGOs show life in the global South. Why don’t they use photographers who live there?</figcaption></figure>



<p>It’s not just UNICEF. The strip aboveshows reports published (often jointly) by World Vision, Save the Children, Plan International, ChildFund, and W.H.O., as well as UNICEF. The first report is titled<em> Progress on household drinking water, sanitation and hygiene 2000-2017; Special focus on inequalities.</em> It was published by UNICEF and the World Health Organization. The cover photograph is by an Italian man.</p>



<p>In these five reports, U.N. agencies and international NGOs tell how they help people in Africa, Asia and the Middle East. But the cover photos (and often most or all of the photos inside) were taken by Americans and Western Europeans.</p>



<p>If these organizations are concerned about inequalities, why don’t they make it a policy to always hire local talent in the countries they wish to help?</p>



<p>It seems like a no-brainer. Hiring local photographers would provide jobs where jobs are needed. It would help people in the global South to build their resumes and develop stronger skills. It would reward those who have invested in doing so. It would mean that aid money which is supposed to help these countries, would actually go <em>into </em>them without immediately bouncing back out. It would show respect for the local people, rather than treating them merely as a population in need of help.</p>



<p>Aid agencies typically hire local staff only for low- and mid-level positions. That’s an important story, but a different one. Here, I’m asking: Why do they so often pass up local talent when they need video work, graphic designers, and photographers, in favor of Westerners?</p>



<p>For some jobs, such as a big video project, they may argue that the skills needed aren’t available locally. Given the poor state of public education in most of these countries – much of which is a result of policies pushed by the U.N. – there may be some validity to this. But freezing out locals is not going to improve the situation, it simply creates greater inequality. At the very least, these agencies should insist on local involvement as much as possible. They do not.</p>



<p>In the case of photography, there’s simply no good excuse. Every country has photographers able to produce excellent work… and they’ll charge less than a globe-trotting Westerner. Why does the aid industry give so many assignments — especially the bigger ones with higher pay and travel opportunities and which are crucial for building up resumes — to Westerners?</p>



<p>Several factors are clearly at work.</p>



<p>First, it’s easier. Aid workers usually only stay in a country for one or two years. They socialize with other ex-pats, who often make a comfortable living by providing free-lance services to NGOs. Nurturing these connections comes naturally, and will benefit everybody’s career and social life.</p>



<p>Furthermore, the aid industry comes with an Us-and-Them mindset. <em><strong>We </strong></em>are the ones with answers, and we’re paid to help. <em><strong>They </strong></em>are the ones who need our help; they’ll be the subject of our photos. In this ethos, Westerners are the first people considered for any skilled job.</p>



<p>And there is a bias, whatever you wish to call it. The Western aid worker is more comfortable working with other Westerners. There are cultural, and often language, differences if you work with local talent. Aid workers may, without even consciously thinking about it, assume that people like themselves will do a better job.</p>



<p>But while we might understand all this, that doesn’t excuse it. Karma colonialism is the nicest term we can think of for this behavior.</p>



<h2>Twitter comments</h2>



<p><em>The tweet which announced this story has generated some lively commentary. Here are highlights.</em></p>



<p><strong>Taremwa karakire, @TaremwaD:</strong> I relate to this story. I was once part of a USAID project in Uganda where they always hired photographers all the way from USA to come and document the project.</p>



<p><strong>Learning counsel, @Its_Kimani: </strong>The writer dares not say the reasons as: (1) Perpetuating the White saviour complex; (2) Reinforcement of racist stereotypes by having people not tell their own story.<br>[<em>Author replies:</em> I decided it was more effective to let readers draw their own conclusions on this. I don’t know if that makes me a coward, or a diplomat. You, sir, are neither. Thanks for commenting.]</p>



<p><strong>RazorRibbon, @SpacemanAp:</strong> Because their photographer will capture Africa in the worse light possible and keep perpetuating the image of Africa they want the world to see and believe as per their real mission.<br>[<em>Author replies:</em> This is an important dynamic, and I missed it in the story. Several readers made similar comments, such as the next couple. Thank you, all.]</p>



<p><strong>Mana, @Jun1orm: </strong>A local photographer will do his best to make his community show the best it has to offer while a foreign one will do exactly what they want.</p>



<p><strong>iNk, @nwaoma007:</strong> The Australian man knows that his brief is to tell a specific facet of the story which promotes the narrative UNICEF wants to disseminate.</p>



<p><strong>Khanyi, @choolwekm:</strong> It seems to me they want to maintain the status quo to remain relevant.</p>



<p><strong>Sunflower, @TalelovesSuga:</strong> This is why I don’t like it when a white person points a camera at me… You can find yourself on a UNICEF poster.</p>



<p><strong>Ruby, @Oghenerume9:</strong> I had always known this in my heart but it was confirmed when I was working for one NGO and actually saw with my eyes that it was all fraud. Life has been worse for the poor in developing countries since more and more NGO’s came around</p>



<p><strong>O&amp;B, @The44Families: </strong>This is business people. You really believe UNICEF and Co want equality and self sufficiency in Africa, Asia, and Latin America?<br>Say they succeed is their purported goals, what then will be the reason(s) to keep them around?<br>They need the needy to be relevant</p>



<p><strong>Takor, @Takor61802157:</strong> Foreign bodies talk negative things about third world countries, they don’t take photos of the good things in third world countries only negative ones.</p>



<p><strong>david pearce, @davidpe11188501: </strong>A local head of UNRWA [United Nations Relief and Works Agency for Palestine Refugees in the Near East] told me that the children learn to cry on command for foreign visitors, esp photographers. Otherwise, the children look and act quite normally. They need crying children to sell their product.<br>[<em>Author adds:</em> In her book <em>The Crisis Caravan,</em> Dutch journalist Linda Polman records many stories of the quest for heart-rending photos. Of a camp for amputees in Sierra Leone, she writes: “Like pit bulls in a kindergarten, journalists from all over the world pounced on the story of the amputees. From CNN and the New York Times to Dutch public television and the South China Morning Post, they all managed to find Murray Town Camp. ‘It’s never been so easy to collect money as it is with the pictures of these poor devils,’ said an INGO staff member in Freetown.”]</p>



<p><strong>Faye Pixie, @PixieFaye:</strong> The Us and Them thing is so true. Even if some of these organizations are trying to be benign, they’re still being colonialists about our countries. Photography is inherently biased as you choose what you want to photograph to “spread awareness”.</p>



<p><strong>Benson Ndehi, @BNdehi: </strong>I once had a small gig with UNICEF in Nairobi. The waste was unbelievable. Everyone had a network printer, for example.<br>[<em>Author adds:</em> In his book<em> Another Quiet American,</em> Brett Dakin tells of when he was working in a government tourism office that got support from the U.N. Development Programme. One day the UNDP sent them a new printer, which could do everything: Print in color, collate, and bind. They had never asked for it, didn’t need it, and never figured out how to use it, so it sat unused. He writes, “The UNDP had simply decided to dump a few thousand dollars of gear on the NTA. Whether or not it would be of use was immaterial.”</p>



<h2>Related stories</h2>



<figure><ul data-carousel-extra="{&quot;blog_id&quot;:1,&quot;permalink&quot;:&quot;https:\/\/karmacolonialism.org\/why-dont-aid-agencies-like-to-hire-local-talent\/&quot;}"><li><figure><img data-attachment-id="2997" data-permalink="https://karmacolonialism.org/blog/shadow-ghostwriters-p-272-v02-thumb/" data-orig-file="https://i2.wp.com/karmacolonialism.org/wp-content/uploads/2020/05/shadow-ghostwriters-p-272-v02-thumb.jpg?fit=275%2C200&amp;ssl=1" data-orig-size="275,200" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="shadow ghostwriters -p-272- v02 thumb" data-image-description="" data-medium-file="https://i2.wp.com/karmacolonialism.org/wp-content/uploads/2020/05/shadow-ghostwriters-p-272-v02-thumb.jpg?fit=275%2C200&amp;ssl=1" data-large-file="https://i2.wp.com/karmacolonialism.org/wp-content/uploads/2020/05/shadow-ghostwriters-p-272-v02-thumb.jpg?fit=275%2C200&amp;ssl=1" loading="lazy" width="275" height="200" src="https://karmacolonialism.files.wordpress.com/2020/05/shadow-ghostwriters-p-272-v02-thumb.jpg?w=275&amp;resize=275%2C200" alt="" data-id="2997" data-full-url="https://karmacolonialism.files.wordpress.com/2020/05/shadow-ghostwriters-p-272-v02-thumb.jpg" data-recalc-dims="1" data-lazy-src="https://karmacolonialism.files.wordpress.com/2020/05/shadow-ghostwriters-p-272-v02-thumb.jpg?w=275&amp;is-pending-load=1#038;resize=275%2C200" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure></li><li><figure><img data-attachment-id="2824" data-permalink="https://karmacolonialism.org/why-does-the-aid-industry-resist-cash-transfers/cash-assistance-refugees-p-264-275x200/" data-orig-file="https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/05/cash-assistance-refugees-p-264-275x200-1.jpg?fit=275%2C200&amp;ssl=1" data-orig-size="275,200" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="cash assistance refugees -p-264- 275×200" data-image-description="" data-medium-file="https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/05/cash-assistance-refugees-p-264-275x200-1.jpg?fit=275%2C200&amp;ssl=1" data-large-file="https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/05/cash-assistance-refugees-p-264-275x200-1.jpg?fit=275%2C200&amp;ssl=1" loading="lazy" width="275" height="200" src="https://karmacolonialism.files.wordpress.com/2020/05/cash-assistance-refugees-p-264-275x200-1.jpg?w=275&amp;resize=275%2C200" alt="" data-id="2824" data-full-url="https://karmacolonialism.files.wordpress.com/2020/05/cash-assistance-refugees-p-264-275x200-1.jpg" data-recalc-dims="1" data-lazy-src="https://karmacolonialism.files.wordpress.com/2020/05/cash-assistance-refugees-p-264-275x200-1.jpg?w=275&amp;is-pending-load=1#038;resize=275%2C200" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure></li></ul><figcaption><strong>Left:</strong> <a href="https://karmacolonialism.org/the-shadow-government/">The Shadow Government.</a> UNICEF produces reports — not always good ones! — for weak governments to publish as their own. This ends up being even worse than it sounds.<br><strong>Right:</strong> <a href="https://karmacolonialism.org/cash-transfers-an-alternative-approach-to-aid/">Cash transfers.</a> Why not just give aid funds directly to the people you want to help? This approach has been done, results have been studied — and it proves quite effective.</figcaption></figure>



<figure><ul data-carousel-extra="{&quot;blog_id&quot;:1,&quot;permalink&quot;:&quot;https:\/\/karmacolonialism.org\/why-dont-aid-agencies-like-to-hire-local-talent\/&quot;}"><li><figure><img data-attachment-id="2213" data-permalink="https://karmacolonialism.org/dumping-on-the-global-south-and-getting-paid-for-it/100818-n-7241l-026-5/" data-orig-file="https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/04/seabees-djibouti-p-5-275x200-1.jpg?fit=275%2C200&amp;ssl=1" data-orig-size="275,200" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;11&quot;,&quot;credit&quot;:&quot;U.S. Navy&quot;,&quot;camera&quot;:&quot;NIKON D3&quot;,&quot;caption&quot;:&quot;100818-N-7241L-026\rDJIBOUTI (Aug. 18, 2010) Seabees assigned to Naval Mobile Construction Battalion (NMCB) 7, Combined Joint Task Force-Horn of Africa Detachment, prepare concrete forms at Ecole 5, a five-room school under construction near Camp Lemonnier. (U.S. Navy photo by Mass Communication Specialist 2nd Class Nathan Laird\/Released)&quot;,&quot;created_timestamp&quot;:&quot;1282156815&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;14&quot;,&quot;iso&quot;:&quot;200&quot;,&quot;shutter_speed&quot;:&quot;0.002&quot;,&quot;title&quot;:&quot;100818-N-7241L-026&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="100818-N-7241L-026" data-image-description="" data-medium-file="https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/04/seabees-djibouti-p-5-275x200-1.jpg?fit=275%2C200&amp;ssl=1" data-large-file="https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/04/seabees-djibouti-p-5-275x200-1.jpg?fit=275%2C200&amp;ssl=1" loading="lazy" width="275" height="200" src="https://karmacolonialism.files.wordpress.com/2020/04/seabees-djibouti-p-5-275x200-1.jpg?w=275&amp;resize=275%2C200" alt="" data-id="2213" data-full-url="https://karmacolonialism.files.wordpress.com/2020/04/seabees-djibouti-p-5-275x200-1.jpg" data-recalc-dims="1" data-lazy-src="https://karmacolonialism.files.wordpress.com/2020/04/seabees-djibouti-p-5-275x200-1.jpg?w=275&amp;is-pending-load=1#038;resize=275%2C200" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure></li><li><figure><img data-attachment-id="2207" data-permalink="https://karmacolonialism.org/dumping-on-the-global-south-and-getting-paid-for-it/needy-children-p-22-275x200-2/" data-orig-file="https://i2.wp.com/karmacolonialism.org/wp-content/uploads/2020/04/needy-children-p-22-275x200-1.jpg?fit=275%2C200&amp;ssl=1" data-orig-size="275,200" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="needy children -p-22- 275×200" data-image-description="" data-medium-file="https://i2.wp.com/karmacolonialism.org/wp-content/uploads/2020/04/needy-children-p-22-275x200-1.jpg?fit=275%2C200&amp;ssl=1" data-large-file="https://i2.wp.com/karmacolonialism.org/wp-content/uploads/2020/04/needy-children-p-22-275x200-1.jpg?fit=275%2C200&amp;ssl=1" loading="lazy" width="275" height="200" src="https://karmacolonialism.files.wordpress.com/2020/04/needy-children-p-22-275x200-1.jpg?w=275&amp;resize=275%2C200" alt="" data-id="2207" data-full-url="https://karmacolonialism.files.wordpress.com/2020/04/needy-children-p-22-275x200-1.jpg" data-recalc-dims="1" data-lazy-src="https://karmacolonialism.files.wordpress.com/2020/04/needy-children-p-22-275x200-1.jpg?w=275&amp;is-pending-load=1#038;resize=275%2C200" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure></li></ul><figcaption><strong>Left:</strong> <a href="https://karmacolonialism.org/pygmalion-and-golem-poor-expectations-get-poor-results/">Pygmalion and Golem.</a> U.S. Navy crew builds a school in Djibouti. That seems nice. But it sends a deeply harmful message: “You can’t do anything without our help.”<br><strong>Right:</strong> <a href="https://karmacolonialism.org/unicef-needs-the-needy/">Unicef needs the “needy.”</a> This photo is from a Unicef fundraising appeal for “needy families.” It is the very opposite of the “empowerment” that they talk about.</figcaption></figure>



<h2>Other stories of interest</h2>



<figure><ul data-carousel-extra="{&quot;blog_id&quot;:1,&quot;permalink&quot;:&quot;https:\/\/karmacolonialism.org\/why-dont-aid-agencies-like-to-hire-local-talent\/&quot;}"><li><figure><img data-attachment-id="2206" data-permalink="https://karmacolonialism.org/dumping-on-the-global-south-and-getting-paid-for-it/pope-francis-p-170-275x200-2/" data-orig-file="https://i1.wp.com/karmacolonialism.org/wp-content/uploads/2020/04/pope-francis-p-170-275x200-1.jpg?fit=275%2C200&amp;ssl=1" data-orig-size="275,200" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="pope francis -p-170- 275×200" data-image-description="" data-medium-file="https://i1.wp.com/karmacolonialism.org/wp-content/uploads/2020/04/pope-francis-p-170-275x200-1.jpg?fit=275%2C200&amp;ssl=1" data-large-file="https://i1.wp.com/karmacolonialism.org/wp-content/uploads/2020/04/pope-francis-p-170-275x200-1.jpg?fit=275%2C200&amp;ssl=1" loading="lazy" width="275" height="200" src="https://karmacolonialism.files.wordpress.com/2020/04/pope-francis-p-170-275x200-1.jpg?w=275&amp;resize=275%2C200" alt="" data-id="2206" data-full-url="https://karmacolonialism.files.wordpress.com/2020/04/pope-francis-p-170-275x200-1.jpg" data-recalc-dims="1" data-lazy-src="https://karmacolonialism.files.wordpress.com/2020/04/pope-francis-p-170-275x200-1.jpg?w=275&amp;is-pending-load=1#038;resize=275%2C200" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure></li><li><figure><img data-attachment-id="2211" data-permalink="https://karmacolonialism.org/dumping-on-the-global-south-and-getting-paid-for-it/superbowl-shirts-p-4-275x200-2/" data-orig-file="https://i1.wp.com/karmacolonialism.org/wp-content/uploads/2020/04/superbowl-shirts-p-4-275x200-1.jpg?fit=275%2C200&amp;ssl=1" data-orig-size="275,200" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="superbowl shirts -p-4- 275×200" data-image-description="" data-medium-file="https://i1.wp.com/karmacolonialism.org/wp-content/uploads/2020/04/superbowl-shirts-p-4-275x200-1.jpg?fit=275%2C200&amp;ssl=1" data-large-file="https://i1.wp.com/karmacolonialism.org/wp-content/uploads/2020/04/superbowl-shirts-p-4-275x200-1.jpg?fit=275%2C200&amp;ssl=1" loading="lazy" width="275" height="200" src="https://karmacolonialism.files.wordpress.com/2020/04/superbowl-shirts-p-4-275x200-1.jpg?w=275&amp;resize=275%2C200" alt="" data-id="2211" data-full-url="https://karmacolonialism.files.wordpress.com/2020/04/superbowl-shirts-p-4-275x200-1.jpg" data-recalc-dims="1" data-lazy-src="https://karmacolonialism.files.wordpress.com/2020/04/superbowl-shirts-p-4-275x200-1.jpg?w=275&amp;is-pending-load=1#038;resize=275%2C200" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure></li></ul><figcaption><strong>Left:</strong> <a href="https://karmacolonialism.org/who-says-its-colonialism/">The new colonialism.</a> Is the West really trying to impose a new form of colonialism? Many people have made the suggestion. Pope Francis, for example.<br><strong>Right:</strong> <a href="https://karmacolonialism.org/world-vision-passes-out-losing-team-tshirts/">World Vision</a> undermines local economies by giving away leftover merchandise where it’s not needed. It makes no sense — until you understand the financial incentives.</figcaption></figure>



<figure><ul data-carousel-extra="{&quot;blog_id&quot;:1,&quot;permalink&quot;:&quot;https:\/\/karmacolonialism.org\/why-dont-aid-agencies-like-to-hire-local-talent\/&quot;}"><li><figure><img data-attachment-id="2482" data-permalink="https://karmacolonialism.org/unesco-nokia-report-p-245-275x200/" data-orig-file="https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/04/unesco-nokia-report-p-245-275x200-1.jpg?fit=275%2C200&amp;ssl=1" data-orig-size="275,200" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="unesco nokia report -p-245- 275×200" data-image-description="" data-medium-file="https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/04/unesco-nokia-report-p-245-275x200-1.jpg?fit=275%2C200&amp;ssl=1" data-large-file="https://i0.wp.com/karmacolonialism.org/wp-content/uploads/2020/04/unesco-nokia-report-p-245-275x200-1.jpg?fit=275%2C200&amp;ssl=1" loading="lazy" width="275" height="200" src="https://karmacolonialism.files.wordpress.com/2020/04/unesco-nokia-report-p-245-275x200-1.jpg?w=275&amp;resize=275%2C200" alt="" data-id="2482" data-full-url="https://karmacolonialism.files.wordpress.com/2020/04/unesco-nokia-report-p-245-275x200-1.jpg" data-recalc-dims="1" data-lazy-src="https://karmacolonialism.files.wordpress.com/2020/04/unesco-nokia-report-p-245-275x200-1.jpg?w=275&amp;is-pending-load=1#038;resize=275%2C200" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure></li><li><figure><img data-attachment-id="3113" data-permalink="https://karmacolonialism.org/timor-where-did-all-the-aid-go/crystalball-seascape-p-208-275x200/" data-orig-file="https://i1.wp.com/karmacolonialism.org/wp-content/uploads/2020/06/crystalball-seascape-p-208-275x200-1.jpg?fit=275%2C200&amp;ssl=1" data-orig-size="275,200" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="crystalball seascape -p-208- 275×200" data-image-description="" data-medium-file="https://i1.wp.com/karmacolonialism.org/wp-content/uploads/2020/06/crystalball-seascape-p-208-275x200-1.jpg?fit=275%2C200&amp;ssl=1" data-large-file="https://i1.wp.com/karmacolonialism.org/wp-content/uploads/2020/06/crystalball-seascape-p-208-275x200-1.jpg?fit=275%2C200&amp;ssl=1" loading="lazy" width="275" height="200" src="https://karmacolonialism.files.wordpress.com/2020/06/crystalball-seascape-p-208-275x200-1.jpg?w=275&amp;resize=275%2C200" alt="" data-id="3113" data-full-url="https://karmacolonialism.files.wordpress.com/2020/06/crystalball-seascape-p-208-275x200-1.jpg" data-recalc-dims="1" data-lazy-src="https://karmacolonialism.files.wordpress.com/2020/06/crystalball-seascape-p-208-275x200-1.jpg?w=275&amp;is-pending-load=1#038;resize=275%2C200" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure></li></ul><figcaption><strong>Left:</strong> <a href="https://karmacolonialism.org/unesco-sells-its-brand-nokia-buys/">Cellphones and literacy.</a> UNESCO took money from big tech to publish a deceitful report that benefited the company that gave it the money. If an African president had done that, what would we call it?<br><strong>Right:</strong> <a href="https://karmacolonialism.org/better-ways-to-help/">What would make a better future?</a> There are ways that wealthier countries can genuinely help others, if they want to. Give the aid money …</figcaption></figure></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://karmacolonialism.org/why-dont-aid-agencies-like-to-hire-local-talent/">https://karmacolonialism.org/why-dont-aid-agencies-like-to-hire-local-talent/</a></em></p>]]>
            </description>
            <link>https://karmacolonialism.org/why-dont-aid-agencies-like-to-hire-local-talent/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24617019</guid>
            <pubDate>Mon, 28 Sep 2020 14:35:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[2 Months of Daily Blogging]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24616932">thread link</a>) | @mcrittenden
<br/>
September 28, 2020 | https://critter.blog/2020/09/28/2-months-of-daily-blogging/ | <a href="https://web.archive.org/web/*/https://critter.blog/2020/09/28/2-months-of-daily-blogging/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-content" role="main">

	
<article id="post-1598">

	
<!-- .entry-header -->

	<div>

		<div>

			
<p>For the past 8 weeks, I’ve posted to this blog every weekday. That’s 40 posts and counting. </p>



<figure><img data-attachment-id="1601" data-permalink="https://critter.blog/image-4-3/" data-orig-file="https://mikecrittendenhome.files.wordpress.com/2020/09/image-4.png" data-orig-size="787,234" data-comments-opened="0" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-4" data-image-description="" data-medium-file="https://mikecrittendenhome.files.wordpress.com/2020/09/image-4.png?w=300" data-large-file="https://mikecrittendenhome.files.wordpress.com/2020/09/image-4.png?w=580" src="https://mikecrittendenhome.files.wordpress.com/2020/09/image-4.png?w=787" alt="" srcset="https://mikecrittendenhome.files.wordpress.com/2020/09/image-4.png 787w, https://mikecrittendenhome.files.wordpress.com/2020/09/image-4.png?w=150 150w, https://mikecrittendenhome.files.wordpress.com/2020/09/image-4.png?w=300 300w, https://mikecrittendenhome.files.wordpress.com/2020/09/image-4.png?w=768 768w" sizes="(max-width: 787px) 100vw, 787px"></figure>



<p>It started as a personal experiment (because <a href="https://critter.blog/2020/09/21/experimenting-on-myself/">personal experiments are my new thing</a>):</p>



<figure><div>
<div><blockquote data-width="550" data-dnt="true"><p lang="en" dir="ltr">Inspired by Seth Godin, I'm gonna do an experiment for the month of August where I wrote a blog post every weekday. We'll see how much I hate this by the end.</p>— Mike Crittenden (@mcrittenden) <a href="https://twitter.com/mcrittenden/status/1290647256576729092?ref_src=twsrc%5Etfw">August 4, 2020</a></blockquote></div>
</div></figure>



<p>By the time August was up, I was having too much fun to stop.</p>



<hr>



<p>Here are a few of my original misconceptions:</p>



<p>“<em>I won’t be able to think of that many topics!</em>” Turns out, the more I write, the easier it is to think of more things to write about. My ideas list is growing faster than my published list. </p>



<p>“<em>It’ll start to feel like a chore.</em>” I’m not saying this will never happen, but it hasn’t happened yet. I still look forward to it every day. </p>



<p>“<em>It will take a lot of time, and some days I’ll be too busy.” </em>I got around this by telling myself that it’s fine if some posts are only a paragraph or two. My rule of thumb is “anything longer than a Tweet is long enough for a blog post.” Everyone should write more, <a href="https://mementomori.bearblog.dev/shorter/">shorter blog posts</a>.</p>



<p>“<em>Nobody will read it.” </em>First of all, I was wrong. I got a lot more traffic than I expected (see the stats at the bottom). But it didn’t matter anyway, because traffic is irrelevant. Writing without an audience is better than not writing, which brings me to:</p>



<p><i>“</i><em>Writing won’t benefit me much.” </em>Oh, past Critter, you sweet little idiot. Writing daily has helped me in two major ways:</p>



<p><strong>#1: I don’t know what I think about something until I start writing about it</strong>. I make connections and discover opinions I didn’t know existed in my brain. Yeah I know, “everyone knows that writing helps you think.” But experiencing it firsthand has been bizarre and fantastic.</p>



<p><strong>#2: It’s helpful to have a blog post handy when I get into a discussion with someone</strong>. Instead of summarizing my thoughts over and over again to different people, I can shoot them a link. <em>Beware, it’s tricky to do this without sounding like a know-it-all jerk</em>. But in the right context and with the right tone, it can work and save everyone a lot of time.</p>



<hr>



<p>Finally, some stats for those 8 weeks, if you’re curious:</p>



<ul><li>Total views: 67,198</li><li>Total visitors: 49,560</li><li>Most popular post: <a href="https://critter.blog/2020/08/11/that-coworker-who-never-stops-refactoring/">That coworker who never stops&nbsp;refactoring</a> (23,370 views)</li><li>2nd most popular post: <a href="https://critter.blog/2020/08/10/wiki-bankruptcy/">Wiki bankruptcy</a> (17,852 views)</li><li>Biggest referrer: Hacker News (responsible for 31,023 views)</li></ul>



<p>The <a href="https://en.wikipedia.org/wiki/Pareto_principle">Pareto Principle</a> (i.e., the 80/20 rule) is popping up here. Out of those 40 posts, the top 2 brought in almost 2/3 of the views. In fact, the top 8 posts (i.e., the top 20%) accounted for 85% of the views. So instead of 80/20 it’s <strong>85/20</strong> so far!</p>



<p>Another example: Hacker News gave me almost half of my traffic.</p>



<hr>



<p>I plan to keep the streak going as long as possible. Seth Goden says <a href="https://seths.blog/2018/10/the-first-1000-are-the-most-difficult/">the first 1000 are the most difficult</a>, and I’d like to see if that’s true.</p>



<p>If you’ve made it this far, <strong>I challenge you to write every day for just one week</strong>. If you want to stop at the end of the week, then stop. But if not, keep going and see where it takes you. </p>

		</div><!-- .entry-content -->

	</div><!-- .post-inner -->

	<!-- .section-inner -->

	
	<!-- .pagination-single -->

	
</article><!-- .post -->

</div></div>]]>
            </description>
            <link>https://critter.blog/2020/09/28/2-months-of-daily-blogging/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24616932</guid>
            <pubDate>Mon, 28 Sep 2020 14:27:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The failed promise of Web Components]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24616859">thread link</a>) | @flipchart
<br/>
September 28, 2020 | https://lea.verou.me/2020/09/the-failed-promise-of-web-components/ | <a href="https://web.archive.org/web/*/https://lea.verou.me/2020/09/the-failed-promise-of-web-components/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

		<div>

			
<p>Web Components had so much potential to empower HTML to do more, and make web development more accessible to non-programmers and easier for programmers. Remember how exciting it was every time we got new shiny HTML elements that actually <em>do stuff</em>? Remember how exciting it was to be able to do sliders, color pickers, dialogs, disclosure widgets straight in the HTML, without having to include any widget libraries?</p>



<p>The promise of Web Components was that we’d get this convenience, but for a much wider range of HTML elements, developed much faster, as nobody needs to wait for the full spec + implementation process. We’d just include a script, and boom, we have more elements at our disposal!</p>



<p>Or, that was the idea. Somewhere along the way, the space got flooded by JS frameworks aficionados, who revel in complex APIs, overengineered build processes and dependency graphs that look like the roots of a banyan tree.</p>



<figure><img src="https://live.staticflickr.com/2025/32441377780_e3acf6de12_b.jpg" alt=""><figcaption>This is what the roots of a Banyan tree look like. <a href="https://www.flickr.com/photos/79721788@N00/32441377780/">Photo by David Stanley on Flickr (CC-BY)</a>. </figcaption></figure>



<p>Perusing the components on <a href="https://www.webcomponents.org/">webcomponents.org</a> fills me with anxiety, and I’m perfectly comfortable writing JS — I write JS for a living! What hope do those who can’t write JS have? Using a custom element from the directory often needs to be preceded by a ritual of npm flugelhorn, import clownshoes, build quux, all completely unapologetically because “here is my truckload of dependencies, yeah, what”. Many steps are even omitted, likely because they are “obvious”. Often, you wade through the maze only to find the component doesn’t work anymore, or is not fit for your purpose.</p>



<p>Besides setup, the main problem is that HTML is not treated with the appropriate respect in the design of these components. They are not designed as closely as possible to standard HTML elements, but <em>expect</em> JS to be written for them to do anything. HTML is simply treated as a shorthand, or worse, as merely a marker to indicate where the element goes in the DOM, with all parameters passed in via JS. I recall <a href="https://adactio.com/articles/12839#webcomponents">a wonderful talk by Jeremy Keith</a> a few years ago about this very phenomenon, where he discussed <a href="https://shop.polymer-project.org/">this e-shop Web components demo by Google</a>, which is the poster child of this practice. These are the entire contents of its <code>&lt;body&gt;</code> element:</p>



<pre><code>&lt;body&gt;
	&lt;shop-app unresolved=""&gt;SHOP&lt;/shop-app&gt;
	&lt;script src="node_assets/@webcomponents/webcomponentsjs/webcomponents-loader.js"&gt;&lt;/script&gt;
	&lt;script type="module" src="src/shop-app.js"&gt;&lt;/script&gt;
	&lt;script&gt;window.performance&amp;&amp;performance.mark&amp;&amp;performance.mark("index.html");&lt;/script&gt;
&lt;/body&gt;</code></pre>



<p>If this is how Google is leading the way, how can we hope for contributors to design components that follow established HTML conventions?</p>



<p>Jeremy criticized this practice from the aspect of backwards compatibility: when JS is broken or not enabled, or the browser doesn’t support Web Components, the entire website is blank. While this is indeed a serious concern, my primary concern is one of <strong>usability</strong>: <strong>HTML is a lower barrier to entry language</strong>. Far more people can write HTML than JS. Even for those who do eventually write JS, it often comes after spending years writing HTML &amp; CSS.</p>



<p>If components are designed in a way that requires  JS, this excludes thousands of people from using them. And even for those who <em>can</em> write JS, HTML is often easier: you don’t see many people rolling their own sliders or using JS-based ones once <code>&lt;input type="range"&gt;</code> became widely supported, right?</p>



<p>Even when JS is unavoidable, it’s not black and white. A well designed HTML element can reduce the amount and complexity of JS needed to a minimum. Think of the <code><a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/dialog">&lt;dialog&gt;</a></code> element: it usually does require *some* JS, but it’s usually rather simple JS. Similarly, the <code>&lt;video&gt;</code> element is perfectly usable just by writing HTML, and has a comprehensive JS API for anyone who wants to do fancy custom things.</p>



<p>The other day I was looking for a simple, dependency free, tabs component. You know, the canonical example of something that is easy to do with Web Components, the example 50% of tutorials mention. I didn’t even care what it looked like, it was for a testing interface. I just wanted something that is small and works like a normal HTML element. Yet, it proved so hard I ended up writing my own!</p>



<h3>Can we fix this?</h3>



<p>I’m not sure if this is a design issue, or a documentation issue. Perhaps for many of these web components, there are easier ways to use them. Perhaps there are vanilla web components out there that I just can’t find. Perhaps I’m looking in the wrong place and there is another directory somewhere with different goals and a different target audience. </p>



<p>But if not, and if I’m not alone in feeling this way, we need a directory of web components with strict inclusion criteria:</p>



<ul><li><strong>Plug and play.</strong> No dependencies, no setup beyond including one <code>&lt;script&gt;</code> tag. If a dependency is absolutely <em>needed</em> (e.g. in a map component it doesn’t make sense to draw your own maps), the component loads it automatically if it’s not already loaded.</li><li>Syntax and API follows <a href="https://www.smashingmagazine.com/2017/02/designing-html-apis/"><strong>conventions established by built-in HTML elements</strong></a> and anything that <em>can</em> be done without the component user writing JS, <em>is</em> doable without JS, per <a href="https://www.w3.org/2001/tag/doc/leastPower.html">the W3C principle of least power</a>.</li><li><strong>Accessible by default</strong> via sensible ARIA defaults, just like normal HTML elements.</li><li><strong>Themable</strong> via <code><a href="https://developer.mozilla.org/en-US/docs/Web/CSS/::part">::part()</a></code>, selective inheritance and custom properties. Very minimal style by default. Normal CSS properties should just “work” to the the extent possible.</li><li><strong>Only one component of a given type</strong> in the directory, that is <strong>flexible</strong> and <strong>extensible</strong> and continuously iterated on and improved by the community. Not 30 different sliders and 15 different tabs that users have to wade through. No branding, no silos of “component libraries”. Only elements that are designed as closely as possible to what a browser would implement in every way the current technology allows.</li></ul>



<p>I would be up for working on this if others feel the same way, since that is not a project for one person to tackle. <em>Who’s with me?</em></p>



<p><strong>UPDATE:</strong> Wow this post blew up! Thank you all for your interest in participating in a potential future effort. I’m currently talking to stakeholders of some of the existing efforts to see if there are any potential collaborations before I go off and create a new one. <a href="https://twitter.com/leaverou">Follow me on Twitter to hear about the outcome</a>!</p>

		</div><!-- .entry-content -->

	</div></div>]]>
            </description>
            <link>https://lea.verou.me/2020/09/the-failed-promise-of-web-components/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24616859</guid>
            <pubDate>Mon, 28 Sep 2020 14:20:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Is Y Combinator worth it?]]>
            </title>
            <description>
<![CDATA[
Score 202 | Comments 119 (<a href="https://news.ycombinator.com/item?id=24616649">thread link</a>) | @gk1
<br/>
September 28, 2020 | https://drodio.com/our-ycombinator-experience/ | <a href="https://web.archive.org/web/*/https://drodio.com/our-ycombinator-experience/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://drodio.com/content/images/size/w300/2020/09/IMG_1431.jpg 300w,
                            https://drodio.com/content/images/size/w600/2020/09/IMG_1431.jpg 600w,
                            https://drodio.com/content/images/size/w1000/2020/09/IMG_1431.jpg 1000w,
                            https://drodio.com/content/images/size/w2000/2020/09/IMG_1431.jpg 2000w" sizes="(max-width: 800px) 400px,
                            (max-width: 1170px) 700px,
                            1400px" src="https://drodio.com/content/images/size/w2000/2020/09/IMG_1431.jpg" alt="Is Y Combinator worth it?">
            </figure>

            <section>
                <div>
                    <p>Back in late 2016, I started <a href="http://www.armory.io/">Armory </a>in my garage (pictured above) with my two co-founders. We also applied to Y Combinator. I often have founders ask me if YC is worth it right around now, during YC application season. </p><p>The short answer is <code>Yes, you should at least apply to see if you get in.</code> The longer answer is <code>It depends a lot more on <em>you </em>than YC (kind of like asking "is exercise worth it?").</code> Armory participated in the Winter 2017 batch, as well as the Fall 2019 Growth program.</p><p>YC is not just one thing. It's a series of people, experiences and resources that scale with your company as it grows. Here's a detailed look into what's been valuable, and what you can do to get full value from YC:</p><h2 id="optimizing-our-initial-w17-yc-batch-experience-">Optimizing our initial W17 YC &nbsp;batch experience:</h2><ul><li>YC's biggest initial benefit is that it creates intense focus in a short period of time, and at the end of the program, YC has nailed product-market-fit to help you pick up a good sized initial investment via Demo Day. If you hit it hard, you won't have any problem raising money. That alone is very valuable in the early stages of a startup's life, and it's a self-reinforcing cycle: The more initial traction you have, the more investment you can quickly pick up (and vice versa).</li><li>We secured several million dollars in SAFE notes in the span of a couple of days after demo day – and we could have picked up more if we'd wanted to. More on that below. It helps to be organized because you'll have to prioritize and manage a lot of investor interest in a short period of time. </li></ul><blockquote>I'd recommend using a Trello board like the one below to manage investor interest (you can <strong><a href="https://trello.com/b/jsHJLVMB/ycombinator-demo-day/drodio/recommend">make a copy of it here</a></strong>). This board is also valuable for future rounds; I used a variation of it for Armory's Series A and B as well <em>(that's a whole other blog post; LMK if you'd like me to write it).</em></blockquote><!--kg-card-begin: html--><p><img src="https://p-qkfvwn.b3.n0.cdn.getcloudapp.com/items/xQuL5Bz9/Screen%20Recording%202020-09-11%20at%2003.41.17%20PM.mp4" width="800"></p><!--kg-card-end: html--><ul><li>If you decide to apply to YC, the #1 piece of advice I can provide is <code>Show, Don't Tell.</code> The more you can show YC that you've already got early traction, the more likely it is that your application will be accepted for an interview, and that you'll get in (or at least invited back for a 2nd interview.) And it's good practice to focus on getting early, external validation, because that's what you'll be doing non-stop once you get into YC. Nicolae has <a href="https://medium.com/@nicolaerusan/modeling-what-startup-growth-actually-looks-like-73cc4720230e">a good breakdown here</a> of the 5% to 6% <em>weekly</em> growth that you should be targeting. The YC batch experience is an intense experience where you identify your top growth metric, focus on optimizing it, measure your results, and then do <em>whatever it takes </em>to unblock that growth. Rinse and repeat each week with all the founders around you doing the same thing. &nbsp;</li><li>You're more likely to be accepted if you are not a solo founder. I personally like the "hacker and hustler" combo for the fastest growth iterations &amp; unblocking. If you're afraid of getting strong external signals of validation for an MVP product, you won't do well in YC. One of my favorite sayings is <code>perfectionists are <em>imperfect </em>with their time.</code> And time is in short supply, with the Demo Day clock looming from the very first day of the program. A few other very appropriate sayings: <code>Build the right thing fast, instead of the wrong thing right,</code> and <code>There's <em>always </em>one more thing you can do [to increase growth]. And after that, there's <em>one more thing</em>.</code> I find that often, people are their own enemies when it comes to the headspace you need to be in to be a successful YC founder. Don't doubt yourself. Believe in your ability to create value. And don't be shy about being relentless in pursuing that external validation to prove it.</li></ul><p>Below is Armory's Demo Day video from 2017. I've blurred out some of our early customer names, but other than that, it's the full video. This is the first time it's been shown publicly. We spent the YC batch program getting our first customers to sign contracts (the "hustler" piece) while we built out the first version of Armory's platform (the "hacker" piece), which is built using an open source continuous software delivery project called <a href="http://www.spinnaker.io/">Spinnaker</a>. By the time Demo Day rolled around, we had multiple paid customer commitments and could articulate a long-term vision for Armory<em> (which has remained largely unchanged since our early days in the garage – that's also a whole other blog post!).</em> I've heard people say there's $100 Billion worth of VC funds in the room for YC Demo Day. I can believe that, and I wouldn't be surprised if it's well over that. </p><figure><iframe width="612" height="344" src="https://www.youtube.com/embed/IvZjbN2kil0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></figure><ul><li><strong>Other things we did well: </strong>YC has guest speakers come speak once weekly at dinner during the program, and there are really valuable nuggets of wisdom shared during those dinners. For example, Lyle Fong, the founding CEO of Lithium, gave a talk at a dinner about how he had to educate CMOs on the kind of social media platform Lithium was, so he made a maturity model diagram. That sparked Armory's creation of this "<a href="http://go.armory.io/stages">Stages of Software Delivery</a>" diagram, which we've used extensively to help Global 2,000 enterprises understand what their future pain points will be as they adopt the cloud, as well as the benefits they can expect to garner.</li><li><strong>Things we could have done better: </strong>We've never utilized YC Office Hours as effectively as we could have. There are YC partners and domain experts that make themselves available to talk through challenges a business is having. I recommend leaning into that more than we did.</li></ul><h2 id="optimizing-yc-s-growth-program-ycg-f19-">Optimizing YC's Growth Program (YCG F19):</h2><figure><img src="https://drodio.com/content/images/2020/09/0CC9DBF0-5451-4BDD-8F7F-2F1BEDA3D330.jpeg" alt="" srcset="https://drodio.com/content/images/size/w600/2020/09/0CC9DBF0-5451-4BDD-8F7F-2F1BEDA3D330.jpeg 600w, https://drodio.com/content/images/size/w1000/2020/09/0CC9DBF0-5451-4BDD-8F7F-2F1BEDA3D330.jpeg 1000w, https://drodio.com/content/images/size/w1600/2020/09/0CC9DBF0-5451-4BDD-8F7F-2F1BEDA3D330.jpeg 1600w, https://drodio.com/content/images/2020/09/0CC9DBF0-5451-4BDD-8F7F-2F1BEDA3D330.jpeg 2388w" sizes="(min-width: 720px) 720px"></figure><p>In the fall of 2019, I participated in the "graduate" YC program, for YC companies that are scaling. I consider this program to be even more valuable than the initial YC batch experience. It's more intimate – our group was about 15 CEOs – and everyone is dealing with (or has just dealt with) the same types of issues. If the first YC batch experience is about figuring how to drive relentless early traction and growth, the YC growth program is about figuring out how to scale that growth, in every sense of the word – how to scale your functions, your executives, your culture, your size, your<em>self</em>. And the vibe is entirely different: In the early YC batch days, you're just trying to make your startup <em>become relevant and survive.</em> But in the Growth program, it's clear that all the startups who have made it this far are solving a real problem for the world, and have a lot of growth in front of them. And the problems are much more abstract – more about how to align <em>people and processes </em>to scale the magic that's made the business get this far. I've made some incredible, true friendships from this group, and we continue to help each other via WhatsApp and meet on a regular basis. </p><ul><li>My #1 tip is to capture as much of the content as you can to share in permissioned ways with your executive team and managers. By the time you participate in this program, your company will have grown well beyond just the founders. Many of the great tips you'll hear will come from the dinners you attend, as well as how your fellow CEOs have managed similar issues. I highly recommend <a href="http://go.armory.io/Otter">installing Otter.ai on your phone</a> so you can ask the person sharing pro-tips if you can record it and make an auto-transcript to share with your execs. I did this a bunch during the program <em>(Oleg, I'm looking at you, buddy!).</em></li></ul><p>If you have the chance to do the YC Growth program, you should absolutely do it. <em>(I'll invite the CEOs from our batch to leave more thoughts in the comments below).</em></p><h2 id="the-big-thing-to-understand-about-yc-before-you-join-">The big thing to understand about YC <em>before </em>you join:</h2><p>So yes, YC is very much worthwhile. What you get out of YC will very much be dependent on what you put into it. I haven't even mentioned the internal YC forum (cheekily called "BookFace") where several thousand YC alums serve as a resource for each other, as well as YC's <a href="https://www.workatastartup.com/">Work at a Startup</a> recruiting tool, the private VC resource rating directory, YC's <a href="https://blog.ycombinator.com/ycs-series-a-guide/">Series A program</a>, and other similar resources. &nbsp;</p><p>Here's the main thing you need to understand before you apply: YC has <a href="https://www.ycombinator.com/deal/">a "standard deal"</a> where they take a 7% stake in your company. YC also gets pro-rata rights in future funding rounds. If your company is really successful, you may find yourself in a position where you need to negotiate some of those pro-rata rights in order to make the round work (i.e., make enough room to accomodate new new investor demand). While YC will do its best to work with you based on your specific situation, you need to understand that you're signing up for a standard deal up-front, and that early seed-stage commitment may upset other investors. Other angel/seed/Series A investors can become unhappy when having to live on a cap table alongside YC.</p><p>Hope that helps! Feel free to ask any questions in the comments – and if you're also a YC alum, please share your pro-tips as well!</p>
                </div>
            </section>


            

            


        </article>

    </div>
</div></div>]]>
            </description>
            <link>https://drodio.com/our-ycombinator-experience/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24616649</guid>
            <pubDate>Mon, 28 Sep 2020 13:59:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Mental Models for Challenging Problems]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24616598">thread link</a>) | @nicotesla
<br/>
September 28, 2020 | https://blog.codelitt.com/5-mental-models/ | <a href="https://web.archive.org/web/*/https://blog.codelitt.com/5-mental-models/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section id="Post__Main-Content">
        <div>
          <p>I'm an avid reader. For a long time now, my primary source of information has come in the form of newsletters. The main advantage of the newsletter format for me is that I can surround myself with information sources I trust. This approach makes it easy to organize the content I'm interested in. Of course, my interests are always evolving depending on what's happening at any given moment in my life. I try not to replace the types of information I expose myself to, but <em>am</em> always trying to find the best sources. It's great because on any given day I can find an article about the latest release of a JS library alongside one about Engineering leadership and management.</p><p>A while ago I started noticing that most of my newsletters would share an article or two on <strong>mental models</strong>. I loved the name but didn't really engage with the idea initially. One day, I made the decision to bookmark <a href="https://www.julian.com/blog/mental-model-examples">this article</a> and promised myself that I would read it someday. I suppose I was tired of seeing headlines about the concept (and remaining uninformed about it). </p><figure><img src="https://res-1.cloudinary.com/hepw4b4yn/image/upload/q_auto/v1/ghost-blog-images/mental-models-engineers-2.jpg"></figure><p>The day I decided to dive into the topic, I immediately found myself wishing that I'd taken it up sooner. Being the CTO at Codelitt, I spend a considerable chunk of my energy on decision making. Logically, this means that any improvement in this area would mean a big return in time and effort which could then be applied elsewhere in the company. </p><p>The following is an excerpt from <a href="https://www.julian.com/blog/mental-model-examples">Julian's blog</a> where he provides an awesome explanation on the subject.</p><blockquote>Mental models do two things: they help you assess how systems work and they help you make better decisions. These two concepts underlie everything you do.<p>For example, how does a rocket engine work? And which type of rocket fuel should you use?</p><p>The rocket engine is a system for you to assess. It has many parts that depend on each other, and you want to understand how. As for which type of fuel to use, that's a decision you make.</p></blockquote><p>Below we have a few mental models that I find myself using on a daily basis when making software engineering decisions. Each concept has a summary that is pulled directly from one source - <a href="https://fs.blog/">Shane Parrish's Farnam Street blog</a> - which I highly recommend.</p><h3 id="redundancy">Redundancy</h3><p>A good engineer never assumes the perfect reliability of the components of the system. He or she builds in redundancy to protect the integrity of the total system. Without the application of this robustness principle, tangible and intangible systems tend to fail over time.</p><h3 id="bottlenecks">Bottlenecks</h3><p>A bottleneck describes the place at which a flow (of a tangible or intangible) is stopped, thus holding it back from continuous movement. As with a clogged artery or a blocked drain, a bottleneck in production of any good or service can be small but have a disproportionate impact if it is in the critical path.</p><figure><img src="https://res-4.cloudinary.com/hepw4b4yn/image/upload/q_auto/v1/ghost-blog-images/mental-models-engineers-3.jpg"></figure><h3 id="emergence">Emergence</h3><p>Higher-level behavior tends to emerge from the interaction of lower-order components. The result is frequently not linear – not a matter of simple addition – but rather non-linear, or exponential. An important resulting property of emergent behavior is that it cannot be predicted from simply studying the component parts.</p><h3 id="first-principles-thinking">First Principles Thinking</h3><p>First principles thinking is one of the best ways to reverse-engineer complicated situations and unleash creative possibility. Sometimes called reasoning from first principles, it’s a tool to help clarify complicated problems by separating the underlying ideas or facts from any assumptions based on them. What remains are the essentials. If you know the first principles of something, you can build the rest of your knowledge around them to produce something new.</p><h3 id="first-conclusion-bias">First-Conclusion Bias</h3><p>As Charlie Munger famously pointed out, the mind works a bit like a sperm and egg: the first idea gets in and then the mind shuts. Like many other tendencies, this is probably an energy-saving device. Our tendency to settle on first conclusions leads us to accept many erroneous results and cease asking questions; it can be countered with some simple and useful mental routines.</p><p>If you would like to see more on mental models, check out the <a href="https://fs.blog/mental-models/">Farnan Street blog</a>.</p><hr><p>Want to read more about how Kaio thinks?</p><ul><li><a href="https://blog.codelitt.com/contractor-to-cto/">Check out his journey to become CTO of Codelitt</a></li><li><a href="https://blog.codelitt.com/design-system/">Read about his perspective on the importance of establishing a design system</a></li></ul>
        </div>
      </section></div>]]>
            </description>
            <link>https://blog.codelitt.com/5-mental-models/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24616598</guid>
            <pubDate>Mon, 28 Sep 2020 13:54:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why you need a service registry]]>
            </title>
            <description>
<![CDATA[
Score 9 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24616442">thread link</a>) | @lawrjone
<br/>
September 28, 2020 | https://blog.lawrencejones.dev/service-registry/ | <a href="https://web.archive.org/web/*/https://blog.lawrencejones.dev/service-registry/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><p>Building a service registry, a structure that tracks all people, services and
systems that interact with your infrastructure, can be extremely powerful.</p>

<p>If you pick the right format, it can be the glue between totally distinct
toolchains. Placing the registry at the heart of all your other tools means you
no longer need to worry about keeping it up-to-date: the registry defines what
is created, rather than describing it.</p>

<p>By distributing the registry so every developer, infrastructure component or
one-off script can easily read it, you’ll find use cases for this data
everywhere. You can even push this data into systems like your monitoring stack,
allowing automated systems to make decisions on the ownership information it
provides.</p>

<p>As part of a revamp of our infrastructure tooling, we’ve introduced a service
registry into GoCardless. This post explains how we built the registry and some
of the use cases we’ve found for it.</p>



<p>The GoCardless service registry is a <a href="https://jsonnet.org/">Jsonnet</a> library, stored as a
file inside the same Git repository that contains our infrastructure
configuration. Jsonnet, for those not familiar, is an extension to JSON that
aims to support flexible reuse and customisation of data structures.</p>

<p>Jsonnet files evaluate to JSON, and the service registry is no different:</p>

<div><div><pre><code><span>$</span><span> </span>jsonnet registry.jsonnet
<span>{
  "clusters": [...],
  "services": [...],
  "teams": [...],
  "projects": [...],
  ...,
}
</span></code></pre></div></div>

<p>Perhaps you thought a service registry was a webserver, maybe hooked up to a
database, serving the data via a REST API? That wouldn’t be strange, and there
are many systems that do just that, but I’d suggest the approach of building a
registry out of a single JSON file (compiled from whatever templating language
you choose, be it Jsonnet or otherwise) has several advantages:</p>

<ul>
  <li>JSON files are so universally compatible that you’ll be able to use this
anywhere</li>
  <li>If you’ve already adopted Git-ops workflows, tracking changes to the registry
in Git should feel very natural</li>
  <li>It’s just data, and your registry is only as good as the data you put in it.
Removing the distraction of building an API means you encourage a focus on
building the right data model, which is what really matters</li>
</ul>

<p>From the output of the <code>jsonnet registry.jsonnet</code> command, you can see we’re
tracking our Kubernetes <code>clusters</code>, any <code>services</code> that we run, organisation
<code>teams</code> who interact with the services, and Google Cloud Platform <code>projects</code>.</p>

<p>You don’t need to start by tracking all these types, but the simplicity of a
Jsonnet library means it costs very little to add a new type. We began with
<code>services</code>, then wanted to ensure no service referenced an invalid team. It was
a natural evolution to add <code>teams</code>, and this pattern has happened many times
over.</p>

<h2 id="service-entry-make-it-rain">Service entry (make-it-rain)</h2>

<p>Our registry began as a list of services, where each service had a
<code>metadata.jsonnet</code> that defined its service entry.</p>

<p>For the purpose of this post, imagine we have a (fake) service called
<code>make-it-rain</code>, which has a service entry that looks like this:</p>

<div><div><pre><code><span>// Example service called make-it-rain, powering a dashboard of falling</span>
<span>// gold coins whenever anyone takes a payment via GoCardless.</span>
<span>//</span>
<span>// Banking teams love money, which is why they created this dashboard.</span>
<span>// It's officially owned by banking-integrations, but core-banking</span>
<span>// sometimes optimise the React code.</span>
<span>//</span>
<span>// It consumes data about new payments from Google Pub/Sub, and has a</span>
<span>// separate Google Cloud Platform project for each of its environments,</span>
<span>// of which there are two: staging and production.</span>
<span>service</span><span>.</span><span>new</span><span>(</span><span>'make-it-rain'</span><span>,</span> <span>'gocardless/make-it-rain'</span><span>)</span> <span>+</span>
<span>service</span><span>.</span><span>mixin</span><span>.</span><span>withTeam</span><span>(</span><span>'banking-integrations'</span><span>)</span> <span>+</span>
<span>service</span><span>.</span><span>mixin</span><span>.</span><span>withAlertsChannel</span><span>(</span><span>'make-it-rain-alerts'</span><span>)</span> <span>+</span>
<span>service</span><span>.</span><span>mixin</span><span>.</span><span>withGoogleServices</span><span>([</span>
  <span>'pubsub.googleapis.com'</span><span>,</span>
<span>])</span> <span>+</span>
<span>service</span><span>.</span><span>mixin</span><span>.</span><span>withEnvironments</span><span>([</span>
  <span>environment</span><span>.</span><span>map</span><span>(</span>
    <span>// By default, every environment should have banking-integrations as</span>
    <span>// admins, and core-banking as operators (they provide on-call cover</span>
    <span>// for the falling gold coins).</span>
    <span>environment</span><span>.</span><span>mixin</span><span>.</span><span>rbac</span><span>.</span><span>withAdmins</span><span>(</span><span>'banking-integrations'</span><span>)</span> <span>+</span>
    <span>environment</span><span>.</span><span>mixin</span><span>.</span><span>rbac</span><span>.</span><span>withOperators</span><span>(</span><span>'core-banking'</span><span>),</span>
    <span>function</span><span>(</span><span>environment</span><span>)</span> <span>[</span>
      <span>environment</span><span>.</span><span>new</span><span>(</span><span>'staging'</span><span>)</span> <span>+</span>
      <span>environment</span><span>.</span><span>mixin</span><span>.</span><span>withGoogleProject</span><span>(</span><span>'gc-prd-make-it-stag-833e'</span><span>)</span> <span>+</span>
      <span>environment</span><span>.</span><span>mixin</span><span>.</span><span>withTargets</span><span>([</span>
        <span>argocd</span><span>.</span><span>new</span><span>(</span><span>cluster</span><span>=</span><span>'compute-staging-brava'</span><span>,</span> <span>namespace</span><span>=</span><span>'make-it-rain'</span><span>),</span>
      <span>]),</span>
      <span>// Unlike most services, the production environment should permit</span>
      <span>// a non-engineering team to open consoles. Sometimes we take a</span>
      <span>// manual payment outside of GoCardless, and banking-operations</span>
      <span>// open a make-it-rain console and run a script, so we don't miss</span>
      <span>// any gold coins.</span>
      <span>environment</span><span>.</span><span>new</span><span>(</span><span>'production'</span><span>)</span> <span>+</span>
      <span>environment</span><span>.</span><span>mixin</span><span>.</span><span>rbac</span><span>.</span><span>withOperatorsMixin</span><span>(</span><span>'banking-operations'</span><span>)</span> <span>+</span>
      <span>environment</span><span>.</span><span>mixin</span><span>.</span><span>withGoogleProject</span><span>(</span><span>'gc-prd-make-it-prod-1eb1'</span><span>)</span> <span>+</span>
      <span>environment</span><span>.</span><span>mixin</span><span>.</span><span>withTargets</span><span>([</span>
        <span>argocd</span><span>.</span><span>new</span><span>(</span><span>cluster</span><span>=</span><span>'compute-banking'</span><span>,</span> <span>namespace</span><span>=</span><span>'make-it-rain'</span><span>),</span>
      <span>]),</span>
    <span>],</span>
  <span>),</span>
<span>])</span>
</code></pre></div></div>

<p>Take a moment to read the Jsonnet- this produces a JSON structure that you can
see <a href="https://gist.github.com/lawrencejones/b209a1a5da864b987cbedb1dffef6116#file-make-it-rain-json">here</a>. It includes a definition of the service and
all its environments, with a list of deployment targets for each environment
that defines where the deployment lives.</p>

<p>There’s some configuration of team permissions and Google Cloud Platform
references- we’ll see how we can use them next.</p>



<p>Once you have a list of service entries like make-it-rain, we can use it
to tightly integrate with all the rest of our infrastructure tools.</p>

<p>Most infrastructure teams deal with many (in my mind, too many) tools.  The
GoCardless team provisions infrastructure with <a href="https://www.terraform.io/">terraform</a>, manages
virtual machines with <a href="https://www.chef.io/products/chef-infra">Chef</a>, and <a href="https://kubernetes.io/">Kubernetes</a> resources with
Jsonnet templating. Other teams may use far more.</p>

<p>Thankfully, our service registry is plain ol’ JSON, and easily consumed by all
of these tools. Once imported, we can begin provisioning infrastructure in
response to changes in the registry. This is a change from the registry
describing the infrastructure at a point-in-time, to becoming the definition
what really exists.</p>

<p>When you must update registry to create infrastructure, you guarantee the
registry is up-to-date, and know it can no longer become stale. This allows you
to trust the registry in use-cases that weren’t possible if it could fall
out-of-date, a benefit we’ll see when we integrate it with our
<a href="#tooling">tools</a>.</p>

<p>Let’s see how this works in practice.</p>

<h2 id="kubernetes">Kubernetes</h2>

<p>When someone creates a service like make-it-rain, we’ll import their entry into
the registry. Our CD pipelines will detect a registry change and begin
provisioning core resources required for every service.</p>

<p>First, we have a Jsonnet templated cluster service that we use to create
privileged Kubernetes cluster resources, such as namespaces. As the templating
imports the registry as just another Jsonnet file, it will detect we’re missing
a namespace (<code>make-it-rain</code>) in the <code>compute-staging-brava</code> and
<code>compute-banking</code> clusters, and automatically create them.</p>

<p>After we have a namespace, we’ll create the supporting resources.  Included in
this are resource quotas, limiting the amount of cluster resource make-it-rain
could consume- these limits can be tweaked or overriden in the cluster service
Jsonnet:</p>

<div><div><pre><code><span>// utopia/services/cluster/instances/compute-staging.jsonnet</span>
<span>cluster</span> <span>{</span>
  <span>spaces</span><span>+:</span> <span>{</span>
    <span>'make-it-rain'</span><span>+:</span> <span>{</span>
      <span>quota</span><span>+:</span> <span>{</span>
        <span>spec</span><span>+:</span> <span>{</span>
          <span>// These React apps are getting crazy...</span>
          <span>hard</span><span>+:</span> <span>{</span> <span>cpu</span><span>:</span> <span>'32'</span><span>,</span> <span>memory</span><span>:</span> <span>'32Gi'</span> <span>},</span>
        <span>},</span>
      <span>},</span>
    <span>},</span>
  <span>},</span>
<span>}</span>
</code></pre></div></div>

<p>Permissions are one of the more complicated things about managing Kubernetes
clusters. Especially when aiming for a Devops workflow, with application
engineers empowered to care for their own Kubernetes resources, you want to
establish a consistent permission model up-front. Consistency means you can
accurately describe your security stance for audits, and helps maintain
productivity for engineers who work across multiple teams.</p>

<p>Your registry, being the authoritative definition of service RBAC, can be used
to power your Kubernetes RBAC and enforce that consistency. Looking at our
make-it-rain production environment, we can see the RBAC fields:</p>

<div><div><pre><code><span>{</span><span>
  </span><span>"type"</span><span>:</span><span> </span><span>"Service"</span><span>,</span><span>
  </span><span>"spec"</span><span>:</span><span> </span><span>{</span><span>
    </span><span>"name"</span><span>:</span><span> </span><span>"make-it-rain"</span><span>,</span><span>
    </span><span>"repository"</span><span>:</span><span> </span><span>"gocardless/make-it-rain"</span><span>,</span><span>
    </span><span>"team"</span><span>:</span><span> </span><span>"banking-integrations"</span><span>,</span><span>
    </span><span>"environments"</span><span>:</span><span> </span><span>[</span><span>
      </span><span>{</span><span>
        </span><span>"type"</span><span>:</span><span> </span><span>"Environment"</span><span>,</span><span>
        </span><span>"spec"</span><span>:</span><span> </span><span>{</span><span>
          </span><span>"name"</span><span>:</span><span> </span><span>"staging"</span><span>,</span><span>
          </span><span>"rbac"</span><span>:</span><span> </span><span>{</span><span>
            </span><span>"admins"</span><span>:</span><span> </span><span>[</span><span>
              </span><span>"banking-integrations"</span><span>
            </span><span>],</span><span>
            </span><span>"operators"</span><span>:</span><span> </span><span>[</span><span>
              </span><span>"core-banking"</span><span>
            </span><span>],</span><span>
            </span><span>"viewers"</span><span>:</span><span> </span><span>[]</span><span>
          </span><span>}</span><span>
        </span><span>}</span><span>
      </span><span>},</span><span>
      </span><span>...</span><span>
    </span><span>]</span><span>
  </span><span>}</span><span>
</span><span>}</span><span>
</span></code></pre></div></div>

<p>We made a decision to model just three roles for a service, viewer, operator and
admin- from our experience, it seems this is flexible enough for almost all use
cases. We thought it would be great if all permissions granted to humans were
derived from these member lists, instead of scattering the membership across our
infrastructure configuration (Kubernetes, terraform, Chef).</p>

<p>Now we have our registry, we can do just that. Using Kubernetes permissions as
an example, it’s simple to:</p>

<ul>
  <li>Identify the list of teams who are viewers, operators, or admins for any
services that exist within each cluster namespace</li>
  <li>Use these lists to create <code>RoleBinding</code>s in the service namespace, granting
appropriate permissions to each member of the roles</li>
</ul>

<p>We implement this in a single file, <code>cluster/app/spaces-rbac.jsonnet</code>, which
allows us to map over all namespaces in a cluster and provision the
<code>RoleBinding</code> Kubernetes resources. Jsonnet is great for this type of data
manipulation, proving–yet again!–how using a static registry does not limit how
flexibly you can query the data.</p>

<h2 id="google-cloud-platform">Google Cloud Platform</h2>

<p>It’s not just Kubernetes resources in Jsonnet, though. GoCardless is a heavy
user of Google Cloud Platform, and if this permission model is sound, we should
be able to apply it to our Cloud estate too.</p>

<p>For this, we have a …</p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.lawrencejones.dev/service-registry/">https://blog.lawrencejones.dev/service-registry/</a></em></p>]]>
            </description>
            <link>https://blog.lawrencejones.dev/service-registry/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24616442</guid>
            <pubDate>Mon, 28 Sep 2020 13:35:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Zig's New Relationship with LLVM]]>
            </title>
            <description>
<![CDATA[
Score 430 | Comments 262 (<a href="https://news.ycombinator.com/item?id=24615916">thread link</a>) | @todsacerdoti
<br/>
September 28, 2020 | https://kristoff.it/blog/zig-new-relationship-llvm/ | <a href="https://web.archive.org/web/*/https://kristoff.it/blog/zig-new-relationship-llvm/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>While not yet at version 1.0, Zig is about to reach a new level of maturity and stability.</p><div><p>In the early days, Zig was but a thin frontend in front of LLVM. This was instrumental for getting started quickly and filling in gaps of Andrew’s knowledge as a compiler developer. Now, the training wheels of the bicycle are coming off, and LLVM is transitioning into an optional component.</p>
<p><span>
      <a href="https://kristoff.it/static/2cc08c37498f0cec888734c477d5f358/98017/protty1.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
        <source srcset="https://kristoff.it/static/2cc08c37498f0cec888734c477d5f358/f5e3c/protty1.webp 100w,
https://kristoff.it/static/2cc08c37498f0cec888734c477d5f358/f2fbe/protty1.webp 200w,
https://kristoff.it/static/2cc08c37498f0cec888734c477d5f358/e227a/protty1.webp 400w,
https://kristoff.it/static/2cc08c37498f0cec888734c477d5f358/efddf/protty1.webp 441w" sizes="(max-width: 400px) 100vw, 400px" type="image/webp">
        <source srcset="https://kristoff.it/static/2cc08c37498f0cec888734c477d5f358/c0399/protty1.png 100w,
https://kristoff.it/static/2cc08c37498f0cec888734c477d5f358/9ec3c/protty1.png 200w,
https://kristoff.it/static/2cc08c37498f0cec888734c477d5f358/c7805/protty1.png 400w,
https://kristoff.it/static/2cc08c37498f0cec888734c477d5f358/98017/protty1.png 441w" sizes="(max-width: 400px) 100vw, 400px" type="image/png">
        <img src="https://kristoff.it/static/2cc08c37498f0cec888734c477d5f358/c7805/protty1.png" alt="protty1" title="protty1">
      </picture>
  </a>
    </span></p>
<p>The work to replace the current C++ compiler implementation with a new pure Zig version has begun. Moving to a self-hosted implementation is usually considered a step towards maturity, with most benefits being felt by developers of the language itself. As an example, <a href="https://www.youtube.com/watch?v=cF1zJYkBW4A" target="_blank" rel="nofollow noopener noreferrer">Go lost</a> some speed of compilation by switching to the self-hosted compiler but, in exchange, it streamlined the toolchain, removed dependencies, and improved the whole development experience.</p>
<p>The move to a self-hosted compiler for Zig has similar advantages for the core contributors, but it also <strong>makes LLVM an optional dependency</strong>, <strong>increases compilation speed</strong> (instead of losing it), and adds an amazing feature for debug builds of your code: <strong>incremental compilation with in-place binary patching</strong>, <a href="https://kristoff.it/blog/what-is-zig-comptime/">another</a> <a href="https://kristoff.it/blog/zig-colorblind-async-await/">unique</a> Zig feature.</p>
<h2 id="speeding-up-compilation"><a href="#speeding-up-compilation" aria-label="speeding up compilation permalink"></a>Speeding up compilation</h2>
<p>Most languages offer some form of caching to speed up compilation, starting from C’s compilation units, up to modules, packages, and other comparable boundaries in more modern languages.</p>
<p>Zig also implements a caching system that comes particularly handy when building a project that mixes C and Zig source code, or when using Zig as a C compiler with the <code>zig cc</code> command. Zig keeps track of all the files involved in the compilation, so it can very easily know when an object file can be reused, and this is <a href="https://andrewkelley.me/post/zig-cc-powerful-drop-in-replacement-gcc-clang.html" target="_blank" rel="nofollow noopener noreferrer">only one of the advantages</a> of using Zig to compile C code.</p>
<p>Zig sources always get bundled into a single compilation unit, so the caching system in its current form doesn’t provide any speedup when editing and recompiling a pure Zig project. The upside is that, not only compiling Zig code is very fast, but also that incremental compilation will provide smart caching for Zig code, more than making up for what we can’t get from simple caching.</p>
<h2 id="incremental-compilation"><a href="#incremental-compilation" aria-label="incremental compilation permalink"></a>Incremental compilation</h2>
<p>Incremental compilation is a form of caching that acts at a higher granularity level than normal “compilation unit”-level caching. The Rust blog has a <a href="https://blog.rust-lang.org/2016/09/08/incremental.html" target="_blank" rel="nofollow noopener noreferrer">great post</a> that explains how it works.</p>
<figure>
    <span>
      <a href="https://kristoff.it/static/c8f09130b97cf9ec0fd60c85eb21418e/d4d70/rust.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
        <source srcset="https://kristoff.it/static/c8f09130b97cf9ec0fd60c85eb21418e/f5e3c/rust.webp 100w,
https://kristoff.it/static/c8f09130b97cf9ec0fd60c85eb21418e/f2fbe/rust.webp 200w,
https://kristoff.it/static/c8f09130b97cf9ec0fd60c85eb21418e/e227a/rust.webp 400w,
https://kristoff.it/static/c8f09130b97cf9ec0fd60c85eb21418e/965c5/rust.webp 600w,
https://kristoff.it/static/c8f09130b97cf9ec0fd60c85eb21418e/0cbce/rust.webp 800w,
https://kristoff.it/static/c8f09130b97cf9ec0fd60c85eb21418e/16e88/rust.webp 964w" sizes="(max-width: 400px) 100vw, 400px" type="image/webp">
        <source srcset="https://kristoff.it/static/c8f09130b97cf9ec0fd60c85eb21418e/c0399/rust.png 100w,
https://kristoff.it/static/c8f09130b97cf9ec0fd60c85eb21418e/9ec3c/rust.png 200w,
https://kristoff.it/static/c8f09130b97cf9ec0fd60c85eb21418e/c7805/rust.png 400w,
https://kristoff.it/static/c8f09130b97cf9ec0fd60c85eb21418e/34e8a/rust.png 600w,
https://kristoff.it/static/c8f09130b97cf9ec0fd60c85eb21418e/8ff1e/rust.png 800w,
https://kristoff.it/static/c8f09130b97cf9ec0fd60c85eb21418e/d4d70/rust.png 964w" sizes="(max-width: 400px) 100vw, 400px" type="image/png">
        <img src="https://kristoff.it/static/c8f09130b97cf9ec0fd60c85eb21418e/c7805/rust.png" alt="rust" title="Taken from the Rust blog post linked above.">
      </picture>
  </a>
    </span>
    <figcaption>Taken from the Rust blog post linked above.</figcaption>
  </figure>
<p>In the case of Rust, the compiler builds a dependency graph at the AST level and then saves it to disk alongside the cached intermediate results (object files). When a new compilation is requested, the compiler will be able to easily notice which parts of the AST have changed and thus invalidate all the intermediate results that depend on it.</p>
<p>One important detail about this graph is the fact that the right-most box is always invalidated. In other words, the final executable is always re-linked from scratch starting from a mix of old and newly generated object files. It’s clear that this has to be the case, since the final executable depends on everything else and so any meaningful change to the code will invalidate it, but this is where the Zig self-hosted compiler brings a new ingenious idea to the table.</p>
<h2 id="in-place-binary-patching"><a href="#in-place-binary-patching" aria-label="in place binary patching permalink"></a>In-Place Binary Patching</h2>
<p>As of Zig version 0.6.0, regardless of the type of release (debug, release-safe, release-fast), there is always a final step delegated to <strong>LLVM, which takes at least 70% of the total compilation time</strong> including when compiling a debug build, where optimizations aren’t even enabled.</p>
<p><strong>The self-hosted compiler will not depend on LLVM for debug builds</strong> and will be able to cut compilation time considerably, <strong>basically reducing that 70% to almost zero</strong>, just by virtue of being a simpler piece of software compared to LLVM. </p>
<p>On top of that, since the compiler will have full control over the whole process, it will generate machine code using an ad-hoc strategy optimized for incremental compilation, allowing the compiler to patch the final executable in-place with the new changes. </p>
<p>In-place binary patching is based on a granularity of top-level declarations. Each global variable and function can be independently patched because the final binary is structured as a sequence of loosely coupled blocks. Another important characteristic is that all this information is kept in memory, so the compiler will stay open between compilations.</p>
<p> If you want to see the self-hosted compiler in action, here’s a 5 minute demo by Andrew:</p>

          <p>
            <iframe src="https://www.youtube-nocookie.com/embed/R5FKgi9BYyU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
          </p>
          
<h2 id="designing-machine-code-for-incremental-compilation"><a href="#designing-machine-code-for-incremental-compilation" aria-label="designing machine code for incremental compilation permalink"></a>Designing machine code for incremental compilation</h2>
<p>Efficient in-place binary patching is something that can only be accomplished by tightly coupling the compiler frontend and backend. Part of the reason this feature is so rarely seen in the wild is that it goes against our better sense of abstraction and clean code organization. But we must never forget: abstraction is just a tool to reach a practical outcome, and not always the most appropriate one.</p>
<p>In order to perform in-place binary patching, we need code to be <a href="https://en.wikipedia.org/wiki/Position-independent_code" target="_blank" rel="nofollow noopener noreferrer">position independent</a>. This allows us to move it around in virtual memory when a function grows outside its allocated boundary. We also need to be able to reference virtual addresses indirectly, so that N callsites do not need to be updated when a function is moved to a new virtual address.</p>
<p>To accomplish this Zig uses a Global Offset Table for all function calls.</p>
<p>However, that only solves functions. There are more components to consider here, such as debug information. When we add new lines to a function, that modifies the debug information, which is used to print stack traces! Solving this involves creatively organizing an allocation scheme for debug line information, and figuring out how to do NOPs. Andrew’s journey here involved creating a <a href="http://dwarfstd.org/ShowIssue.php?issue=200803.1" target="_blank" rel="nofollow noopener noreferrer">proposal for a new DWARF line number opcode</a>.</p>
<p>This problem must be solved repeatedly for each kind of linking backend - ELF, DWARF, PE, PDB, MachO, and WebAssembly. Special thanks for the contributors who have stepped up and taken on the added challenge of supporting in-place binary patching: <a href="https://github.com/alexnask" target="_blank" rel="nofollow noopener noreferrer">Alexandros Naskos</a>, <a href="http://www.jakubkonka.com/" target="_blank" rel="nofollow noopener noreferrer">Jakub Konka</a>, and <a href="https://ifreund.xyz/" target="_blank" rel="nofollow noopener noreferrer">Isaac Freund</a>.</p>
<p>Be on the lookout for a more technical post on <a href="https://andrewkelley.me/" target="_blank" rel="nofollow noopener noreferrer">Andrew’s blog</a>, where he’ll dive into some of these fascinating details — <strong>including how this design gets us 90% of the way to hot code swapping!</strong></p>
<h2 id="when-is-it-going-to-be-ready"><a href="#when-is-it-going-to-be-ready" aria-label="when is it going to be ready permalink"></a>When is it going to be ready?</h2>
<p>The self-hosted backend is <a href="https://github.com/ziglang/zig/projects/2" target="_blank" rel="nofollow noopener noreferrer">still a work in progress</a>, but all the functionalities presented in this post have been designed and prototyped to the point where it’s just a matter of doing the methodical part of the work.</p>
<p>The self-hosted backend will ship in Zig 0.7.0 behind a flag, supporting only a subset of the Zig language. In the meantime, the core development team and a few other contributors are sprinting forward with more language support and additional targets. The current aim is to fully replace the C++ implementation with the self-hosted backend for Zig 0.8.0, roughly 7 months from now.</p>
<p>If you like where Zig is going, there’s no better time <a href="https://github.com/ziglang/zig/wiki/Community" target="_blank" rel="nofollow noopener noreferrer">to join the Zig community</a> than now, and if you want to help speed the development up, please <a href="https://ziglang.org/zsf/" target="_blank" rel="nofollow noopener noreferrer">consider donating to the Zig Software Foundation</a> to allow core developers to spend more time working on Zig.</p>
<figure>
    <span>
      <a href="https://kristoff.it/static/2ec6c0ef555e8f1383c4cb5d7b3edba2/8eeed/protty2.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
        <source srcset="https://kristoff.it/static/2ec6c0ef555e8f1383c4cb5d7b3edba2/f5e3c/protty2.webp 100w,
https://kristoff.it/static/2ec6c0ef555e8f1383c4cb5d7b3edba2/f2fbe/protty2.webp 200w,
https://kristoff.it/static/2ec6c0ef555e8f1383c4cb5d7b3edba2/e227a/protty2.webp 400w,
https://kristoff.it/static/2ec6c0ef555e8f1383c4cb5d7b3edba2/44d87/protty2.webp 407w" sizes="(max-width: 400px) 100vw, 400px" type="image/webp">
        <source srcset="https://kristoff.it/static/2ec6c0ef555e8f1383c4cb5d7b3edba2/c0399/protty2.png 100w,
https://kristoff.it/static/2ec6c0ef555e8f1383c4cb5d7b3edba2/9ec3c/protty2.png 200w,
https://kristoff.it/static/2ec6c0ef555e8f1383c4cb5d7b3edba2/c7805/protty2.png 400w,
https://kristoff.it/static/2ec6c0ef555e8f1383c4cb5d7b3edba2/8eeed/protty2.png 407w" sizes="(max-width: 400px) 100vw, 400px" type="image/png">
        <img src="https://kristoff.it/static/2ec6c0ef555e8f1383c4cb5d7b3edba2/c7805/protty2.png" alt="protty2" title="Thanks to kprotty for the cute doodles!">
      </picture>
  </a>
    </span>
    <figcaption>Thanks to kprotty for the cute doodles!</figcaption>
  </figure></div></div>]]>
            </description>
            <link>https://kristoff.it/blog/zig-new-relationship-llvm/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24615916</guid>
            <pubDate>Mon, 28 Sep 2020 12:32:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Multivariate Temporal Autoencoder for Predictive Reconstruction of Deep Sequence]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24615738">thread link</a>) | @shivinski
<br/>
September 28, 2020 | https://altumintelligence.com/articles/a/Multivariate-Temporal-Autoencoder-for-Predictive-Reconstruction-of-Deep-Sequences | <a href="https://web.archive.org/web/*/https://altumintelligence.com/articles/a/Multivariate-Temporal-Autoencoder-for-Predictive-Reconstruction-of-Deep-Sequences">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
	
	<div>
		<div id="description-text-article">
			<p>
	This article demonstrates a multi-branch deep neural network approach to tackling the problem of multivariate temporal sequence prediction by modelling a latent state vector representation of data windows through the use of a recurrent autoencoder and predictive model.
</p>

<p>
	The research paper for this article can be downloaded from the following link:
</p>
<p>
	<a href="https://bit.ly/MvTAe" target="_blank"><i></i> https://bit.ly/MvTAe</a>
</p>
<p>
	The complete code can be found at the following GitHub repository:
</p>
<p>
	<a href="https://github.com/jaungiers/MvTAe-Multivariate-Temporal-Autoencoder" target="_blank"><i></i> https://github.com/jaungiers/MvTAe-Multivariate-Temporal-Autoencoder</a>
</p>

<h2>Introduction</h2>

<p>
	Temporal sequence prediction problems have been studied for centuries using ever more complex methods with the aim of capturing hidden patterns within and predicting those patterns going forward. Any temporal process has drivers which determine its behavior, in theory any and all of these drivers can be modelled given enough data about that process at a point in time and a complex enough model - in practice however this is currently unfeasible for a variety of reasons, the main of which are capturing the data, computing the captured dimensionality of the data and modelling the complex interaction of many dimensions interacting in various correlated ways.
</p>
<p>
	An example of the complexity of such a problem might be the seemingly stochastic path of a raindrop down a window. By all respects this raindrop would appear to be taking a random walk down the windowpane, with the left and right movements seemingly unable to be determined or modelled. Consider however having the position of every water molecule, every glass molecule, their respective temperatures and their historical interactions graph with every other molecule available as data at every granular point in time. Given this information, it is reasonable to assume that there exists a model which can be created that is accurately able to specify where the raindrop will go next, and by extrapolating, where it will end up when it reaches the bottom of the windowpane.
</p>
<p>
	The problem of course with the above example is that there currently exists no such method of capturing every observable aspect of a universe at a point in time. Hence for now the best we can do is look to create a model to approximate the hidden drivers of the raindrop given the best data we can gather.
</p>
<p>
	Whilst this isn't optimal for the example raindrop problem, the good news is that there are ample problems where a large amount of data can be gathered at very fine points in time and hence a model can be created to forecast the problem process.
</p>
<p>
	Processes which have a small, closed universe of potential drivers that influence their behavior are easier to forecast for greater sequential steps ahead, whereas processes which are exposed to a great variety of influencing drivers succumb to the exponential decay of accuracy through chaos and as such are only able to be modelled very short sequential steps ahead. The more influencing drivers of a system can be worked into the model however, the more accurate the prediction process will be going forward.
</p>
<p>
	This research focuses on building a model which can process multivariate temporal sequences of data, which in real-world data problems act as the influencing drivers of a process and which learns to build a hyperdimensional approximate representation of the drivers and process in an unsupervised manner. This trained hyperdimensional hidden representation then acts to train a secondary predictive model branch to forecast sequential steps ahead. The model is created using a multi-branch deep neural network approach utilizing the autoencoding principle and building on a sequence to sequence approach created by Sutskever et al. for creating the hyperdimensional hidden state representation. The model is henceforth referred to as Multivariate Temporal Autoencoder (MvTAe).
</p>
<p>
	The dataset used in this research is created to be of a toy-dataset nature used to demonstrate the MvTAe model in simple yet fully functional circumstances. This research is not concerned with the other major challenge of real-world usage concerning observation, measurement and data processing.
</p>

<br>
<h2>Synthetic Multivariate Temporal Dataset</h2>

<p>
	<img src="https://altumintelligence.com/assets/multivariate-temporal-autoencoder-for-predictive-reconstruction-of-deep-sequences/dataset_full.png">
	<br><span>Synthetic multivariate temporal data across all dimensions</span>
</p>

<p>
	To train and test our multivariate temporal autoencoder model we create a synthetic toy-dataset which contains several specific dimensions:
</p>

<ul>
	<li>sine_1 : a sinusoidal wave with a cycle period of 100 timesteps and an amplitude of 1.</li>
	<li>sine_2 : a sinusoidal wave with a cycle period of 1000 timesteps and an amplitude of 5.</li>
	<li>noise : a gaussian distribution of stochastic noise between -1 and +1.</li>
	<li>combined_signal : a sum of sine_1 and sine_2. This will be used as the Y target variable we are looking to predict and will NOT be included in the X training data that the autoencoder branch of MvTAe sees.</li>
</ul>

<p>
	The dataset is created in this way as to provide a way to test our autoencoder model for several important attributes. The first sinusoidal wave is a repeating pattern over time which will test the ability of our model to capture the sequential process of this pattern. The second sinusoidal wave creates a longer term cyclical sequence pattern which our model will not be able to see in full for each training example and hence it tests the models ability to capture cyclical trends. The noise dimension adds an extra dimension of redundant information to test the models ability to identify and disregard dimensions which do not contribute to the latent drivers of the data. Finally, the combined signal will test the ability of the predictive branch of MvTAe to combine signals from the two visible dimensions into this third hidden target dimension.
</p>
<p>
	In the autoencoder branch of the model this combined signal dimension is not used as input, since in this stage the aim is to create a latent vector representation of the visible X dimensions of the dataset. In the second-stage predictive branch the combined signal is used as the Y target for future predictions.
</p>
<p>
	To feed our model, the dataset is split into sliding windows of length N with step S between each window. This approach allows the training of our autoencoder branch to lookback across N temporal steps to determine relationship patterns within the temporal sequence. The Y targets of our first-stage autoencoder branch are the inverse of our inputs along the temporal axis. The Y targets of our second-stage predictive branch will be the <img src="http://latex.codecogs.com/svg.latex?t_{i+1}">combined_signal dimension for each window of <img src="http://latex.codecogs.com/svg.latex?t_{i-N}%20\rightarrow%20t_{i}">.
</p>

<p>The code for creating the normalized sliding windows across dimensions and the accompanying target variable is below</p>
<pre><code>idx_front = 0
idx_rear = window_size
features_x = ['sine_1', 'sine_2', 'noise']
feature_y = 'combined'

tr_data_windows_size = int(np.ceil((data['sine_1'][:idx_val_split].shape[0]-window_size-1)/step_size))
tr_data_windows = np.empty((tr_data_windows_size, len(features_x), window_size))
tr_data_windows_y = np.zeros(tr_data_windows_size)

i = 0
pbar = tqdm(total=tr_data_windows_size-1, initial=i)
while idx_rear + 1 &lt; data['sine_1'][:idx_val_split].shape[0]:
    # create x data windows
    for j, feature in enumerate(features_x):
        _data_window, _hi, _lo = norm(data[feature][idx_front:idx_rear])
        tr_data_windows[i][j] = _data_window
        
    # create y along same normalized scale
    _, hi, lo = norm(data[feature_y][idx_front:idx_rear])
    _y = norm(data[feature_y][idx_rear], hi, lo)[0]
    tr_data_windows_y[i] = _y
    
    idx_front = idx_front + step_size
    idx_rear = idx_front + window_size
    i += 1
    pbar.update(1)
pbar.close()

# reshape input into [samples, timesteps, features]
tr_data_size = tr_data_windows.shape[0]
tr_input_seq = tr_data_windows.swapaxes(1,2)
</code>
</pre>


<p>
	<img src="http://latex.codecogs.com/svg.latex?W_{normalized}%20=%20\frac{W_{(i-N)\rightarrow%20i}^{k}%20-%20min(W_{(i-N)%20\rightarrow%20i}^{k})}{max(W_{(i-N)%20\rightarrow%20i}^{k})%20-%20min(W_{(i-N)%20\rightarrow%20i}^{k})}">
	<br><span>Eqn. 1 Normalization process</span>
</p>

<p>
	As is standard practice when training deep neural networks for optimal converging performance, we normalize our data. As we are dealing with temporal data windows along multiple dimensions, we treat each window and each dimension within the window as independent in terms of normalization. What this means is that for each window W of dimension k we normalize the data independently of all other k dimensions within that window. For the normalization process itself we use standard MinMax Normalization. As such, the normalization process can be summed up as per eqn. 1.
</p>

<pre><code>def norm(data, hi=None, lo=None):
    hi = np.max(data) if not hi else hi
    lo = np.min(data) if not lo else lo
    if hi-lo == 0:
        return 0, hi, lo
    y = (data-lo)/(hi-lo)
    return y, hi, lo
</code>
</pre>


<p>
	<img src="http://latex.codecogs.com/svg.latex?W_{denormalized}%20=%20W_{(i-N)%20\rightarrow%20i}^{k}%20\times%20(hi_{i}^{k}%20-%20lo_{i}^{k})%20+%20lo_{i}^{k}">
	<br><span>Eqn. 2 De-normalization process</span>
</p>

<p>
	Furthermore, when used in real-world predictive applications it is usually advantageous for the final predictive output of the model to be on the absolute scale of the input data. As such, a de-normalization process is required to bring data back to the input scale. With MinMax normalization we normalize data using the min (lo) and max (hi) values of the data window and hence these values created during the normalization process are required for the de-normalization process. We define this de-normalization process as per eqn. 2.
</p>

<pre><code>def reverse_norm(y, hi, lo):
    x = y*(hi-lo)+lo
    return x
</code>
</pre>

<p>
	<img src="https://altumintelligence.com/assets/multivariate-temporal-autoencoder-for-predictive-reconstruction-of-deep-sequences/datawindows_subplots.png">
	<br><span>Normalized data window dimensions and Y target normalized with the hi, lo values from the X window</span>
</p>

<br>
<h2>Multivariate Temporal Autoencoder Model (MvTAe)</h2>

<p>
	<img src="https://altumintelligence.com/assets/multivariate-temporal-autoencoder-for-predictive-reconstruction-of-deep-sequences/mvtae_model_diagram.png">
	<br><span>High-level architecture diagram of the MvTAe model</span>
</p>

<p>
	The first-stage in our predictive problem is the representation of our multidimensional temporal sequences in an optimized vector format representing the features of the multivariate series in such a way that the full series dynamics can be captured. This process can more commonly be known as feature engineering and is usually a step that requires domain knowledge and a manual feature creation process when building approximations of latent drivers.
</p>
<p>
	The MvTAe model acts to compress the sequence into a hidden state vector representation in an unsupervised manner, …</p></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://altumintelligence.com/articles/a/Multivariate-Temporal-Autoencoder-for-Predictive-Reconstruction-of-Deep-Sequences">https://altumintelligence.com/articles/a/Multivariate-Temporal-Autoencoder-for-Predictive-Reconstruction-of-Deep-Sequences</a></em></p>]]>
            </description>
            <link>https://altumintelligence.com/articles/a/Multivariate-Temporal-Autoencoder-for-Predictive-Reconstruction-of-Deep-Sequences</link>
            <guid isPermaLink="false">hacker-news-small-sites-24615738</guid>
            <pubDate>Mon, 28 Sep 2020 12:11:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Alar: The making of an open-source dictionary]]>
            </title>
            <description>
<![CDATA[
Score 101 | Comments 9 (<a href="https://news.ycombinator.com/item?id=24615530">thread link</a>) | @ronakjain90
<br/>
September 28, 2020 | https://zerodha.tech/blog/alar-the-making-of-an-open-source-dictionary/ | <a href="https://web.archive.org/web/*/https://zerodha.tech/blog/alar-the-making-of-an-open-source-dictionary/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>ನಮಸ್ಕಾರ (Namaskāra)! This is not a post on fintech, or even technology for that matter. This is the story of a product of tenacity, selflessness, and passion; a product that will transcend and outlive most technology we know of. This is the story of a massive dictionary that will become the window to a language spoken by tens of millions of people for generations to come, a resource its author has donated to posterity. This is the story of V. Krishna, <a href="https://alar.ink/"><em>Alar</em></a>, his Kannada-English dictionary, and its accidental discovery and open sourcing at an unlikely place, a stock brokerage, Zerodha. This post is also a personal note, something I have not attempted in a long time.</p><h3 id="prologue">Prologue</h3><p>I have been running <a href="https://olam.in/">Olam</a>, an English-Malayalam and Malayalam-Malayalam dictionary, since 2010. It was built out of the frustration of not having an easily accessible online Malayalam dictionary, of the frustration at dictionary websites that insulted the reader’s intelligence with poor usability, terrible ad-ridden spamminess, and no respect for language. Olam’s website has stayed exactly the same for 10 years. It has an input box that responds to dictionary lookups in under ~50ms, exactly as it did in 2010. It is actively used by millions of Malayalam speakers.</p><p>The first version of the Olam corpus was seeded with unattributed word lists I scraped together from random parts of the web, and several thousand entries I entered myself. Since then, the English-Malayalam dictionary has been expanding slowly with crowdsourced entries.</p><p>The entire Olam corpus is <a href="https://olam.in/open">open source</a> (licensed under <a href="https://opendatacommons.org/licenses/odbl/summary/">OdBL</a>), or open data, rather. While the English-Malayalam corpus is crowdsourced, the Malayalam-Malayalam corpus (now known as the <em>Datuk Corpus</em>) was created out of the mammoth digitisation project the late <a href="https://www.asianetnews.com/pravasam/datuk-kj-joseph-passes-away-pm1xdr">“Datuk” K. J. Joseph</a> undertook in the late 90s, when he single-handledly digitised an out-of-copyright Malayalam-Malayalam dictionary along with many other books and posted them online at the expense of copious amounts of time out of his retirement. He was a Malayali settled in Malaysia, a prominent active social worker and educator. The Malaysian government conferred the title “Datuk” upon him in recognition of his exemplary services in the country, which then ended up being his nickname too. I do not know of the origin of the dictionary Datuk digtised, but it is poignant to think that the original author’s work lives on after a century.</p><p>I discovered the RTF file Datuk had posted a decade prior on an inactive Yahoo groups page around the time I was working on Olam. Needless to say, I was stumped by the scope of this project, and immediately started working on integrating it into Olam. It took more than two years of on and off work to convert the text from the original ASCII input to Unicode, and to clean, structure, and correct close to 200,000 entries. The dataset was named <em>Datuk Corpus</em>, and was published on Olam in 2013. I wrote to the Swathanthra Malayalam Computing (SMC) mailing list <a href="http://lists.smc.org.in/pipermail/discuss-smc.org.in/2013-May/015592.html">announcing it</a>, and we launched it with some fanfare at the SMC conference held in Thrissur, Kerala, that year. Datuk’s story was covered by the press, and his work was now open and available to everyone.</p><p>Shortly thereafter, I was connected to Datuk by an old friend of his I had met at the conference, and we spoke briefly on the phone. He had seen the news clip of the dictionary’s release, and was thrilled to know that his work was now accessible as he had originally intended. Open data lives on. He found it amusing that a random stranger had somehow unearthed a relic he had lost to the annals of internet history. Life is absurd like that, shaped in infinite ways by tiny, random events.</p><p>Datuk <a href="https://www.facebook.com/amma.org.my/posts/tribute-to-the-late-datuk-k-j-josephthe-late-datuk-k-j-joseph-was-a-prominent-ed/2157527197640248/">passed away in January 2019</a>. He was 89 years old. RIP Datuk. Your work’s utility will span generations. The data you created will proliferate and continue to be useful to humanity in ways we never imagined. Such is the beauty of open data. I consider it a privilege to have been able to speak to you just that one time.</p><h3 id="open-data">Open data</h3><blockquote><p><a href="https://en.wikipedia.org/wiki/Open_data">Open data</a> is the idea that some data should be freely available to everyone to use and republish as they wish, without restrictions from copyright, patents or other mechanisms of control.</p></blockquote><p>I shudder to think of a world without Wikipedia. The open data movement shares strong parallels with the Free and Open Source Software (FOSS) movement. The gist is that certain knowledge should be freely available to everyone with no restrictions and with one goal—collective advancement of humanity.</p><p>I consider dictionaries to be on top of that list. The stepping stone to language, the underpinning of civilisation. Dictionaries should be open, free, and easily accessible to everyone, everywhere. If we cannot share something as fundamental as language without motives of profit, we ought to do some serious introspection as members of an advanced civilisation.</p><p>An open data dictionary for every Indian language, the largest collection of open source dictionaries in the world, would be an immense resource for not only India but for humanity in general. Ideally, this is the kind of project governments should do. State governments could very easily partner with local universities and undertake the creation and maintenance of open data dictionaries.</p><p>That said, at Zerodha, we would be happy to fund projects to create high quality open data dictionaries if there are scholars out there working on them.</p><h3 id="a-kannada-dictionary">A Kannada dictionary</h3><p>I moved from Kerala to Bengaluru in early 2012 to get access to fast internet. Bengaluru is a melting pot of people from all over India, and English is the glue that holds the “IT sector” together. I can comprehend Kannada speech reasonably well and speak rather poorly, but cannot read the script, thanks to the lack of opportunities to learn over the many years spent between home, where we speak Malayalam, and work, an English speaking environment. With the guilt of not being able to learn Kannada, and the great satisfaction of having Olam as an open data corpus, I had been looking for ways to build a Kannada dictionary right after I had moved to Bengaluru.</p><p>Sometime in 2016, I presented the idea of having an open source Kannada dictionary created from scratch to Nithin. He was immediately on board to commission the project. A perk I enjoy, the privilege of having a resourceful backer who believes in public good. Not knowing where to start, I asked around a few places but nothing materialised for the next two years, and as always, I continued to bring up this conversation once in a while.</p><p>Then, sometime in October 2018, I randomly brought up the conversation again, and Srihari, who had just joined the tech team, happened to overhear it. He vaguely remembered that someone in his family had been associated with a dictionary for a long time. This would be one of those minuscule, random events that would significantly change the timeline; the <a href="https://en.wikipedia.org/wiki/Butterfly_effect">Butterfly effect</a> in action. I crossed my fingers and he soon setup a meeting with V. Krishna, the relative of his. Shortly thereafter, Srihari, Sharath (also from the tech team), and I went to <a href="http://www.kagapa.in/">KaGaPa’s</a> office to meet V. Krishna and to find out what exactly it was that Srihari remembered about him and a dictionary. KaGaPa (Kannada Ganaka Parishat) created the popular Nudi font and input method for Kannada, an important early innovation for digital Kannada, and V. Krishna had worked with them on several projects.</p><p>V. Krishna and Narasimhamurthy, KaGaPa’s proprietor, spoke passionately about Kannada literature and digitisation projects in the quaint little office room, surrounded by stacks of old Kannada books and literature. It was the perfect setting. Then, the extremely soft-spoken and mild-mannered V. Krishna fired up a computer and showed us his lifelong side project, his Kannada-English dictionary. Researched and written over a period of more than 40 years, 150,000+ Kannada words and 240,000+ English definitions, all neatly typed up in a Word document, complete with parts of speech tags and phonetic notations with diacritics for Kannada words. The ambition of the project, its scholarly quality, the depth of the data, the culmination of one man’s passion, perseverance, and tenacity over a lifetime, all lying in obscurity, stumbled upon by sheer coincidence. Absolutely mind blowing.</p><h3 id="v-krishna">V. Krishna</h3><figure><img src="https://zerodha.tech/static/images/vkrishna-alar.png" alt="V. Krishna's photo" height="150"></figure><p>V. Krishna was born in 1950 in the Malanayakana Halli village in Mysore district in Karnataka. He studied in a Kannada medium school, followed by a year at a pre-university college that he was forced to drop out of before moving to Bengaluru with his family in 1968.</p><p>He found a job at the Indian Agricultural Research Institute (IARI) in 1970. At IARI, around this time, noticing him struggle with the English language, his boss casually suggested that he procure a dictionary to learn English. This conversation would turn out to be pivotal, and would set V. Krishna on a lifelong journey of language research and scholarship, an amazing case of autodidacticism.</p><p>So, he took his boss’s advice and got himself an English dictionary and started studying it. Then he got himself another dictionary, and another, until he had five of them. At the same time, he took an interest in Kannada literature and started studying Kannada and English together. To help with this, he started jotting down notes, and at some point, began structuring them. A dictionary was being born. In the meanwhile, he took evening classes and obtained a commerce degree in 1976 from MES college, Malleshwaram.</p><p>Around 1980, <a href="https://en.wikipedia.org/wiki/Kannada_Sahitya_Parishat">Kannada Sahitya Parishattu</a> published a Kannada - English dictionary, and unsurprisingly, V. Krishna got himself a copy. He was surprised by the sheer number of errors he spotted—more than 200 in the first 50 pages. He wrote to the editor with his findings, and impressed by it, the editor met him in person in Bengaluru, where V. Krishna presented his manuscripts to him. Surprised by its quality, he suggested that V. Krishna continue his work and turn it into a full-fledged dictionary. This was the …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://zerodha.tech/blog/alar-the-making-of-an-open-source-dictionary/">https://zerodha.tech/blog/alar-the-making-of-an-open-source-dictionary/</a></em></p>]]>
            </description>
            <link>https://zerodha.tech/blog/alar-the-making-of-an-open-source-dictionary/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24615530</guid>
            <pubDate>Mon, 28 Sep 2020 11:37:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Scorpion Transforming Computer Chair]]>
            </title>
            <description>
<![CDATA[
Score 102 | Comments 101 (<a href="https://news.ycombinator.com/item?id=24615351">thread link</a>) | @jacquesm
<br/>
September 28, 2020 | https://www.cluvens.net/news/this-villainous-scorpion-can-transform | <a href="https://web.archive.org/web/*/https://www.cluvens.net/news/this-villainous-scorpion-can-transform">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.cluvens.net/news/this-villainous-scorpion-can-transform</link>
            <guid isPermaLink="false">hacker-news-small-sites-24615351</guid>
            <pubDate>Mon, 28 Sep 2020 11:07:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[WebRTC Streaming Free Trial with full features]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24615349">thread link</a>) | @kerrarbone
<br/>
September 28, 2020 | https://antmedia.io/free-trial | <a href="https://web.archive.org/web/*/https://antmedia.io/free-trial">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<section>
</section>
<section>
<div>
<div>
<div>
<div> <h2>What Does Your <span>14 days</span> Free Trial Include?</h2> <table> <tbody> <tr> <td>Ultra Low Latency<br> One-to-Many WebRTC Streaming</td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>End-to-End Latency </td> <td>0.5 Seconds (500ms)</td> </tr> <tr> <td>Scaling </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>RTMP(Ingesting) to WebRTC (Playing) </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>Hardware Encoding (GPU) </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>Adaptive Bitrate </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>Secure Streaming </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>iOS &amp; Android WebRTC SDK </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>iOS &amp; Android RTMP SDK </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>H.264,H.265 and VP8 </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>RTMP, RTSP, MP4 and HLS Support </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>WebRTC to RTMP Adapter </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>360 Degree Live &amp; VoD Streams </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>Web Management Dashboard </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>IP Camera Support </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>Re-stream Remote Streams </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>Open Source </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>Simulcasting to Periscope </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>Simulcast to Facebook &amp; Youtube </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>Support</td> <td>E-mail, On-site</td> </tr> </tbody> </table></div><div>
<h2><span>Free</span> Trial Sign Up</h2>
<div>
<div id="wpforms-32285"><form id="wpforms-form-32285" data-formid="32285" method="post" enctype="multipart/form-data" action="/free-trial/" data-token="13e5cb6f6af763785252f5ffbfd20a16"><div><p><label for="wpforms-32285-field_0">Name Surname <span>*</span></label></p><p><label for="wpforms-32285-field_1">Email <span>*</span></label></p><p><label for="wpforms-32285-field_5">Company Name</label></p><p><label for="wpforms-32285-field_2">Tell us about your project &amp; Let us help <span>*</span></label></p></div></form></div></div></div></div></div></div></section></div></div>]]>
            </description>
            <link>https://antmedia.io/free-trial</link>
            <guid isPermaLink="false">hacker-news-small-sites-24615349</guid>
            <pubDate>Mon, 28 Sep 2020 11:07:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Behavioral Programming and Tic Tac Toe (2018)]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24615316">thread link</a>) | @sktrdie
<br/>
September 28, 2020 | https://lmatteis.github.io/react-behavioral/ | <a href="https://web.archive.org/web/*/https://lmatteis.github.io/react-behavioral/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://lmatteis.github.io/react-behavioral/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24615316</guid>
            <pubDate>Mon, 28 Sep 2020 11:02:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Building Trust, and how we never see your Cryptocurrency Private Keys]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24615290">thread link</a>) | @timothy-quinn
<br/>
September 28, 2020 | https://blog.congruentlabs.co/building-trust-and-how-we-never-see-your-crypto-private-keys/ | <a href="https://web.archive.org/web/*/https://blog.congruentlabs.co/building-trust-and-how-we-never-see-your-crypto-private-keys/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://blog.congruentlabs.co/content/images/size/w300/2020/09/Cryptography-Banner.png 300w,
                            https://blog.congruentlabs.co/content/images/size/w600/2020/09/Cryptography-Banner.png 600w,
                            https://blog.congruentlabs.co/content/images/size/w1000/2020/09/Cryptography-Banner.png 1000w,
                            https://blog.congruentlabs.co/content/images/size/w2000/2020/09/Cryptography-Banner.png 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://blog.congruentlabs.co/content/images/size/w2000/2020/09/Cryptography-Banner.png" alt="Building Trust, and how we never see your Cryptocurrency Private Keys">
            </figure>

            <section>
                <div>
                    <p>Building trust with new products and services is difficult, but I'm hoping as the <a href="https://signata.net/">Product Manager for Signata</a> I can help build that trust with you by trying to be as open as possible in how our product works. You might simply ask why we don't just make our code base open source and be done with it? Well, making a product open source isn't as simple as flipping the switch to "public" for our source code - there are several larger considerations that have to take place around access controls to the repositories, ensuring we're in a state that we can actually make it public, test and release processes, legal requirements, intellectual property, and more.</p><p>We aren't discounting the idea of open sourcing our products (or at least some parts of them), but for now we're just a start-up and we just don't have the resources available to make that transition safely without sacrificing effort on actually building our products.</p><p>So, for now, I'll at least give some in-depth information on how we provide assurance to you, dear reader, that we never see your cryptocurrency private keys.</p><h2 id="so-what-s-a-private-key">So, what's a Private Key?</h2><p>Most people who've dabbled in cryptocurrency will run into the idea of <em>addresses</em> - these are effectively the names of our wallets that we can share with one-another to send coins around.</p><figure><img src="https://blog.congruentlabs.co/content/images/2020/09/image-19.png" alt=""><figcaption>An example of a LItecoin address</figcaption></figure><p>What that address actually is though is just a unique way of sharing a <em>public key</em>. When you start looking into the world of cryptography, you'll probably come across the concept of <strong>key pairs </strong>- these are <em>public </em>and <em>private </em>keys that are mathematically related to each other. A cryptocurrency address is just a form of <strong>Public Key</strong>, and if it's your own public key then you've likely got its corresponding <strong>Private Key</strong> as well. It's got the <em>public</em> part in it's name because you can safely share it around with the general public. The <em>private</em> part though is something you just keep to yourself - should you ever give anyone your private key, then they have the power to send your cryptocurrency wherever they want as if it was their own.</p><p>You can also think of it like internet banking - your address/public key is like your bank account number that you give out to people for them to send you money. The username and password you log in to your internet banking website with is like your private key. If you gave someone else your login details for your bank, then they'll be able to pretend to be you, and send your money wherever they like.</p><blockquote>Just as a side note: a cryptographic "key" is just a really large number - basically a number that is so large and complex that it's next to impossible to guess.</blockquote><p>Now that we've defined private keys, let's look at your YubiKeys.</p><h2 id="how-we-set-up-your-yubikey">How we set up your YubiKey</h2><p>One of the core design decisions we made with Signata was to remain zero-knowledge about our customer's private keys. If we were to have any knowledge of them, then that would just open us up to becoming a huge target for hackers. Being zero-knowledge comes with a price though - should you, dear reader, ever forget your passwords for Signata, then we don't have any way to recover your data for you.</p><p>So before we look at how we protect your cryptocurrency Private Keys, we first need to look at how exactly we set up your YubiKeys with our system, as they're integral to how we protect your cryptocurrency Private Keys.</p><p>When you first add a YubiKey to Signata, you'll get asked for your <strong>Recovery Passphrase</strong> and <strong>PIN</strong>:</p><figure><img src="https://blog.congruentlabs.co/content/images/2020/09/image-20.png" alt="" srcset="https://blog.congruentlabs.co/content/images/size/w600/2020/09/image-20.png 600w, https://blog.congruentlabs.co/content/images/2020/09/image-20.png 893w" sizes="(min-width: 720px) 720px"></figure><p>Your Recovery Passphrase is a 12 word mnemonic, which is converted into a "seed" - you set this up the first time you run the desktop application. We then convert that seed into a byte buffer, and then we make two ten thousand round PBKDF2 hashes of those bytes. The first hash is without any salt, and the second hash is with some salt that's saved in your account:</p><!--kg-card-begin: markdown--><pre><code>const seed = mnemonicToSeedSync(recoveryPassphrase).toString('hex');
const seedBuf = Buffer.from(seed, 'hex');
const h1 = forge.util.bytesToHex(forge.pkcs5.pbkdf2(seed, '', 10000, 512));
const h2 = forge.util.bytesToHex(forge.pkcs5.pbkdf2(h1, recoveryPassphraseSalt, 10000, 512));
</code></pre>
<!--kg-card-end: markdown--><p>The 2-step hashing process is designed to serve the purpose of letting us validate the first hash with some locally cached data that doesn't need to know the salt assigned to your account, and the second hash is stored in your Signata account so we can verify your hash on our back-end servers without you actually needing to send us your mnemonic. If someone managed to compromise our back-end servers and happened to be intercepting a check of your passphrase, then they couldn't actually see your mnemonic as they'd only see a PBKDF2 hash of it.</p><p>If the PBKDF2 hashes of your seed are correct for your account, then we move to the next step. We generate a new RSA2048 Encryption Key Pair for your YubiKey, and then we use AES128 encryption to encrypt it with the "seed" that we generated before. We then store that encrypted RSA Encryption Key Pair onto your account as a secure backup, and the RSA Key Pair is injected into the Encryption Key slot of your YubiKey.</p><blockquote>We use RSA2048 because that's the maximum most smartcard like devices can handle, including YubiKeys. We also use AES128 as there are no known weaknesses with that algorithm, and it doesn't have export restrictions like AES256 has in some countries.</blockquote><p>With your recovery passphrase you can, at any time, set up additional YubiKeys with the same Encryption Key installed in them if you want spare devices or you happen to lose your YubiKey.</p><p>Now, onto the cryptocurrency addresses themselves.</p><h2 id="how-we-protect-your-private-keys">How we protect your Private Keys</h2><p>With your YubiKey set up as above, we're then ready to start protecting your cryptocurrency addresses. Adding or Importing addresses works in exactly the same way, with the Import just relying on you providing the private key of the address instead of us creating a new one for you.</p><figure><img src="https://blog.congruentlabs.co/content/images/2020/09/image-21.png" alt="" srcset="https://blog.congruentlabs.co/content/images/size/w600/2020/09/image-21.png 600w, https://blog.congruentlabs.co/content/images/2020/09/image-21.png 889w" sizes="(min-width: 720px) 720px"></figure><p>When an address is created in Signata, the very first thing that happens is we generate a <strong>session key</strong>:</p><!--kg-card-begin: markdown--><pre><code>const sessionKey = await generateSessionKey();
const sessionKeyBuf = Buffer.from(sessionKey.randomString, 'ascii');
</code></pre>
<!--kg-card-end: markdown--><p>Then we RSA encrypt that session key using your YubiKey:</p><!--kg-card-begin: markdown--><pre><code>const encryptedSessionKey = await newDevice.encryptData(sessionKey.randomString);
</code></pre>
<!--kg-card-end: markdown--><p>And then we create your new address and AES encrypt the WIF and Private Key using that session key:</p><!--kg-card-begin: markdown--><pre><code>const encryptedWif = await aesEncrypt(
    sessionKeyBuf,
    Buffer.from(privateKey.toWIF(), 'utf-8'),
);
</code></pre>
<!--kg-card-end: markdown--><p>So why don't we just RSA encrypt the WIF and Private Key with the YubiKey and skip that session key step? Well encrypting data with the RSA algorithm is not designed for encrypting large amounts of data. In fact if you try to, a lot of libraries and devices will actually just refuse to process it if it's too big, because it's just too slow and inefficient.</p><p>Instead, RSA is designed to work <em>in conjunction with</em> symmetric algorithms like AES - you use AES to encrypt data, and then you use RSA to encrypt the much smaller AES encryption key. You end up with the ability to encrypt large amounts of data quickly and securely, but also the added usefulness of asymmetric encryption with RSA.</p><blockquote>If you're wondering how those session keys are generated - we actually do that in Python, generating them using your underlying Operating System random number generators.</blockquote><p>So in the end we store in the Signata database 3 values. Your session key encrypted by the YubiKey, the cipher text encrypted by the session key, and some salt that was generated for the encryption process.</p><!--kg-card-begin: markdown--><pre><code>wif = {
    sessionKey: encryptedSessionKey.encryptedData,
    cipherText: encryptedWif.encryptedData,
    salt: encryptedWif.salt,
};
</code></pre>
<!--kg-card-end: markdown--><p>To get access to your Private Key again, we just reverse the process - we decrypt the session key using your YubiKey, and then we decrypt the cipher text with the session key and salt together.</p><h2 id="how-it-all-fits-together">How it all fits together</h2><p>At a higher level, you can think of our zero-knowledge feature like this:</p><ol><li>Your recovery passphrase protects your YubiKeys. We have zero knowledge of your recovery passphrase.</li><li>Your YubiKeys protect your cryptocurrency addresses.</li><li>The Signata database never stores anything that isn't encrypted by you.</li></ol><p>I apologise if this got a little too deep into the weeds - a lot of terminology and concepts were thrown in without explanation, but I wanted to make sure anyone who already understands cryptography would at least get the gist of what we're talking about. If you have any questions, or want anything better explained, just let me know in the comments below :)</p><p>Start protecting your cryptocurrency today <a href="https://signata.net/">by downloading Signata for free</a>.</p>
                </div>
            </section>

            
            
        </article>
    </div>
</div></div>]]>
            </description>
            <link>https://blog.congruentlabs.co/building-trust-and-how-we-never-see-your-crypto-private-keys/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24615290</guid>
            <pubDate>Mon, 28 Sep 2020 10:57:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Crypto Reading]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24615223">thread link</a>) | @dayve
<br/>
September 28, 2020 | https://danromero.org/crypto-reading/ | <a href="https://web.archive.org/web/*/https://danromero.org/crypto-reading/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>Below is a list of worthwhile posts about cryptocurrency. Best to read in chronological order:</p><div>
  <p>Â© 2020 | <a href="https://danromero.org/feed.xml">RSS</a> | <a href="https://twitter.com/dwr">@dwr</a> | <a href="mailto:hello@danromero.org" title="">hello@danromero.org</a></p>
</div></div>]]>
            </description>
            <link>https://danromero.org/crypto-reading/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24615223</guid>
            <pubDate>Mon, 28 Sep 2020 10:44:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Addiction Ruined the Internet]]>
            </title>
            <description>
<![CDATA[
Score 15 | Comments 9 (<a href="https://news.ycombinator.com/item?id=24615106">thread link</a>) | @midef
<br/>
September 28, 2020 | https://www.superhighway98.com/addiction | <a href="https://web.archive.org/web/*/https://www.superhighway98.com/addiction">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div data-block-type="2" id="block-765880898db5f75e5dd3"><div><p>Before there was a World Wide Web, there was Usenet: a distributed forum and archive* that was designed for academia. </p><p>The freshmen who flooded Usenet groups each fall were reined-in and taught the ropes by established users. But, that noble task became impossible in September 1993. That was the month when "America Online" granted Usenet access to its customers.</p><h2><strong>The September that never ended</strong></h2><p>Recalling the parable of "<a href="https://en.wikipedia.org/wiki/Eternal_September">Eternal September</a>" is my admission that the Internet has been on the supposed road to ruin since before the World Wide Web was even born. Of course, the web was never a utopia and I don't believe that it should be one.</p><p>Nevertheless, I do believe that the Internet is becoming a tool that is used to divide, to misinform and to escape reality. There are many scapegoats, but modern-day poisons like social media are merely a new form of an old disease: addiction.</p><h2><strong>America offline</strong></h2><p>In the last two decades, America has suffered a staggering rise in <a href="https://apnews.com/article/f1f81ade0748410aaeb6eeab7a772bf7">alcohol consumption</a>, <a href="https://www.pewtrusts.org/en/research-and-analysis/blogs/stateline/2019/11/01/as-meth-use-surges-one-region-tries-to-combat-the-pull">methamphetamine use</a>, <a href="https://www.hsph.harvard.edu/nutritionsource/an-epidemic-of-obesity/">obesity</a>, <a href="https://www.americangaming.org/new/commercial-casino-gaming-revenue-reaches-41-7-billion-in-2018-an-all-time-high/">gambling</a>, <a href="https://www.debt.org/faqs/americans-in-debt/">consumer debt</a>, <a href="https://www.cdc.gov/injury/features/prescription-drug-overdose/index.html">prescription drug abuse</a>, <a href="https://www.pewresearch.org/politics/interactives/political-polarization-1994-2017/">political polarization</a>, <a href="https://www.cdc.gov/injury/features/prescription-drug-overdose/index.html">overdose deaths</a> and <a href="https://www.nimh.nih.gov/health/statistics/suicide.shtml">suicide</a>.</p><p>This rise in compulsive, self-destructive behavior predates the web, but the Internet is where those suffering from mental illness are spending an increasing amount of time. Meanwhile, technologists are actively trying to <a href="https://www.amazon.com/Hooked-How-Build-Habit-Forming-Products/dp/1591847788">exploit this sickness for profit</a>.</p><h2><strong>Addiction is a contagious disease</strong></h2><p>If something is addicting, it will ultimately result in <em>a</em>nger, <em>d</em>enial, <em>d</em>ouble-standards, <em>i</em>ntolerance, <em>c</em>ontrarianism, <em>t</em>rickery, <em>i</em>ntransigence, <em>n</em>egligence and <em>g</em>aslighting. Those who've lived with an addict know these forms of emotional abuse.</p><p>But, the side effects of this abuse aren't easily predicted.</p><p>A child with an alcoholic parent may grow up to shun alcohol as an adult, yet become addicted to seeking the love and attention of unavailable persons. Meanwhile, an available person, like a child, may turn to video games or social media to cope with the reality of having a cold and unloving parent. And so the cycle continues.</p><h2><strong>Sharing a digital household</strong></h2><p>When we visit Facebook, Twitter, YouTube, Reddit, comment sections and forums, we make ourselves vulnerable to emotional abuses that we carry into our offline lives. And what we do (or avoid doing) in our offline lives is reflected in our online behaviors.</p><h2><strong>Grant me the serenity…</strong></h2><p>I cannot “fix” other people, therefore, I cannot fix the web. But, if I’m courageous enough to admit my own shortcomings, I can work toward fixing myself.  Maybe the web will get better as a result.</p><p><a href="https://www.superhighway98.com/">&lt;- RETURN TO SUPERHIGHWAY 98</a></p><p><a href="https://www.superhighway98.com/seo">HOW SEO RUINED THE INTERNET -&gt;</a></p><p>###</p><p>*Google acquired the Usenet archive in 2001, and after letting it languish for more than a decade, <a href="https://www.vice.com/en_us/article/jp5a77/google-a-search-company-has-made-its-internet-archive-impossible-to-search">rendered it unsearchable in 2015</a>. </p><p>This is one of several ways that <a href="https://www.superhighway98.com/google">Google has ruined the Internet</a>.<br></p></div></div></div></div>]]>
            </description>
            <link>https://www.superhighway98.com/addiction</link>
            <guid isPermaLink="false">hacker-news-small-sites-24615106</guid>
            <pubDate>Mon, 28 Sep 2020 10:28:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[View my personal Blog site]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24615075">thread link</a>) | @pr2tik1
<br/>
September 28, 2020 | https://pr2tik1.github.io/blog/ | <a href="https://web.archive.org/web/*/https://pr2tik1.github.io/blog/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <div>
<p>Welcome to my personal blog page. I write mostly on deep learning and data science. I will be happy to hear your thoughts and feedback. 
View my recent posts below.</p>

<h2 id="-thought">💭 Thought</h2>

<hr>

<p>Inspired by Patrick Winston’s teachings(<a href="https://youtu.be/Unzc731iCUY">view here</a>) where he shares the formula of the quality of communication. 
For me, <b>Quality of work</b> is the combination of Knowledge, Practice and Persistence.</p>

<p><img src="https://pr2tik1.github.io/blog/images/Eqn.png" alt=""></p>

<h2 id="-medium-posts">📜 Medium Posts</h2>

<hr>

<p>View my post published on Medium,</p>

<ul>
  <li><a href="https://towardsdatascience.com/what-happens-to-programmers-in-2020-d04a6bd7452f?source=friends_link&amp;sk=a880dfdd6435c792d33b284c98705a62">What happens to developers in 2020?</a></li>
  <li><a href="https://towardsdatascience.com/exploring-neural-networks-and-their-fascinating-effectiveness-81ebc054cb16">Universal Approximation Theorem of Neural Networks</a></li>
  <li><a href="https://towardsdatascience.com/understanding-kaplan-meier-estimator-68258e26a3e4">Kaplan Meier Estimator</a></li>
  <li><a href="https://medium.com/towards-artificial-intelligence/neural-networks-from-scratch-a-brief-introduction-for-beginners-d3776599aaac?source=friends_link&amp;sk=701beeb7b1be2e6c85c641119ca35e72">Exploring Neural Networks</a></li>
</ul>

<hr>





  

  <!-- Hide posts if front matter flag hide:true -->
  
  

  <!-- Sort posts by rank, then date -->
  
  
  

 
  

   <!-- Assemble final sorted posts array -->
  
  <ul><li><div><p><img src="https://pr2tik1.github.io/blog/images/img.png">
      </p><div>
        <h3>
          <a href="https://pr2tik1.github.io/blog/pytorch/cnn/pca/t-sne/2020/09/08/Sketch-Recognition.html">
            Doodle Images Classification using PyTorch
          </a>
        </h3>
        <p>Multi-Class Image Classification using modified LeNet architecture(Convolutional Neural Network) implemented in PyTorch.</p>
          <p>Sep 8, 2020</p>
      </div>
  </div></li></ul>

    </div>
      </div>
    </div></div>]]>
            </description>
            <link>https://pr2tik1.github.io/blog/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24615075</guid>
            <pubDate>Mon, 28 Sep 2020 10:22:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Visualize Supporting and Disputing Citations]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24615070">thread link</a>) | @darosati
<br/>
September 28, 2020 | https://scite.ai/visualizations/global-analysis-of-genome-transcriptome-9L4dJr?dois[0]=10.1038%2Fmsb.2012.40&dois[1]=10.7554%2Felife.05068&focusedElement=10.7554%2Felife.05068 | <a href="https://web.archive.org/web/*/https://scite.ai/visualizations/global-analysis-of-genome-transcriptome-9L4dJr?dois[0]=10.1038%2Fmsb.2012.40&dois[1]=10.7554%2Felife.05068&focusedElement=10.7554%2Felife.05068">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="scite-app"><div><div><p><a href="https://scite.ai/visualizations/global-analysis-of-genome-transcriptome-9L4dJr?dois[0]=10.1038%2Fmsb.2012.40&amp;dois[1]=10.7554%2Felife.05068&amp;focusedElement=10.7554%2Felife.05068">Announcing Visualizations: see scite Smart Citations in context. <span>Explore now.</span></a></p><div><div><div><div><div><p><a href="https://scite.ai/"><img alt="logo" src="https://d1lv4filxk1370.cloudfront.net/assets/images/beta-logo.svg"></a></p></div></div></div></div><div><div><div><div><div><p><label><span>Show mentioning cites</span></label></p><p><label><span>Hide unselected cites</span></label></p><p><label>Layout</label></p></div></div></div></div></div></div><div><div><div><div><div><h3>About us</h3><p>scite is a Brooklyn-based startup that helps researchers better discover and evaluate scientific articles through Smart Citationsâ€“citations that display the context of the citation and describe whether the article provides supporting or contradicting evidence. Scite is used by researchers from dozens of countries and is funded in part by the National Science Foundation and the National Institute of Drug Abuse of the National Institutes of Health</p><div><p><img loading="lazy" alt="national science foundation logo" src="https://d1lv4filxk1370.cloudfront.net/assets/images/nsf-logo.png"></p><picture><source srcset="https://d1lv4filxk1370.cloudfront.net/assets/images/nih-logo.webp" type="image/webp"><img loading="lazy" alt="national institute of health logo" src="https://d1lv4filxk1370.cloudfront.net/assets/images/nih-logo.jpg"></picture></div></div><div><h3>Contact</h3><div><p>hi@scite.ai</p><p>334 Leonard St, #6</p><p>Brooklyn, NY 11211</p></div></div></div><div><p><span>Copyright Â© <!-- -->2020<!-- --> scite Inc. All rights reserved.</span></p></div></div></div></div></div></div></div></div>]]>
            </description>
            <link>https://scite.ai/visualizations/global-analysis-of-genome-transcriptome-9L4dJr?dois[0]=10.1038%2Fmsb.2012.40&amp;dois[1]=10.7554%2Felife.05068&amp;focusedElement=10.7554%2Felife.05068</link>
            <guid isPermaLink="false">hacker-news-small-sites-24615070</guid>
            <pubDate>Mon, 28 Sep 2020 10:21:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Durability: NVMe Disks]]>
            </title>
            <description>
<![CDATA[
Score 117 | Comments 44 (<a href="https://news.ycombinator.com/item?id=24614893">thread link</a>) | @ingve
<br/>
September 28, 2020 | https://www.evanjones.ca/durability-nvme.html | <a href="https://web.archive.org/web/*/https://www.evanjones.ca/durability-nvme.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<h3>[ 2020-September-27 10:26 ]</h3>
<p>Durability is the guarantee that data can be accessed after a failure. It seems like this should be very simple: either your system provides durable data storage, or it does not. However, durability is not a binary yes/no property, and instead should be defined as the <em>kinds</em> of failures you want your data to survive. Since there is usually some performance penalty for durability, many systems provide a way for only "important" writes to be durable, while "normal" writes will eventually be durable, with no specific guarantee about when. Finally, durability is rarely tested, since <em>really</em> testing it involves cutting the power to computer systems, which is disruptive and hard to automate. Production environments are designed to avoid these failures, so bugs are rarely triggered and hard to reproduce.</p>

<p>I've recently been investigating the durability guarantees in cloud platforms. I decided to start at the beginning: what guarantees are provided by the disks we connect to computers? To find out, I read the relevant sections of the Non-Volatile Memory Express (NVMe) specification (version 1.4a), since it is the newest standard for high-performance SSDs. It also has an <a href="https://nvmexpress.org/developers/nvme-specification/">easy to find, freely available specification</a>, unlike the older SATA or SCSI standards that were originally designed for magnetic disks. In the rest of this article, I will attempt to summarize the durability NVMe devices provide. I believe that most of this should also apply to SATA and SCSI. NVMe was designed as a higher performance replacement for those protocols, so the semantics can't be too different.</p>


<h2>Ordering and atomicity</h2>

<p>Before we can discuss durability, we should discuss some basic semantics of NVMe writes. Commands are submitted to devices using a set of queues. At some time later, the device acknowledges that the commands have completed. There is no ordering guaranteed between commands. From Section 6.3: "each command is processed as an independent entity without reference to other commands [...]. If there are ordering requirements between these commands, host software or the associated application is required to enforce that ordering". This means if the order matters, the software needs to wait for commands to complete before issuing the next commands. However, read commands are guaranteed to return the most completed write (Section 6.4.2.1), although they may also return data from uncompleted writes that have been queued.</p>

<p>A related issue with concurrent updates is atomicity. If there are concurrent writes to overlapping ranges, what are the permitted results? The answer is there are no guarantees. Specifically, "After execution of command A and command B, there may be an arbitrary mix of data from command A and command B in the LBA [logical block address] range specified" (Section 6.4.2). This seems to permit literally any result in the case of concurrent writes, such as alternating bytes from command A and command B.</p>

<p>NVMe includes <em>optional</em> support for atomic writes, with different values for "normal operation" and after power failure. The couple of NVMe devices I looked at don't support atomic writes, but apparently some higher-end devices do. The device exposes the size of atomic writes so software can configure itself to use it. For example, <a href="https://mariadb.com/kb/en/atomic-write-support/">see the MariaDB documentation about atomic writes</a>. This can replace MySQL's "doublewrite buffer," which is a mechanism that provides atomic writes on devices that don't natively support them (nearly all disks).</p>

<p>Basically, NVMe provides "weak" semantics similar to shared memory in multi-threaded programs. There are no guarantees if there are concurrent operations. This means if the order of writes matters, the software needs to submit the commands and wait for them to complete, and never have concurrent writes to overlapping ranges.</p>


<h2>The Flush command</h2>

<p>Without special commands, NVMe provides no guarantees about what data will survive a power failure (Section 5.15.2.2, Figure 249 in the documentation about the Atomic Write Unit Power Fail (AWUPF) field and Section 6.4.2.1). My reading of this means devices are permitted to return an error for all ranges where writes were "in flight" at the time of failure. If you want to be completely safe, you should avoid overwriting critical data by using write-ahead logging. This matches the semantics I found during <a href="https://www.evanjones.ca/intel-ssd-durability.html">power fail testing of SATA magnetic hard drives and SSDs in 2010</a>.</p>

<p>The first NVMe mechanism that can be used to ensure data is durably written is the Flush command (Section 6.8). It writes everything in the write cache to non-volatile memory. More specifically, "The flush applies to all commands [...] completed by the controller prior to the submission of the Flush command" (Section 6.8). This means if you want a durable write, you need to submit the write, wait for it to complete, submit the flush, and wait for that to complete. If you submit writes after submitting the flush, but before it completes, they might also be flushed ("The controller may also flush additional data and/or metadata", section 6.8). Most importantly, if you issue a flush, and it fails in the middle, there is no guarantee about what writes might exist on disk. The disk could have any of the writes, with no relation to the order they were submitted or completed. It could also choose to return an error for all the ranges.</p>


<h2>Force Unit Access (FUA)</h2>

<p>The second mechanism to ensure durability is to set the Force Unit Access option on Write commands. This means that "the controller shall write that data and metadata, if any, to non-volatile media before indicating command completion" (Section 6.15 Figure 404). In other words, data written with a FUA write should survive power failures, and the write will not complete until that is true. Interestingly, you can also specify FUA on a Read command, which is a bit surprising. It forces the referenced data to be flushed to non-volatile media, before reading it (Section 6.9, Figure 374). This mean you can do a set of normal writes, then selectively flush a small portion of it by executing a FUA read of the data you want committed.</p>


<h2>Disabling write caching</h2>

<p>The last mechanism that may ensure durability is to explicitly disable the write cache. If an NVMe device has a volatile write cache, it must be controllable. This means you can disable it (Section 5.21.1.6). It appears to me that if the cache is disabled, then every write must not complete until it is written to non-volatile media, which should be equivalent to setting the FUA bit on every write. However, this is not clearly described in the specification, and I suspect this is rarely used.</p>


<h2>Devices with power loss protection</h2>

<p>Finally, it is worth pointing out that some disks provide "power loss protection." This means the device has been designed to complete any in-flight writes when power is lost. This can be implemented by providing backup power with a supercapacitor or battery that is used to flush the cache. In theory, these devices should show that they do not have volatile write cache, so software could detect that and just use normal writes. However, these devices should ideally also treat FUA writes the same as non-FUA writes, and ignore cache flushes. As a result, I think it is best to design software for disks that have caches, since it can then work with any storage device. If you are using a device with power loss protection, you should still get better performance and some additional protection from failures.</p>

</div></div>]]>
            </description>
            <link>https://www.evanjones.ca/durability-nvme.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24614893</guid>
            <pubDate>Mon, 28 Sep 2020 09:52:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Type Systems Explained with Examples]]>
            </title>
            <description>
<![CDATA[
Score 101 | Comments 17 (<a href="https://news.ycombinator.com/item?id=24614788">thread link</a>) | @thanato0s
<br/>
September 28, 2020 | https://thevaluable.dev/type-system-explained/ | <a href="https://web.archive.org/web/*/https://thevaluable.dev/type-system-explained/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">
    <article>
        

        <section>
            
                <picture>
                    
                        <source srcset="https://thevaluable.dev/images/2020/type_system/type_psycho.webp" type="image/webp">
                    
                    <img src="https://thevaluable.dev/images/2020/type_system/type_psycho.jpg" alt="The Joker is just a type of psychopath">
                </picture>
            

            <p>“My language is better because it has a strong type system!” screams Dave, your colleague developer, trying to push the programming language Cobol for the next micro-service of your company.</p>
<p>Among developers, discussions about programming languages and their type systems can get quickly emotional. During these discussions, we often hear the words “type systems”, “data type”, “type inference”, “static typing”, “weak typing”, “coercion”, and more.</p>
<p>The goal of this article is to see the meaning of all these words with examples, for you to have good foundations and understand the type system of your favorite programming language. More precisely, we’ll see:</p>
<ul>
<li>What are types and why we need them.</li>
<li>When type checking occurs.</li>
<li>What are primitive types, composite types, and Abstract Data Types (ADT).</li>
<li>Type declaration and changing types can be implicit or explicit.</li>
<li>Types and functions.</li>
<li>What is type strength.</li>
<li>What is type safety.</li>
</ul>
<p>I’ll use two different languages to illustrate the ideas, Golang and PHP. If you don’t know them, don’t worry! The examples are straightforward and easy to understand.</p>
<p>I encourage you to use some <a href="https://repl.it/languages/php_cli" target="_blank" rel="noopener">PHP interpreter online</a> and the <a href="https://play.golang.org/" target="_blank" rel="noopener">Go playground</a> while reading, to play and experiment by yourself. This will help you understand the different concepts.</p>
<p>I won’t go into the gory details here. As you’ll see, it’s difficult to generalize something which is specific to a programming language. Yet, this article will give you a good overview of the usual properties of most type systems.</p>
<p>We’ll go progressively from clear waters to the muddy ideas, so take your rubber boots, get ready for the swamp, and let’s go!</p>
<h2 id="whats-a-type">What’s a Type?</h2>

<picture>
    <source srcset="https://thevaluable.dev/images/2020/type_system/state_not_type.webp" type="image/webp">
    <img src="https://thevaluable.dev/images/2020/type_system/state_not_type.jpg" alt="State and types are different">
</picture>



<p>A type system is made of types. I know, it’s mind-blowing. These types are also called <em>data types</em>. What are they?</p>
<h3 id="syntax-and-semantics">Syntax and Semantics</h3>
<p>First, let’s clarify the difference between the <em>syntax</em> and the <em>semantics</em> of a programming language.</p>
<p>For example. the syntax of your mother tongue is the set of rules dictating the structure of the sentences. For many spoken languages, you’ll need a subject and a verb for your sentence to be correct. Programming languages have instead different constructs like <em>expressions</em>, <em>control structures</em>, or <em>statements</em>.</p>
<p>For example, for a <em>if</em> statement to be syntactically correct in PHP, you’ll need:</p>
<ul>
<li>A condition.</li>
<li>Some parenthesis.</li>
<li>Some curly brackets.</li>
</ul>
<p>Something like that: <code>if (1 == 1) { echo "I knew it!"; }</code>.</p>
<p>The semantics, on the other hand, is the meaning behind the constructs. For example, the semantics of an <em>if</em> statement can be explained as follows:</p>
<ol>
<li>The condition is executed.</li>
<li>If the condition is true, the body of the statement is executed.</li>
<li>If the condition is false, the body of the statement is not executed.</li>
<li>The execution continues.</li>
</ol>
<p>Why do we need syntax and semantics? It’s meant to communicate with two different kinds of entities:</p>
<ul>
<li>Your colleagues developers need to understand what the heck you did.</li>
<li>The computer needs to “understand” the instructions to execute them.</li>
</ul>
<h3 id="definition-of-a-type">Definition of a Type</h3>
<p>To come back to our subject, a type can attach semantics to data. For example, in PHP:</p>
<div><pre><code data-lang="php"><span>&lt;?php</span>
<span>$integer</span><span>;</span>

<span>echo</span> <span>gettype</span><span>(</span><span>65</span><span>)</span> <span>.</span> <span>"</span><span>\n</span><span>"</span><span>;</span>
<span>// =&gt; integer
</span><span></span>
<span>$integer</span> <span>=</span> <span>65</span><span>;</span>
<span>echo</span> <span>gettype</span><span>(</span><span>$integer</span><span>)</span> <span>.</span> <span>"</span><span>\n</span><span>"</span><span>;</span>
<span>// =&gt; integer
</span></code></pre></div><p>In Mathematics, 65 is part of the set of integers, so PHP consider the value 65 as type integer. Your variable <code>$integer</code> has the value 65 too, so it’s an integer, too.</p>
<p>When you assign 65 to the variable <code>$integer</code>, you give it a semantics it didn’t have when we declared it, on the second line. This semantics will let you know what you can do with the variable.</p>
<p>We can conclude that:</p>
<ul>
<li>A type gives semantics to a piece of data.</li>
<li>A type is a set of value. For the type integer, it will be a range of decimals. For the type string, it will be a range of possible strings.</li>
</ul>
<h2 id="why-do-we-need-types">Why Do We Need Types?</h2>
<h3 id="representation-and-semantics">Representation and Semantics</h3>
<p>When you declare a variable and assign it a value, the memory hold this value in <em>binary</em>. Our counting system is <em>decimal</em>, which means that the numbers we know and use everyday are very different in binary:</p>
<div><pre><code data-lang="php"><span>&lt;?php</span>

<span>$integer</span> <span>=</span> <span>65</span><span>;</span> <span>// Decimal notation
</span><span></span>
<span>printf</span><span>(</span><span>"Binary notation of %d: %b </span><span>\n</span><span>"</span><span>,</span> <span>$integer</span><span>,</span> <span>$integer</span><span>);</span>
<span>// =&gt; Binary notation of 65: 1000001 
</span></code></pre></div><p>Binary is just another way to represent numbers, <em>and only numbers</em>.</p>
<p>Dave, your colleague developer, is full of questions while reading these lines. “How does a character, such as ‘A’, is saved in memory?”, he wonders. “It’s not a number! It can’t be represented in binary!”.</p>
<p>Dave is right. The character has to be converted first into a decimal number, following the <a href="https://en.wikipedia.org/wiki/ASCII" target="_blank" rel="noopener">ASCII standards</a>. Then, this <em>ASCII code</em> is saved in memory.</p>
<p>Now, let’s try this:</p>
<div><pre><code data-lang="php"><span>&lt;?php</span>

<span>$integer</span> <span>=</span> <span>65</span><span>;</span>
<span>printf</span><span>(</span><span>"Binary representation: %b </span><span>\n</span><span>"</span><span>,</span> <span>$integer</span><span>);</span>
<span>// =&gt; Binary representation: 1000001
</span><span></span>
<span>$character</span> <span>=</span> <span>'A'</span><span>;</span>
<span>$ascii</span> <span>=</span> <span>ord</span><span>(</span><span>'A'</span><span>);</span> <span>// Ascii code of 'A'.
</span><span></span><span>printf</span><span>(</span><span>"Ascii code: %d </span><span>\n</span><span>"</span><span>,</span> <span>$ascii</span><span>);</span>
<span>// =&gt; Ascii code: 65
</span><span></span>
<span>printf</span><span>(</span><span>"Binary representation: %b </span><span>\n</span><span>"</span><span>,</span> <span>$ascii</span><span>);</span>
<span>// =&gt; Binary representation: 1000001 
</span></code></pre></div><p>The integer 65 and the string ‘A’ have the same binary representation in memory!</p>
<p>Dave is confused. In despair, he asks the sky: “How the hell our program knows that <code>$integer</code> is equal to 65, and <code>$character</code> is equal to <code>'A</code>’? How?”. Indeed, in memory, the two values of <code>$character</code> and <code>$integer</code> are exactly the same: <code>1000001</code>. When we use these two variables in our code, the type system of our language will <em>interpret</em> the two values in memory, and it will decide what is a character and what is an integer.</p>
<p>This is important to understand, since this interpretation is not always accurate for some types, like floating point numbers.</p>
<p>A type will determine as well how you store a value in memory. For a character and an integer, we saw that they are stored the same way. The way to store floating point numbers, for example, is quite different.</p>
<p>The memory storage is nicely abstracted by the type system for us, developers, not to think about these confusing 0 and 1. You can then focus on more important problems, at least when the abstraction doesn’t leak. If you’re not sure what’s an abstraction, I wrote <a href="https://thevaluable.dev/abstraction-software-development/">a detailed article about it</a>.</p>
<h3 id="a-set-of-rules">A Set of Rules</h3>
<p>A type system is as well a set of rule, more or less strict. You can’t do everything you want with some types.</p>
<p>Let’s take another example:</p>
<div><pre><code data-lang="php"><span>&lt;?php</span>

<span>printf</span><span>((</span><span>3</span> <span>+</span> <span>"Hello World!"</span><span>)</span> <span>.</span> <span>"</span><span>\n</span><span>"</span><span>);</span>
<span>// =&gt; PHP Warning:  A non-numeric value encountered in /home/myusername/phpgoodies/test.php on line 3
</span><span>// =&gt; 3
</span><span></span>
<span>printf</span><span>(</span><span>"The execution continue!"</span><span>);</span>
<span>// =&gt; The execution continue!
</span></code></pre></div><p>This code makes little sense. I try to reinvent Mathematics by adding the integer 3 to a string. PHP will throw a warning, but it will still give a result, <code>3</code>. When you violate the rules of a type system, the outcome can range between these two extremes:</p>
<ol>
<li>The interpreter or compiler will silently try to fix the problem and continue.</li>
<li>The interpreter or compiler will throw an error and stop.</li>
</ol>
<p>In the case of our example, PHP will throw a warning, but the execution will still continue. You can even get rid of the warning in the infamous <code>php.ini</code>.</p>
<p>To compare with another language, let’s take the exact same thing in Golang:</p>
<div><pre><code data-lang="golang"><span>package</span> <span>main</span>

<span>import</span> <span>(</span>
	<span>"fmt"</span>
<span>)</span>

<span>func</span> <span>main</span><span>()</span> <span>{</span>
    <span>fmt</span><span>.</span><span>Println</span><span>(</span><span>3</span> <span>+</span> <span>"Hello World"</span><span>)</span>
    <span>// =&gt; invalid operation: 3 + "Hello World" (mismatched types untyped int and untyped string)
</span><span></span>    <span>fmt</span><span>.</span><span>Println</span><span>(</span><span>"The execution continue!"</span><span>)</span>
<span>}</span>
</code></pre></div><p><em><a href="https://play.golang.org/p/2JEY0GmHnYF" target="_blank" rel="noopener">Playground</a></em></p>
<p>The compiler will grant you with an error and your program won’t even be compiled.</p>
<h2 id="built-in-types-vs-our-own-abstractions">Built-in Types vs Our Own Abstractions</h2>
<p>Programming languages, more often than not, have a whole set of types you can use, out of the box. These types are called <em>primitive types</em>. For example: <code>integer</code>, <code>boolean</code>, <code>float</code>, and more.</p>
<p>Often, you’ll be able to use as well <em>composite types</em>, a type containing multiple values, and possibly multiple primitive types. It’s what we call more commonly <em>data structures</em>.</p>
<p>For example, an array is a composite type:</p>
<div><pre><code data-lang="php"><span>&lt;?php</span>

<span>$integerArray</span> <span>=</span> <span>[</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>];</span>
<span>$multipleTypeArray</span> <span>=</span> <span>[</span><span>1</span><span>,</span> <span>"two"</span><span>,</span> <span>5.4</span><span>];</span>

</code></pre></div><p>The rules attached to these data types are imposed by the compiler (or the interpreter) of your language of choice.</p>
<p>Since the raise of the <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.136.3043&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Abstract Data Types</a> (ADT), you have the power, in high level programming languages, to create your own types. For example, in PHP:</p>
<div><pre><code data-lang="php"><span>&lt;?php</span>

<span>class</span> <span>Shipment</span>
<span>{</span>
    <span>public</span> <span>function</span> <span>send</span><span>()</span>
    <span>{</span>
        <span>echo</span> <span>"Send powerful shipment!"</span><span>;</span>
    <span>}</span>
<span>}</span>

<span>$shipment</span> <span>=</span> <span>new</span> <span>Shipment</span><span>();</span>
<span>$shipment</span><span>-&gt;</span><span>send</span><span>();</span>
</code></pre></div><p>When you write <code>$shipment = new Shipment()</code>, you create an instance of the class <code>Shipment</code>. The object <code>$shipment</code> can be considered as well of type <code>Shipment</code>.</p>
<p>When you think about it, a type can be seen as a set of possible values, a category, or a group. A class has the <a href="https://www.merriam-webster.com/dictionary/class" target="_blank" rel="noopener">same definition (see entry 3)</a>.</p>
<p>We can say as well that the object <code>$shipment</code> is an abstraction of its class, and its interface (the way you interact with the abstraction) is the method <code>send()</code>.</p>
<p>We created some rules for our new type:</p>
<ul>
<li>The only interface available is the method <code>send()</code>.</li>
<li>The method <code>send()</code> return a string, not an integer.</li>
</ul>
<p>In Golang, you don’t have classes, but you can create custom types, too:</p>
<div><pre><code data-lang="golang"><span>package</span> <span>main</span>

<span>import</span> <span>(</span>
	<span>"fmt"</span>
<span>)</span>

<span>type</span> <span>minute</span> <span>int</span>

<span>func</span> <span>(</span><span>m</span> <span>minute</span><span>)</span> <span>second</span><span>()</span> <span>int</span>  <span>{</span>
    <span>return</span> <span>int</span><span>(</span><span>m</span><span>)</span> <span>*</span> <span>60</span>
<span>}</span>

<span>func</span> <span>main</span><span>()</span> <span>{</span>
	<span>var</span> <span>min</span> <span>minute</span>
	<span>min</span> <span>=</span> <span>2</span>
	<span>fmt</span><span>.</span><span>Printf</span><span>(</span><span>"%d minutes are %d seconds"</span><span>,</span> <span>min</span><span>,</span> <span>min</span><span>.</span><span>second</span><span>())</span>
    <span>// 2 minutes are 120 seconds
</span><span></span><span>}</span>
</code></pre></div><p><em><a href="https://play.golang.org/p/rm1aRW2vZip" target="_blank" rel="noopener">Playground</a></em></p>
<p>In that case, the new type <code>minute</code> gets the same set of rules as the type <code>int</code>. Then, you can attach methods to this new type <code>minute</code>, like the method <code>second()</code>.</p>
<h2 id="type-checking">Type Checking</h2>

<picture>
    <source srcset="https://thevaluable.dev/images/2020/type_system/type_wizard.webp" type="image/webp">
    <img src="https://thevaluable.dev/images/2020/type_system/type_wizard.jpg" alt="Some types are obvious">
</picture>



<p>If types push us to respect some rules, a programming language need an algorithm to check if we respect them. This is called <em>type checking</em>.</p>
<p>Even if type systems can be very similar or very different, depending on the programming language, we are humans, so we need to group these disparate things in categories to understand them.</p>
<p>There are two important categories of type checking: <em>static type …</em></p></section></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://thevaluable.dev/type-system-explained/">https://thevaluable.dev/type-system-explained/</a></em></p>]]>
            </description>
            <link>https://thevaluable.dev/type-system-explained/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24614788</guid>
            <pubDate>Mon, 28 Sep 2020 09:38:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Do things that don't require scale]]>
            </title>
            <description>
<![CDATA[
Score 27 | Comments 2 (<a href="https://news.ycombinator.com/item?id=24614691">thread link</a>) | @davnicwil
<br/>
September 28, 2020 | https://davnicwil.com/do-things-that-dont-require-scale/ | <a href="https://web.archive.org/web/*/https://davnicwil.com/do-things-that-dont-require-scale/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-reactid="41"><div data-reactid="42"><div data-reactid="43"><p data-reactid="45">If you're interested in startups, "Do things that don't scale" is something you've likely heard. To me it's execution advice - in the beginning use your relatively small size as a strength by doing things that have outsized short term impact but are infeasible in the long run at scale.</p><p data-reactid="46"><!-- react-text: 47 -->Here's a corollary for the business idea itself: also make sure you start by doing things that don't <!-- /react-text --><span data-reactid="48">require</span><!-- react-text: 49 --> scale.<!-- /react-text --></p><p data-reactid="51">If your small size is the only advantage you have for execution, you'd better make sure the actual business idea works when it's all you have, too, because execution doesn't matter if the idea fundamentally won't work.</p><p data-reactid="52">It sounds obvious, but it's a mistake many people make - I've made it myself repeatedly - and it's especially hard to spot when you're caught up in how great you think an idea is. It might indeed be great, but you're just not in a position to make it work when you're starting from scatch.</p><p data-reactid="53"><!-- react-text: 54 -->You can do things that don't scale to accelerate a good idea, but by definition you actually can't do that if the idea itself<!-- /react-text --><!-- react-text: 55 --> <!-- /react-text --><span data-reactid="56">requires</span><!-- react-text: 57 --> scale. The idea has to work on day 1, for customer 1.<!-- /react-text --></p><p data-reactid="59">A common root of the issue, I believe, is looking at already successful products and imagining how they could be iterated upon or diverged from in new and interesting ways. Such divergent ideas are often very reasonable, good even, the problem is that they require the same scale to work as the products they are based upon.</p><p data-reactid="60"><!-- react-text: 61 -->It's all too easy to ignore that part, or convince yourself that the huge scale is somehow a positive since it proves there's a strong market for this product. It's the other way round: often the strong market &amp; huge scale <!-- /react-text --><span data-reactid="62">are</span><!-- react-text: 63 --> the product.<!-- /react-text --></p><p data-reactid="65">Here's a few good ways to think about whether your idea is one that requires scale or not. Of course every product is different, and there are many more, but following the 80% rule I think most ideas can be caught with the following:</p><p data-reactid="66">First, look at the history of the product it's based upon or one it's similar to. Did it come out of an already huge company? If not, how did it evolve as it scaled? Did it keep roughly the same manifestation and business model, or did these change substantially? Be skeptical if so.</p><p data-reactid="67">Second, does it have network effects? These are obvious in B2C, less so in B2B. Be skeptical if you have a per-seat pricing model and your strategy is bottom-up expansion within organisations starting from a few accounts. Be extra skeptical if these accounts are free.</p><p data-reactid="68">Third, how synchronous and mission critical is it? Be skeptical if it going down would cause interruption to workflows that couldn't be worked around or deferred. Be incredibly skeptical if this is true round the clock and on weekends. That kind of service level implies significant and robust automation and support, which require scale.</p><p data-reactid="69">Fourth, how much does the business model depend on volume? Losing a bit of money on first customers as you bootstrap and learn is not an issue. Be skeptical, though, if this would need to be sustained to bootstrap your way to some required volume which is quite a way beyond those first few customers.</p></div></div></div></div>]]>
            </description>
            <link>https://davnicwil.com/do-things-that-dont-require-scale/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24614691</guid>
            <pubDate>Mon, 28 Sep 2020 09:23:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[In Defense of XML]]>
            </title>
            <description>
<![CDATA[
Score 69 | Comments 115 (<a href="https://news.ycombinator.com/item?id=24614404">thread link</a>) | @tannhaeuser
<br/>
September 28, 2020 | https://blog.frankel.ch/defense-xml/ | <a href="https://web.archive.org/web/*/https://blog.frankel.ch/defense-xml/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main"> <div> <article itemscope="" itemtype="http://schema.org/BlogPosting"> <meta itemprop="mainEntityOfPage" content="//defense-xml/"> <meta itemprop="description" content="">  <figure itemscope="" itemprop="image" itemtype="http://schema.org/ImageObject"> <meta itemprop="url" content="https://blog.frankel.ch/assets/resources/defense-xml/XML_icon.svg"> </figure> <section> <div itemprop="articleBody"> <p>When I started my career, XML was ubiquitous. The meta-information in a Java JAR file - the manifest - follows a proprietary format. But Java EE designers built it from the ground up on XML: meta-information of all artifacts is in XML format <em>e.g.</em> <code>web.xml</code>, <code>ejb-jar.xml</code>, <code>application.xml</code>, etc.</p> <p>Java EE is one example I experienced personally. But XML was everywhere in the enterprise world at the time. Its prevalence manifested itself in two areas: configuration and data transfer.</p> <p>Ever since then, it would be an euphemism to say XML has been losing in popularity. Other formats, such as JSON and YAML, have replaced it in the hearts of developers. In this post, I’d like to:</p> <ul><li><span>Explore some of the reasons why the mighty XML has fallen</span></li><li><span>Raise some downsides of the popular alternatives</span></li><li><span>And describe how XML already solved those problems</span></li></ul> <div> <h2 id="the-downfall-of-xml">The downfall of XML</h2> <div> <p>I think there are several reasons that led to the downfall of XML. It’s not a single one, but the conjunction of them that led to the current state.</p> <div> <h3 id="associated-with-enterprise">Associated with "Enterprise"</h3> <p>I’m afraid the worst flaw of XML is its close association with the enterprise world. As everybody knows, Enterprise is notoriously bad - by definition: bloated, heavy, not nimble, etc. And yes, that’s sarcasm if you wondered.</p> <p>In general, perception trumps truth. Developers are no different in that regard. In the end, that’s how Hype-Driven Developers - and most developers, perceive XML nowadays.</p> </div> <div> <h3 id="lack-of-integration-with-front-end">Lack of integration with front-end</h3> <p>One of the main usages of XML was in the realm <a href="https://en.wikipedia.org/wiki/SOAP" target="_blank" rel="noopener">SOAP web services</a>. Let’s be frank about it: the ease of consuming those web services from JavaScript and/or the browser is not spectacular.</p> <p>It' no wonder that JavaScript Object Notation, <em>aka</em> JSON became a <em>de facto</em> standard. JSON brought <a href="https://en.wikipedia.org/wiki/Representational_state_transfer" target="_blank" rel="noopener">REST</a> along with it. As its name implies, JSON is JavaScript native, while XML is not.</p> </div> <div> <h3 id="steep-first-steps">Steep first steps</h3> <p>JSON is quite easy to start with, YAML even more so. Even with bare XML, one has the concept of namespaces, which are not beginner-friendly. XML allows one document to use elements from different namespaces. On the flip side, it makes designing simple documents more complicated.</p> <p>XML has a lot of powerful features, but all this power can be confusing to beginners. I willingly admit that they make easy things more complex than they should.</p> </div> <div> <h3 id="performance">Performance</h3> <p>I’ve stumbled upon the performance "argument" a couple of time. This is usually "proven" by using a sample describing the same in XML, JSON and YAML. Because of its opening <strong>and</strong> closing tags, the writer shows that XML is quite noisy compared to the other two.</p> <p>IMHO, this argument is shallow, as all 3 formats are text-based. Thus, you can - and should - compress files. Parsing might be a bit slower, but it depends a lot on the exact parser (and the associated technology stack). In the end, the overhead of transmitting and parsing in XML - if any - is negligible compared to the total time in the whole use-case.</p> <p>People who favor YAML over JSON use the same reasoning: less characters.</p> </div> <div> <h3 id="abuse">Abuse</h3> <p>The above reasons are more or less congruent with XML. Yet, I’m more than willing to admit architects have been abusing XML. I’ve personally seen SOAP webservices with payload in the order of several megabytes. At that time, you might imagine the performance of such design was not stellar.</p> </div> </div> </div> <div> <h2 id="failings-of-alternatives">Failings of alternatives</h2> <div> <p>JSON, YAML &amp; al. all have their own failings. Here’s a sample of them:</p> <ul><li><span>JSON has no comments. The most usual fallback is to use the <code>"_comment"</code> property.</span><div> <div> <pre><code data-lang="json"><span>{</span><span>
  </span><span>"foo"</span><span>:</span><span> </span><span>{</span><span>
    </span><span>"_comment"</span><span> </span><span>:</span><span> </span><span>"My important comment"</span><span>,</span><span>
    </span><span>"bar"</span><span>:</span><span> </span><span>true</span><span>
  </span><span>}</span><span>
</span><span>}</span></code></pre> </div> </div></li><li><span>YAML has no governing body. Individuals manage the specification.</span></li><li><span>YAML has <strong>22 ways to write booleans</strong> - no less!</span></li></ul> <div> <div> <div> <blockquote> <p>Anyone who uses YAML long enough will eventually get burned when attempting to abbreviate Norway.</p> </blockquote>  </div> </div> </div> <div> <div><blockquote><div lang="en" dir="ltr"><p>YAML is to config syntaxes what Python is to programming languages. Seriously, significant whitespaces?</p><p>Have fun with a 500 lines of YAML to configure your Kubernetes deployment, and come back to explain why *you* hate it.</p></div>— Nicolas Frankel (@Home for a long time I believe) (@nicolas_frankel) <a href="https://twitter.com/nicolas_frankel/status/1273888063463325697?ref_src=twsrc%5Etfw">June 19, 2020</a></blockquote>  </div> </div> <p>To cope with the above, other formats have poped-up:</p> <ul><li><span><a href="https://github.com/toml-lang/toml" target="_blank" rel="noopener"><abbr title="Tom’s Obvious, Minimal Language">TOML</abbr></a> draws its inspiration from the <a href="https://en.wikipedia.org/wiki/INI_file" target="_blank" rel="noopener"><code>.ini</code></a> format. It allows nested hierarchies of properties</span></li><li><span>Lightbend pushes the <abbr title="Human-Optimized Config Object Notation">HOCON</abbr> format:</span></li></ul> <div> <div> <div> <blockquote> <p>This is an informal spec, but hopefully it’s clear.</p> </blockquote>  </div> <p>This one statement doesn’t fill me confidence.</p> </div> </div> </div> </div> <div> <h2 id="the-original-sin-the-lack-of-grammar">The original sin: the lack of grammar</h2> <div> <p>Whatever the format, regardless of their own specific downsides, one of the most important issue is for clients to decide if the read data is <em>correct</em> or not.</p> <p>When using JSON and YAML, the different clients need to provide <em>ad hoc</em> validation. Issues arise when the provider changes the data format:</p> <ol><li><span>How to make clients aware that the format changed?</span></li><li><span>What information to communicate to the client about the format change?</span></li><li><span>How to keep validation synchronized across clients?</span></li></ol> <p>XML has this issue solved since the beginning by providing a <strong>grammar</strong>. A grammar plays the same role for a XML document as constraints and types in a SQL database. The most important difference is that you can externalize the grammar.</p> <p>Several XML grammar implementations are available: <a href="https://www.w3.org/XML/1998/06/xmlspec-report-19980910.htm" target="_blank" rel="noopener">Document Type Definition</a>, <a href="https://www.w3.org/XML/Schema" target="_blank" rel="noopener">XML Schema</a>, <a href="https://relaxng.org/" target="_blank" rel="noopener">Relax NG</a>, etc. The most widespread one is XML Schema. Since a XML Schema is also written in XML format, a web server can host it. Then you can reference it by a publicly-accessible URL.</p> <p>This approach solves the above issues: when a client receives an XML document, the former looks at the XML Schema URL. It can then fetch it, and check that the data conforms to the schema.</p> <p>Changing the data format is as simple as versioning the XML Schema file, and publishing it under a new URL.</p> </div> </div> <div> <h2 id="other-benefits-of-xml">Other benefits of XML</h2> <div> <p>In this section, I’d like to list a couple of benefits of using XML.</p> <div> <dl> <dt>Public open stewardship</dt> <dd> <p>XML is not under the stewardship of a single person or a company, but of a <abbr title="Non-Governmental Organization">NGO</abbr>, namely the <a href="https://www.w3.org/" target="_blank" rel="noopener">W3C</a>. A W3C specification has a <a href="https://www.w3.org/2019/Process-20190301/" target="_blank" rel="noopener">publicly documented process and defined lifecycle</a>.</p> </dd> <dt>Battle-proven</dt> <dd> <p>XML is not hype, but benefits from plenty of documentation, blog posts, and <a href="https://stackoverflow.com/questions/tagged/xml" target="_blank" rel="noopener">FAQs</a> available</p> </dd> <dt>Composable</dt> <dd> <p>While XML doesn’t strictly enforces namespaces, it’s considered a good practice. This way, similarly-named entities defined in different namespaces can co-exist in the same document without confusion about semantics.</p> </dd> <dt>Different flavors</dt> <dd> <p>XML parsing comes into two flavors:</p> <ol><li><span>Tree-based parsing <em>i.e.</em> <a href="https://dom.spec.whatwg.org/" target="_blank" rel="noopener">Document Object Model</a>. It loads the whole document in memory</span></li><li><span>Event-based parsing <em>i.e.</em> <a href="http://www.saxproject.org/" target="_blank" rel="noopener">Simple API for XML</a>. It makes possible the parsing of large documents. Note that SAX is <strong>not</strong> a W3C specification.</span></li></ol> </dd> <dt>Implementation in different languages</dt> <dd> <p>Every commonly-used language in the industry offers at least one XML parsing implementation. This is either baked in the standard library that comes along the language, or available in a third-party one. Here are a couple of them:</p> </dd> </dl> </div>  <div> <dl> <dt>Document transformation</dt> <dd> <p><abbr title="eXtensible Stylesheet Language Transformation">XSLT</abbr> is <a href="https://www.w3.org/TR/1999/REC-xslt-19991116" target="_blank" rel="noopener">a W3C specification</a>. It allows to transform one XML document into another document in a declarative way. Target documents can be either XML themselves, or not.</p> </dd> <dt>Document querying</dt> <dd> <p><abbr title="XML Path Language">XPath</abbr> is <a href="https://www.w3.org/TR/2017/REC-xpath-31-20170321/" target="_blank" rel="noopener">another W3C specification</a>. It defines how to query XML documents, similar to CSS selectors.</p> </dd> </dl> </div> </div> </div> <div> <h2 id="conclusion">Conclusion</h2> <div> <p>XML has a lot of advantages compared to other more alternative technologies. In addition to what I described above, it benefits from a <a href="https://www.w3.org/standards/xml/" target="_blank" rel="noopener">rich ecosystem</a>.</p> <p>It’s not considered hype by a lot of young (and not so young) developers. I believe would be beneficial if our industry would value more battle-proven technologies than new shiny ones.</p> </div> </div>    </div> </section>   </article> </div> </div></div>]]>
            </description>
            <link>https://blog.frankel.ch/defense-xml/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24614404</guid>
            <pubDate>Mon, 28 Sep 2020 08:40:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[TILs about Node.js Fundamentals from the Node.js Design Patterns Book]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24614312">thread link</a>) | @loige
<br/>
September 28, 2020 | https://www.swyx.io/til-node-fundamentals-design-patterns-book/ | <a href="https://web.archive.org/web/*/https://www.swyx.io/til-node-fundamentals-design-patterns-book/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p>I started reading <a href="https://www.nodejsdesignpatterns.com/">Node.js Design Patterns</a> this week. I got the Third Edition, and have not spent any time looking into what's changed from prior editions. The first 6 chapters cover fundamental knowledge, before getting into the meaty named Design Patterns, so these notes are from that first "half" of the book.</p>
<section>
  <h2 id="1-libuv-and-the-reactor-pattern"><a href="#1-libuv-and-the-reactor-pattern">1. <code>libuv</code> and the Reactor Pattern</a></h2>
  <p><code>libuv</code> is something I've often heard about as a low level Node.js library, but now I have a glimpse of what it does for us. As the book says:</p>
  <blockquote>
    <p>Libuv represents the low-level I/O engine of Node.js and is probably the most important component that Node.js is built on. Other than abstracting the underlying system calls, libuv also implements <strong>the reactor pattern</strong>, thus providing an API for creating event loops, managing the event queue, running asynchronous I/O operations, and queuing other types of task.</p>
  </blockquote>
  <p>The <a href="https://en.wikipedia.org/wiki/Reactor_pattern">Reactor pattern</a>, together with demultiplexing, <a href="https://www.youtube.com/watch?v=8aGhZQkoFbQ">event queues and the event loop</a>, is core to how this works - a tightly coordinated dance of feeding async events into a single queue, executing them as resources free up, and then popping them off the event queue to call callbacks given by user code.</p>
  <p>
    <img src="https://dev-to-uploads.s3.amazonaws.com/i/mj0r6gwfwiv5nqcfhkn5.png" alt="Alt Text">
  </p>
</section>
<section>
  <h2 id="2-module-design-patterns"><a href="#2-module-design-patterns">2. Module Design Patterns</a></h2>
  <p>I am superficially familiar with the differences between CommonJS modules and ES Modules. But I liked the explicit elaboration of 5 module definition patterns in CommonJS:</p>
  <ul>
    <li>Named exports: <code>exports.foo = () =&gt; {}</code></li>
    <li>Exporting a function: <code>module.exports = () =&gt; {}</code></li>
    <li>Exporting a class: <code>module.exports = class Foo() {}</code></li>
    <li>Exporting an instance: <code>module.exports = new Foo()</code> which is <em>like</em> a singleton, except when it is not because of multiple instances of the same module.</li>
    <li>Monkey patching other modules (useful for <a href="https://github.com/nock/nock">nock</a>)</li>
  </ul>
  <p>In ES Modules, I enjoyed the explanation of "read-only live bindings", which will look weird to anyone who has never seen it and has always treated modules as stateless chunks of code:</p>
  <pre><code><span>// counter.js</span>
<span>export</span><span> </span><span>let</span><span> count </span><span>=</span><span> </span><span>0</span>
<span>export</span><span> </span><span>function</span><span> </span><span>increment</span><span> </span><span>()</span><span> </span><span>{</span>
<span>   </span><span>count</span><span>++</span><span> </span>
<span>}</span>

<span>// main.js</span>
<span>import</span><span> </span><span>{</span><span> </span><span>count</span><span>,</span><span> </span><span>increment</span><span> </span><span>}</span><span> </span><span>from</span><span> </span><span>'</span><span>./counter.js</span><span>'</span>
<span>console</span><span>.</span><span>log</span><span>(count) </span><span>// prints 0</span>
<span>increment</span><span>()</span>
<span>console</span><span>.</span><span>log</span><span>(count) </span><span>// prints 1</span>
<span>count</span><span>++</span><span> </span><span>// TypeError: Assignment to constant variable!</span>
</code></pre>
  <p>This mutable module internal state pattern is endemic in <a href="https://twitter.com/swyx/status/1221586490674696193?s=20">Svelte and Rich Harris' work</a> and I enjoy how simple it makes code look. I don't know if there are scalability issues with this pattern but so far it seems to work fine for ES Modules people.</p>
  <p>The last important topic I enjoyed was ESM and CJS interop issues. <code>ESM</code> doesn't offer <code>require</code>, <code>__filename</code> or <code>__dirname</code>, so you have to reconstruct them if needed:</p>
  <pre><code><span>import</span><span> </span><span>{</span><span> </span><span>fileURLToPath</span><span> </span><span>}</span><span> </span><span>from</span><span> </span><span>'</span><span>url</span><span>'</span>
<span>import</span><span> </span><span>{</span><span> </span><span>dirname</span><span> </span><span>}</span><span> </span><span>from</span><span> </span><span>'</span><span>path</span><span>'</span>
<span>const</span><span> __filename </span><span>=</span><span> </span><span>fileURLToPath</span><span>(</span><span>import</span><span>.</span><span>meta</span><span>.</span><span>url) </span>
<span>const</span><span> __dirname </span><span>=</span><span> </span><span>dirname</span><span>(__filename)</span>

<span>import</span><span> </span><span>{</span><span> </span><span>createRequire</span><span> </span><span>}</span><span> </span><span>from</span><span> </span><span>'</span><span>module</span><span>'</span>
<span>const</span><span> require </span><span>=</span><span> </span><span>createRequire</span><span>(</span><span>import</span><span>.</span><span>meta</span><span>.</span><span>url)</span>
</code></pre>
  <p>ESM also cannot natively import JSON, as of the time of writing, whereas CJS does. You can work around this with the <code>require</code> function from above:</p>
  <pre><code><span>import</span><span> </span><span>{</span><span> </span><span>createRequire</span><span> </span><span>}</span><span> </span><span>from</span><span> </span><span>'</span><span>module</span><span>'</span>
<span>const</span><span> require </span><span>=</span><span> </span><span>createRequire</span><span>(</span><span>import</span><span>.</span><span>meta</span><span>.</span><span>url) </span>
<span>const</span><span> data </span><span>=</span><span> </span><span>require</span><span>(</span><span>'</span><span>./data.json</span><span>'</span><span>) </span>
<span>console</span><span>.</span><span>log</span><span>(data)</span>
</code></pre>
  <p>Did you know that? I didn't!</p>
  <blockquote>
    <p>In semi-related news - <a href="https://twitter.com/MylesBorins/status/1311033983824793601?s=20">Node v14.13 will allow named imports from CJS modules</a> - probably the last step in ESM "just working" in Node.js</p>
  </blockquote>
</section>
<section>
  <h2 id="3-unleashing-zalgo"><a href="#3-unleashing-zalgo">3. Unleashing Zalgo</a></h2>
  <p>APIs are usually either sync or async in Node.js, but TIL you can design APIs that are <em>both</em>:</p>
  <pre><code><span>function</span><span> </span><span>createFileReader</span><span> </span><span>(</span><span>filename</span><span>)</span><span> </span><span>{</span><span> </span>
<span>  </span><span>const</span><span> </span><span>listeners</span><span> </span><span>=</span><span> [] </span>
<span>  </span><span>inconsistentRead</span><span>(</span><span>filename</span><span>,</span><span> </span><span>value</span><span> </span><span>=&gt;</span><span> </span><span>{</span>
<span>    </span><span>listeners</span><span>.</span><span>forEach</span><span>(</span><span>listener</span><span> </span><span>=&gt;</span><span> </span><span>listener</span><span>(</span><span>value</span><span>)) </span>
<span>  </span><span>}</span><span>)</span>
<span>  </span><span>return</span><span> </span><span>{</span>
<span>    </span><span>onDataReady</span><span>:</span><span> </span><span>listener</span><span> </span><span>=&gt;</span><span> </span><span>listeners</span><span>.</span><span>push</span><span>(</span><span>listener</span><span>) </span>
<span>  </span><span>}</span>
<span>}</span>
</code></pre>
  <p>This looks innocent enough, except when you use it as async and then sync:</p>
  <pre><code><span>const</span><span> reader1 </span><span>=</span><span> </span><span>createFileReader</span><span>(</span><span>'</span><span>data.txt</span><span>'</span><span>)  </span><span>// async</span>
<span>reader1</span><span>.</span><span>onDataReady</span><span>(data </span><span>=&gt;</span><span> </span><span>{</span>
<span>   </span><span>console</span><span>.</span><span>log</span><span>(</span><span>`</span><span>First call: </span><span>${</span><span>data</span><span>}`</span><span>)</span>
<span>   </span><span>const</span><span> </span><span>reader2</span><span> </span><span>=</span><span> </span><span>createFileReader</span><span>(</span><span>'</span><span>data.txt</span><span>'</span><span>)  </span><span>// sync</span>
<span>   </span><span>reader2</span><span>.</span><span>onDataReady</span><span>(</span><span>data</span><span> </span><span>=&gt;</span><span> </span><span>{</span>
<span>     </span><span>console</span><span>.</span><span>log</span><span>(</span><span>`</span><span>Second call: </span><span>${</span><span>data</span><span>}`</span><span>) </span>
<span>   </span><span>}</span><span>)</span>
<span>}</span><span>)</span>
<span>// only outputs First call - never outputs Second call</span>
</code></pre>
  <p>This is because module caching in Node makes the first call async and the second call sync. <a href="https://blog.izs.me/2013/08/designing-apis-for-asynchrony">izs famously called this "releasing Zalgo"</a> in a blogpost.</p>
  <p>You can keep Zalgo caged up by:</p>
  <ul>
    <li>using direct style functions for synchronous APIs (instead of <a href="https://en.wikipedia.org/wiki/Continuation-passing_style">Continuation Passing Style</a>)</li>
    <li>make I/O purely async by only using async APIs, using CPS, and deferring synchronous memory reads by using <code>process.nextTick()</code></li>
  </ul>
  <p>The same line of thinking can also be done for EventEmitter Observers as it is for Callbacks.</p>
  <blockquote>
    <p>Callbacks should be used when a result must be returned in an asynchronous way, while events should be used when there is a need to communicate that something has happened.</p>
  </blockquote>
  <p>You can combine both the Observer and Callback patterns, for example with the <a href="https://www.npmjs.com/package/glob"><code>glob</code> package</a> which takes both a callback for its simplier, critical functionality and a <code>.on</code> for advanced events.</p>
  <p>A note on ticks and microtasks:</p>
  <ul>
    <li><code>process.nextTick</code> sets up a microtask, which executes just after the current operation and before any other I/O</li>
    <li>whereas <code>setImmediate</code> runs after ALL I/O events have been processed.</li>
    <li><code>process.nextTick</code> executes earlier, but runs the risk of I/O [starvation] (<a href="https://en.wikipedia.org/wiki/Starvation_(computer_science)">https://en.wikipedia.org/wiki/Starvation_(computer_science)</a>) if takes too long.</li>
    <li><code>setTimeout(callback, 0)</code> is yet another phase behind <code>setImmediate</code>.</li>
  </ul>
</section>
<section>
  <h2 id="4-managing-async-and-limiting-concurrency-with-async"><a href="#4-managing-async-and-limiting-concurrency-with-async">4. Managing Async and Limiting Concurrency with <code>async</code></a></h2>
  <p>It's easy to spawn race conditions and accidentally launch unlimited parallel execution bringing down the server, with Node.js. The <a href="https://github.com/caolan/async">Async</a> library gives battle tested utilities for defining and executing these issues, in particular, queues that offer limited concurrency.</p>
  <p>The book steps you through 4 versions of a simple web spider program to develop the motivations for requiring managing async processes and describe the subtle issues that present themselves at scale. I honestly cant do it justice, I didn't want to just copy out all the versions and discussions of the web spider project as that is a significant chunk of the book, you're just gonna have to read thru these chapters yourself.</p>
</section>
<section>
  <h2 id="5-streams"><a href="#5-streams">5. Streams</a></h2>
  <p>I've often commented that <a href="https://twitter.com/swyx/status/1201528574236217345">Streams are the best worst kept secret of Node.js</a>. Time to learn them. Streams are more memory and CPU efficient than full buffers, but they are also more <em>composable</em>.</p>
  <p>Each stream is an instance of <code>EventEmitter</code>, streaming either binary chunks or discrete objects. Node offers <a href="https://nodejs.org/api/stream.html#stream_api_for_stream_consumers">4 base abstract stream classes</a>:</p>
  <ul>
    <li><code>Readable</code> (where you can read in <a href="https://nodejs.org/api/stream.html#stream_two_reading_modes">flowing (push) or paused (pull) mode</a>)</li>
    <li><code>Writable</code> - you're probably familiar with <code>res.write()</code> from Node's <code>http</code> module</li>
    <li><code>Duplex</code>: both readable and writable</li>
    <li><code>Transform</code>: a special duplex stream with two other methods: <code>_transform</code> and <code>_flush</code>, for data transformation</li>
    <li><code>PassThrough</code>: a <code>Transform</code> stream that doesnt do any transformation - useful for observability or to implement late piping and lazy stream patterns.</li>
  </ul>
  <pre><code><span>import</span><span> </span><span>{</span><span> </span><span>PassThrough</span><span> </span><span>}</span><span> </span><span>from</span><span> </span><span>'</span><span>stream</span><span>'</span>
<span>let</span><span> bytesWritten </span><span>=</span><span> </span><span>0</span>
<span>const</span><span> monitor </span><span>=</span><span> </span><span>new</span><span> </span><span>PassThrough</span><span>() </span>
<span>monitor</span><span>.</span><span>on</span><span>(</span><span>'</span><span>data</span><span>'</span><span>,</span><span> </span><span>(</span><span>chunk</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>{</span>
<span>  </span><span>bytesWritten</span><span> </span><span>+=</span><span> </span><span>chunk</span><span>.</span><span>length</span><span> </span>
<span>}</span><span>)</span>
<span>monitor</span><span>.</span><span>on</span><span>(</span><span>'</span><span>finish</span><span>'</span><span>,</span><span> </span><span>()</span><span> </span><span>=&gt;</span><span> </span><span>{</span><span> </span>
<span>  </span><span>console</span><span>.</span><span>log</span><span>(</span><span>`${</span><span>bytesWritten</span><span>}</span><span> bytes written</span><span>`</span><span>)</span>
<span>}</span><span>)</span>
<span>monitor</span><span>.</span><span>write</span><span>(</span><span>'</span><span>Hello!</span><span>'</span><span>) monitor</span><span>.</span><span>end</span><span>()</span>

<span>// usage</span>
<span>createReadStream</span><span>(filename)</span>
<span> </span><span>.</span><span>pipe</span><span>(</span><span>createGzip</span><span>())</span>
<span> </span><span>.</span><span>pipe</span><span>(monitor) </span><span>// passthrough stream!</span>
<span> </span><span>.</span><span>pipe</span><span>(</span><span>createWriteStream</span><span>(</span><span>`${</span><span>filename</span><span>}</span><span>.gz</span><span>`</span><span>))</span>
</code></pre>
  <p>izs recommends <a href="https://www.npmjs.com/package/minipass">minipass</a> which implement a PassThrough stream with some better features. Other useful stream utils:</p>
  <ul>
    <li><a href="https://github.com/maxogden/mississippi">https://github.com/maxogden/mississippi</a></li>
    <li><a href="https://www.npmjs.com/package/streamx">https://www.npmjs.com/package/streamx</a></li>
    <li>You can make streams lazy (create proxies for streams, so the stream instance isn't until some piece of code is consuming) with <a href="https://www.npmjs.com/package/lazystream">lazystream</a>.</li>
  </ul>
  <p>Although the authors do recommend that piping and error handling be best organized with the native <a href="https://nodejs.org/api/stream.html#stream_stream_pipeline_source_transforms_destination_callback">stream.pipeline</a> function.</p>
</section>
</div></div>]]>
            </description>
            <link>https://www.swyx.io/til-node-fundamentals-design-patterns-book/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24614312</guid>
            <pubDate>Mon, 28 Sep 2020 08:28:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The patch-based Git workflow]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24613989">thread link</a>) | @garritfra
<br/>
September 28, 2020 | https://garrit.xyz/blog/patch-based-git-workflow/ | <a href="https://web.archive.org/web/*/https://garrit.xyz/blog/patch-based-git-workflow/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>September 28, 2020</p><p>If you have ever contributed to an open source project, chances are you have opened a pull request on GitHub or a similar platform to present your code to the maintainers. While this is a very approachable way of getting your code reviewed, some projects have decided against using pull requests and instead accept patches via email.</p><h2 id="an-introduction-to-patches"><a href="#an-introduction-to-patches"></a>An introduction to patches</h2><p>A patch is essentially a git commit expressed in plain text. It describes what commit the change is based on, and what has changed. A basic patch looks like this:</p><div><pre><p><span>From 92132241233033a123c4fa833449d6a0d550219c Mon Sep 17 00:00:00 2001</span></p><p><span>From: Bob &lt;bob@example.com&gt;</span></p><p><span>Date: Tue, 25 May 2009 15:42:16 +0200</span></p><p><span>Subject: [PATCH 1/2] first change</span></p><p><span>---</span></p><p><span> test.txt |    1 +-</span></p><p><span> 1 files changed, 1 insertions(+), 1 deletions(-)</span></p><p><span>diff --git a/test.txt b/test.txt</span></p><p><span>index 7634da4..270eb95 100644</span></p><p><span>--- a/test.txt</span></p><p><span>+++ b/test.txt</span></p><p><span>@@ -1 +1 @@</span></p><p><span>-Hallo Bob</span></p><p><span>+Hallo Alice!</span></p></pre></div><p>As you can see, it is very readable for both the reviewer and the machine.</p><h2 id="sending-and-receiving-patches"><a href="#sending-and-receiving-patches"></a>Sending and receiving patches</h2><p>The easiest way you can generate a patch from a commit is to use <code>git-format-patch</code>:</p><p>This will generate a <code>.patch</code> file, that can be embedded into an email and sent to the maintainers. Oftentimes they will then reply to your mail with some inline comments about your code.</p><p>To simplify this process further, git has the <code>send-email</code> command, which let’s you send the patch directly to someone without needing to embed it manually. I won’t go into details about this, but there is a <a href="https://git-send-email.io/">well written guide</a> on how to set it up.</p><p>If you have received a patch from someone, you can apply it to your tree with the <code>am</code> (apply mail) command:</p><div><pre><p><span>git am &lt; 0001-first-change.patch</span></p></pre></div><p>check your <code>git log</code> to see the patch in form of the latest commit.</p><h2 id="why-even-bother"><a href="#why-even-bother"></a>Why even bother</h2><p>You might think that this is just a silly and outdated approach to collaborative development. “Why not simply open a pull request?” you might ask. Some projects, especially low-level oriented ones like the Linux kernel, do not want to rely on third-party platforms like GitHub to host their code, with good reasons:</p><ol><li>Everyone can participate! You don’t need to register an account on some proprietary website to collaborate in a project that uses a patch-based workflow. You don’t even have to expose your identity, if you don’t want to. All you need is an email-address, and frankly most of us have one.</li><li>It’s plain simple! Once you get used to generating and applying patches on the command line, it is in fact easier and faster than opening a pull request in some clunky GUI. It doesn’t get simpler than plain text.</li><li>It is rewarding! Once you have submitted a patch to a project, there is no better feeling than getting a simple “Applied, thanks!” response from a maintainer. And if it’s a response that contains feedback rather than an approval, it feels even better to submit that reworked code again and get it eventually applied.</li></ol><h2 id="conclusion"><a href="#conclusion"></a>Conclusion</h2><p>The patch-based workflow is an alternative way to collaborate with developers. If it helps you in your day to day business depends on the projects you are contributing to, but in the end it is always good to have many tools under your belt.</p></div></div>]]>
            </description>
            <link>https://garrit.xyz/blog/patch-based-git-workflow/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24613989</guid>
            <pubDate>Mon, 28 Sep 2020 07:35:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Low Code Evaluation Tool]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24613739">thread link</a>) | @kinj28
<br/>
September 28, 2020 | https://www.dronahq.com/low-code-evaluation-tool/?ref=show-hn | <a href="https://web.archive.org/web/*/https://www.dronahq.com/low-code-evaluation-tool/?ref=show-hn">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="nz-content">
	<div>
			<!-- post start -->
	<div id="post-10448" class="page">
		<section>
			<div data-animation-speed="35000" data-parallax="false"><div><div><div data-effect="none" data-align="left"><div>
        <!-- ******************************************************************************** -->
    <!-- Block 7 CFA - Form -->
    <!-- ******************************************************************************** -->
    <div id="download">
      <div>
        <div><div><p>How top enterprise IT Leaders are selecting the right tools for accelerating growth at speed?</p>
<p>Evaluation Tool takes into consideration various parameters, providing clear direction for enterprise leaders. It offers a practical way forward with selecting the right platform.</p>
<p>Download it today.</p>
</div>
            
        </div>
        
      </div>
    </div></div></div></div></div></div><div data-animation-speed="35000" data-parallax="false"><div><div><div data-effect="none" data-align="left"><div>
        <!-- ******************************************************************************** -->
    <!-- Block 7 CFA - Form -->
    <!-- ******************************************************************************** -->
    <div id="">
      <div>
        <div><p> <h2>How app development platforms rank  </h2> </p>
            
        </div>
        <div>
            <p><img src="https://www.dronahq.com/wp-content/uploads/2020/08/Integration-Pages-Illustrations-1.png" alt="" width="660" height="460" srcset="https://www.dronahq.com/wp-content/uploads/2020/08/Integration-Pages-Illustrations-1.png 660w, https://www.dronahq.com/wp-content/uploads/2020/08/Integration-Pages-Illustrations-1-300x209.png 300w" sizes="(max-width: 660px) 100vw, 660px"></p>
            
        </div>
      </div>
    </div></div></div></div></div></div><div data-animation-speed="35000" data-parallax="false"><div><div><div data-effect="none" data-align="left"><div><!-- ******************************************************************************** -->
    <!-- Feature - Three Columns -->
    <!-- ******************************************************************************** -->
    <div id="">
        <div>
            <p>
                        How this scorecard works</p>
            <p>This tool was built work as a simple litmus test for you to evaluate necessary LC/NC platform capabilities for your enterprise.</p>
            <div>
        <div>
                    <p><img src="https://www.dronahq.com/wp-content/uploads/2019/06/Work-Order-Maintenance.svg" alt="Inventory processes management">
                    </p>
                    <p>
                        Scoring a Platform</p>
                    <p>Head over to ‘Scoring’ tab. Enter the vendor name, date of the evaluation and start evaluating vendors against the parameters. Click on the drop-down cells to select the best-fit answer. A score will automatically get assigned.</p>
                    
                </div>
        <div>
                    <p><img src="https://www.dronahq.com/wp-content/uploads/2019/07/HR-Apps.svg" alt="visbility and improves analytics">
                    </p>
                    <p>
                        Comparing Vendors</p>
                    <p>For every criterion listed in the ‘Scoring’ tab, a weight has been assigned to parameters that facilitate app development. Under the ‘Comparison’ tab, you can find the aggregate weight calculated.</p>
                    
                </div>
        <div>
                    <p><img src="https://www.dronahq.com/wp-content/uploads/2019/06/Better-Customer-Satisfaction.svg" alt="Better Customer Satisfaction">
                    </p>
                    <p>
                        No-Code Weightage</p>
                    <p>We assigned the greatest weight to parameters that facilitate development without coding on account of the high speed of development no-code provides. The weight reduces as we move towards code.</p>
                    
                </div>
        </div>
        </div>
    </div>
        <!-- ********************** -->
        <!-- block 4 Single Image -->
        <!-- ********************** -->
        <div id="">
            <div><p>Trusted by </p> 
                
               
                   <p><img src="https://www.dronahq.com/wp-content/uploads/2018/01/DHQ-Customers-stripe.png" alt="DronaHQ Customer using Low Code app development tools"></p>                
            </div>
        </div></div></div></div></div></div>
		</section>
	</div>
	<!-- post end -->
	</div>
</div><div>
				
											
							
            <p>Copyright © Deltecs Infotech Pvt Ltd. All Rights Reserved</p>						
														
			</div></div>]]>
            </description>
            <link>https://www.dronahq.com/low-code-evaluation-tool/?ref=show-hn</link>
            <guid isPermaLink="false">hacker-news-small-sites-24613739</guid>
            <pubDate>Mon, 28 Sep 2020 07:01:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Stupid solutions: Live server push without JS]]>
            </title>
            <description>
<![CDATA[
Score 121 | Comments 23 (<a href="https://news.ycombinator.com/item?id=24613610">thread link</a>) | @lawik
<br/>
September 27, 2020 | https://underjord.io/live-server-push-without-js.html | <a href="https://web.archive.org/web/*/https://underjord.io/live-server-push-without-js.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<small>2020-09-25</small><!-- RSS:2020-09-25T17:00:00Z -->
<p>
    So in my post <a href="https://underjord.io/is-this-evil.html">Is this evil?</a> I covered a way of tracking users with CSS. While
    thinking about those weird ways of using the web I also started thinking about pushing live data to clients
    without JS. Or at least maintaining a connection.
    So WebSockets requires JS. WebRTC requires JS. Even HLS (video streaming), which would otherwise
    be super cool, with captions for accessibility. But no. Or rather, maybe on Apple platforms. Eh. Not good enough.
</p>
<p>And then it hit me. From some old Nerves projects I'd seen, that there is a standard for just sending a stream of
    JPEG frames as a video. MJPEG. Did you know about MJPEG? Lots of people don't. It is used by lots of webcams and
    security cameras. Common option for Raspberry Pi hacks as well. MJPEG is super simple which is its big advantage.
</p>
<p>But video is what we expect. I was going for something else. So this could be used for a CI status light, showing
    any amount of visual status information. I use it for this:</p>
<p><img alt="live visual readership indicator" loading="lazy" src="https://count.underjord.io/"></p>
<p>That's live. Or dead if my server falls over.</p>
<p>So how does MJPEG work. Well, you take an <code>&lt;img&gt;</code> tag and you shove an MJPEG URL into it. Done.</p>
<p>Okay, that's how you use it. Not how it works. I implemented it in Elixir, Elixir is quite good at keeping state and
    serving updates. Links are below. But basically the browser opens the connection, receives some headers and some
    chunks of data and then realizes it is dealing with MJPEG. It wil then just expect the chunks to keep coming.
    Indefinitely. Because this is live video. Frame by frame of JPEG.</p>
<p>The basic code for the MJPEG headers and chunking was lifted from a pi camera repo made by the Nerves team. It had a
    lot of Frank Hunleth and Connor Rigby in it so kudos to those guys as always. This is what I did with it: <a href="https://github.com/lawik/mjpeg/blob/master/lib/mjpeg.ex">lawik/mjpeg</a></p>
<p>My server implementation is here and uses the above code: <a href="https://github.com/lawik/mjpeg_example/blob/master/lib/mjpeg_example.ex">lawik/mjpeg_example</a></p>
<p>So I receive the connection and then that calls my MjpegExample GenServer to persist the connection and keep track of
    how to notify that connection about new data. It also triggers an update to notify everyone already connected.</p>
<p>This is not polished, it is hammered together and I'm curious to see if it falls over the next time I get a decent
    amount of traffic.</p>
<p>I really like this approach because it is a fun hack that simply happens to work across browsers and quite well at
    that. I like how it is just an img element and no frills. I added lazy loading because that works more nicely with
    things like Google Lighthouse scores and the loading experience (your browser doesn't spend a few seconds thinking
    about loading the image).</p>
<p>Unfortunately it is absolutely a poor choice to actually use aside from fun and hacky stuff. There is no good way of
    doing accessibility with it. You can update the pixels and that is it. Unless you can chunk-stream a txt-file in an
    iframe.
    Haven't tried that yet...</p>
<p>So, don't use it. But isn't it pretty neat?</p>
<p>Of course, much like the CSS tracking, this can be used for evilish things. You can absolutely keep track of how long
    someone keeps receiving your frames and use that for your analytics. I also find it neat that it lets me know
    concurrent users. I just log the number along with sending it out because I want to know at what number it breaks
    and out of curiosity but you can probably do some mildly untoward things with this. Oh.. Wait. You could serve
    rotating ads with this. You know what the last frame you sent was so if you get a click on it you can direct that
    particular user to the right thing. New title: Ad placement entirely without JS, ugh, no thanks. Moving on.</p>
<p>If you know how to make this more dirty, hacky and fun or even more useful or accessible feel free to get in touch at
    <a href="mailto:lars@underjord.io">lars@underjord.io</a> or on
    Twitter where I'm <a href="https://twitter.com/lawik">@lawik</a>.</p><p><img alt="live visual readership indicator" loading="lazy" src="https://count.underjord.io/"></p>

</div></div>]]>
            </description>
            <link>https://underjord.io/live-server-push-without-js.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24613610</guid>
            <pubDate>Mon, 28 Sep 2020 06:36:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Absolute Minimum Every Engineer Must Know About Authentication and Encryption]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24611972">thread link</a>) | @ldelossa
<br/>
September 27, 2020 | https://www.ldelossa.is/blog/absolute-minimum-cryptography | <a href="https://web.archive.org/web/*/https://www.ldelossa.is/blog/absolute-minimum-cryptography">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div> <div><article>  <p>It's common to hear engineers muttering "cryptography is scary" or "oh no its a cert problem."</p> <p>The topic is a dense one full of maths, mailing lists, and vulnerabilities which quite literally shock the world. It's understandable how a lot of engineers put learning about the topics to another day.</p> <p>In my career I have been asked to build two different certificate signing backends for IoT purposes. These tasks provided the opportunity to work with authentication, encryption, and cryptography at a lower level then typical.</p> <p>This post will outline the bare minimum engineers should understand before working with authentication and encryption systems.</p> <h2>Part 1: Conceptual Overview</h2> <p>This section will provide a gentle introduction to authentication and encryption. These topics are to be read as a conceptual overview and not as literal implementation details.</p> <h3>Authentication and Encryption</h3> <p>Cryptography can lend itself to many utilities but as software developers our usage centers around authentication and encryption.</p> <p>Authentication is the act of identification. Cryptography can guarantee authentication and thus provide trust that the subject you are communicating with is indeed who they say they are. Authentication is implemented by cryptographic signing.</p> <p>Encryption is the act of concealing communications from unintended audiences. When a communication is encrypted it is guaranteed to be viewable only by the intended party.</p> <p>Authentication and encryption are used together to device a notion of trust in our applications and on the internet.</p> <h3>Cryptographic Signing</h3> <p>Cryptographically signing a message proves authentication in a single direction. It works like so:</p> <ul> <li>Sender: constructs a message to be sent.</li> <li>Sender: constructs a key to sign the message with.</li> <li>Sender: uses a signing algorithm to sign the bits of the message with the constructed keys.</li> <li>Sender: sends message along with signature to client.</li> <li>Receiver: receives the message and signature.</li> <li>Receiver: retrieves the sender's key.</li> <li>Receiver: uses the sender's key to sign the received message.</li> <li>Receiver: compares the sender's signature with the one computed by the receiver itself.</li> </ul> <p>If the receiver sees both signatures as equal and knows it can trust the key used to create the signature, the receiver can trust the message is from the sender.</p> <p>The various ways to securely transfer the sender's key to the receiver will be covered a bit later in the post.</p> <p>Typical signing algorithms are:</p> <ul> <li>HS256 - HMAC with SHA256</li> <li>RSA256 - RSA Signature with SHA256</li> </ul> <h3>Cryptographic Encryption</h3> <p>While similar in procedure, cryptographic encryption serves a separate purpose. It works like so:</p> <ul> <li>Sender: constructs a key</li> <li>Sender: construct a message to send</li> <li>Sender: run the message bits and the key through an encryption algorithm, producing unintelligible ciphertext.</li> <li>Sender: sends cipher text message to receiver.</li> <li>Receiver: receives the message</li> <li>Receiver: retrieves the sender's key</li> <li>Receiver: runs the message's ciphertext and the retrieved key through the same encryption algorithm, producing an intelligible message.</li> </ul> <p>In the above scenario the key is being utilized on every message to encrypt the message and conceal its contents.</p> <p>Typical encryption algorithms are:</p> <ul> <li>DES &amp; 3DES</li> <li>RSA</li> <li>Blowfish</li> <li>AES</li> </ul> <h2>Part 2: Applied Technologies</h2> <p>Several widely used technologies apply signing and encryption in practice. We will cover the following:</p> <ul> <li>Private Key Infrastructure And x509 Certificates</li> <li>TLS (SSL)</li> <li>JSON Web Tokens / JSON Web Signatures</li> </ul> <h3>Private Key Infrastructure And x509 Certificates</h3> <p>Private key infrastructure, or PKI for short, is a grouping of technologies, protocols, and policies. This grouping can be used in tandem to ensure both authentication and encryption and securely transfer keys between parties.</p> <p>PKI is based on a private/public key model. In this model a private key is used for signing or encryption while the public key is used for verification or decryption.</p> <p>*aside: Often the terms "asymmetric" and "symmetric" encryption come up. When the same key is used to encrypt and decrypt a message, this is known as "symmetric" encryption. When a key is used to encrypt a message as a different key is used to decrypt the message, this is known as "asymmetric" encryption. Public/Private key encryption is considered "asymmetric".</p> <p>The private key is kept secret and used to sign data while the public key can verify what the private key signs. The public key can never be used to derive the private key and this is mathematically proven.</p> <p><em>aside: PKI infrastructure will typically use RSA public and private keys. We dig into this more later in the post.</em></p> <p>In our examples above the sender would sign a message with its private key, make its public key available to the receiver, and the receiver would verify the message utilizing the sender's public key.</p> <p>PKI is called an 'infrastructure' because it provides a trust policy in addition to authentication and encryption.</p> <p>In PKI the trust policy takes the form of a tree. At the root of the tree is the "root CA", where CA is short for certificate authority. The root can create one or more "intermediate CA(s)" by creating and signing their certificate with its own private key, providing authenticity that the intermediate CA was created by the root. This creates a chain of trust as I can confirm an intermediate is signed by its root by obtaining the root's public key and verifying the certificate's signature.</p> <p>The intermediate CA is then kept online while the root CA is kept offline. This is for security purposes, if the intermediate CA private keys are compromised they can be revoked and the collateral damage can be managed. If the root CA's key is compromised all certificates created by any CA in the tree must be revoked.</p> <p>A diagram can help provide a visual aide.</p> <p><img alt="pki hierarchy diagram" src="https://www.ldelossa.is/pki_hierarchy_diagram.png"></p> <p>Each node in the chain has both a private key and a certificate.</p> <p>PKI utilizes a standardized certificate model specified in <a href="https://tools.ietf.org/html/rfc2459">rfc-2459</a>.</p> <p>A certificate is an envelope containing metadata and the public key of the owner. It may be used as follows:</p> <ul> <li>Sender: Signs a message with it's private key.</li> <li>Sender: Sends message to receiver.</li> <li>Receiver: Receives message.</li> <li>Receiver: Obtains the sender's certificate.</li> <li>Receiver: Verifies the certificate's authenticity by following the certificate trust chain.</li> <li>Receiver: Extracts public key from certificate and verifies message.</li> </ul> <p>Note that it is not enough to simply extract the public key and verify the message. The receiver must verify the encountered certificate was indeed signed by the issuer's private key. This is typically performed by the receiver having a local copy of popular root and intermediate certificates, extracting the public key from the one matching the issuer of the encountered certificate, and verifying the signature.</p> <p>It is worthwhile to take a pragmatic look at setting up a root CA, intermediate, and signing client certificates. A wonderful tutorial can be found <a href="https://jamielinux.com/docs/openssl-certificate-authority/">here</a></p> <h3>TLS</h3> <p>TLS utilizes PKI to implement encryption over HTTP also known as "HTTPS". TLS guarantees that every bit of data between two HTTP clients is encrypted and unintelligible to any other parties which may route the traffic.</p> <p>TLS is a protocol which exchanges asymmetric keys, generates symmetric keys, and uses the symmetric keys to encrypt data between parties.</p> <p>When a browser connects to an HTTPS website a handshake occurs. Within this handshake the server's certificate is verified and a set of symmetric keys are crafted. All communication on this secure channel is now encrypted and decrypted with the symmetric keys.</p> <p>The reason symmetric keys are used is for performance. Encrypting and decrypting with a private/public key can be expensive due to key size. Encryption and decryption can occur quicker with smaller symmetric keys.</p> <p>TLS also provides authentication.</p> <p>Each https server is assigned a client certificate. From our PKI diagram, client certificates are the leafs. When a user requests information from a server, the user's browser will check the server's certificate. If the browser cannot prove the certificate was created by a trusted root or intermediate CA the connection will fail.</p> <p><em>aside: if you ever had to install a certificate bundle to a server because ssl was failing you are installing a well known set of trusted root and intermediate certificates. This is used in the above verification process.</em></p> <p>With TLS comes maintenance. TLS certificates expire over time and must be kept up to date. Traditionally a server TLS certificate would be purchased from a well known root CA such as DigiTrust. Today, "let's encrypt" has paved the way for free certificates, albeit these certs expire much sooner then ones you can purchase from a trusted root ca.</p> <h3>JSON Web Tokens and JSON Web Encryption</h3> <p>JSON Web Tokens or JWT for short has become a popular form of authentication in modern web applications. When coupled with JSON Web Encryption both authentication and encryption can be utilized.</p> <p>The ubiquity of JWT and JSE is due to it's simplicity and ease of use. Both specifications use JSON to transfer a signed and optionally encrypted token between parties.</p> <p>This token can optionally contain claims, key/value information potentially useful for the receiving party along with several other "sections" which are base64 encoded and signed. The full details of generating a token can be viewed <a href="https://jwt.io/introduction/">here</a>.</p> <p>The flow of jwt interaction follows:</p> <ul> <li>Sender: generates the header and the payload for the JWT.</li> <li>Sender: generate the signature for the JWT utilizing a key.</li> <li>Sender: places the token in an "authorization" http header.</li> <li>Receiver: parses the "authorization" header and retrieves the token.</li> <li>Receiver: retrieves the sender's key.</li> <li>Receiver: verifies the signature portion with the sender's key.</li> </ul> <p>JWT alone provides no key transfer facilities and the token's data is in plain text. However, with JSON Web Encryption (JWE) it becomes possible to piggyback off PKI and retrieve public keys via the public key infrastructure.</p> <p>More than a high level overview is further then this post would like to go. If you are interested in …</p></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.ldelossa.is/blog/absolute-minimum-cryptography">https://www.ldelossa.is/blog/absolute-minimum-cryptography</a></em></p>]]>
            </description>
            <link>https://www.ldelossa.is/blog/absolute-minimum-cryptography</link>
            <guid isPermaLink="false">hacker-news-small-sites-24611972</guid>
            <pubDate>Mon, 28 Sep 2020 01:28:29 GMT</pubDate>
        </item>
    </channel>
</rss>
