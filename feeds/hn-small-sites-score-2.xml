<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 2]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 2. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Tue, 01 Dec 2020 08:32:01 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-2.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Tue, 01 Dec 2020 08:32:01 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Testing webhooks using netcat and ngrok]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25243963">thread link</a>) | @root993
<br/>
November 29, 2020 | https://www.sankalpjonna.com/posts/testing-webhooks-using-netcat-and-ngrok | <a href="https://web.archive.org/web/*/https://www.sankalpjonna.com/posts/testing-webhooks-using-netcat-and-ngrok">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><figure id="w-node-184f6be698e4-87eecd11"><p><img src="https://uploads-ssl.webflow.com/5e0b01877436086c8beecd1a/5fc36150f7e15501838a699f_3C877AnDdlSJbqfxjZakSrkuQ6NuM0CNdqLoQvUNkeH3u3ScA-BmwT0Qbkb0NsFSDoElP6iR-iS4zhQ-QNa_cEMzBQ0XbHk6xsMTaU1EypUH-XFO7m_sywyykBR2iA_6wc4QMQHL.png" alt=""></p></figure><p>A very common challenge faced by many software products is the need for the application server to communicate with other third party application servers. To be more specific, my application server needs to be notified when a particular event takes place in a 3rd party application server.<br></p><p>For instance, I am currently building a <a href="https://www.delightchat.io/" target="_blank">customer support software</a> in which customer queries can come from various channels of communication including Facebook, Instagram and WhatsApp. This means that if someone sends a message on Facebook messenger, that message has to ultimately reach my application server in some way so I can perform the necessary operations.<br></p><p>The most common way to solve this problem is with the use of webhooks.<br></p><h3><strong>What is a webhook?</strong><br></h3><p>A webhook is nothing but an API end point that is created in your own application server which is made public and it is called by other applications to provide your application with real time updates for various events.<br></p><p>The product that I am currently building integrates with Shopify and every time somebody places an order on a merchants Shopify store, my application needs to be notified in real time so that the order can be indexed in my database and my application then has the capability to query all orders placed by a particular user.<br></p><p>To solve this, I would have to create an API end point on my server in the form of <em>/webhooks/shopify/orders/create</em> and register this URL with Shopify. Whenever Shopify receives a new order in their system, their server will fire this URL along with the order data.<br></p><h3><strong>How to inspect a webhook?</strong><br></h3><p>Since a webhook is called from a 3rd party server that is out of your control, you have no way of knowing what kind of data will be sent in the webhook unless you actually create an API, host it somewhere that is public and register the endpoint with the 3rd party software. <br></p><p>If this seems like a lot of work just to view the contents of a webhook, products like <a href="https://requestbin.com/" target="_blank">requestbin</a> and <a href="https://beeceptor.com/" target="_blank">beecepter</a> were built exactly for this purpose. You can get a free endpoint from these products and register it in the 3rd party software after which you can trigger the event you are looking for and view the contents of the request on their dashboard.<br></p><p>However I believe in simplicity and these services come with their own set of constraints. So after a little bit of research I found that the netcat utility that is installed by default In most UNIX based machines solves this problem quite elegantly.<br></p><h5>Using netcat to inspect a webhook<br></h5><p>In the most simplest form, you can set up a webhook listener with a simple terminal command<br></p><p>&lt;p&gt;CODE:&nbsp;https://gist.github.com/sankalpjonn/b0a8efaad418feeae02e4285582e2ce5.js&lt;/p&gt;<br></p><p>This command will listen for any requests coming on the port 3000 and display it on the terminal itself. Let us try it out by entering <a href="http://localhost:8000/my/webhook/path" target="_blank">http://localhost:3000/my/webhook/path</a> on the browser and see what happens.<br></p><figure id="w-node-060a8fc15179-87eecd11"><p><img src="https://uploads-ssl.webflow.com/5e0b01877436086c8beecd1a/5fc34225f58539412e8fad89_gN3PHr5rIsoV-eU02OEnVfFRqw10WcvJV73B2eTxDnL1IanCGiOyMGuomBwxTo7LJV2QmqfyFgSYRT5cNlIIK-YsdFC3Pt8k4U_vmrVt65rlvqVDok1o44yR-_rKqVr0YGLXKV8U.png" alt=""></p><figcaption>Inspecting the contents of a http request<br></figcaption></figure><p>The output on the terminal contains the entire http request that was received by localhost:3000 including the http method, headers and content. But there are two clear problems here:<br></p><ul role="list"><li>The browser keeps waiting for a response from the netcat server which is never received, so it eventually times out.</li><li>The netcat server displays the webhook request and then shuts down after which it will no longer accept any more requests unless you re-run the command.<br></li></ul><p>Both of these problems have simple solutions. Let’s go through them in order.<br></p><h5><strong>Returning a response</strong> <br></h5><p>One can easily create a simple HTTP message and return that as the response to the requests made to this netcat server. Here is how you do it:<br></p><p>Create a file called response.txt with these contents.<br></p><p>&lt;p&gt; CODE:&nbsp;https://gist.github.com/sankalpjonn/0753a1ac3a4442eedb3c0f811740e4ec.js &lt;/p&gt;<br></p><p>Echo the contents of this file and pipe the netcat command to it.<br></p><p>&lt;p&gt; CODE:&nbsp;https://gist.github.com/sankalpjonn/5fffa3d857bab441dd85fad1061da384.js &lt;/p&gt;<br></p><p>Now upon hitting localhost:3000 on your browser, you will receive a response in the browser.</p><figure id="w-node-7c8443ed0138-87eecd11"><p><img src="https://uploads-ssl.webflow.com/5e0b01877436086c8beecd1a/5fc342e7f19477bb7649ae48_Wxi5VFUp0fleE1u1NrVneah1s_TxVNEXXKaXw092dB-lfc3M7ljNOgoTIp0uvn8WiEZT071uf0P4vNbTQcrknLXckCJGsShTHzmMEoxUJ39O9rvCIfqbbUDtXnBSH67vAy2-X6wv.png" alt=""></p><figcaption>netcat server with response</figcaption></figure><p>‍</p><h5>Running the netcat server in a loop<br></h5><p>To prevent the netcat server from shutting down after a request is received, all we have to do is run the commands in a loop. This can be done with a small modification to the above command.</p><p>‍<br>&lt;p&gt; CODE:&nbsp;https://gist.github.com/sankalpjonn/eb8413365d91e566c66e456adffbf6c3.js &lt;/p&gt;</p><p>Now when you make a request to localhost:3000, the request content will be displayed on the terminal and a response will be sent to the client making the request but the netcat server will continue to run and accept new requests.<br></p><p>To demonstrate the fact that you can inspect any http request using this setup, let us make another request to localhost:3000 but this time use a PUT method with some query parameters, some headers and some data using cURL.</p><p>&lt;p&gt;&nbsp;CODE:&nbsp;https://gist.github.com/sankalpjonn/1468715b5fbe8c97bae095e874200cef.js &lt;/p&gt;<br></p><figure id="w-node-e056beb644f0-87eecd11"><p><img src="https://uploads-ssl.webflow.com/5e0b01877436086c8beecd1a/5fc343941c29737cf9f65b15_hq7ITM8kEJRvb5wm5yDYvA-sno9OUgLRFw_tb3r3OKuL1PpeAfl68wPSHrhu4YTTmI79qNO_viY6rGyqoLk5c15mO5J6syZOBRCG1WUCyHs2-QHyP-a0e4KIxEpaiNsa80HJ9_TX.png" alt=""></p><figcaption>Viewing the contents of a PUT request with headers, query params and body</figcaption></figure><p>‍</p><p>Great! we can now inspect any type of webhook that is made to localhost:3000.<br></p><h3>Making your webhook public using ngrok<br></h3><p>Now that we have a fake server setup using netcat that is capable of accepting requests, displaying their content and sending back a valid response, all we need is a method to expose this server to the public internet. <br></p><p>This is because, we need to register this webhook on some third party software like Shopify and that cannot be done unless the URL is publicly accessible. <br></p><p>Fortunately, this can be done with a single command using ngrok. Ngrok is a very nifty tool that lets you expose a service running on localhost to the public internet by generating a temporary URL that is made public. You can also get a permanent URL by signing up for their paid plan.<br></p><p>After installing ngrok from <a href="https://ngrok.com/download" target="_blank">here</a> and unzipping it as shown in the description, all you have to do is run this command on a new terminal window while your netcat server is still running.</p><p>&lt;p&gt; CODE:&nbsp;https://gist.github.com/sankalpjonn/4c4018e45ed36130521bcfd63a89b0c3.js &lt;p&gt;<br></p><p>You instantly get a http and https endpoint that routes traffic from the public internet to localhost:3000. You can try it out by coping the generated URL and pasting it in your browser.<br></p><figure id="w-node-c3deabacf619-87eecd11"><p><img src="https://uploads-ssl.webflow.com/5e0b01877436086c8beecd1a/5fc3644d77f60530b57eb061_dsKbmpkfNStyhz9XbENYJpmZrvkI76nl6XvFPiSQ_3fxKmgjrKEwbuf3rIpBySPTELVe6G8DD1Jfoe55tz9WyrIp0tXvT-z1xF1S1IZY24UbahJfeQ7W6cQMlQzkcYSUTqJv0XcT.png" alt=""></p><figcaption>get the public URL&nbsp;from here</figcaption></figure><figure id="w-node-e545581c77d3-87eecd11"><p><img src="https://uploads-ssl.webflow.com/5e0b01877436086c8beecd1a/5fc344670af890c127167066_2yKt6iyvATWXH1MMlAHioMB_5aCTs6yjWKvas0cBdjEr01JKWQUSKVW37o9pUZY2Fw1oCYWbFm-2qhJaoU1wtefBvQ84u9cJBqNzyZTH8onY3_Lnn0h3kPKDvMP4UOS_UtmJmOoD.png" alt=""></p><figcaption>Making a request using the public URL</figcaption></figure><p>‍</p><p>And it works!<br></p><h3>Summary</h3><p>A webhook can be tested by running just two simple terminal commands on separate terminal windows. </p><p>&lt;p&gt; CODE:&nbsp;https://gist.github.com/sankalpjonn/a3f41a14f7ea1d386423fdb73193f65f.js&lt;/p&gt;</p><p>You can now test your webhooks with the public URL&nbsp;derived from ngrok and registering this URL&nbsp;in the 3rd party software that will be calling the webhook.</p><h3>Closing notes<br></h3><p>As I mentioned before, for those of you who believe that using an existing service that was built for this very purpose is better, I would highly recommend <a href="https://beeceptor.com/" target="_blank">beeceptor</a>. <br></p><p>You can view all your requests on a dashboard like this.</p><figure id="w-node-4211e4369e4b-87eecd11"><p><img src="https://uploads-ssl.webflow.com/5e0b01877436086c8beecd1a/5fc344ae3677d6cf3746c74d_Vj0hedFYlVuj06uI7RCNsm38rnBwc4TGHw-A8ovEoPNGrysxauEJhIJq98e7Nn0oh8Wa_1WQmKRDGK7c8ib226ZWSwZYdzLrGRnfWZr_P6QWkfsaS9LI7FMEjhdOKA81aJIrkeBm.png" alt=""></p><figcaption>Beecepter dashboard</figcaption></figure><p>‍</p><p>The downside is that if you don’t want to pay for this service, the free plan has many constraints, once of which is that if you refresh this page, your previous request data is lost, so you can use this only as a one time thing.<br></p><p>The reason I like to go with the netcat + ngrok method is because the whole thing is completely in my control and it always feels good to hack something together of your own rather than use an existing service.</p></div></div>]]>
            </description>
            <link>https://www.sankalpjonna.com/posts/testing-webhooks-using-netcat-and-ngrok</link>
            <guid isPermaLink="false">hacker-news-small-sites-25243963</guid>
            <pubDate>Sun, 29 Nov 2020 09:13:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Convert Markdown to HTML with Pandoc]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25243905">thread link</a>) | @arthurk
<br/>
November 29, 2020 | https://www.arthurkoziel.com/convert-md-to-html-pandoc/ | <a href="https://web.archive.org/web/*/https://www.arthurkoziel.com/convert-md-to-html-pandoc/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
        
        <time datetime="2020-11-29">November 29, 2020</time>
<p>In this post I’ll describe how to use <a href="">Pandoc</a> to convert Markdown to a full HTML page (including header/footer).</p>
<p>The Pandoc version used for the examples below is 2.11.2.</p>
<h2 id="what-is-pandoc">What is Pandoc?</h2>
<p>Pandoc is an open-source document converter that is written in Haskell. It was initially released in 2006 and has been under active development since then.</p>
<p>The goal of Pandoc is to convert a document from one markup format to another. It distinguishes between input formats and output formats. As of writing this it supports <a href="https://pandoc.org/MANUAL.html#input-formats">38 input formats</a> and <a href="https://pandoc.org/MANUAL.html#output-formats">59 output formats</a>.</p>
<p>In this post we’ll use Markdown as an input format and HTML as an output format.</p>
<h2 id="preparing-the-html-template">Preparing the HTML template</h2>
<p>To generate a full HTML page we have to use Pandoc’s standalone mode which will use a <a href="https://pandoc.org/MANUAL.html#templates">template</a> to add header and footer.</p>
<p>Pandoc ships with a default template, if you wish to use that skip this section and omit the <code>--template</code> argument.</p>
<p>The template we’ll use is this (save it to <code>template.html</code>):</p>
<pre><code>&lt;!doctype html&gt;
&lt;html lang="en"&gt;
  &lt;head&gt;
    &lt;meta charset="utf-8"&gt;
    &lt;meta name="date" content='$date-meta$'&gt;
    &lt;title&gt;$title$&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;p&gt;Date: $date$&lt;/p&gt;
$body$
  &lt;/body&gt;
&lt;/html&gt;</code></pre>
<p>Pandoc’s template variables can have <a href="https://pandoc.org/MANUAL.html#interpolated-variables">different formats</a>, the one we’re using here start and end with a dollar sign:</p>
<ul>
<li><code>$date$</code>: A date in a parsable format. See the <a href="https://pandoc.org/MANUAL.html#variables-set-automatically">date-meta docs</a> for a list of recognized formats for the <code>date</code> variable. We use this to show when the document was created</li>
<li><code>$date-meta$</code>: The <code>date</code> parsed to ISO 8601 format. This is automatically done</li>
<li><code>$title$</code>: The document title</li>
<li><code>$body$</code>: The document body in HTML (the converted Markdown)</li>
</ul>
<p>We only need to set the <code>date</code> and <code>title</code> in the Markdown document via a metadata block.</p>
<h2 id="writing-the-markdown-file">Writing the Markdown file</h2>
<p>Create a Markdown file <code>doc.md</code> with the following content:</p>
<pre><code>---
title: My Document
date: September 22, 2020
---

## Test
some text</code></pre>
<p>The beginning of the document is the metadata block with required <code>date</code> and <code>title</code> variables mentioned above.</p>
<p>Several Markdown variants are <a href="https://pandoc.org/MANUAL.html#Markdown-variants">supported</a> such as GitHub-Flavored markdown. This example uses <a href="https://pandoc.org/MANUAL.html#pandocs-markdown">Pandoc’s extended markdown</a> which is the default input for files with the <code>md</code> extension.</p>
<h2 id="converting-the-document">Converting the document</h2>
<p>Run the following command to generate the HTML page:</p>
<pre><code>pandoc --standalone --template template.html doc.md</code></pre>
<p>Pandoc will try to guess the input format from the file extension (<code>.md</code> will use the Markdown input format) and output it to HTML (the default output format).</p>
<p>The output will be printed to the terminal:</p>
<pre><code>&lt;!doctype html&gt;
&lt;html lang="en"&gt;
  &lt;head&gt;
    &lt;meta charset="utf-8"&gt;
    &lt;meta name="date" content='2020-09-22'&gt;
    &lt;title&gt;My Document&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;p&gt;Date: September 22, 2020&lt;/p&gt;
&lt;h2 id="test"&gt;Test&lt;/h2&gt;
&lt;p&gt;some text&lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</code></pre>
<p>To save the document to a file we can either redirect stdout or use the <code>-o</code> argument:</p>
<pre><code>pandoc --standalone --template template.html doc.md -o doc.html</code></pre>
<h2 id="conclusion">Conclusion</h2>
<p>In this example we’ve converted Markdown to a standalone HTML page that is using a custom template.</p>
<p>This was just a simple example of what Pandoc is capable to do. The standalone mode coupled with the bundled default templates makes it easy to generate a wide variety of outputs such as HTML presentations, Jupyter notebooks or PDF documents.</p>
    </article></div>]]>
            </description>
            <link>https://www.arthurkoziel.com/convert-md-to-html-pandoc/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25243905</guid>
            <pubDate>Sun, 29 Nov 2020 08:55:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Citibank Executive Says Bitcoin Will Trade at $318,000 by End of 2021]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25243828">thread link</a>) | @rajeshbotsfolio
<br/>
November 29, 2020 | https://botsfolio.com/crypto-bot/bitcoin-citibank/ | <a href="https://web.archive.org/web/*/https://botsfolio.com/crypto-bot/bitcoin-citibank/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                     <p>
                        “Bitcoin will see huge price swings before finally settling at the predicted price” Fitzpatrick, MDCitibank.
                     </p>

                     

               
                        <h3>Comparative Trends</h3>
                        <p>
                            Writing in a report named, Bitcoin: 21st Century Gold, Fitzpatrick makes the flood contention 
                            for bitcoin. He says the computerized gold’s present direction gives off an impression of being like that of gold 
                            during the 1970s. 
                        </p>

                        <p>
                            Before basic changes were executed in the mid-1970s, gold had gone through 50 years of exchanging 
                            the $20-$35 territory. Notwithstanding, after changes were initiated gold flooded. It as of late contacted another 
                            record-breaking high in Augustprior to settling at just shy of $1,900 per ounce. 
                        </p>
                             
                        <p>
                            As per one report that examined 
                            Fitzpatrick’s paper, it is this “auxiliary change in the advanced money related system that introduced a universe of 
                            monetary indiscipline, deficiencies, and expansion.” Therefore, the Citibank manager contends that bitcoin, which 
                            went to the front in the fallout of the “Incomparable Financial emergency” of 2008-2009, will undoubtedly have a 
                            comparative run. 
                        </p>
               
                        <p>
                            With the Covid-19 pandemic actually draining economies around the globe, governments will keep reacting to the emergency by printing more cash. 
                            This thuslywill profit place of refuge resources that perform well in inflationary periods. 
                        </p>
               
                        <h3>Bitcoin Better Than Gold</h3>
               
                        <p>
                            In any case, Fitzpatrick clarifies that albeit gold is required to profit by the storm of new cash entering dissemination, the 
                            valuable metal has novel constraints that don’t appear to distress bitcoin. In his review, Fitzpatrick notes: 
                        </p><p>
                            Gold has limitations, for example, stockpiling, non-compact, and might be even called ‘the previous news’ regarding a 
                            monetary fence. Bitcoin is the new gold. To help this view, the Citibank supervisor refers to a portion of bitcoin’s 
                            key credits which incorporate the advanced cash’s “restricted gracefully, 
                            simplicity of development across fringes, and hazy proprietorship.” Consequently, Fitzpatrick accepts more 
                            speculators will pick bitcoin over gold thus. 
                        </p>

                        <p>
                            In the interim, Fitzpatrick predicts that bitcoin will be exposed to 
                            more administrative limitations going ahead. Be that as it may, dissimilar to other computerized monetary standards, 
                            for example, national bank advanced monetary standards (CBDCs), bitcoin can’t be seized, thus making it a safer 
                            resource.
                        </p>
                    </div></div>]]>
            </description>
            <link>https://botsfolio.com/crypto-bot/bitcoin-citibank/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25243828</guid>
            <pubDate>Sun, 29 Nov 2020 08:37:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A better Kubernetes from the ground up]]>
            </title>
            <description>
<![CDATA[
Score 22 | Comments 3 (<a href="https://news.ycombinator.com/item?id=25243159">thread link</a>) | @mr-karan
<br/>
November 28, 2020 | https://blog.dave.tf/post/new-kubernetes/ | <a href="https://web.archive.org/web/*/https://blog.dave.tf/post/new-kubernetes/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
	<div>
		<p><span>Written by</span>
        David Anderson
        <br>
        <span>on&nbsp;</span><time datetime="2020-11-28 00:00:00 +0000 UTC">November 28, 2020</time>
</p>
		


		

		<p>Recently I had a chat with the excellent <a href="https://timewitch.net/">Vallery Lancey</a>, about Kubernetes. Specifically, what we would do differently if we built something new, from the ground up, with no regard for compatibility with Kubernetes. I found that conversation so stimulating that I feel the need to write things down, so here we are.</p>

<p>Before we get started, I want to stress a few things.</p>

<ul>
<li>This is not a fully formed design. Some of these things may not work at all, or require significant redesign. Each section is one random piece of the entire puzzle.</li>
<li>These are not solely my ideas. Some I <em>think</em> are original, but like many things in the Kubernetes community it’s the product of collective thinking. I know at least <a href="https://timewitch.net/">Vallery</a> and <a href="https://twitter.com/maisem_ali">Maisem Ali</a> have influenced my thinking at one time or another, and I’m forgetting many more. If you like an idea, it was a group effort. If you hate it, it’s entirely mine.</li>
<li>Some of these things are polarizing. I’m designing something that makes <em>me</em> happy.</li>
</ul>

<h2 id="guiding-principles">Guiding principles</h2>

<p>My experience of Kubernetes comes from two very different places: authoring <a href="https://www.metallb.org/">MetalLB</a> for bare metal clusters, and operating a large fleet of clusters-as-a-service in <a href="https://cloud.google.com/kubernetes-engine">GKE SRE</a>. Both of these taught me that Kubernetes is extremely complex, and that most people who are trying to use it are not prepared for the sheer amount of work that lies between the marketing brochure and the system those brochures promise.</p>

<p>MetalLB taught me that it’s not possible to build robust software that integrates with Kubernetes. I think MetalLB makes a damn good go of it, but Kubernetes still makes it far too easy to construct broken configurations, and far too hard to debug them. GKE SRE taught me that even the foremost Kubernetes experts cannot safely operate Kubernetes at scale. (Although GKE SRE does a spectacular job with the tools they’re given.)</p>

<p>Kubernetes is the C++ of orchestration software. Immensely powerful, includes all the features, looks deceptively simple, and <em>will</em> hurt you repeatedly until you join its priesthood and devote your life to its mysteries. And even then, the matrix of possible ways to configure and deploy it is so large that you’re never on firm footing.</p>

<p>Continuing that analogy, my guide star is Go. If Kubernetes is C++, what would the Go of orchestration systems look like? Aggressively simple, opinionated, grown slowly and warily, and you can learn it in under a week and get on with what you were actually trying to accomplish.</p>

<p>With that, let’s get going. Starting with Kubernetes, and with a license to completely and utterly break compatibility, what would I do?</p>

<h2 id="mutable-pods">Mutable pods</h2>

<p>In Kubernetes, pods are mostly (but not entirely) immutable after creation. If you want to change a pod, you don’t. Make a new one and delete the old one. This is unlike most other things in Kubernetes, which are mostly mutable and gracefully reconcile towards the new spec.</p>

<p>So, I’m going to make pods be not special. Make them entirely read-write, and reconcile them like you would any other object.</p>

<p>The immediately useful thing I get from that is in-place restarts. If scheduling constraints and resource allocations haven’t changed, guess what? SIGTERM runc, restart runc with different parameters, and you’re done. Now pods look like regular old systemd services, that can move between machines <em>if necessary</em>.</p>

<p>Note that this doesn’t require doing mutability at the runtime layer. If you change a pod definition, it’s still mostly fine to terminate the container and restart it with a new configuration. The pod is still holding onto the resource reservation that got it scheduled onto this machine, so conceptually it’s equivalent to <code>systemctl restart blah.service</code>. You could try to be fancy and make some operations actually update in place at the runtime level as well, but don’t have to. The main benefit is decoupling scheduling, pod lifetime, and lifetime at the runtime layer.</p>

<h2 id="version-control-all-the-things">Version control all the things</h2>

<p>Sticking at the pod layer for a bit longer: now that they’re mutable, the next obvious thing I want is rollbacks. For that, let’s keep old versions of pod definitions around, and make it trivial to “go back to version N”.</p>

<p>Now, a pod update looks like: write an updated definition of the pod, and it updates to match. Update broken? Write back version N-1, and you’re done.</p>

<p>Bonus things you get from this: a diffable history of what happened to your cluster, without needing GitOps nonsense. By all means keep the GitOps nonsense if you want, it has benefits, but you can answer a basic “what changed?” question using only data in the cluster.</p>

<p>This needs a bit more design. In particular, I want to separate out external changes (human submits a new pod) from mechanical changes (some internals of k8s alter a pod definition). I haven’t thought through how to encode both those histories and make both accessible to operators and automation. Maybe it could also be completely generic, wherein a “changer” identifies itself when submitting a new version, and you can then query for changes by or excluding particular changers (think similar to how label queries work at the minute). Again, more design needed there, I just know that I want versioned objects with an accessible history.</p>

<p>We’ll need garbage collection eventually. That said, changes to single pods should delta-compress really well, so my default would be to just keep everything until it becomes a truly dumb amount of data, and figure something out at that point. Keeping everything also acts as a useful mild pressure to avoid “death by a thousand changes” in the rest of the system. Prefer to have fewer, more meaningful changes over a flurry of control loops each changing one field in pursuit of convergence.</p>

<p>Once we have this history, we can do some neat minor things too. For example, the node software could keep container images for the last N versions pinned to the machine, so that rollbacks are as fast as they can possibly be. With an accessible history, you can do this more precisely than “GC older than 30 days and hope”. Generalizing, all the orchestration software can use older versions as GC roots for various resources, to make rollbacks faster. Rollbacks being the primary way of ending outages, this is a very valuable thing to have.</p>

<h2 id="replace-deployment-with-pinneddeployment">Replace Deployment with PinnedDeployment</h2>

<p>This is a short section to basically say that <a href="https://timewitch.net/">Vallery</a> knocked it out of the park with her <a href="https://timewitch.net/post/2019-12-30-pinneddeployments/">PinnedDeployment</a> resource, which lets operators explicitly control a rollout by tracking 2 versions of the deployment state. It’s a deployment object designed by an SRE, with a crisp understanding of what SREs want in a deployment. I love it.</p>

<p>This combines super well with the versioned, in-place pod updates above, and I really don’t have anything to add. It’s clearly how multi-pod things should work. There’s probably some tweaking required to adapt from the Kubernetes-constrained world to this new wonderful unconstrained universe, but the general design is perfect.</p>

<h2 id="explicit-orchestration-workflows">Explicit orchestration workflows</h2>

<p>The biggest issue I have with the “API machinery” bits of Kubernetes is the idea of orchestration as a loose choreography of independent control loops. On the surface, this seems like a nice idea: you have dozens of little control loops, each focused on doing one small thing. When combined in a cluster, they indirectly cooperate with each other to push the state forward and converge on the desired end state. So, what’s the problem?</p>

<p>The problem is that it’s entirely impossible to debug when it goes wrong. A typical failure mode in Kubernetes is that you submit a change to the cluster, then repeatedly refresh waiting for stuff to converge. When it doesn’t… Well, you’re screwed. Kubernetes doesn’t know the difference between “the system has converged successfully” and “a control loop is wedged and is blocking everything else.” You can hope that the offending control loop posted some events to the object to help you, but by and large they don’t.</p>

<p>At which point your only option is to cat the logs of every control loop that might be involved, looking for the one that was wedged. You can make this a bit faster if you have intimate knowledge of all the control loops and what each one does, because that lets you infer from the object’s current state which loop might be trying to run right now.</p>

<p>The key thing to notice here is that the complexity has been shifted from the designer of the control loop to the cluster operator. It’s easy (though not trivial) to make a control loop that does a dinky little thing in isolation. But to operate a cluster with dozens of these control loops requires the operator to assimilate the behavior of all of them, their interactions with each other, and try to reason about an extremely loosely coupled system. This is a problem because you have to write and test the control loop once, but work with it and its bugs many more times. And yet, the bias is to simplify the thing you only do once.</p>

<p>To fix this, I would look to systemd. It solves for a similar lifecycle problem: given a current state and a target, how do you get from A to B? The difference is that in systemd, the steps and their dependencies are made explicit. You <em>tell</em> systemd that your unit is a required part of <code>multi-user.target</code> (aka “normally-booted happy system”), that it must run after filesystems have been mounted, but before networking it brought up, and so forth. You can also depend on other concrete parts of the system, for example to say that your thing needs to run whenever sshd is running (sounds like a sidecar, right?).</p>

<p>The net result of this is that systemd can tell you precisely what piece of the system malfunctioned, or is still working on its thing, or failed a precondition. It can also print you a graph of the system’s boot process, and analyze it for things like “what’s the long pole of bootup?”</p>

<p>I want to steal all this wholesale, and plop it into my cluster orchestration …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.dave.tf/post/new-kubernetes/">https://blog.dave.tf/post/new-kubernetes/</a></em></p>]]>
            </description>
            <link>https://blog.dave.tf/post/new-kubernetes/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25243159</guid>
            <pubDate>Sun, 29 Nov 2020 05:35:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Futures Explained in 200 Lines of Rust]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25242849">thread link</a>) | @rustshellscript
<br/>
November 28, 2020 | https://cfsamson.github.io/books-futures-explained/introduction.html | <a href="https://web.archive.org/web/*/https://cfsamson.github.io/books-futures-explained/introduction.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
        <!-- Provide site root to javascript -->
        

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        

        <!-- Set the theme before any content is loaded, prevents flash -->
        

        <!-- Hide / unhide sidebar before it is displayed -->
        

        <nav id="sidebar" aria-label="Table of contents">
            
            
        </nav>

        <div id="page-wrapper">

            <div class="page">
                
                
                

                
                
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                

                <div id="content">
                    <main>
                        
<p>This book aims to explain Futures in Rust using an example driven approach,
exploring why they're designed the way they are, and how they work. We'll also
take a look at some of the alternatives we have when dealing with concurrency
in programming.</p>
<p>Going into the level of detail I do in this book is not needed to use futures
or async/await in Rust. It's for the curious out there that want to know <em>how</em>
it all works.</p>
<h2><a href="#what-this-book-covers" id="what-this-book-covers">What this book covers</a></h2>
<p>This book will try to explain everything you might wonder about up until the
topic of different types of executors and runtimes. We'll just implement a very
simple runtime in this book introducing some concepts but it's enough to get
started.</p>
<p><a href="https://github.com/stjepang">Stjepan Glavina</a> has made an excellent series of
articles about async runtimes and executors, and if the rumors are right there
is more to come from him in the near future.</p>
<p>The way you should go about it is to read this book first, then continue
reading the <a href="https://stjepang.github.io/">articles from stejpang</a> to learn more
about runtimes and how they work, especially:</p>
<ol>
<li><a href="https://stjepang.github.io/2020/01/25/build-your-own-block-on.html">Build your own block_on()</a></li>
<li><a href="https://stjepang.github.io/2020/01/31/build-your-own-executor.html">Build your own executor</a></li>
</ol>
<p>I've limited myself to a 200 line main example (hence the title) to limit the
scope and introduce an example that can easily be explored further.</p>
<p>However, there is a lot to digest and it's not what I would call easy, but we'll
take everything step by step so get a cup of tea and relax.</p>
<p>I hope you enjoy the ride.</p>
<blockquote>
<p>This book is developed in the open, and contributions are welcome. You'll find
<a href="https://github.com/cfsamson/books-futures-explained">the repository for the book itself here</a>. The final example which
you can clone, fork or copy <a href="https://github.com/cfsamson/examples-futures">can be found here</a>. Any suggestions
or improvements can be filed as a PR or in the issue tracker for the book.</p>
<p>As always, all kinds of feedback is welcome.</p>
</blockquote>
<h2><a href="#reader-exercises-and-further-reading" id="reader-exercises-and-further-reading">Reader exercises and further reading</a></h2>
<p>In the last <a href="https://cfsamson.github.io/books-futures-explained/conclusion.html">chapter</a> I've taken the liberty to suggest some
small exercises if you want to explore a little further.</p>
<p>This book is also the fourth book I have written about concurrent programming
in Rust. If you like it, you might want to check out the others as well:</p>
<ul>
<li><a href="https://cfsamson.gitbook.io/green-threads-explained-in-200-lines-of-rust/">Green Threads Explained in 200 lines of rust</a></li>
<li><a href="https://cfsamson.github.io/book-exploring-async-basics/">The Node Experiment - Exploring Async Basics with Rust</a></li>
<li><a href="https://cfsamsonbooks.gitbook.io/epoll-kqueue-iocp-explained/">Epoll, Kqueue and IOCP Explained with Rust</a></li>
</ul>
<h2><a href="#credits-and-thanks" id="credits-and-thanks">Credits and thanks</a></h2>
<p>I'd like to take this chance to thank the people behind <code>mio</code>, <code>tokio</code>,
<code>async_std</code>, <code>futures</code>, <code>libc</code>, <code>crossbeam</code> which underpins so much of the
async ecosystem and and rarely gets enough praise in my eyes.</p>
<p>A special thanks to <a href="https://twitter.com/jonhoo">jonhoo</a> who was kind enough to
give me some valuable feedback on a very early draft of this book. He has not
read the finished product, but a big thanks is definitely due.</p>
<h2><a href="#translations" id="translations">Translations</a></h2>
<p><a href="https://stevenbai.top/rust/futures_explained_in_200_lines_of_rust/">This book has been translated to Chinese</a> by <a href="https://github.com/nkbai">nkbai</a>.</p>

                    </main>

                    <nav aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        
                            <a rel="next" href="https://cfsamson.github.io/books-futures-explained/0_background_information.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i></i>
                            </a>
                        

                        
                    </nav>
                </div>
            </div>

            <nav aria-label="Page navigation">
                

                
                    <a rel="next" href="https://cfsamson.github.io/books-futures-explained/0_background_information.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i></i>
                    </a>
                
            </nav>

        </div>

        

        
        <!-- Google Analytics Tag -->
        
        

        
        
        

        
        
        

        
        
        
        
        
        
        

        
        
        
        
        

        
        
        

        <!-- Custom JS scripts -->
        

        

    

</div>]]>
            </description>
            <link>https://cfsamson.github.io/books-futures-explained/introduction.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25242849</guid>
            <pubDate>Sun, 29 Nov 2020 04:06:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Undeleting a file overwritten with mv]]>
            </title>
            <description>
<![CDATA[
Score 90 | Comments 30 (<a href="https://news.ycombinator.com/item?id=25242444">thread link</a>) | @todsacerdoti
<br/>
November 28, 2020 | https://behind.pretix.eu/2020/11/28/undelete-flv-file/ | <a href="https://web.archive.org/web/*/https://behind.pretix.eu/2020/11/28/undelete-flv-file/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">

    <article>

        

<!--         <header class="post-header">
            <a id="blog-logo" href="https://behind.pretix.eu">
                
                    <span class="blog-title">pretix – behind the scenes</span>
                
            </a>
        </header> -->

        <!-- <span class="post-meta">
            <time datetime="2020-11-28">28 Nov 2020</time>
            
                on Technology, Forensics, and Linux
            
        </span> -->

        <!-- <h1 class="post-title">Undeleting a file overwritten with mv</h1> -->

        <section>
            <p>It’s been a while since we shared the story of an incident with you, and that’s probably a good thing –
most operational incidents we had in the past year were “boring” enough in nature to fix them easily.
This time, we’ve got a story of a data loss, caused by pure and simple human error – and the story of
how we recovered the data.</p>

<p>Even though it is quite embarrassing how the data loss happened, we think it’s worth sharing the story
of its recovery, as it might allow you to learn a few useful things in case you ever end up in a
similar situation.</p>

<p>As you might have seen, over the last 7 months we’ve extended our offerings beyond ticketing to allow
our customers to transform their events into the digital space as long as the global pandemic makes
traditional event formats impossible. The result of our effort is a joint venture called 
<a href="https://venueless.org/">Venueless</a> that you should absolutely check out if you haven’t yet.</p>

<p>One component of the virtual events we run on venueless is <strong>live video streaming</strong>. In this process,
our customers use a tool like <a href="https://obsproject.com/">OBS</a> or <a href="https://streamyard.com/">StreamYard</a>
to create a live video stream. The stream is then sent to an <strong>encoding server</strong> of ours via RTMP.
On the encoding server, we re-encode the stream into different quality levels and then distribute
it to our very own tiny streaming CDN.</p>

<p>Venueless currently does <strong>not yet</strong> include a video-on-demand component and usually, our customers record
their content at the source, e.g. with OBS or StreamYard, and process or publish them on their own.
However, just to be safe, we keep a recording of the incoming stream as well. This isn’t currently
part of our promoted service offering, we rather see it as a free backup service to our clients in case they
lose their recording. Given that we already consider it to just be a backup, we currently don’t make any
further backups of this data.</p>

<h4 id="data-loss">Data loss</h4>

<p>Usually, we delete these recordings after a while, but in some cases, our customers ask us to get them, e.g.
because their own recording failed, or because StreamYard only records the first 8 hours of every
stream. Since this doesn’t happen a lot, it’s not yet an automated process in our system. Whenever a customer
requests a recording we SSH into the respective encoding server and move the recording file to a
directory that’s accessible through HTTP, like this:</p>

<pre><code>/var/recordings $ mv recording-12345.flv public/
</code></pre>

<p>That’s it, we share the link with the customer, and the process is done. One of the simplest steps possible
in all this. Yesterday, a customer asked us for the recordings of the two last streams of their event. Just
before finishing up for the week, I wanted to supply them with the required file, SSH’d into the server,
looked for the correct files and typed…</p>

<pre><code>/var/recordings $ mv recording-16678.flv recording-16679.flv
</code></pre>

<p><strong>Oops.</strong> I hit return before typing out <code>public/</code>, and therefore replaced the last stream with the
second-last, losing one of the videos.</p>

<h4 id="damage-control">Damage control</h4>

<p>Having a very naive understanding of how file systems work, I knew that the <code>mv</code> command has only
changed the directory listing of the file system, but hasn’t actually wiped the file from the disk,
so I knew there is likely still a chance to recover the file, if it’s not overwritten by something
else in the meantime.</p>

<p>Since I didn’t manage to re-mount the root partition as read-only to avoid further damage softly,
I used the <a href="https://www.kernel.org/doc/html/latest/admin-guide/sysrq.html">big hammer</a> to remount
everything read-only immediately:</p>

<pre><code># echo u &gt; /proc/sysrq-trigger
</code></pre>

<p>Uhm, okay, this worked, but how do I install any data recovery tools now? After some experiments,
I decided it would be easiest to reboot into the recovery system provided by our server provider
<a href="https://www.hetzner.com/">Hetzner</a>. So I configured the boot loader to boot their recovery system
from the network and forcefully rebooted the server.</p>

<p>To be able to perform disk dumps and have some operational flexibility without downloading a 2 TB
disk image to my local machine (which would take rougly a week), I also quickly purchased
a <a href="https://www.hetzner.com/storage/storage-box">Hetzner Storage Box</a> with 5 TB space.</p>

<h4 id="failed-attempts">Failed attempts</h4>

<p>Just before I executed my fatal <code>mv</code> command, I executed <code>ls -lisah</code> to get a directory listing
of the files:</p>

<pre><code>3146449 1.1G -rw-r--r-- 1 www-data www-data 1.1G Nov XX XX:XX recording-16678.flv
3146113 1.6G -rw-r--r-- 1 www-data www-data 1.6G Nov XX XX:XX recording-16679.flv
</code></pre>

<p>This meant I <strong>knew</strong> the inode number of the deleted file! As I mentioned before, my understanding
of file systems was (and is) rather naive, and I was pretty optimistic to be able to recover the
file using that information. Isn’t that sort of what a journaling file system is for?</p>

<p>Recovering the file this way hover appeared to be impossible. <a href="http://ext4magic.sourceforge.net/howto_en.html">ext4magic</a>
and <a href="http://extundelete.sourceforge.net/">extundelete</a> are powerful tools that did find some 
deleted files on my disk – but not the one I was looking for, even after trying different options
for over two hours.</p>

<p>I did not spend the time to really understand how ext4 works, but from what I gathered from various
blogs, I was pretty much out of luck since the inode did no longer contain the relevant information
and ext4magic also wasn’t able to <a href="http://ext4magic.sourceforge.net/howto_en.html#Recovery_process_5">recover the neccessary information from the journal</a>
either.</p>

<pre><code>debugfs:  inode_dump &lt;3146113&gt;
0000  a081 0000 8503 0000 e83a c15f e83a c15f  .........:._.:._
0020  e83a c15f 0000 0000 7200 0100 0800 0000  .:._....r.......
0040  0000 0800 0100 0000 0af3 0100 0400 0000  ................
0060  0000 0000 0000 0000 0100 0000 e6eb c000  ................
0100  0000 0000 0000 0000 0000 0000 0000 0000  ................
*
0140  0000 0000 92d0 2cf5 0000 0000 0000 0000  ......,.........
0160  0000 0000 0000 0000 0000 0000 6fb2 0000  ............o...
0200  2000 e3fb 208a 515b 7c65 5d5a 7c65 5d5a   ... .Q[|e]Z|e]Z
0220  e83a c15f 7c65 5d5a 0000 0000 0000 0000  .:._|e]Z........
0240  0000 0000 0000 0000 0000 0000 0000 0000  ................
*
</code></pre>

<p>However, if you’re in a similar situation – the ext4magic how-tos are really helpful and worth a try.</p>

<h4 id="successful-recovery">Successful recovery</h4>

<p>There is this one other approach to file recovery that is often recommended on the internet, usually
for “small text files”: Just <code>grep</code> your whole disk for known parts of its contents! So why wouldn’t
this work on larger non-text files as well?</p>

<p>The first problem is obviously what to grep for. The only thing I know about the missing file, apart
from its rough size, is that it’s a FLV video file. Luckily, <a href="https://en.wikipedia.org/wiki/Flash_Video#Flash_Video_Structure">all FLV files</a>
that contain video start with the byte sequence <code>FLV\x01\x05</code>. So let’s search our 2 TB disk for
that byte sequence and print out the byte offset of all occurences!</p>

<pre><code>cat /dev/md2 \
	| pv -s 1888127576000 \
	| grep -P --byte-offset --text 'FLV\x01\x05' \
	| tee -a /mnt/storagebox/grep-log.txt
</code></pre>

<p>This took roughly 7 hours. The <code>pv</code> command with the (rough) total size of the disk is optional, but gives you
a nice progress bar. Overall, this took a little over 6 hours on our server.</p>

<p><code>grep</code> works line-based, which in a binary file menas “any byte sequence between two ASCII line breaks”. The
log file therefore contained lots of lines like this:</p>

<pre><code>184473878409:&lt;some binary data&gt;FLV&lt;some binary data&gt;
</code></pre>

<p>In total, the search found 126 FLV file headers on our disk. This was pretty reassuring, since we had 122 FLV files
still known to the file system – so there are at least four FLV byte sequences without a filename!</p>

<pre><code># find /mnt/disk/var/recordings/ -name '*.flv' -not -empty -ls | wc -l
122
</code></pre>

<p>Now, I needed to find out which of the 126 byte sequences did not have a filename. Since I really didn’t want
to spend all weekend with a deep-dive into the ext4 disk layout, I went for an easier solution: For every file
still known in the file system, I computed a hash of the first 500 kilobytes of the file:</p>

<figure><pre><code data-lang="python"><span>#!/usr/bin/python3
</span><span>import</span> <span>glob</span>
<span>import</span> <span>hashlib</span>
<span>import</span> <span>os</span>

<span>hashsize</span> <span>=</span> <span>500</span> <span>*</span> <span>1024</span>
<span>known_hashes</span> <span>=</span> <span>{}</span>
<span>not_deleted_files</span> <span>=</span> <span>sorted</span><span>(</span>
    <span>glob</span><span>.</span><span>glob</span><span>(</span><span>'/mnt/disk/var/recordings/*.flv'</span><span>)</span> <span>+</span> 
    <span>glob</span><span>.</span><span>glob</span><span>(</span><span>'/mnt/disk/var/recordings/public/*.flv'</span><span>)</span>
<span>)</span>
<span># Ignore files shorter than our hash size
</span><span>not_deleted_files</span> <span>=</span> <span>[</span>
    <span>f</span> <span>for</span> <span>f</span> <span>in</span> <span>not_deleted_files</span>
    <span>if</span> <span>os</span><span>.</span><span>stat</span><span>(</span><span>f</span><span>).</span><span>st_size</span> <span>&gt;</span> <span>hashsize</span>
<span>]</span>

<span>for</span> <span>fname</span> <span>in</span> <span>not_deleted_files</span><span>:</span>
    <span>with</span> <span>open</span><span>(</span><span>fname</span><span>,</span> <span>'rb'</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
        <span>h</span> <span>=</span> <span>hashlib</span><span>.</span><span>md5</span><span>(</span><span>f</span><span>.</span><span>read</span><span>(</span><span>hashsize</span><span>)).</span><span>hexdigest</span><span>()</span>
        <span>if</span> <span>h</span> <span>in</span> <span>known_hashes</span><span>:</span>
            <span>print</span><span>(</span><span>"duplicate hash found:"</span><span>)</span>
        <span>known_hashes</span><span>[</span><span>h</span><span>]</span> <span>=</span> <span>fname</span>
        <span>print</span><span>(</span><span>h</span><span>,</span> <span>fname</span><span>)</span>

<span>print</span><span>(</span>
    <span>len</span><span>(</span><span>not_deleted_files</span><span>),</span> <span>"files with"</span><span>,</span>
    <span>len</span><span>(</span><span>known_hashes</span><span>),</span> <span>"hashes"</span>
<span>)</span></code></pre></figure>

<p>Interestingly, two files from the completely different customers shared the same hash of the first 500 kilobytes.
I haven’t tested it yet, but my theory is that those were streams that just did not contain any audio or video
in their first minutes, but only empty frames. However, since I knew this isn’t the case for my missing file,
I felt confident in proceeding with this approach.</p>

<p>Next, I computed the same hash for every byte offest found by grep and compared it to the hashes found in the
previous step:</p>

<figure><pre><code data-lang="python"><span>grep_log</span> <span>=</span> <span>'/mnt/storagebox/grep-log.txt'</span>
<span>disk</span> <span>=</span> <span>'/dev/md2'</span>

<span>print</span><span>(</span><span>"Parsing grep log…"</span><span>)</span>
<span>positions</span> <span>=</span> <span>[]</span>
<span>with</span> <span>open</span><span>(</span><span>grep_log</span><span>,</span> <span>'rb'</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
    <span>for</span> <span>line</span> <span>in</span> <span>f</span><span>.</span><span>read</span><span>().</span><span>split</span><span>(</span><span>b'</span><span>\n</span><span>'</span><span>):</span>
        <span>if</span> <span>not</span> <span>line</span><span>:</span>  <span># ignore empty line e.g. at end of file
</span>            <span>continue</span>
        <span>pos</span><span>,</span> <span>data</span> <span>=</span> <span>line</span><span>.</span><span>split</span><span>(</span><span>b':'</span><span>,</span> <span>1</span><span>)</span>
        <span>pos</span> <span>=</span> <span>int</span><span>(</span><span>pos</span><span>.</span><span>decode</span><span>())</span>
        <span># add offset of FLV within line
</span>        <span>binoffset</span> <span>=</span> <span>data</span><span>.</span><span>index</span><span>(</span><span>b"FLV</span><span>\x01</span><span>"</span><span>)</span>
        <span>pos</span> <span>+=</span> <span>binoffset</span> 
        <span>positions</span><span>.</span><span>append</span><span>(</span><span>pos</span><span>)</span>

<span>print</span><span>(</span><span>"Computing hashes of files on disk…"</span><span>)</span>
<span>found_hashes</span> <span>=</span> <span>{}</span>
<span>with</span> <span>open</span><span>(</span><span>disk</span><span>,</span> <span>'rb'</span><span>)</span> <span>as</span> <span>f</span><span>:</span>
    <span>for</span> <span>p</span> <span>in</span> <span>positions</span><span>:</span>
        <span>f</span><span>.</span><span>seek</span><span>(</span><span>p</span><span>)</span>
        <span>d</span> <span>=</span> <span>f</span><span>.</span><span>read</span><span>(</span><span>hashsize</span><span>)</span>
        <span>h</span> <span>=</span> <span>hashlib</span><span>.</span><span>md5</span><span>(</span><span>d</span><span>).</span><span>hexdigest</span><span>()</span>
        <span>if</span> <span>h</span> <span>in</span> <span>known_hashes</span><span>:</span>
            <span>print</span><span>(</span><span>"At offset"</span><span>,</span> <span>p</span><span>,</span> <span>"found known hash"</span><span>,</span> <span>h</span><span>,</span>
                  <span>"corresponding to"</span><span>,</span> <span>known_hashes</span><span>[</span><span>h</span><span>])</span>
        <span>else</span><span>:</span>
            <span>print</span><span>(</span><span>"At offset"</span><span>,</span> <span>p</span><span>,</span> <span>"found unknown hash"</span><span>,</span> <span>h</span><span>)</span>
        <span>found_hashes</span><span>[</span><span>h</span><span>]</span> <span>=</span> <span>p</span>

<span>unknown_hashes</span> <span>=</span> <span>{</span>
    <span>h</span><span>:</span> <span>p</span> <span>for</span> <span>h</span><span>,</span> <span>p</span> <span>in</span> <span>found_hashes</span><span>.</span><span>items</span><span>()</span>
    <span>if</span> <span>h</span> <span>not</span> <span>in</span> <span>known_hashes</span>
<span>}</span>
<span>files_not_found</span> <span>=</span> <span>[</span>
    <span>fname</span> <span>for</span> <span>h</span><span>,</span> <span>fname</span> <span>in</span> <span>known_hashes</span><span>.</span><span>items</span><span>()</span>
   …</code></pre></figure></section></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://behind.pretix.eu/2020/11/28/undelete-flv-file/">https://behind.pretix.eu/2020/11/28/undelete-flv-file/</a></em></p>]]>
            </description>
            <link>https://behind.pretix.eu/2020/11/28/undelete-flv-file/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25242444</guid>
            <pubDate>Sun, 29 Nov 2020 02:21:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Colour Science for Python – 0.3.16]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25242149">thread link</a>) | @kelsolaar
<br/>
November 28, 2020 | https://www.colour-science.org/posts/colour-0316-is-available/ | <a href="https://web.archive.org/web/*/https://www.colour-science.org/posts/colour-0316-is-available/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody text">
    <div>
<p><a href="https://github.com/colour-science/colour/releases/tag/v0.3.16">Colour 0.3.16</a>
has finally been released!</p>
<!-- TEASER_END -->
<p>This release integrates all the <a href="https://summerofcode.withgoogle.com/">GSoC</a>
work from Pawel (<a href="https://github.com/enneract">@enneract</a>), most of the code
from Nishant (<a href="https://github.com/njwardhan">@njwardhan</a>) and, the
optimizations from Omar (<a href="https://github.com/OmarWagih1">@OmarWagih1</a>).
We would like to thank them again for their great contributions!</p>
<p><img alt="/images/Blog_Colour_Rendition_Report.png" src="https://www.colour-science.org/images/Blog_Colour_Rendition_Report.png"></p><p>With this release, we stop testing for
<a href="https://www.python.org/downloads/release/python-350/">Python 3.5</a> and,
<a href="https://docs.scipy.org/doc/scipy/reference/release.1.1.0.html">Scipy&gt;=1.1.0</a>
becomes the minimum version. This is also the <strong>last feature release to
support</strong> <a href="https://www.python.org/downloads/release/python-270/">Python 2.7</a>.
We will also trim the deprecation code in the next version thus, please make
sure to update your code accordingly.</p>
<p>Besides the various minor changes and fixes, the highlights of this release are:</p>
<ul>
<li><p>Support for <em>Jakob and Hanika (2019)</em>, <em>Mallett and Yuksel (2019)</em> and,
<em>Otsu, Yamamoto and Hachisuka (2018)</em> spectral upsampling methods thanks to
Pawel's contributions as part of GSoC 2020.</p></li>
<li><p>Support for the computation of the <em>CIE 2017 Colour Fidelity Index</em> and
<em>ANSI/IES TM-30-18 Colour Fidelity Index</em> colour quality metrics thanks to
Pawel's contributions as part of GSoC 2020.</p></li>
<li><p>Support for generation of the <em>ANSI/IES TM-30-18 Colour Rendition Report</em>
thanks to Pawel's contributions as part of GSoC 2020.</p></li>
<li><p>Improvements of the LUT IO support thanks to Nishant's contributions as
part of GSoC 2020.</p></li>
<li><p>Performance improvements thanks to Omar's contributions as part of GSoC
2020.</p></li>
<li><p>Support for <em>ACES Input Device Transform (IDT)</em> generation: The
implementation follows to some extent
<a href="https://github.com/ampas/rawtoaces">RAW to ACES v1</a> and
<a href="https://www.dropbox.com/s/ouwnid1aevqti5d/P-2013-001.pdf?dl=0">P-2013-001</a>
procedure.</p></li>
<li>
<p>New <em>ISO</em> spectral datasets:</p>
<ul>
<li><p>ISO 6728 Standard Lens</p></li>
<li><p>ISO 7589 Diffuser</p></li>
<li><p>ISO 7589 Photographic Daylight</p></li>
<li><p>ISO 7589 Sensitometric Daylight</p></li>
<li><p>ISO 7589 Studio Tungsten</p></li>
<li><p>ISO 7589 Sensitometric Studio Tungsten</p></li>
<li><p>ISO 7589 Photoflood</p></li>
<li><p>ISO 7589 Sensitometric Photoflood</p></li>
<li><p>ISO 7589 Sensitometric Printer</p></li>
</ul>
</li>
<li><p>Support for IGPGTG colourspace by <em>Hellwig and Fairchild (2020)</em>.</p></li>
<li><p>The <cite>colour.SpectralDistribution.interpolate</cite> and
<cite>colour.MultiSpectralDistributions.interpolate</cite> methods now honour class
instantiation time interpolation parameters instead of blindly applying
<em>CIE 167:2005</em> recommendation, this introduces minor numerical changes.</p></li>
<li><p>Many definitions, methods and module attributes have been renamed to
improve consistency and we are reaching a satisfactory point in that
regard, hopefully, the names will be much more stable from now on.</p></li>
</ul>
<p>Please visit the <a href="https://github.com/colour-science/colour/releases/tag/v0.3.16">releases page</a>
for complete details.</p>
<p>Our other dependent Python packages have also been updated accordingly:</p>
<ul>
<li><p><a href="https://github.com/colour-science/colour-demosaicing/releases/tag/v0.1.6">Colour - Demosaicing - 0.1.6</a></p></li>
<li><p><a href="https://github.com/colour-science/colour-hdri/releases/tag/v0.1.8">Colour - HDRI - 0.1.8</a></p></li>
<li><p><a href="https://github.com/colour-science/colour-checker-detection/releases/tag/v0.1.2">Colour - Checker Detection - 0.1.2</a></p></li>
<li>
<p><a href="https://github.com/colour-science/colour-datasets/releases/tag/v0.1.1">Colour - Datasets - 0.1.1</a></p>
<ul>
<li>
<p>The following new datasets have been added:</p>
<blockquote>
<ul>
<li><p>4050598 : Spectral Upsampling Coefficient Tables - Jakob and
Hanika (2019)</p></li>
<li><p>4051012 : Measured Commercial LED Spectra - Brendel (2020)</p></li>
</ul>
</blockquote>
</li>
</ul>
</li>
</ul>
</div>
    </div></div>]]>
            </description>
            <link>https://www.colour-science.org/posts/colour-0316-is-available/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25242149</guid>
            <pubDate>Sun, 29 Nov 2020 01:24:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How deadly is Covid-19?]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25241663">thread link</a>) | @Malbolge
<br/>
November 28, 2020 | https://sebastianrushworth.com/2020/10/24/how-deadly-is-covid-19/ | <a href="https://web.archive.org/web/*/https://sebastianrushworth.com/2020/10/24/how-deadly-is-covid-19/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div> <amp-auto-ads type="adsense" data-ad-client="ca-pub-1298191191396779" i-amphtml-layout="container"></amp-auto-ads><p><a href="https://cornucopia.cornubot.se/2020/10/september-2020-den-minst-dodliga.html" target="_blank" rel="noreferrer noopener">September 2020 was the least deadly month in Swedish history</a>, in terms of number of deaths per 100,000 population. Ever. And I don’t mean the least deadly September, I mean the least deadly month. Ever. To me, this is pretty clear evidence of two things. First, that covid is not a very deadly disease. And second, that Sweden has herd immunity.</p><p><a href="https://twitter.com/sebrushworth/status/1319222055343185921" target="_blank" rel="noreferrer noopener">When I posted this information on my twitter feed</a>, the response from proponents of further lockdown was that the reason September was such an un-deadly month, was because everyone has already died earlier in the pandemic. To me, that seems like a pretty self-defeating argument. Why?</p><p>Because 6,000 people have died of covid in Sweden, a country with a population of 10,000,000 people. 6,000 people is 0,06% of the population. If it is enough for that tiny a fraction of a population to die of a pandemic for the pandemic to peter out so completely that a country can have its least deadly month ever, then the pandemic was never that deadly to begin with.</p><p>In August, <a href="https://sebastianrushworth.com/2020/08/04/how-bad-is-covid-really-a-swedish-doctors-perspective/" target="_blank" rel="noreferrer noopener">I wrote an article where I proposed that the mortality for covid is only 0,12%</a>, roughly the same as influenza. That number was based on a back-of-the-envelope calculation. I figured that, since the death rate had dropped continuously for months and was at very low levels, Sweden must have reached a point where it had herd immunity. And I figured that at least 50% of the population must have been infected for herd immunity to have been reached. 50% of Sweden’s population is five million people. 6,000 / 5,000,000 = 0,12%</p><p>At the beginning of October, one of the World Health Organisation’s executive directors, Mike Ryan, <a href="https://www.irishtimes.com/news/ireland/irish-news/covid-19-world-in-for-a-hell-of-a-ride-in-coming-months-dr-mike-ryan-says-1.4370626" target="_blank" rel="noreferrer noopener">said that the WHO estimated that 750 million people had so far been infected with covid</a>. At that point, one million people had died of the disease. That gives a death rate for covid of 0,13% . So the WHO said that the death rate is 0,13% . Not too far off my earlier back-of-envelope estimation. This of course begs the question why there are continued lockdowns for a disease that is no worse than the flu.</p><p>A short while later, the <a href="https://www.who.int/bulletin/online_first/BLT.20.265892.pdf" target="_blank" rel="noreferrer noopener">WHO released an analysis by professor John Ioannidis</a>, with his estimate of the covid death rate. This analysis was based on seroprevalance data, i.e. data on how many people were shown to have antibodies to covid in their bloodstream at different times in different countries, which was correlated with the number of deaths in those countries. Through this analysis, professor Ioannidis reached the conclusion that covid has an overall mortality rate of around 0,23% (in other words, one in 434 infected people die of the disease). For people under the age of seventy, the mortality rate was estimated at 0,05% (in other words, one in 2,000 infected people under the age of 70 die of the disease).</p><p><a href="https://sebastianrushworth.com/2020/09/28/herd-immunity-without-antibodies/" target="_blank" rel="noreferrer noopener">As I’ve discussed before, I don’t think antibody data gives a very complete picture</a>, since there are studies showing that a lot of people don’t produce measurable antibodies in their bloodstreams, but still have immunity, either thanks to a T-cell response, or thanks to local antibody production in the respiratory tract. So I think that the fatality rate is significantly lower than what the analysis by professory Ioannidis found, and more in line with what the WHO stated earlier in October.</p><p>But even if the antibody based number is the correct number, then covid still is not a very deadly disease. For comparison, the 1918 flu pandemic is thought to have had an infection fatality rate of 2,5%, i.e. one in forty infected people died. So the 1918 flu was 11 times more deadly than covid if you go by professor Ioannidis antibody based numbers, and 19 times more deadly than covid if you go by the fatality rate provided 12 days earlier by the WHO’s Mike Ryan.</p><p>And this is missing one big point about covid. The average person who dies from covid is over 80 years old and has multiple underlying health conditions. In other words, their life expectancy is very short. The average person who died in the 1918 pandemic was in their late 20’s. So each death in the 1918 pandemic actually meant around 50 years more of life lost per person than each death in the covid pandemic. Multiply that by the fact that it had a 19 times higher death rate, and the 1918 flu was in fact 950 times more deadly than covid, in terms its capacity to shorten people’s lives.</p><p>Ok, I’ve discussed the fatality rate of the 1918 flu pandemic, and compared that to covid. But what about the fatality rate of the common cold viruses that are constantly circulating in society? How does covid compare to them?</p><p>Many people think that the common cold viruses are harmless. But in fact, among elderly people with underlying health conditions, they are frequently deadly. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5343795/" target="_blank" rel="noreferrer noopener">A study carried out in 2017</a> found that, among frail elderly people, rhinovirus is actually more deadly than regular influenza. In that study, the 30 day mortality for frail elderly people admitted to hospital due to a rhinovirus infection was 10% . For frail elderly people admitted to hospital due to influenza, 30 day mortality was 7% .</p><p>What is my point?  If you are old and frail, and have underlying health conditions, then even that most harmless of all infections, the so called “common cold”, can be deadly. In fact, it often is. Covid-19 is not a unique disease, and does not appear to have a noticeably higher mortality rate than the so called “common cold”.</p><p>There is one final aspect to all this that needs to be discussed. And that is the effect of covid on overall mortality. If it turns out that covid has no effect on overall mortality, then that really brings in to question why we are locking down, since we’re not actually preventing any deaths. So, what is the effect of covid on overall mortality?</p><p>Let’s look at Sweden, since that is perhaps the country that has taken the most relaxed approach of any to preventing spread, and which should therefore also reasonably be expected to have had the highest impact on its overall death rate. From January to September 2020, <a href="https://cornucopia.cornubot.se/2020/10/september-2020-least-deadly-month-ever.html" target="_blank" rel="noreferrer noopener">Sweden experienced 687 deaths per 100,000 population</a>. The last time Sweden had a deadlier year was 2015. Personally, I don’t remember any big deadly pandemic happening in 2015.</p><p>In fact, 2020 is so far one of the least deadly years in Swedish history, and is largely in line with the average for the preceding five years. To be precise, it is 2,7% higher than the average for the preceding five years, which is well within the margin of error. In 2019, mortality was 6% lower than the average, so it should be expected that 2020 would have a slightly higher mortality than average, even without covid.</p><p>What does this mean? It means that covid, a supposedly deadly viral pandemic, has not killed enough Swedes to have any noticeable impact on overall mortality.</p><p>How can this be explained, when we know that 6,000 Swedes have died of covid?</p><p>As I see it, there are two possible explanations. The first is that most people who died “of” covid actually died with covid. In other words, they had a positive covid test and were therefore characterized as covid deaths, when the actual cause of death was something else. The second is that most people who died of covid were so old, and so frail, and had so many underlying health conditions, that even without covid, they would have died by now. There are no other reasonable explanations.</p><p>I am not saying that covid is nothing, or that it doesn’t exist. I am saying that it is a virus with a marginal effect on longevity. And yet, public policy in most countries has been driven by doomsday scenarios based on completely unrealistic numbers.  To put it simply, we’ve acted like we’re dealing with a global ebola outbreak, when covid is much more like the common cold.</p><p>UPDATE (26th October 2020): After SCB updated their numbers it has become clear that September 2020 was in fact the second least deadly month in Swedish history, not the least deadly month. That award goes to June 2019.</p><p>You might also enjoy reading my article <a href="https://sebastianrushworth.com/2020/09/19/covid-19-does-sweden-have-herd-immunity/" target="_blank" rel="noreferrer noopener">about why I think Sweden has herd immunity</a>, or enjoy watching <a href="https://sebastianrushworth.com/2020/10/13/covid-podcast-with-ivor-cummins/" target="_blank" rel="noreferrer noopener">my conversation with Ivor Cummins of Fat Emperor about covid-19</a>.</p><section id="blog_subscription-17"></section><div><div><p> I am a practicing physician in Stockholm, Sweden. My main interests are evidence based medicine, medical ethics, and medical history. I frequently get asked questions by my patients about health, diet, exercise, supplements, and medications. The purpose of this blog is to try to understand what the science says and to translate it in to a format that non-scientists can understand. <a href="https://sebastianrushworth.com/author/doctorsebastian/" rel="author"> View all posts by Sebastian Rushworth, M.D. </a></p></div></div></div></div>]]>
            </description>
            <link>https://sebastianrushworth.com/2020/10/24/how-deadly-is-covid-19/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25241663</guid>
            <pubDate>Sat, 28 Nov 2020 23:55:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: SimpleLogin – protect online privacy using email alias]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 3 (<a href="https://news.ycombinator.com/item?id=25241372">thread link</a>) | @sonmicrosystems
<br/>
November 28, 2020 | https://simplelogin.io/blog/an-email-for-each-website/ | <a href="https://web.archive.org/web/*/https://simplelogin.io/blog/an-email-for-each-website/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="hello-bar" role="alert">
    <p>
    SimpleLogin is featured in
    <a href="https://lespepitestech.com/startup-de-la-french-tech/simplelogin" target="_blank" rel="noopener" data-toggle="tooltip" title="aka the French ProductHunt">Les PÃ©pites Tech â†—</a>
  </p></div><div id="content" role="main">
    <div>
        

        

        

        <blockquote>
<p>Why do I receive so many spams?</p>
</blockquote>

<p>When this question was asked by my girlfriend (now wife ðŸ˜…), my immediate answer was “Stop giving away your email” and I suggested creating a secondary email for “suspicious” websites. Also, using the same email everywhere is like leaving the same <strong>footprint</strong> on the Internet, allowing advertisers to <code>cross-reference</code> your online behavior.</p>



<p>
    <img src="https://simplelogin.io/blog/footprint.jpeg" alt="Fingerprint image">
</p>


<p>She followed the advice, created a second email and was happy at first. But now she doesn’t even check this mailbox as there are so many spams in it ðŸ’�ðŸ�»â€�â™€ï¸�.</p>

<p>So creating a second email is not a true solution. She needs more than 2, maybe hundreds. <strong>And why not an email for each website</strong>? But she cannot go to Gmail or Outlook to create hundreds of accounts, this is unmanageable. There must be a better way.</p>

<p>The solution is <code>email alias</code>. An alias is a normal email address but all emails sent to an alias will be <strong>forwarded</strong> to your real email address. Alias acts therefore as a <strong>shield</strong> (or a proxy) for your real email address. An alias can be disabled anytime, making the spams stop.</p>

<p>Nowadays, some websites allow to unsubscribe quickly but a lot of them still make unsubscribing a difficult process. Some wouldn’t even honor the request. And this doesn’t stop the websites from cross-referencing your data with your email being the common key.</p>

<p>Let’s make spammers’ life harder with email alias!</p>


        <p>
            Written by <img src="https://simplelogin.io/images/son.jpg" alt="Author Image">
            Son Nguyen Kim
            <a href="https://twitter.com/nguyenkims">
                [Follow on Twitter]
            </a>
        </p>


        
        <h3>Other posts</h3>
        
        


        <hr>
        <div>
            <p>Wonder why you received so many spams? Protect your email address with SimpleLogin alias. </p>
            <p><span>
                <a href="https://simplelogin.io/">Learn More</a>
            </span>
        </p></div>
    </div>
</div></div>]]>
            </description>
            <link>https://simplelogin.io/blog/an-email-for-each-website/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25241372</guid>
            <pubDate>Sat, 28 Nov 2020 23:11:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Isetta: Writing a Game Engine from Scratch]]>
            </title>
            <description>
<![CDATA[
Score 99 | Comments 4 (<a href="https://news.ycombinator.com/item?id=25241361">thread link</a>) | @da_big_ghey
<br/>
November 28, 2020 | https://isetta.io/blogs/week-0/ | <a href="https://web.archive.org/web/*/https://isetta.io/blogs/week-0/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <div data-md-component="container">
          
            
              
            
            
              
            
          
          <div>
            <article>
              
                
                
                
<h2 id="introduction">Introduction<a href="#introduction" title="Permanent link">¶</a></h2>
<p>The Isetta Engine is a student-driven project about demystifying game engine development and providing a roadmap and relevant knowledge for novice developers. To do so, <a href="https://isetta.io/team/">our team</a> will make a game engine by ourselves starting from a collection of base frameworks, and document the process, pitfalls, and advice for our audience with periodic blogs. Besides that, we will conduct interviews with experienced professionals to augment our novice perspective. We believe the novice perspective from our blogs and expert perspective from the interviews will nicely come together and form a complete document to help people get started. </p>
<p>The reason we think more work needs to be done in this field is that too many game engine developers wait until the completion of the engine, typically years, to talk about their development. For studios, this is because they consider the final product to be the game, not the engine. For others, it may be because the engine is what they see as valuable, not the writing. As a result, these talks typically lose the minutiae of the actual daily struggles that took place in the development process. There are others who document their development which has been going on for years, which makes it a daunting task for newcomers to start following along. </p>
<p>Although the project is aimed at helping novice developers, this is not to be used as a sole source of learning engine development. Being new engine developers ourselves, we can't guarantee the way we develop the engine will be correct, which is why interviews will help the project remain grounded. This means others who are learning can use what we've done as a guide and not necessarily the ground truth. The blogs won't be a walkthrough/tutorial/step-by-step instructions on how to develop an engine. We are learning as we go and think our journey is what can be valuable to you. </p>
<h3 id="about-the-project">About the Project<a href="#about-the-project" title="Permanent link">¶</a></h3>
<p>This project is being done as a student-pitched project at the <a href="https://www.etc.cmu.edu/">Entertainment Technology Center</a> (ETC). The ETC is an interdisciplinary Master's degree program at Carnegie Mellon University where students' main focus is working on small teams on a project each semester during a 3-month time period. Throughout the semester, a team's work will be presented to faculty and peers with feedback and critique being presented to help aid in the project development. Our particular project idea has gone through multiple iterations to do the following:</p>
<ol>
<li>Simplify the engine to be feasible within 3 months and</li>
<li>Deliver content that would be useful, and hopefully enjoyable, to consume.</li>
</ol>
<p>As of writing this, we've learned that creating content that will satisfy both is difficult and time-consuming, so we will be focusing on writing these milestone-type blogs as well as posting various types of content to test which is the best form of presenting our work. The short project duration also forces us to think clearly about our scope and be lean on the features we include before starting. </p>
<h3 id="schedule">Schedule<a href="#schedule" title="Permanent link">¶</a></h3>
<p>During the course of this project <strong>(08/26 - 12/16, 2018</strong>), a blog post will be published every week to share our thoughts and process, and an interview will be published every 1-2 weeks. The interview schedule depends on our progress on the engine itself, as each interview's topic will be themed around our current work.</p>
<p>For latest schedule, see our <a href="https://isetta.io/schedule/">schedule</a> page.</p>
<h3 id="prerequisites">Prerequisites<a href="#prerequisites" title="Permanent link">¶</a></h3>
<p>Although we will cover some basic features of engine development, it will profoundly help if you have experience in C++ programming and developing software, especially games, as our project won't provide step-by-step instructions on how to do everything. For a list of resources on how to gain related knowledge, please go to the <a href="#Readings">Readings</a> section. Additional resources will be posted on our <a href="https://isetta.io/resources/">resource page</a>.</p>
<p>Another prerequisite is passion for learning game engine development. As you are still reading this, we assume you are as excited about this as we are. This will be a bumpy ride, but you will have us on your side.</p>
<h2 id="research">Research<a href="#research" title="Permanent link">¶</a></h2>
<p>Being a student-pitched ETC project means that the project needed to pass through a pitch process of consulting and convincing faculty in the program. This allowed us to receive feedback about what could be considered a reasonable/manageable scope and where we might hit challenges for a general project. For this project to be a valid ETC project as well as accomplish our mission statement, there needs to be a fine balance between documentation and development. </p>
<p>Before confronting the big monster of engine development and documentation, we thought it would be a good idea to gear up by getting input from people who have actually done this. During our pitch process, we reached out and got the chance to talk to numerous industry professionals and got extremely helpful advice from them. All of these suggestions helped us shape our project into what it is now and provided invaluable knowledge on how to start a game engine. Thus we encourage you, too, to approach professionals and get advice if possible. We've compiled our notes from our conversations with them into a write-up, which will be published soon.</p>
<h3 id="why-another-engine">Why another engine?<a href="#why-another-engine" title="Permanent link">¶</a></h3>
<p>Using an existing game engine like <a href="https://unity3d.com/">Unity</a>, <a href="https://www.unrealengine.com/en-US/what-is-unreal-engine-4">Unreal</a> or <a href="https://www.panda3d.org/">Panda3D</a> is always a handy option to make a game. These well-established engines have a strong collection of tools and APIs so that developers can focus on making the game, not the wheels. However, there is the limitation of not having full control over all systems in the engine as well as not knowing how the engine is processing the game logic and assets. These can obstruct the complex systems of an engine, so although you may have an understanding of how a physics or graphics engine works, each engine operates differently and optimizes for different constraints. </p>
<p>In terms of learning about game engines and how to develop one, these established engines aren't a good source. Panda3D, originally developed by Disney and expanded by past ETC projects, has an older codebase in 2018 with limited community involvement. It is also not using the current industry standard language (C++). Unity and Unreal are both too massive and too cutting-edge to be suitable engine learning material for novices. In addition, Unity's source code isn't publicly available so you technically can't learn from it. The huge codebase sets a high threshold for any beginner to get started.</p>
<h2 id="roadmap">Roadmap<a href="#roadmap" title="Permanent link">¶</a></h2>
<p>The Isetta Engine will support the most primitive form of networked multiplayer twin-stick shooter game. Networked multiplayer was selected to be a part of the engine because it offers significant design and development challenge on every level of the project, and will help differentiate this engine from others being developed. We decided to create the engine in 3D for two reasons: Most AAA engines are 3D, and 3D requires more math and problem solving for us as developers to learn and grow from.</p>
<p>While planning, and before we knew too much about game engines, we had a basic idea of what a game engine would consist of. The image below displays the second/third iteration of what the Isetta engine would look like. We were initially naive thinking we may be able to do both networking as well as physics, however quickly came to grips that would balloon the scope too much. The audio and graphics were and are still planned to be imported from external libraries, and more of the discussion of what is imported and why will be included in a future blog. This diagram of the engine will soon be replaced with more in-depth explanations.</p>
<p><img alt="alt_text" src="https://isetta.io/images/blogs/week-0/pitch_architecture.png" title="Engine Architecture During Pitch"></p>
<h3 id="genre">Genre<a href="#genre" title="Permanent link">¶</a></h3>
<p>As for our choice of the twin-stick shooter genre, we came to the decision after lengthy consideration of the components required to build other game types as well as how that genre would utilize multiplayer. Twin-stick shooters can effectively have little to no physics, which is different from collisions (this will be explained in <a href="https://isetta.io/blogs/week-1/">week 1 blog</a>). Likewise, the information passed between networked sessions is relatively minimal and not too strict on latency. What's more, a twin-stick shooter specializes in simplistic gameplay that doesn't need a world editor or too much design. </p>
<p>In a Skype meeting, <a href="https://waltdestler.com/">Walt Destler</a> explained to us that each game -and more particularly, each genre- requires vastly different netcode solutions. This is also one of the reasons why we prefer netcode over physics, as it can greatly narrow down the genre options. For example, multiplayer shooters, specifically PvP shooters, require small amounts of information to be passed (i.e. bullet and player locations) from server to client with relatively low latency. PvP shooters can also feature client-side prediction <sup id="fnref:0"><a href="#fn:0" rel="footnote">1</a></sup> as well as the additional requirement of lobby/matchmaking with usually more than 2 players. On the other hand, genres like turn-based strategy require large amounts of information to be passed (all units, decisions, resources, etc.) to all users without too much concern for latency or prediction.</p>
<h3 id="building-with-an-example-game">Building with an Example Game<a href="#building-with-an-example-game" title="Permanent link">¶</a></h3>
<p>The other piece of advice we frequently heard from professionals and our faculty alike was the benefit of developing a game in conjunction with the engine. Doing so, they explained, allows you to prove and demonstrate your engine works as expected. The game can also test features to show immediate edge cases of the engine. </p>
<p>Another nicety of developing an engine is that feature creep can be prevented when you keep expanding certain features that won't be utilized in the final product. What the game built from this engine <em>won't</em> be is something original or necessarily fun. However, that's not to say a fun, original game couldn't be created from this engine. The idea of our sample game is to intentionally be derivative so features of a basic twin-stick shooter will be already included in the engine, rather than only specific …</p></article></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://isetta.io/blogs/week-0/">https://isetta.io/blogs/week-0/</a></em></p>]]>
            </description>
            <link>https://isetta.io/blogs/week-0/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25241361</guid>
            <pubDate>Sat, 28 Nov 2020 23:09:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Thank You, Tony]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25241102">thread link</a>) | @sethbannon
<br/>
November 28, 2020 | https://elizabethyin.com/2020/11/28/thank-you-tony/ | <a href="https://web.archive.org/web/*/https://elizabethyin.com/2020/11/28/thank-you-tony/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-4860">
	
	<!-- .entry-header -->

	<div>
		
<p>Coincidentally enough, I had decided that I wanted to write a letter to my mentor Tony this weekend, thanking him for the huge impact he’s had on my life. But I suppose that has now morphed into a more public letter. </p>



<p>Being in a zombie, half blurry-eyed state, it’s a little tough to articulate well what Tony has meant to me. We go through life, and a lot of people have an impact on us, and it’s often hard to say that a particular person caused you to take a particular path. But, Tony is one of the few people whom I can definitely say my life would not at all be what it is without him. </p>



<p>I first met him many years ago in Dec 1996 when I was just starting high school. My best friend Jennifer asked me what I was doing for winter break and if I wanted to help out her cousin Tony with his new internet startup. I had no idea what an internet startup was, but I had nothing going on, so I agreed to go. </p>



<p>On the first day of our holiday break, Jennifer and I hopped on the Caltrain from the peninsula (Silicon Valley) to head to his office in SOMA (San Francisco). It was exciting! We couldn’t drive, but we could go to a startup office all by ourselves! I followed Jennifer to the address. I think their office was actually someone’s apartment. And when we got there, there seemingly was no one there. We hung around for a while, and maybe about 10am or so some people started to trickle in – this was my introduction to startup life. </p>



<p>Tony had more or less just graduated from Harvard, as he had spent a few short months at Oracle after graduation, and then decided to quit to build his own company LinkExchange. Leaving a big employer so quickly was highly unusual back then, and he would go on to do many non-conforming things that he was right about. </p>



<p>LinkExchange was an ad network. If you had a website and hosted LinkExchange ad banners, for every 2 impressions you generated, you received 1 free impression of your banner. Back then because the internet was still new, people paid attention to banners and clicked on them like crazy, so this was incredibly effective in helping generate traffic for people. LinkExchange would then sell the remaining ad impressions. They had launched the service earlier that year, and by that December, they were already growing like crazy. </p>



<p>I definitely was *not helpful*. We put together some tables and chairs. And made ethernet cables from scratch. Yes, from scratch. We had to cut cables, splice the individual wires, and insert them into plastic tips and crimp them. I was terrible at this, and Jennifer largely fixed all of my mistakes (as always). Since I was so bad at the ethernet cable making process, they asked me to help them put together an internal webpage to keep all their meetings / schedule together. Great! I could put my basic HTML skills to use. I don’t think they liked the amateurish turquoise background that I chose, so someone at LinkExchange quickly fixed it. </p>



<p>I was more of a nuisance than help, but what I saw that day was super inspiring. Here were a bunch of friends who were getting together to build things. They could wear whatever they wanted, saunter in whenever they wanted, and eat all the pizza they wanted. And they all had varied tasks and were taking business meetings in what looked like a kitchen? There was never a dull moment. It was the dream. Neither of my parents worked in tech and certainly not startups, so this was incredibly eye-opening, and immediately, I knew that this was what I was going to do when I grew up – become an entrepreneur and start companies. Without this exposure to startups in 1996, I don’t think I would have made this realization (if at all) for many years.</p>



<p>Fast forward a few years later. Microsoft bought LinkExchange just 2 years later for a reported $265m. Tony and his college friend Alfred (who also was at LinkExchange) decided to start a startup incubator with their own money. (As a side note, the food at their restaurant in this incubator was fantastic!) This was bold and unique, because Idealab was really the only one doing something somewhat similar at the time. The concept of accelerators or incubators would not come in a big way until years later. </p>



<p>I don’t think Tony knew this, but while in high school, I cold-emailed almost every single one of his portfolio companies to ask them for an internship. I was determined to work at a startup as soon as possible in high school and start my startup career. I ended up working at one of his portfolio companies (probably unbeknownst to him) in the summer of 2000 which really help set me on my path to a career in startups. Often that first opportunity is the hardest to land, and each subsequent opportunity opens more and more doors.</p>



<p>By late 2000, the stock market had crashed and everyone was fleeing startups. Newspapers ran headlines saying that tech was dead and that all programming jobs would be outsourced. They couldn’t have been more wrong, but most people panicked and stopped investing in startups. Tony, was always a first-principles thinker, and he leaned into this and ended up co-leading one of his portfolio companies Zappos. As an e-commerce company that provided free shipping and free returns, Zappos’ margins were razor thin. This scared most VCs. (This would certainly scare me.) But, he believed that it could work if customers had a great experience and would become repeat customers. So, he poured a lot of his own capital into the business when no VC at the time would touch this company. While many VCs pontificate about high level things, he looked at problems from a bottom’s up approach which made his thinking often unique from others. </p>



<p>In order to make the Zappos model work, he needed customer service to be top notch, reliable, and have a lower cost. It was clear to him that this model wouldn’t work in pricey San Francisco. So, he made another bold decision to move the company to Henderson, NV, which could provide all of those things. This was at a time when certainly most people believed you can only build an internet business in the San Francisco Bay Area. In hindsight, when they ended up needing to hire tons of people, being the BEST internet company in the Las Vegas area allowed them to swoop up talent that would have been a huge war to win in San Francisco.  The other thing that was apparent to me, is that the Zappos family was incredibly diverse — and free to be themselves. Tony’s attention to culture and cultivating an open and welcoming work family left a lasting impact on me, and he started doing this well before it became a trend. Like so many of you, his book <em>Delivering Happiness</em> has affected my own thinking.</p>



<p>Fast forward many years, Jennifer and I were working on our own startup, and we were grappling with a particular challenge. We decided to consult Tony and get his advice. In a very Tony-like way, he basically told us to do what we thought was best. I remember feeling frustrated at the time, because…I didn’t know what was best! I had wanted him to tell me what he thought was best! But a good mentor helps you find your own answer – he/she doesn’t tell you the answer.  </p>



<p>In that meeting, he also pulled out a book and told me to read it. The book was <em>Start with Why</em> by Simon Sinek. So I read it. The gist of the book is that to build a great company, you need to go back to first principles. Most people start building a company by trying to sell their product details. “Buy my shoes! They are grey with stripes.” But customers, employees, and everyone want to rally around something much bigger — something inspirational that the company stands for. It actually took me many years to process this and actually figure out how to implement this — I didn’t get this right for LaunchBit, but after marinating on this advice for years, this is what we’ve been striving to do at Hustle Fund. We aren’t just capital deployers. We are trying to create a movement and a philosophy. That particular meeting with Tony is one that I play in my head over and over – I feel like I finally understand what he was getting at and that is what I most wanted to tell him this weekend. </p>



<p>If cynics believe that it’s impossible to get ahead in life without being an asshole, Tony Hsieh is a good example of someone who defies the norm. He was never flashy and always kind and generous to all. I could go on and on in writing this, but thank you, Tony, for helping me in so many ways that you may not have been aware. For your kindness, generosity, and inspiration.</p>



<p>Sending big hugs to the entire Hsieh family whom I consider to be my second family and who took me in during my formative years and had a huge impact on who I am today.</p>
	</div><!-- .entry-content -->

	
	<!-- .entry-footer -->

</article></div>]]>
            </description>
            <link>https://elizabethyin.com/2020/11/28/thank-you-tony/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25241102</guid>
            <pubDate>Sat, 28 Nov 2020 22:31:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Growl in Retirement]]>
            </title>
            <description>
<![CDATA[
Score 366 | Comments 173 (<a href="https://news.ycombinator.com/item?id=25241030">thread link</a>) | @flyingyeti
<br/>
November 28, 2020 | http://336699.org/GrowlRetirement | <a href="https://web.archive.org/web/*/http://336699.org/GrowlRetirement">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section id="container">
  <article id="8GugXoUrk4po3a8J7EZnDS">
	<time datetime="2020-11-28">November 28, 2020</time>
  
	<p>Growl is being retired after surviving for 17 years. With the announcement of Apple’s new hardware platform, a general shift of developers to Apple’s notification system, and a lack of obvious ways to improve Growl beyond what it is and has been, we’re announcing the retirement of Growl as of today.</p>

<p>It’s been a long time coming. Growl is the project I worked on for the longest period of my open source career. However at WWDC in 2012 everyone on the team saw the writing on the wall. This was my only WWDC. This is the WWDC where Notification Center was announced. Ironically Growl was called Global Notifications Center, before I renamed it to Growl because I thought the name was too geeky. There’s even a sourceforge project for Global Notifications Center still out there if you want to go find it.</p>

<p>We’ve had a lot of support over the years; from our hosting providers at <a href="http://www.networkredux.com/">Network Redux</a>, <a href="http://www.cachefly.com/">CacheFly</a> and others, to all of the apps using our framework, bindings, or any other integration. Special thanks go to <a href="https://adium.im/">Adium</a> and <a href="https://colloquy.app/">Colloquy</a>. Without these two projects having developers who wanted different types notifications, Growl wouldn’t have existed. Without Growl I do not know that we would have any sort of decent notification system in OS X, iOS, Android or who knows what else. </p>

<p>Special thanks goes to <a href="https://www.transifex.com/">Transifex</a> who made localizing into 24 languages a lot easier than anything else we tried. It’s a fantastic product, if you make software please try it. Our localizers were fantastic people and should all be commended for their work. </p>

<p>For developers we recommend transitioning away from Growl at this point. The apps themselves are gone from the app store, however the code itself still lives. Everything from our rake build system to our code is available for use on our <a href="https://github.com/growl/growl/">GitHub page</a></p>

  <figure id="kudo_8GugXoUrk4po3a8J7EZnDS">
    <a href="#kudo">
      
    </a>
    <p>1,752</p>
    <p>Kudos</p>
  </figure>
  <figure id="kudo_side_8GugXoUrk4po3a8J7EZnDS">
    <a href="#kudo">
      
    </a>
    <p>1,752</p>
    <p>Kudos</p>
  </figure>
</article>

</section></div>]]>
            </description>
            <link>http://336699.org/GrowlRetirement</link>
            <guid isPermaLink="false">hacker-news-small-sites-25241030</guid>
            <pubDate>Sat, 28 Nov 2020 22:19:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Blogging vs. Blog Setups]]>
            </title>
            <description>
<![CDATA[
Score 229 | Comments 95 (<a href="https://news.ycombinator.com/item?id=25240939">thread link</a>) | @Kye
<br/>
November 28, 2020 | https://rakhim.org/honestly-undefined/19/ | <a href="https://web.archive.org/web/*/https://rakhim.org/honestly-undefined/19/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Hi, I'm Rakhim. I teach, program, make podcasts, comics and videos on computer science at <a href="https://codexpanse.com/">Codexpanse.com</a>. You can learn more about <a href="https://rakhim.org/about">my work</a> and even <a href="https://www.patreon.com/rakhim">support me</a> via Patreon.</p><p><form action="https://buttondown.email/api/emails/embed-subscribe/rakhim" method="post" target="popupwindow" onsubmit="window.open('https://buttondown.email/rakhim','popupwindow')"><label for="bd-email">Oh, and I have a monthly non-spammy personal newsletter:</label><br>

</form></p><nav><p><a href="https://rakhim.org/">Home</a>
<span>|</span>
<a href="https://blog.rakhim.org/">Blog</a>
<span>|</span>
<a href="https://rakhim.org/about">About</a>
<span>|</span>
<a href="https://codexpanse.com/">Courses</a>
<span>|</span>
<a href="https://rakhim.org/talks">Talks</a>
<span>|</span>
<a href="https://rakhim.org/honestly-undefined">Comics</a>
<span>|</span>
<a href="https://rakhim.org/bookshelf">Bookshelf</a>
<span>|</span>
<a href="https://www.youtube.com/c/codexpanse">YT</a>
<span>|</span>
<a href="https://twitter.com/freetonik">TW</a>
<span>|</span>
<a href="https://rakhim.org/index.xml">RSS</a></p></nav><p>© Rakhim Davletkaliyev, 2020<br>Powered by <a href="https://gohugo.io/">Hugo</a>, <a href="https://www.netlify.com/">Netlify and the Everett interpretation of QM.</a></p></div></div>]]>
            </description>
            <link>https://rakhim.org/honestly-undefined/19/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25240939</guid>
            <pubDate>Sat, 28 Nov 2020 22:03:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Evolution of tree data structures for indexing: more exciting than it sounds]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25240883">thread link</a>) | @erthalion
<br/>
November 28, 2020 | https://erthalion.info/2020/11/28/evolution-of-btree-index-am/ | <a href="https://web.archive.org/web/*/https://erthalion.info/2020/11/28/evolution-of-btree-index-am/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<p><span>28 Nov 2020</span></p><h2 id="0-how-to-read-me">0. How to read me?</h2>
<p>I have to admit, my research blog posts are getting longer and longer. From one
side I find it genuinely encouraging, because if one gets so much information
just by scratching the topic, imagine what’s hidden beneath the surface! One
university professor once said “what could be interesting in databases?”, and
it turns out freaking a lot! On the other side it certainly poses problems for
potential readers. To overcome them I would suggest an interesting approach:
print this blog post out, or open it on your tablet/e-reader, where you can
make notes with a pencil or markers. Now while reading it try to spot ideas
particularly exciting for you and mark them. Along the way there would be
definitely some obscure parts or questions, write them on the sides as well.
You can experiment with the diagrams, changing or extending them, or just
drawing funny faces. But do not read everything at once, have no fear of
putting it aside for a while, and read in chunks that are convenient for you.
Some parts could be skipped as the text is build out of relatively independent
topics. The table of contents can help and guide you. Having said that we’re
ready to embark on the journey.</p>
<ul>
<li>
<a href="#1-introduction">Introduction</a>
</li>
<li>
<a href="#2-rum-conjecture">RUM conjecture</a>
</li>
<li>
<a href="#3-b-tree-basics">B-tree basics</a>
</li>
<li>
<a href="#4-beyond-the-hard-leaves-of-basics">Beyond the hard leavers of
basics</a>
<ul>
<li>
<a href="#41-key-normalization">Key normalization</a>
</li>
<li>
<a href="#42-prefix-truncation">Prefix truncation</a>
</li>
<li>
<a href="#43-dynamic-prefix-truncation">Dynamic prefix truncation</a>
</li>
<li>
<a href="#44-suffix-truncation">Suffix truncation</a>
</li>
<li>
<a href="#45-indirection-vector">Indirection vector</a>
</li>
<li>
<a href="#46-sb-tree">SB-tree</a>
</li>
</ul>
</li>
<li>
<a href="#5-why-is-it-not-enough">Why is it not enough?</a>
<ul>
<li>
<a href="#51-partitioned-b-tree">Partitioned B-tree</a>
</li>
<li>
<a href="#52-hybrid-indexes">Hybrid indexes</a>
</li>
<li>
<a href="#53-bw-tree">Bw-Tree</a>
</li>
<li>
<a href="#54-dptree">DPTree</a>
</li>
</ul>
</li>
<li>
<a href="#6-trie">Trie</a>
</li>
<li>
<a href="#7-learned-indexes">Learned indexes</a>
</li>
<li>
<a href="#8-is-that-all">Is that all?</a>
</li>
<li>
<a href="#references">References</a>
</li>
</ul>
<h2 id="1-introduction">1. Introduction</h2>
<p>Whenever we speak about indexes, especially in PostgreSQL context, there is a
lot to talk about: B-tree, Hash, GiST, SP-GiST, GIN, BRIN, RUM. But what if I
tell you that even the first item in this list alone hiding astonishing number
of interesting details and years of research? In this blog post I’ll try to
prove this statement, and we will be concerned mostly with B-tree as a data
structure.</p>
<p><img src="https://erthalion.info/public/img/btree-joke.png" width="80%"></p>

<p>Let’s start systematically and take a look at the definition first:</p>
<blockquote>
<p>B-tree is a self-balancing tree data structure that maintains sorted data and
allows searches, sequential access, insertions, and deletions in logarithmic
time.</p>
</blockquote>
<p>What is your first association with the concept of B-tree? Mine is “old and
well researched, or in other words boring”. And indeed apparently it was first
introduced in <a href="https://infolab.usc.edu/csci585/Spring2010/den_ar/indexing.pdf">1970</a>! Not only that, already in 1979 they
were <a href="http://cgi.di.uoa.gr/~ad/M149/ubiquitous_btree.pdf">ubiquitous</a>. Does it mean there is nothing exciting left any
more? Once upon a time I came across a remarkable read called
<a href="https://dl.acm.org/doi/10.1561/1900000028">Modern B-Tree techniques</a> which inspired me to dig
deeper into the topic and read bunch of shiny new whitepapers. Afterwards
totally by chance I’ve stumbled upon a book “Database Internals: A Deep Dive
into How Distributed Data Systems Work”, which contains great sections on
B-tree design. Both works were the triggers to write this blog post. What was I
saying about nothing exciting left? At the end I couldn’t be more wrong.</p>
<p>It turns out that there are multitude of interesting ideas and techniques
around B-Trees. They’re all coming from the desire to satisfy different (often
incompatible) needs, as well as adapt to emerging hardware. To demonstrate how
many of those exist, lets play a game. Below you can find a table of names I’ve
found in various science papers, together with a couple of silly names I’ve
come up myself. Can you find out the fake ones?</p>
<table>
<tbody>
<tr>
<td>B-tree</td>
<td>B<sup>+</sup>-tree</td>
<td>B<sub>link</sub>-tree</td>
<td>DPTree</td>
</tr>
<tr>
<td>wB<sup>+</sup>-tree</td>
<td>NV Tree</td>
<td>FPTree</td>
<td>FASTFAIR</td>
</tr>
<tr>
<td>HiKV</td>
<td>Masstree</td>
<td>Skip List</td>
<td>ART</td>
</tr>
<tr>
<td>WORT</td>
<td>CDDS Tree</td>
<td>Bw Tree</td>
<td>HOT</td>
</tr>
<tr>
<td>KISS Tree</td>
<td>VAST Tree</td>
<td>FAST</td>
<td>HV Tree</td>
</tr>
<tr>
<td>UB Tree</td>
<td>LHAM</td>
<td>MDAM</td>
<td>Hybrid B<sup>+</sup> Tree</td>
</tr>
</tbody>
</table>

<p>Any ideas? Well, I have a confession to make – all of them are real, I just
don’t have enough imagination to come up with such names. Having this in mind
hopefully you understand that if we want to make a survey, the first step would
be to establish some classification. Not only this will help us to structure
the material, but also will explain why on earth anyone would need to invent so
many variations of what we though was so simple!</p>
<h2 id="2-rum-conjecture">2. RUM conjecture</h2>
<p>To classify different index access methods we need to think about the following
ambitious question – is there anything common between almost any index access
method? The authors of <a href="https://stratos.seas.harvard.edu/files/stratos/files/rum.pdf">RUM conjecture</a> provide an interesting
insight about this topic:</p>
<blockquote>
<p>The fundamental challenges that every researcher, systems architect, or
designer faces when designing a new access method are how to minimize, i)
read times, ii) update cost , and iii) memory (or storage) overhead.</p>
<p>In this paper, we conjecture that when optimizing the read-update-memory
overheads, optimizing in any two areas negatively impacts the third</p>
</blockquote>
<p>This essentially states that if an index access method could be specified as a
point inside “Read”, “Update” (on the Fig. 1 it’s called “Write” for
convenience of drawing), “Memory” space we can observe an interesting
invariant. Every time we modify one index access method to have less overhead
for reading or memory footprint (i.e. shift the corresponding point closer to
“Read”/”Memory” corners), we inevitably loose on the updating workload (i.e.
getting further away from “Write” corner).</p>
<figure>
<img src="https://erthalion.info/public/img/rum.png" width="50%">
<br>
<figcaption>
Fig 1. RUM space
</figcaption>
</figure>
<p>In fact as a non-scientist I would even speculate that there should be another
dimension called “Complexity”, but the idea is still clear. I will try to show
this invariant at work via examples in this blog post, but it already gives us
some ground under the feet and opportunity to visually represent different
versions of B-tree by moving point on the triangle back and forth. But first
let’s recall the basics.</p>
<h2 id="3-b-tree-basics">3. B-tree basics</h2>
<p>So what is B-tree? Well, it’s a tree data structure: a root node, some number
of branch nodes (marked grey) and a bunch of leaf nodes (marked green):</p>
<figure>
<img src="https://erthalion.info/public/img/btree.png" width="100%">
<br>
<figcaption>
Fig 2. B-tree nodes arrangement
</figcaption>
</figure>
<p>Every node of this tree is usually a page of some certain size and contains
keys (shaded slices of a node) and pointers to other nodes (empty slices with
arrows). Keys on page are kept in sorted order to facilitate fast search within
a page.</p>
<p>The original B-tree design assumed to have user data in all nodes,
branch and leaf. But nowadays the standard approach is a variation called
B<sup>+</sup>-tree, where user data is present only in leaf nodes and branch
nodes contains separator keys (pivot tuples in PostgreSQL terminology). In this
way separation between branch and leave nodes become more strict, allowing
better flexibility for choosing format of former and making deletion operations
can affect only latter. In fact the original B-tree design is barely worth
mentioning these days and I’m doing this just to be precise. Since
B<sup>+</sup>-tree is sort of default design, we’ll use B-tree and
B<sup>+</sup>-tree interchangeably in this text from now on. An interesting
thing to mention here is that the only requirements for separator keys is to
guide search algorithm to a correct leaf node. As long as they fulfil this
condition they can contain anything, no other requirements exist.</p>
<p>Strictly speaking, only child pointers are truly necessary in this design, but
quite often databases also maintain additional neighbour pointers, e.g. what you
can see on the Fig. 2 between the leaf nodes. It could be helpful for some
operations like index scan, but need to be taken into account for node
split/merge operations. PostgreSQL uses <a href="https://www.csd.uoc.gr/~hy460/pdf/p650-lehman.pdf">Lehman-Yao</a> version,
called B<sub>link</sub>-tree, with links to both left and right sibling nodes
(the left link one is actually not presented in the original
B<sub>link</sub>-tree design, and it makes backward scan somewhat interesting),
and there are even implementations like WiredTiger with
<a href="https://github.com/wiredtiger/wiredtiger/blob/f08bc4b18612ef95a39b12166abcccf207f91596/src/include/btmem.h#L550">parent pointers</a>.</p>
<p>Having all this in place one can perform a search query by following the path
marked red on the Fig. 2, first hitting the root, finding a proper separator
key, following a downlink and landing on a correct page where we deploy binary
search to find the resulting key.</p>
<p>Until now, we were talking only about static parts of B-tree design, but of
course there is more to it. For example there is one dynamic aspect of much
importance (quite often it even scares developers like a nightmare), namely
page splits. What do we need to do when there is a new value to insert, but the
target page does not have enough space like on the following diagram?</p>
<figure>
<img src="https://erthalion.info/public/img/page-split-1.png" width="60%">
<br>
<figcaption>
Fig 3. B-tree page split (a)
</figcaption>
</figure>
<p>What happens here is we’re trying to insert the new value (shaded box) into the
page with not enough space for it. To maintain the three balanced we need to
allocate another leaf page, distribute keys between old and new leaf, promote a
new separator key into the parent page and update all required links
(left/right siblings if present):</p>
<figure>
<img src="https://erthalion.info/public/img/page-split-2.png" width="60%">
<br>
<figcaption>
Fig 4. B-tree page split (b)
</figcaption>
</figure>
<p>Curiously enough the new separator key could be chosen freely, it could be any
value as long as it separates both pages. We can see what does it change in the
optimization section.</p>
<p>Locking is obviously an important part of a page split. No one wants to end up
with concurrency issues when pages get updated while in the middle of a split,
so a page to be split is write-locked as well as e.g. right sibling to update
left-link if present.</p>
<p>As you can see, page splits are introducing performance overhead. We need to
bring in a new page, move elements around and everything should be consistent
and correctly locked. And already at this pretty much basic point we already
can see some interesting trade-offs. For example B*-tree modification tries to
rebalance data between neighbouring nodes to postpone page split as long as
possible. In terms of trade-offs it looks like a balance between complexity and
insert overhead.</p>
<p>I didn’t tell you everything about B<sub>link</sub>-tree and it’s going to be our next
topic example in this section. Not only Lehman-Yao version adds a link to the
neighbour, it also introduces a “high key” to each page, which is an upper bound
on the keys that are allowed on page. While obviously introducing a bit memory
overhead those two changes make it possible to detect a concurrent page split
by checking the page high key, which allows the tree to be searched …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://erthalion.info/2020/11/28/evolution-of-btree-index-am/">https://erthalion.info/2020/11/28/evolution-of-btree-index-am/</a></em></p>]]>
            </description>
            <link>https://erthalion.info/2020/11/28/evolution-of-btree-index-am/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25240883</guid>
            <pubDate>Sat, 28 Nov 2020 21:54:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Second Swiss firm allegedly sold encrypted spying devices]]>
            </title>
            <description>
<![CDATA[
Score 193 | Comments 131 (<a href="https://news.ycombinator.com/item?id=25240179">thread link</a>) | @secfirstmd
<br/>
November 28, 2020 | https://www.swissinfo.ch/eng/second-swiss-firm-allegedly-sold-encrypted-spying-devices/46186432 | <a href="https://web.archive.org/web/*/https://www.swissinfo.ch/eng/second-swiss-firm-allegedly-sold-encrypted-spying-devices/46186432">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<section>
<div>
<div>
<figure>
<picture>
<source srcset="https://www.swissinfo.ch/resource/image/46186430/landscape_ratio3x2/880/587/a683e845a8c9bdb270a5b635dfc947ed/gO/omnisec.jpg" media="(min-width: 900px)">
<source srcset="https://www.swissinfo.ch/resource/image/46186430/landscape_ratio3x2/580/387/a683e845a8c9bdb270a5b635dfc947ed/Gx/omnisec.jpg" media="(min-width: 321px)">

</picture>
<figcaption>
Omnisec is the second Swiss company that allegedly sold manipulated encryption devices to US intelligence services. <span>Keystone / Walter Bieri</span>
</figcaption> </figure>
</div>
</div><p>Swiss public television, SRF, has found a second company besides Crypto AG&nbsp;was involved in manufacturing manipulated devices allegedly used for spying by foreign intelligence.</p>
<span>This content was published on November 26, 2020 - 11:34</span>
<time datetime="2020-11-26T11:34:28+01:00">

</time><p>According to <a rel="noopener" target="_blank" href="https://www.srf.ch/news/schweiz/verschluesselungsgeraete-geheimdienstaffaere-weitere-schweizer-firma-rueckt-in-den-fokus">SRF sources</a>, the Swiss company Omnisec AG had ties to US intelligence services. This follows revelations in February by SRF, German television ZDF and <em>The Washington Post</em> that Zug-based firm Crypto AG was at the heart of a huge international spying operation led by the CIA, and to a lesser extent by the German BND spy agency.&nbsp;Omnisec was one of the largest competitors of Crypto AG.</p><p>Swiss cryptologist and professor Ueli Maurer was a consultant for Omnisec for years and told SRF that in 1989 US intelligence services (National Security Agency) contacted Omnisec through him.</p><p>Of concern are the OC-500 series devices. Devices were sold to several Swiss federal agencies. However, Swiss&nbsp;authorities only noticed the devices weren't secure&nbsp;in the mid-2000s.</p><p>Several Swiss companies also received manipulated devices from Omnisec, including Switzerland’s largest bank, UBS. It is unclear whether the authorities informed UBS about the weak devices in the mid-2000s. UBS told SRF that it does not comment on security matters but that it had no indications that sensitive data were exposed at the time.</p>
<p>Omnisec, founded in 1987, manufactured voice, fax and data encryption equipment. It was dissolved a few years ago. The most recent head of the company, Clemens Kammer, told SRF that Omnisec customers “have and will continue to place great value on security, confidentiality, discretion and reliability in business relationships”.</p><p>Some politicians have called for further investigations into these latest allegations that may reveal who, if anyone, in the federal government knew of Omnisec’s business affairs with foreign intelligence.</p><h2>Crypto affair</h2><p>Earlier this month, a nine-month&nbsp;<a rel="noopener" target="_blank" href="https://web.archive.org/web/20201110183555/https:/www.parlament.ch/press-releases/Pages/mm-gpdel-2020-11-10.aspx">investigation</a>&nbsp;by the Swiss parliamentary audit committee (GPDel), found that the Swiss intelligence service knew that the US Central Intelligence Agency was behind the Swiss-based Crypto AG as far back as 1993. The report says that Swiss intelligence later collaborated with them to gather information from foreign sources.&nbsp;</p><p>More than 100 countries bought encryption devices from the Zug-based company, which did business under the guise of Swiss neutrality. In reality, the firm belonged to the CIA and Germany intelligence service, which could freely read what it encrypted. Information intercepted with the help of Crypto’s devices changed the course of events, including the Iran hostage crisis of 1979.</p> </section>
</div></div>]]>
            </description>
            <link>https://www.swissinfo.ch/eng/second-swiss-firm-allegedly-sold-encrypted-spying-devices/46186432</link>
            <guid isPermaLink="false">hacker-news-small-sites-25240179</guid>
            <pubDate>Sat, 28 Nov 2020 20:18:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Hacking Printers Wiki, an open approach to share knowledge on printr]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25239627">thread link</a>) | @rolph
<br/>
November 28, 2020 | http://hacking-printers.net/wiki/index.php/Main_Page | <a href="https://web.archive.org/web/*/http://hacking-printers.net/wiki/index.php/Main_Page">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="bodyContent">
									<p>From Hacking Printers</p>
								
												
				<div id="mw-content-text" lang="en" dir="ltr"><div>
<p>This is the <b>Hacking Printers Wiki</b>, an open approach to share knowledge on printer (in)security.
</p>
</div>
<table id="mp-upper">

<tbody><tr>
<td>

</td>
<td>
</td>
<td>
<table id="mp-right">
<tbody><tr>
<td> <h2 id="mp-otd-h2"><span id="Tools">Tools</span></h2>
</td></tr>
<tr>
<td>
<ul><li> <a href="http://hacking-printers.net/wiki/index.php/PRET" title="PRET">PRET</a>, <a href="http://hacking-printers.net/wiki/index.php/Praeda" title="Praeda">Praeda</a>, <a href="http://hacking-printers.net/wiki/index.php/PFT" title="PFT">PFT</a>, <a href="http://hacking-printers.net/wiki/index.php/BeEF" title="BeEF">BeEF</a></li></ul>
</td></tr>
<tr>
<td> <h2 id="mp-itn-h2"><span id="Fundamentials">Fundamentials</span></h2>
</td></tr>
<tr>
<td>
<ul><li> <b><a href="http://hacking-printers.net/wiki/index.php/Fundamentals#Printer_Control_Languages" title="Fundamentals">Printer languages</a></b>
<ul><li> <a href="http://hacking-printers.net/wiki/index.php/PJL" title="PJL">PJL</a>, <a href="http://hacking-printers.net/wiki/index.php/PCL" title="PCL">PCL</a>, <a href="http://hacking-printers.net/wiki/index.php/PostScript" title="PostScript">PostScript</a></li></ul></li>
<li> <b><a href="http://hacking-printers.net/wiki/index.php/Fundamentals#Network_printing_protocols" title="Fundamentals">Network protocols</a></b>
<ul><li> <a href="http://hacking-printers.net/wiki/index.php/LPD" title="LPD">LPD</a>, <a href="http://hacking-printers.net/wiki/index.php/IPP" title="IPP">IPP</a>, <a href="http://hacking-printers.net/wiki/index.php/Raw" title="Raw">Raw</a>, <a href="http://hacking-printers.net/wiki/index.php/SMB" title="SMB">SMB</a></li></ul></li></ul>
</td></tr>
<tr>
<td> <h2 id="mp-otd-h2"><span id="Attack_Carriers">Attack Carriers</span></h2>
</td></tr>
<tr>
<td>
<ul><li> <a href="http://hacking-printers.net/wiki/index.php/USB_drive_or_cable" title="USB drive or cable">USB drive or cable</a></li>
<li> <a href="http://hacking-printers.net/wiki/index.php/Port_9100_printing" title="Port 9100 printing">Port 9100 printing</a></li>
<li> <a href="http://hacking-printers.net/wiki/index.php/Cross-site_printing" title="Cross-site printing">Cross-site printing</a></li></ul>
</td></tr>
<tr>
<td> <h2 id="mp-otd-h2"><span id="Countermeasures">Countermeasures</span></h2>
</td></tr>
<tr>
<td>
<ul><li> <a href="http://hacking-printers.net/wiki/index.php/Countermeasures#Vendors" title="Countermeasures">Vendors</a>, <a href="http://hacking-printers.net/wiki/index.php/Countermeasures#Admins" title="Countermeasures">Admins</a>, <a href="http://hacking-printers.net/wiki/index.php/Countermeasures#Users" title="Countermeasures">Users</a></li></ul>
</td></tr>
<tr>
<td> <h2 id="mp-otd-h2"><span id="Bibliography">Bibliography</span></h2>
</td></tr>
<tr>
<td>
<ul><li> <a href="http://hacking-printers.net/wiki/index.php/Bibliography" title="Bibliography">Literature on printer security</a></li></ul>
</td></tr>
<tr>
<td> <h2 id="mp-otd-h2"><span id="References">References</span></h2>
</td></tr>
<tr>
<td>
<ul><li> <a href="http://hacking-printers.net/wiki/index.php/References" title="References">Printer language references</a></li></ul>
</td></tr></tbody></table>
</td></tr></tbody></table>
<div>
<table id="mp-center">
<tbody><tr>
<td><h2 id="mp-tfl-h2"><span id="Beyond_Printers">Beyond Printers</span></h2></td>
</tr><tr>
<td><p>Comming soon: <i>Hacking PostScript processing websites</i></p></td>
</tr>
</tbody></table>
</div>

<!-- 
NewPP limit report
Cached time: 20201130203806
Cache expiry: 86400
Dynamic content: false
CPU time usage: 0.052 seconds
Real time usage: 0.055 seconds
Preprocessor visited node count: 19/1000000
Preprocessor generated node count: 56/1000000
Post‐expand include size: 0/2097152 bytes
Template argument size: 0/2097152 bytes
Highest expansion depth: 1/40
Expensive parser function count: 0/100
-->

<!-- 
Transclusion expansion time report (%,ms,calls,template)
100.00%    0.000      1 - -total
-->

<!-- Saved in parser cache with key wiki:pcache:idhash:1-0!*!0!!*!*!* and timestamp 20201130203806 and revision id 648
 -->
</div>					
								
							</div></div>]]>
            </description>
            <link>http://hacking-printers.net/wiki/index.php/Main_Page</link>
            <guid isPermaLink="false">hacker-news-small-sites-25239627</guid>
            <pubDate>Sat, 28 Nov 2020 19:01:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA['Welcome to Toronto' sign altered to read 'Ontario's capital in overdose deaths']]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25239598">thread link</a>) | @app4soft
<br/>
November 28, 2020 | https://toronto.citynews.ca/2020/11/23/toronto-welcome-sign-altered-overdose-deaths/ | <a href="https://web.archive.org/web/*/https://toronto.citynews.ca/2020/11/23/toronto-welcome-sign-altered-overdose-deaths/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="article-meta">
        
    <p itemprop="datePublished">Posted Nov 23, 2020 1:09 pm EST</p>
    <p itemprop="dateModified">Last Updated Nov 23, 2020 at 10:07 pm EST</p>             
        
    </div><div id="main-article">
    <div>
	    					
	    
        

        <div id="article-body-content" itemprop="articleBody"> 
        	<p>An unofficial addition has been made to a sign welcoming drivers to Toronto that casts the province’s capital in a less than flattering light.</p>
<p>The sign, on the border with Mississauga along Burnamthorpe Road, used to read “Welcome to Toronto. Ontario’s capital.” With the unauthorized addendum, it now reads “Ontario’s capital in overdose deaths.”</p>
<p>The addition appears to be a plank covered in a vinyl sign, printed to mimic the original sign’s colours and font, attached below it with zip-ties.</p>
<p>CityNews viewers alerted us to the changes and say a few signs in the area have been altered in this manner.</p>

<p>According to the chief coroner’s office, an estimated 50 to 80 people per week are dying of overdoses in Ontario.</p>
<p>Across the country, the COVID-19 pandemic has exacerbated the opioid crisis. Opioid overdoses have risen sharply since March as the border closure and limited access to services raise fatal risks for drug users.</p>
<p>It is unclear who created the additional signage and whether it is in fact referring to this disturbing rise in numbers — a stark reversal of the 13 per cent decline in fatal opioid overdoses between 2018 and 2019.</p>
<p>The signs were addressed in the city’s daily COVID-19 briefing question and answer session on Monday.</p>
<p>Mayor John Tory said he has repeatedly expressed “deep concern” about the overdose rates and feels not enough attention has been given to the issue.</p>
<p>“While this isn’t necessarily the single best way to draw attention to this, it is something that’s got us talking about it and I think the more we talk about it the more we advocate for greater action on the part of all governments,” said Tory.</p>
<p>Tory said Toronto Public Health has a “significant harm reduction program,” but added that the city needs more provincial support.</p>
<p>The city said the signs have been addressed by city staff and have now been corrected.</p>
<p><em>With files from The Canadian Press</em></p>
        </div>
        
		        
		 
        
                    
        	</div><!--/ 16 -->     
</div></div>]]>
            </description>
            <link>https://toronto.citynews.ca/2020/11/23/toronto-welcome-sign-altered-overdose-deaths/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25239598</guid>
            <pubDate>Sat, 28 Nov 2020 18:56:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[30 years later, QBasic is still the best]]>
            </title>
            <description>
<![CDATA[
Score 294 | Comments 122 (<a href="https://news.ycombinator.com/item?id=25239424">thread link</a>) | @ohjeez
<br/>
November 28, 2020 | http://www.nicolasbize.com/blog/30-years-later-qbasic-is-still-the-best/ | <a href="https://web.archive.org/web/*/http://www.nicolasbize.com/blog/30-years-later-qbasic-is-still-the-best/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<p><span><em>(5 minutes read)</em></span></p>
<p><span>My oldest son Noah turned 7 three months ago. If he could trade his family for a 2 hour session of playing minecraft, he would do it in a heartbeat. The other love of his life is Super Mario Maker, and&nbsp;it’s been a thrill to see him play the same game and levels that I played when I was his age. About 5 months ago, I left my family for my yearly pilgrimage of <a href="http://ludumdare.com/compo/">ludum dare</a>: a game dev competition during which I lock myself away with friends, return to a&nbsp;state of primitive caveman, not sleep for 48h, and create&nbsp;a full game from scratch (play it at the end of this post!) As I proudly showed my revolutionary AAA title to my wife, Noah was naturally intrigued and I introduced him to the world of code, showing him how simple&nbsp;words&nbsp;(he had just learned how to read) produced an actual game. Since&nbsp;that very day, Noah&nbsp;has been asking me repeatedly to teach him how to make&nbsp;his own video games. And for the past 5 months, I have been looking for the holy grail of language/IDE for kids in the hope of turning that spark of interest into a memorable experience…</span></p>
<p><span>My quest has led me to&nbsp;endless forums, through which I have tried countless suggestions: SmallBasic, Pico-8, Smalltalk, Scratch, etc. I have even inquired of the Great&nbsp;Oracles of StackOverflow,&nbsp;to&nbsp;no avail. After 5 months,&nbsp;I&nbsp;ended up with a disappointing conclusion: nothing is even close to what I had back in another era. 30 years later, QBasic is still the best when it comes to&nbsp;discovering programming.&nbsp;</span></p>
<blockquote>
<p><span>“OMG please don’t teach him GOTOs!!”</span></p>
</blockquote>
<pre><code>10 PRINT “OH NO, WHAT ARE YOU DOING?!!!”
20 GOTO 10</code></pre>
<p><span>Yes, QBasic is a terrible procedural language. It introduces one to concepts widely considered harmful, uses awkward syntax for implicit declarations, is not case sensitive, is non-zero-based, etc. the list goes on… When developing a skill, it is much better to acquire the right reflexes from the start rather than have to correct years of bad practice. Following this advice, I should have probably started off with&nbsp;the basics of the ruby language which I love. Yet, while most of those QBasic concepts are today generally considered&nbsp;as red flags by our peers, they each served a very specific&nbsp;purpose at the time: to keep the language simple and accessible, a notion that every other language has left behind in favor of flexibility, complexity and logic.</span></p>
<p><span>I installed QBasic on my son’s 11” HP Stream today, having to hack a DOSBox manual installation. He double clicked the icon on his desktop and in a split second, we were in the IDE, greeted with the introduction screen which brought back so many memories to my mind:</span></p>
<p><img src="https://upload.wikimedia.org/wikipedia/en/0/01/QBasic_Opening_Screen.png" alt="" width="640" height="400"></p>
<p><span>I then told Noah that there was a very sacred ritual, mandatory&nbsp;for anyone who enters the secret&nbsp;inner&nbsp;circle of programmers, to start off with a program that greets every other programmer out there. As I dictated the formula, he slowly&nbsp;searched for each&nbsp;key, carefully&nbsp;typing with his right finger&nbsp;the magic words: <code>PRINT “hello world”</code></span></p>
<p><span>He pressed F5 and looked amazed as he saw his code being compiled into text rendered on his black screen. He smiled, gave me a high-five, and then scribbled down the code in his little notebook so that he could remember later.</span></p>
<p><img src="http://nicolasbize.com/blog/images/noah_1.jpg" alt="" width="800" height="600"></p>
<p>We went on to a couple more commands: CLS, COLOR, PLAY, INPUT, and IF. There was nothing to explain: no complexity, no awkward operator, no abstract concepts, no documentation that needed to be read, no notion of objects/class/methods, no framework to install, no overwhelming menu/buttons in the IDE, no special keyword or parenthesis. It was code in its purest simplicity and form.</p>
<p><span>After less than an hour, he wrote his first program on his own – an interactive and incredibly subtle application which lets you know the computer’s feelings towards&nbsp;you as an individual and sensible human being:</span></p>
<p><img src="http://nicolasbize.com/blog/images/noah_3.jpg" alt="" width="800" height="600"></p>
<p><span>…which he ran with utmost pride for his cousin and best friend Christian:</span></p>
<p><img src="http://nicolasbize.com/blog/images/noah_4.jpg" alt="" width="800" height="600"></p>
<p><span>…after which he proceeded to easily explain him&nbsp;</span><span><b>how</b></span><span>&nbsp;it worked and what the code was doing!</span></p>
<p><img src="http://nicolasbize.com/blog/images/noah_5.jpg" alt="" width="800" height="600"></p>
<p><span>And so it was that in a single hour, my 7 year old was able to not only write his first text game, but also to experience the fun and thrill that comes from creating, compiling and executing his own little program. Bonus points, it all fit on a single notebook page:</span></p>
<p><img src="http://nicolasbize.com/blog/images/noah_2.jpg" alt="" width="600" height="800"></p>
<p><span>I was so glad that he was able to understand why I keep saying that I have the best job in the world.&nbsp;</span><span>My only regret today was to realize that in more than 30 years, we have not been able to come up with something&nbsp;better for our kids: Qbasic has a limited set of simple keywords (the entire help fits on a single F1 screen and is packed with simple examples!), does not distract the coder with any visual artifacts, has a very confined and cosy dev environment, shows errors as early as possible, compiles and executes the code in a heartbeat with a single key, and is extremely straightforward. &nbsp;We have built more robust and more complex languages/frameworks/IDEs (which are of course necessary for any real-life application), but we have never really made a simpler or more direct access to the thrill of programming than QBasic. Even running QBasic today has become dreadful&nbsp;to the novice that uses&nbsp;a modern Mac/PC/Linux machine, whereas it used to simply require inserting a 3,5” floppy in the A:\ disk drive…</span></p>
<p><span>Enough rant, today is all about the celebration of yet another person who discovered the excitement and beauty of programming!</span></p>
<p><span>Cheers!</span></p>
<p><span>(as promised, <a href="http://nicolasbize.com/ld34/">my AAA title</a> for which I await&nbsp;EA’s call to purchase copyrights)</span></p>
			</div></div>]]>
            </description>
            <link>http://www.nicolasbize.com/blog/30-years-later-qbasic-is-still-the-best/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25239424</guid>
            <pubDate>Sat, 28 Nov 2020 18:32:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Machine Learning for Art with Google’s Emil Wallner]]>
            </title>
            <description>
<![CDATA[
Score 22 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25239217">thread link</a>) | @andreyk
<br/>
November 28, 2020 | https://www.letstalkai.show/e/mlart-interview/ | <a href="https://web.archive.org/web/*/https://www.letstalkai.show/e/mlart-interview/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
	<div>


		
			<div id="post-15958234">
				<p><a href="https://www.letstalkai.show/">
										<img src="https://pbcdn1.podbean.com/imglogo/image-logo/7703921/Lets_Talk_Logo.jpg"></a></p><div>
				<div>
					
					<p>Nov 27th, 2020 by <a title="Posts by Skynet Today">Skynet Today</a> </p>
				</div>

				<div>
					 <div>
<p>An interview with <span>Emil Wallner, the creator of mlart.co . Emil is an internet-educated, independent machine learning researcher, and resident at the Google Arts &amp; Culture Lab. As a resident at Google he is using machine learning to explore art and culture. Part-time, he applies machine learning to logical tasks such as programming and mathematics.</span></p>
<p>Subscribe: <a href="https://feed.podbean.com/aitalk/feed.xml">RSS</a> | <a href="https://podcasts.apple.com/us/podcast/lets-talk-ai/id1502782720">iTunes</a> | <a href="https://open.spotify.com/show/17HiNdxcoKJLLNibIAyUch">Spotify</a> | <a href="https://www.youtube.com/channel/UCKARTq-t5SPMzwtft8FWwnA">YouTube</a></p>
<div>
<p>Check out coverage of similar topics at <a href="http://www.skynettoday.com/">www.skynettoday.com</a></p>
<p>Theme: Deliberate Thought Kevin MacLeod (incompetech.com)</p>
</div>
</div>
									</div>

				

   <p><span id="postbar_15958234"> <span> | </span><a href="https://www.podbean.com/site/EpisodeDownload/PBF380DAKYNN9" target="_blank">Download</a> </span></p>			</div>

		


	</div>
</div>
</div></div>]]>
            </description>
            <link>https://www.letstalkai.show/e/mlart-interview/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25239217</guid>
            <pubDate>Sat, 28 Nov 2020 18:08:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Opinion: RMS Does Not See the Future of Emacs]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 7 (<a href="https://news.ycombinator.com/item?id=25239111">thread link</a>) | @ashton314
<br/>
November 28, 2020 | https://lambdaland.org/posts/2020-11-future-of-emacs/ | <a href="https://web.archive.org/web/*/https://lambdaland.org/posts/2020-11-future-of-emacs/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

      <p>
        <h3>RMS Does Not See the Future of Emacs
        </h3>
        
        </p>

    <p>I am an avid <a href="https://emacs.org/">Emacs</a> user. I’m using it right now to compose this post. I use it every single day for everything from work to school to personal notes. Most of my activity on GitHub comes from me tweaking little things in my configuration files. I now have an editor that perfectly fits my hands. Emacs is a big part of my life.</p>
<p>I’m afraid it’s dying.</p>
<p>Richard Stallman, one of the principle creators of Emacs and the head of the GNU Project, has made several choice in the past several months that I consider to be detrimental to the Emacs community and harmful for Emacs' further growth. RMS doesn’t seem to care that much about making Emacs appealing to new users, and I think this is a mistake. Emacs derives its strength from being uniquely customizable and extensible; the more people we get using Emacs, the more good extensions, packages, tutorials, etc. will be available for Emacs. Some of the growth-hostile things include:</p>
<ul>
<li>Shutting down suggestions for making Emacs start with a sensible set of defaults that would make it significantly easier for beginners to get started<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></li>
<li>Purging links to the most popular (and most useful!) Emacs package repositories, Melpa and Marmalade, just because they <em>might</em> contain links to sites with non-free Javascript<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></li>
<li>Ignoring community-driven development and exercising veto rule in cases where I personally think it was unwarranted<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup></li>
</ul>
<p>I can appreciate strong leadership; I think for creating most things, having a single leader drive the development of a product gives it focus and direction that otherwise might kill it off. (I think Python is a good example of this at work.) In this case with Emacs, however, I think RMS is badly out of touch and should focus on what we as a community can do to make Emacs more robust so that future generations of programmers will have a strong motivation to use Emacs—a desire to run free software motivates precious few people in their selection of their tools. We should make it more appealing for its features and performance as well.</p>
<p>Some areas where Emacs stands to improve are:</p>
<ul>
<li><strong>Beginner-friendliness</strong> The default Emacs theme looks awful. No computer user used to the comforts of macOS or Windows would want to go near that ugly beast. It should have a pretty-looking theme by default. One idea would be to make it so that a new user can select some pre-built themes.</li>
<li><strong>Performance</strong> There are some exciting things happening with gccemacs on this front. I’m not running that right now, as compiling Emacs master on macOS is a little persnickety. Improving its rendering engine would help too. I recognize that that is a big undertaking, and unfortunately I have little to offer in this regard.</li>
<li><strong>Ease of contribution</strong> Why not host Emacs development on a self-hosted GitLab instance? Or use some other issue tracker? I understand that there are some advantages for mailing lists, but the set of programmers who are a.) familiar with that work flow, and b.) prefer it, is dwindling. An issue/PR-style flow makes a lot more sense for most developers, and I think it would go a long way to enriching community involvement in Emacs' core development.</li>
</ul>
<p>These are just my thoughts, and will likely evolve over time. Unfortunately I cannot devote as much time as I would like to improving Emacs, though I do enjoy <a href="https://github.com/ashton314/gilded-select">learning to write packages</a> when I have the time.</p>
<p>Good luck, all you Emacs maintainers out there. You’re heroes.</p>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://lwn.net/Articles/819452/">https://lwn.net/Articles/819452/</a> <a href="#fnref:1" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p><a href="https://github.com/emacs-mirror/emacs/commit/5daa7a5fd4aced33a2ae016bde5bb37d1d95edf6">https://github.com/emacs-mirror/emacs/commit/5daa7a5fd4aced33a2ae016bde5bb37d1d95edf6</a> <a href="#fnref:2" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p><a href="http://ergoemacs.org/misc/rms_emacs_tyrant_2018-03.html">http://ergoemacs.org/misc/rms_emacs_tyrant_2018-03.html</a> <a href="#fnref:3" role="doc-backlink">↩︎</a></p>
</li>
</ol>
</section>
    </div></div>]]>
            </description>
            <link>https://lambdaland.org/posts/2020-11-future-of-emacs/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25239111</guid>
            <pubDate>Sat, 28 Nov 2020 17:56:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Artist built usable 40k Space Marine armor (video, swe)]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25239082">thread link</a>) | @m_eiman
<br/>
November 28, 2020 | https://www.svt.se/nyheter/lokalt/norrbotten/har-dundrar-han-fram-i-sin-2-7-meter-langa-warhammer-drakt | <a href="https://web.archive.org/web/*/https://www.svt.se/nyheter/lokalt/norrbotten/har-dundrar-han-fram-i-sin-2-7-meter-langa-warhammer-drakt">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><article><div><figure><div><div><div><div><div><p><img alt="" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></p></div></div></div><div><div><p>Javascript måste vara påslaget för att kunna spela video</p></div></div><div><div><p>SVT stödjer inte uppspelning i din webbläsare. Vi rekommenderar därför att du byter till en annan webbläsare.</p></div></div></div></div><figcaption><span>I klippet ser du Svjan Köppe när han intar soptippen i sin massiva Warhammer-dräkt. <span>Foto: <!-- -->Marcel Köppe/Privat</span></span></figcaption></figure></div><p><section><span title="28 november 2020 kl 06:49"><span>Publicerad</span> <time datetime="2020-11-28T06:49:43+01:00">28 november 2020</time></span></section></p><div><p>Han har byggt en monsterdrake och en pansarbjörn tidigare. Nu har Svjan Köppe i Älvsbyn slagit till med en 2,7 meter lång Warhammerkrigare.</p><p>– Jag skapade den på golvet i lägenheten, säger han.</p></div><div><div><p>Efter den uppmärksammade Game of Thrones-draken och den 150 kg tunga björnen väljer Svjan Köppe att satsa mer på höjd och rörlighet. Den nya skapelsen är en ”Space Marine” från spelet Warhammer 40&nbsp;000.</p><p>Det tog fyra månader att bygga jätten som mäter 2,7 meter och kan röra sig framåt. Verkstaden? Hemma på golvet i lägenheten och det är ingen lättviktare.</p><p>– Bara svärdet är väldigt tungt och maffigt att hålla i. Dessutom ska det bara hållas med en hand som figuren gör, säger han.</p><p><em>I klippet ser du Svjan iklädd Warhammer-dräkten och hör om hans framtida projekt</em></p><figure><div data-lpid="29247098"><div><p><img alt="" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></p></div></div><figcaption><span>Svjan Köppe, Älvsbyn, har byggt en GoT-drake och Pansarbjörn tidigare. Nu har han färdigställt en Warhammer-figur. <span>Foto: <!-- -->Jimmy Bäckström/SVT</span></span></figcaption></figure></div><section></section></div><section><p><span>SVT:s nyheter ska stå för saklighet och opartiskhet. Det vi publicerar ska vara sant och relevant. Vid akuta nyhetslägen kan det vara svårt att få alla fakta bekräftade, då ska vi berätta vad vi vet – och inte vet. </span><a href="https://www.svt.se/nyheter/sa-arbetar-vi-pa-svt-nyheter"><span>Läs mer</span></a></p></section></article></div></div>]]>
            </description>
            <link>https://www.svt.se/nyheter/lokalt/norrbotten/har-dundrar-han-fram-i-sin-2-7-meter-langa-warhammer-drakt</link>
            <guid isPermaLink="false">hacker-news-small-sites-25239082</guid>
            <pubDate>Sat, 28 Nov 2020 17:52:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Crispr- A Crack in the Creation book reading notes]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25238740">thread link</a>) | @orsenthil
<br/>
November 28, 2020 | http://xtoinfinity.com/posts/2020/11/27/a-crack-in-creation-by-jennifer-doudna-and-samuel-steinberg.html | <a href="https://web.archive.org/web/*/http://xtoinfinity.com/posts/2020/11/27/a-crack-in-creation-by-jennifer-doudna-and-samuel-steinberg.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody text">
    <div>
<p><strong>A Crack in Creation: Gene Editing and Unthinkable power to control evolution</strong>
by Jennifer Doudna and Samuel Steinberg is a book on gene-editing and a
technology called CRISPR.</p>
<p>The book is a personal narration of Jennifer Doudna as she explains the
development of CRISPR and it's discovery for use in gene editing.  Rather than a
review, this are notes while reading this book.  CRISPR is molecular structure
found in Bacteria, but now more popular term, commonly associated with a gene
editing technique.</p>
<p><em>Given the technical nature of this article, I must have used the text from the
sources only with slight modification for explanation. References should give
the materials I consulted to write this post. In you notice technical
inaccuracy, I aplogize, please point out, and I will correct it.</em></p>
<p><strong>Terms</strong></p>
<p>As I reader, I found reviewing biological terms helped me understand the
material better.</p>
<div>
<p><img alt="https://dl.dropbox.com/s/3cn83qir2ip3nso/Screenshot_2020-11-28-04-50.png" src="https://dl.dropbox.com/s/3cn83qir2ip3nso/Screenshot_2020-11-28-04-50.png"></p><p>DNA, the language of life. Figure from <em>A Crack in Creation</em> book.</p>
</div>
<p>Human Body is made of <strong>cells</strong>, in-fact trillions of cells. Each of these cells
contain something called DNA. <strong>DNA</strong> is like recipe, just like a food recipe,
but for building and maintaining living organisms.</p>
<p>Cells use DNA to make proteins. Proteins are the workhorses of the body, they do
all the stuff we need to do to survive, from digesting food to making other
proteins.  Proteins are molecules made up of cells.</p>
<p>DNA is made up of a long combination of some very basic organic components
called Adenine, Thymine, Guanine and Cytosine. Human DNA consists of about 3
billion of these. The sequence of these determines the information available for
building and maintaining an organism, similar to the way in which letters of the
alphabet appear in a certain order to form words and sentences.</p>
<p>In the nucleus of each cell, the DNA molecule is packaged into thread-like
structure called <strong>chromosomes</strong>. A <em>Chromosome</em> is a DNA containing structure.</p>
<p>RNA are like cousins of DNA, which has an oxygen atom with it. One type called
messager RNA, mRNA, act as carrier of information to different cells, carrying
information from DNA to those cells to produce proteins.</p>
<p>So far, in above definitions, we didn't emphasize on <em>heredity</em> , that is,
sending information from parent to child yet. As soon as we start talking about
<em>heredity</em>, we use the term, <strong>Genes</strong>.</p>
<p>A gene is the basic physical and functional unit of heredity. Each chromosome of
human body has many genes.</p>
<p>If we take a single cell from human body, and find out the entire set of
<em>genetic information</em> in the chromosomes of that cell, we call that a
<strong>Genome</strong>. A Genome, from <strong>Gen</strong> e and Chromos <strong>ome</strong>, is the entire set of
<em>genetic</em> instructions found inside a cell.</p>
<p><strong>CRISPR in bacteria</strong></p>
<p>Single celled organisms like Bacteria were using a technique to fight off some
diseases.  The term CRISPR was given to an identified characteristic in
Bacterial DNA sequence, which was used to produce a protein called CAS-9, which
in turn, helped to kill the enemy virus.</p>
<p>CRISPR stands for Clustered Regularly Interspaced Short Palindromic Repeats and
is a family of DNA Sequences found in genomes of bacteria. CAS9 stands for
CRISPR associated protein 9.</p>
<p>The bacteria were found to capture snippets of DNA from invading viruses and use
them to create DNA segments known as CRISPR arrays. The CRISPR arrays allow the
bacteria to "remember" the viruses. If the viruses attack again, the bacteria
produce RNA segments from the CRISPR arrays to target the viruses' DNA. The
bacteria then use Cas9 to cut the DNA apart and kill the virus.</p>
<div>
<p><img alt="https://dl.dropbox.com/s/staumxmyll7ypty/Screenshot_2020-11-27-19-41.png" src="https://dl.dropbox.com/s/staumxmyll7ypty/Screenshot_2020-11-27-19-41.png"></p><p>CRISPR in Bacteria. Figure from <em>Crack in the Creation</em>.</p>
</div>
<p><strong>CRISPR Shaping Human Genome</strong></p>
<p>The CRISPR-Cas9 system works similarly in the lab. Researchers create a small
piece of RNA with a short "guide" sequence that attaches (binds) to a specific
target sequence of DNA in a genome. The RNA also binds to the Cas9 enzyme. As in
bacteria, the modified RNA is used to recognize the DNA sequence, and the Cas9
enzyme cuts the DNA at the targeted location. Although Cas9 is the enzyme that
is used most often, other enzymes (for example Cpf1) can also be used. Once the
DNA is cut, researchers use the cell's own DNA repair machinery to add or delete
pieces of genetic material, or to make changes to the DNA by replacing an
existing segment with a customized DNA sequence</p>
<p>When CRISPR was determind that it could be used in lab on living organisms, the
potential for shaping the genome unfolded.</p>
<p>First time ever, in over 100,000 years, we have ability to shape the <em>Homo
Sapien</em> evolution by mechanisms other than random mutation and natural
selection.</p>
<p>In humans, CRISPR can be used to do a precise repair and produce a normal
protein from a non-functional gene.</p>
<p><img alt="https://dl.dropbox.com/s/zhew8671tk9dnx6/Screenshot_2020-11-28-04-04.png" src="https://dl.dropbox.com/s/zhew8671tk9dnx6/Screenshot_2020-11-28-04-04.png"></p><p>CRISPR enables scientists to edit and <em>fix</em> single incorrect letters of DNA from
3.2 billion letters that comprise the human genome. It can also be used to
perform even more complicated edits to Human DNA.</p>
<p>A relatively straightforward DNA editing has transformed every genetic disease,
at-least the diseases for which we know the underlying mutation(s) into a
potentially treatable disease.</p>
<p><strong>CRISPR on Animals</strong></p>
<p>CRISPR has been used to create gene edited mouse wherein the genome of the
embroyo was edited and introduced back into womb to have an offspring with
the desirable characteristics embedded at time of birth.</p>
<div>
<p><img alt="https://dl.dropbox.com/s/l0gxu7imj7v3pqa/Screenshot_2020-11-28-03-51.png" src="https://dl.dropbox.com/s/l0gxu7imj7v3pqa/Screenshot_2020-11-28-03-51.png"></p><p>Gene Edited Mouse. Figure from <em>A Crack in Creation</em>.</p>
</div>
<p>And we have used gene editing to create animals desirable characteristics</p>
<div>
<p><img alt="https://dl.dropbox.com/s/7hdtn904xxas57q/Screenshot_2020-11-28-04-01.png" src="https://dl.dropbox.com/s/7hdtn904xxas57q/Screenshot_2020-11-28-04-01.png"></p><p>Gene edited animals. Figure from <em>A crack in creation</em>.</p>
</div>
<p>This is currently used in practice. Like Recombinetics uses gene editing for
dehorning cattle, a safer method than physical dehorning using hot iron-rods.</p>
<p><a href="https://recombinetics.com/our-technology/"><img alt="https://dl.dropbox.com/s/r5op4mkkhju3s8p/Screenshot_2020-11-28-07-34.png" src="https://dl.dropbox.com/s/r5op4mkkhju3s8p/Screenshot_2020-11-28-07-34.png"></a></p><hr>
<p><strong>Pigs as Bio Reactors</strong></p>
<p>An important field of bio technology is regenerative medicine, desired by human
society who are fighting of some disease eithe naturally or have lost some
ability due an accident.</p>
<p>Many scientists see the pig itself as a source of medicine. It is seen
that we might be using pigs as bioreactors to produce valuable drugs like
therapeutic human proteins, which are too complex to synthesize from scratch and
can only be produced in living cells.</p>
<p><img alt="https://dl.dropbox.com/s/lw2lleanrept8qf/Screenshot_2020-11-28-04-07.png" src="https://dl.dropbox.com/s/lw2lleanrept8qf/Screenshot_2020-11-28-04-07.png"></p><p>Scientists have already been looking to
other transgenic animals to produce these biopharmaceutical drugs, or
farmaceuticals, as they’re colloquially called.</p>
<p><a href="https://www.revivicor.com/index.html">Revivicor</a> is a company that is using CRISPR to produce regenerative medicine,
following the process exactly outlined above. A workflow from their website
gives the details on how Pigs are used as Bio Reactors for regenerative
medicine.</p>
<p><a href="https://www.revivicor.com/"><img alt="https://www.revivicor.com/images/RevivicorTechPoster-04-2010.jpg" src="https://www.revivicor.com/images/RevivicorTechPoster-04-2010.jpg"></a></p><hr>
<p><strong>Malaria Resistant Mosquitos</strong></p>
<p>The deadliest animal on earth, Mosquito can also be killed using CRISPR. The
idea seems to create malaria resistant mosquitoes using gene editing so that
the entire family is disabled from being a carriers of malaria.</p>
<p><img alt="https://dl.dropbox.com/s/qwejig01w6zcyox/Screenshot_2020-11-28-04-23.png" src="https://dl.dropbox.com/s/qwejig01w6zcyox/Screenshot_2020-11-28-04-23.png"></p><hr>
<p><strong>CRISPR for Therapeutics</strong></p>
<p>CRISPR can be utilized to edit the germ cells outside the body.
The edited germ cells can be planted inside for beneficiary aspects.</p>
<div>
<p><img alt="https://dl.dropbox.com/s/8ekvzjzwupi63t7/Screenshot_2020-11-28-04-27.png" src="https://dl.dropbox.com/s/8ekvzjzwupi63t7/Screenshot_2020-11-28-04-27.png"></p><p>Ex-vivo CRISPR therapy. A Crack In The Creation.</p>
</div>
<p>For targeted drug delivery, like fixing the lung or particular muscle instead
of injecting the drug into blood stream.</p>
<div>
<p><img alt="https://dl.dropbox.com/s/7zaa0afr6gdipha/Screenshot_2020-11-28-04-31.png" src="https://dl.dropbox.com/s/7zaa0afr6gdipha/Screenshot_2020-11-28-04-31.png"></p><p>In-vivo CRISPR therapy. A Crack In the Creation.</p>
</div>
<p>Adult Homo sapiens are among the last animals to be treated with CRISPR, human
cell: have been subjected to more CRISPR gene editing than those of any other
organism.</p>
<p>Scientists have applied CRISPR in lung cells to correct the genetic mutation
that causes cystic fibrosis, in blood cells to correct the mutations that cause
sickle cell disease and beta-thalassemia, and in muscle cells to correct the
mutations that cause Duchenne muscular dystrophy.</p>
<p>Scientists have used CRISPR to edit and repair mutations in stem cells, which
can then be coaxed to transform into virtually any cell or tissue type in the
body.</p>
<p>Even as CRISPR continues to be useful, it's power as a technology and it's
potential misuse is a concern for everyone.</p>
<blockquote>
Whether we'll ever have the intellectual and moral capacity to
guide our own genetic destiny is an open question - one that has been in my
mind since I began to realize what CRISPR is capable of.
- Jennifer Doudna</blockquote>
<p>And Jennifer Doudna shares her stance as she says, that the nature will still be
our supreme master.</p>
<blockquote>
Any mutations that CRISPR might make—intentional or not—would almost certainly
pale in comparison to the genetic storm that rages inside each of us from
birth to death. As one writer put it, “Genetic editing would be a droplet in
the maelstrom of naturally churning genomes.” If CRISPR could eliminate a
disease-causing mutation in the embryo with high certainty and only a slight
risk of introducing a second off-target mutation elsewhere, the potential
payoffs might well outweigh the dangers.
- Jennifer Doudna</blockquote>

</div>
    </div></div>]]>
            </description>
            <link>http://xtoinfinity.com/posts/2020/11/27/a-crack-in-creation-by-jennifer-doudna-and-samuel-steinberg.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25238740</guid>
            <pubDate>Sat, 28 Nov 2020 17:06:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Apple Silicon M1: A Developer's Perspective]]>
            </title>
            <description>
<![CDATA[
Score 361 | Comments 438 (<a href="https://news.ycombinator.com/item?id=25238608">thread link</a>) | @steipete
<br/>
November 28, 2020 | https://steipete.com/posts/apple-silicon-m1-a-developer-perspective/ | <a href="https://web.archive.org/web/*/https://steipete.com/posts/apple-silicon-m1-a-developer-perspective/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div> <p><img src="https://d33wubrfki0l68.cloudfront.net/7dbf04694dfeeec9985b2ebb27cc1faff153111b/75e93/assets/img/2020/m1/m1.jpg"></p><p>The excitement around Apple’s new M1 chip is <a href="https://www.singhkays.com/blog/apple-silicon-m1-black-magic/">everywhere</a>. I bought a MacBook Air 16GB M1 to see how viable it is as main development machine - here’s an early report after a week of testing.</p><h2 id="xcode">Xcode</h2><p>Xcode runs FAST on the M1. Compiling the <a href="https://pspdfkit.com/">PSPDFKit PDF SDK</a> (debug, arm64) can almost compete with the fastest Intel-based MacBook Pro Apple offers to date, with <a href="https://twitter.com/steipete/status/1332052251712614405?s=21">8:49 min vs 7:31 min</a>. For comparison, my Hackintosh builds the same in less than 5 minutes.</p><p>One can’t overstate how impressive this is for a fan-less machine. Apple’s last experiment with fan-less MacBooks was the 12-inch version from 2017, which builds the same project in 41 minutes.</p><p>Our tests mostly ran just fine, although I found <a href="https://github.com/Aloshi/dukglue/pull/27">a bug specific to arm64</a> that we missed before, as we don’t run our tests on actual hardware on CI. Moving the Simulator to the same architecture as shipping devices will be beneficial and will help find more bugs.</p><p>Testing iOS below 14 is problematic. It seems <a href="https://twitter.com/steipete/status/1332654247809257473?s=21">WebKit is crashing in a memory allocator</a>, throwing EXC_BAD_INSTRUCTION (code=EXC_I386_INVOP, subcode=0x0) (Apple folks: FB8920323). Performance also seems really bad, with Xcode periodically <a href="https://twitter.com/steipete/status/1332348616145563653?s=21">freezing</a> and the whole system becoming so <a href="https://twitter.com/steipete/status/1332648748158246922?s=21">slow</a> that the mouse cursor gets choppy. Some Simulators even make problems on iOS 14, <a href="https://twitter.com/steipete/status/1331628274783543297?s=21">such as the iPad Air (4th gen) which still emulates Intel</a>, so try to avoid that one.</p><p>We were extremely excited to be moving our CI to Mac Mini’s with M1 chip and are <a href="https://www.macstadium.com/m1-mini">waiting on MacStadium to release devices</a>, however it seems we will have to restrict tests to iOS 14 for that to work. With our current schedule, we plan to drop iOS 12 in Q3 2021 and iOS 13 in Q3 2022, so it will be a while until we can fully move to Apple Silicon.</p><p>There is a chance that Apple fixes these issues, however it’s not something to count on - given that this only affects older versions of iOS, the problem will at some point just “go away”.</p><p><strong>Update:</strong> We’re working around the WebKit crashes for now via detecting Rosetta2 translation at runtime and simply skipping the tests where WebKit is used. This isn’t great, but luckily we’re not using WebKit a lot in our current project. <a href="https://gist.github.com/steipete/e15b1fabffc7da7d49c92e3fbd06971a">See my Gist for details</a>. Performance seems acceptable if you restrict parallel testing to at most two instances - else the system simply runs out of RAM and swapping is just really slow.</p><h2 id="docker">Docker</h2><p>We use Docker to automate our Website and load environments for our <a href="https://pspdfkit.com/pdf-sdk/web/">Web and Server PDF SDKs</a>. Docker posted a <a href="https://www.docker.com/blog/apple-silicon-m1-chips-and-docker/">status update blog post</a> about the current state of things, admitting that it currently won’t work but that they’re <a href="https://github.com/docker/roadmap/issues/142">working on it</a>. There are more <a href="https://finestructure.co/blog/2020/11/27/running-docker-on-apple-silicon-m1-follow-up">hacky ways to use Apple’s Hypervisor to run Docker container manually</a>, however this needs arm-based containers.</p><p>I expect a solution in Q1 2021 that runs arm-based containers. We’ll have to do some work to add arm-support (something already on the roadmap) so this is only a transitional issue.</p><h2 id="virtualization-and-windows">Virtualization and Windows</h2><p>To test our <a href="https://pspdfkit.com/pdf-sdk/windows/">Windows PDF SDK</a>, most folks are using a VMware virtual machine with Windows 10 and Visual Studio. Currently none of the Mac virtualisation solutions support Apple Silicon, however both <a href="https://appleinsider.com/articles/20/11/11/parallels-confirms-apple-m1-support-amid-silence-from-other-virtualization-companies">VMware and Parallels</a> are working on it. I do not expect Virtualbox to be updated <a href="https://forums.virtualbox.org/viewtopic.php?f=8&amp;t=98742">anytime soon</a>.</p><p>I expect that eventually we’ll be able to run ARM-based Windows with commercial tooling. Various <a href="https://9to5mac.com/2020/11/27/arm-windows-virtualization-m1-mac/">proof-of-concepts</a> already exist, and performance seems <a href="https://twitter.com/imbushuo/status/1332772957609922561?s=21">extremely promising</a>. Microsoft currently doesn’t sell ARM-based Windows, so getting a license will be interesting.</p><p>ARM-Windows can emulate x86 applications, and Microsoft is working on <a href="https://www.neowin.net/news/it039s-official-x64-emulation-is-coming-to-windows-on-arm">x64 emulation</a>, which is already rolling out in Insider builds. In a few months, it should be possible to develop and test our Windows SDK with Visual Studio on M1 in reasonable performance.</p><p>Running older versions of macOS might be more problematic. We currently support macOS 10.14 with our <a href="https://pspdfkit.com/blog/2017/pspdfkit-for-macos/">AppKit PDF SDK</a> and macOS 10.15 with the <a href="https://pspdfkit.com/blog/2019/pspdfkit-for-mac-catalyst/">Catalyst PDF SDK</a>, both OS releases that require testing. It remains to be seen if VMWare or Parallels include a complete x64 emulation layer. This would likely be really slow, so I wouldn’t count on it.</p><p><img src="https://d33wubrfki0l68.cloudfront.net/bbf100981357e4bc26c49722d31253daf76d293d/5b456/assets/img/2020/m1/memory.png" alt=""></p><p>Lastly, 16 GB RAM just isn’t a lot. When running parallel tests, the machine starts to heavily swap and performance really goes down the drain. This will be even more problematic with virtual machines running. Future machines will offer 32 GB options to alleviate this issue.</p><p><strong>Update:</strong> <a href="https://gist.github.com/niw/e4313b9c14e968764a52375da41b4278#file-readme-md">How to run Windows 10 on ARM in Qemu with Hypervisor.framework patches on Apple Silicon Mac</a></p><h2 id="android-studio">Android Studio</h2><p>IntelliJ is working on porting the <a href="https://youtrack.jetbrains.com/issue/JBR-2526">JetBrains Runtime</a> to Apple Silicon. The apps currently work through Rosetta 2, however building via Gradle is <a href="https://www.reddit.com/r/androiddev/comments/jx4ntt/apple_macbook_air_m1_is_very_slow_in_gradle_builds/">extremely slow</a>. Gradle creates code at runtime, which seems a particular bad combination with the Rosetta 2 ahead-of-time translation logic.</p><p>I expect that most issues will be solved by Q1 2021, however it will likely be some more time until all Java versions run great on ARM. A lot of effort has been put into <a href="https://bell-sw.com/java/arm/performance/2019/01/15/the-status-of-java-on-arm/">loop unrolling and vectorisation</a>, not everything there is available on ARM just yet.</p><p><strong>Update:</strong> <a href="https://www.azul.com/press_release/azul-announces-support-of-java-builds-of-openjdk-for-apple-silicon/">Azul offers macOS JDKs for arm64</a>, including for <a href="https://www.azul.com/downloads/zulu-community/?os=macos&amp;architecture=arm-64-bit&amp;package=jdk">Java 8</a>.</p><h2 id="homebrew">Homebrew</h2><p><a href="https://brew.sh/">Homebrew</a> currently works via Rosetta 2. Just prefix everything with <code>arch -x86_64</code> and it’ll just work. It is possible to install an additional (arm-based) version of Homebrew <a href="https://soffes.blog/homebrew-on-apple-silicon">under <code>/opt/homebrew</code></a> and mix setup, as <a href="https://github.com/Homebrew/brew/issues/7857">more and more software</a> is adding support for arm.</p><p>This is not a problem currently (performance is good) and will eventually just work natively.</p><h2 id="applications">Applications</h2><p>Most applications just work, Rosetta is barely noticeable. Larger apps to take a longer initial performance hit (e.g. Microsoft Word takes <a href="https://www.zdnet.com/article/microsoft-office-will-be-about-20-second-slower-initially-on-apple-silicon-rosetta-2/">around 20 seconds</a> until everything is translated), but then these binaries are cached and subsequent runs are fast.</p><p>There’s the occasional app that can’t be translated and fails on startup (e.g. <a href="https://beamer-app.com/download">Beamer</a> or the <a href="https://www.google.com/intl/en_gh/drive/download/">Google Drive “Backup and Sync” client</a>), but this is rare. Some apps are confused about their place on disk and ask to be moved to the Applications directory, when really it’s just the translated binary that runs somewhere else. Most of these dialogs can be ignored. Some apps (e.g. Visual Studio Code) <a href="https://twitter.com/steipete/status/1331884524934995968?s=21">block auto-updating</a> as the translated app location is readonly. However, in case of VS Code, the Insider build is already updated to ARM and just works.</p><p>Electron-based apps are slow if they run on Rosetta. It seems the highly optimized V8 JavaScript compiler blocks ahead-of-time translation. The latest stable version of Electron (Version 11) already <a href="https://www.electronjs.org/blog/apple-silicon">fully supports Apple Silicon</a>, and companies like Slack already updated their beta version to run natively.</p><p>Google just shipped <a href="https://www.macworld.com/article/3597749/google-releases-chrome-87-with-support-for-apple-silicon-macs.html">Chrome that runs on ARM</a>, however there’s still quite a performance gap between it and Apple Safari, which just <em>flies</em> on Apple Silicon.</p><h2 id="conclusion">Conclusion</h2><p>The new M1 MacBooks are fast, beautiful and silent and the hype is absolutely justified. There’s still a lot to do on the software-front to catch up, and the bugs around older iOS Simulators are especially problematic.</p><p>All of that can be fixed in software and the whole industry is currently working on making the experience better, so by next year, when Apple updates the 16-inch MacBook Pro and releases the next generation of their M chip line, it should be absolutely possible to use a M1 Mac as main dev machine.</p><p>For the time being, the M1 will be my <del>travel</del> secondary laptop, and I’ll keep working on the 2,4 GHz 16-inch MacBook Pro with 32 GB RAM, which just is the faster machine. I’ll be much harder to accept the loud, always-on fans though, now that I know what soon will be possible.</p></div></div>]]>
            </description>
            <link>https://steipete.com/posts/apple-silicon-m1-a-developer-perspective/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25238608</guid>
            <pubDate>Sat, 28 Nov 2020 16:47:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[No Config for Old Men]]>
            </title>
            <description>
<![CDATA[
Score 261 | Comments 215 (<a href="https://news.ycombinator.com/item?id=25238523">thread link</a>) | @zdw
<br/>
November 28, 2020 | https://datagubbe.se/noconf/ | <a href="https://web.archive.org/web/*/https://datagubbe.se/noconf/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>


<p><b>...or anyone else, for that matter.</b></p>

<p><i>Autumn 2020</i></p>

<p>Ask anyone who's really into cooking and they'll tell you how important it is to have a kitchen that's arranged just right. To someone who rarely cooks, a kitchen is probably just a place to store a few pots and a toaster, and the placement of such stuff doesn't matter much: canned soup and microwave dinners are designed for ease of preparation.</p>


<p>For the enthusiastic home cook, however, a lot of things can go wrong. Often there's only a second's notice to fix something that might ruin a perfect meal or set you back hours of hard toil. Hollandaise starting to split? Better bring out that ice cold water post haste! Garlic burning? Better chop some more right this instant, or the pasta will be overcooked and there goes the Aglio e Oglio.</p>


<p>To achieve speed, you have to know where your knives, pots, pans, spoons, whisks and other utensils are and you want to be able to arrange them so that they're easy to reach. Seasonings, spices, herbs and condiments should be within reach from the stove. Then there's the mise en place before the actual cooking begins: chopping all the vegetables, cubing the meat, slicing the bacon and so on.</p>


<p>All of this is, thankfully, easy to achieve. Even the most cramped kitchen will, after having been battle tested through a few meals, be as optimal as possible according to the cook's personal preference. The same goes for painters, carpenters, car mechanics and even office dwellers. Hammer missing from the hammer hook? For shame! Stapler not to the immediate right of the stack of post-its? Well it should be!</p>


<h3>The regulated kitchen</h3>


<p>Now imagine if you couldn't organize your kitchen to your heart's content. Not because you're lacking the funds or skills, but because some federally appointed clerk is constantly coming to inspect it.</p>


<p>"Nah," he says, adjusting his clip-on tie, "you can't put your spices there. Against regulations. All spices must be kept more than two yards from the stove at all times." He then goes on to explain that you'll have to call him every time you want to use the stove, just to make sure the pots you're using are compliance tested. Plus, they have to be stored in the government approved pot storage cabinet below the sink, otherwise they're not fit for kitchen use.</p>


<p>Want to change the color of the counter top? Sorry, no can do. Want to switch from glass bowls to stainless? Alas, you used to be able a few years ago, but nobody wants stainless anymore anyway, right? The salt <i>can</i> be put next to the stove, but it's not recommended and might change in the near future. Knives are now to be honed every Tuesday afternoon by a designated craftsman. Sure, you can postpone a few times, but eventually, you just have to live with that. Plus, there's going to be some dudes coming to inventory your pantry on a regular basis, most likely when you're in the middle of cooking something really complicated. There's no use in protesting - this is all for your own good.</p>

<p>And yes.


</p><p>Of course this is a metaphor for computers.</p>


<h3>The curious disappearance of configuration</h3>


<p>For the casual user, some of this can <i>maybe</i> be convenient - or at least not annoying. If someone who rarely uses the kitchen has decided to whip up a home cooked feast, it doesn't matter that the spices are kept strangely far from the stove: they're just happy they found them. And <i>maybe</i> there is, for the casual user, some "security" in lock-in efforts such as MacOS calling home<sup><a href="#footnote1">[1]</a></sup> to check if a program is allowed to run and that web browsers automatically block certain URLs.</p>


<p>Likewise, maybe a handful of confused beginners are helped by the fact that certain system settings are extremely hard to find, or that you're supposed to put all your photos in a specific directory, or that you can't decide what partition you want to install a program on, or that some indexing service starts running when you least expect it, or that not a single application gives a crap about the few color settings you're allowed to make.</p>


<p>For the power user, such things range from nuisances to something that seriously hampers productivity and creativity.</p>


<p>It used to be that whenever I got a new computer, I spent a day or two setting it up. I selected the fonts I wanted to use, I picked the colors I liked for window decorations and GUI elements, I installed my preferred tools and utilities and I organized the desktop icons and program launchers to my liking. It took a bit of time, but it was a labor of love. In times of trouble I was, if nothing else, at least the boss of my own desktop environment.</p>


<p>I don't know of any proprietary OS where I can do that anymore. Linux is, considering what's going on with the major distributions, desktop environments and UI toolkits, seemingly heading the same way. Sure, pick your own window manager, see if we care - we've got client side decorations! Want to theme your GUI? Yeah, but not in our Snap packages you won't! Want to turn off cursor blinking? Mmmmyyeeaahhh, not too sure about that. Oh, you started a GUI file manager? Hey, enjoy the ten new folders we've littered your home directory with! They all start with capital letters: designed for typing convenience in a case sensitive file system.</p>


<p>Of course I still spend a fair amount of time setting up a Windows machine, but these days it's not the joyful experience of configuring the best fonts and nicest colors and arranging the icons on the start menu in the correct order for my muscle memory. Instead it's usually a week of swearing over removed settings and working hard to find the ones that actually remain, or trying various registry hacks to circumvent seemingly unchangeable defaults. I'm working against the system instead of with it, and someone else is trying to boss me around.</p>


<h3>A better example</h3>


<p>All of this could be different. It used to be. On my <a href="https://datagubbe.se/ltmag/">Amiga</a>, I could configure <i>everything</i>. Apart from things like fonts and colors I could draw my own mouse pointer, tiling desktop backgrounds and icons. (Yes, the system really shipped with separate little paint programs just for pointers, desktop tiles and icons.) I could customize double click speed and key repeat rates on millisecond levels. I could even control the exact position of individual icons and the size and position of every individual directory window opened.</p>


<p>Most casual users didn't care about all that, but they didn't have to. The system came with a reasonable set of defaults and when or if they grew more proficient and wanted to change something about their daily working environment, they had the option to do so.</p>


<p>This was a great approach to users. Instead of being treated like an incompetent moron and placed in a walled garden, you were entrusted and empowered. Something as simple as drawing my own mouse pointer on the Amiga was a profound and formative experience for me. As corny as it sounds, it was as if the guys who built this amazing machine put it in my hands and said, "Hey kid, you're in charge. This computer is yours. Learn how to use it and you can make it do anything." It was a call for exploration and creativity.</p>


<p>Today, I can't even change the system font in Windows. I can select an "accent color", but most applications completely ignore it. Every program defaults to downloading into a Downloads folder and I've lost count of how many times I've changed its folder view from grouped to not grouped, only to discover it's been magically changed back the next time I browse it. Lots of settings have been removed completely while others are buried deep in strange places where a user clearly isn't really supposed to venture.</p>


<p>The problem is that there's no toggle for enabling "advanced mode". I'm just supposed to accept that I can no longer change simple things I've been able to configure for the past thirty years. Someone, somewhere just decided that all users have the same basic skill level and that the defaults are always acceptable.</p>


<h3>General purpose Instagram cameras</h3>


<p>I suppose the lack of configurability is a metaphor for personal computing in general: we're not buying our machines, we're renting them by way of bizarrely complex EULA:s for everything from the firmware to the OS and we're not supposed to be curious or creative, we're supposed to sit back and passively consume advertisements. The base level of creative computer use is no longer exploring programming or graphics or music, but photographing a meal someone else has prepared and then applying a predefined sepia filter to said photo. The base level of configuring a system is no longer picking some personal favorites among fonts and colors, but - maybe - selecting between dark and light mode.</p>


<p>
On the flip side, more and more people also need to use computers 
for actually producing stuff - not least programming all those ad 
delivery platforms and the curiously unconfigurable operating systems 
they run on. But there are also armies of innocents; office workers,
administrators, hotel clerks, librarians, teachers - those people now often 
have no choice but to strain their eyes staring at black text on bright 
white backgrounds, unable to select a font they find easier to read.
</p>

<p>Too bad they can no longer store their spices close to the stove, but hey, who cares? Let them take one for the team. We're all swimming in ad revenue and if people learn to configure things, maybe they'll suddenly realize that the presence of those ads should be configurable as well.</p>

<br>

<hr>



<p id="footnote1">
<sup>1</sup> In case my sarcasm isn't coming through here: No, it's not secure. The privacy and security problems this entails are huge.
</p>

</div></div>]]>
            </description>
            <link>https://datagubbe.se/noconf/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25238523</guid>
            <pubDate>Sat, 28 Nov 2020 16:35:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Parsing All of Wikipedia to an Offline Encyclopedia]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 4 (<a href="https://news.ycombinator.com/item?id=25238316">thread link</a>) | @kxrm
<br/>
November 28, 2020 | https://daveshap.github.io/DavidShapiroBlog/automation/2020/11/15/parsing-all-wikipedia.html | <a href="https://web.archive.org/web/*/https://daveshap.github.io/DavidShapiroBlog/automation/2020/11/15/parsing-all-wikipedia.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="container">
<div>

<header>
 
  <h2>I want an offline encyclopedia and am extremely masochistic</h2>
   <h3>15 November 2020</h3>
    <a href="https://daveshap.github.io/DavidShapiroBlog">Home</a>&nbsp;—&nbsp;
    <a href="https://daveshap.github.io/DavidShapiroBlog/categories.html">Categories</a>
</header>

<hr>

<section id="main_content">


<p>I’m working on a project where I want to have an offline encyclopedia. It’s for training deep learning networks so it needs to be in plain text English. 
No markup, no special characters. Human readable without any interpreters, renderers, or parsers. I’ve got plenty of disk space, so that’s not a concern. 
Once I parse out all the Wikipedia articles I can get my hands on, I will create an index. Or I might index them into SOLR or something like that. Not sure yet.
I’m also going to implement it as a resource for a massive <a href="https://towardsdatascience.com/building-a-question-answering-system-part-1-9388aadff507">SQUAD repo</a>. I have uploaded a copy of the final result to <a href="https://www.kaggle.com/ltcmdrdata/plain-text-wikipedia-202011">Kaggle Datasets</a>. I’ve stashed the script in a <a href="https://github.com/daveshap/PlainTextWikipedia">dedicated repo on GitHub</a>.</p>



<p>The first thing I should have learned is that Wikipedia is written in a demented Frankenstein language called WikeMedia Text. It’s a hybrid of HTML/XML and Markdown. 
It has no consistency and is the worst example of spaghetti code I’ve ever seen. I’m sure there are better implementations today, but I can see how and why it ended up the way it did.
For instance, you need to be able to create robust references and links, so the URL syntax is way jacked. It relies on a lot of procedural generation at display time.
Personally, if it were done again today, I think something like Jekyll would be way better. Instead of rendering again and again every time someone visits a page, render it once after each edit.
But that’s just me. So instead we’re left with this horrible hybrid language that should die in a fire.</p>

<p>Fine, it is what it is. I’m an expert automator, dammit, and if a machine can automatically render this nonsense, then I sure as hell can <strong>unrender it</strong>.</p>

<h2 id="attempt-1---brute-force-regex">Attempt 1 - Brute Force Regex</h2>

<p>“Brute Force Regex” (BFR) is not a real thing. It’s just something I’ve been doing for years now in my automation habits. Usually, as a naive approach, I’ll try and do some 
search-and-replace jiggery pokery to just remove unwanted junk. Sometimes this is textual formatting, like brackets around tables or other HTML tags. 
So I ended up with the following function. Caution, it’s not pretty. This was just an experiment, and I wanted to share it so you would see what doesn’t work.</p>

<div><div><pre><code><span>def</span> <span>basic_replacements</span><span>(</span><span>text</span><span>):</span>
    <span>replacements</span> <span>=</span> <span>[</span>
    <span>(</span><span>'&amp;lt;'</span><span>,</span><span>'&lt;'</span><span>),</span>
    <span>(</span><span>'&amp;gt;'</span><span>,</span><span>'&gt;'</span><span>),</span>
    <span>(</span><span>'&amp;quot;'</span><span>,</span><span>'"'</span><span>),</span>
    <span>(</span><span>"'''"</span><span>,</span><span>' = '</span><span>),</span>
    <span>(</span><span>"'{2,}"</span><span>,</span><span>''</span><span>),</span>
    <span>(</span><span>'</span><span>\n</span><span>'</span><span>,</span><span>' '</span><span>),</span>
    <span>(</span><span>r'\n'</span><span>,</span><span>' '</span><span>),</span>
    <span>(</span><span>'</span><span>\\</span><span>n'</span><span>,</span><span>' '</span><span>),</span>
    <span>(</span><span>'r</span><span>\\</span><span>n'</span><span>,</span><span>' '</span><span>),</span>
    <span>(</span><span>'&lt;ref.*?&gt;'</span><span>,</span><span>''</span><span>),</span>
    <span>(</span><span>'&lt;/ref&gt;'</span><span>,</span><span>''</span><span>),</span>
    <span>(</span><span>'http.*?\s'</span><span>,</span><span>''</span><span>),</span>
    <span>(</span><span>'\s+'</span><span>,</span><span>' '</span><span>),</span>
    <span>]</span>
    <span>text</span> <span>=</span> <span>text</span><span>.</span><span>replace</span><span>(</span><span>'</span><span>\\</span><span>n'</span><span>,</span><span>' '</span><span>)</span>
    <span>for</span> <span>r</span> <span>in</span> <span>replacements</span><span>:</span>
        <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>[</span><span>0</span><span>],</span> <span>r</span><span>[</span><span>1</span><span>],</span> <span>text</span><span>)</span>
    <span>return</span> <span>text</span>
</code></pre></div></div>

<p>I came up with this scheme because, at first glance, WikiMedia Text looked like a mixture of some basic HTML and some Markdown. 
I figured I could handle it with some basic regex replacements. This worked… to an extent. There were a few problems with it though.</p>

<ol>
  <li>Couldn’t handle nested square brackets or curly brackets, and it turns out there are a lot of those</li>
  <li>Quickly became intractable when I encountered escaped unicode literals like <code>\u2013</code>. They are frigging everywhere.</li>
</ol>

<p>So I wrote two more functions to try and tackle the bracketed stuff. These are things like links, citations, and pictures. Since I want a text-only Wikipedia, 
I really just needed to strip it all away.</p>

<div><div><pre><code><span>def</span> <span>remove_double_curly</span><span>(</span><span>text</span><span>):</span>
    <span>while</span> <span>True</span><span>:</span>
        <span>before</span> <span>=</span> <span>len</span><span>(</span><span>text</span><span>)</span>
        <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>''</span><span>,</span> <span>''</span><span>,</span> <span>text</span><span>)</span> 
        <span>after</span> <span>=</span> <span>len</span><span>(</span><span>text</span><span>)</span>
        <span>if</span> <span>before</span> <span>==</span> <span>after</span><span>:</span>
            <span>return</span> <span>text</span>


<span>def</span> <span>remove_double_brackets</span><span>(</span><span>text</span><span>):</span>
    <span>while</span> <span>True</span><span>:</span>
        <span>before</span> <span>=</span> <span>len</span><span>(</span><span>text</span><span>)</span>
        <span>double_brackets</span> <span>=</span> <span>re</span><span>.</span><span>findall</span><span>(</span><span>'\[\[.*?\]\]'</span><span>,</span> <span>text</span><span>)</span>
        <span>for</span> <span>db</span> <span>in</span> <span>double_brackets</span><span>:</span>
            <span>if</span> <span>'|'</span> <span>in</span> <span>db</span><span>:</span>
                <span>new</span> <span>=</span> <span>db</span><span>.</span><span>split</span><span>(</span><span>'|'</span><span>)[</span><span>-</span><span>1</span><span>].</span><span>strip</span><span>(</span><span>']'</span><span>)</span>
                <span>text</span> <span>=</span> <span>text</span><span>.</span><span>replace</span><span>(</span><span>db</span><span>,</span> <span>new</span><span>)</span>
            <span>else</span><span>:</span>
                <span>new</span> <span>=</span> <span>db</span><span>.</span><span>strip</span><span>(</span><span>'['</span><span>).</span><span>strip</span><span>(</span><span>']'</span><span>)</span>
                <span>text</span> <span>=</span> <span>text</span><span>.</span><span>replace</span><span>(</span><span>db</span><span>,</span> <span>new</span><span>)</span>
        <span>after</span> <span>=</span> <span>len</span><span>(</span><span>text</span><span>)</span>
        <span>if</span> <span>before</span> <span>==</span> <span>after</span><span>:</span>
            <span>return</span> <span>text</span>
</code></pre></div></div>

<p>These functions worked-ish. You might notice the carat <code>^</code> in the curly function. This told it to match anything except another open curly bracket. This forced it to find the
innermost nested curly brackets. Again, this mostly worked, but it failed a few times and I gave up trying to figure out why. The square brackets are a bit different, as
they tend not to be nested but the inner syntax could be several different things. I opted for the simplest possible way and even so, it missed a few things. No idea why.</p>

<h2 id="attempt-15---literal-evals">Attempt 1.5 - Literal Evals</h2>

<p>I suppose I should rewind and give some context. Wikipedia dump files are effing huge. Even with 32GB of RAM on my desktop, I was rapidly running out of memory just 
loading one chunk at a time. So that meant I had to read each file line by line. Like so:</p>

<div><div><pre><code><span>with</span> <span>open</span><span>(</span><span>file</span><span>,</span> <span>'r'</span><span>,</span> <span>encoding</span><span>=</span><span>'utf-8'</span><span>)</span> <span>as</span> <span>infile</span><span>:</span>
    <span>for</span> <span>line</span> <span>in</span> <span>infile</span><span>:</span>
        <span>line</span> <span>=</span> <span>literal_eval</span><span>(</span><span>f'"""</span><span>{</span><span>line</span><span>}</span><span>"""'</span><span>)</span>  <span># this works... sometimes
</span>        <span>if</span> <span>'&lt;page&gt;'</span> <span>in</span> <span>line</span><span>:</span>  <span># new article
</span>            <span>article</span> <span>=</span> <span>''</span>
        <span>elif</span> <span>'&lt;/page&gt;'</span> <span>in</span> <span>line</span><span>:</span>  <span># end of article
</span>            <span>article</span> <span>+=</span> <span>line</span>
</code></pre></div></div>

<p>This works great for just reading the thing one at a time. One consistency in the Wikipedia dumps is that every page starts and ends with <code>&lt;page&gt;</code> and <code>&lt;/page&gt;</code> respectively.
This served as a great demarcation. So I tried to handle the unicode literals as they were coming in with <a href="https://www.kite.com/python/docs/ast.literal_eval">ast.literal_eval</a>. 
Spoiler: It worked. A little bit. This function frequently bombs out for various reasons.</p>

<h2 id="attempt-2---existing-parsers">Attempt 2 - Existing Parsers</h2>

<p>I finally gave up on manually parsing WikiMedia Text and found some extant parsers. First up is <a href="https://pypi.org/project/wikitextparser/">wikitextparser</a> which, as of this writing, is actively maintained.
Second up was the simple <a href="https://pypi.org/project/html2text/">html2text</a> which got some of the stuff the first missed. These premade parsers are great in that they 
don’t require me to use any of my own brain power! They are, however, far slower than my regex replace functions. It can’t be avoided, though.</p>

<p>So now my output looks more like this:</p>

<div><div><pre><code><span>[</span><span>
 </span><span>{</span><span>
  </span><span>"id"</span><span>:</span><span> </span><span>"4413617"</span><span>,</span><span>
  </span><span>"text"</span><span>:</span><span> </span><span>"The Samajtantrik Sramik Front is a national trade union federation in Bangladesh. It is affiliated with the World Federation of Trade Unions..."</span><span>,</span><span>
  </span><span>"title"</span><span>:</span><span> </span><span>"Samajtantrik Sramik Front"</span><span>
 </span><span>},</span><span>
 </span><span>{</span><span>
  </span><span>"id"</span><span>:</span><span> </span><span>"2618"</span><span>,</span><span>
  </span><span>"text"</span><span>:</span><span> </span><span>"Aeacus (; also spelled Eacus; Ancient Greek: </span><span>\u</span><span>0391</span><span>\u</span><span>1f30</span><span>\u</span><span>03b1</span><span>\u</span><span>03ba</span><span>\u</span><span>03cc</span><span>\u</span><span>03c2 Aiakos or Aiacos) was a mythological king of the island of Aegina..."</span><span>,</span><span>
  </span><span>"title"</span><span>:</span><span> </span><span>"Aeacus"</span><span>
 </span><span>},</span><span>
 </span><span>{</span><span>
  </span><span>"id"</span><span>:</span><span> </span><span>"3201"</span><span>,</span><span>
  </span><span>"text"</span><span>:</span><span> </span><span>"[[File:Global_Temperature_And_Forces.svg|thumb|upright=1.35|right|Observed temperature from NASA. vs the 1850</span><span>\u</span><span>20131900 average used by the IPCC as a pre- industrial baseline.. The primary driver for increased global temperatures in the industrial era is human activity, with natural forces adding variability. Figure 3.1 panel 2, Figure 3.3 panel 5.]] Attribution of recent climate change is the "</span><span>,</span><span>
  </span><span>"title"</span><span>:</span><span> </span><span>"Attribution of recent climate change"</span><span>
 </span><span>},</span><span>
</span><span>]</span><span>
</span></code></pre></div></div>

<p>It’s much cleaner and moving in the right direction but I still have to figure out the literal eval reliably and a few square brackets are making it through as well. 
These premade parsers are far slower but one advantage of cleaning up Wikipedia articles is that they end up far smaller without the markup. If you just want the accumulated 
knowledge in plain text format, it ends up being a fraction of the size.</p>



<p>I can tolerate a few aberrations here and there but the perfectionist in me wants to do better. Anyways, here’s my script as it stands today:</p>

<div><div><pre><code><span>import</span> <span>re</span>
<span>import</span> <span>os</span>
<span>import</span> <span>json</span>
<span>from</span> <span>uuid</span> <span>import</span> <span>uuid4</span>
<span>import</span> <span>gc</span>
<span>from</span> <span>html2text</span> <span>import</span> <span>html2text</span> <span>as</span> <span>htt</span>
<span>import</span> <span>wikitextparser</span> <span>as</span> <span>wtp</span>


<span>archive_dir</span> <span>=</span> <span>'d:/WikipediaArchive/'</span>
<span>dest_dir</span> <span>=</span> <span>'D:/enwiki20201020/'</span>
<span>chars_per_file</span> <span>=</span> <span>40</span> <span>*</span> <span>1000</span> <span>*</span> <span>1000</span>  <span># create a consistently sized chunk (~40MB each)
</span>

<span>def</span> <span>dewiki</span><span>(</span><span>text</span><span>):</span>
    <span>text</span> <span>=</span> <span>wtp</span><span>.</span><span>parse</span><span>(</span><span>text</span><span>).</span><span>plain_text</span><span>()</span>
    <span>text</span> <span>=</span> <span>htt</span><span>(</span><span>text</span><span>)</span>
    <span>text</span> <span>=</span> <span>text</span><span>.</span><span>replace</span><span>(</span><span>'</span><span>\\</span><span>n'</span><span>,</span><span>' '</span><span>)</span>
    <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>'\s+'</span><span>,</span> <span>' '</span><span>,</span> <span>text</span><span>)</span>
    <span>return</span> <span>text</span>
    

<span>def</span> <span>analyze_chunk</span><span>(</span><span>text</span><span>):</span>
    <span>try</span><span>:</span>
        <span>if</span> <span>'&lt;redirect title="'</span> <span>in</span> <span>text</span><span>:</span>  <span># this is not the main article
</span>            <span>return</span> <span>None</span>
        <span>else</span><span>:</span>
            <span>title</span> <span>=</span> <span>text</span><span>.</span><span>split</span><span>(</span><span>'&lt;title&gt;'</span><span>)[</span><span>1</span><span>].</span><span>split</span><span>(</span><span>'&lt;/title&gt;'</span><span>)[</span><span>0</span><span>]</span>
            <span>if</span> <span>':'</span> <span>in</span> <span>title</span><span>:</span>  <span># this is a talk, category, or other (not a real article)
</span>                <span>return</span> <span>None</span>
        <span>serial</span> <span>=</span> <span>text</span><span>.</span><span>split</span><span>(</span><span>'&lt;id&gt;'</span><span>)[</span><span>1</span><span>].</span><span>split</span><span>(</span><span>'&lt;/id&gt;'</span><span>)[</span><span>0</span><span>]</span>
        <span>content</span> <span>=</span> <span>text</span><span>.</span><span>split</span><span>(</span><span>'&lt;/text'</span><span>)[</span><span>0</span><span>].</span><span>split</span><span>(</span><span>'&lt;text'</span><span>)[</span><span>1</span><span>].</span><span>split</span><span>(</span><span>'&gt;'</span><span>,</span> <span>maxsplit</span><span>=</span><span>1</span><span>)[</span><span>1</span><span>]</span>
        <span>content</span> <span>=</span> <span>dewiki</span><span>(</span><span>content</span><span>)</span>
        <span>return</span> <span>{</span><span>'title'</span><span>:</span> <span>title</span><span>,</span> <span>'text'</span><span>:</span> <span>content</span><span>,</span> <span>'id'</span><span>:</span> <span>serial</span><span>}</span>
    <span>except</span><span>:</span>
        <span>return</span> <span>None</span>


<span>def</span> <span>save_data</span><span>(</span><span>data</span><span>):</span>
    <span>if</span> <span>len</span><span>(</span><span>data</span><span>)</span> <span>==</span> <span>0</span><span>:</span>
        <span>return</span>
    <span>filename</span> <span>=</span> <span>dest_dir</span> <span>+</span> <span>str</span><span>(</span><span>uuid4</span><span>())</span> <span>+</span> <span>'.json'</span>
    <span>print</span><span>(</span><span>'Saving:</span><span>\t</span><span>'</span><span>,</span> <span>filename</span><span>)</span>
    <span>with</span> <span>open</span><span>(</span><span>filename</span><span>,</span> <span>'w'</span><span>,</span> <span>encoding</span><span>=</span><span>'utf-8'</span><span>)</span> <span>as</span> <span>outfile</span><span>:</span>
        <span>json</span><span>.</span><span>dump</span><span>(</span><span>data</span><span>,</span> <span>outfile</span><span>,</span> <span>sort_keys</span><span>=</span><span>True</span><span>,</span> <span>indent</span><span>=</span><span>1</span><span>)</span>


<span>def</span> <span>main</span><span>(</span><span>file</span><span>):</span>
    <span>print</span><span>(</span><span>file</span><span>)</span>
    <span>outdata</span> <span>=</span> <span>list</span><span>()</span>
    <span>article</span> <span>=</span> <span>''</span>
    <span>total_len</span> <span>=</span> <span>0</span>
    <span>with</span> <span>open</span><span>(</span><span>archive_dir</span> <span>+</span> <span>file</span><span>,</span> <span>'r'</span><span>,</span> <span>encoding</span><span>=</span><span>'utf-8'</span><span>)</span> <span>as</span> <span>infile</span><span>:</span>
        <span>for</span> <span>line</span> <span>in</span> <span>infile</span><span>:</span>
            <span>if</span> <span>'&lt;page&gt;'</span> <span>in</span> <span>line</span><span>:</span>  <span># new article begins
</span>                <span>article</span> <span>=</span> <span>''</span>
            <span>elif</span> <span>'&lt;/page&gt;'</span> <span>in</span> <span>line</span><span>:</span>  <span># end of article
</span>                <span>doc</span> <span>=</span> <span>analyze_chunk</span><span>(</span><span>article</span><span>)</span>
                <span>if</span> <span>doc</span><span>:</span>
                    <span>outdata</span><span>.</span><span>append</span><span>(</span><span>doc</span><span>)</span>
                    <span>total_len</span> <span>+=</span> <span>len</span><span>(</span><span>doc</span><span>[</span><span>'text'</span><span>])</span>
                    <span>if</span> <span>total_len</span> <span>&gt;=</span> <span>chars_per_file</span><span>:</span>
                        <span>save_data</span><span>(</span><span>outdata</span><span>)</span>
                        <span>outdata</span> <span>=</span> <span>list</span><span>()</span>
                        <span>total_len</span> <span>=</span> <span>0</span>
            <span>else</span><span>:</span>
                <span>article</span> <span>+=</span> <span>line</span>
    <span>save_data</span><span>(</span><span>outdata</span><span>)</span>

    
<span>if</span> <span>__name__</span> <span>==</span> <span>'__main__'</span><span>:</span>
    <span>for</span> <span>file</span> <span>in</span> <span>os</span><span>.</span><span>listdir</span><span>(</span><span>archive_dir</span><span>):</span>
        <span>if</span> <span>'bz2'</span> <span>in</span> <span>file</span><span>:</span>
            <span>continue</span>
        <span>main</span><span>(</span><span>file</span><span>)</span>
        <span>gc</span><span>.</span><span>collect</span><span>()</span>
</code></pre></div></div>

<p>It’s not the most elegant solution but for just over 100 lines of code, it will parse almost all of Wikipedia and save it to 40MB chunks of JSON.
Running this script looks like the following:</p>

<div><div><pre><code><span>(</span>base<span>)</span> C:<span>\O</span>fflineWikipedia&gt;python jsonify_wikipedia.py</code></pre></div></div></section></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://daveshap.github.io/DavidShapiroBlog/automation/2020/11/15/parsing-all-wikipedia.html">https://daveshap.github.io/DavidShapiroBlog/automation/2020/11/15/parsing-all-wikipedia.html</a></em></p>]]>
            </description>
            <link>https://daveshap.github.io/DavidShapiroBlog/automation/2020/11/15/parsing-all-wikipedia.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25238316</guid>
            <pubDate>Sat, 28 Nov 2020 16:01:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Emacs Conference 2020]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25238266">thread link</a>) | @dangom
<br/>
November 28, 2020 | https://emacsconf.org/2020/ | <a href="https://web.archive.org/web/*/https://emacsconf.org/2020/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div class="page">



<div id="pagebody">







<div id="content" role="main">
<p>EmacsConf 2020 | Online Conference | <strong>November 28 and 29, 2020</strong><br>
<a href="https://emacsconf.org/i/emacsconf-logo1-256.png"><img src="https://emacsconf.org/i/emacsconf-logo1-256.png" width="256" height="256" alt="EmacsConf logo"></a><br>
<a href="https://emacsconf.org/2020/schedule/"><strong>Schedule</strong></a> | <a href="https://emacsconf.org/2020/poster/"><strong>Poster</strong></a> | <a href="https://emacsconf.org/2020/planning/">Planning</a> |
<a href="https://emacsconf.org/conduct/">Code of Conduct</a></p>

<p>EmacsConf is the conference about the joy of Emacs, Emacs Lisp, and
memorizing key sequences.</p>

<p>EmacsConf 2020 was on November 28 (Sat) and November 29 (Sun),
2020 from 9am-5pm Toronto/EST time; equivalently, 6am-2pm PST,
2pm-10pm UTC, 3pm-11pm Zurich/CET.</p>

<p>It made sense to hold EmacsConf 2020 as a virtual (online) conference
again this year, especially now, given the current state of the world
with the ongoing global pandemic. We remain fully committed to
freedom, and we will continue using our infrastructure and streaming
setup consisting entirely of <a href="https://www.gnu.org/philosophy/free-sw.html">free software</a>, much like the
last EmacsConf. Check out the <a href="https://emacsconf.org/2020/schedule/"><strong>Schedule</strong></a> and
<a href="https://emacsconf.org/2020/poster/"><strong>Poster</strong></a> for more details.</p>

<h2>Watching</h2>

<p>Over the next few weeks, we'll split up the bulk video recordings into
individual talks. We'll post the videos and links on the individual
talk pages, and we'll send an update to the
<a href="https://lists.gnu.org/mailman/listinfo/emacsconf-discuss">emacsconf-discuss</a> mailing list. In the meantime,
please enjoy <a href="https://emacsconf.org/2019/talks/">last year's talks</a>.</p>

<h2>Participating</h2>

<p>For audience questions specifically, we experimented with using a
collaboratively-editable Etherpad as the primary means of collecting
audience questions. <a href="https://etherpad.wikimedia.org/p/emacsconf-2020">https://etherpad.wikimedia.org/p/emacsconf-2020</a>
We also took questions from our IRC channel (<code>#emacsconf</code> on
<code>chat.freenode.net</code>), with volunteers adding questions from that
channel to the pad on behalf of folks who were not able to or prefer
not to use the web-based questions pad.</p>

<p>Come hang out with us in <code>#emacsconf</code> on <code>chat.freenode.net</code>.  You can
join the chat using <a href="ircs://chat.freenode.net:6697/emacsconf">your favourite IRC client</a>, or by visiting
<a href="https://chat.emacsconf.org/">chat.emacsconf.org</a> in your web browser, a self-hosted instance
of <a href="https://thelounge.chat/">The Lounge</a> free software web IRC client for EmacsConf.</p>

<p>To follow up after the conference, please check the <a href="https://emacsconf.org/2020/schedule/">schedule</a> for
the link to the individual talk page. Over the next few weeks, we'll
add notes from the pad, other resources, and followup contact
information from speakers.</p>

<h2>Updates</h2>

<p>Be sure to subscribe to our mailing list
<a href="https://lists.gnu.org/mailman/listinfo/emacsconf-discuss">emacsconf-discuss</a> for discussion and
announcements about the conference.</p>

</div>







</div>



</div></div>]]>
            </description>
            <link>https://emacsconf.org/2020/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25238266</guid>
            <pubDate>Sat, 28 Nov 2020 15:54:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A minimal Android system for RISC-V]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25238101">thread link</a>) | @homarp
<br/>
November 28, 2020 | https://plctlab.github.io/aosp/create-a-minimal-android-system-for-riscv.html | <a href="https://web.archive.org/web/*/https://plctlab.github.io/aosp/create-a-minimal-android-system-for-riscv.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <section id="main_content">
        

<p>Chen Wang, 2020.11.24</p>

<p>After a period of hard work, we can now run an Android “minimal system” on QEMU of RISC-V.</p>

<p>The following is a brief summary of the current work. Although the road ahead is still long, there is something to have a look.</p>

<h2 id="1-requirement-analysis">1. Requirement analysis</h2>

<p>The full name of our project is “AOSP for RISC-V”, and all the source code is currently opened on github: <a href="https://github.com/aosp-riscv">https://github.com/aosp-riscv</a>. The ultimate goal of our project is to port Android on RISC-V. Of course, this goal is huge.</p>

<p>But in the short term, we still have a small goal, which is described in one sentence: <code>based on the RISC-V platform, realize the kernel part of Android running on QEMU, and run the Android Shell</code>.</p>

<p>Based on the above objectives, the specific analysis is to realize a minimal Android system. The meaning of “minimal system” here is the so-called “bootable unix-style command line operating system”. In the traditional sense, a complete “minimal system” is described on the left side of the figure below. From bottom to top, the bottom is the hardware (note: hardware is not part of our “minimal system”). The first layer of software running on the hardware is the “Operating System Kernel”, and upon the OS kernel is the <a href="https://en.wikipedia.org/wiki/C_standard_library">“C library”</a>. Based on the C Library, we can build a minimal file system, which is essentially a bunch of command-line tools. These command-line tools must include at least a <a href="https://en.wikipedia.org/wiki/Init">“init”</a>, which is used to start the basic login shell in cooperation with the kernel, and one <a href="https://en.wikipedia.org/wiki/Unix_shell">“Shell”</a> is used to interact with users and call other tools and programs. With this “minimal system”, our big goal has a foundation.</p>

<p><img src="https://plctlab.github.io/aosp/diagrams/mini-system.png" alt="a minimal unix-style os"></p>

<p>Through the investigation of AOSP, I roughly summarized the work we need to achieve as follows:</p>

<ul>
  <li>Hardware part: Here we first use QEMU for RISC-V to simulate.</li>
  <li>OS Kernel: The kernel of Android uses Linux, of course, it has some patches of its own.</li>
  <li>C Library: Android has its own C library, which is bionic. It differs from the GNU C library (glibc) in that it is designed for devices with low memory and processor capabilities and running on Linux. It is released under the BSD license, rather than using the GNU public license like glibc.</li>
  <li>Root filesystem: Android has its own complex file system organization. As the goal of our experiment, what we need is the most streamlined and smallest file system. There is no need to transplant the complete Android system over, so I chose toybox to implement our various kind of command line tool. Someone may ask why we don’t use the more famous busybox. The reason is still related to the software license. Busybox uses GPL, while toybox uses BSD, which is more in line with Android’s appetite. Therefore, toybox is included in the source tree of AOSP, but busybox is not. In addition, I need to mention, because the implementation of toybox is very simple, the shell that provided by toybox does not work properly. Fortunately, Android already has its own official Shell, which is mksh, so we use mksh directly.</li>
</ul>

<h2 id="2-introduction-to-porting-work">2. Introduction to porting work</h2>

<p>The above talked about what needs to be done in the overall porting work. In fact, there are still many details in the specific implementation. Since our final goal (to transplant AOSP as a whole to RISC-V) is far from being achieved, I will briefly sort out what I have achieved so far, just for memo:</p>

<ul>
  <li>Operating platform (hardware): currently adopt QEMU temporarily.</li>
  <li>AOSP version: tag based on <code>android-10.0.0_r39</code>.</li>
  <li>Toolchain environment: building of AOSP has been completely migrated to LLVM/CLANG, but GNU tools are still used in linking. Since the prebuild tool chain that comes with AOSP does not support RISC-V, I have to build my own LLVM/Clang and GNU-tools.</li>
  <li>Kernel porting: the kernel version I used tag <code>android-5.4-stable</code> for andorid common repository plus tag <code>android11-release</code> for configs repository.</li>
  <li>The porting of the BIONIC library, as mentioned earlier, is based on the tag <code>android-10.0.0_r39</code> too. Considering the requirements in first phase, only the static library of libc is implemented, the dynamic library of libc is not implemented, neither for libm/libdl/libstdc++/linker till now (but I will handle them soon later). In other words, the following executable programs such as toybox and mksh are statically linked. libc is the most important part of bionic, and the composition is quite complex. The main components and the dependencies between them are briefly summarized in following diagram:</li>
</ul>

<p><img src="https://plctlab.github.io/aosp/diagrams/bionic-libc.png" alt="bionic libc"></p>

<ul>
  <li>toybox: As mentioned earlier, it is based on tag <code>android-10.0.0_r39</code>, but with a lot of tailoring. Because the toybox in Android includes many Android-specific features, such as SELinux and encryption, etc. In order not to involve too much effort in current phase, I disabled these functions and only retain some basic common functions.</li>
  <li>mksh: The Shell looks relatively simple, just make sure there is no problem with the compilation.</li>
  <li>There is also a big work involved in the construction of the build system. I did not use the native Soong system that comes with AOSP, because in the pre-research process, I found that it is not easy to add a new ARCH from scrach in the existing AOSP build system (i.e. to use the traditional lunch + m). AOSP’s building system is too complicated and mature for existing ARCH that supported, but it is not friendly to latecomers. In order to reduce the risk and focus on the key areas in advance, I chose to use make and rewrite the makefiles for modules (bionic/toybox/mksh) that need to be ported. Of course, we still need to find a chance to move to AOSP Soong later, just let’s do it later.</li>
</ul>

<h2 id="3-steps-to-make-this-minimal-system">3. Steps to make this minimal system</h2>

<p>After some work, the above small goal has been initially completed, and at least one of the “minimal Android systems” we defined above can be launched on QEMU. The related porting and modification have been opened on github. Allow me brief following steps and you are welcomed to have a try and test, submit PR, or directly participate in our AOSP porting work.</p>

<h3 id="31-environmental-preparation">3.1 Environmental preparation</h3>

<p>The experiment is based on Ubuntu 20.04 LTS</p>

<div><div><pre><code>$ lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description: Ubuntu 20.04 LTS
Release: 20.04
Codename: focal
</code></pre></div></div>

<p>The software that needs to be installed in advance is as follows:</p>

<div><div><pre><code>$ sudo apt install autoconf automake autotools-dev curl libmpc-dev libmpfr-dev libgmp-dev \
                  gawk build-essential bison flex texinfo gperf libtool patchutils bc \
                  zlib1g-dev libexpat-dev git \
                  libglib2.0-dev libfdt-dev libpixman-1-dev \
                  libncurses5-dev libncursesw5-dev
</code></pre></div></div>

<p>Then create a working directory <code>riscv64-linux</code>, the following operations are performed under this directory.</p>

<div><div><pre><code>$ mkdir riscv64-linux
$ cd riscv64-linux
</code></pre></div></div>

<h4 id="311-building-gnu-toolchain">3.1.1 Building GNU toolchain</h4>

<p>Download source code</p>

<div><div><pre><code>$ git clone https://github.com/riscv/riscv-gnu-toolchain
</code></pre></div></div>

<p>Enter the source directory:</p>



<p>Note that the main repository of clone above does not contain the contents of the sub-repositories, so you need to continue to update the sub-repositories. Note that the sub-repository of qemu is excluded first, because the complete download of qemu is too large; second, qemu is actually not needed for the toolchain compilation itself.</p>

<div><div><pre><code>$ git rm qemu
$ git submodule update --init --recursive
</code></pre></div></div>

<p>Wait patiently for the sub-repository download to complete.</p>

<p>Note that due to I want to install the tools to <code>/opt/riscv64</code>, so sudo is required for make.</p>

<div><div><pre><code>$ ./configure --prefix=/opt/riscv64
$ sudo make linux -j $(nproc)
</code></pre></div></div>

<p>Export the installation path of the toolchain. You can also write to the <code>.bashrc</code> file.</p>

<div><div><pre><code>export PATH="$PATH:/opt/riscv64/bin"
</code></pre></div></div>

<p>Test whether the toolchain is installed successfully.</p>

<div><div><pre><code>$ riscv64-unknown-linux-gnu-gcc -v
</code></pre></div></div>

<p>Output similar to the following shows that the toolchain is compiled and installed normally.</p>

<div><div><pre><code>Using built-in specs.
COLLECT_GCC=riscv64-unknown-linux-gnu-gcc
COLLECT_LTO_WRAPPER=/opt/riscv64/libexec/gcc/riscv64-unknown-linux-gnu/10.1.0/lto-wrapper
Target: riscv64-unknown-linux-gnu
Configured with: /home/u/ws/riscv64-linux/riscv-gnu-toolchain/riscv-gcc/configure --target=riscv64-unknown-linux-gnu --prefix=/opt/riscv64 --with-sysroot= /opt/riscv64/sysroot --with-system-zlib --enable-shared --enable-tls --enable-languages=c,c++,fortran --disable-libmudflap --disable-libssp --disable-libquadmath- -disable-libsanitizer --disable-nls --disable-bootstrap --src=.././riscv-gcc --disable-multilib --with-abi=lp64d --with-arch=rv64imafdc --with-tune =rocket'CFLAGS_FOR_TARGET=-O2 -mcmodel=medlow''CXXFLAGS_FOR_TARGET=-O2 -mcmodel=medlow'
Thread model: posix
Supported LTO compression algorithms: zlib
gcc version 10.1.0 (GCC)
</code></pre></div></div>

<h4 id="312-build-llvmclang-tool-chain">3.1.2 Build LLVM/Clang tool chain</h4>

<p>Make sure to return to the working directory <code>riscv64-linux</code> first.</p>

<p>After updating Ubuntu 20.04 LTS to the latest state, the software requirements for building llvm/clang should have been supported by default. Other tools needed in the compilation process are basically available on Ubuntu. If something is missed, please install it yourself.</p>

<p>Download the source code of llvm. The official source code repository is at github: <a href="https://github.com/llvm/llvm-project">https://github.com/llvm/llvm-project</a>.</p>

<div><div><pre><code>$ git clone https://github.com/llvm/llvm-project
</code></pre></div></div>

<p>After downloading, enter the root directory of the source code repository and check out the corresponding version. I choose thhe official release version <code>10.0.1-final</code> and switch to the <code>10.x</code> branch.</p>

<div><div><pre><code>$ cd llvm-project/
$ git checkout release/10.x
$ mkdir build
$ cd build
$ cmake -G "Unix Makefiles" \
-DCMAKE_BUILD_TYPE=Release \
-DCMAKE_INSTALL_PREFIX=../install \
-DLLVM_TARGETS_TO_BUILD="RISCV" \
-DLLVM_ENABLE_PROJECTS="clang;libcxx;libcxxabi" \
-DLLVM_DEFAULT_TARGET_TRIPLE="riscv64-unknown-linux-gnu" \
../llvm
$ make -j $(nproc)
$ make install
</code></pre></div></div>

<p>Simply check the results of the installation.</p>

<div><div><pre><code>$ ls ../install/ -l
total 20
drwxrwxr-x 2 u u 4096 October 9 11:37 bin
drwxrwxr-x 7 u u 4096 Oct 9 11:37 include
drwxrwxr-x 4 u u 4096 October 9 11:37 lib
drwxrwxr-x 2 u u 4096 October 9 11:37 …</code></pre></div></div></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://plctlab.github.io/aosp/create-a-minimal-android-system-for-riscv.html">https://plctlab.github.io/aosp/create-a-minimal-android-system-for-riscv.html</a></em></p>]]>
            </description>
            <link>https://plctlab.github.io/aosp/create-a-minimal-android-system-for-riscv.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25238101</guid>
            <pubDate>Sat, 28 Nov 2020 15:29:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Dataframes and the proliferation of bad code in data science]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25237677">thread link</a>) | @soumendra
<br/>
November 28, 2020 | https://databiryani.com/dataframes/machine-learning/data-science/2020/11/28/dataframes-and-the-proliferation-of-bad-code-in-data-science.html | <a href="https://web.archive.org/web/*/https://databiryani.com/dataframes/machine-learning/data-science/2020/11/28/dataframes-and-the-proliferation-of-bad-code-in-data-science.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <ul>
<li><a href="#why-good-code-is-important">Why good code is important?</a></li>
<li><a href="#why-data-scientists-write-bad-code">Why data scientists write bad code?</a></li>
<li><a href="#cult-of-kaggle">Cult of Kaggle</a></li>
<li><a href="#state-mismanagement">State Mismanagement</a>
<ul>
<li><a href="#what-is-state-mismanagement">What is state mismanagement?</a></li>
<li><a href="#why-state-management-is-important">Why state management is important?</a></li>
<li><a href="#state-mismanagement-with-dataframes">State mismanagement with DataFrames</a></li>
</ul>
</li>
<li><a href="#poorly-abstracted-codebases">Poorly abstracted codebases</a>
<ul>
<li><a href="#consequences">Consequences</a></li>
</ul>
</li>
<li><a href="#how-to-do-better">How to do better?</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#discuss-this-post">Discuss this post</a></li>
</ul>

<p>Developing a solution with machine learning is equal parts science and software engineering. The science of it is important, certainly, but your success will be equally dependant, if not more, on how you engineer your solution. Given the abundance of libraries that implement state-of-the-art algorithms for easy consumption and the data (and pre-trained models) to go with them, your ability to engineer your code well is the critical component for your success.</p>

<blockquote>
  <p>Between two somewhat comparable data scientists, the one with better software engineering skills is more likely to succeed in a machine learning project than the one with better machine learning skills.</p>
</blockquote>

<p>The reasons are twofold. First, the barrier to entry to build effective models is getting lower as many high-level libraries make it increasingly easier to solve a wide variety of problems in machine learning and deep learning. State-of-the-art results become available (as pre-trained models) in a matter of weeks after publication. There is always a tutorial around the corner showing you how to tackle your ml/dl problems with freely available tools.</p>

<p>Secondly, the data science culture is a mess, leading to a focus on model performance rather than real-life performance (which can be measured only after deployment). For data scientists, the impetus is more on getting the experiments right, quickly; whatever it takes to achieve that! But the value from data science/machine learning projects comes from their deployment, not modeling experiments.</p>

<blockquote>
  <p>A more effective data scientist is the one who deploys more often, not the one with better model scores.</p>
</blockquote>

<p>The culture around this is changing though, and we hope to be a part of that change.</p>



<p>In this issue, we focus on our perception of the core reasons coming from working with and educating a lot of data scientists over the years.</p>

<p>Primarily, we think the problem arises from how the <strong>Cult of Kaggle</strong> (not in the way you think) promoted a habit of <strong>State Mismanagement</strong>, which results in <strong>Poorly abstracted codebases</strong> hurting data science projects.</p>



<p>Kaggle is a competition platform for machine learning and deep learning, and a lot of people complain that Kaggle is not real data science. That it sets up wrong expectations with its super-clean datasets neatly divided into train/test-sets, freeing the data scientist to focus only on the modeling problem and not have to worry about data issues.</p>

<p>While this is a valid criticism, we believe that there is a place and utility for such a platform. Given the fantastic community that has come up around Kaggle, it is now one of the most popular platforms for new (and old) data scientists to hone their craft.</p>

<p>The problem is the outsized role Kaggle plays in the education of data scientists. Given a new problem, most will try to get to those train/test-sets (and possibly a validation-set thrown in) and try to get a model that “works” in the <a href="https://en.wikipedia.org/wiki/Empirical_risk_minimization">empirical risk management</a> sense (on the test-set, of course).</p>

<blockquote>
  <p>For data scientists, the impetus is more on getting the experiments right, quickly; whatever it takes to achieve that! But the value from data science/machine learning projects comes from their deployment, not modelling experiments.</p>
</blockquote>

<p>This results in code that has bad abstractions. This code is usually propped up with all sorts of hacky engineering practices and put into production.</p>

<p>But if we have to put our finger on one bad abstraction that is pervasive in the data science (engineering) universe and responsible for much of the bad code, it will be state mismanagement.</p>



<p>DataFrames are one of the most ubiquitous and useful data structures, no matter what stack you are using, and they are also very useful to see if a codebase is likely to be bad.</p>

<blockquote>
  <p>If most of the functions in your codebase accept dataframes as input and return dataframes as output, you are likely not modelling the entities correctly and writing hacky code to patch things.</p>
</blockquote>

<h2 id="what-is-state-mismanagement">
What is state mismanagement?</h2>

<p>If you are coming from a traditional programming background, having done webapps or backends or any of the myriad things we do when we code, you’ll be used to creating some kind of abstraction to model the entities involved in your code. A customer, a purchase, a geographic location - any of these could be an entity of interest given the problem we have.</p>

<p>With <a href="https://en.wikipedia.org/wiki/Object-oriented_programming">OOP</a>, you may have created classes to represent and model the state/behaviour of those entities. With <a href="https://en.wikipedia.org/wiki/Functional_programming">FP</a>, you may have focussed more on the state and the transitions those entities go through.</p>

<p>By <strong>state mismanagement</strong>, we refer to the situation when we don’t create these abstractions.</p>

<h2 id="why-state-management-is-important">
Why state management is important?</h2>

<p>These abstractions are very useful. We can write tests against them. We can use them to debug our code. We can persist them and inspect them later when something goes wrong. These abstractions also make code readable - we can follow the state and transitions of entities to understand what is happening. We can also use the <a href="https://en.wikipedia.org/wiki/Type_system">type of these abstractions</a> to debug/reason about our code <a href="https://en.wikipedia.org/wiki/Strong_and_weak_typing">if it is possible</a>.</p>

<h2 id="state-mismanagement-with-dataframes">
State mismanagement with DataFrames</h2>

<p>DataFrames list all observations of an experiment as rows and attributes of those observations as columns. They are similar to the tables in databases we are used to, but most dataframes in a data science project will typically combine many tables (each table typically represents an entity) into a single object (which will represent/hide states of many entities together).</p>

<p>When we don’t explicitly model different entities, our ability to reason about and debug/inspect them gets diminished significantly. We can’t write tests against functions that do things with those entities and their states. Most of our functions will accept dataframes as input and return dataframes as output, and will typically be in violation of the <a href="https://en.wikipedia.org/wiki/SOLID">SOLID principle</a>.</p>

<p>There are, of course, situations where dataframes are the correct solution. But someone not used to thinking about hidden entities and their states may not use them optimally.</p>

<blockquote>
  <p>The habit of including the states of many entities together in a single dataframe is the most prevalent reason for bad code that we have seen.</p>
</blockquote>

<p>When <a href="https://www.thoughtworks.com/insights/blog/coding-practices-data-scientists">we talk about the poor standard of software engineering</a> in data science, we talk a lot about peripheral issues, but incorrect and <a href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b">leaky abstractions</a> are the main reason why most of the codebase we have seen suck. If you don’t even model your data adequately, how do you become <a href="http://karpathy.github.io/2019/04/25/recipe/#1-become-one-with-the-data">one with the data</a>?</p>

<p>Models doing well in training and poorly in production, whether due to <a href="https://en.wikipedia.org/wiki/Concept_drift">drift</a> or <a href="https://twitter.com/araffin2/status/1329382226421837825">underspecification</a>, is a <a href="https://arxiv.org/abs/2011.03395">real problem</a>. Without the necessary abstractions in place, it becomes very difficult to monitor, detect, and debug issues when they arise.</p>

<div><p>
If you like what you have read so far, you may consider subscribing to our newsletter.
</p>
</div>



<p>When we say poor abstractions result in poor code, lack of <a href="https://en.wikipedia.org/wiki/SOLID">SOLID</a>-ness is mostly what we mean. A lot of the hacky software engineering practices we have seen in the wild arise as a consequence.</p>

<h2 id="consequences">
Consequences</h2>

<ul>
  <li>
<strong>Deploying models</strong>: The pattern for consuming data in production is different compared to training. Abstractions created to explore/test/validate data in training can be mostly reused in deployment, and the lack of them hurts greatly.</li>
  <li>
<strong>Writing tests</strong>: It is hard to write meaningful tests if models (datastructures) for the entities in data are not available. This is why we are not seeing more tests in ML even though everyone is talking about them.</li>
  <li>
<strong>Monitoring models</strong>: So your credit default model is starting to perform poorly. Is this across the board, or only for a certain segment for certain entities (geocode, user-acquisition-channel cohort, or month)? Hard to answer this quickly in real-time while your deployed models are falling apart (and losing money) if you have not spent the time and effort to model the data and set up appropriate tests/monitoring.</li>
  <li>
<strong>Reproducibility</strong>: Reproducibility can come in many forms. You may be unable to reproduce the older models during monthly updates (data and hyperparameter versioning is another issue entirely), or the same deep learning-based model may produce different embeddings in a cpu-based deployment server when moved from a gpu-based training server. Without correct abstractions to debug with, you’ll be left wondering what is going wrong.</li>
  <li>
<strong>Commenting and reading code</strong>: Poorly written code with a lot of dataframe needs a lot of documentation, and anyone who has worked in a live codebase knows that good comment coverage is a moving goalpost (moving as the code changes).</li>
</ul>

<p>(Note: If you are working with a Python stack, type-annotation is your friend. If you have to use a lot of <em>any</em> or <em>DataFrame</em> types, you are probably doing it wrong. In a correctly architected codebase, type-annotation can completely replace all forms of commenting and still make reading the codebase a breeze.)</p>



<p>As @seanjtaylor said, <em>there are no hard problems, only slow iterations</em>.</p>

<p>If you have an existing codebase that you think suffers from the issues we outline, start by refactoring small chunks of it (larger chunks will mean slower iterations and you’ll learn slower). If you are starting a new codebase, you can identify and model your entities and <a href="http://karpathy.github.io/2019/04/25/recipe/">be one with your data</a> before you start modeling.</p>

<p>And if you already kaggle, don’t stop. Just recognize that the things in Kaggle that give you dopamine hits should not be the same things that give you dopamine hits when you work on a production system. Hack that dopamine!</p>



<p>We are heavily indebted to our colleagues at Difference Engine for the thoughtful conversations and prompts. Particularly, this essay would not have been possible without the insightful …</p></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://databiryani.com/dataframes/machine-learning/data-science/2020/11/28/dataframes-and-the-proliferation-of-bad-code-in-data-science.html">https://databiryani.com/dataframes/machine-learning/data-science/2020/11/28/dataframes-and-the-proliferation-of-bad-code-in-data-science.html</a></em></p>]]>
            </description>
            <link>https://databiryani.com/dataframes/machine-learning/data-science/2020/11/28/dataframes-and-the-proliferation-of-bad-code-in-data-science.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25237677</guid>
            <pubDate>Sat, 28 Nov 2020 14:27:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Cloud Native Computing Foundation Announces Etcd Graduation]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25237523">thread link</a>) | @talonx
<br/>
November 28, 2020 | https://www.cncf.io/announcements/2020/11/24/cloud-native-computing-foundation-announces-etcd-graduation/ | <a href="https://web.archive.org/web/*/https://www.cncf.io/announcements/2020/11/24/cloud-native-computing-foundation-announces-etcd-graduation/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
	<article>
		
						

		<div>
			
<figure><img loading="lazy" width="1024" height="506" src="https://www.cncf.io/wp-content/uploads/2020/11/CNCF_Grad_Cards_etcd-1024x506.jpg" alt="" srcset="https://www.cncf.io/wp-content/uploads/2020/11/CNCF_Grad_Cards_etcd-1024x506.jpg 1024w, https://www.cncf.io/wp-content/uploads/2020/11/CNCF_Grad_Cards_etcd-300x148.jpg 300w, https://www.cncf.io/wp-content/uploads/2020/11/CNCF_Grad_Cards_etcd-768x380.jpg 768w, https://www.cncf.io/wp-content/uploads/2020/11/CNCF_Grad_Cards_etcd-1536x759.jpg 1536w, https://www.cncf.io/wp-content/uploads/2020/11/CNCF_Grad_Cards_etcd-325x161.jpg 325w, https://www.cncf.io/wp-content/uploads/2020/11/CNCF_Grad_Cards_etcd-700x346.jpg 700w, https://www.cncf.io/wp-content/uploads/2020/11/CNCF_Grad_Cards_etcd-320x158.jpg 320w, https://www.cncf.io/wp-content/uploads/2020/11/CNCF_Grad_Cards_etcd-515x255.jpg 515w, https://www.cncf.io/wp-content/uploads/2020/11/CNCF_Grad_Cards_etcd-640x316.jpg 640w, https://www.cncf.io/wp-content/uploads/2020/11/CNCF_Grad_Cards_etcd-1280x633.jpg 1280w, https://www.cncf.io/wp-content/uploads/2020/11/CNCF_Grad_Cards_etcd.jpg 2001w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p><em>Widely used data store solution for orchestrators has seen </em><em>200 distinct contributors in the past 12 months</em></p>



<p><strong>SAN FRANCISCO, Calif. – November 24, 2020 – </strong><a href="https://www.cncf.io/">The Cloud Native Computing Foundation</a>® (CNCF®), which builds sustainable ecosystems for cloud native software, today announced the graduation of etcd. To move from the maturity level of <a href="https://github.com/cncf/toc/blob/master/process/graduation_criteria.adoc">incubation to graduation</a> etcd has demonstrated growing adoption, an open governance process, feature maturity, and a strong commitment to community, sustainability, and inclusivity.</p>



<p><a href="https://etcd.io/">etcd</a> is a distributed, reliable key-value store and provides a reliable way to store data that needs to be accessed by a distributed system or cluster of machines. Applications of any complexity, from a simple web app to Kubernetes, can read data from and write data into etcd. The project was created at CoreOS in 2013 and joined CNCF in December 2018 as an incubating project.&nbsp;</p>



<p>“The etcd project is a key component inside Kubernetes along with many other projects that depend on etcd for reliable distributed data storage,” said Chris Aniszczyk, CTO of the Cloud Native Computing Foundation. “We remain impressed by the milestones that etcd continues to reach in scale and mature handling of its recent security audit, we look forward to cultivating its community as a graduated project.”</p>



<p>etcd is used in production by <a href="https://github.com/etcd-io/etcd/blob/master/ADOPTERS.md">many companies</a>, including Alibaba, Amazon, Baidu, Cisco, EMC, Google, Huawei, IBM, Red Hat, Uber, Verizon, and more, and projects including Kubernetes, CoreDNS, M3, Rook, and TiKV.</p>



<p>“Having etcd as our meta-store in Placement Driver and our inspiration for Raft implementation in production has proven to be a great choice for TiKV and TiDB, ensuring data consistency and high availability across TiDB clusters,” said Ed Huang, co-founder and CTO at PingCAP. “We are proud and glad to be part of its graduation journey, and we’d love to be involved in its ecosystem development more in the future.”</p>



<p>The <a href="https://github.com/etcd-io/etcd/blob/master/MAINTAINERS">maintainer</a> team currently consists of 10 members, with a healthy distribution of corporations represented, including Alibaba, Amazon, Cockroach Labs, Google Cloud, IBM, Indeed, and Red Hat. Three new maintainers have been added since etcd became an incubating project. Over the last 12 months, 200 distinct contributors have authored pull requests.</p>



<p>“After seven years of development, etcd has reached maturity and become the cornerstone of many distributed systems. The most important decision for its success was joining the CNCF community and growing its maintainers across many organizations,” said Xiang, etcd maintainer, CNCT TOC member, and engineering director at Alibaba Cloud. “We are excited to see its graduation at CNCF. etcd is the centerpiece powering the container service and many other critical services at Alibaba Cloud. We are looking forward to improving its stability, reliability, and performance with the community in the future.”</p>



<p>A third-party <a href="https://www.cncf.io/blog/2020/08/05/etcd-security-audit/">security audit</a> sponsored by CNCF was performed in July 2020 for the latest major release of etcd, v3.4 by Trail of Bits. According to the <a href="https://github.com/etcd-io/etcd/blob/master/security/SECURITY_AUDIT.pdf">report</a>, the etcd codebase represents a mature and heavily adopted product, and there were no significant issues found in the core components of etcd. One high severity issue was found in the etcd gateway, which the team addressed with fixes and backported into etcd supported releases.&nbsp;</p>



<p>The project also went through Jepsen testing, which analyzes open source distributed systems to check if they fulfill their consistency guarantees, in January 2020. The <a href="https://etcd.io/blog/jepsen-343-results/">results</a> showed maturity in the project functionality. The Jepsen team also pointed out a few areas for improvements, which were implemented by the etcd team.&nbsp;</p>



<p>“From the beginning, etcd was designed to ease consensus store operations, making it attractive for use with container orchestration systems like Kubernetes. etcd’s selection as the control plane storage for Kubernetes proved a great fit, and two projects have grown and matured together,” said Joe Betz, etcd maintainer and software engineer at Google Cloud. “We are excited to see etcd’s dedication toward reliability, scalability, and quality recognized by the CNCF with this graduation. Today’s announcement is a testament to the maturity of etcd and its readiness for production workloads.”&nbsp;</p>



<p>“Today’s major milestone of the graduation of etcd, could not have been accomplished without the work of the community and the support from the CNCF,” said Sahdev Zala, senior software engineer, open technology, IBM and etcd maintainer. “etcd is playing a critical role providing a distributed key-value store that is highly available and meets the strong consistency requirements demanded by large scale Kubernetes clusters.”</p>



<p>“Open source software powers our lives in so many ways,” said Bob Wise, General Manager of Kubernetes at AWS. “From Linux to Kubernetes, open communities of builders from all sizes of organizations and walks of life spend considerable time creating and maintaining projects that underpin much of the internet, telecommunications, finance, transportation, gaming, retail, and healthcare systems we use every day.&nbsp; etcd is one of these critical projects, and we’re proud to have etcd as a core part of Amazon EKS and to be involved in helping the project grow and thrive. We are fervent supporters of etcd’s graduation and look forward to collaborating with etcd and other CNCF projects to build secure, reliable, powerful, and scalable open source software.”</p>



<p>To officially graduate from incubating status, the project was certified for <a href="https://bestpractices.coreinfrastructure.org/en/projects/3192">CII Best Practices Badge</a>, completed security audits and addressed vulnerabilities, defined its own <a href="https://github.com/etcd-io/etcd/blob/master/GOVERNANCE.md">governance</a>, and adopted the <a href="https://github.com/etcd-io/etcd/blob/master/code-of-conduct.md">CNCF Code of Conduct</a>.</p>



<p><strong>etcd Background</strong></p>



<p>etcd is a distributed, reliable key-value store for the most critical data of a distributed system, with a focus on being:</p>



<ul><li>Simple: well-defined, user-facing API (gRPC)</li><li>Secure: automatic TLS with optional client cert authentication</li><li>Fast: benchmarked 10,000 writes/sec</li><li>Reliable: properly distributed using Raft</li></ul>



<p>To learn more about etcd, visit <a href="https://etcd.io/">etcd.io</a>.&nbsp;</p>



<p><strong>Additional Resources</strong></p>



<ul><li><a href="https://www.cncf.io/newsroom/newsletter/">CNCF Newsletter</a></li><li><a href="https://twitter.com/cloudnativefdn/">CNCF Twitter</a></li><li><a href="https://cncf.io/">CNCF Website</a></li><li><a href="https://cncf.io/join">Learn About CNCF Membership</a></li><li><a href="http://www.cncf.io/people/end-user-community.">Learn About the CNCF End User Community</a></li></ul>



<p><strong>About Cloud Native Computing Foundation</strong></p>



<p>Cloud native computing empowers organizations to build and run scalable applications with an open source software stack in public, private, and hybrid clouds. The Cloud Native Computing Foundation (CNCF) hosts critical components of the global technology infrastructure, including Kubernetes, Prometheus, and Envoy. CNCF brings together the industry’s top developers, end users, and vendors, and runs the largest open source developer conferences in the world. Supported by more than 500 members, including the world’s largest cloud computing and software companies, as well as over 200 innovative startups, CNCF is part of the nonprofit Linux Foundation. For more information, please visit www.cncf.io.</p>



<p><em>###</em></p>



<p><em>The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our trademark usage page. Linux is a registered trademark of Linus Torvalds.</em></p>



<p><strong>Media Contact</strong></p>



<p>Katie Meinders</p>



<p>The Linux Foundation</p>



<p><a href="mailto:PR@CNCF.io">PR@CNCF.io</a></p>


			<hr>
			
		</div>
				</article>
</div></div>]]>
            </description>
            <link>https://www.cncf.io/announcements/2020/11/24/cloud-native-computing-foundation-announces-etcd-graduation/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25237523</guid>
            <pubDate>Sat, 28 Nov 2020 13:56:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Is Probability Real?]]>
            </title>
            <description>
<![CDATA[
Score 192 | Comments 168 (<a href="https://news.ycombinator.com/item?id=25237356">thread link</a>) | @EbTech
<br/>
November 28, 2020 | https://www.arameb.com/blog/2020/11/22/probability | <a href="https://web.archive.org/web/*/https://www.arameb.com/blog/2020/11/22/probability">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>Today, I want to address an issue with statements involving chance. To demonstrate, let’s first consider a statement that doesn’t involve chance:</p>

<p>“<em>A cubic die tossed onto a flat surface will come to rest on one of its six sides.</em>”</p>

<p>This claim can be empirically tested, with various dice and surfaces. If any one of our experiments results in the die spinning endlessly on a corner, we will have disproven the claim. We may have to refine the claim’s conditions; for instance, by requiring the presence of gravity. Nonetheless, it’s fairly clear what it means for the statement to be true or false. Now let’s try to make a claim involving probability:</p>

<p>“<em>If a pair of standard dice are thrown, the probability of their face-up sides summing to nine will be one in nine (about 0.11 or 11%).</em>”</p>

<p>What does it mean for this statement to be true? Unlike the first statement, this one doesn’t specify which result we’ll actually see. How can we possibly hope to test it, or to make use of its information?</p>



<p>Within the realm of abstract mathematics, we’re free to model probability in a way that fits our intuitions. Imagine a multiverse containing an infinity of possible worlds, whose total <em>measure</em> is 100%. Define the probability of an <em>event</em>, such as that of rolling a nine, to be the measure assigned to the subset of worlds in which the event actually occurs.</p>

<p>In the abstract formalism, we’re allowed to assign the measure however we like, subject to Kolmogorov’s axioms: the measure must be non-negative, countably additive, and total to 100%. By respecting the symmetry of an idealized die,<sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup> we might argue that only one such assignment makes sense; from it, we can calculate the probability of any event involving dice rolls.</p>

<p>There are two shortcomings to this approach. Firstly, we won’t always deal with nicely symmetrical objects for which direct a priori arguments are possible. Thus, we still need a means of testing probabilistic claims using real-life observations. Secondly, such arguments can never be airtight: after all, how can we hope to infer the measure on a hypothetical multiverse, when we only ever experience <em>one</em> world? Indeed, a realist might question if it makes any sense to discuss the chances of an event happening: either it happens or it doesn’t!</p>



<p>You might be more trusting of someone who puts their money where their mouth is. To back up a definite claim, not involving chance, I can simply agree to pay a penalty if it turns out I’m wrong.</p>

<p>This idea can be extended to probabilistic claims in the following manner: consider a lottery that pays a $90 jackpot if the next roll of a pair of dice yields a nine. If the maximum that I’m willing to pay to play is $10, this indicates that I believe a not-nine roll is eight times more likely than a nine roll. This approach is appealing because, after all, the <em>raison d’être</em> of probability theory is to explain the decision-making of individuals facing uncertainty.</p>

<p>If another gambler’s view conflicts with mine, you may aggregate our beliefs by creating a market on which we buy and sell predictions. Consider a contract that pays $100 (plus interest) when a specified event occurs. Its price on the market can be interpreted as the percentage probability of that event. Thus, to say an event is twice as “likely” as another, simply means its market price is double.</p>

<p>Unlike your typical gambler, a frictionless market offers transparent near-identical buy and sell prices. As a result, any violations of Kolmogorov’s axioms become money-making arbitrage opportunities. Arbitrage activity acts as an enforcer of the axioms, creating what economists call the <em>risk-neutral probability measure</em>.</p>

<p>In real markets, however, this probability measure exhibits several inconsistenties. Firstly, it depends on which currency is used: as an extreme example, we wouldn’t buy a dollar-denominated wager that only pays out if the dollar collapses, no matter how likely we imagine the collapse to be. Secondly, this measure is sensitive to (non-diversifiable) risk: if a widely-believed prophecy held that rolling a nine would induce a catastrophic famine, the market would value this outcome a lot more, because everyone wants to buy insurance against such a catastrophe. Thirdly, markets can be misinformed: indeed, one motivation for participating in a market is to try to beat it! And finally, liquid markets are hard to set up.</p>

<p>For these reasons, we abandon this approach. We’ll seek to define probability in terms of actual outcomes instead of human bets. Nonetheless, human bets are what inspired the creation of probability theory: it’s hard to think of any other practical application! Therefore, we should remember to revisit the matter once we’ve found an appealing probability concept. Ultimately, we must be able to explain <em>how</em> individuals and markets behave with respect to our concept, and answer <em>why</em> they should care about it at all.</p>

<p>These questions are incredibly subtle: the theory of evolution by natural selection tells us that individuals are wired to use strategies that enabled their ancestors’ survival; however, the nature of probabilistic beliefs is that a wide range of outcomes are plausible. Indeed, while a coin will always land heads or tails, it’s considered unwise to bet your life savings on either heads or tails. Intuitively speaking, the rationale is that you’re almost certain to lose <em>eventually</em>, if you keep playing this way. This idea of repeated trials inspires our next interpretation, which happens to be the most popular among scientists.</p>



<p>According to the frequentist school of thought, a probabilistic statement is not to be taken literally. Although it refers to a single event, the statement should be taken as shorthand for a claim involving a very large collection of similar events. Imagine rolling the dice over and over. The probabilistic claim that we started with is converted into the following:</p>

<p>“<em>If a pair of standard dice are thrown repeatedly, then in the limit as the number of throws goes to infinity, the proportion of nines converges to one in nine (about 0.11 or 11%).</em>”</p>

<p>The short-run probability is replaced by a long-run proportion. Given an infinite sequence of rolls, this statement unambiguously reveals itself to be true or false. In light of the frequentist interpretation, we can even make more sense of our earlier interpretations. While we only experience one world, repeating an experiment under similar conditions is like observing the experiment in a parallel universe: whether we count trials or worlds, the math is virtually identical. In the limit of infinitely many bets, we can make some unambiguous conclusions about the quality of a gambler’s strategy, too: this is how casinos ensure that the house always wins!</p>

<p>Testing our claim is a simple matter: we roll the dice, over and over, and over and over… infinity times. Oops. Of course, there is no such thing as an experiment with infinity trials. Our arms will get tired, the dice will wear out, the Sun will explode, and all the free energy in the universe will be consumed. At best, we can do a very large number of trials. Let’s say we roll dice 9,000 times; one in nine of these would be 1,000. Perhaps we won’t roll exactly 1,000 nines, so let’s interpret our claim with a suitable margin of error, called a <em>confidence interval</em>:</p>

<p>“<em>If a pair of standard dice are thrown 9,000 times, then the face-up sides will sum to nine for between 920 and 1,080 of the throws.</em>”</p>

<p>The probability of obtaining between 920 and 1,080 nines can be calculated to be 99.3%.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2">2</a></sup> Thus, we’ve turned a probabilitic statement into a much more certain but still probabilistic statement. If we observe 1,100 nines, we should be able to dismiss the probabilistic claim as false. And yet, if every household on Earth were to independently perform this 9,000-throw experiment, we should expect that a great many of their results would fall outside the confidence interval. They would disagree on the truth of our statement!</p>

<p>There’s no getting around it: despite its intuitive appeal, the frequentist definition of probability is circular, reducing probability claims to probability claims.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3">3</a></sup> To end the cycle, the frequentist chooses a threshold (say, 99%) beyond which to treat events as objective truths. This grants the claim an empirically observable meaning. And yet, the frequentist must take care not to consider too many such events, for otherwise the probability of <em>at least one</em> of the events failing may ALSO exceed the threshold of certainty: a logical contradiction.</p>

<p>Things only work out nicely in the limit of infinite sample size. Statisticians mainly deal with experiments which can be repeated so many times that, for most practical purposes, their conclusions can be treated as definite. Non-philosophers are usually happy to ignore a sub-1% chance of error; and if that’s not good enough, make it 0.0001%! Confidence can be increased by gathering more data, i.e., increasing the sample size.</p>

<p>This approach turns out to be very powerful. By designing more complex hypotheses in which probabilities vary as a function of context variables, even some phenomena that aren’t easily repeatable can be statistically analyzed. For example, weather forecasts are based on well-tested models that use measurements of variables such as temperature, pressure, humidity, and wind.</p>

<p>On the other hand, statistical models of sports games, democratic elections, or company stocks tend to be less testable: the interactions are very complex and there are too few outcomes from which to extrapolate. Similarly, when you try to predict which colleges will admit you or which of your friends will start a business, you don’t make your case using repeatable tests. Clearly, the frequentist interpretation cannot apply. One may argue that no conclusions in these cases would hold up to a scientific standard; nonetheless, if we seek a theory of decision-making under uncertainty, there’s no …</p></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.arameb.com/blog/2020/11/22/probability">https://www.arameb.com/blog/2020/11/22/probability</a></em></p>]]>
            </description>
            <link>https://www.arameb.com/blog/2020/11/22/probability</link>
            <guid isPermaLink="false">hacker-news-small-sites-25237356</guid>
            <pubDate>Sat, 28 Nov 2020 13:23:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Always leave the code better than you found it]]>
            </title>
            <description>
<![CDATA[
Score 47 | Comments 70 (<a href="https://news.ycombinator.com/item?id=25237341">thread link</a>) | @hans1729
<br/>
November 28, 2020 | https://letterstoanewdeveloper.com/2020/11/23/always-leave-the-code-better-than-you-found-it/ | <a href="https://web.archive.org/web/*/https://letterstoanewdeveloper.com/2020/11/23/always-leave-the-code-better-than-you-found-it/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>Dear new developer,</p>



<p>I’ve spent a lot of my time maintaining working code. I think that is more typical of software developers than working in greenfield development. Yes, there are definitely jobs where you are writing more new code than maintaining, upgrading, bug fixing and improving old code (startups without product market fit being one, consulting being another) but in general code is expensive and folks want to run it for a long time. </p>



<p>Often you’ll jump into code to fix a bug, investigate an issue or answer a question.</p>



<p>When you do so, improve it. This doesn’t mean you rewrite it, or upgrade all the libraries it depends on, or rename all the variables. </p>



<p>You don’t need to transform it. </p>



<p>But you should make it better. Just clean it up a bit. Doing so makes everyone’s lives just a bit better, helps the codebase in a sustainable way, and assists the business by making its supporting infrastructure more flexible.</p>



<p>What are some ways to improve the code when you are in it?</p>



<p><strong>Document</strong></p>



<p>Whether that is a comment that explains something tricky, a larger piece of documentation external to the code which explains how to interact with it, or fixing a typo, trustworthy documentation is key to interacting with code. This is a good way to start improving a codebase because it has minimal impact on the actual code. Therefore it is low risk. But if you’ve ever had a great comment explain a confusing bit of code, you’ll appreciate the time this effort can save.</p>



<p>You can also help documentation by removing old, crufty docs. If you see a comment that doesn’t apply, remove it. If there’s cut and paste documentation which doesn’t apply, get rid of it. That cleans up the code for the next person to come along (who might be you).</p>



<p><strong>Write a test or improve a test</strong> </p>



<p>Tests help you write maintainable, extensible code that others can change fearlessly. If you run across code that isn’t tested and you have time and the supporting framework to write one, do so. </p>



<p>Even if it tests simple functionality such as “can I instantiate this object” or “how does this function react when I pass it two null values”, an additional test will help the robustness of the code. </p>



<p><strong>Refactor it</strong></p>



<p>This is one of the most flexible improvements. Refactoring code can range from renaming a variable to be more true to its nature to an overhaul of an entire module. Start small and don’t get wrapped up in perfection. Make the code clearer in intent. </p>



<p>It’s easy with refactoring to get wound around an axle and make too many changes and end up with broken things. Timeboxing is one technique I use to avoid, or at least minimize, my tendencies toward this when refactoring. If all I have is 30 minutes, I’ll make my changes smaller in scope.</p>



<p>A warning about refactoring. Don’t refactor what you don’t understand. Don’t drive by refactor. Discuss your plan with someone more familiar with the code; <code>git blame</code> is your friend. Especially if the code is not well tested, you want to make sure you don’t do more harm than good.</p>



<p><strong>Upgrade a dependency</strong></p>



<p>It’s sometimes a winding path, but upgrading your dependencies regularly is a good way to maintain the code. I remember working in a fork of struts. It was an important application for the company, but we didn’t spend the time upgrading the dependencies, because it was too painful. Eventually, parts of the code became harder to update. The entire application couldn’t benefit from newer technologies and paradigms because of the older dependencies holding it back. </p>



<p>It never feels good to spend time updating a dependency; to me this always feels like running in place. But if you don’t do so, eventually dependencies will end of life and you’ll be forced to update. That’ll be even less pleasant. </p>



<p><strong>How to do it</strong></p>



<p><em>Based on feedback (<a href="https://www.reddit.com/r/programming/comments/k2cbtb/always_leave_the_code_better_than_you_found_it/gdv0kg9/?utm_source=reddit&amp;utm_medium=web2x&amp;context=3">this comment</a>, among others), I added this section on Nov 28.</em></p>



<p>When you are making these changes to improve the code, you’ll help out code reviewers and your future self by making changes that are improving the code separate from changes that add functionality. Whether you do this in separate pull requests, tickets, or commits depends on your team culture. Ask about that. But such separation will make it easier for people who aren’t familiar with the changes to understand them and give feedback on them, whether that is a code review this week or someone reviewing this component two years from now.</p>



<p><strong>Why do it</strong></p>



<p>All of these actions not only help others because they improve the quality of the code, they also provide examples to other developers on how to do so. For example, it is far easier to write the second test in a suite than the first. You can cut and paste a lot of the setup code and tweak only what is different. The first bit of documentation will inspire more.</p>



<p>Code isn’t everything, but it is an important work output. Whenever you touch it, you should strive to leave it in a better place that it was before you did so.</p>



<p>Sincerely,</p>



<p>Dan</p>
	</div></div>]]>
            </description>
            <link>https://letterstoanewdeveloper.com/2020/11/23/always-leave-the-code-better-than-you-found-it/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25237341</guid>
            <pubDate>Sat, 28 Nov 2020 13:19:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Grep.app, a GitHub code search engine to search for code patterns and examples]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25237338">thread link</a>) | @hackerpain
<br/>
November 28, 2020 | https://grep.app/search?q=pattern | <a href="https://web.archive.org/web/*/https://grep.app/search?q=pattern">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://grep.app/search?q=pattern</link>
            <guid isPermaLink="false">hacker-news-small-sites-25237338</guid>
            <pubDate>Sat, 28 Nov 2020 13:19:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What were these Roman objects used for?]]>
            </title>
            <description>
<![CDATA[
Score 52 | Comments 37 (<a href="https://news.ycombinator.com/item?id=25237271">thread link</a>) | @jd115
<br/>
November 28, 2020 | http://www.celticnz.co.nz/Dodecahedron/Decoding%20the%20Druidic%20Dodecahedron1.html | <a href="https://web.archive.org/web/*/http://www.celticnz.co.nz/Dodecahedron/Decoding%20the%20Druidic%20Dodecahedron1.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
<p><strong>DECODING THE DRUIDIC DODECAHEDRON </strong></p>

<p><img src="http://www.celticnz.co.nz/OrgImages/Dodecahedron/Dodecahedron%20Types.jpg" width="900" height="438"></p>
<p><strong>These bronze artefacts are so-called <em>Roman</em> dodecahedra (plural of dodecahedron)  of which about 120 have been found in the Celtic countries of Europe. Strangely, none of these have been found in Italy, home turf of the Romans.</strong> <strong>To the left is seen a dodecahedron from the Römermuseum Schwarzenacker, Homburg, Germany collection. To the right is seen another pristine example from the Hunt Museum in Limerick Ireland </strong></p>

<p><img src="http://www.celticnz.co.nz/OrgImages/Dodecahedron/Dodecahedron%20Types3.jpg" width="935" height="438"></p>
<p><strong>To the left is a dodecahedron from Archaeological  Service Canton Aargau, Brugg, Switzerland (Vindonissa Museum). To the right is a dodecahedron from the Gallo-Roman Museum, Tongeren, Belgium. </strong></p>
<p>The basic design of these artefacts remains relatively constant (12 pentagonal faces with knop-shaped legs), however, there can be a range of different  incised designs in the layout of the faces<strong>. </strong></p>
<p>The actual function of these artefacts cannot be explained by our historians or archaeologists, although many theories abound,  none of which seem particularly convincing. Also, there is no  mention of these items in old historical records of the Romans or those of anyone else for that matter. They are such an enigma that academics have pretty-much come to the defeatist conclusion that <em><strong>"we will never know what these dodecahedrons were used for". </strong></em></p>
<p>In reviewing the academic literature, one begins to see why the mystery will probably never be solved within that community. Here are some of the impediments and pitfalls our experts create for themselves:</p>
<p>All archaeological measurements are given in metric increments (centimetres or millimetres), which is a modern system that was created as much as 2000-years after some of these dodecahedra were cast in bronze. Earlier exemplars in stone or wood could predate the latter, more refined ones, by yet another thousand years or more. This  metric measurement preoccupation renders the  encoded numbers invisible or unrecognisable. Our academics know a large swathe of the ancient measurements that were in use two thousand years ago or before, so why not apply them to this study? </p>
<p>Most assuredly, the various sized holes in the 12-faces, the concentric circles that circumnavigate them, as well as the additional incised lines or dotted circles, etc, infer that there is something vividly measurable going on and it's evident that each hole or ring  has been very purposely fabricated to represent a known, precise and sought-after measurement.</p>
<p><img src="http://www.celticnz.co.nz/OrgImages/Dodecahedron/Calibrated.jpg" width="900" height="476"> </p>
<p><strong>A number of dodecahedra have calibration marks along their edges or around the holes in their faces. To the left is seen a calibrated dodecahedron from Hereford Museum, Kenchester and to the right is seen another from Goodrich Castle. Yet another one found in Wales shows etched calibration marks. </strong></p>
<p>In their reports,  archaeologists will often round out measurements of the holes in the dodecahedron faces to the nearest millimetre only, which is grossly insufficient. It's much appreciated by researchers when measurements can be supplied to 1/10th  of a millimetre, but  it is nigh on impossible  to acquire more refined measurements undertaken with electronic vernier callipers. Precise measurements and scaling must precede any serious analysis of these artefacts, but our experts don't seem to think there would be anything significant to find anyway, so don't subject the dodecahedra to rigorous measurement analysis.</p>
<p>Despite the fact that 25.4 millimetres (1 British Standard Inch) or fraction expressions of the same will recur repeatedly in the faces of dodecahedra, no-one seems to have arrived at the conclusion that the enigmatic<em><strong> "inch"</strong></em> recurrence is worthy of serious investigation. </p>
<p>Almost anything considered high-art, technically advanced or an engineering feat within ancient European civilisations seems to be automatically attributed to the Romans, which is certainly the case for these dodecahedra artefacts. This  is a hangover of biased, classicist-isolationist historical  interpretations of  European history where, first came the  Roman conquerors, who finally retreated after centuries of domination, only to be replaced by Rome's State church. With the pedigree of the church being Greco-Roman it was in their interests to push the concept that the Roman armies had found everything in the regions they invaded to be backwards, rudimentary or crude until Rome delivered the great-unwashed masses out of their depravity and into  enlightenment and high-civilisation. </p>
<p>This general process of misattribution further hobbles or seriously limits proper investigation into the purpose and function of dodecahedra artefacts, inasmuch as focus is  directed, almost exclusively, to what Roman armies might have used them for as range-finders or other battle related paraphernalia, including club-heads. The druids or others don't get  any serious consideration, despite the fact that they are historically recorded as having been  advanced mathematicians and astronomers.</p>
<p>Within controlled academia the ability to do open-ended, ground-breaking research can be severely curtailed by the requirements of <em><strong>"peer review"</strong></em> by colleagues.  This is largely a racket that ensures one doesn't move too far from the protected consensus opinion upon which academic reputations are built and anyone venturing too far out into left field is considered a maverick who threatens the insulated central body.</p>
<p>By the same token academia, these days, is lumbered with deep-set social responsibilities and crippling political agendas. When it comes to European history, especially, the requirement is to be self-effacing, while lauding and applauding the (often meager) accomplishments of other ethnicities. Under  guises of political correctness and racial sensitivity, the door is opened for antagonistic non-European, prejudiced individuals, to write and interpret  our history for us, while pushing their own  cultural-Marxist or similar wheel-barrows.  All one has to do is sit through lectures in our western universities to see just how far out of kilter the abysmal problem of  European history-misrepresentation has become.</p>
<p>But, with regards to the dodecahedra artefacts,what one person can build, another can generally duplicate or back-engineer, sufficient to make sense of what's going on. </p>
<p>So, let's cut to the chase, bypass all the  confusion of modern-day naysayers and source testimony directly from an observer who lived contemporary to the druids, before they fell under Roman domination </p>
<p>Julius Caesar, who was a very thorough historian, writes the       following regarding the late era druids of his time (circa 55 BC) and        practices within their many universities in Britain, where students from Gaul       and elsewhere, including Rome, went for training:</p>
<p><strong>'They hold aloof from war and do not pay war taxes; they       are excused from military service and exempt from all liabilities. Tempted       by these great advantages, many young men assemble of their own motion to       receive their training, many are sent by parents and relatives. Report says       that in the schools of the Druids they learn by heart a great number of verses,       and therefore some persons remain twenty years under training'. </strong></p>
<p><strong>'They do not think it proper to commit these utterances to       writing, although in all other matters and in their public and private accounts       they make use of Greek characters. I believe that they have adopted the practice       for two reasons- that they do not wish the rule to become common property,       nor those who learn the rule to rely on writing and so neglect the cultivation       of memory; and, in fact, it does usually happen that the assistance of writing       tends to relax the diligence of the student and the action of memory...They       also lecture on the stars in their motion, the magnitude       of the Earth and its divisions, on natural history, on the power and       government of God; and instruct the youth in these subjects' <em>(see De Ballo       Gallico, VII, 15, 16.).</em></strong></p>
<p>Historian, Isabel Hill Elder  writes, <strong>'The students at these       colleges numbered at times sixty thousand of the youth and young nobility       of Britain and Gaul. Caesar comments on the fact that the Gauls sent their       youth to Britain to be educated...It required twenty years to master the complete       circle of Druidic knowledge. Natural philosophy, astronomy,       mathematics, geometry, medicine, jurisprudence, poetry and oratory       were all proposed and taught-natural philosophy and       astronomy with severe exactitude' (Elder refers to <em>Strabo I IV,       page 197. Caesars Comm. Lib V. Sueotonius, V Calegula. E. Campion, Accounts       of Ireland, pg. 18.).</em></strong></p>
<p>Isabel Hill Elder further writes,<strong> 'The education system       adopted by the Druids is traced to about 1800 BC when Hu Gardarn Hysicion       (Isaacson), or Hu the Mighty, led the first colony of Cymri into Britain from       Defrobane, where Constantinople now stands'.</strong> </p>
<p>Further commenting on Hu       Gardarn Hysicion, Isabel Hill Elder writes that he, <strong>'is commemorated in       Welsh archaeology as having made poetry the vehicle of memory'.</strong> Elsewhere       she writes, he <strong>'is said to have mnemonically systematized the wisdom of       the ancients...'.</strong> She goes on to say,<strong> 'The published compositions of       the Druids and Bards form but a very small portion of the extant remains of       their works. The Myvyrian MSS. alone, now in the British Museum, amount to       47 volumes of poetry, in 1600 pages, besides about 2000 epigrammatic stanzas.       Also in the same collection are 53 volumes of prose, in about 15,300 pages,       containing many curious documents on various subjects...' <em>(see Celt, Druid       and Culdee, pages 54 &amp; 55).</em></strong></p>
<p>With the testimony of Julius Caesar ringing in our ears, lets see how the druids would have used the dodecahedron artefacts as <em><strong>"memory"</strong></em>devices for mathematically encoding astronomical cycles,<em><strong> ("the stars in their motion")</strong></em>, navigation systems <em><strong>("the …</strong></em></p></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://www.celticnz.co.nz/Dodecahedron/Decoding%20the%20Druidic%20Dodecahedron1.html">http://www.celticnz.co.nz/Dodecahedron/Decoding%20the%20Druidic%20Dodecahedron1.html</a></em></p>]]>
            </description>
            <link>http://www.celticnz.co.nz/Dodecahedron/Decoding%20the%20Druidic%20Dodecahedron1.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25237271</guid>
            <pubDate>Sat, 28 Nov 2020 13:03:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Top Linux (command line interface) CLI tools]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25236863">thread link</a>) | @Vlad81b
<br/>
November 28, 2020 | https://www.vladimircicovic.com/2020/11/top-linux-command-line-interface-cli-tools | <a href="https://web.archive.org/web/*/https://www.vladimircicovic.com/2020/11/top-linux-command-line-interface-cli-tools">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p>Working in a Linux environment requires knowledge of Linux cli tools and troubleshooting. In this article, it would
be presented CLI tools that are most important for troubleshooting.
Short story
Let say you are working with Linux and your work would be: 1% once-time setup and 99% troubleshooting. So as we can see from these homemade statistics you going to spend most of your time in CLI and finding “why does not work”. This is not a trivial task. It requires knowledge of how Linux works, how subsystems of Linux works,
how complete “line” from typing command up to running, delivering some results works, logs.
Yes, logs are the most important.
So let me start with naming tools and then a short description of some of them.
fs2chk
du/dh
strace/ltrace
lsof
ldd
tcpdump
netstat
mailq/showq
traceroute/tracepath
ping/telnet/nc
dig
curl
nmap
top
ps
pkg manager(rpm, yum, apt, others)
lsmod
awk
sed
vim</p>
<h3>fs2chk</h3>
<p>Used for file system integrity check. Not all time you will have regular shutdown or reboot –
sometimes it happens power goes off and your server gets back with a file system issue. Usually, in that
case is best to use fs2chk</p>
<h3>du/dh</h3>
<p>Command du is used to show space usage per partition. Sometimes happen your partition are used with
some dumb files/logs (crash files) and you need to see what is happening because application or service you try to run report free space issue. Command dh is used to check directory with sub-
directories and discover what file occupied most of the space.</p>
<h3>strace/ltrace</h3>
<p>Running application it just stops at some point. You don’t have logs. Nothing. And there is no debug switch (for example ssh -vvv, where you can see all steps that are done) or any other way to see what is hell going on. So strace is for functions that is used and ltrace are library that are used at some point.
We mostly need trace tool to see details of operation for some applications. Example: we run applications or services and we see an issue and we are not sure why.</p>
<h3>lsof</h3>
<p>You have an issue removing files/files because they are used by some unknown application. Or you want to see how much is open files so you can see if the maximum limit for open files is reached.</p>
<h3>ldd</h3>
<p>When the application does not run at all – the usual suspect is missing the library. This tool is handy to discover which library missing.</p>
<h3>tcpdump</h3>
<p>Connection to server sometimes has issues. So the best way to check what is going on and to troubleshooting is tcpdump. You can pick up the interface, type of protocol, from/to IP, or any low level for all TCP/IP layers.</p>
<h3>netstat</h3>
<p>Simple to see the status of open ports, connection state, and other information that we need to see if services running properly on a given port.</p>
<h3>mailq/showq</h3>
<p>The most vital service in each company is email. And sometimes you need to see what is happening with email (sending or recv). Best tool for this is mailq/showq (it is the same tool, showq is a new one that replace
mailq on older Linux)</p>
<h3>ethtool</h3>
<p>On a low level could happen issue with our ethernet connection and we want to review our cable/port on Linux. So the tool is best for this job. Beside this, you can review other specific ethernet things (auto-negotiation etc)</p>
<h3>traceroute/tracepath</h3>
<p>Ideal to see between server and client if there is a network path issue as also a delay between them.</p>
<h3>ping/telnet/nc</h3>
<p>The very handy tool on the first step to see if is server up as also services.</p>
<h3>dig</h3>
<p>With this tool, you can perform all DNS troubleshooting. Review MX records, A, NS, etc.</p>
<h3>curl</h3>
<p>One of the best tools for troubleshooting different protocols: HTTP, RTMP, FTP, etc. It also has a benchmark integrated for a view of response (DNS, first byte, etc).</p>
<h3>nmap</h3>
<p>A very good tool for discovering services, open ports, and other useful information. Also, you can use on your servers to check if there is some unusual thing and secure them.</p>
<h3>top</h3>
<p>Active process list with memory, CPU, parent/child connection, and other information that helps to see where the issue starts.</p>
<h3>ps</h3>
<p>Process list, you can check and see what is currently running (very quick, the first step for troubleshooting)
pkg manager(rpm, yum, apt, others)
In troubleshooting we need to verify or to find some library or application – so this tool is best for that
operation.</p>
<h3>lsmod</h3>
<p>People who never have issues with kernel modules would never use this tool or get this tool seriously.
The tool provides information about loaded kernel modules as also usage, memory, etc.</p>
<h3>awk/sed/grep</h3>
<p>In a bunch of logs sometimes is a need to find proper information. All these tools are swiss knives for bash scripting and handy for parsing logs for specific information.</p>
<h3>vim</h3>
<p>The best editor in the world. Learn so you can answer on an interview how to quit vim.</p>
</div></div>]]>
            </description>
            <link>https://www.vladimircicovic.com/2020/11/top-linux-command-line-interface-cli-tools</link>
            <guid isPermaLink="false">hacker-news-small-sites-25236863</guid>
            <pubDate>Sat, 28 Nov 2020 10:52:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[First made-in-China nuclear reactor online]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25236824">thread link</a>) | @chefkoch
<br/>
November 28, 2020 | https://www.bangkokpost.com/world/2026891/first-made-in-china-nuclear-reactor-online | <a href="https://web.archive.org/web/*/https://www.bangkokpost.com/world/2026891/first-made-in-china-nuclear-reactor-online">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://www.bangkokpost.com/world/2026891/first-made-in-china-nuclear-reactor-online</link>
            <guid isPermaLink="false">hacker-news-small-sites-25236824</guid>
            <pubDate>Sat, 28 Nov 2020 10:43:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Beating the Markets with Artificial Intelligence Driven Portfolios]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 3 (<a href="https://news.ycombinator.com/item?id=25236677">thread link</a>) | @hydershykh
<br/>
November 28, 2020 | https://tradytics.com/blog/beating-the-market-with-ai-driven-portfolios | <a href="https://web.archive.org/web/*/https://tradytics.com/blog/beating-the-market-with-ai-driven-portfolios">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                <div>
                    <div>
                        <article>
                            <div>
                                
                                <p>

Quantitative finance is an inherently secretive field where people who become profitable never reveal their secret formulas of success. However, this has started to change in recent years thanks to the likes of <a href="https://www.amazon.com/Advances-Financial-Machine-Learning-Marcos/dp/1119482089">Marcos Lopez de Prado</a>, <a href="https://www.amazon.com/Algorithmic-Trading-Winning-Strategies-Rationale/dp/1118460146">Ernest Chan</a>, and others. Although algorithmic trading remains a difficult problem to solve, the knowledge that these people have given to the public is invaluable. At Tradytics, we are continuously reading the latest research and trying new ideas to create profitable trading algorithms. <b>Today, we are going to talk about one such AI system that has resulted in us beating the market by 20% in the last 1 month</b>. Although this time period is very short, we want to give others a taste of how things work at Tradytics and how we go about solving different problems. We are also going to continue creating automated systems with an intent to consistently beat the market. <b>Let us now talk about how we created 5 AI driven portfolios that each gained over 30% returns in the last 30 days.</b>

</p>





<h3> A Million Portfolios </h3>

<p>

Let's start with a basic idea of what we do before diving into the details. 

</p><blockquote>

	<p>We generate millions of random portfolios, optimize their allocation using existing and proprietary portfolio optimization algorithms, and rank them using our AI engine.</p></blockquote>



<p>There has been thousands of papers on portfolio optimization in the last three decades. Starting with <a href="https://www.investopedia.com/terms/m/modernportfoliotheory.asp">markowitz portfolio optimization theory</a> to the more recent <a href="https://www.coursera.org/learn/trading-strategies-reinforcement-learning">reinforcement learning based optimization</a> methods, there are countless research papers to read and implement. However, as is the case with the majority of proposed methods in literature, the portfolios are created based on historical returns. If there is one thing we know about the stock market, it's that historical returns are not always representatives of future returns. This is a major problem that one needs to solve in order to create effective portfolios. As market regimes change, algorithms that were based on historical data suddenly become ineffective. Therefore, we need a way to combine the literature with a novel mechanism of adding some predictive power to our portfolio optimization strategies.</p>





<h4> Portfolio Optimization Strategies at Tradytics </h4>

<p>

	At Tradytics, we use 3 strategies from literature and two proprietary ones based on genetic algorithms. These strategies are:

	</p><ul>

		<li><a href="https://www.thebalance.com/minimum-variance-portfolio-overview-4155796">Minimum Variance Portfolio (MVP)</a></li>

		<li><a href="https://logical-invest.com/app/portfolio/maxsharpe/max-sharpe-portfolio">Maximum Sharpe Portfolio (MSR)</a></li>

		<li><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3070416">Eigen Portfolios</a></li>

		<li>Genetic Portfolios (GA)</li>

		<li>Exponential Genetic Portfolios</li>

	</ul>

	<p>MVP, MSR, Eigen, and GA based portfolio optimizations have been studied in the literature in depth and many trading firms still use them. This is where we start as well but we add something important on top to make these strategies work well. As we said in the start of this section, our basic idea is to generate millions of random portfolios and rank them using our custom AI algorithms. The ranking system is where our novelty comes in. The following image visually explains our portfolio selection process.</p>

	<p> 

	<img src="https://i.ibb.co/m4N3gB4/block-diagram.png"></p><p>After generating millions of portfolios and optimizing their weights with the aforementioned strategies, we use our proprietary ranking system that ranks all these portfolios in terms of their future predicted returns. We keep the best portfolio for each of the five strategies which goes to our website at <a href="https://tradytics.com/ai-portfolios.">Tradytics AI Portfolios</a>. Let us now dive a bit deep into the portfolio generation and selection process.</p>





<h4> Portfolio Generation: A Case of 25 Random Portfolios </h4>

<div><p>

	The image below shows 25 random portfolios from a collection of millions of portfolios we generated on October 31st. We start with selecting 5 random stocks for every portfolio and optimize the portfolio on 1 year of historical data up till October 23rd. After optimization, we backtest the portfolios to look at their historical returns. Since optimization is being done on historical data, the backtested returns are expected to be high which is what we can see in the image below.

	</p><p> 

	<img src="https://i.ibb.co/gVM2VrD/backtest-returns.png"></p><p> 

	However, there is a huge problem here. Our optimization procedure only looks at historical data and has no inherent predictive power. It implicitly makes an assumption that historical returns are predictive of future returns. This is not always true and solely relying on this assumption can lead to huge losses. We can see this if we forward test our 25 portfolios from October 23rd to October 31st, 2020. 

	</p><p> 

	<img src="https://i.ibb.co/CQmyFQY/forward-returns.png"></p><p> 

	Although profitable on historical data, the portfolios are all at loss when ran live - some are down 20% in a week. This illustrates the problem at hand with portfolio optimization methods. Let us try to solve it.

</p></div>





<h3> Portfolio Selection: Tradytics AI Ranking </h3>



<p>

At Tradytics, we go one step further from portfolio optimization. Once we get these millions of optimized portfolios, we use our proprietary AI ranking algorithm to rank them based on what the AI thinks will be their future returns. The ranking procedure is basically a predictive model that predicts the returns of portfolios based on their allocation by the optimization method, their spreads with each other, and their historical returns. Once the ranking is done, we simply pick the best portfolio from each of the five strategies noted above, thus giving us 5 portfolios every month. Our main goal with these portfolios is to generate large returns and consistently beat the market. In order to preserve our alpha, we will not give any technical details away regarding our ranking system.

</p>



<div><p>

The first set of portfolios we generated was on October 31st. Our top 5 portfolios had the following allocations and stocks in them. The green weights indicate longing the stock while the red weights suggest shorting.

</p><p> 

<img src="https://i.ibb.co/kqn36Gv/allocation.png"></p></div><p>

At the time of creating these portfolios, the weights did not make much sense by simply looking at each individual stock. However, since the ranking engine has been trained on a large amount of data and has high predictive power, it extracted certain patterns that made it confident that these portfolios would end up being profitable. Let us take a look at the returns of each individual stock as well as the entire portfolio.</p>





<h4> Portfolio Results: Top 5 Portfolios from October </h4>

<div><p>When looking at the backtest results of the top portfolios, it is easy to see that the historical returns are quite volatile would yield a low sharpe ratio. However, since history is not always the future and our ranking engine is tasked to predict the future returns, these portfolios were selected because of high AI confidence in larger returns.

</p><p> 

	<img src="https://i.ibb.co/Mcgj8HV/picked-backtest-returns.png"></p><p> 



When these 5 portfolios were run live for the month of November, these yielded very high returns as compared to the market. The <b>$SPY</b> index gained about <b>10%</b> in the month of November. The following image shows the gains of each of our portfolios.</p><p> 

<img src="https://i.ibb.co/PTPnqcj/portfolio-returns.png"></p></div><p>The results are quite impressive. All portfolios have garnered gains of above <b>30% in just 30 days</b> with some of them touching about <b>40%</b> cumulative returns. Now, we admit that there is a bias in the results here because of the strong bull market in the month of November. However, when the AI was ranking these portfolios, the bull market was not very strong - <b>$SPY</b> was down <b>2%</b> in October. It was the ranking engine that was able to find predictive patterns in the portfolios which would eventually result in large profits. This demonstrates the effectiveness of using machine learning and artificial intelligence in portfolio selection.</p>





<h3> What's Next </h3>

<p>

Tradytics is a fairly new company in the quantitative finance game. We realize that these are short term results and we need to show consistency before we can make any claims about the AI capabilities of our toolkits. Our plan is to keep adding these portfolios every month and record the performance for atleast one year. The hope here is to consistently beat the market with significantly high returns. We will keep you updated. If you have any questions, please do not hesitate to reach out to us at our <a href="https://discord.gg/QuvE2Z8">Discord</a>.

</p>


                            </div>
                        </article>
                    </div>
                    <!-- end of single card col-->
                </div>
                <!-- end of blog post row -->
            </div></div>]]>
            </description>
            <link>https://tradytics.com/blog/beating-the-market-with-ai-driven-portfolios</link>
            <guid isPermaLink="false">hacker-news-small-sites-25236677</guid>
            <pubDate>Sat, 28 Nov 2020 10:09:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Interpretable Machine Learning]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25236675">thread link</a>) | @e2e4
<br/>
November 28, 2020 | https://christophm.github.io/interpretable-ml-book/ | <a href="https://web.archive.org/web/*/https://christophm.github.io/interpretable-ml-book/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        

        <div tabindex="-1" role="main">
          <div>

            <section id="section-">

<div id="summary">

<p><img src="https://christophm.github.io/interpretable-ml-book/images/title_page.jpg" width="500"></p>
<p>Machine learning has great potential for improving products, processes and research. But <strong>computers usually do not explain their predictions</strong> which is a barrier to the adoption of machine learning. This book is about making machine learning models and their decisions interpretable.</p>
<p>After exploring the concepts of interpretability, you will learn about simple, <strong>interpretable models</strong> such as decision trees, decision rules and linear regression. Later chapters focus on general model-agnostic methods for <strong>interpreting black box models</strong> like feature importance and accumulated local effects and explaining individual predictions with Shapley values and LIME.</p>
<p>All interpretation methods are explained in depth and discussed critically. How do they work under the hood? What are their strengths and weaknesses? How can their outputs be interpreted? This book will enable you to select and correctly apply the interpretation method that is most suitable for your machine learning project.</p>
<p>The book focuses on machine learning models for tabular data (also called relational or structured data) and less on computer vision and natural language processing tasks. Reading the book is recommended for machine learning practitioners, data scientists, statisticians, and anyone else interested in making machine learning models interpretable.</p>
<p>You can buy the PDF and e-book version (epub, mobi) <a href="https://leanpub.com/interpretable-machine-learning">on leanpub.com</a>.</p>
<p>You can buy the print version <a href="http://www.lulu.com/shop/christoph-molnar/interpretable-machine-learning/paperback/product-24036234.html">on lulu.com</a>.</p>
<p><strong>About me:</strong> My name is Christoph Molnar, I'm a statistician and a machine learner. My goal is to make machine learning interpretable.</p>
<p>Mail: <a href="mailto:christoph.molnar.ai@gmail.com">christoph.molnar.ai@gmail.com</a></p>
<p>Website: <a href="https://christophm.github.io/">https://christophm.github.io/</a></p>
<p>Follow me on Twitter! <a href="https://twitter.com/ChristophMolnar">@ChristophMolnar</a></p>
<p>Cover by <a href="https://twitter.com/YvonneDoinel">@YvonneDoinel</a></p>
<div>
<p><img src="https://christophm.github.io/interpretable-ml-book/images/by-nc-sa.png" alt="Creative Commons License"></p><p>Creative Commons License</p>
</div>
<p>This book is licensed under the <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.</p>
</div>
            </section>

          </div>
        </div>
      </div></div>]]>
            </description>
            <link>https://christophm.github.io/interpretable-ml-book/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25236675</guid>
            <pubDate>Sat, 28 Nov 2020 10:08:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Permissionless Apprenticeship]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25236350">thread link</a>) | @vitabenes
<br/>
November 28, 2020 | https://www.value.app/feed/permissionless-apprenticeship-ryan-doyle | <a href="https://web.archive.org/web/*/https://www.value.app/feed/permissionless-apprenticeship-ryan-doyle">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p><em>Guest author: </em><a href="https://twitter.com/ryan___doyle"><em>Ryan Doyle</em></a><em>.<br></em></p><p>I grew up on a crop farm in the Finger Lakes of New York. When I was eighteen, I had the opportunity to go to school in Los Angeles. For the first time since my great grandmother came here from Ireland, a Doyle left the “middle finger” of NY.<br></p><p>For four years, I studied “business.” For someone used to working with their hands, it was an uncomfortable amount of theory. Outside of my classes, I was tinkering. Projects like a bitcoin brokerage, seeking a patent for a garden tool, and a few ecommerce shops became my bridge between theory and action.<br></p><figure id="w-node-301981ba2eb8-f921d246"><p><img src="https://uploads-ssl.webflow.com/5f4c1c4bc17267761b21d253/5fbec1e09b509d00db535009_Zenb_bZC0Ru9U1T6r_e18Btl3jkmLftJ5wgKpgveNW5hxfeWyNreW-f9lDxexy1XCN_Q-RJ5LmUxnsx1QJTPUOIRdHby5cpAnQQO_rxLyoF89Ns5ZhBqe2pX3vLwY4u9Sw.jpeg" alt=""></p></figure><p>Quickly, building became my passion. As I graduated, I joined a software startup in Palo Alto to smile-and-dial as an entry-level sales person. I figured I’d see how these companies were built from the inside out so I could emulate it someday.<br></p><p>Hustle and elbow grease took me from that role, to another startup in NYC, where I helped build the sales team. I went from phone jockey to deal-closer. On my nights and weekends, I was teaching myself to code.</p><p><br>One of those nights, I found Visualize Value on Twitter. The simple theories resonated with my action-oriented mindset. In particular, the idea of a <strong>Permissionless Apprenticeship</strong>. (<a href="https://twitter.com/jackbutcher/status/1261139777061113858?lang=en">link to tweet thread</a>)<br></p><p>This was something I was already doing in sales. I’d see a company that might be worth a few million dollars to the company. I’d live their brand for a few days, experiencing everything I could. Then I’d compile it and send it to the highest executive I could find, showing the gap we could fill.<br></p><h3>It had the highest return on time of anything I did. <br></h3><p>In the VV community of designers and makers, I accidentally found out that this was valued by anybody in business, not just Fortune 500 companies. I started doing “Free Sales Advice Friday,” and business owners would share what they were working on. I would reply with scrappy tactics to go out and find new customers, specifically for their business. </p><figure id="w-node-1ed3a476e59c-f921d246"><p><img src="https://uploads-ssl.webflow.com/5f4c1c4bc17267761b21d253/5fbec238bc87551cc41f9091_SBYXJoLLf8A93EpVRlXUzHAg9WB-jV66ry6jmT-imj15yB8FWSyI0I30xN_Vt3e7QVL4i0yxUoIDuQ_3pjeU_-buVwkKgWt-IVLB_ZmEOlzI91RTg3W6cdmATD8V6XMqsA.png" alt=""></p></figure><p>‍</p><p><em>"Some of the best help I've received online, thank you."&nbsp;—&nbsp;Jordan Godbey</em></p><p><em>"That is superb advice." —&nbsp;Andy Whyte</em></p><p><em>"Ryan, this thread is gold!"&nbsp;—&nbsp;Ben Ford<br>‍</em></p><p>I didn’t think too much of the feedback and kept working on my sales job. But I knew, mentally and emotionally, I was ready to make the leap into something of my own. I could code. I could sell. The pandemic provided the perfect reason to hide away for a year and hustle, nobody was hanging out without me anyhow.<br></p><p>In August, I quit my job, moved back to the farm to extend my runway, and committed myself to building. I felt like I had the skills I needed. The community was there through my sales work. I hadn’t realized it yet, but the community was there through Visualize Value as well.</p><p>I hoped salespeople would care about what I was building. <strong>As I applied my sales advice to my own software project, I found that more builders cared about how I approached sales. </strong>In months of Free Sales Advice Fridays, I advised 100+ business owners and explored so many different angles they could take.<br></p><p>There was already sales advice out on the internet. But the message didn’t matter to these people, the medium did. And I was the medium. To tap into that I built a small guide on pre-selling your product to customers and tested the offer of a premium newsletter. Then this happened:<br></p><figure id="w-node-b130da7c6edb-f921d246"><p><img src="https://uploads-ssl.webflow.com/5f4c1c4bc17267761b21d253/5fbec29f91c0ddef19a5840c_vO6r4tsysvvlePHfASruxWYEHU110pu8Z7TTAT-rcyVbeRxEeKK0YK0RFG3nqC-uqzIJA39A8_2JPvM3foisdyBXsYbgPiWzhz8M4AdWzt52IJ_F8zh6uK8xqSiwiZCEDw.png" alt=""></p></figure><p>On day 1, I hit $170 MRR. More revenue than I was making from my software project. Almost entirely profit. From what was already in my head. It was astonishing to me, that something so innate to me was worthy of an individual’s money.<br></p><h3>By sharing that innate knowledge through Free Sales Advice Friday, I was really embodying the permissionless apprenticeship. Every week I was able to hone my message in a zero-stakes, zero-permission environment. <br></h3><p>If I hadn’t sought out ways to provide free value, I wouldn’t have realize what people would pay me for. By the time I discovered what people wanted to pay for, I had built up enough “brand equity” for early adopters to trust me.<br></p><p>I’m still building my software projects. But my early adopters for my sales guides will allow me to do so with an audience who will test and support me, while paying me for the opportunity to do so.<br></p><p>I still do Free Sales Advice Fridays and stay active in the <a href="https://shop.visualizevalue.com/products/membership">Visualize Value Community</a>. When you join, look out for the weekly post in #general. Looking forward to giving you sales advice, too :)</p></div></div></div>]]>
            </description>
            <link>https://www.value.app/feed/permissionless-apprenticeship-ryan-doyle</link>
            <guid isPermaLink="false">hacker-news-small-sites-25236350</guid>
            <pubDate>Sat, 28 Nov 2020 08:54:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Generalized f-Mean]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25236328">thread link</a>) | @goldenkey
<br/>
November 28, 2020 | https://churchofthought.org/blog/2017/03/26/the-generalized-mean-an-algorithmic-approach/ | <a href="https://web.archive.org/web/*/https://churchofthought.org/blog/2017/03/26/the-generalized-mean-an-algorithmic-approach/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

		<div>

			<p>In school, we are first taught the arithmetic mean, then later the geometric mean, and maybe one or two others like the harmonic/contraharmonic mean.</p>
<p>But most teachers do not teach the logic or derivation behind these means. They merely provide the formulas and perhaps provide some scenarios where each mean would be more useful than the others.</p>
<p>I’d like to illustrate the definition of a mean, in the most abstract sense, from a programmer’s perspective, an algorithmic point of view.</p>
<h2>A Balancing Act</h2>
<hr>
<p>Suppose we have a list of N numbers, and a binary operation $f(a,b)$</p>
<p>If we apply that binary operation to one number from the list, we must&nbsp;apply its inverse operation to another number from the list.</p>
<p>We can continue applying $f$ to one element then $f^{-1}$ to another element as many times as we need to, until every number in the list is equal. That number that all elements are equal to…is the mean!</p>
<p>Means are just balancing acts!</p>
<p>** I am aware we haven’t specified the most&nbsp;optimized way to perform this algorithm to ensure it halts.</p>

<h2>Example</h2>
<hr>
<p>let our list be $(2,4,6)$ and our $f(a,b)=a+b$</p>
<p>then we apply the operation and its inverse to two elements respectively:</p>
<p>$(f(2,2), 4 ,f^{-1}(6,2)) = (2+2, 4 ,6-2) =&nbsp;(4,4,4)$</p>
<p>So our mean is 4.&nbsp;We just performed the&nbsp;<em>arithmetic mean</em>!</p>

<h2>What is it good for?</h2>
<hr>
<p>Well, given ANY invertible&nbsp;symmetric binary function, we can define a mean and derive its “shortcut” formula!</p>
<p>Here are the functions that define the means we usually come across:</p>
<p>arithmetic mean: $f(a,b) = a+b$<br>
geometric mean: $f(a,b) = ab$<br>
geometric mean (alternative definition): $f(a,b) = ln(a) + ln(b)$<br>
harmonic mean: $f(a,b) = 1/(1/a + 1/b)$</p>

<h2>Favoring Large or Small Numbers</h2>
<hr>
<p>Now, most of us learn that geometric mean tends to favor smaller&nbsp;numbers.</p>
<p>For example, the arithmetic mean of 1&nbsp;and 100 is $50\frac{1}{2}$. But the geometric mean of 1 and 100 is 10.</p>
<p>Knowing what we do now, can we create a mean that favors smaller numbers&nbsp;even more?</p>
<p>I don’t know if these already exist in the formal mathematics universe, ie. books or published papers. So let me know if you find a previous name for them!</p>
<p>Franken-mean: $f(a,b) = a^b b^a$</p>
<p>Franken-mean v2: $f(a,b) = a^b + b^a$</p>
<p>You might ask if we can skew our mean toward smaller numbers&nbsp;even more than our above exponential mean. We sure can. Using something called Tetration, simply put – it is iterated exponentiation. There are even operations that encompass iterated tetration and so on…these kind of operations are called&nbsp;hyperoperations. So we can make a mean skewed toward lows or highs as much as we want, and it’s beautiful isn’t it? Before different means are taught, the theory of what defines a mean should really be taught.</p>
<p>But before I forget,&nbsp;you must be saying: “Where’s my shortcut formulas?!!”</p>
<p>Okay okay.</p>

<h2>Deriving the Closed Formulas</h2>
<hr>
<p>Let’s derive the arithmetic mean first. Assume we have a list of 4 elements, ${x,y,z,t}$</p>
<p>Given the binary symmetric&nbsp;function that defines the arithmetic mean $f(a,b)=a+b$,</p>
<p>we let $g(a) = f(a,f(a,f(a,a))) = a + a + a + a = 4a$</p>
<p>Our formula for the mean of the 4 numbers in our list will be:</p>
<p>$g^{-1}(f(x,f(y,f(z,t))))$</p>
<p>We know $g(a) = 4a$, so $g^{-1}(a) = a/4$</p>
<p>That means our mean is $(x+y+z+t)/4$, which is what we expected from the arithmetic mean</p>
<p>Above was just a demonstration for a fixed size list of 4 elements. The concept is the same for any sized list. We can use induction to extrapolate what the formula will be for a list of n&nbsp;elements, $(x+y+z+t…….+w)/n$</p>

<h2>Mathematica code for the generalized mean</h2>
<hr>
<pre id=""><code>GeneralizedMean[l_List, f_Function] := (
InverseFunction[
(Fold[f, ConstantArray[#, Length[l]]]) &amp;
] [Fold[f, l]]
);
</code></pre>
<ul>
<li>Arithmetic Mean
<pre id=""><code>In:= GeneralizedMean[{x,y,z,t},(#+#2)&amp;]
Out= 1/4 (t+x+y+z)</code></pre>
</li>
<li>Geometric Mean
<pre id=""><code>In:= GeneralizedMean[{x, y, z, t}, (#*#2) &amp;]
Out= (t x y z)^(1/4)
</code></pre>
</li>
<li>Harmonic Mean
<pre id=""><code>In:= GeneralizedMean[{x, y, z, t}, (1/(1/# + 1/#2)) &amp;]
Out= 4/(1/t + 1/x + 1/y + 1/z)
</code></pre>
</li>
<li>Franken Mean v1
<pre id=""><code>In:= GeneralizedMean[{x, y}, (#^#2 * #2^#) &amp;]
Out= Log[x^y y^x]/(2 ProductLog[1/2 Log[x^y y^x]])
</code></pre>
</li>
</ul>
<p>I had to use 2 variables for the above, because Mathematica’s symbolic inverses aren’t strong enough.<br>
Franken Mean v2 does not even compute with symbolics or even numbers.<br>
I will eventually work on this to hopefully get at least numerics to work nomatter what.</p>
<h2>Alternative Implementation</h2>
<hr>
<p>This is an alternative implementation which seems to yield better results in some cases:</p>
<pre id=""><code>GeneralizedMean[l_List, f_Function] := (
Module[{x},
Solve[Fold[f, l] == Fold[f, ConstantArray[x, Length[l]]], x][[-1,
1, 2]]
]);
</code></pre>

<h2>Create your own means!</h2>
<hr>
<p>Enjoy creating your own means! Number theoretic means on lists of primes and that sort of stuff should be very interesting to explore, cheers!</p>
<h3><strong>Continued in:</strong></h3>

<h3><a href="https://churchofthought.org/means-of-infinite-sets-and-more/"><strong>Part Two: Means of Infinite Sets</strong></a></h3>
<p><em>P.S.</em></p>
<p>Kolmogorov actually did something similar and came up with what’s called the <a href="https://en.wikipedia.org/wiki/Quasi-arithmetic_mean">Generalized f-mean</a></p>
<p>But he restricted his construct to only functions of the form $f(a,b) = g(a) + g(b)$, where g is the function that determines the mean and is to be specified.</p>


		</div><!-- .entry-content -->

	</div></div>]]>
            </description>
            <link>https://churchofthought.org/blog/2017/03/26/the-generalized-mean-an-algorithmic-approach/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25236328</guid>
            <pubDate>Sat, 28 Nov 2020 08:51:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Made my personal site into a desktop environment influenced by Windows and macOS]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25235800">thread link</a>) | @DustinBrett
<br/>
November 27, 2020 | https://dustinbrett.github.io/x/ | <a href="https://web.archive.org/web/*/https://dustinbrett.github.io/x/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://dustinbrett.github.io/x/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25235800</guid>
            <pubDate>Sat, 28 Nov 2020 06:45:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Bill Gates is wrong about education]]>
            </title>
            <description>
<![CDATA[
Score 31 | Comments 64 (<a href="https://news.ycombinator.com/item?id=25235672">thread link</a>) | @rajlego
<br/>
November 27, 2020 | https://supermemo.guru/wiki/Bill_Gates_is_wrong_about_education | <a href="https://web.archive.org/web/*/https://supermemo.guru/wiki/Bill_Gates_is_wrong_about_education">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="mw-content-text" lang="en" dir="ltr"><p><small>This text is part of: "<i><a href="https://supermemo.guru/wiki/Problem_of_Schooling" title="Problem of Schooling">I would never send my kids to school</a></i>" by <a href="https://supermemo.guru/wiki/Piotr_Wozniak" title="Piotr Wozniak">Piotr Wozniak</a> (2017)</small>
</p>


<h2><span id="Bill_Gates_is_my_hero">Bill Gates is my hero</span></h2>
<p>Bill Gates was an early guiding light and <a href="https://supermemo.guru/wiki/SuperMemo_World" title="SuperMemo World">our</a> inspiration. When <a href="https://supermemo.guru/wiki/Krzysztof_Biedalak" title="Krzysztof Biedalak">Krzysztof Biedalak</a> and I made our first $3 investment in a corporate rubber stamp, which was a post-communist obligation for all companies in Poland in 1991, Microsoft was a multibillion-dollar company. How could we not have been blinded by inspiration? We wanted to write a universally useful piece of software and the world would be ours - we thought. Bill Gates's software philosophy, based on respect for backward compatibility, sheltered <a href="https://supermemo.guru/wiki/SuperMemo" title="SuperMemo">SuperMemo</a> on its path to its painfully slow adoption. Software and database compatibility have been preserved for 30 years now. My first pieces of knowledge in medical sciences, typed in on Dec 13, 1987, are still there in my collection, well-memorized and useful. Without Gates and his stance on compatibility, I would have lost all that knowledge to some upgrade hiccup long ago. When Gates moved to philanthropy, he has secured his place on my list of the greatest people who ever walked this planet. Perhaps as many as <a href="https://supermemo.guru/wiki/Bill_Gates_saved_over_100_million_children" title="Bill Gates saved over 100 million children">30-120 million kids have been saved by Bill's foundation</a>. This begs a vital question: Why is Gates so awfully wrong about education? Why does he fall for the same old myth that <a href="https://supermemo.guru/wiki/Myth:_You_can_improve_education_by_throwing_more_money_in_it" title="Myth: You can improve education by throwing more money in it">investing in education will produce better outcomes</a>? The education system is wrong and it must be redesigned. <a href="https://supermemo.guru/wiki/Compulsory_schooling_must_end" title="Compulsory schooling must end">Compulsory schooling must end</a>. See: <a href="https://supermemo.guru/wiki/Grand_Education_Reform" title="Grand Education Reform">Grand Education Reform</a>
</p>
<h2><span id="Could_Bill.27s_great_mind_be_wrong.3F">Could Bill's great mind be wrong?</span></h2>
<p>Everyone who disagrees with a great mind needs to pause and re-examine. Gates got sensational credentials. He sports a genius mind. He has seen more places that I could possibly ever manage to visit in Google Maps. He has spoken to more great people than I have had a chance to read about. He has visited more schools that I have seen on pictures. He started his forays into education in 1999. In contrast, I started thinking about "the system" only in 2016 when getting ready to write this <a href="https://supermemo.guru/wiki/Problem_of_Schooling" title="Problem of Schooling">book</a>. This makes me into a fledgling with an immature point of view. Gates himself is a great example of a brisk student who has turned his skills and talents into a monumental achievement. His credentials are so much better than mine!
</p>
<h2><span id="Bill_Gates.27s_perspective">Bill Gates's perspective</span></h2>
<p>Could this just be that Gates's perspective is so much different than mine?
</p><p>He looks at the education system like at the operating system. Measure the performance, look for bottlenecks in the system, fix the parameter here and there, videotape a great teacher, and make others copy the method, and manufacture greatness.
</p><p>He looks at education like a philanthropic job. Like he treats health problems with mass vaccinations, he looks for a simple formula which could improve the education of the masses with some industrial move? He seems less focused on letting the brightest thrive, and more focused on preventing the weakest from dropping out. He wants to bring up the average using some <a href="https://supermemo.guru/wiki/Testing" title="Testing">standardized testing</a> approach.
</p><p>He looks at education like a big company that needs to be managed effectively with departments, and sub-departments. With a clear division of responsibility. <a href="https://supermemo.guru/wiki/Modern_schooling_is_like_Soviet_economy#Schooling_is_like_Soviet_Economy" title="Modern schooling is like Soviet economy">With an industrial goal in mind?</a>
</p><p>Could this be that this great capitalist shows more socialist thinking than a little well-indoctrinated ex-communist like myself?
</p><p>There is a different perspective of an employer and an employee, esp. in a creative position. Gates looks at the number of college graduates. I look for specific skills and creative powers. Actual degrees do not matter much if you take time to get into a particular brain.
</p><p>He looks at students like productive workers. The heretic idea of a longer school day must have come from the factory model thinking. Longer days, more production, more manufacturing.
</p><p>He looks at education from a societal point of view, while I look at the brain of an individual. He wants to move the masses to high achievement, while I want to produce more little Bill Gateses.
</p><p>Unlike myself, Bill Gates does not focus on having more Bill Gateses. He focuses on helping the poor, in boosting qualifications of the middle class, and adds "<i>you can't run a society on top 5%</i>". He is right, however, that top 5% can forge a path in education that would inspire all the rest. They cannot be run through a compulsory system set on pushing through the remaining 95%.
</p><p>Gates's approach would be great for some poorer countries (e.g. in Africa). Where there are no schools, industrial approach and good management could quickly improve health, eliminate poverty, and provide basic education.
</p><p>My approach is probably more suited to well-developed nations where the industrial approach makes people sick of schooling. With social awareness and education on the rise, we look for more little future Noble Prize winners and future Bill Gateses.
</p><p>His own kids get the best kind of learning. During his trips around the world, they get to visit places like the Large Hadron Collider at CERN. This could spark a life-long passion that could turn them into future particle physicists or another incarnation of <a href="https://supermemo.guru/wiki/Tim_Berners-Lee" title="Tim Berners-Lee">Tim Berners-Lee</a>.  
</p><p>Where Gates optimizes for improving the average, I am looking for the optimum of peak intellectual performance.
</p><p>Last but not least, could Gates's approach be an afterglow of his dropping out from Harvard. I see that over and over again, dropouts seem to suffer from this life-long hangover about what could have been? They tend to over-appreciate the power of schooling or the power of college. In the same way, I might be under-appreciating my own degrees. Gates is <a href="https://supermemo.guru/wiki/Thiel_on_competition_for_degrees" title="Thiel on competition for degrees">the opposite of Peter Thiel</a> who studiously climbed the educational ladder until he stumbled to see the light. Thiel is now one of the staunchest critics of college.
</p>
<h2><span id="Bill_Gates.27s_formula_for_success">Bill Gates's formula for success</span></h2>
<p>I see Gates's own life as a simple formula for success in science, engineering, or life in general:
</p>
<ul><li> healthy childhood of few concerns (preferably without the relegation to <a href="https://supermemo.guru/wiki/Daycare_misery" title="Daycare misery">daycare</a>)</li>
<li> healthy approach to schooling with pranks, rebellions, disobedience, and freedom </li>
<li> minor trajectory nudges within the <a href="https://supermemo.guru/wiki/Push_zone" title="Push zone">push zone</a> by inspirational tutors. If tutors are not parents, this might be the most expensive part of the formula</li>
<li> breakthrough passion, e.g. for tinkering with computers or software</li>
<li> healthy education, possibly interrupted by some breakthrough decision (e.g. <i>Let me drop out from Harvard to set up the greatest software company in the world</i>)</li>
<li> relentless lifelong pursuit of goals born from that <a href="https://supermemo.guru/wiki/Passion_and_memory" title="Passion and memory">youthful passion</a></li>
<li> voracious reading (see: <a href="https://supermemo.guru/wiki/Bill_Gates_and_his_non-incremental_reading" title="Bill Gates and his non-incremental reading">Bill Gates and his non-incremental reading</a>)</li></ul>
<p>Only Bill Gates truly knows it, but my understanding of his life story is that his future was determined by just one major factor: getting his hands on a computer. He was good at math and programming. So are dozens of kids in my neighborhood. My thinking comes from the fact that I was also strongly affected by my first contact with computers.
</p>

<p>When <a href="https://supermemo.guru/wiki/First_steps_of_SuperMemo" title="First steps of SuperMemo">I got my first computer in 1986</a>, ZX Spectrum, I was 24 and experienced wild elation with computer's obedience in executing my commands. I told the computer what to do, and it did it perfectly without asking questions. That was wonderful. I started writing my <a href="https://supermemo.guru/wiki/Plan" title="Plan">program for planning my day</a> on paper long before I got the computer on my desk. I was eager to see it work! As ZX Spectrum would load programs from a cassette tape, I could not easily dream of having <a href="https://supermemo.guru/wiki/SuperMemo" title="SuperMemo">SuperMemo</a>. It needed access to some disk drive. I got my first PC with a 360 KB <a href="https://en.wikipedia.org/wiki/Floppy_disk">floppy disk</a> drive only <a href="https://supermemo.guru/wiki/SuperMemo_1.0_for_DOS_(1987)" title="SuperMemo 1.0 for DOS (1987)">in 1987</a></p>
<p>The above hypothetical formula for educational success is simple and effective. Only a few might ever dream to replicate the scope of Bill's success. If that formula does not bring serial Nobel Prize winners, it should at least bring up a great deal of happy and fulfilled individuals. Freedom to explore the world is essential and it is denied to a great deal of kids in the modern world. When Peter Thiel pays kids to drop out from college, he looks for this type of free thinking experience that can change one life and then can change the world.
</p>
<h2><span id="My_attempt_at_employing_Gates.27s_formula">My attempt at employing Gates's formula</span></h2>

<p>I am happy with <a href="https://supermemo.guru/wiki/Exponential_adoption_of_spaced_repetition" title="Exponential adoption of spaced repetition">my achievements in life</a>. I have followed the formula employed by Gates. However, there were some exceptions. Perhaps I could use them as an excuse for not being as wildly successful as Bill? I was sent to <a href="https://supermemo.guru/wiki/Daycare_misery" title="Daycare misery">daycare</a>, and I am sure this slowed down my development. The time I spent with my brother was more intellectual and inspirational by two orders of magnitude. However, he was a student and could not babysit the little me for ever. In later years, I was forced into a degree of conformity by an ever-present threat of being enlisted by the army in service of the Warsaw Pact. In 1986, I was finally free of the army service, and could finally drop out, however, I was not ready. There was no market economics culture in Poland of the 1980s. I read about entrepreneurial science in Science in 1989 (Oct 31, 1989). This was the first time when it occurred to me that my research into memory might actually be a seed of a business. Initially, though, my passions led me in the direction of a PhD. Schooling told me that science done by entrepreneurs is inferior to science done in academia. I thought of <a href="https://supermemo.guru/wiki/SuperMemo" title="SuperMemo">SuperMemo</a> as an opportunity to earn money for a trip to America. It seems that while Gates was fast to mature as a little entrepreneur, I needed 28 long years to even start thinking of my <a href="https://supermemo.guru/wiki/SuperMemo_World" title="SuperMemo World">own business</a>.</p>
<h2><span id="Reconciling_Gates_and_Woz">Reconciling Gates and Woz</span></h2>
<p>Gates wants better teachers, better education, verification, <a href="https://supermemo.guru/wiki/Testing" title="Testing">testing</a>, measuring, carrot-and-stick for teachers, etc. In contrast, I stand with Steve Jobs. Jobs told the kids to <a href="https://supermemo.guru/wiki/Fundamental_law_of_learning" title="Fundamental law of learning">rebel</a>!
</p><p>Gates believes that the key to the <a href="https://supermemo.guru/wiki/Reform" title="Reform">great future education system</a> is the teacher. He is almost right. If we could populate present schools with great teachers, I wouldn't ever need to write <a href="https://supermemo.guru/wiki/Problem_of_Schooling" title="Problem of Schooling">this book</a>. The problem is that <a href="https://supermemo.guru/wiki/Progressive_education" title="Progressive education">a great teacher is simply a truly great man</a>. Great teaching requires a degree of genius. We need millions of those great people. How can we possibly hope to produce hundreds of thousands …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://supermemo.guru/wiki/Bill_Gates_is_wrong_about_education">https://supermemo.guru/wiki/Bill_Gates_is_wrong_about_education</a></em></p>]]>
            </description>
            <link>https://supermemo.guru/wiki/Bill_Gates_is_wrong_about_education</link>
            <guid isPermaLink="false">hacker-news-small-sites-25235672</guid>
            <pubDate>Sat, 28 Nov 2020 06:12:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Considerations Before Graduating as an Engineer]]>
            </title>
            <description>
<![CDATA[
Score 20 | Comments 7 (<a href="https://news.ycombinator.com/item?id=25235665">thread link</a>) | @Ninroot
<br/>
November 27, 2020 | https://reflexio.debec.eu/considerations-before-graduating-engineer | <a href="https://web.archive.org/web/*/https://reflexio.debec.eu/considerations-before-graduating-engineer">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

        
        <figure>
            <img alt="cover" src="https://reflexio.debec.eu/assets/piano.svg">
            <figcaption><a href="https://www.pinterest.de/pin/521995413033512913/" target="_blank">credit</a></figcaption>
            
        </figure>
        
        <p>In 2018, I graduated from <a href="https://en.wikipedia.org/wiki/%C3%89cole_pour_l%27informatique_et_les_techniques_avanc%C3%A9es">EPITA</a>, a french engineering “Grande École” specialized in computer science. The subjects I had treated there were very technical: from the implementation of an ISO file reader to a <a href="https://www.lrde.epita.fr/~tiger/tiger.html">Tiger compiler</a>.</p>

<p>The technical focus, taught in technical programs such as my engineering school, has led me to some small surprises during my first steps in the business world. The following list outlines what I would have liked to have learned before graduation. The article is obviously influenced by computer science, which is the specialty of my school. Nevertheless, I hope that the students will find some teaching in it, whatever the technical field they are targeting.</p>



<h2 id="human-is-the-new-challenge">Human is the New Challenge</h2>

<p>Coming from a school whose implicit motto was “the more technical, the better”, I had to quickly get used to the idea that the new technicality coming out of school was primarily human. Don’t make me wrong: having technical knowledge is as necessary as expected by the business world. But if many of us have been trained to take up technical challenges, we have been much less prepared for the strangeness of human behavior.</p>

<blockquote>
  <p>Technical skills should no longer be your challenge, your challenge should now be human.</p>
</blockquote>

<p>Schopenhauer said in The Art of Being Right “<em>A man may be objectively in the right, and nevertheless in the eyes of bystanders, and sometimes in his own, he may come off worst</em>” which in our case could be perfectly translated as “<em>You may be technically right and yet not be supported by others.</em>”. Sometimes engaging technical decisions are taken on the basis of very non-technical arguments, for example:</p>
<ul>
  <li>you were sick on the day of an important meeting;</li>
  <li>your level in English is not good enough to argue well enough;</li>
  <li>your manager used to work at XYZ, therefore XYZ technology is chosen for the project;</li>
  <li>your ambition to change a design frightens the rest of the team, despite the little amount effort it actually requires;</li>
  <li>your colleague X doesn’t like you, therefore it doesn’t like your design;</li>
  <li>etc.</li>
</ul>

<p>The list is endless. Just keep in mind that if you want technical decisions to be accepted, you will have to arm yourself with skills that go well beyond technique, starting with <em>negotiation</em> for example.</p>



<p>The school doesn’t just train engineers in languages or tools, it establishes a culture and a discipline that becomes stronger or weaker by joining a firm. Depending on the business sector, company, project or team, not everyone gives the same attention to software engineering. It is very unlikely that this discipline is the core business of your company, nor even the academic background of your colleagues. The good practices, taken for granted, can then be violently disrupted: no code review, no unit tests, sometimes even no version-control tool.</p>

<p>The intuitive reflex of a young engineer is to bring a purely technical solution: no version-control tool? let’s install git! No code review? Let’s protect the master branch! No unit tests? Install the right testing library! Etc. But here we are, the problem is <em>not</em> technical, the problem is primarily human, more particularly cultural. Changing the corporate culture, or more modestly, <em>the culture of a team is far more difficult to change than any tool</em>.</p>

<p>I will always remember the long struggle to get GitLab for an old project. This was the kind of indispensable work tool for a software engineer. The battle was long, not because the tool did not exist (it just had to be downloaded and installed) but because the required culture was not ready.</p>

<p>It takes time and effort to change a culture, but it is well worth the effort. A tool can always be dropped, a <code>git push</code> can always be forced, but a disciplined team is hardly swindled. So when choosing between a very well equipped team and a very disciplined team, <em>choose discipline without hesitation</em>!</p>

<h2 id="professional--genius">Professional &gt; Genius</h2>

<p>In university, a lot of credit was given to geniuses who, alone, could compile a few thousand lines of complicated code in a single night’s work. When I entered the business world, my vision changed a lot from that kind of profile, capable of doing a lot of work in record time. Their confidence (or arrogance) leads to an extra commitment, which makes it difficult to meet delivery deadlines. And because geniuses understand complex things, they <a href="https://reflexio.debec.eu/principles-for-better-design#keep-it-simple-stupid">design complex systems</a> that no one understands. Their loneliness makes them unique masters of their subjects, thereby nurturing their self-esteem. Knowledge remains in their heads and their presence becomes necessary for every decision.</p>

<p><a href="https://armament.solutions/tactics/single-points-of-failure.html"><img src="https://reflexio.debec.eu/assets/single_point_of_failure.svg" alt="Single point of failure"></a></p>

<p>But geniuses also have 24-hour days and quickly turn out to be the <em>bottleneck</em> of everything, slowing down everyone else. You don’t need to be a software architecture expert to agree that it is not desirable to over-commit and underestimate a single resource: apart from the inability to scale, the resource is the single point of failure. <a href="https://en.wikipedia.org/wiki/Single_point_of_failure">The design of a single point of failure is unforgivable</a>. Once the engineer gets sick, goes on vacation, or leaves the company, the entire team will have to pay back months or even years of unprofessionalism.</p>

<p>It is a black and exaggerated portrait that I paint here. I can nevertheless bet that you will encounter this kind of profile in your career which, in addition to having a negative impact on entire projects (and on people’s health), will sometimes be claimed as a hero by the organization. <em>Real geniuses are the geniuses also capable of <a href="https://reflexio.debec.eu/principles-for-better-design#keep-it-simple-stupid">designing simplicity</a>, taking people on board while humble towards humanity and their workload</em>. These are called great professionals. They spread knowledge and responsibility in order to become not the single point of failure or the bottle neck.</p>

<blockquote>
  <p>Geniuses build scalable services, professionals are scalable geniuses. - <a href="https://twitter.com/arnaud_debec/status/1327584444849524736">Twitter</a></p>
</blockquote>

<!--
## Read Through What Is Ask From You

If you asked your manager if you had to do unit tests, he would tell you not to waste time with it.

> If I had asked people what they wanted, they would have said faster horses. - [Henry Ford](https://hbr.org/2011/08/henry-ford-never-said-the-fast)

 -->

<h2 id="side-note">Side note</h2>

<p>Do you think there’s a principle missing? Send me your comments! This list will certainly be extended and refined, subscribe to the <a data-formkit-toggle="78a863d4eb" href="https://reflexio-debec.ck.page/78a863d4eb">newsletter</a> if you wish to be notified about it.</p>

    </div></div>]]>
            </description>
            <link>https://reflexio.debec.eu/considerations-before-graduating-engineer</link>
            <guid isPermaLink="false">hacker-news-small-sites-25235665</guid>
            <pubDate>Sat, 28 Nov 2020 06:09:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Supermassive Lens on the Constants of Nature]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25235211">thread link</a>) | @CapitalistCartr
<br/>
November 27, 2020 | http://m.nautil.us/issue/93/forerunners/a-supermassive-lens-on-the-constants-of-nature | <a href="https://web.archive.org/web/*/http://m.nautil.us/issue/93/forerunners/a-supermassive-lens-on-the-constants-of-nature">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
			<p><span>T</span>he 2020 Nobel Prize in Physics went to three researchers who confirmed that Einstein’s general relativity predicts black holes, and established that the center of our own galaxy houses a supermassive black hole with the equivalent of 4 million suns packed into a relatively small space. Besides expanding our understanding of black holes, the strong gravitational field around the supermassive black hole is a lab to study nature under extreme conditions. Researchers, including one of the new Nobel Laureates, <a href="http://alliance.nautil.us/article/199/opening-a-new-window-into-the-universe" target="_blank">Andrea Ghez</a> at UCLA, have measured how the intense gravity changes the fine structure constant, one of the constants of nature that defines the physical universe, and in this case, life within it. This research extends other ongoing efforts to understand the constants and whether they vary in space and time.&nbsp;The hope is to find clues to resolve issues in the Standard Model of elementary particles and in current cosmology.</p><figure data-alt="Perkowitz_BREAKER-1"><img src="http://static.nautil.us/17968_916cbd6f20415c2214d441deaefedf75.png" width="733" alt=""><figcaption><span><strong>NOBEL LAUREATE:</strong> Andrea Ghez won science’s biggest prize for her co-discovery of a supermassive black hole in the center of our galaxy. She has also precisely defined the elliptical paths of stars orbiting the galactic center.</span><span>Wikimedia Commons</span></figcaption></figure> <p>Besides Ghez, the other Nobel Laureates honored in 2020 are Roger Penrose at Cambridge University, who deepened our theoretical understanding of black holes; and Reinhard Genzel, of the Max Planck Institute for Extraterrestrial Physics in Garching, Germany. Ghez and Genzel carried out parallel but separate observations and analysis that led each to deduce the presence of our galactic supermassive black hole. At 27,000 light-years away, obtaining good data required huge telescopes. Ghez worked with the Keck Observatory on Mauna Kea in Hawaii, and Genzel used the Very Large Telescope in Chile. Each researcher found that the motion of the stars they observed arose from an enormous mass at the center of the galaxy. They obtained the same value, 4 million times the mass of our sun, in a region only as big as our solar system—definitive evidence of a supermassive black hole.</p><p>Ghez’s research at Keck made her a co-author <a href="https://arxiv.org/abs/2002.11567" target="_blank">in a paper</a> published this year, in which Aurélien Hees of the Paris Observatory and 13 international colleagues presented results for the fine structure constant near our galactic supermassive black hole. Remarkably, Ghez’s Nobel Prize-winning results supporting this research combined today’s theories and astronomical techniques with ideas dating back to Johannes Kepler and Isaac Newton to examine the motion of stars near the supermassive black hole. This is another example of Newton’s insight about how science advances when he wrote in 1675, “If I have seen further it is by standing on the shoulders of giants.”</p><blockquote><p>The constant in the strong gravity near the black hole could be a clue to modifying the Standard Model.</p> </blockquote><p>German astronomer Kepler is one such giant who changed science when he presented his laws of planetary motion in 1609. He was the first to show that the planets do not orbit the sun in divinely inspired perfect circles, as had been assumed. The orbits are ellipses with the sun at a focus of the ellipse, one of the two points symmetrically offset from the center that define how to construct an ellipse. Kepler also found a mathematical relation between the size of a planetary orbit and how long it takes the planet to complete a circuit.<br></p><p>In 1687 Newton gave Kepler’s laws a deeper, more coherent physical basis. Newton’s law of gravitation, based on mutual attraction between bodies, showed that a celestial object in a closed orbit around a mass follows an elliptical path that depends on that mass. This result, which today is taught in introductory astronomy, is the heart of how Ghez found the mass of the supermassive black hole. Her years of careful observations precisely defined the elliptical paths of stars orbiting the galactic center; then she used Newton’s theory to calculate the mass at the center (general relativity, which replaces Newton’s law, predicts black holes but Newton’s approach is sufficiently accurate for the stellar orbits around the supermassive black hole). Knowledge of these orbits would be crucial for measuring the fine structure constant in the strong gravity near the supermassive black hole. How that constant depends on gravity could be a clue to modifying the Standard Model or general relativity to deal with dark matter and dark energy, the two great puzzles of contemporary physics.</p> <p><span>T</span>his particular examination fits into a bigger, long-term examination of the fundamental constants of nature, each of which tells us something about the scope or scale of our deepest theories. Along with other constants, the fine structure constant (denoted by the Greek letter α), appears in the Standard Model, the quantum field theory of elementary particles. The numerical value of α defines how strongly photons and electrically charged particles interact through the electromagnetic force, which controls the universe along with gravity and the strong and weak nuclear forces. Among its effects, electromagnetism determines the degree of repulsion between protons and how electrons behave in an atom. If the value of α were much different from the one we know, that would affect whether nuclear fusion within stars produces the element carbon or whether atoms can form stable complex molecules. Both are necessary for life, another reason α is significant.</p><p>Other constants represent other major physical theories: <i>c</i>, the speed of light in vacuum, is crucial in relativity; <i>h</i>, the constant derived by Max Planck (now taken as “h-bar,” or <i>ħ</i> = <i>h</i>/2<i>π</i>), sets the tiny size of quantum effects; and <i>G</i>, the gravitational constant in Newton’s theory and general relativity, determines how astronomical bodies interact. In 1899 Planck used just these three to define a universal measurement system based on natural properties and not on any human artifacts. This system, he wrote, would be the same “for all times and all civilizations, extraterrestrial and non-human ones.”</p><blockquote><p>It raises the notion that out of many multiverses, the one where we exist is the one with the winning value.</p> </blockquote><p>Planck derived natural units of length, time, and mass from <i>c</i>, <i>ħ</i>, and <i>G</i>: <i>L<sub>P</sub></i> = 1.6 x 10<sup>-35</sup> meters, <i>T<sub>P</sub></i> = 5.4 x 10<sup>-44</sup> seconds, and <i>M<sub>P</sub></i> = 2.2 x 10<sup>-8</sup> kilograms. Too small to be practical, they have conceptual weight. In today’s universe the gravitational interaction between elementary particles is too weak to affect their quantum behavior. But place the bodies a tiny Planck length <i>L<sub>P</sub></i> apart, less than the diameter of an elementary particle, and their gravitational interaction becomes strong enough to rival quantum effects. This defines the “Planck era” 10<sup>-44</sup> seconds after the Big Bang, when gravitational and quantum effects were of similar strength and would require a combined theory of quantum gravity instead of the two separate theories we have today.<br></p><p>Nevertheless, to some physicists, <i>c</i>, <i>ħ</i>, and <i>G</i> are not truly fundamental because they depend on units of measurement. Consider for instance that <i>c</i> is 299,792 km/sec in metric units but 186,282 miles/sec in English units, This shows that physical units are cultural constructs rather than inherent in nature (in 1999, NASA’s Mars Climate Orbiter fatally crashed because two scientific teams forgot to check which measurement system the other had used). Constants that are pure numbers, however, would translate perfectly between cultures and even between us and aliens with unimaginably different units of measurement.</p><p>The fine structure constant α stands out as carrying this favored purity. In 1916 it appeared in calculations for the wavelengths of light emitted or absorbed as the single electron in hydrogen atoms jumps between quantum levels. Niels Bohr’s early quantum theory predicted the main wavelengths but spectra showed additional features. To explain these, the German theorist Arnold Sommerfeld added relativity to the quantum theory of the hydrogen atom. His calculations depended on a quantity he called the fine structure constant. It includes <i>ħ</i>, <i>c</i>, and the charge on the electron <i>e</i>, another constant of nature; and the permittivity <i>ε</i><sub>0</sub>&nbsp; that represents the electrical properties of vacuum. Remarkably, the physical units in this odd collection cancel out, leaving only the pure number 0.0072973525693.</p> <figure data-alt="Perkowitz_BREAKER-2"><img src="http://static.nautil.us/17967_a52357f1ce8160dee6563b6a3391ffa8.png" width="733" alt=""><figcaption><span><strong>GIANT SHOULDERS:</strong> This year’s Nobel winners drew on astronomical techniques dating back to Isaac Newton, who wrote of scientific advances, “If I have seen further it is by standing on the shoulders of giants.”</span><span>Nicku / Shutterstock</span></figcaption></figure><p>Sommerfeld used α just as a parameter, but it gained fame in the late 1920s when it reappeared in advanced work on relativistic quantum mechanics by the French physicist Paul Dirac, and then in what the English astronomer Arthur Eddington hoped would be a Theory of Everything. He planned to merge quantum theory and relativity to derive the properties of the universe such as the number of elementary particles in it, and its constants, α among them.</p><p>One twist in Eddington’s approach was that he considered the quantity 1/α rather than α, because his analysis showed that it must be an integer as well as a pure number. This was consistent with a contemporary measurement that yielded 1/α = 137.1, tantalizingly near 137 exactly. Eddington’s calculations gave instead 136, close enough to raise interest. Further measurements however confirmed that 1/α = 137.036. Eddington’s attempts to justify his different result were unconvincing and for this and other reasons his theory has not survived.</p><p>But α and “137” remain linked, which is why Richard Feynman called 137 a “magic number.” What he meant has nothing to do with numerology. Rather it is that we know how to measure the value of α but not how to derive it from any theories we know. This is true also for the other fundamental constants, including pure numbers such as the ratio of the proton and electron masses, and is a lack in the Standard Model. Nevertheless, the value of α is critical in …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://m.nautil.us/issue/93/forerunners/a-supermassive-lens-on-the-constants-of-nature">http://m.nautil.us/issue/93/forerunners/a-supermassive-lens-on-the-constants-of-nature</a></em></p>]]>
            </description>
            <link>http://m.nautil.us/issue/93/forerunners/a-supermassive-lens-on-the-constants-of-nature</link>
            <guid isPermaLink="false">hacker-news-small-sites-25235211</guid>
            <pubDate>Sat, 28 Nov 2020 04:13:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Design of Diskprices.com]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25235018">thread link</a>) | @neilpanchal
<br/>
November 27, 2020 | https://neil.computer/notes/the-design-of-diskprices-com/ | <a href="https://web.archive.org/web/*/https://neil.computer/notes/the-design-of-diskprices-com/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <article>
        
        <p>I was browsing the interwebs and came across this beauty: <a href="https://diskprices.com/">https://diskprices.com/</a></p><figure><img src="https://neil.computer/content/images/2020/11/Screen-Shot-2020-11-27-at-7.15.01-PM.png" alt="Diskprices.com" srcset="https://neil.computer/content/images/size/w600/2020/11/Screen-Shot-2020-11-27-at-7.15.01-PM.png 600w, https://neil.computer/content/images/size/w1000/2020/11/Screen-Shot-2020-11-27-at-7.15.01-PM.png 1000w, https://neil.computer/content/images/size/w1600/2020/11/Screen-Shot-2020-11-27-at-7.15.01-PM.png 1600w, https://neil.computer/content/images/size/w2400/2020/11/Screen-Shot-2020-11-27-at-7.15.01-PM.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Diskprices.com</figcaption></figure><p>There is a distinct absense of a header, branding, anything that takes away screen real-estate. The design of Diskprices.com orients towards what the user came to the website to do - to look at the list of available harddrives and compare their prices. It's in the domain name!</p><p>Furthermore, the performance of this website is stellar. It loads almost instantly. And the list (although its not sortable) gets the job done, it is sorted by price already which is the most important attribute.</p><p>Diskprices.com deserves the UI/UX award of the decade. We've lost our ability to design user interfaces laser focused towards the <em>user</em>. Instead, we have purple gradients, scroll jacking, responsive bullshit, emojis, animations and many other things designers do today. The utilitarian approach of Diskprices.com is refreshing, although the contemporary designers cast it off as 'brutalist design', thereby marking it as a statement of fashion.</p><p>The creators of Diskprices.com didn't just do this by chance, it is a deliberate attempt as stated in the FAQ:</p><figure><img src="https://neil.computer/content/images/2020/11/Screen-Shot-2020-11-27-at-7.24.17-PM.png" alt="Diskprices.com FAQ" srcset="https://neil.computer/content/images/size/w600/2020/11/Screen-Shot-2020-11-27-at-7.24.17-PM.png 600w, https://neil.computer/content/images/size/w1000/2020/11/Screen-Shot-2020-11-27-at-7.24.17-PM.png 1000w, https://neil.computer/content/images/2020/11/Screen-Shot-2020-11-27-at-7.24.17-PM.png 1210w" sizes="(min-width: 720px) 720px"><figcaption>Diskprices.com FAQ</figcaption></figure><p>Quoting:</p><blockquote>Do you need a graphic designer?<br>No. This site is designed to maximize information density, accessibility, and performance. More whitespace, colors, and icons won't help.</blockquote><p>If the design of the object, service or product is to enable the user, why is it shameful to keep it undecorated?</p><p>Folks at Diskprices.com, if you're reading this, beer is on me if we ever meet.</p>
        </article>
</div></div>]]>
            </description>
            <link>https://neil.computer/notes/the-design-of-diskprices-com/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25235018</guid>
            <pubDate>Sat, 28 Nov 2020 03:31:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Running Python on .NET 5]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25234986">thread link</a>) | @gilad
<br/>
November 27, 2020 | https://tonybaloney.github.io/posts/running-python-on-dotnet-5-with-pyjion.html | <a href="https://web.archive.org/web/*/https://tonybaloney.github.io/posts/running-python-on-dotnet-5-with-pyjion.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <div>
                <p><em>This post is an update on the Pyjion project to plug the .NET 5 CLR JIT compiler into Python 3.9.</em></p>
<p>.NET 5 was released on November 10, 2020. It is the cross-platform and open-source replacement of the <a href="https://github.com/dotnet/core"><strong>.NET Core</strong></a> project and the <strong>.NET</strong> project that ran exclusively on Windows since the late 90’s.</p>
<p>.NET is formed of many components:</p>
<ul>
<li>3 builtin languages, C#, F# and VB.NET, each with its own compiler</li>
<li>A standard library</li>
<li>A common intermediate language to abstract the high level languages from the core runtime. This is a standard known as <a href="https://github.com/tonybaloney/ecma-335/tree/master/docs">ECMA 335 CIL</a>.</li>
<li>A common language runtime (CLR) that compiles CIL into native machine code so that it can be executed and packages executables into .exe formats.</li>
</ul>
<p><img alt=".NET architecture" src="https://tonybaloney.github.io/img/posts/Common_Language_Infrastructure.png"></p>
<p>.NET 5 CLR comes bundled with a performant JIT compiler (codenamed RyuJIT) that will compile .NETs CIL into native machine instructions on Intel x86, x86-64, and ARM CPU architectures.</p>
<p>You can write code in a number of languages, like C++, C#, F# and compile those into CIL and then into native machine code (as a binary executable) on macOS, Linux, and Windows. Pretty neat.</p>
<p>But this is a blog about Python. So what does this have to do with Python?</p>
<p>Pyjion is a project to replace the core execution loop of CPython by transpiling CPython bytecode to ECMA CIL and then using the .NET 5 CLR to compile that into machine code. It then executes the machine-code compiled JIT frames at runtime instead of using the native execution loop of CPython.</p>
<h2>Very-quick overview of Python’s compiler</h2>
<p>When CPython compiles Python code, it compiles it into an intermediate format, similar to .NET, called Python bytecode. This bytecode is cached on disk so that when you import a module that hasn’t changed, it doesn’t compile it every time. You can see the bytecode by disassembling any Python function:</p>
<pre><code>&gt;&gt;&gt; import dis
&gt;&gt;&gt; def half(x):
...    return x/2
... 
&gt;&gt;&gt; dis.dis(half)
  2           0 LOAD_FAST                0 (x)
              2 LOAD_CONST               1 (2)
              4 BINARY_TRUE_DIVIDE
              6 RETURN_VALUE
</code></pre>

<p>To execute anything on a CPU, you have to provide the OS with machine-code instructions. This can be accomplished by compiling them up-front using a compiled like the C or C++ compilers. They compile code into executable formats as either shared libraries or standalone executables. <a href="https://tonybaloney.github.io/posts/extending-python-with-assembly.html"><em>See my post on Python/assembly for a bit more info on this topic</em></a>.</p>
<p>CPython converts the bytecode into machine code instructions like looping over them in a precompiled function, called the evaluation loop. This is essentially a big for loop with a switch statement. The compiled version of CPython that you’re running already has the instructions required. This is why CPython’s evaluation loop is an “AOT”, or “Ahead of Time” compiled library:</p>
<p><img alt="diagram 1" src="https://tonybaloney.github.io/img/posts/Slide1.png"></p>
<p><strong>Note: There is a lot more to CPython’s compiler. I’ve written a <a href="https://realpython.com/products/cpython-internals-book/">whole book on the CPython compiler and the internals of CPython</a> if you want to learn more.</strong></p>
<p>There are a few issues with this approach. The biggest is speed. A series of inline machine-code instructions is very performant. CPython has to make judgements at runtime for which code branch to follow every time your function is run. This leads to CPython being 100x slower in “tight-loop” problems where its executing the same thing again and again. The machine-code is compiled ahead of time and it has to loop around to get to the right instructions. Checkout my PyCon talk for a more in-depth explanation:</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/I4nkgJdVZFA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>The most common way around this performance barrier is to compile Python extensions from C. This produces a custom binary with inline machine-code instructions for the task at hand. This is how most machine-learning and data science libraries like numpy, pandas, SKL are put together. This approach is still AOT compiling the code. It also requires a lot of knowledge of C. This approach has worked really well for the data science community, where algorithms can be performant and leverage low-level platforms <a href="https://numba.pydata.org/numba-doc/latest/cuda/index.html">like GPUs or specialised AI chipsets</a>.</p>
<p>There are a few issues with the AOT extension module approach. One is that it still uses the evaluation loop. C extension modules are a set of functions. Once you call the C-compiled function, its in the performant code, but your Python code that’s calling it still lives inside Python’s loop. If you want to leverage a compiled library and your Python code is doing some heavy number crunching, you end up having to use an API of functions, like numpy, instead of a more fluent Python API:</p>
<pre><code>&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; a = np.ones([9, 5, 7, 4])
&gt;&gt;&gt; c = np.ones([9, 5, 4, 3])
&gt;&gt;&gt; np.dot(a, c).shape
(9, 5, 7, 9, 5, 3)
&gt;&gt;&gt; np.matmul(a, c).shape
(9, 5, 7, 3)
</code></pre>

<h2>What Pyjion does to solve this issue</h2>
<p>A few releases of Python ago (CPython specifically, the most commonly used version of Python) in 3.7 a new API was added to be able to swap out “frame execution” with a replacement implementation. This is otherwise known as <a href="https://www.python.org/dev/peps/pep-0523/">PEP 523</a>. PEP 523 also added the capability to store additional attributes in <em>code objects</em> (compiled Python code.</p>
<p>Pyjion does not compile Python code. It compiles Python frames (code objects, like blocks, functions, methods, classes) into machine-code at runtime using a performant JIT:</p>
<p><img alt="diagram 2" src="https://tonybaloney.github.io/img/posts/Slide2.png"></p>
<p>CPython compiles the Python code, so whatever language features and behaviours there are in CPython 3.9, like the walrus operator, <a href="https://www.python.org/dev/peps/pep-0584">the dictionary union operator</a>, will all work exactly the same with this extension enabled. This also means that this extension uses the same standard library as Python 3.9.</p>
<p>Pyjion is a “pip installable” package for standard CPython that JIT compiles all Python code at runtime using the .NET 5 JIT compiler. You can use off-the-shelf CPython 3.9 on macOS, Linux or Windows. After installing this package you just import the module and enable the JIT.</p>
<p>Once a frame has been compiled, the binary code is cached in memory and reused every time the function is called:</p>
<p><img alt="diagram 3" src="https://tonybaloney.github.io/img/posts/Slide3.png"></p>
<h2>Using Pyjion</h2>
<p>To get started, you need to have .NET 5 installed, with Python 3.9 and the Pyjion package (I also recommend using a virtual environment).</p>
<p>After importing pyjion, enable it by calling <code>pyjion.enable()</code> which sets a compilation threshold to 0 (the code only needs to be run once to be compiled by the JIT):</p>
<pre><code>&gt;&gt;&gt; import pyjion
&gt;&gt;&gt; pyjion.enable()
</code></pre>

<p>Any Python code you define or import after enabling pyjion will be JIT compiled. You don’t need to execute functions in any special API, its completely transparent:</p>
<pre><code>&gt;&gt;&gt; def half(x):
...    return x/2
&gt;&gt;&gt; half(2)
1.0
</code></pre>

<p>Pyjion will have compiled the <code>half</code> function into machine code on-the-fly and stored a cached version of that compiled function inside the function object.
You can see some basic stats by running <code>pyjion.info(f)</code>, where <code>f</code> is the function object:</p>
<pre><code>&gt;&gt;&gt; pyjion.info(half)
{'failed': False, 'compiled': True, 'run_count': 1}
</code></pre>

<p>You can see the machine code for the compiled function by disassembling it in the Python REPL.
Pyjion has essentially compiled your small Python function into a small, standalone application.
Install <code>distorm3</code> first to disassemble x86-64 assembly and run <code>pyjion.dis.dis_native(f)</code>:</p>
<pre><code>&gt;&gt;&gt; import pyjion.dis
&gt;&gt;&gt; pyjion.dis.dis_native(half)
00000000: PUSH RBP
00000001: MOV RBP, RSP
00000004: PUSH R14
00000006: PUSH RBX
00000007: MOV RBX, RSI
0000000a: MOV R14, [RDI+0x40]
0000000e: CALL 0x1b34
00000013: CMP DWORD [RAX+0x30], 0x0
00000017: JZ 0x31
00000019: CMP QWORD [RAX+0x40], 0x0
0000001e: JZ 0x31
00000020: MOV RDI, RAX
00000023: MOV RSI, RBX
00000026: XOR EDX, EDX
00000028: POP RBX
00000029: POP R14
...
</code></pre>

<p>The complex logic of converting a portable instruction set into low-level machine instructions is done by .NET’s CLR JIT compiler.</p>
<p>All Python code executed after the JIT is enabled will be compiled into native machine code at runtime and cached on disk. For example, to enable the JIT on a simple <code>app.py</code> for a Flask web app:</p>
<pre><code>import pyjion
pyjion.enable()

from flask import Flask
app = Flask(__name__)

@app.route('/')
def hello_world():
    return 'Hello, World!'

app.run()
</code></pre>

<p>That’s it.</p>
<h2>Will this be compatible with my existing Python code? What about C Extensions?</h2>
<p>The short answer is- if your existing Python code runs on CPython 3.9 – <strong>yes</strong> it will be compatible. To make sure, Pyjion has been tested against the full CPython “test suite” on all platforms. In fact, it was the first JIT ever to pass the test suite.</p>
<p>Thats because this isn’t a Python runtime, it uses the existing Python compiler to compile your code into Python bytecode (low level instructions).</p>
<p>Pyjion uses the same dynamic module loader as CPython, so if you import a Python extension from your virtual environment, it will work just the same in Pyjion.</p>
<h2>Project History</h2>
<p>Pyjion isn’t new. Brett Cannon and Dino Viehland started the Pyjion project 4 years ago. This was the first JIT to pass the full CPython test suite.
There were some limitations to the original proof-of-concept:</p>
<ul>
<li>Written against an old version of .NET Core</li>
<li>Required custom patches of .NET and compiling from source</li>
<li>Required custom patches of CPython and compiling from source</li>
<li>Only worked on Windows</li>
<li>It was written for Python 3.6 before PEP 523 was agreed and merged</li>
</ul>
<p>Has much changed since Python 3.6? To the average user, not really. But under the hood, the implementation of a few things has completely changed:</p>
<ul>
<li>Function calls</li>
<li>Iterators</li>
<li>Exception Handling</li>
<li>Dictionary, list and set comprehensions</li>
<li>Generators and coroutines</li>
</ul>
<p>Actually, a <strong>lot</strong> has changed in the last few releases of CPython. The new fork to get Pyjion working with the latest version of everything was a big undertaking…</p>
<p><img alt="not-much-has-changed" src="https://tonybaloney.github.io/img/posts-original/not-much-has-changed.png"></p>
<p>The goal with the latest patch was to get the project up to the condition of:</p>
<ul>
<li>Using the release binaries of .NET 5 and CPython 3.9</li>
<li>Making it work across all platforms</li>
<li>Implement the PEP523 interface</li>
<li>Implement all the new features of Python 3.9</li>
<li>Making the package “pip installable” from PyPi</li>
<li>Improving the test coverage</li>
<li>Adding a disassembler (both machine-code and CIL) to aid development</li>
</ul>
<h2>Is this faster?</h2>
<p>The short answer a little, but not by much (yet).</p>
<p>JIT compiling something …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://tonybaloney.github.io/posts/running-python-on-dotnet-5-with-pyjion.html">https://tonybaloney.github.io/posts/running-python-on-dotnet-5-with-pyjion.html</a></em></p>]]>
            </description>
            <link>https://tonybaloney.github.io/posts/running-python-on-dotnet-5-with-pyjion.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25234986</guid>
            <pubDate>Sat, 28 Nov 2020 03:25:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Be Resilient]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25234941">thread link</a>) | @Brajeshwar
<br/>
November 27, 2020 | https://psyche.co/guides/resilience-is-like-a-muscle-build-it-up-when-life-pulls-down | <a href="https://web.archive.org/web/*/https://psyche.co/guides/resilience-is-like-a-muscle-build-it-up-when-life-pulls-down">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section><div><h2 data-guide-section-number="1"><span>Need to know</span></h2><div><p>Adversity is everywhere. It can strike when youâ€™re least expecting it, and it might be accompanied by unpleasant, albeit normal, reactions such as anxiety, excessive worry, disappointment, grief, shame, frustration and sadness. Moving on from, and even growing through, a difficult or traumatic experience can be hard, but it <em>is</em> possible.</p>
<p>Iâ€™m sure youâ€™ve already heard, read or witnessed many inspiring stories of people who have bounced back from adversity, such as the death of a loved one, losing a job, serious physical illnesses, accidents, disasters or wars. But what should we do when weâ€™re faced with hardship ourselves? How will we deal with our pain? Can we prepare ourselves for this inevitable experience?</p>
<p>The answers to these questions are not straightforward, but the psychological concept of â€˜resilienceâ€™ can help. Given that weâ€™re all currently in the midst of an adverse situation â€“ the COVID-19 pandemic â€“ understanding resilience is especially pertinent. Resilience is defined as the ability to navigate successfully through, and recover from, stressful circumstances or crisis situations, and to do so in a way that leads to healthy functioning over time. That is, resilience is not only about bouncing back, but also about experiencing some sort of growth, such as finding meaning and purpose, self-awareness or experiencing improvement in interpersonal relationships.</p>
<p>Defining resilience might sound easy, but itâ€™s a more complex concept than you might think. First, many people display resilience immediately following exposure to a hardship or potentially traumatic event. And in the long term, most people who have gone through traumatic experiences donâ€™t show signs of depression or anxiety problems later in life. Consider the <a href="https://journals.sagepub.com/doi/10.1111/j.1467-9280.2006.01682.x" rel="nofollow noreferrer noopener">study</a> of New York residents in the wake of the terrorist attack of <span>11 September</span> 2001: the researchers found that <span>65 per cent</span> of those questioned had returned to their normal level of functioning within six months. You too might be capable of more resilience than you realise.</p>
<p>Second, although some people seem disposed to deal more effectively with stress and anxiety, and to better regulate their emotions, resilience is not a single trait that you either possess or you donâ€™t. Rather, itâ€™s a set of skills, including behaviours and thoughts that can be improved through learning and exposure to new experiences.</p>
<p>Third, although individual characteristics matter for resilience, <a href="https://pubmed.ncbi.nlm.nih.gov/11202047/" rel="nofollow noreferrer noopener">contextual</a> factors also have an influence, such as the social, health and economic resources available to you. For instance, you might be predisposed for resilience but, if you were brought up in an unsupportive and stressful environment by abusive parents, you might not develop it. In fact, as well as being inaccurate, it is unfair and harmful to see resilience purely as an individual trait â€“ people who struggle to recover from a negative life event might think that thereâ€™s something inherently wrong with them, which isnâ€™t true. Access to certain external resources is a major factor in anyoneâ€™s ability to display resilience.</p>
<p>Fourth, resilience is <a href="https://pubmed.ncbi.nlm.nih.gov/22559117/" rel="nofollow noreferrer noopener">dynamic</a>. You can be resilient in one context but then your capacity for resilience, or your ability to draw on available resources, might not be enough for another, possibly more demanding or difficult, situation. All of us can be more resilient at one stage in our lives but less so in another.</p>
<p>Fifth, being resilient doesnâ€™t mean that you wonâ€™t have a wound or a scar. For example, in one <a href="https://pubmed.ncbi.nlm.nih.gov/12416919/" rel="nofollow noreferrer noopener">study</a> of more than 200 people who had experienced the death of their spouse, even those identified as the most resilient reported having at least some grief symptoms. Almost everyone suffers some negative effects, such as emotional strain, throughout the journey of adversity but resilient individuals manage to recover well.</p>
<p>Lastly, it might sound paradoxical, but resilience comes from being in touch with adversity, not from trying to stay positive all the time, or from always running away from difficulties in life. Many of us are taught from an early age that we should avoid difficulties or stress, and itâ€™s true that toxic chronic stress is a <a href="https://pubmed.ncbi.nlm.nih.gov/27417486/" rel="nofollow noreferrer noopener">risk</a> factor for mental health problems. But exposure to some level of stress provides you with the necessary challenge to become stronger in the face of hardship, as long as you learn to cope successfully. By contrast, if youâ€™re overly avoidant of challenges in life then, when an unavoidable hardship arises, you wonâ€™t have developed the necessary skills to cope.</p>
<p>Understanding the complex, dynamic nature of resilience is important because it shows that there is no magic pill or a recipe that will make you resilient. Each individual will have their own way of coping with distress, their own pace of recovery, and levels of learning from a crisis. It is also totally fine to fail to recover quickly or entirely from a particular adversity. Itâ€™s okay to get hurt or lost during a difficult time.</p></div></div></section><section><div><h2 data-guide-section-number="2"><span>What to do</span></h2><div><p>Science doesnâ€™t have all the answers on how one becomes resilient, but what we do know is that it requires learning to tap into both inner and external resources. Iâ€™ll touch on some of the most fundamental ones.</p>
<p><strong>Connect with others</strong></p>
<p>During difficult times, it is common to want to withdraw from the world. This can be for varied reasons, such as feelings of shame, the fear of being judged, or not wishing to be a drain on others. Although there is nothing wrong with wanting solitude during difficult times, it is also important that you stay in touch with people, at least to an extent. <a href="https://pubmed.ncbi.nlm.nih.gov/12555794/" rel="nofollow noreferrer noopener">Research</a> shows that the risk of developing post-traumatic stress disorder is higher for people who lack post-trauma social support (bear in mind that, even if you have friends and family, if you avoid seeing or talking to them entirely, then it will be impossible for them to help you).</p>
<p>People who choose to connect with others and nurture their relationships, as opposed to isolating themselves, tend to become better at coping with a hardship and growing through their experience. The emotional and instrumental social support you get from your intimate relationships, and from your communities, can also give you the motivation to handle stress in a healthy way.</p>
<p>So, when difficulties are overwhelming, try reaching out to others who can provide support. There are a few different ways you could do this. One is simply by talking about what youâ€™re going through. It can be frustrating to talk to someone who just pretends that theyâ€™re listening or who is judgmental, so try to find someone who is accepting and good at listening. You could also try letting them know in advance that all you need is to be listened to. Another approach is to ask specifically for instrumental help, such as information, advice or help with daily tasks. More resilient people are usually aware that they canâ€™t solve every problem on their own. You might find it especially difficult to ask for help if youâ€™re used to handling problems on your own, or if you see relying on others as a sign of weakness. Try to remember that it takes courage to ask for help, and being in need simply means that youâ€™re human.</p>
<p>Here are a few more ideas for how to connect with others and get support: if you exercise or go for a walk, try inviting someone else along. Make a commitment to call or email loved ones regularly. Make use of the power of play â€“ laugh with friends or get silly. If there are social groups that share a common interest or hobby of yours, join them to exchange ideas or to get to know new people. Support others informally or through volunteer organisations; helping others makes us feel happy and valued.</p>
<p>Most importantly, donâ€™t wait for a disaster to occur to connect; make sure you have supportive relationships that nurture your sense of self-worth and need for intimacy, which in turn can contribute to resilience. If youâ€™re physically distant from your loved ones, look for ways to socially connect on a regular basis. Even the presence and support of a small number of people you can rely on can make a huge difference when adversity strikes.</p>
<p><strong>Accept and focus on what you can control</strong></p>
<p>About seven years ago, I was diagnosed with peripheral neuropathy, which is a type of nerve damage. For me, this chronic condition manifests itself as a constant sharp pain and burning sensation on my feet. My life was miserable for six months before the diagnosis, and the pain was unbearable. I could barely walk for five minutes at a time. Upon the diagnosis, I was prescribed medication that eased my pain. Although itâ€™s manageable now, the pain is always there, and Iâ€™ll probably be on medication for the rest of my life. For the first few months, I had difficulty accepting this. Back then, I <span>was 35</span> and had been physically very active before developing this illness. At least a hundred times I asked myself how it was possible. Rejecting and blaming myself, others or the world seemed to provide some relief but it didnâ€™t get me anywhere.</p>
<p>Then one day I decided to stop wrestling with my pain and to start acknowledging it. This didnâ€™t mean that I liked the situation â€“ I hated it â€“ but it provided me with the space to start being proactive and to find effective coping strategies. The more I accepted my situation and my pain, the less pain I felt. A <a href="https://pubmed.ncbi.nlm.nih.gov/16139188/" rel="nofollow noreferrer noopener">study</a> that involved experimentally inducing pain in <span>62 men and</span> women showed the effectiveness of acceptance â€“ those taught acceptance experienced less sensory pain compared with a control group who used simple distraction.</p>
<p>Note that acceptance is not about giving up or quitting. Itâ€™s about gently noticing whatâ€™s going on, and allowing unpleasant experiences to exist, without attempting to change or deny them. With acceptance, you can choose to do what really matters to you and follow your values more easily. In his <a href="https://stevenchayes.com/a-liberated-mind/" rel="nofollow noreferrer noopener">book</a> <em>A Liberated Mind</em> (2019), the American psychologist <a href="https://aeon.co/essays/how-to-live-a-values-driven-life-in-the-face-of-dark-emotions" rel="noopener">Steven Hayes</a>, the founder of <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3635495/" rel="nofollow noreferrer noopener">ACT</a> (acceptance and …</p></div></div></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://psyche.co/guides/resilience-is-like-a-muscle-build-it-up-when-life-pulls-down">https://psyche.co/guides/resilience-is-like-a-muscle-build-it-up-when-life-pulls-down</a></em></p>]]>
            </description>
            <link>https://psyche.co/guides/resilience-is-like-a-muscle-build-it-up-when-life-pulls-down</link>
            <guid isPermaLink="false">hacker-news-small-sites-25234941</guid>
            <pubDate>Sat, 28 Nov 2020 03:16:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Type Theory and Applications [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25234780">thread link</a>) | @azhenley
<br/>
November 27, 2020 | https://metatheorem.org/includes/pubs/comp.pdf | <a href="https://web.archive.org/web/*/https://metatheorem.org/includes/pubs/comp.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://metatheorem.org/includes/pubs/comp.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-25234780</guid>
            <pubDate>Sat, 28 Nov 2020 02:47:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Link Between Curiosity and Creativity]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25234758">thread link</a>) | @dbustac
<br/>
November 27, 2020 | https://danielbusta.com/link/ | <a href="https://web.archive.org/web/*/https://danielbusta.com/link/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-content" role="main">

	
<article id="post-177">

	
<!-- .entry-header -->

	<div>

		<div>

			
<figure><img loading="lazy" width="580" height="387" src="https://i1.wp.com/danielbusta.com/wp-content/uploads/2020/11/photo-1585986675915-8291caac4f03_ixlibrb-1.2.jpg?resize=580%2C387&amp;ssl=1" alt="" srcset="https://i1.wp.com/danielbusta.com/wp-content/uploads/2020/11/photo-1585986675915-8291caac4f03_ixlibrb-1.2.jpg?resize=1024%2C683&amp;ssl=1 1024w, https://i1.wp.com/danielbusta.com/wp-content/uploads/2020/11/photo-1585986675915-8291caac4f03_ixlibrb-1.2.jpg?resize=300%2C200&amp;ssl=1 300w, https://i1.wp.com/danielbusta.com/wp-content/uploads/2020/11/photo-1585986675915-8291caac4f03_ixlibrb-1.2.jpg?resize=768%2C512&amp;ssl=1 768w, https://i1.wp.com/danielbusta.com/wp-content/uploads/2020/11/photo-1585986675915-8291caac4f03_ixlibrb-1.2.jpg?w=1051&amp;ssl=1 1051w" sizes="(max-width: 580px) 100vw, 580px" data-recalc-dims="1"><figcaption><em>Picture by <a href="https://unsplash.com/@markusspiske" target="_blank" rel="noreferrer noopener">@markusspiske</a></em></figcaption></figure>



<p>There seems to be a strong, almost causal relationship between curiosity and creativity.</p>



<p>My hypothesis is that curious people constantly do three things that are essential for the creative process: exploring, observing and asking questions.</p>



<p><strong>Curious individuals are interested in nearly everything.</strong></p>



<p>They’re always learning about and exploring new subjects. As a result, they hold in their heads a wide-ranging reservoir of information and mental models. This allows them to cross-pollinate ideas, extrapolate concepts and make unexpected connections.</p>



<p><strong>Another thing they’re very good at is observing.&nbsp;</strong></p>



<p>Curious people tend to go through life at a different pace than everyone else. They move relatively slowly. And whenever something catches their attention, they stop to contemplate it. Their ability to look extensively at and into things allows them to unveil patterns that most people are too busy to see.</p>



<p><strong>Finally, curious people like to ask tons of questions.</strong></p>



<p>As everything else, questions are subject to probabilities — the more questions you ask, the higher your chances of asking a good question. Good questions, in turn, help them find things that don’t work or don’t make sense. Thus, opening the doors for breakthroughs and innovation.</p>



<p>By giving yourself permission to cultivate curiosity, you’re also laying the foundation for a prolific, creative life.</p>


<p><em>This piece is part of a series of 30 atomic essays where I explore what it means to be a&nbsp;<a href="https://rationalcreatives.substack.com/" target="_blank" rel="noreferrer noopener">rational creative</a>&nbsp;and the different aspects of being a creator online. You can read all the others essays&nbsp;<a href="https://twitter.com/dbustac/status/1328419048070279174?s=20" target="_blank" rel="noreferrer noopener">here</a>.</em></p>
		</div><!-- .entry-content -->

	</div><!-- .post-inner -->

	<!-- .section-inner -->

	
	<!-- .pagination-single -->

	
		<!-- .comments-wrapper -->

		
</article><!-- .post -->

</div></div>]]>
            </description>
            <link>https://danielbusta.com/link/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25234758</guid>
            <pubDate>Sat, 28 Nov 2020 02:44:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Parsing All of Wikipedia to an Offline Encyclopedia]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25234490">thread link</a>) | @miles
<br/>
November 27, 2020 | https://daveshap.github.io/DavidShapiroBlog/automation/2020/11/15/parsing-all-wikipedia.html | <a href="https://web.archive.org/web/*/https://daveshap.github.io/DavidShapiroBlog/automation/2020/11/15/parsing-all-wikipedia.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="container">
<div>

<header>
 
  <h2>I want an offline encyclopedia and am extremely masochistic</h2>
   <h3>15 November 2020</h3>
    <a href="https://daveshap.github.io/DavidShapiroBlog">Home</a>&nbsp;—&nbsp;
    <a href="https://daveshap.github.io/DavidShapiroBlog/categories.html">Categories</a>
</header>

<hr>

<section id="main_content">


<p>I’m working on a project where I want to have an offline encyclopedia. It’s for training deep learning networks so it needs to be in plain text English. 
No markup, no special characters. Human readable without any interpreters, renderers, or parsers. I’ve got plenty of disk space, so that’s not a concern. 
Once I parse out all the Wikipedia articles I can get my hands on, I will create an index. Or I might index them into SOLR or something like that. Not sure yet.
I’m also going to implement it as a resource for a massive <a href="https://towardsdatascience.com/building-a-question-answering-system-part-1-9388aadff507">SQUAD repo</a>. I have uploaded a copy of the final result to <a href="https://www.kaggle.com/ltcmdrdata/plain-text-wikipedia-202011">Kaggle Datasets</a>. I’ve stashed the script in a <a href="https://github.com/daveshap/PlainTextWikipedia">dedicated repo on GitHub</a>.</p>



<p>The first thing I should have learned is that Wikipedia is written in a demented Frankenstein language called WikeMedia Text. It’s a hybrid of HTML/XML and Markdown. 
It has no consistency and is the worst example of spaghetti code I’ve ever seen. I’m sure there are better implementations today, but I can see how and why it ended up the way it did.
For instance, you need to be able to create robust references and links, so the URL syntax is way jacked. It relies on a lot of procedural generation at display time.
Personally, if it were done again today, I think something like Jekyll would be way better. Instead of rendering again and again every time someone visits a page, render it once after each edit.
But that’s just me. So instead we’re left with this horrible hybrid language that should die in a fire.</p>

<p>Fine, it is what it is. I’m an expert automator, dammit, and if a machine can automatically render this nonsense, then I sure as hell can <strong>unrender it</strong>.</p>

<h2 id="attempt-1---brute-force-regex">Attempt 1 - Brute Force Regex</h2>

<p>“Brute Force Regex” (BFR) is not a real thing. It’s just something I’ve been doing for years now in my automation habits. Usually, as a naive approach, I’ll try and do some 
search-and-replace jiggery pokery to just remove unwanted junk. Sometimes this is textual formatting, like brackets around tables or other HTML tags. 
So I ended up with the following function. Caution, it’s not pretty. This was just an experiment, and I wanted to share it so you would see what doesn’t work.</p>

<div><div><pre><code><span>def</span> <span>basic_replacements</span><span>(</span><span>text</span><span>):</span>
    <span>replacements</span> <span>=</span> <span>[</span>
    <span>(</span><span>'&amp;lt;'</span><span>,</span><span>'&lt;'</span><span>),</span>
    <span>(</span><span>'&amp;gt;'</span><span>,</span><span>'&gt;'</span><span>),</span>
    <span>(</span><span>'&amp;quot;'</span><span>,</span><span>'"'</span><span>),</span>
    <span>(</span><span>"'''"</span><span>,</span><span>' = '</span><span>),</span>
    <span>(</span><span>"'{2,}"</span><span>,</span><span>''</span><span>),</span>
    <span>(</span><span>'</span><span>\n</span><span>'</span><span>,</span><span>' '</span><span>),</span>
    <span>(</span><span>r'\n'</span><span>,</span><span>' '</span><span>),</span>
    <span>(</span><span>'</span><span>\\</span><span>n'</span><span>,</span><span>' '</span><span>),</span>
    <span>(</span><span>'r</span><span>\\</span><span>n'</span><span>,</span><span>' '</span><span>),</span>
    <span>(</span><span>'&lt;ref.*?&gt;'</span><span>,</span><span>''</span><span>),</span>
    <span>(</span><span>'&lt;/ref&gt;'</span><span>,</span><span>''</span><span>),</span>
    <span>(</span><span>'http.*?\s'</span><span>,</span><span>''</span><span>),</span>
    <span>(</span><span>'\s+'</span><span>,</span><span>' '</span><span>),</span>
    <span>]</span>
    <span>text</span> <span>=</span> <span>text</span><span>.</span><span>replace</span><span>(</span><span>'</span><span>\\</span><span>n'</span><span>,</span><span>' '</span><span>)</span>
    <span>for</span> <span>r</span> <span>in</span> <span>replacements</span><span>:</span>
        <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>r</span><span>[</span><span>0</span><span>],</span> <span>r</span><span>[</span><span>1</span><span>],</span> <span>text</span><span>)</span>
    <span>return</span> <span>text</span>
</code></pre></div></div>

<p>I came up with this scheme because, at first glance, WikiMedia Text looked like a mixture of some basic HTML and some Markdown. 
I figured I could handle it with some basic regex replacements. This worked… to an extent. There were a few problems with it though.</p>

<ol>
  <li>Couldn’t handle nested square brackets or curly brackets, and it turns out there are a lot of those</li>
  <li>Quickly became intractable when I encountered escaped unicode literals like <code>\u2013</code>. They are frigging everywhere.</li>
</ol>

<p>So I wrote two more functions to try and tackle the bracketed stuff. These are things like links, citations, and pictures. Since I want a text-only Wikipedia, 
I really just needed to strip it all away.</p>

<div><div><pre><code><span>def</span> <span>remove_double_curly</span><span>(</span><span>text</span><span>):</span>
    <span>while</span> <span>True</span><span>:</span>
        <span>before</span> <span>=</span> <span>len</span><span>(</span><span>text</span><span>)</span>
        <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>''</span><span>,</span> <span>''</span><span>,</span> <span>text</span><span>)</span> 
        <span>after</span> <span>=</span> <span>len</span><span>(</span><span>text</span><span>)</span>
        <span>if</span> <span>before</span> <span>==</span> <span>after</span><span>:</span>
            <span>return</span> <span>text</span>


<span>def</span> <span>remove_double_brackets</span><span>(</span><span>text</span><span>):</span>
    <span>while</span> <span>True</span><span>:</span>
        <span>before</span> <span>=</span> <span>len</span><span>(</span><span>text</span><span>)</span>
        <span>double_brackets</span> <span>=</span> <span>re</span><span>.</span><span>findall</span><span>(</span><span>'\[\[.*?\]\]'</span><span>,</span> <span>text</span><span>)</span>
        <span>for</span> <span>db</span> <span>in</span> <span>double_brackets</span><span>:</span>
            <span>if</span> <span>'|'</span> <span>in</span> <span>db</span><span>:</span>
                <span>new</span> <span>=</span> <span>db</span><span>.</span><span>split</span><span>(</span><span>'|'</span><span>)[</span><span>-</span><span>1</span><span>].</span><span>strip</span><span>(</span><span>']'</span><span>)</span>
                <span>text</span> <span>=</span> <span>text</span><span>.</span><span>replace</span><span>(</span><span>db</span><span>,</span> <span>new</span><span>)</span>
            <span>else</span><span>:</span>
                <span>new</span> <span>=</span> <span>db</span><span>.</span><span>strip</span><span>(</span><span>'['</span><span>).</span><span>strip</span><span>(</span><span>']'</span><span>)</span>
                <span>text</span> <span>=</span> <span>text</span><span>.</span><span>replace</span><span>(</span><span>db</span><span>,</span> <span>new</span><span>)</span>
        <span>after</span> <span>=</span> <span>len</span><span>(</span><span>text</span><span>)</span>
        <span>if</span> <span>before</span> <span>==</span> <span>after</span><span>:</span>
            <span>return</span> <span>text</span>
</code></pre></div></div>

<p>These functions worked-ish. You might notice the carat <code>^</code> in the curly function. This told it to match anything except another open curly bracket. This forced it to find the
innermost nested curly brackets. Again, this mostly worked, but it failed a few times and I gave up trying to figure out why. The square brackets are a bit different, as
they tend not to be nested but the inner syntax could be several different things. I opted for the simplest possible way and even so, it missed a few things. No idea why.</p>

<h2 id="attempt-15---literal-evals">Attempt 1.5 - Literal Evals</h2>

<p>I suppose I should rewind and give some context. Wikipedia dump files are effing huge. Even with 32GB of RAM on my desktop, I was rapidly running out of memory just 
loading one chunk at a time. So that meant I had to read each file line by line. Like so:</p>

<div><div><pre><code><span>with</span> <span>open</span><span>(</span><span>file</span><span>,</span> <span>'r'</span><span>,</span> <span>encoding</span><span>=</span><span>'utf-8'</span><span>)</span> <span>as</span> <span>infile</span><span>:</span>
    <span>for</span> <span>line</span> <span>in</span> <span>infile</span><span>:</span>
        <span>line</span> <span>=</span> <span>literal_eval</span><span>(</span><span>f'"""</span><span>{</span><span>line</span><span>}</span><span>"""'</span><span>)</span>  <span># this works... sometimes
</span>        <span>if</span> <span>'&lt;page&gt;'</span> <span>in</span> <span>line</span><span>:</span>  <span># new article
</span>            <span>article</span> <span>=</span> <span>''</span>
        <span>elif</span> <span>'&lt;/page&gt;'</span> <span>in</span> <span>line</span><span>:</span>  <span># end of article
</span>            <span>article</span> <span>+=</span> <span>line</span>
</code></pre></div></div>

<p>This works great for just reading the thing one at a time. One consistency in the Wikipedia dumps is that every page starts and ends with <code>&lt;page&gt;</code> and <code>&lt;/page&gt;</code> respectively.
This served as a great demarcation. So I tried to handle the unicode literals as they were coming in with <a href="https://www.kite.com/python/docs/ast.literal_eval">ast.literal_eval</a>. 
Spoiler: It worked. A little bit. This function frequently bombs out for various reasons.</p>

<h2 id="attempt-2---existing-parsers">Attempt 2 - Existing Parsers</h2>

<p>I finally gave up on manually parsing WikiMedia Text and found some extant parsers. First up is <a href="https://pypi.org/project/wikitextparser/">wikitextparser</a> which, as of this writing, is actively maintained.
Second up was the simple <a href="https://pypi.org/project/html2text/">html2text</a> which got some of the stuff the first missed. These premade parsers are great in that they 
don’t require me to use any of my own brain power! They are, however, far slower than my regex replace functions. It can’t be avoided, though.</p>

<p>So now my output looks more like this:</p>

<div><div><pre><code><span>[</span><span>
 </span><span>{</span><span>
  </span><span>"id"</span><span>:</span><span> </span><span>"4413617"</span><span>,</span><span>
  </span><span>"text"</span><span>:</span><span> </span><span>"The Samajtantrik Sramik Front is a national trade union federation in Bangladesh. It is affiliated with the World Federation of Trade Unions..."</span><span>,</span><span>
  </span><span>"title"</span><span>:</span><span> </span><span>"Samajtantrik Sramik Front"</span><span>
 </span><span>},</span><span>
 </span><span>{</span><span>
  </span><span>"id"</span><span>:</span><span> </span><span>"2618"</span><span>,</span><span>
  </span><span>"text"</span><span>:</span><span> </span><span>"Aeacus (; also spelled Eacus; Ancient Greek: </span><span>\u</span><span>0391</span><span>\u</span><span>1f30</span><span>\u</span><span>03b1</span><span>\u</span><span>03ba</span><span>\u</span><span>03cc</span><span>\u</span><span>03c2 Aiakos or Aiacos) was a mythological king of the island of Aegina..."</span><span>,</span><span>
  </span><span>"title"</span><span>:</span><span> </span><span>"Aeacus"</span><span>
 </span><span>},</span><span>
 </span><span>{</span><span>
  </span><span>"id"</span><span>:</span><span> </span><span>"3201"</span><span>,</span><span>
  </span><span>"text"</span><span>:</span><span> </span><span>"[[File:Global_Temperature_And_Forces.svg|thumb|upright=1.35|right|Observed temperature from NASA. vs the 1850</span><span>\u</span><span>20131900 average used by the IPCC as a pre- industrial baseline.. The primary driver for increased global temperatures in the industrial era is human activity, with natural forces adding variability. Figure 3.1 panel 2, Figure 3.3 panel 5.]] Attribution of recent climate change is the "</span><span>,</span><span>
  </span><span>"title"</span><span>:</span><span> </span><span>"Attribution of recent climate change"</span><span>
 </span><span>},</span><span>
</span><span>]</span><span>
</span></code></pre></div></div>

<p>It’s much cleaner and moving in the right direction but I still have to figure out the literal eval reliably and a few square brackets are making it through as well. 
These premade parsers are far slower but one advantage of cleaning up Wikipedia articles is that they end up far smaller without the markup. If you just want the accumulated 
knowledge in plain text format, it ends up being a fraction of the size.</p>



<p>I can tolerate a few aberrations here and there but the perfectionist in me wants to do better. Anyways, here’s my script as it stands today:</p>

<div><div><pre><code><span>import</span> <span>re</span>
<span>import</span> <span>os</span>
<span>import</span> <span>json</span>
<span>from</span> <span>uuid</span> <span>import</span> <span>uuid4</span>
<span>import</span> <span>gc</span>
<span>from</span> <span>html2text</span> <span>import</span> <span>html2text</span> <span>as</span> <span>htt</span>
<span>import</span> <span>wikitextparser</span> <span>as</span> <span>wtp</span>


<span>archive_dir</span> <span>=</span> <span>'d:/WikipediaArchive/'</span>
<span>dest_dir</span> <span>=</span> <span>'D:/enwiki20201020/'</span>
<span>chars_per_file</span> <span>=</span> <span>40</span> <span>*</span> <span>1000</span> <span>*</span> <span>1000</span>  <span># create a consistently sized chunk (~40MB each)
</span>

<span>def</span> <span>dewiki</span><span>(</span><span>text</span><span>):</span>
    <span>text</span> <span>=</span> <span>wtp</span><span>.</span><span>parse</span><span>(</span><span>text</span><span>).</span><span>plain_text</span><span>()</span>
    <span>text</span> <span>=</span> <span>htt</span><span>(</span><span>text</span><span>)</span>
    <span>text</span> <span>=</span> <span>text</span><span>.</span><span>replace</span><span>(</span><span>'</span><span>\\</span><span>n'</span><span>,</span><span>' '</span><span>)</span>
    <span>text</span> <span>=</span> <span>re</span><span>.</span><span>sub</span><span>(</span><span>'\s+'</span><span>,</span> <span>' '</span><span>,</span> <span>text</span><span>)</span>
    <span>return</span> <span>text</span>
    

<span>def</span> <span>analyze_chunk</span><span>(</span><span>text</span><span>):</span>
    <span>try</span><span>:</span>
        <span>if</span> <span>'&lt;redirect title="'</span> <span>in</span> <span>text</span><span>:</span>  <span># this is not the main article
</span>            <span>return</span> <span>None</span>
        <span>else</span><span>:</span>
            <span>title</span> <span>=</span> <span>text</span><span>.</span><span>split</span><span>(</span><span>'&lt;title&gt;'</span><span>)[</span><span>1</span><span>].</span><span>split</span><span>(</span><span>'&lt;/title&gt;'</span><span>)[</span><span>0</span><span>]</span>
            <span>if</span> <span>':'</span> <span>in</span> <span>title</span><span>:</span>  <span># this is a talk, category, or other (not a real article)
</span>                <span>return</span> <span>None</span>
        <span>serial</span> <span>=</span> <span>text</span><span>.</span><span>split</span><span>(</span><span>'&lt;id&gt;'</span><span>)[</span><span>1</span><span>].</span><span>split</span><span>(</span><span>'&lt;/id&gt;'</span><span>)[</span><span>0</span><span>]</span>
        <span>content</span> <span>=</span> <span>text</span><span>.</span><span>split</span><span>(</span><span>'&lt;/text'</span><span>)[</span><span>0</span><span>].</span><span>split</span><span>(</span><span>'&lt;text'</span><span>)[</span><span>1</span><span>].</span><span>split</span><span>(</span><span>'&gt;'</span><span>,</span> <span>maxsplit</span><span>=</span><span>1</span><span>)[</span><span>1</span><span>]</span>
        <span>content</span> <span>=</span> <span>dewiki</span><span>(</span><span>content</span><span>)</span>
        <span>return</span> <span>{</span><span>'title'</span><span>:</span> <span>title</span><span>,</span> <span>'text'</span><span>:</span> <span>content</span><span>,</span> <span>'id'</span><span>:</span> <span>serial</span><span>}</span>
    <span>except</span><span>:</span>
        <span>return</span> <span>None</span>


<span>def</span> <span>save_data</span><span>(</span><span>data</span><span>):</span>
    <span>if</span> <span>len</span><span>(</span><span>data</span><span>)</span> <span>==</span> <span>0</span><span>:</span>
        <span>return</span>
    <span>filename</span> <span>=</span> <span>dest_dir</span> <span>+</span> <span>str</span><span>(</span><span>uuid4</span><span>())</span> <span>+</span> <span>'.json'</span>
    <span>print</span><span>(</span><span>'Saving:</span><span>\t</span><span>'</span><span>,</span> <span>filename</span><span>)</span>
    <span>with</span> <span>open</span><span>(</span><span>filename</span><span>,</span> <span>'w'</span><span>,</span> <span>encoding</span><span>=</span><span>'utf-8'</span><span>)</span> <span>as</span> <span>outfile</span><span>:</span>
        <span>json</span><span>.</span><span>dump</span><span>(</span><span>data</span><span>,</span> <span>outfile</span><span>,</span> <span>sort_keys</span><span>=</span><span>True</span><span>,</span> <span>indent</span><span>=</span><span>1</span><span>)</span>


<span>def</span> <span>main</span><span>(</span><span>file</span><span>):</span>
    <span>print</span><span>(</span><span>file</span><span>)</span>
    <span>outdata</span> <span>=</span> <span>list</span><span>()</span>
    <span>article</span> <span>=</span> <span>''</span>
    <span>total_len</span> <span>=</span> <span>0</span>
    <span>with</span> <span>open</span><span>(</span><span>archive_dir</span> <span>+</span> <span>file</span><span>,</span> <span>'r'</span><span>,</span> <span>encoding</span><span>=</span><span>'utf-8'</span><span>)</span> <span>as</span> <span>infile</span><span>:</span>
        <span>for</span> <span>line</span> <span>in</span> <span>infile</span><span>:</span>
            <span>if</span> <span>'&lt;page&gt;'</span> <span>in</span> <span>line</span><span>:</span>  <span># new article begins
</span>                <span>article</span> <span>=</span> <span>''</span>
            <span>elif</span> <span>'&lt;/page&gt;'</span> <span>in</span> <span>line</span><span>:</span>  <span># end of article
</span>                <span>doc</span> <span>=</span> <span>analyze_chunk</span><span>(</span><span>article</span><span>)</span>
                <span>if</span> <span>doc</span><span>:</span>
                    <span>outdata</span><span>.</span><span>append</span><span>(</span><span>doc</span><span>)</span>
                    <span>total_len</span> <span>+=</span> <span>len</span><span>(</span><span>doc</span><span>[</span><span>'text'</span><span>])</span>
                    <span>if</span> <span>total_len</span> <span>&gt;=</span> <span>chars_per_file</span><span>:</span>
                        <span>save_data</span><span>(</span><span>outdata</span><span>)</span>
                        <span>outdata</span> <span>=</span> <span>list</span><span>()</span>
                        <span>total_len</span> <span>=</span> <span>0</span>
            <span>else</span><span>:</span>
                <span>article</span> <span>+=</span> <span>line</span>
    <span>save_data</span><span>(</span><span>outdata</span><span>)</span>

    
<span>if</span> <span>__name__</span> <span>==</span> <span>'__main__'</span><span>:</span>
    <span>for</span> <span>file</span> <span>in</span> <span>os</span><span>.</span><span>listdir</span><span>(</span><span>archive_dir</span><span>):</span>
        <span>if</span> <span>'bz2'</span> <span>in</span> <span>file</span><span>:</span>
            <span>continue</span>
        <span>main</span><span>(</span><span>file</span><span>)</span>
        <span>gc</span><span>.</span><span>collect</span><span>()</span>
</code></pre></div></div>

<p>It’s not the most elegant solution but for just over 100 lines of code, it will parse almost all of Wikipedia and save it to 40MB chunks of JSON.
Running this script looks like the following:</p>

<div><div><pre><code><span>(</span>base<span>)</span> C:<span>\O</span>fflineWikipedia&gt;python jsonify_wikipedia.py</code></pre></div></div></section></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://daveshap.github.io/DavidShapiroBlog/automation/2020/11/15/parsing-all-wikipedia.html">https://daveshap.github.io/DavidShapiroBlog/automation/2020/11/15/parsing-all-wikipedia.html</a></em></p>]]>
            </description>
            <link>https://daveshap.github.io/DavidShapiroBlog/automation/2020/11/15/parsing-all-wikipedia.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25234490</guid>
            <pubDate>Sat, 28 Nov 2020 02:03:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Secure, Redundant DNS Using CoreDNS]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25234382">thread link</a>) | @m_sahaf
<br/>
November 27, 2020 | https://www.caffeinatedwonders.com/2020/11/27/secure-dns-proxy/ | <a href="https://web.archive.org/web/*/https://www.caffeinatedwonders.com/2020/11/27/secure-dns-proxy/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">
        


    
    <p><time>November 27, 2020</time></p><div>
        <p>Although <a href="https://transparencyreport.google.com/https/overview?hl=en&amp;time_os_region=chrome-usage:1;series:time;groupby:os&amp;lu=load_os_region&amp;load_os_region=chrome-usage:1;series:page-load;groupby:os">an increasing portion of the web is adopting HTTPS</a>, a core part of the web infrastrcture lags behind in the form of plaintext DNS. The authentication problem of DNS is resolved by DNSSEC, but the queries are still plaintext. There’s currently a number of competing solutions that offer both authenticity and privacy concerns, namely DNS-over-TLS, DNS-over-HTTPS, and <a href="https://dnscrypt.info/">DNSCrypt</a>. I’ll leave it to Cloudflare’s blog to explain DNS, DoT and DoH, and the work towards both: <a href="https://blog.cloudflare.com/dns-encryption-explained/">DNS Encryption Explained</a>. Due to operating systems not yet supporting encrypted DNS resolvers, some work needs to be done by the user, and this post describes part of the work using <a href="https://coredns.io/">CoreDNS</a>.</p>
<p>CoreDNS is pluggable DNS and service discovery server written in Go and the blessed DNS resolver of Kubernetes. Configuring CoreDNS is handled via setting up a series of <a href="https://coredns.io/plugins/">middlewares</a> in the Corefile, which is what its config file is called. The <a href="https://coredns.io/manual/toc/">CoreDNS Manual</a> page does a brilliant job describing what CoreDNS is, how it works, how to install it, the structure of the configuration file, and more. To build our secure resolver, let’s pick up one of the examples off that page and gradually build the enhanced version.</p>
<p>Start by creating a file named <code>Corefile</code> containing the following, which is lifted verbatim from CoreDNS website:</p>
<div><div>
<table><tbody><tr><td>
<pre><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span></code></pre></td>
<td>
<pre><code data-lang="caddyfile"><span>.</span> <span>{</span>
    <span>forward</span> <span>.</span> <span>8</span><span>.8.8.8</span> <span>9</span><span>.9.9.9</span>
    <span>log</span>
<span>}</span>
</code></pre></td></tr></tbody></table>
</div>
</div><p>This is very simple, but let’s break it apart. The <code>.</code> means listen on port 53, the default port for UDP DNS, and across all interfaces. The configuration instructs CoreDNS to forward all requests to either <code>8.8.8.8</code> (Google DNS) or <code>9.9.9.9</code> (Quad9), picking either of them at random, and logs everything to stdout. This is not secure yet. This setup will forward all DNS queries in plaintext UDP packets. Let’s start by forwarding everything to <a href="https://developers.google.com/speed/public-dns/docs/dns-over-tls">Google DNS over TLS</a> resolvers without fallback:</p>
<div><div>
<table><tbody><tr><td>
<pre><code><span>1
</span><span>2
</span><span>3
</span><span>4
</span><span>5
</span><span>6
</span><span>7
</span><span>8
</span></code></pre></td>
<td>
<pre><code data-lang="caddyfile"><span>.</span> <span>{</span>
    <span>forward</span> <span>.</span> <span>tls://8.8.8.8</span> <span>tls://8.8.4.4</span> <span>{</span><span>
</span><span>		# This tells CoreDNS the subject name
</span><span>		# on the certificate is: dns.google
</span><span></span>		<span>tls_servername</span> <span>dns.google</span>
	<span>}</span>
    <span>log</span>
<span>}</span>
</code></pre></td></tr></tbody></table>
</div>
</div><p>At this point the setup has redundancy against Google’s services only. What if Google DNS service is down? The possibiity of a service going down is not far fetched, and we must account for it. However, note how the <code>tls_servername</code> directive can only be defined once, so it is bound to whatever upstream IP address we’re using. We can get around this by breaking the upstream into more local resolvers. I’ll add Cloudflare DNS as backup:</p>
<div><div>
<table><tbody><tr><td>
<pre><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span></code></pre></td>
<td>
<pre><code data-lang="caddyfile"><span>.:53</span> <span>{</span>
	<span>forward</span> <span>.</span> <span>127.0.0.1</span><span>:</span><span>5301</span> <span>127.0.0.1</span><span>:</span><span>5302</span> <span>[</span><span>::1</span><span>]</span><span>:5301</span> <span>[</span><span>::1</span><span>]</span><span>:5302</span>
	<span>log</span>
<span>}</span>
<span>.:5301</span> <span>{</span>
	<span>forward</span> <span>.</span> <span>tls://8.8.8.8</span> <span>tls://8.8.4.4</span> <span>{</span>
		<span>tls_servername</span> <span>dns.google</span>
	<span>}</span>
<span>}</span>
<span>.:5302</span> <span>{</span>
	<span>forward</span> <span>.</span> <span>tls://1.1.1.1</span> <span>tls://1.0.0.1</span> <span>{</span>
		<span>tls_servername</span> <span>cloudflare-dns.com</span>
	<span>}</span>
<span>}</span>
</code></pre></td></tr></tbody></table>
</div>
</div><p>Now we have established the pattern. To add more backups, add more server blocks where the upstream is defined and the respective <code>tls_servername</code> is configured, and add the localhost IP address to the main server of <code>.:53</code>. This can be further enhanced by adding caching, prefetching, and loading the <code>hosts</code> file. The final setup will look like this:</p>
<div><div>
<table><tbody><tr><td>
<pre><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span><span>26
</span><span>27
</span><span>28
</span></code></pre></td>
<td>
<pre><code data-lang="caddyfile"><span>.:53</span> <span>{</span>
	<span>hosts</span> <span>/path/to/hosts</span> <span>{</span>
		<span>fallthrough</span>
	<span>}</span>
	<span>cache</span> <span>{</span>
		<span>prefetch</span> <span>2</span> <span>30m</span>
	<span>}</span>
	<span>forward</span> <span>.</span> <span>127.0.0.1</span><span>:</span><span>5301</span> <span>127.0.0.1</span><span>:</span><span>5302</span> <span>[</span><span>::1</span><span>]</span><span>:5301</span> <span>[</span><span>::1</span><span>]</span><span>:5302</span> <span>127.0.0.1</span><span>:</span><span>5303</span> <span>[</span><span>::1</span><span>]</span><span>:5303</span> <span>{</span>
		<span>policy</span> <span>random</span>
	<span>}</span>
<span>}</span>
<span>.:5301</span> <span>{</span>
	<span>forward</span> <span>.</span> <span>tls://8.8.8.8</span> <span>tls://8.8.4.4</span> <span>{</span>
		<span>tls_servername</span> <span>dns.google</span>
	<span>}</span>
<span>}</span>
<span>.:5302</span> <span>{</span>
	<span>forward</span> <span>.</span> <span>tls://1.1.1.1</span> <span>tls://1.0.0.1</span> <span>{</span>
		<span>tls_servername</span> <span>cloudflare-dns.com</span>
	<span>}</span>
<span>}</span>
<span>.:5303</span> <span>{</span>
	<span>cache</span>
	<span>forward</span> <span>.</span> <span>tls://9.9.9.9</span> <span>{</span>
		<span>tls_servername</span> <span>dns.quad9.net</span>
	<span>}</span>
<span>}</span>

</code></pre></td></tr></tbody></table>
</div>
</div><p>Note that <code>policy</code> directive added in the first server block. Using <code>random</code> is superfluous because it is the default policy. The other policies available are <code>sequential</code> and <code>round_robin</code>. Regardless of the configured policy, CoreDNS will still perform health-checks and use a healthy upstream, so your queries are still answered even if one Google and/or Cloudflare are down. Configure your operating system to use your loopback address as the DNS server, set up CoreDNS service to start at boot, and your DNS queries are secure going forward.</p>

        
    </div>
    

    

    


        
        </div></div>]]>
            </description>
            <link>https://www.caffeinatedwonders.com/2020/11/27/secure-dns-proxy/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25234382</guid>
            <pubDate>Sat, 28 Nov 2020 01:44:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Textbook Recommendations for Software Engineers]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25234227">thread link</a>) | @victorbreder
<br/>
November 27, 2020 | https://breder.org/5/ | <a href="https://web.archive.org/web/*/https://breder.org/5/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>



<p>2020-11-27</p>

<p>Here are my favorite books on Mathematics, Computer Science and Artificial Intelligence:</p>

<h2>Mathematics</h2>

<p><strong>Calculus, by James Stewart</strong> -- Derivatives and integrals for single-variable, multi-variable and vector functions. Also touches partial derivatives, differential equations and infinite series.</p>

<p><strong>Linear Algebra Done Right, by Sheldon Axler</strong> -- Real and complex vector spaces, linear transformations, eigenvalues and eigenvectors, inner product spaces, trace and determinants.</p>

<p><strong>Probability and Statistics, by Jay Devore</strong> -- Random variables, probability distributions, point estimation, statistical inference, regression and correlation.</p>


<h2>Computer Science</h2>

<p><strong>Introduction to Programming Using Java, by David Eck</strong> -- Also known as "JavaNotes". Variables and types, objects and classes, control flow, arrays, GUI programming, I/O, files, networking, threads.

</p><p><strong>The C Programming Language, by Kerninghan and Ritchie</strong> -- The classical K^&amp;R book on the ubiquitous C programming language. Variables, data types, control flow, functions, pointers, arrays.</p>

<p><strong>Introduction to Algorithms, by Thomas Cormen</strong> -- Algorithmic complexity, sorting, data structures, dynamic programming, divide-and-conquer, greedy algorithms, graph algorithms, string matching.</p>

<p><strong>Design Pattern, by Gamma</strong> -- The classical book on patterns for tackling recurring higher-level problems written by the "Gang of Four".</p>

<p><strong>Compilers, by Aho</strong> -- Another classic book, known as "The Purple Dragon Book", tackling the inner-workings of compilers.</p>


<h2>Artificial Intelligence</h2>

<p><strong>Artificial Intelligence, by Russel and Norvig</strong> -- A complete overview of the field of AI: Search, constraint satisfaction, logic, uncertain knowledge and reasoning, learning, natural language processing.</p>

<p><strong>Deep Learning, by Ian Goodfellow</strong> -- A complete reference for Machine Learning and Deep Learning: artificial neural networks, regularization, convolutional networks, recursive networks, auto-encoders, generative models.</p>


</div></div>]]>
            </description>
            <link>https://breder.org/5/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25234227</guid>
            <pubDate>Sat, 28 Nov 2020 01:16:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[MmWave vs. Sub-6 GHz 5G – What’s the Difference?]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25233982">thread link</a>) | @elephd
<br/>
November 27, 2020 | https://www.onesdr.com/2020/11/19/mmwave-vs-sub-6-ghz-5g-whats-the-difference/#more-1996 | <a href="https://web.archive.org/web/*/https://www.onesdr.com/2020/11/19/mmwave-vs-sub-6-ghz-5g-whats-the-difference/#more-1996">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="text">
			
<p>For the past few years we’ve been hearing about mmWave 5G and now, in 2020 it’s finally here! mmWave 5G promises amazing speeds of 10 Gigabits/second, very low latency of around 1 millisecond, and many other amazing features. It currently operates in the frequency range between 24 GHz and 40 GHz. </p>



<p>Verizon has deployed this technology in many major cities across the USA. Here is a map that shows Verizon’s mmWave or <a rel="noreferrer noopener" href="https://www.verizon.com/about/news/fastest-5g-network-world-just-got-bigger-and-better" target="_blank">Ultra Wideband 5G</a> deployments.  </p>







<figure><img loading="lazy" width="1024" height="399" src="https://www.onesdr.com/wp-content/uploads/2020/11/Verizon-5G-1024x399.png" alt="" srcset="https://www.onesdr.com/wp-content/uploads/2020/11/Verizon-5G-1024x399.png 1024w, https://www.onesdr.com/wp-content/uploads/2020/11/Verizon-5G-300x117.png 300w, https://www.onesdr.com/wp-content/uploads/2020/11/Verizon-5G-768x299.png 768w, https://www.onesdr.com/wp-content/uploads/2020/11/Verizon-5G-1536x598.png 1536w, https://www.onesdr.com/wp-content/uploads/2020/11/Verizon-5G.png 2031w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>mmWave 5G or 5G Ultra Wideband deployments in USA</figcaption></figure>



<p>While mmWave 5G deployments have been limited to date, they will grow over time. Verizon will also be deploying Sub-6 GHz 5G networks this year. As the name suggests these networks will be deployed at frequencies below 6 GHz. </p>



<p>In 2020 we have also seen many new 5G phones – some of which <a href="https://amzn.to/3ffJ30C">support mmWave</a> and <a href="https://amzn.to/3923Cwy">others that don’t</a>.</p>



<h4><strong>The biggest differences between mmWave 5G and all other wireless systems</strong></h4>



<p>As the name suggests, mmWave 5G operates at mmWave frequencies starting at 24 GHz. Why do we need to move to mmWave frequencies? To support higher upload and download speeds we need more bandwidth. A single channel of mmWave 5G can be 400 MHz wide. That large amount of contiguous bandwidth is simply not available at lower frequencies as almost all the spectrum has already been allocated for other applications. </p>



<p>The picture below shows the bandwidth of 5G systems relative to other cellular technologies. </p>



<div><figure><img loading="lazy" src="https://www.onesdr.com/wp-content/uploads/2020/11/2g3g4g5g-bandwidths-1024x236.png" alt="" width="690" height="159" srcset="https://www.onesdr.com/wp-content/uploads/2020/11/2g3g4g5g-bandwidths-1024x236.png 1024w, https://www.onesdr.com/wp-content/uploads/2020/11/2g3g4g5g-bandwidths-300x69.png 300w, https://www.onesdr.com/wp-content/uploads/2020/11/2g3g4g5g-bandwidths-768x177.png 768w, https://www.onesdr.com/wp-content/uploads/2020/11/2g3g4g5g-bandwidths.png 1446w" sizes="(max-width: 690px) 100vw, 690px"><figcaption>Bandwidths of cellular technologies</figcaption></figure></div>



<p>The maximum bandwidth of 5G is 20 times larger than the maximum bandwidth of a 4G signal. That makes sense when you compare the theoretical maximum achievable speed of a 4G network at 300 Mbps with that of a 5G network at 10,000 Mbps.  </p>



<h4><strong><span>Let’s now take a look at some of the major RF related challenges with mmWave 5G relative to previous generations of wireless systems</span></strong></h4>



<h3><strong>Radio Signals in the Real World</strong></h3>



<p>To put things in context, we have never operated our cellular communication systems at mmWave frequencies. The highest we have operated at is around 2 GHz for 4G systems and 6 GHz for Wi-Fi systems.</p>



<div><figure><img loading="lazy" src="https://www.onesdr.com/wp-content/uploads/2020/11/frequency-ranges-1024x576.png" alt="" width="669" height="376" srcset="https://www.onesdr.com/wp-content/uploads/2020/11/frequency-ranges-1024x576.png 1024w, https://www.onesdr.com/wp-content/uploads/2020/11/frequency-ranges-300x169.png 300w, https://www.onesdr.com/wp-content/uploads/2020/11/frequency-ranges-768x432.png 768w, https://www.onesdr.com/wp-content/uploads/2020/11/frequency-ranges-1536x864.png 1536w, https://www.onesdr.com/wp-content/uploads/2020/11/frequency-ranges.png 1600w" sizes="(max-width: 669px) 100vw, 669px"><figcaption>Frequency ranges for different wireless technologies</figcaption></figure></div>



<p>The only wireless systems we have implemented at mmWave frequencies are point-to-point, line-of-sight links. We will, for the first time ever, have to deal with operating in challenging mmWave RF environments. </p>



<p>What does that mean?</p>



<p>mmWave signals don’t propagate very far. They are in fact obstructed by even the smallest of objects on account of their small wavelength. Trees, buildings, moisture – all affect mmWave signal propagation. As a result, cities and suburbs are very challenging environments from a mmWave perspective. </p>



<div><figure><img loading="lazy" width="1024" height="576" src="https://www.onesdr.com/wp-content/uploads/2020/11/objects-that-affect-5G-1024x576.png" alt="" srcset="https://www.onesdr.com/wp-content/uploads/2020/11/objects-that-affect-5G-1024x576.png 1024w, https://www.onesdr.com/wp-content/uploads/2020/11/objects-that-affect-5G-300x169.png 300w, https://www.onesdr.com/wp-content/uploads/2020/11/objects-that-affect-5G-768x432.png 768w, https://www.onesdr.com/wp-content/uploads/2020/11/objects-that-affect-5G-1536x864.png 1536w, https://www.onesdr.com/wp-content/uploads/2020/11/objects-that-affect-5G.png 1600w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Things that affect mmWave 5G signals</figcaption></figure></div>



<p>So how do we propose to solve the problem? The plan is to deploy about five times as many 5G base stations as with 4G and to use a technology called beamforming. As the name suggests this focuses RF energy beams in specific directions – toward users in particular. </p>



<p>However there still remains the fact that we have a relatively poor understanding of how exactly RF behaves at these frequencies in city environments where things change fast. Buildings and other structures appear relatively quickly and sometimes unexpectedly. The behavior of RF signals will then change accordingly and often unpredictably.</p>



<p>Unlike with previous systems operating at 2 GHz, we have not had the benefit of four decades of research and development. We have barely had two years to develop an understanding of how mmWave will work.</p>



<h3><strong>Complex Phone Development</strong></h3>



<p>The average phone today has multiple wireless sub-systems operating in it. Everything from 2G to 4G with Wi-Fi, Bluetooth, NFC and sometimes even FM. In small cell phone dimensions this is potentially chaos from an RF standpoint with signals interfering with one another. Increased power consumption of mmWave components is another significant issue as it results in more heat dissipation and lower battery life.</p>



<div><figure><img loading="lazy" src="https://www.onesdr.com/wp-content/uploads/2020/11/phone-with-multiple-standards.png" alt="" width="332" height="373" srcset="https://www.onesdr.com/wp-content/uploads/2020/11/phone-with-multiple-standards.png 703w, https://www.onesdr.com/wp-content/uploads/2020/11/phone-with-multiple-standards-267x300.png 267w" sizes="(max-width: 332px) 100vw, 332px"><figcaption>Different wireless technologies in a phone today</figcaption></figure></div>



<h3><strong>Higher Build and Development Costs</strong></h3>



<p>mmWave phones have multiple RF antenna modules designed to improve signal reception – a complex task without a doubt. Each of these has its own amplification, power management and transceiver. This complexity adds to the cost of hardware and results in a factor of <a href="https://benchmarking.ihsmarkit.com/614670/the-first-5g-phone-in-the-united-states-is-a-compromise-of-cost-and-design-ihs-markit-says" target="_blank" rel="noreferrer noopener">three times higher</a> relative to the average 4G smartphone.</p>



<p><a href=""></a>What about testing mmWave phones on the manufacturing floor? As mentioned earlier, most of our wireless communication systems have been operating below 2 GHz. Even when you consider Wi-Fi, we don’t get any higher than 6 GHz. There are a number of RF testing products such as <a href="https://amzn.to/3fg0luz">spectrum analyzers </a>and <a href="https://amzn.to/2UFEG5N">signal generators</a> that operate up to 6 GHz. They are not very expensive. 5G mmWave however requires test equipment that has to cover 40 GHz at least. Such equipment can cost as much as $100,000 or more. Not very affordable at all.</p>



<h2><strong>Summary</strong></h2>



<p>In this post we have talked about some of the RF challenges that we need to overcome to make mmWave 5G a success. We provided an overview of about three broad areas: RF propagation, Phone hardware and Development costs. The challenges in each of these areas are very daunting but exciting at the same time.</p>



<p><a href="https://www.onesdr.com/2020/09/27/how-to-measure-5g-radiation/">Read our post on how to measure 5G radiation</a></p>



<figure><img loading="lazy" width="1024" height="146" src="https://www.onesdr.com/wp-content/uploads/2020/02/mailing-list-2-1024x146.png" alt="Mailing List" srcset="https://www.onesdr.com/wp-content/uploads/2020/02/mailing-list-2-1024x146.png 1024w, https://www.onesdr.com/wp-content/uploads/2020/02/mailing-list-2-300x43.png 300w, https://www.onesdr.com/wp-content/uploads/2020/02/mailing-list-2-768x110.png 768w, https://www.onesdr.com/wp-content/uploads/2020/02/mailing-list-2-1536x220.png 1536w, https://www.onesdr.com/wp-content/uploads/2020/02/mailing-list-2.png 2000w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



	<div data-blog-id="171021735">
		<div>
			<form aria-describedby="wp-block-jetpack-mailchimp_consent-text">
				
												

				<p id="wp-block-jetpack-mailchimp_consent-text">
					<br>_________________________________________________<br>Icons made by&nbsp;<a href="https://www.flaticon.com/authors/freepik" target="_blank" rel="noreferrer noopener">Freepik</a>&nbsp;from&nbsp;<a href="https://www.flaticon.com/" target="_blank" rel="noreferrer noopener">www.flaticon.com</a> 				</p>

				
			</form>
			
				<p>
					Processing…				</p>
				<p>
					Success! You're on the list.				</p>
				<p>
					Whoops! There was an error and we couldn't process your subscription. Please reload the page and try again.				</p>

					</div>
	</div>
	











		</div></div>]]>
            </description>
            <link>https://www.onesdr.com/2020/11/19/mmwave-vs-sub-6-ghz-5g-whats-the-difference/#more-1996</link>
            <guid isPermaLink="false">hacker-news-small-sites-25233982</guid>
            <pubDate>Sat, 28 Nov 2020 00:38:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Bullshit Release Notes]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25233944">thread link</a>) | @neilpanchal
<br/>
November 27, 2020 | https://neil.computer/notes/bullshit-release-notes/ | <a href="https://web.archive.org/web/*/https://neil.computer/notes/bullshit-release-notes/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
    
<header>
    
</header>
        <!--Display single note. This template is dispatched by post.hbs-->
<main>
        <article>
        <h2>Bullshit Release Notes</h2>
        <figure><img src="https://neil.computer/content/images/2020/11/slack-update.png" alt="Recent Slack Update - Release Notes" srcset="https://neil.computer/content/images/size/w600/2020/11/slack-update.png 600w, https://neil.computer/content/images/size/w1000/2020/11/slack-update.png 1000w, https://neil.computer/content/images/size/w1600/2020/11/slack-update.png 1600w, https://neil.computer/content/images/2020/11/slack-update.png 1844w" sizes="(min-width: 720px) 720px"><figcaption>Recent Slack Update - Release Notes</figcaption></figure><blockquote>We've tinkered with the internal workings and polished some rough edges. The app is now better than it was.</blockquote><p>How about actually telling us what changed? This is a common practice in already hostile iOS and macOS App stores.</p><p>One or more of the following is true:</p><ul><li>Developers who build these apps are lazy and negligent</li><li>Lawyers have advised not to publish release notes with updates as a way to protect the company from liability</li><li>We must &nbsp;hide "technical" information that might overwhelm our users</li></ul><p>First one, I do not believe can be true. Developers generally want to keep track of what they did and they would already have this information in their PRs/repos. </p><p>Second, I think this is plausible but then again, the EULA says you can't sue the company for breaking something.</p><p>Third - this is the most likely reason. Why are we treating the general public like they're too afraid of complexity? It's not like the release notes make people nervous or fatigued. It is <em>optional</em> to read. It's tucked away.</p><p>I've had a number of paid apps that have gone downhill since and are no longer usable. They've incrementally degraded the experience <em>without informing the user. </em>I am left with a tough choice - never update the app or take a chance.</p><p>This practice needs to stop, I don't see why release notes cannot be part of a massive publicly traded company and the software being used by millions of people.</p>
        </article>
</main>
<p>
    <a href="https://neil.computer/">← Back to Home</a>
</p>

    


    

</div>]]>
            </description>
            <link>https://neil.computer/notes/bullshit-release-notes/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25233944</guid>
            <pubDate>Sat, 28 Nov 2020 00:31:13 GMT</pubDate>
        </item>
    </channel>
</rss>
