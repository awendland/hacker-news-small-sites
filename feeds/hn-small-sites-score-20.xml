<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 20]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 20. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Sun, 01 Nov 2020 04:22:08 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-20.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Sun, 01 Nov 2020 04:22:08 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Getting Audio Visualizations Working with Web Audio API]]>
            </title>
            <description>
<![CDATA[
Score 38 | Comments 6 (<a href="https://news.ycombinator.com/item?id=24938292">thread link</a>) | @arcatech
<br/>
October 29, 2020 | https://dwayne.xyz/post/audio-visualizations-web-audio-api | <a href="https://web.archive.org/web/*/https://dwayne.xyz/post/audio-visualizations-web-audio-api">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <h2>Web Audio API</h2>
<p>I’ve been working on getting WebRTC video chat working here on the website for a few weeks now. I finally got to the point where both text, video chat, and screen sharing all work really well, but somewhere in the back of my mind I kept thinking about complaints about “<a href="https://www.psychologytoday.com/us/blog/brain-waves/202007/why-zoom-fatigue-is-real-and-what-you-can-do-about-it">Zoom fatigue</a>” during the pandemic:</p>
<blockquote>
<p>Zoom fatigue, Hall argues now, is real. “Zoom is exhausting and lonely because you have to be so much more attentive and so much more aware of what’s going on than you do on phone calls.” If you haven’t turned off your own camera, you are also watching yourself speak, which can be arousing and disconcerting. The blips, delays and cut off sentences also create confusion. Much more exploration needs to be done, but he says, “maybe this isn’t the solution to our problems that we thought it might have been.” Phone calls, by comparison, are less demanding. “You can be in your own space. You can take a walk, make dinner,” Hall says.</p>
</blockquote>

<p>It’s kind of an interesting thing to have on your mind while spending weeks writing/debugging/testing video chat code.</p>
<p>So I decided to add an audio-only mode. And if I was gonna do that, I had to show something cool in place of the video. So I figured I would try to add audio visualizations when one or both of the users didn’t have video on. Using the relatively recent<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup> <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API">Web Audio API</a> seemed like the right way to go.</p>
<p>Here’s what I came up with:</p>
<div><video controls="" muted="" autoplay="" playsinline="" loop=""><source src="https://media.dwayne.xyz/blog/audio-visualization.mp4" type="video/mp4"></video><p>Screen recording of the local audio visualization. I cycle through bar graph in light mode, bar graph in dark mode, sine wave in dark mode, then sine wave in light mode.</p></div>
<h2>Creating and hooking up an AnalyserNode</h2>
<p>To create audio visualizations, the first thing you’ll need is an <code>AnalyserNode</code>, which you can get from the <code>createAnalyser</code> method of a <code>BaseAudioContext</code>. You can get both of these things pretty easily<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup> like this:</p>
<pre><span>1</span><span>const</span> audioContext <span>=</span> <span>new</span> <span>window</span>.AudioContext();
<span>2</span><span>const</span> analyser <span>=</span> audioContext.createAnalyser();
</pre><p>Next, create a <code>MediaStreamAudioSourceNode</code> from an existing data stream (I use either the local or remote data streams from either <code>getUserMedia</code> or from the ‘track’ event of <code>RTCPeerConnection</code> respectively) using <code>AudioContext.createMediaStreamSource</code>. Then you can connect that audio source to the analyser object like this:</p>
<pre><span>1</span><span>const</span> audioSource <span>=</span> <span>this</span>.audioContext.createMediaStreamSource(stream);
<span>2</span>audioSource.connect(analyser);
</pre>
<h2>Using requestAnimationFrame</h2>
<p><code>window.requestAnimationFrame</code> is nice. Call it, passing in your drawing function, and then inside that function call <code>requestAnimationFrame</code> again. Get yourself a nice little recursive loop going that’s automatically timed properly by the browser.</p>
<p>In my situation, there will either be 0, 1, or 2 visualizations running, since either side can choose either video chat, audio-only (…except during screen sharing), or just text chat. So I have one loop that draws both. It looks like this:</p>
<pre><span>1</span><span>const</span> drawAudioVisualizations <span>=</span> () =&gt; {
<span>2</span>    audioCancel <span>=</span> <span>window</span>.requestAnimationFrame(drawAudioVisualizations);
<span>3</span>    localAudioVisualization.draw();
<span>4</span>    remoteAudioVisualization.draw();
<span>5</span>};
</pre><p>I created the class for those visualization objects, and they handle whether or not to draw. They each contain the analyser, source, and context objects for their visualization.</p>
<p>Then when I detect that loop doesn’t have to run anymore, I can cancel it using that <code>audioCancel</code> value:</p>
<pre><span>1</span><span>window</span>.cancelAnimationFrame(audioCancel);
<span>2</span>audioCancel <span>=</span> <span>0</span>;
</pre>
<h2>Configuring the Analyser</h2>
<p>Like in the <a href="https://github.com/mdn/voice-change-o-matic/blob/gh-pages/scripts/app.js">example you’ll see a lot</a> if you look at the MDN documentation for this stuff, I provide options for two audio visualizations: frequency bars and a sine wave. Here’s how I configure the analyser for each type:</p>
<pre><span> 1</span><span>switch</span> (<span>this</span>.type) {
<span> 2</span>    <span>case</span> <span>'frequencybars'</span><span>:</span>
<span> 3</span>        <span>this</span>.analyser.minDecibels <span>=</span> <span>-</span><span>90</span>;
<span> 4</span>        <span>this</span>.analyser.maxDecibels <span>=</span> <span>-</span><span>10</span>;
<span> 5</span>        <span>this</span>.analyser.smoothingTimeConstant <span>=</span> <span>0.85</span>;
<span> 6</span>        <span>this</span>.analyser.fftSize <span>=</span> <span>256</span>;
<span> 7</span>        <span>this</span>.bufferLength <span>=</span> <span>this</span>.analyser.frequencyBinCount;
<span> 8</span>        <span>this</span>.dataArray <span>=</span> <span>new</span> Uint8Array(<span>this</span>.bufferLength);
<span> 9</span>        <span>break</span>;
<span>10</span>    <span>default</span><span>:</span>
<span>11</span>        <span>this</span>.analyser.minDecibels <span>=</span> <span>-</span><span>90</span>;
<span>12</span>        <span>this</span>.analyser.maxDecibels <span>=</span> <span>-</span><span>10</span>;
<span>13</span>        <span>this</span>.analyser.smoothingTimeConstant <span>=</span> <span>0.9</span>;
<span>14</span>        <span>this</span>.analyser.fftSize <span>=</span> <span>1024</span>;
<span>15</span>        <span>this</span>.bufferLength <span>=</span> <span>this</span>.analyser.fftSize;
<span>16</span>        <span>this</span>.dataArray <span>=</span> <span>new</span> Uint8Array(<span>this</span>.bufferLength);
<span>17</span>        <span>break</span>;
<span>18</span>}
</pre><div><p>I’ve adjusted these numbers a lot, and I’m gonna keep doing it. A note about <code>fftSize</code> and <code>frequencyBinCount</code>: <code>frequencyBinCount</code> is set right after you set <code>fftSize</code> and it’s usually just half the <code>fftSize</code> value. These values are about the amount of data you want to receive from the main analyser functions I’m about to talk about next. As you can see, they directly control the size of the data array that you’ll use to store the audio data on each draw call.
</p></div>
<h2>Using the Analyser</h2>
<p>On each draw call, depending on the type of visualization, call either <code>getByteFrequencyData</code> or <code>getByteTimeDomainData</code> with the array that was created above, and it’ll be filled with data. Then you run a simple loop over each element and start drawing. Here’s my sine wave code:</p>
<pre><span> 1</span><span>this</span>.analyser.getByteTimeDomainData(<span>this</span>.dataArray);
<span> 2</span><span>this</span>.ctx.lineWidth <span>=</span> <span>2</span>;
<span> 3</span><span>this</span>.ctx.strokeStyle <span>=</span> audioSecondaryStroke;
<span> 4</span>
<span> 5</span><span>this</span>.ctx.beginPath();
<span> 6</span>
<span> 7</span><span>let</span> v, y;
<span> 8</span><span>for</span> (<span>let</span> i <span>=</span> <span>0</span>; i <span>&lt;</span> <span>this</span>.bufferLength; i<span>++</span>) {
<span> 9</span>    v <span>=</span> <span>this</span>.dataArray[i] <span>/</span> <span>128.0</span>;
<span>10</span>    y <span>=</span> v <span>*</span> height <span>/</span> <span>2</span>;
<span>11</span>
<span>12</span>    <span>if</span> (i <span>===</span> <span>0</span>) {
<span>13</span>        <span>this</span>.ctx.moveTo(x, y);
<span>14</span>    } <span>else</span> {
<span>15</span>        <span>this</span>.ctx.lineTo(x, y);
<span>16</span>    }
<span>17</span>
<span>18</span>    x <span>+=</span> width <span>*</span> <span>1.0</span> <span>/</span> <span>this</span>.bufferLength;
<span>19</span>}
<span>20</span>
<span>21</span><span>this</span>.ctx.lineTo(width, height <span>/</span> <span>2</span>);
<span>22</span><span>this</span>.ctx.stroke();
</pre><div><p>The fill and stroke colors are dynamic based on the website color scheme.
</p></div>
<h2>Good ol' Safari</h2>
<p>So I did all of this stuff I just talked about, but for <strong>days</strong> I could <em>not</em> get this to work in Safari. Not because of errors or anything, but because both <code>getByteFrequencyData</code> and <code>getByteTimeDomainData</code> just filled the array with 0s every time. No matter what I did. I was able to get the audio data in Firefox just fine.</p>
<p>So at first, I figured it just didn’t work at all in Safari and I would just have to wait until Apple fixed it. But then I came across <a href="https://mdn.github.io/voice-change-o-matic/">this sample audio project</a> and noticed it worked just fine in Safari.</p>
<div><p>So I studied the code for an hour trying to understand what was different about my code and theirs. I made a lot of changes to my code to make it more like what they were doing. One of the big differences is that they’re connecting the audio source to different audio distortion nodes to actually change the audio. I just want to create a visualization so I wasn’t using any of those objects.
</p></div>
<h3>Audio Distortion Effects</h3>
<p>The <code>BaseAudioContext</code> has a few methods you can use to create audio distortion objects.</p>
<ul>
<li><code>WaveShaperNode</code>: Use <code>BaseAudioContext.createWaveShaper</code> to create a non-linear distortion. You can use a custom function to change the audio data.</li>
<li><code>GainNode</code>: Use <code>BaseAudioContext.createGain</code> to control the overall gain (volume) of the audio.</li>
<li><code>BiquadFilterNode</code>: Use <code>BaseAudioContext.createBiquadFilter</code> to apply some common audio effects.</li>
<li><code>ConvolverNode</code>: Use <code>BaseAudioContext.createConvolver</code> to apply reverb effects to audio.</li>
</ul>
<p>Each one of these objects has a <code>connect</code> function where you pass another context, output, or filter. Each one has a certain number of inputs and outputs. Here’s an example from that sample project of connecting all of them:</p>
<pre><span>1</span>source <span>=</span> audioCtx.createMediaStreamSource(stream);
<span>2</span>source.connect(distortion);
<span>3</span>distortion.connect(biquadFilter);
<span>4</span>biquadFilter.connect(gainNode);
<span>5</span>convolver.connect(gainNode);
<span>6</span>gainNode.connect(analyser);
<span>7</span>analyser.connect(audioCtx.destination);
</pre><p><strong>Note</strong>: Don’t connect to your audio context <code>destination</code> if you’re just trying to create a visualization for a call. The user will hear themselves talking.</p>
<div><p>Anyway, I tried adding these things to my code to see if that would get it working in Safari, but I had no luck.
</p></div>
<h2>Figuring out the Safari issue</h2>
<p>I was starting to get <em>real</em> frustrated trying to figure this out. I was gonna let it go when I thought Safari was just broken (because it usually is), but since I knew it <em>could</em> work in Safari, I couldn’t leave it alone.</p>
<p>Eventually I downloaded the actual HTML and Javascript files from that sample and started removing shit from their code, running it locally and seeing if it worked. Which it did. So now I’m editing my own code, and <em>their code</em>, to get them to be pretty much the same. Which I did. And <strong>still</strong> theirs worked and mine didn’t.</p>
<p>Next I just started desperately logging every single object at different points in my code to figure out what the fuck was going on. Then I noticed something.</p>
<div><p><img src="https://media.dwayne.xyz/blog/audio-context-suspended.png" alt="Dev console showing the output of logging this.audioContext. The state attribute is shown as suspended"></p><p>Output of logging the audio context object.</p></div>
<p>The <code>state</code> is “suspended”? Why? I don’t know. I did the same log in the sample code (that I had downloaded and was running on my machine) and it was “running”.</p>
<p>This is the code that fixes it:</p>
<pre><span>1</span><span>this</span>.audioSource <span>=</span> <span>this</span>.audioContext.createMediaStreamSource(<span>this</span>.stream);
<span>2</span><span>this</span>.audioSource.connect(<span>this</span>.analyser);
<span>3</span><span>this</span>.audioContext.resume(); <span>// Why??????
</span></pre><div><p>Calling <code>resume</code> changes the state and then everything works. To this day I still don’t know why the sample code didn’t need that line.
</p></div>
<h2>Drawing the image and supporting light/dark modes</h2>
<p>Like everything else on my site, all of this must support different color schemes (and screen sizes, and mobile devices). That was surprisingly difficult when trying to draw an SVG on the canvas.</p>
<p>I’m using <a href="https://fontawesome.com/">FontAwesome</a> for all my icons on the site. I wanted to use one of them for these visualizations. The FontAwesome files are all SVGs (which is great), but I didn’t know how to draw the image in different colors in Javascript. The way I decided to do this was to load the SVG file into a Javascript <code>Image</code> object, then draw that onto the canvas each draw call.</p>
<p>That worked, but it only drew it black even after changing the fill and stroke colors. So after some web searching I read about someone deciding to draw out an image on an offscreen canvas, reading all the image data, and …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://dwayne.xyz/post/audio-visualizations-web-audio-api">https://dwayne.xyz/post/audio-visualizations-web-audio-api</a></em></p>]]>
            </description>
            <link>https://dwayne.xyz/post/audio-visualizations-web-audio-api</link>
            <guid isPermaLink="false">hacker-news-small-sites-24938292</guid>
            <pubDate>Fri, 30 Oct 2020 01:55:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Vega-Lite: A Grammar of Interactive Graphics]]>
            </title>
            <description>
<![CDATA[
Score 152 | Comments 36 (<a href="https://news.ycombinator.com/item?id=24937954">thread link</a>) | @tosh
<br/>
October 29, 2020 | https://vega.github.io/vega-lite/ | <a href="https://web.archive.org/web/*/https://vega.github.io/vega-lite/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  <section>
    <p>
  <strong>Vega-Lite</strong> is a high-level grammar of interactive graphics. It provides a concise, declarative JSON syntax to create an expressive range of visualizations for data analysis and presentation.
</p>

<p><span>
  <span>
    Vega-Lite specifications describe visualizations as encoding mappings from data to <strong>properties of graphical marks</strong> (e.g., points or bars).
    The Vega-Lite compiler <strong>automatically produces visualization components</strong> including axes, legends, and scales.
    It determines default properties of these components based on a set of <strong>carefully designed rules</strong>.
    This approach allows Vega-Lite specifications to be concise for quick visualization authoring, while giving user control to override defaults and customize various parts of a visualization.
    As we also designed Vega-Lite to support data analysis, Vega-Lite supports both <strong>data transformations</strong> (e.g., aggregation, binning, filtering, sorting) and <strong>visual transformations</strong> (e.g., stacking and faceting).
    Moreover, Vega-Lite specifications can be <strong>composed</strong> into layered and multi-view displays, and made <strong>interactive with selections</strong>.
  </span>
  <span>
  <a href="https://vega.github.io/vega-lite/tutorials/getting_started.html">Get started<br><small>Latest Version: 4.17.0</small></a>
  <a href="https://vega.github.io/editor/#/custom/vega-lite">Try online</a>
  </span>
</span></p>

<p>Compared to <a href="https://vega.github.io/vega">Vega</a>, Vega-Lite provides a more concise and convenient form to author common visualizations. As Vega-Lite can compile its specifications to Vega specifications, users may use Vega-Lite as the <em>primary</em> visualization tool and, if needed, transition to use the lower-level Vega for advanced use cases.</p>

<p>For more information, read our <a href="https://medium.com/@uwdata/de6661c12d58">introduction article to Vega-Lite v2 on Medium</a>, watch our <a href="https://www.youtube.com/watch?v=9uaHRWj04D4">OpenVis Conf talk about the new features in Vega-Lite v2</a>, see the <a href="https://vega.github.io/vega-lite/docs/">documentation</a> and take a look at our <a href="https://vega.github.io/vega-lite/examples/">example gallery</a>. Follow us on <a href="https://twitter.com/vega_vis">Twitter at @vega_vis</a> to stay informed about updates.</p>

<h2 id="example">Example</h2>



<h2 id="additional-links">Additional Links</h2>

<ul>
  <li>Award winning <a href="https://idl.cs.washington.edu/papers/vega-lite">research paper</a> and <a href="https://www.youtube.com/watch?v=9uaHRWj04D4">video of our OpenVis Conf talk</a> on the design of Vega-Lite</li>
  <li>Listen to a Data Stories episode about <a href="http://datastori.es/121-declarative-visualization-with-vega-lite-and-altair-with-dominik-moritz-jacob-vanderplas-kanit-ham-wongsuphasawat/">Declarative Visualization with Vega-Lite and Altair</a>
</li>
  <li>
<a href="http://json-schema.org/">JSON schema</a> specification for <a href="https://github.com/vega/schema">Vega-Lite</a> (<a href="https://vega.github.io/schema/vega-lite/v4.json">latest</a>)</li>
  <li>Ask questions about Vega-Lite on <a href="https://stackoverflow.com/tags/vega-lite">Stack Overflow</a> or <a href="https://bit.ly/join-vega-slack-2020">Slack</a>
</li>
  <li>Fork our <a href="https://bl.ocks.org/domoritz/455e1c7872c4b38a58b90df0c3d7b1b9">Vega-Lite Block</a>, or <a href="https://beta.observablehq.com/@domoritz/vega-lite-demo">Observable Notebook</a>.</li>
</ul>

<h2 id="users">Users</h2>

<p>Vega-Lite is used by thousands of data enthusiasts, developers, journalists, data scientists, teachers, and researchers across many organizations. Here are some of them. Learn about integrations on our <a href="https://vega.github.io/vega-lite/ecosystem.html">ecosystem page</a>.</p>



<h2 id="team">Team</h2>

<p>The development of Vega-Lite is led by the alumni and members of the <a href="https://idl.cs.washington.edu/">University of Washington Interactive Data Lab</a> (UW IDL), including <a href="https://twitter.com/kanitw">Kanit “Ham” Wongsuphasawat</a> (now at Apple), <a href="https://twitter.com/domoritz">Dominik Moritz</a> (now at CMU / Apple), <a href="https://twitter.com/arvindsatya1">Arvind Satyanarayan</a> (now at MIT), and <a href="https://twitter.com/jeffrey_heer">Jeffrey Heer</a> (UW IDL).</p>

<p>Vega-Lite gets significant contributions from its community–in particular <a href="https://willium.com/">Will Strimling</a>, <a href="https://github.com/YuhanLu">Yuhan (Zoe) Lu</a>, <a href="https://github.com/invokesus">Souvik Sen</a>, <a href="https://github.com/chanwutk">Chanwut Kittivorawong</a>, <a href="https://github.com/mattwchun">Matthew Chun</a>, <a href="https://github.com/AkshatSh">Akshat Shrivastava</a>, <a href="https://github.com/Saba9">Saba Noorassa</a>, <a href="https://github.com/sirahd">Sira Horradarn</a>, <a href="https://github.com/donghaoren">Donghao Ren</a>, and <a href="https://github.com/haldenl">Halden Lin</a>. Please see the <a href="https://github.com/vega/vega-lite/graphs/contributors">contributors page</a> for the full list of contributors.</p>

  </section>
</div></div>]]>
            </description>
            <link>https://vega.github.io/vega-lite/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24937954</guid>
            <pubDate>Fri, 30 Oct 2020 00:52:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Bizarre Design Choices in Zoom’s End-to-End Encryption]]>
            </title>
            <description>
<![CDATA[
Score 63 | Comments 37 (<a href="https://news.ycombinator.com/item?id=24937298">thread link</a>) | @notRobot
<br/>
October 29, 2020 | https://soatok.blog/2020/10/28/bizarre-design-choices-in-zooms-end-to-end-encryption/?hac | <a href="https://web.archive.org/web/*/https://soatok.blog/2020/10/28/bizarre-design-choices-in-zooms-end-to-end-encryption/?hac">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

		<div>

			
<p>Zoom recently announced that they were going to make end-to-end encryption available to all of their users–not just customers.</p>



<figure><div>

</div></figure>



<p>This is a good move, especially for people living in countries with <a href="https://soatok.blog/2020/07/02/how-and-why-america-was-hit-so-hard-by-covid-19/">inept leadership that failed to address the COVID-19 pandemic</a> and therefore need to conduct their work and schooling remotely through software like Zoom. I enthusiastically applaud them for making this change.</p>



<div><figure><img data-attachment-id="1333" data-permalink="https://soatok.blog/soatoktelegrams2020-08/" data-orig-file="https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png" data-orig-size="512,512" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="SoatokTelegrams2020-08" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png?w=512" src="https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png?w=512" alt="" srcset="https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png 512w, https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png?w=150 150w, https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png?w=300 300w" sizes="(max-width: 512px) 100vw, 512px"><figcaption>End-to-end encryption, on by default, is a huge win for everyone who uses Zoom. (Art by <a href="https://twitter.com/lynxvsjackalope">Khia</a>.)</figcaption></figure></div>



<p>The end-to-end encryption capability arrives on the heels of their acquisition of <a href="https://keybase.io/">Keybase</a> in earlier this year. Hiring a team of security experts and cryptography engineers seems like a good move overall.</p>



<p>Upon hearing this news, I decided to be a good neighbor and take a look at their source code, with the reasoning, “If so many people’s privacy is going to be dependent on Zoom’s security, I might as well make sure they’re not doing something ridiculously bad.”</p>



<p>Except I couldn’t find their source code anywhere online. But they did publish <a href="https://github.com/zoom/zoom-e2e-whitepaper">a white paper on Github</a>…</p>







<h2>Disclaimers</h2>



<p>What follows is the opinion of some guy on the Internet with a fursona–so whether or not you choose to take it seriously should be informed by this context. It is not the opinion of anyone’s employer, nor is it endorsed by Zoom, etc. Tell your lawyers to calm their nips.</p>



<p>More importantly, I’m not here to hate on Zoom for doing a good thing, nor on the security experts that worked hard on making Zoom better for their users. The responsibility of security professionals is to the users, after all.</p>



<p>Also, these aren’t zero-days, so don’t try to lecture me about “responsible” disclosure. (That term is also <a href="https://adamcaudill.com/2015/11/19/responsible-disclosure-is-wrong/">problematic</a>, by the way.)</p>



<p>Got it? Good. Let’s move on.</p>







<h2>Bizarre Design Choices in Version 2.3 of Zoom’s E2E White Paper</h2>



<p>Note: I’ve altered the screenshots to be white text on a black background, since my blog’s color scheme is darker than a typical academic PDF. You can find the source <a href="https://github.com/zoom/zoom-e2e-whitepaper/blob/d3be2a5a3e16be04f1199b92630f180ba79cb51c/zoom_e2e.pdf">here</a>.</p>



<h3>Cryptographic Algorithms</h3>



<div><figure><img data-attachment-id="1744" data-permalink="https://soatok.blog/zoom-e2e-02/" data-orig-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-02.png" data-orig-size="784,652" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zoom-e2e-02" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-02.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-02.png?w=580" src="https://soatok.files.wordpress.com/2020/10/zoom-e2e-02.png" alt=""></figure></div>



<p>It’s a little weird that they’re calculating a signature over SHA256(Context) || SHA256(M), considering Ed25519 uses SHA512 internally.</p>



<p>It would make just as much sense to sign Context || M directly–or, if pre-hashing large streams is needed, SHA512(Context || M).</p>



<div><figure><img data-attachment-id="1740" data-permalink="https://soatok.blog/zoom-e2e-01/" data-orig-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-01.png" data-orig-size="1039,788" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zoom-e2e-01" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-01.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-01.png?w=580" src="https://soatok.files.wordpress.com/2020/10/zoom-e2e-01.png" alt=""></figure></div>



<p>At the top of this section, it says it uses libsodium’s <code>crypto_box</code> interface. But then they go onto… not actually use it.</p>



<p>Instead, they wrote their own protocol using HKDF, two SHA256 hashes, and XChaCha20-Poly1305.</p>



<p>While secure, this isn’t <em>really</em> using the crypto_box interface.</p>



<p>The only part of the libsodium interface that’s being used is <code><a href="https://github.com/jedisct1/libsodium/blob/927dfe8e2eaa86160d3ba12a7e3258fbc322909c/src/libsodium/crypto_box/curve25519xsalsa20poly1305/box_curve25519xsalsa20poly1305.c#L35-L46">crypto_box_beforenm()</a></code>, which could easily have been a call to <code>crypto_scalarmult()</code>instead (since they’re passing the output of the scalar multiplication to HKDF anyway).</p>







<p>Also, the SHA256(a) || SHA256(b) pattern returns. Zoom’s engineers must love SHA256 for some reason.</p>



<p>This time, it’s in the additional associated data for the XChaCha20-Poly1305. </p>



<p>Binding the ciphertext and the signature to the same context string is a sensible thing to do, it’s just the concatenation of SHA256 hashes is a bit weird when SHA512 exists.</p>



<h3>Meeting Leader Security Code</h3>



<div><figure><img data-attachment-id="1746" data-permalink="https://soatok.blog/zoom-e2e-03/" data-orig-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-03.png" data-orig-size="760,733" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zoom-e2e-03" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-03.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-03.png?w=580" src="https://soatok.files.wordpress.com/2020/10/zoom-e2e-03.png" alt=""></figure></div>



<p>Here we see Zoom using the a SHA256 of a constant string (“<code>Zoombase-1-ClientOnly-MAC-SecurityCode</code>“) in a construction that tries but fails to be HMAC.</p>



<p>And then they concatenate it with the SHA256 hash of the public key (which is already a 256-bit value), and then they hash the whole thing again.</p>



<p>It’s redundant SHA256 all the way down. The redundancy of “MAC” and “SecurityCode” in their constant string is, at least, consistent with the rest of their design philosophy.</p>



<p>It would be a real shame if double-hashing carried the risk of <a href="https://eprint.iacr.org/2013/382">invalidating security proofs</a>, or if <a href="https://cseweb.ucsd.edu/~mihir/papers/kmd5.pdf">the security proof for HMAC</a> required a high Hamming distance of padding constants and this design decision also later <a href="https://eprint.iacr.org/2012/684.pdf">saved HMAC from related-key attacks</a>.</p>



<h3>Hiding Personal Details</h3>



<figure><img data-attachment-id="1750" data-permalink="https://soatok.blog/zoom-e2e-04/" data-orig-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png" data-orig-size="739,603" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zoom-e2e-04" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png?w=580" src="https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png?w=739" alt="" srcset="https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png 739w, https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png?w=150 150w, https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png?w=300 300w" sizes="(max-width: 739px) 100vw, 739px"></figure>



<p>Wait, you’re telling me Zoom was aware of HMAC’s existence this whole time?</p>



<div><figure><img data-attachment-id="1202" data-permalink="https://soatok.blog/soatoktelegrams2020-02/" data-orig-file="https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png" data-orig-size="512,512" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="SoatokTelegrams2020-02" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png?w=512" src="https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png?w=512" alt="" srcset="https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png 512w, https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png?w=150 150w, https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png?w=300 300w" sizes="(max-width: 512px) 100vw, 512px"><figcaption>I give up!</figcaption></figure></div>



<h2>Enough Pointless Dunking, What’s the Takeaway?</h2>



<p>None of the design decisions Zoom made that I’ve criticized here are security vulnerabilities, but they do demonstrate an early lack of cryptography expertise in their product design.</p>



<p>After all, the weirdness is almost entirely contained in section 3 of their white paper, which describes the “Phase I” of their rollout. So what I’ve pointed out here appears to be mostly legacy cruft that wasn’t risky enough to bother changing in their final design.</p>



<p>The rest of their paper is pretty straightforward and pleasant to read. Their design makes sense in general, and each phase includes an “Areas to Improve” section.</p>



<p>All in all, if you’re worried about the security of Zoom’s E2EE feature, the only thing they can really do better is to publish the source code (and link to it from the whitepaper repository for ease-of-discovery) for this feature so independent experts can publicly review it.</p>



<p>However, they seem to be getting a lot of mileage out of the experts on their payroll, so I wouldn’t count on that happening.</p>

		</div><!-- .entry-content -->

	</div></div>]]>
            </description>
            <link>https://soatok.blog/2020/10/28/bizarre-design-choices-in-zooms-end-to-end-encryption/?hac</link>
            <guid isPermaLink="false">hacker-news-small-sites-24937298</guid>
            <pubDate>Thu, 29 Oct 2020 23:23:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: The most advanced VPN and unblocker with industry-first features]]>
            </title>
            <description>
<![CDATA[
Score 49 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24937073">thread link</a>) | @Oeck
<br/>
October 29, 2020 | http://www.oeck.com/features/ | <a href="https://web.archive.org/web/*/http://www.oeck.com/features/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
	<div>
		<!--XF:EXTRA_OUTPUT-->

		

		

		
	

		
	<!--[if lt IE 9]><div class="blockMessage blockMessage&#45;&#45;important blockMessage&#45;&#45;iconic">You are using an out of date browser. It  may not display this or other websites correctly.<br />You should upgrade or use an <a href="https://www.google.com/chrome/browser/" target="_blank">alternative browser</a>.</div><![endif]-->


		
			<div>
			
				
					
				

				
					<p>The things that make us different.</p>
				
			
				
			</div>
		

		<div>
			

			<div>
				
				<div>

	



	
	
	










	<div>
	<div data-aos="fade-up">
		<div>
			<p><img src="http://www.oeck.com/assets/images/web/png/500/smartRouting.png">
			</p>
			<div>
				<div>
					<p><img src="http://www.oeck.com/assets/images/web/png/500/smartRouting.png"></p><p>
					<span>smartRouting</span>
					<br>
					Oeck takes away the need of connecting to a specific region to access streaming content. Get access to the latest shows from around the world without ever switching regions. Our revolutionary smartRouting unblocks some of the most popular services from around the globe. Enjoy fast, automated access to itv in the UK - Hulu in the US - iView in Australia and many more. Simply connect to the VPN location closest to you and we take care of the rest! 
					</p>
					
				</div>				
			</div>
		</div>
	</div>
</div>

<div>
	<div data-aos="fade-up">
		<div>
			<div>
				<p><img src="http://www.oeck.com/assets/images/web/png/500/deviceProfiles.png"></p><p>
				<span>Device Profiles</span>
				<br>
				Device Profiles allow you to take your VPN functionality to the next level. This handy feature allows you to set preferences for streaming services and traffic filtration on a per-device level.
					By doing this you can quickly set up a childs device to block adult content whilst keeping your other devices untouched. It also allows you to set up devices with different streaming regions.
					For example, you can have Netflix USA on one device, Netflix UK on another and Netflix Germany on yet another, whilst all the while being connected to the VPN region closest to you!
				</p>
				
			</div>
			<p><img src="http://www.oeck.com/assets/images/web/png/500/deviceProfiles.png">
			</p>
		</div>
	</div>
</div>

<div>
	<div data-aos="fade-up">
		<div>
			<p><img src="http://www.oeck.com/assets/images/web/png/500/adBlocker.png">
			</p>
			<div>
				<p><img src="http://www.oeck.com/assets/images/web/png/500/adBlocker.png"></p><div>
				<p><span>Cerberus</span>
				<br>
				Get powerful device-level filtering to prevent dangerous content reaching your family and devices. Our unique online guardian Cerberus is the must-have feature for families and individuals. Choose which content to block and prevent threats before they occur. Simply create a profile for your device and select the Cerberus services required.
					</p><p>
					
					You can filter Ads, Malware and Phishing, Adult and Social Networking sites individually or combined!
				</p></div>
				
			</div>
		</div>
	</div>
</div>

<div>
	<div>
		<div>
			<div>
				<p><span>Security</span>
					<br>
					<span>Built deep into our network and culture.</span>
				</p>
				<div data-aos="fade-up"><p>
					We secure your privacy using industry-leading encryption standards, on servers that we own. Our zero hard drive system won’t store any of your data, ever! Quickly block dangerous sites and services at the DNS level to prevent ads, malware, phishing sites and more.
					</p>
					
				</div>
			</div>
		</div>
	</div>	
</div>

<div>
	<div>
		<div data-aos="fade-up">
			<p><span>AES-256</span>
				<br>
				<span>Encryption</span>
			</p>
			<p><span>4096-bit</span>
				<br>
				<span>Key Exchange</span>
			</p>
			<p><span>Zero</span>
				<br>
				<span>Hard Drives</span>
			</p>
			<p><span>Zero</span>
				<br>
				<span>Logging</span>
			</p>
		</div>
	</div>
</div>

<div>
	<div data-aos="fade-up">
		<div>
			<p><img src="http://www.oeck.com/assets/images/web/png/500/portForwarding.png">
			</p>
			<div>
				<p><img src="http://www.oeck.com/assets/images/web/png/500/portForwarding.png"></p><p>
				<span>Advanced Port Forwarding</span>
				<br>
				This is a simple but handy feature with a twist. We issue you ports and you enable ports you select on your device profile(s). Then you simply tell us which port you would like to forward to. No need to configure your client or software to suit us. As an added bonus, you get your very own custom domain name per port. Regardless of which VPN region you connect to, your port-forwarding will always work!
				</p>
				
			</div>
		</div>
	</div>
</div>

<div>
	<div data-aos="fade-up">
		<div>
			<div>
				<p><img src="http://www.oeck.com/assets/images/web/png/500/customFilter.png"></p><p>
				<span>Custom DNS Filter</span>
				<br>
				Our built-in DNS filter allows you to decide what internet traffic you want hitting your device(s). Simply populate your filter list with websites that you want blocked and Oeck's VPN will follow those rules and block the traffic. Domain black lists can be set on a per-device level, which makes is perfect for parents who would like to restrict what their children can access. Best of all, it is completely unique to you.
				</p>
				
			</div>
			<p><img src="http://www.oeck.com/assets/images/web/png/500/customFilter.png">
			</p>
		</div>
	</div>
</div>

<div>
	<div data-aos="fade-up">
		<div>
			<p><img src="http://www.oeck.com/assets/images/web/png/500/customDNS.png">
			</p>
			<div>
				<p><img src="http://www.oeck.com/assets/images/web/png/500/customDNS.png"></p><p>
				<span>Secondary DNS</span>
				<br>
				A feature built for advanced users. Secondary DNS allows you to specify a DNS service to use that will bypass Oeck's DNS. To further simplify the feature, you can still allow Oeck to take control of some of the DNS queries and leave others up to your Secondary DNS.
				</p>
				
			</div>
		</div>
	</div>
</div>

<div>
	<div data-aos="fade-up">
		<div>
			<div>
				<p><img src="http://www.oeck.com/assets/images/web/png/500/serverSelection.png"></p><p>
				<span>Automatic Server Selection</span>
				<br>
				When selecting a VPN region to connect to, our network runs a check of the available servers and resources within that region. It then calculates which server will be the best server for you to connect to. It takes into account the available system resources of each server and so it will always connect you to the best available server. No more server surfing ever again!
				</p>
				
			</div>
			<p><img src="http://www.oeck.com/assets/images/web/png/500/serverSelection.png">
			</p>
		</div>
	</div>
</div>




	




</div>
				
			</div>

			
		</div>

		

		
	</div>
</div></div>]]>
            </description>
            <link>http://www.oeck.com/features/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24937073</guid>
            <pubDate>Thu, 29 Oct 2020 22:53:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Sun is more active now than over the last 8000 years (2004)]]>
            </title>
            <description>
<![CDATA[
Score 44 | Comments 18 (<a href="https://news.ycombinator.com/item?id=24937001">thread link</a>) | @firebaze
<br/>
October 29, 2020 | https://www.mpg.de/research/sun-activity-high | <a href="https://web.archive.org/web/*/https://www.mpg.de/research/sun-activity-high">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  
  
  
  <p>An international team of scientists has reconstructed the Sun's activity over the last 11 millennia and forecasts decreased activity within a few decades</p>
  

  

  <p>The activity of the Sun over the last 11,400 years, i.e., back to the end of the last ice age on Earth, has now for the first time been reconstructed quantitatively by an international group of researchers led by Sami K. Solanki from the Max Planck Institute for Solar System Research (Katlenburg-Lindau, Germany). The scientists have analyzed the radioactive isotopes in trees that lived thousands of years ago. As the scientists from Germany, Finland, and Switzerland report in the current issue of the science journal "Nature" from October 28, one needs to go back over 8,000 years in order to find a time when the Sun was, on average, as active as in the last 60 years. Based on a statistical study of earlier periods of increased solar activity, the researchers predict that the current level of high solar activity will probably continue only for a few more decades.</p>
  
  
<figure data-description="A large sunspot observed on the Sun in early September 2004. The field of view encompasses around 45,000 by 30,000 km of the Sun’s surface - the entire earth would fit into the area several times over. Sunspots appear dark because the strong magnetic field in the them suppresses the transport of energy through gas flow. In the central dark area of the sunspot (umbra) the magnetic field is perpendicular to the surface, whereas in the lighter coloured periphery (penumbra) the magnetic field is largely horizontal to the surface. The image was captured by Vasily Zakharov with a one-meter solar telescope on the island of La Palma. The telescope is operated by the Institute for Solar Physics of the Royal Swedish Academy of Sciences." data-picture="base64;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">
      
    

    
    <figcaption>
        <p>
          A large sunspot observed on the Sun in early September 2004. The field of view encompasses around 45,000 by 30,000 km of the Sun’s surface - the entire earth would fit into the area several times over. Sunspots appear dark because the strong magnetic field in the them suppresses the transport of energy through gas flow. In the central dark area of the sunspot (umbra) the magnetic field is perpendicular to the surface, whereas in the lighter coloured periphery (penumbra) the magnetic field is largely horizontal to the surface. The image was captured by Vasily Zakharov with a one-meter solar telescope on the island of La Palma. The telescope is operated by the Institute for Solar Physics of the Royal Swedish Academy of Sciences.
        </p>
        <p>
          © Max Planck Institute for Solar System Research
        </p>
    </figcaption>
</figure>


<p>The research team had already in 2003 found evidence that the Sun is more active now than in the previous 1000 years. A new data set has allowed them to extend the length of the studied period of time to 11,400 years, so that the whole length of time since the last ice age could be covered. This study showed that the current episode of high solar activity since about the year 1940 is unique within the last 8000 years. This means that the Sun has produced more sunspots, but also more flares and eruptions, which eject huge gas clouds into space, than in the past. The origin and energy source of all these phenomena is the Sun's magnetic field.</p>

<p>Since the invention of the telescope in the early 17th century, astronomers have observed sunspots on a regular basis. These are regions on the solar surface where the energy supply from the solar interior is reduced owing to the strong magnetic fields that they harbour. As a consequence, sunspots are cooler by about 1,500 degrees and appear dark in comparison to their non-magnetic surroundings at an average temperature of 5,800 degrees. The number of sunspots visible on the solar surface varies with the 11-year activity cycle of the Sun, which is modulated by long-term variations. For example, there were almost no sunspots seen during the second half of the 17th century.</p>

<p>For many studies concerning the origin of solar activity and its potential effect on long-term variations of Earth's climate, the interval of time since the year 1610, for which systematic records of sunspots exist, is much too short. For earlier times the level of solar activity must be derived from other data. Such information is stored on Earth in the form of "cosmogenic" isotopes. These are radioactive nuclei resulting from collisions of energetic cosmic ray particles with air molecules in the upper atmosphere. One of these isotopes is C-14, radioactive carbon with a half life of 5730 years, which is well known from the C-14 method to determine the age of wooden objects. The amount of C-14 produced depends strongly on the number of cosmic ray particles that reach the atmosphere. This number, in turn, varies with the level of solar activity: during times of high activity, the solar magnetic field provides an effective shield against these energetic particles, while the intensity of the cosmic rays increases when the activity is low. Therefore, higher solar activity leads to a lower production rate of C-14, and vice versa.</p>

<p>By mixing processes in the atmosphere, the C-14 produced by cosmic rays reaches the biosphere and part of it is incorporated in the biomass of trees. Some tree trunks can be recovered from below the ground thousands of years after their death and the content of C-14 stored in their tree rings can be measured. The year in which the C-14 had been incorporated is determined by comparing different trees with overlapping life spans. In this way, one can measure the production rate of C-14 backward in time over 11,400 years, right to the end of the last ice age. The research group have used these data to calculate the variation of the number of sunspots over these 11,400 years. The number of sunspots is a good measure also for the strength of the various other phenomena of solar activity.</p>

<p>The method of reconstructing solar activity in the past, which describes each link in the complex chain connecting the isotope abundances with the sunspot number with consistent quantitative physical models, has been tested and gauged by comparing the historical record of directly measured sunspot numbers with earlier shorter reconstructions on the basis of the cosmogenic isotope Be-10 in the polar ice shields. The models concern the production of the isotopes by cosmic rays, the modulation of the cosmic ray flux by the interplanetary magnetic field (the open solar magnetic flux), as well as the relation between the large-scale solar magnetic field and the sunspot number. In this way, for the first time a quantitatively reliable reconstruction of the sunspot number for the whole time since the end of the last ice age could be obtained.</p>
<figure data-description="Top: Reconstructed sunspot activity (10 year average) for the last 11,400 years based on C-14 data (blue curve) and the directly observed historical sunspot data since 1610 (red curve). The reliable C-14 data ends around the year 1900 so that the sharp increase in sunspot activity in the 20th century does not appear in the graph. The reconstruction shows clearly that a comparable period of high sunspot activity previously existed over 8000 years ago. Below: An enlarged section of the upper graph (hatched area) with several episodes of higher sun activity; comparable to the 20th century." data-picture="base64;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">
      
    

    
    <figcaption>
        <p>
          Top: Reconstructed sunspot activity (10 year average) for the last 11,400 years based on C-14 data (blue curve) and the directly observed historical sunspot data since 1610 (red curve). The reliable C-14 data ends around the year 1900 so that the sharp increase in sunspot activity in the 20th century does not appear in the graph. The reconstruction shows clearly that a comparable period of high sunspot activity previously existed over 8000 years ago. Below: An enlarged section of the upper graph (hatched area) with several episodes of higher sun activity; comparable to the 20th century.
        </p>
        <p>
          © Max Planck Institute for Solar System Research
        </p>
    </figcaption>
</figure>


<p>Because the brightness of the Sun varies slightly with solar activity, the new reconstruction indicates also that the Sun shines somewhat brighter today than in the 8,000 years before. Whether this effect could have provided a significant contribution to the global warming of the Earth during the last century is an open question. The researchers around Sami K. Solanki stress the fact that solar activity has remained on a roughly constant (high) level since about 1980 - apart from the variations due to the 11-year cycle - while the global temperature has experienced a strong further increase during that time. On the other hand, the rather similar trends of solar activity and terrestrial temperature during the last centuries (with the notable exception of the last 20 years) indicates that the relation between the Sun and climate remains a challenge for further research.</p>
  
</div></div>]]>
            </description>
            <link>https://www.mpg.de/research/sun-activity-high</link>
            <guid isPermaLink="false">hacker-news-small-sites-24937001</guid>
            <pubDate>Thu, 29 Oct 2020 22:44:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Don't contribute anything relevant in web forums]]>
            </title>
            <description>
<![CDATA[
Score 172 | Comments 121 (<a href="https://news.ycombinator.com/item?id=24934569">thread link</a>) | @todsacerdoti
<br/>
October 29, 2020 | https://karl-voit.at/2020/10/23/avoid-web-forums/ | <a href="https://web.archive.org/web/*/https://karl-voit.at/2020/10/23/avoid-web-forums/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
<ul>
<li>Updates
<ul>
<li>2020-10-25 Comment by Erik</li>
</ul></li>
</ul>

<p>

If you're, for example, contributing to a <a href="https://en.wikipedia.org/wiki/Reddit">reddit</a> thread about something which is irrelevant or anything with only a short-term relevance, this article does not apply to you right now.

</p>

<p>

However, as soon as you're helping somebody solving an interesting issue, summarize your experiences with something or write anything that might be cool to be around in a couple of years as well, you do provide potential high-value content. My message to all those authors is: <b>don't use web-based forums</b>.

</p>

<p>

TL;DR: all of the content of closed, centralized services will be lost in the long run. Choose the platform you contribute to wisely now instead of learning through more large data loss events later-on.

</p>

<p>

The longer version is worth your time:

</p>

	  <header><h2>What Do I Mean With Web-Based Forums Here?</h2></header>

<p>

In this article, I'm using the term "web-based forums" as an umbrella term for closed, centralized services like <a href="https://en.wikipedia.org/wiki/Reddit">Reddit</a>, <a href="https://en.wikipedia.org/wiki/Hacker_News">Hacker News</a>, <a href="https://en.wikipedia.org/wiki/Slashdot">Slashdot</a>, Facebook, or any other web-based forum where you are able to add comments, articles, and so forth in most cases only after creating an account.

</p>

<p>

Typically, those services don't provide any possibility to extract or synchronize content. They don't offer open APIs that allow users to choose among different and open user interfaces. They are owned and operated by private companies.

</p>

<p>

Please note that when I'm going to mention more or less only reddit as an example in the next sections, this is because reddit is the only web-based forum <a href="https://www.reddit.com/user/publicvoit">I'm familiar with</a> to a certain level. This does not mean that reddit is worse than other closed, centralized web-based forums. Not at all.

</p>

	  <header><h2>So What's the Issue With Web-Based Forums?</h2></header>

<p>

There is not one issue. There are several things where web-based forums don't qualify for being a platform for quality content. Let's take a look at some of them.

</p>

<p>

I'm glad you're still reading this article and I hope you bear with me until the end of it. Most people will realize and learn about having contributed lots and lots of high-value information only when platforms are down for good. And this is what makes me really sad. It is just like you know that one building of the Library of Alexandria is going to burn down in a few years and people still bring many unique copies of high-quality books into its shelves, unaware of destroying knowledge this way.

</p>

	  <header><h3>Issue: No Backup, No Distribution</h3></header>

<p>

For reasons and examples stated <a href="https://karl-voit.at/cloud">in this article</a>, any centralized web-based service will go offline some day. Some sooner, some later. Popularity is not even a guarantee that a service gets continued, as you can see with <a href="https://killedbygoogle.com/">hundreds of (partly) very well known and used Google services that were shut down</a>. Nothing will be on the web forever. Most people are not aware of this fact. The books set on this machine are more likely to survive history than all of your reddit/Facebook/... contributions:

</p>

<figure>
<img src="https://karl-voit.at/2020/10/23/avoid-web-forums/2019-10-05T22.56.47%20Buchdruckmuseum%20-%20Linotype%20-%20Tastatur%20--%20cliparts%20typography%20history%20publicvoit%20-%20scaled%20width%20630.jpg" alt="" width="630">
<figcaption>A Linotype machine.</figcaption>
</figure>

<p>

So when you begin to be aware of this fact, you might want to think of things you can do to mitigate data loss when services are discontinued or "sunrized" as some marketing experts say.

</p>

<p>

You could, for example, back-up the data of this service. By providing the information on multiple servers, chances are high that not all of them are lost at the same time.

</p>

<p>

This requires certain properties. For example, you need to be able to duplicate the service on multiple servers. To be able to do so, you'll need not only the data but also the software that is providing access to the service. When different organization are running mirrored servers, it is required to openly share the data and software. This can be ensured by using Open Source software or at least open APIs and a business model that does not rely on keeping data and technical things a secret.

</p>

<p>

All major commercial services such as reddit, Facebook and so forth keep everything a secret that is not ultimately necessary to use their services. Their software is a secret, they don't offer open APIs or only very crippled ones, you don't have the possibility to get to the raw data. So no luck there. You do have <a href="https://en.wikipedia.org/wiki/Vendor_lock-in">a lock-in situation</a>.

</p>

<figure>
<img src="https://karl-voit.at/2020/10/23/avoid-web-forums/2020-10-23%20Oatmeal_-_reaching_people_on_the_internet%20--%20publicvoit%20-%20scaled%20width%20630.png" alt="" width="630">
<figcaption>https://theoatmeal.com/comics/reaching_people</figcaption>
</figure>

<p>

Even with personal blogs, "fragile" as they are, you are able to use the <a href="https://archive.org/web/">Wayback Machine of the Internet Archive</a> to back up your blog. For example, every page on my blog contains a link to its archive in the page footer. This ensures that you can not only browse the latest version of all of my blog articles in case of a server breakdown. This also enables you to browse all previous version, probably changed over time. Go ahead, try a few "Archive" links of my articles. If any of my articles start with an "Updates:" section, you know for sure that there are older versions accessible via the Internet Archive.

</p>

<p>

The Wayback Machine does not archive reddit threads. It can not properly back up Facebook pages. <a href="https://help.archive.org/hc/en-us/articles/360004651732-Using-The-Wayback-Machine">It's blinded by corporate secrecy</a> when it comes to archive content for the upcoming generations:

</p>

<blockquote>Why isn't the site I'm looking for in the archive?<br>
Some sites may not be included because the automated crawlers were
unaware of their existence at the time of the crawl. It's also
possible that some sites were not archived because they were password
protected, blocked by robots.txt, or otherwise inaccessible to our
automated systems. Site owners might have also requested that their
sites be excluded from the Wayback Machine.</blockquote>

<p>

Summarizing the things mentioned above: without very good support for data export, service duplication, open standards, any content you provide in closed web-based services will be lost just as <a href="https://www.nytimes.com/2019/03/19/business/myspace-user-data.html">MySpace already lost twelve years of content just so</a>, just to mention one big example.

</p>

	  <header><h3>Issue: User Interface Dictatorship</h3></header>

<p>

When you grew up only knowing centralized web-based forums, you can not imagine the many advantages of having the freedom to choose your preferred user interface. While some people might think this is a minor issue, let me explain a few examples where this makes a huge difference.

</p>

<p>

The first example starts with something that might only annoy people. With comments like on <a href="https://www.reddit.com/r/emacs/comments/hfamm7/those_who_have_tried_out_multiple_zettelkasten/fvx9vu5/">this thread</a>, you clutter up other people's interface for personal gain. It's selfish and distracts from the information consumption.

</p>

<p>

The reason why people are using such reminder bots is multi-fold. First, they don't use a proper todo management system that would be able to remind them to read a certain article in a few days. They externalize this inability to the web-based forum and all of its other users. <a href="https://karl-voit.at/tags/pim">I'm working on fixing these educational issues</a>. Secondly, there is no way to have features that you can use that do not affect other people's interface.

</p>

<p>

Consider people with visual impairment do have special needs. <a href="https://tinyurl.com/y6ncgvjt">The WHO reports</a> an estimate of 285 million people that do are visually impaired, ninety percent of them living in developing countries. Those are not numbers you can simply ignore. It is obvious that they do need different kind of interfaces. Either they have to use a high-contrast interface, highly unusual interface scaling factors, an interface that avoids certain color combinations, text-to-speech systems or <a href="https://en.wikipedia.org/wiki/Braille#Braille_reading">Braille readers</a> that are able to extract the content properly.

</p>

<p>

If a web-based services that - remember from before - does not offer proper open APIs and which does not implement said features, all those people simply can not participate and you can not profit from their knowledge and experience.

</p>

<p>

And even when you think that this is just a minority I can provide examples where everybody profits from choosing his or her own interface.

</p>

<p>

Some services are providing interfaces that aren't working properly on small displays or mobile devices in general. In these cases, without any ability to switch to an alternative app or web-page, you are locked out even with perfect eyesight.

</p>

<p>

When you're using an web-based forum that does not provide the feature that already read articles are marked or collapsed, you need to skim though a thread completely and re-read content to find out new postings when re-visiting the thread after a while. Our time should not spent on senseless tasks like this.

</p>

<p>

Alternative interfaces might provide advanced rating features based on your personal taste and choice so that you are able to filter out the most relevant articles easily and do not clutter your view with irrelevant articles at all. This is also called "scoring". It can be based on keywords, the amount of personal contributions to a longer thread, friendship relationships from your contact management, and so forth.

</p>

<p>

Some people prefer navigating using the keyboard. Either by personal taste or by physical restrictions. If the web-based centralized service only supports mouse-based navigation, you can not use this service.

</p>

<p>

I could continue with examples like that. The common theme is: when one particular centralized web-based forum is not implementing all of those nice features you need or like, you can not use them properly.

</p>

	  <header><h3>Issue: Rule Monopoly and Subjective Censorship</h3></header>

<p>

When you do live in a society with certain set of (legal) rules, providers of relevant web-based forums have to follow and enforce some of them. However, the issue is that this kind of censorship is and will always be related to a particular culture and society at a specific time.

</p>

<p>

For example, in Germany and Austria, being a <a href="https://en.wikipedia.org/wiki/Nazism">Nazi</a> is punishable by law. In the USA, freedom-loving people think fans of the human monsters that tortured and murdered millions of Jews in the Second World War need the possibility to express their personal "opinion". As you can see, there is a different point of view in-between the lines when I write about Nazis compared to an author from the USA who values "freedom of speech" higher than "being a die-hard fan of mass murders". It's a very difficult topic you can not enforce with a world-wide service.

</p>

<p>
</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://karl-voit.at/2020/10/23/avoid-web-forums/">https://karl-voit.at/2020/10/23/avoid-web-forums/</a></em></p>]]>
            </description>
            <link>https://karl-voit.at/2020/10/23/avoid-web-forums/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24934569</guid>
            <pubDate>Thu, 29 Oct 2020 19:27:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A12 – Advancing Network Transparency on the Desktop]]>
            </title>
            <description>
<![CDATA[
Score 30 | Comments 5 (<a href="https://news.ycombinator.com/item?id=24934296">thread link</a>) | @todsacerdoti
<br/>
October 29, 2020 | https://arcan-fe.com/2020/10/28/a12-advancing-network-transparency-on-the-desktop/ | <a href="https://web.archive.org/web/*/https://arcan-fe.com/2020/10/28/a12-advancing-network-transparency-on-the-desktop/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
						
<p>This article is is the main course to the appetiser that was <a href="https://arcan-fe.com/2018/11/16/the-x-network-transparency-myth/">The X Network Transparency Myth</a> (2018). In it, we will go through how the pieces in the Arcan ecosystem tie together to advance the idea of network transparency for the desktop and how it sets the stage for a fully networked desktop.</p>



<p>Some of the points worth recalling from the X article are:</p>



<ol><li>‘transparency’ is evaluated from the perspective of the user; it is not even desirable for the underlying layers to be written so that they operate the same for local rendering as they would across a network. The local-optimal case is necessarily different from the remote one, the mechanisms are not the same and the differences will keep on growing organically with the advancement of hardware and display/rendering techniques.</li><li>side-band protocols splitting up the desktop into multiple IPC systems for audio, meta, fonts, … increases the difficulty to succeed with anything close to a transparent experience, as the network layer needs to take all of these into consideration as well as trying to synchronise them.</li></ol>



<p>To add a little to the first argument: it should also not be transparent to the window manager as some actions have drastically different impact on the user interface side to security and expectations. For example, Clipboard/DND locally is not (supposed to be) a complicated thing. When applied across a network, however, such things can degrade the experience for anything else. Other examples is that you want to block some sensitive inputs from being accidentally forwarded to a networked window and so on, it has happened in the past that the wrong sudo password has, indeed, been sent to the wrong ssh session.</p>



<p>This target has been worked on for a long time, as suggested by this part from the <a href="https://www.youtube.com/watch?v=3O40cPUqLb">old demo</a> from 2012/2013. Already back then the drag/slice to compose-transform-and-share case exposed out of compositor sharing and streaming; something that only now is appearing elsewhere in a comparably limited form.</p>



<p>We are on the third or fourth re-implementation of the idea, and the first one that is considered having a good enough of a design to commit to using and building upon. There are many fascinating nuances to this problem that only appear when you ‘try to go to 11’.</p>



<p>As per usual, parts of this post will be quite verbose and technical. Here are some shortcuts to jump around so that you don’t lose interest from details that seem irrelevant to you.</p>



<ul><li><a href="#primitives">Basic primitives: Arcan-net, A12 and SHMIF</a></li><li><a href="#usecases">Example Usecases</a></li><li><a href="#protocol">Protocol State and Development</a></li><li><a href="#explained">Demo Explained</a></li></ul>



<h2 id="demo">Demos</h2>



<p>Starting with some short clips of the development progress – and then work through the tools and design needed to make this happen. It might be short, but there is a whole world of nuance and detail to it.</p>



<p>(~early 2019) – forced compression, OSX viewer, (bad) audio:</p>



<figure></figure>



<p>Composited Xarcan (desktop to pinephone), compression based on window type:</p>



<figure><p><span><iframe width="640" height="360" src="https://www.youtube.com/embed/CIWZdEkgPfM?version=3&amp;rel=1&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;fs=1&amp;hl=en&amp;autohide=2&amp;wmode=transparent" allowfullscreen="true" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span>
</p></figure>



<p>Here is a native arcan client with crypto, local GPU “hot-unplug” to software rendering handover and compression negotiation (h264):</p>



<figure><p><span><iframe width="640" height="360" src="https://www.youtube.com/embed/_RSvk7mmiSE?version=3&amp;rel=1&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;fs=1&amp;hl=en&amp;autohide=2&amp;wmode=transparent" allowfullscreen="true" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span>
</p></figure>



<p>Here is ‘server-side’ text rendering of text-only windows, font, style and size controlled by presenting device — client migrates back when window is closed:</p>



<figure></figure>



<p><span>In the videos, you can see (if you squint) instances of </span><em>live migration</em><span> between display servers over a network, with a few twists. For example, the decorations, input mapping, font preferences and other visuals change to match the machine that the client is currently </span><em>presenting</em><span> on and that audio also comes along, because </span><a href="https://arcan-fe.com/2017/10/05/awk-for-realtime-multimedia/">Arcan does multimedia</a><span>, not only video. </span></p>



<p><span>What is less visible is that the change in border colour, a security feature in </span><a href="http://durden.arcan-fe.com/">Durden</a><span>, is used to signify that the window comes from a networked source, a property that can also be used to filter sensitive actions. The neo-vim window in the video even goes so far as to have its text surfaces rendered server side, as its </span><a href="https://github.com/letoram/nvim-arcan">UI driver</a><span> is written using our terminal-protocol liberated </span><a href="https://github.com/letoram/arcan/wiki/TUI">TUI API.</a> This is also why the font changes; it is the device you&nbsp;<em>present</em> on that defines visuals and input response, not the device you run the program on.</p>



<p>Also note how the clients “jumps” back when the window is closed on the remote side; this is one of the many payoffs from having a systemic mindset when it comes to&nbsp; ‘<a href="https://arcan-fe.com/2017/12/24/crash-resilient-wayland-compositing/">crash resilience</a>‘ – the IPC system itself is designed in such a way that <em>necessary</em> state can b<span>e reconstructed and&nbsp;</span><em>dynamic</em><span>&nbsp;state is tracked and renegotiated when needed. The effect is that a client is forcefully detached from the current display server with the instruction of switching to another.</span> The keystore (while a work in progress) allows you to define the conditions for when and how it jumps to which machines and picks keys accordingly.</p>



<p>That dynamic state is tracked and can be renegotiated as a ‘reset’ matters on the client level as well, the basic set of guaranteed features when a client opens a local connection roughly generalises between all imaginable window management styles. Those that are dynamically (re-) negotiated cannot be relied upon. So when a client is migrated to a user that has say, accessibility needs, or is in a <a href="https://arcan-fe.com/2018/03/29/safespaces-an-open-source-vr-desktop/">VR environment</a>, the appropriate extras gets added when the client connects there, and then removed when it moves somewhere else. This is an essential primitive for network transparency as a collaboration feature.</p>



<h2 id="primitives">Basic Primitives: Arcan-net, SHMIF and A12</h2>



<p>There are three building blocks in play here, a tool called <em>arcan-net&nbsp;</em>which combines the two others:&nbsp;<em>A12</em> and <a href="https://github.com/letoram/arcan/wiki/SHMIF">SHMIF</a>.</p>



<p>A12 is a <span>‘work in progress’ protocol – it’s not </span><em>the</em><span>&nbsp;</span><a href="https://www.x.org/wiki/Development/X12/">X12</a><span> that some people called for, but it’s “</span><em>a”</em><span> twelve. It strives to be remote optimal – compression tactics based on connectivity, content type and context of use, deferred (presentation side) rendering with data-native representation when possible (pixel buffers as a last resort, not the default); support caching of common states such as fonts; handle cancellation of normally ‘atomic’ operations such as clipboard cut and paste and so on.</span></p>



<p>SHMIF is the IPC system and API used to work with most other parts of Arcan. It is designed to be locally optimal: shared memory and system ABI in lock free ring-buffers preferred over socket/pipe pack/unpack transfers; minimal sustained set of system calls needed (for least-privilege sandboxing); resource allocations on a strict regimen (DoS prevention and exploit mitigation); fixed based set of necessary capabilities and user-controlled opt-in for higher level ones.</p>



<p><span>SHMIF has a large number of features that were specifically picked for correcting the wrongs done to X- like network transparency by the gradual introduction of side-bands and good old fashioned negligence. Part of this is that <em>all necessary and sufficient data exchange</em> used to compose a desktop goes over <em>the same</em> IPC system — one that is free of unnecessary Linuxisms to boot. While it would hurt a bit and take some effort, there are few stops for packing our bags and going someplace else, heck it used to run on Windows and still works on OSX. Rumour has it there are iOS and Android versions hidden away somewhere.</span></p>



<p><span>Contrast this with other setups where you need a large weave of IPC systems to get the same job done; Wayland for video and some input and some metadata; PulseAudio for audio; PipeWire for some video and some audio; D-Bus for some metadata and controls; D-Conf for some other metadata; Spice/RFB(VNC)/RDP for composited desktop sharing; Waypipe for partial Wayland sharing, X11 for partial X / XWayland sharing: SSH+VT***+Terminal emulator for CLI/TUI and less unsafe Waypipe / X11 transport; Synergy for mouse and keyboard and clipboard and so on. Each of these with their own take (or lack thereof) on authentication and synchronization, implementing many of the most difficult tasks again and again in incompatible ways yet still end up with features missing and exponentially more lines of code when compared to the solution here.</span></p>



<p>Back to Arcan-net. It exposes an a12 server and an a12 client, as well as acting as a shmif server, a shmif client and taking care of managing authentication keys. In that sense it behaves like any old network proxy. While not going too far into the practical details, showing off some of the setup might help.</p>



<p>On the active display server side:</p>



<pre>void@123.213.132.1# arcan-net -l 31337</pre>



<p>This will listen for incoming connections on the marked port, and map them to the currently active local connection point. <span>To dive further into the connection point concept, either read the comparison between </span><a href="https://arcan-fe.com/2018/10/17/arcan-versus-xorg-approaching-feature-parity/">Arcan vs Xorg</a><span> or simply think ‘Desktop UI address’; The WM exports named connection points and assigns different policies based on that.</span></p>



<p><span>On the client side we can have the complex-persistent option that forwards new clients as they come:</span></p>



<pre><em>arcan-net</em> -s <em>netdemo</em> <em>123.213.132.1 31337</em><br>ARCAN_CONNPATH=netdemo one_arcan_client &amp;<br>ARCAN_CONNPATH=netdemo another_arcan_client &amp;</pre>



<p>Or the one-time simpler version which forks/exec arcan-net and inherits the connection primitive needed to setup a SHMIF connection:</p>



<pre>ARCAN_CONNPATH=a12://keyid@host:port one_arcan_client</pre>



<p>Or, and this is important for understanding the demo, an api function through the WM:</p>



<pre>target_devicehint(client_vid,"a12://keyid@", true)</pre>



<p>This triggers the SHMIF implementation tied to the window of a client to disconnect from the current display server connection, connect to a remote one through arcan-net, then tell the application part of the client to rebuild essential state as the previous connection has, in a sense, ‘crashed’. The same mechanism is then used to define a fallback (‘should the connection be lost, go here instead’). This is the <em>self-healing</em> aspect of proper <em>resilience</em>.</p>



<p>There are WM APIs for all the possible network sharing scenarios so it can be handled as user interfaces without any command line work.</p>



<p>I mentioned ‘authentication’ before, where is that happening? So this is another part of the …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://arcan-fe.com/2020/10/28/a12-advancing-network-transparency-on-the-desktop/">https://arcan-fe.com/2020/10/28/a12-advancing-network-transparency-on-the-desktop/</a></em></p>]]>
            </description>
            <link>https://arcan-fe.com/2020/10/28/a12-advancing-network-transparency-on-the-desktop/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24934296</guid>
            <pubDate>Thu, 29 Oct 2020 19:07:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Mall real estate company collected 5M images of shoppers]]>
            </title>
            <description>
<![CDATA[
Score 225 | Comments 148 (<a href="https://news.ycombinator.com/item?id=24933583">thread link</a>) | @voisin
<br/>
October 29, 2020 | https://www.cbc.ca/news/politics/cadillac-fairview-5-million-images-1.5781735?cmp=rss | <a href="https://web.archive.org/web/*/https://www.cbc.ca/news/politics/cadillac-fairview-5-million-images-1.5781735?cmp=rss">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>The real estate company behind some of Canada's most popular shopping centres embedded cameras inside its digital information kiosks at 12 shopping malls in major Canadian cities&nbsp;to collect millions of images — and used facial recognition technology without customers' knowledge or consent —&nbsp;according to a new investigation by the federal, Alberta and B.C. privacy commissioners.</p><div><figure><div><p><img loading="lazy" alt="" srcset="" sizes="" src="https://i.cbc.ca/1.5499879.1584406507!/fileImage/httpImage/image.JPG_gen/derivatives/16x9_780/covid-19-pandemic-stores-closed.JPG"></p></div><figcaption>Cadillac Fairview, the real estate company behind some of Canada's most popular shopping centres, embedded cameras inside its digital information kiosks at 12 shopping malls across Canada, according to a new investigation.<!-- --> <!-- -->(Evan Mitsui/CBC)</figcaption></figure><p><span><p>The real estate company behind some of Canada's most popular shopping centres embedded cameras inside its digital information kiosks at 12 shopping malls in major Canadian cities&nbsp;to collect millions of images — and used facial recognition technology without customers' knowledge or consent —&nbsp;according to a new investigation by the federal, Alberta and B.C. privacy commissioners.</p>  <p>"Shoppers had no reason to expect their image was being collected by an inconspicuous camera, or that it would be used, with facial recognition technology, for analysis," said federal Privacy Commissioner Daniel Therrien&nbsp;in a statement.</p>  <p>"The lack of meaningful consent was particularly concerning given the sensitivity of biometric data, which is a unique and permanent characteristic of our body and a key to our identity."</p>  <p>According to the report, the technology&nbsp;Cadillac Fairview used&nbsp;— known as "anonymous video analytics" or AVA— took temporary digital images of the faces of individuals within the field of view of the camera in the directory.</p>  <p><strong><em>WATCH: Shoppers' privacy violated at major Canadian malls: Privacy commissioners:</em></strong></p>  <p><span><span><div><div role="button" tabindex="0" title="Shoppers’ privacy violated at major Canadian malls: Privacy commissioners"><div><p><img src="https://thumbnails.cbc.ca/maven_legacy/thumbnails/949/527/mall-privacy-daigle-291020.jpg" alt=""></p></div></div></div><span>Cadillac Fairview, the real estate company behind some of Canada’s biggest malls, violated the privacy of shoppers by collecting five million images without consent from cameras inside digital information kiosks, an investigation by federal, British Columbia and Alberta privacy commissioners found.<!-- --> <!-- -->2:01</span></span></span></p>  <p>It then used facial recognition software to convert those images into biometric numerical representations of&nbsp;individual faces, about five million images&nbsp;in total.</p>  <p>That sensitive personal information could be used to identify individuals based on their unique facial features, said&nbsp;the commissioners.</p>    <p>The report said the company also kept about 16 hours of video recordings, including some audio, which it had captured during a testing phase at two malls.</p>  <p>Cadillac Fairview said it&nbsp;used AVA technology&nbsp;to assess foot traffic and track shoppers' ages and genders&nbsp;— but not to identify individuals.&nbsp;</p>  <p>The company also argued shoppers were made aware of the activity through decals it placed on shopping mall entry doors that warned cameras were being used for "safety and security" and included the web address for Cadillac Fairview's&nbsp;privacy policy.</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.4762407.1533648115!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/chinook-centre-directory.jpg 300w,https://i.cbc.ca/1.4762407.1533648115!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/chinook-centre-directory.jpg 460w,https://i.cbc.ca/1.4762407.1533648115!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/chinook-centre-directory.jpg 620w,https://i.cbc.ca/1.4762407.1533648115!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/chinook-centre-directory.jpg 780w,https://i.cbc.ca/1.4762407.1533648115!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/chinook-centre-directory.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.4762407.1533648115!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/chinook-centre-directory.jpg"></p></div><figcaption>This directory in Chinook Centre mall in south Calgary uses facial recognition technology.<!-- --> <!-- -->(Sarah Rieger/CBC)</figcaption></figure></span></p>  <p>But the commissioners said that&nbsp;wasn't good enough and did not meet the standard for meaningful consent.&nbsp;</p>  <p>"An individual would not, while using a mall directory, reasonably expect their image to be captured and used to create a biometric representation of their face, which is sensitive personal information, or for that biometric information to be used to guess their approximate age and gender," they wrote.</p>  <p>The privacy watchdogs also took issue with the way the&nbsp;five&nbsp;million images were stored.</p>  <p>Cadillac Fairview&nbsp;said the&nbsp;images taken by camera were briefly analyzed then deleted&nbsp;—&nbsp;but investigators found that the sensitive biometric information generated from the images was being stored in a centralized database by&nbsp;a third-party company,</p>  <p>"Our investigation revealed that&nbsp;[Cadillac Fairview Corporation Limited's]&nbsp;AVA&nbsp;service provider had collected and stored approximately five million numerical representations of faces on&nbsp;CFCL's behalf, on a decommissioned server, for no apparent purpose and with no justification," notes the investigation.</p>  <p>"Cadillac Fairview stated that it was unaware that the database of biometric information existed, which compounded the risk of potential use by unauthorized parties or, in the case of a data breach, by malicious actors."</p>  <h2>Company&nbsp;says technology couldn't identify people</h2>  <p>The company said the technology was used&nbsp;to detect the presence of a human face and&nbsp;assign it&nbsp;"within milliseconds"&nbsp;to an approximate age and gender category and maintains it&nbsp;did not store any images during the pilot program and was not capable of recognizing anyone.&nbsp;</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.5782324.1604000638!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/eaton-centre-decal.jpg 300w,https://i.cbc.ca/1.5782324.1604000638!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/eaton-centre-decal.jpg 460w,https://i.cbc.ca/1.5782324.1604000638!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/eaton-centre-decal.jpg 620w,https://i.cbc.ca/1.5782324.1604000638!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/eaton-centre-decal.jpg 780w,https://i.cbc.ca/1.5782324.1604000638!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/eaton-centre-decal.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5782324.1604000638!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/eaton-centre-decal.jpg"></p></div><figcaption>The decal found on the entrance doors of the CF Toronto Eaton Centre<!-- --> <!-- -->(Office of the Privacy Commissioner report)</figcaption></figure></span></p>  <p>"The five million representations referenced in the [Office of the Privacy Commissioner]&nbsp;report are not faces. These are sequences of numbers the software uses to anonymously categorize the age range and gender of shoppers in the camera's view," Cadillac Fairview spokesperson Jess Savage&nbsp;said in a statement to CBC News.</p>  <p>"The&nbsp;OPC report concludes there is no evidence that CF was using any technology for the purpose of identifying individuals."</p>  <p>CF&nbsp;suspended its&nbsp;use of cameras&nbsp;back in 2018&nbsp;when provincial and federal privacy commissioners launched their probe&nbsp;<a href="https://www.cbc.ca/news/canada/calgary/calgary-malls-1.4760964">following a CBC investigation</a>.</p>  <p>In a statement to CBC News on Thursday, the company said it has deleted the data.</p>  <p>"We subsequently deactivated directory cameras and the numerical representations and associated data have since been deleted," said&nbsp;Savage.</p>  <p>"We take the concerns of our visitors seriously and wanted to ensure they were acknowledged and addressed."</p>  <p>However, the three commissioners said they have concerns about the company's plans going forward.</p>    <p>"The commissioners remain concerned that Cadillac Fairview refused their request that it commit to ensuring express, meaningful consent is obtained from shoppers should it choose to redeploy the technology in the future," said&nbsp;the commissioners'&nbsp;statement.</p>  <h2>No fines under Canadian law</h2>  <p>Savage said Cadillac Fairview&nbsp;accepted and implemented all the recommendations&nbsp;"with the exception of those that speculate about hypothetical future uses of similar technology."</p>  <p>The investigation found the technology was used&nbsp;in five provinces&nbsp;at the following malls:</p>  <ul>   <li>CF Market Mall (Calgary)</li>   <li>CF Chinook Centre (Calgary)</li>   <li>CF Richmond Centre (Richmond, B.C.)</li>   <li>CF Pacific Centre (Vancouver)</li>   <li>CF Polo Park (Winnipeg)</li>   <li>CF Toronto Eaton Centre (Toronto)</li>   <li>CF Sherway Gardens (Toronto)</li>   <li>CF Fairview Mall (Toronto)</li>   <li>CF Lime Ridge (Hamilton, Ont.)</li>   <li>CF Markville Mall (Markham, Ont.)</li>   <li>CF Galeries d'Anjou&nbsp;(Montreal)</li>   <li>CF Carrefour Laval (Laval, Que.)</li>  </ul>  <p>Ann Cavoukian,&nbsp;executive director at the Global Privacy and Security by Design Centre,&nbsp;said a case like this would lead to millions of dollars in fines if it had happened&nbsp;in the United States.</p>  <p>"The commissioners are doing the best they can with the limited resources they have," she said.</p>  <p>"What we have to insist upon is that private&nbsp;sector entities like Cadillac Fairview step up and protect their customers' privacy. Otherwise, why are the customers going to continue shopping there?"</p>  <p>B.C. Information and Privacy Commissioner&nbsp;Michael McEvoy&nbsp;said&nbsp;the fact he and his counterparts can't issue a fine in a&nbsp;case like this should make the case for stronger powers at both the federal and provincial levels.</p>  <p>"Fines in a case like this would have been a consideration. It is an incredible shortcoming of Canadian law," he said.</p>  <p>"We as privacy regulators don't have any authority to levy fines on companies that violate peoples'&nbsp;personal information and that should really change."</p></span></p></div></div>]]>
            </description>
            <link>https://www.cbc.ca/news/politics/cadillac-fairview-5-million-images-1.5781735?cmp=rss</link>
            <guid isPermaLink="false">hacker-news-small-sites-24933583</guid>
            <pubDate>Thu, 29 Oct 2020 18:22:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[On Fexprs and Defmacro]]>
            </title>
            <description>
<![CDATA[
Score 37 | Comments 8 (<a href="https://news.ycombinator.com/item?id=24932701">thread link</a>) | @sea6ear
<br/>
October 29, 2020 | https://www.brinckerhoff.org/scraps/joe-marshall-on-FEXPRS-and-DEFMACRO.txt | <a href="https://web.archive.org/web/*/https://www.brinckerhoff.org/scraps/joe-marshall-on-FEXPRS-and-DEFMACRO.txt">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.brinckerhoff.org/scraps/joe-marshall-on-FEXPRS-and-DEFMACRO.txt</link>
            <guid isPermaLink="false">hacker-news-small-sites-24932701</guid>
            <pubDate>Thu, 29 Oct 2020 17:09:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Open-source, fully customizable voice and chat widgets for the web]]>
            </title>
            <description>
<![CDATA[
Score 68 | Comments 15 (<a href="https://news.ycombinator.com/item?id=24932588">thread link</a>) | @JanKoenig
<br/>
October 29, 2020 | https://www.jovo.tech/news/2020-10-29-jovo-for-web-v3-2 | <a href="https://web.archive.org/web/*/https://www.jovo.tech/news/2020-10-29-jovo-for-web-v3-2">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
<p><a href="#" data-featherlight="/img/news/2020-10-29-jovo-for-web-v3-2/jovo-for-web.jpg"><img src="https://www.jovo.tech/img/news/2020-10-29-jovo-for-web-v3-2/jovo-for-web.jpg" alt="Jovo for Web Open Source Voice and Chat" title="Introducing Jovo for Web: Customizable Voice and Chat for the Browser"></a></p><p>With the release <code>v3.2</code> of the <a href="https://github.com/jovotech/jovo-framework">Jovo Framework</a>, we're excited to present a completely revamped web integration.</p><p><em>Jovo for Web</em> allows you to build fully customizable voice and chat apps that work in the browser. And it even comes with 4 open source templates (gifs below!) that help you get started.</p><ul>
<li><a href="#jovo-for-web-features">Jovo for Web Features</a></li>
<li><a href="#select-from-4-starter-templates">Select from 4 Starter Templates</a>
<ul>
<li><a href="#standalone-voice-experience">Standalone Voice Experience</a></li>
<li><a href="#voice-overlay">Voice Overlay</a></li>
<li><a href="#chat-widget">Chat Widget</a></li>
<li><a href="#embedded-chat">Embedded Chat</a></li>
</ul></li>
<li><a href="#more-new-features">More New Features</a></li>
<li><a href="#how-to-update">How to Update</a>
<ul>
<li><a href="#breaking-changes">Breaking Changes</a></li>
</ul></li>
<li><a href="#a-big-thank-you">A Big Thank You</a></li>
</ul><p><em>Like what we're doing? <a href="https://opencollective.com/jovo-framework">Support us on Open Collective!</a></em> </p><h2 id="jovo-for-web-features"><a href="#jovo-for-web-features">Jovo for Web Features</a></h2><p>Let's build voice and chat apps for the browser!</p><p>In our <a href="https://www.context-first.com/introducing-jovo-v3-the-voice-layer/">v3 announcement</a>, we already mentioned that Jovo works with web apps and websites thanks to the <a href="https://www.context-first.com/introduction-voice-multimodal-interactions/">RIDR Lifecycle</a> and <a href="https://www.jovo.tech/news/www.jovo.tech/marketplace">Jovo Marketplace</a>.</p><p><a href="#" data-featherlight="/img/news/2020-10-29-jovo-for-web-v3-2/jovo-web-ridr-lifecycle.jpg"><img src="https://www.jovo.tech/img/news/2020-10-29-jovo-for-web-v3-2/jovo-web-ridr-lifecycle.jpg" alt="Jovo for Web RIDR Lifecycle" title="Integrate ASR and NLU into Web Apps with Jovo and RIDR"></a></p><p>Today, we're thrilled to announce a completely improved verson of our <strong>Jovo for Web</strong> platform.</p><p>Features include:</p><ul>
<li>Support for speech, text, and touch input</li>
<li>Multimodal: Complex visual and audio output possible</li>
<li>Open source and fully customizable</li>
<li><a href="#select-from-4-starter-templates">4 starter templates</a> built with modern technologies like Vue.js and Tailwind CSS</li>
</ul><p>We can't wait to see and hear what you build with this!</p><h2 id="select-from-4-starter-templates"><a href="#select-from-4-starter-templates">Select from 4 Starter Templates</a></h2><p>To help you get started quickly, we built 4 templates with Vue.js and Tailwind CSS that implement use cases for both voice and chat.</p><h3 id="standalone-voice-experience"><a href="#standalone-voice-experience">Standalone Voice Experience</a></h3><blockquote>
<p>Find this starter on GitHub: <a href="https://github.com/jovotech/jovo-starter-web-standalone">github.com/jovotech/jovo-starter-web-standalone</a></p>
</blockquote><p><a href="#" data-featherlight="/img/news/2020-10-29-jovo-for-web-v3-2/starter-web-standalone.gif"><img src="https://www.jovo.tech/img/news/2020-10-29-jovo-for-web-v3-2/starter-web-standalone.gif" alt="Jovo Starter: Standalone Voice Experience" title="Introducing Jovo for Web: Customizable Voice and Chat in the Browser"></a></p><p>This starter brings your voice experiences into the browser as a standalone web app. This can be seen as an experience equivalent to a smart display. Many Alexa Skills and Google Actions like voice games can be brought to the web using this template.</p><p>The starter includes:</p><ul>
<li>a push-to-talk button</li>
<li>a display of the transcribed speech above the button</li>
<li>app output at the top of the screen</li>
<li>conversational logic that switches to dark/light mode using custom web actons</li>
</ul><p><a href="https://www.jovo.tech/demos/starter-web-standalone">Check out the demo here!</a> Hold the button and say "<em>switch to dark mode.</em>"</p><h3 id="voice-overlay"><a href="#voice-overlay">Voice Overlay</a></h3><blockquote>
<p>Find this starter on GitHub: <a href="https://github.com/jovotech/jovo-starter-web-overlay">github.com/jovotech/jovo-starter-web-overlay</a></p>
</blockquote><p><a href="#" data-featherlight="/img/news/2020-10-29-jovo-for-web-v3-2/starter-web-overlay.gif"><img src="https://www.jovo.tech/img/news/2020-10-29-jovo-for-web-v3-2/starter-web-overlay.gif" alt="Jovo Starter: Voice Overlay" title="Introducing Jovo for Web: Customizable Voice and Chat in the Browser"></a></p><p>This starter adds a speech input button as an overlay to an existing website or web app. Voice interactions like search, customizations, and deep access of features could be added using the overlay.</p><p>The starter includes:</p><ul>
<li>a push-to-talk button</li>
<li>a display of the transcribed speech left to the button</li>
<li>conversational logic that switches to dark/light mode using custom web actons</li>
</ul><p><a href="https://www.jovo.tech/demos/starter-web-overlay">Check out the demo here!</a> Hold the button and say "<em>switch to dark mode.</em>"</p><h3 id="chat-widget"><a href="#chat-widget">Chat Widget</a></h3><blockquote>
<p>Find this starter on GitHub: <a href="https://github.com/jovotech/jovo-starter-web-chatwidget">github.com/jovotech/jovo-starter-web-chatwidget</a></p>
</blockquote><p><a href="#" data-featherlight="/img/news/2020-10-29-jovo-for-web-v3-2/starter-web-chatwidget.gif"><img src="https://www.jovo.tech/img/news/2020-10-29-jovo-for-web-v3-2/starter-web-chatwidget.gif" alt="Jovo Starter: Open Source Chat Widget" title="Introducing Jovo for Web: Customizable Voice and Chat in the Browser"></a></p><p>This starter adds a classic chat widget to your website. Think chatbots and conversational experiences for customer support and more.</p><p>The starter includes:</p><ul>
<li>a bottom-right toggle button</li>
<li>text input and quick replies</li>
<li>conversational logic that asks the user to open the Jovo Docs (redirect not working on iOS due to platform limitations)</li>
</ul><p><a href="https://www.jovo.tech/demos/starter-web-chatwidget">Check out the demo here!</a></p><h3 id="embedded-chat"><a href="#embedded-chat">Embedded Chat</a></h3><blockquote>
<p>Find this starter on GitHub: <a href="https://github.com/jovotech/jovo-starter-web-embeddedchat">github.com/jovotech/jovo-starter-web-embbeddedchat</a></p>
</blockquote><p><a href="#" data-featherlight="/img/news/2020-10-29-jovo-for-web-v3-2/starter-web-embeddedchat.gif"><img src="https://www.jovo.tech/img/news/2020-10-29-jovo-for-web-v3-2/starter-web-embeddedchat.gif" alt="Jovo Starter: Open Source Embedded Chat" title="Introducing Jovo for Web: Customizable Voice and Chat in the Browser"></a></p><p>This starter adds a customizable chat interface to your website that can be used for things like conversational landing pages, FAQs, mobile chat support, and much more.</p><p>The starter includes:</p><ul>
<li>fullsize chat component that can be embedded into an existing website</li>
<li>text input and quick replies</li>
<li>conversational logic that asks the user to open the Jovo Docs (redirect not working on iOS due to platform limitations)</li>
</ul><p><a href="https://www.jovo.tech/demos/starter-web-embeddedchat">Check out the demo here!</a></p><h2 id="more-new-features"><a href="#more-new-features">More New Features</a></h2><p>Alongside the big launch of Jovo for Web, we also shipped some other improvements and bug fixes with the help of our community. <a href="https://github.com/jovotech/jovo-framework/blob/master/CHANGELOG.md">You can find the full changelog here</a>.</p><ul>
<li>We released Google Conversational Actions. <a href="https://www.jovo.tech/news/2020-10-08-google-conversational-actions-builder">Find the announcement here</a>.</li>
<li>New analytics integration: <a href="https://www.jovo.tech/marketplace/jovo-analytics-onedash">OneDash</a>. <em>Thanks to <a href="https://github.com/StepanU">StepanU</a>!</em></li>
<li><a href="https://github.com/jovotech/jovo-framework/pull/838">Dialogflow Genesys integration</a>. <em>Thanks to <a href="https://github.com/dominik-meissner">Dominik Meissner</a>!</em></li>
</ul><h2 id="how-to-update"><a href="#how-to-update">How to Update</a></h2><blockquote>
<p><a href="https://www.jovo.tech/docs/installation/upgrading">Learn more in the Jovo Upgrading Guide</a>.</p>
</blockquote><p>To update to the latest version of Jovo, use the following commands:</p><h3 id="breaking-changes"><a href="#breaking-changes">Breaking Changes</a></h3><p>The "Jovo Web Client" and "Jovo Web Platform" were completely refactored for this release.</p><h2 id="a-big-thank-you"><a href="#a-big-thank-you">A Big Thank You</a></h2><p>Thanks a lot to all the contributors of this release. Everyone of the Jovo core team worked together to make this happen! Special thanks to Max who started working on the web integration more than a year ago as part of his bachelor's thesis.</p><p>Community and core contributors:</p><ul>
<li><a href="https://github.com/StepanU">StepanU</a></li>
<li><a href="https://github.com/dominik-meissner">Dominik Meissner</a></li>
<li><a href="https://github.com/rubenaeg">Ruben Aegerter</a></li>
<li><a href="https://github.com/KaanKC">Kaan Kilic</a></li>
<li><a href="https://github.com/m-ripper">Max Ripper</a></li>
<li><a href="https://github.com/aswetlow">Alex Swetlow</a></li>
</ul><p>And to everyone else who helped with ideas and feature requests in the <a href="https://www.jovo.tech/slack">Jovo Slack</a> and <a href="https://community.jovo.tech/">Jovo Community Forum</a>!</p>
</article></div>]]>
            </description>
            <link>https://www.jovo.tech/news/2020-10-29-jovo-for-web-v3-2</link>
            <guid isPermaLink="false">hacker-news-small-sites-24932588</guid>
            <pubDate>Thu, 29 Oct 2020 16:59:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Large inequality in international energy footprints between income groups]]>
            </title>
            <description>
<![CDATA[
Score 33 | Comments 7 (<a href="https://news.ycombinator.com/item?id=24932205">thread link</a>) | @tonyedgecombe
<br/>
October 29, 2020 | https://sci-hub.tf/10.1038/s41560-020-0579-8 | <a href="https://web.archive.org/web/*/https://sci-hub.tf/10.1038/s41560-020-0579-8">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://sci-hub.tf/10.1038/s41560-020-0579-8</link>
            <guid isPermaLink="false">hacker-news-small-sites-24932205</guid>
            <pubDate>Thu, 29 Oct 2020 16:25:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Introducing Semgrep and r2c]]>
            </title>
            <description>
<![CDATA[
Score 107 | Comments 20 (<a href="https://news.ycombinator.com/item?id=24931985">thread link</a>) | @pabloest
<br/>
October 29, 2020 | https://r2c.dev/blog/2020/introducing-semgrep-and-r2c/ | <a href="https://web.archive.org/web/*/https://r2c.dev/blog/2020/introducing-semgrep-and-r2c/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><div><div><div><p>Free, fast, <a href="https://github.com/returntocorp/semgrep" target="_blank" rel="noopener">open-source</a>, offline, customizable. These are not often words that describe code scanning tools, and that's a shame.</p>
<p>We founded r2c to bring world-class security tools to developers based on our conviction that software will run the most exciting parts of the future: everything from medical equipment to robots to autonomous cars. The security process should not be the foe but rather the enabler of rapid software development. If developers lack tooling that is easy to set up and understand—or if a developer has to convince their manager to spend a few million dollars on advanced security tools each time they change jobs, the future is bleak.</p>
<p>Before founding r2c, we worked on security and developer tools for large companies and governments. It was eye-opening to see that despite massive budgets, their security programs were generally a generation or more behind the tech giants. When it came to security tools for developers, most teams were jaded about scanning code for vulnerabilities; they hated the tools they had to use and usually ignored them beyond doing the minimum necessary to satisfy a compliance checkbox.</p>
<p>What about code scanning at places like Facebook, Apple, Amazon, Netflix, and Google? They don't generally use traditional commercial security tools which ask "how can we find every bug?" Instead, they focus on custom tooling that can build guardrails for developers. This doesn't require million-dollar tools, PhDs in program analysis, or days of compute time. It looks much more like unit tests for security.</p>
<p>We believe there is a gap between traditional compliance tools and simple linters that's ripe for a new approach, and we were fortunate to find partners from Redpoint Ventures and Sequoia Capital who agreed. With them, we raised a $13M Series A round of funding to build a security tool that developers might actually love. We've been working on it quietly for a while now, and we're finally ready to announce it to the world!</p>
<h2>Semgrep</h2>
<p><a href="https://semgrep.dev/" target="_blank" rel="noopener">Semgrep</a>, our open-source product, is specifically designed for eradicating bug classes.
Developers and security engineers can say "this is the safe pattern we always use for (e.g. parsing XML)", write a rule in a few minutes, and enforce that on every editor save, commit, and pull request.</p>
<p>Semgrep is ideal for building security guardrails: start by using frameworks designed with security in mind, then automatically flag code that strays from the <a href="https://semgrep.dev/explore" target="_blank" rel="noopener">secure-by-default path</a>. This is an approach used by <a href="https://landing.google.com/sre/resources/foundationsandprinciples/srs-book/" target="_blank" rel="noopener">Google</a>, <a href="https://about.fb.com/news/2019/01/designing-security-for-billions/" target="_blank" rel="noopener">Facebook</a>, <a href="https://homes.cs.washington.edu/~mernst/pubs/continuous-compliance-ase2020.pdf" target="_blank" rel="noopener">Amazon</a>, Dropbox, Stripe, <a href="https://medium.com/@NetflixTechBlog/scaling-appsec-at-netflix-6a13d7ab6043" target="_blank" rel="noopener">Netflix</a>, and others—a topic <a href="https://events.bizzabo.com/OWASPGlobalAppSec/agenda/session/315858" target="_blank" rel="noopener">Clint Gibler and I presented on at Global AppSec 2020</a>. This approach increases developer productivity, reduces attack surface, minimizes the areas for human inspection and audit, and allows the security team to scalably protect code written by thousands of developers.</p>
<p>The idea behind Semgrep is simple: it feels like a regular search (grep) but is syntax-aware. You can <a href="https://semgrep.dev/learn" target="_blank" rel="noopener">learn Semgrep</a> in a few minutes! And Semgrep can be used for <a href="https://semgrep.dev/explore" target="_blank" rel="noopener">more than just security</a> issues: performance, internationalization, or just annoyances <a href="https://r2c.dev/blog/2020/fixing-leaky-logs-how-to-find-a-bug-and-ensure-it-never-returns" target="_blank" rel="noopener">committed by accident</a>.</p>
<p><span>
      <a href="https://r2c.dev/static/730ca8541e43ed871d2edb7c02226b0c/bde6a/semgrep-foo-small.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Semgrep pattern example" title="Semgrep pattern example" src="https://r2c.dev/static/730ca8541e43ed871d2edb7c02226b0c/bde6a/semgrep-foo-small.png" srcset="https://r2c.dev/static/730ca8541e43ed871d2edb7c02226b0c/bde6a/semgrep-foo-small.png 283w" sizes="(max-width: 283px) 100vw, 283px" loading="lazy">
  </a>
    </span></p>
<p><code>$ semgrep -e foo(1)</code> matches all equivalent variations. <a href="https://semgrep.dev/s/ievans:python-exec" target="_blank" rel="noopener">See a live example of matching <em>exec</em> calls</a></p>
<h2>What's Next?</h2>
<p>Semgrep started as an open-source project at Facebook and we're lucky to have its original author, Yoann Padioleau, on our team at r2c. Since we released the first post-Facebook version (0.4) earlier this year, we've released 25 new versions, added support for 8 new languages, reworked the parsers so we could collaborate with Github on <a href="https://tree-sitter.github.io/" target="_blank" rel="noopener">tree-sitter</a>, been joined by thousands of enthusiastic GitHub followers, and seen over 100K pulls of the Semgrep Docker image.</p>
<p>Our roadmap contains more program analysis features to support the sorts of secure-by-default enforcement that large technology companies are already leveraging so heavily (constant propagation, taint tracking, and more), as well as support for many more languages.</p>
<h2>Batteries Included</h2>
<p>Along with this release of Semgrep, we're announcing the availability of <a href="https://semgrep.dev/" target="_blank" rel="noopener">Semgrep Community</a>, a free, hosted service for managing Semgrep CI as well as Semgrep Teams, a paid service which adds additional features for managing Semgrep that are useful for enterprises. Both these offerrings provide SaaS infrastructure for operating a modern AppSec program. They enable central definition of code standards for your projects and show results where you already work: GitHub, GitLab, Slack, Jira, VS Code, and more.</p>
<p>We're also excited that <a href="https://semgrep.dev/explore" target="_blank" rel="noopener">Semgrep Registry</a> already has 900+ rules written by r2c and the community—you can start running on your project right now! Or if you like to DIY, <a href="https://semgrep.dev/editor" target="_blank" rel="noopener">try writing your own</a>.</p></div></div></div></section></div>]]>
            </description>
            <link>https://r2c.dev/blog/2020/introducing-semgrep-and-r2c/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24931985</guid>
            <pubDate>Thu, 29 Oct 2020 16:07:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Optimizations as a Company of One]]>
            </title>
            <description>
<![CDATA[
Score 29 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24931633">thread link</a>) | @jnfr
<br/>
October 29, 2020 | https://lunchbag.ca/company-of-one/ | <a href="https://web.archive.org/web/*/https://lunchbag.ca/company-of-one/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
		<p><span>
				Written on <time datetime="2020-05-18 16:00:00 +0000 UTC">May 18, 2020</time>
			</span>
		</p>
		

<p>Hello! 👋 My name is Jen and I’m the founder, engineer, designer and customer support at <a href="https://lunchmoney.app/">Lunch Money</a>, a personal finance and budgeting web app.</p>

<p>In short, I am a company of one. I answer the customer support emails and I code and deploy new features. I also manage the <a href="https://developers.lunchmoney.app/" target="_blank">API docs</a> and <a href="https://support.lunchmoney.app/" target="_blank">knowledge base</a>, poke around the logs when there are issues, write the <a href="https://lunchmoney.app/sample_newsletter" target="_blank">bi-monthly newsletters</a> and I designed the logo!</p>

<p>As the company scales, so too must all aspects of my work which I break down into 4 parts: customer support, engineering, product and marketing.</p>

<p><strong>Finding opportunities for process optimization is honestly one of the more fun parts of running a business</strong>. I greatly attribute these to how I’ve been able to stay both solo &amp; sane up until now, currently with 600+ users and $45,000 ARR. In this post, I’m excited to share some of my most successful strategies along with anecdotes from my experience working on Lunch Money.</p>



<p>In the last 14 days, here’s what I got done across all departments:</p>

<ul>
<li><strong>Product:</strong> Launched an internal beta testers program and two sets of new features to test</li>
<li><strong>Engineering:</strong> Wrapped up a 2-week long refactor of some core components on the client-side</li>
<li><strong>Engineering:</strong> Launched a major feature: advanced transaction filters</li>
<li><strong>Engineering:</strong> Closed 13 tickets related to feature improvements and bug fixes</li>
<li><strong>Marketing:</strong> Sent out a newsletter outlining the latest new features</li>
<li><strong>Marketing:</strong> This blog post</li>
<li><strong>Marketing:</strong> Added new pages to the marketing site (<a href="https://lunchmoney.app/features/rules">Rules</a> and <a href="https://lunchmoney.app/features/collaboration">Collaboration</a>) and updated the icons in <a href="https://lunchmoney.app/features">Features</a></li>
<li><strong>Support:</strong> Received 239 inbound support tickets and sent out 358 emails</li>
</ul>

<p>What makes all this work bearable for me is the fact that there is so much variety (which, of course, is the spice of life!). I love being able to switch between tasks to keep the job interesting and my mind refreshed while still being productive overall.</p>

<p><em>Case in point: I just finished a major refactor and pushed out 2 major features to beta, so writing this blog post right now feels like a vacation for my brain.</em></p>

<p>To state the obvious, I enjoy being hyper-efficient (without burning myself out, of course). Even though I am a single person, I can still automate, optimize and parallelize processes.</p>



<h2 id="implement-safeguards">Implement safeguards</h2>

<p><i>Ah, the blissful beginnings of Lunch Money when I’d push code directly to production several times a day.</i></p>

<p>At 500+ users now, those days are long gone and I’ve since needed to adopt the more boring but responsible approach of implementing safeguards to prevent shipping faulty code.</p>

<p>For one, <b>I’ve been thoroughly reviewing my own code before every change</b>. Every major feature, improvement and bug fix lives in a feature branch that I still, by habit from the corporate days, prepend with <code>jen/</code>. After verifying everything works great locally, I make sure to take a break first, whether that’s working on a completely unrelated task, going for a meal or going to bed. The point of this is to ensure that I’m code reviewing with a clear head.</p>

<p>While adhering to these standards of code reviewing myself may add extra time to my overall process, it potentially saves hours of bug-fixing, answering related support tickets and self-loathing (I’m half-kidding here) down the line if I accidentally ship bad code.</p>

<center><blockquote><p lang="en" dir="ltr">New phase of Lunch Money– no more pushing major features straight to production 🤯😂 <a href="https://t.co/RYsz7BnU3L">https://t.co/RYsz7BnU3L</a></p>— Jen (@lunchbag) <a href="https://twitter.com/lunchbag/status/1256987437600927744?ref_src=twsrc%5Etfw">May 3, 2020</a></blockquote> </center>

<p><b>I also recently implemented an internal beta-testing program open to Lunch Money subscribers.</b> With more users accessing my app on a regular basis, the stakes are higher to ship a version that’s as bug-free as possible.</p>

<p>As an engineering team of one, it’s nearly impossible to always get it right the first time, despite having tests (what if I missed an edge case?) or testing locally extensively (how I think my users will use a feature is not always so).</p>

<p>The beta-testing program has provided some additional benefits. It’s nice to have a cohort of users with whom I can have a more candid conversation about Lunch Money and it’s also a great way to show users that their feedback is valued!</p>

<h2 id="automate-later-than-you-need-to">Automate later than you need to</h2>

<p>While automating tasks can save a lot of time in the long-term, it doesn’t always make sense to automate right off the bat.</p>

<p><img src="https://imgs.xkcd.com/comics/is_it_worth_the_time.png"><span>Obligatory XKCD comic (<a href="https://xkcd.com/1205/" target="_blank">link</a>)</span></p>

<p>Automating too late is not a bad thing. I’ll have done the manual work enough times to understand how to eventually automate the task and what edge cases to look out for. I equate it to doing a job yourself before hiring someone– it’s always better to grok the requirements first to some degree so you can understand how to best utilize who you’ve brought on (and appreciate them more!).</p>

<p>I manually triggered the emails for bi-monthly account summaries for months because I wasn’t confident the sending cadence was reasonable. After observing open rates and collecting user feedback, these are now automated to go out on the 5th &amp; 25th of every month.</p>

<p>Something that I’m glad I didn’t spend the time to automate at all was the referral program. If an existing user refers a new user and the new user subscribes to a plan, then both users will receive credit for 1 month.</p>

<p>In the end, the referral program only brought on 11 extra users. It was not a pain at all to manually process rewards for those who did. If I had spent time conjuring up all the edge cases and automating this process, it would not have been worth the time investment.</p>

<h2 id="keep-low-hanging-fruits-in-the-back-pocket">Keep low-hanging fruits in the back pocket</h2>

<p>In my task management system, I use a tag (🍏) to denote which tasks are low-hanging fruits. For the uninitiated, low-hanging fruits are quick tasks that are easy to knock out, such as one-liners or 5-minute fixes.</p>

<p><img src="https://media.giphy.com/media/YP1Jb0JNc7kqFDbdjm/giphy.gif"></p>

<p>I find that keeping these around and tackling them on days when you feel generally unmotivated can really help raise spirits. Still being able to get something completed and shipped is a great way to get out of a slump.</p>



<h2 id="timing-marketing-pushes">Timing marketing pushes</h2>

<p>At Lunch Money, a big part of the business is using the services of Plaid for bank syncing. Plaid charges on a monthly basis which means that if a user signs up on April 30, connects a bank account immediately and doesn’t end up subscribing at the end of their 14-day trial, they will charge me for this user in both April &amp; May’s invoices.</p>

<p>This realization coupled with my intense aversion to paying more than I need to has shaped a lot of practices at Lunch Money.</p>

<p>For one, the data retention policy used to be 30 days which means if your trial ends and you didn’t put in your billing information, your data will be deleted in 30 days. This certainly guarantees that I’ll be overpaying for churned users and is the reason why the data retention policy has since been revised to 5 days.</p>

<p>In total, a user who does not end up subscribing can spend up to 26 days in the Lunch Money system. This comprises of a 14-day trial, the potential for a 1 week trial extension and a 5 day grace period. Assuming enough users connect their bank accounts, the best way to minimize my costs for churned users is to ensure their lifetimes are within one calendar month.</p>

<p><img src="https://lunchbag.ca/uploads/calendar-1.png"></p>

<p>As a result, I always schedule marketing pushes such as blog posts and product launches in the first few days of the month. When I came to this realization, my next Plaid bill went down for the first time.</p>

<h2 id="merging-marketing-and-engineering-for-a-combo-win">Merging marketing and engineering for a combo win</h2>

<p>Taking this a step further, the perk of having a spike in new users is that their trial periods more-or-less overlap. Usually when a new user signs up, they will poke around the product and maybe send in a bug report, a feature request or another piece of feedback. If I reply and address their feedback by putting in a fix or shipping their requested feature within days, more often than not, they end up converting into a happy customer.</p>

<p>I’ve therefore identified the following cycle to maximize potential conversions from spikes in user signups:</p>

<p><img src="https://lunchbag.ca/uploads/gantt-2.png"></p>

<p>After a marketing push, let’s say a product launch, I will see an immediate increase in signups lasting about 3 days and then slowly trailing off.</p>

<p>Over the next few days, I’ll start hearing from these new users via support tickets. I prioritize responding to them and I get to work. Showing these users that I’m committed to improving Lunch Money based on their feedback is a great and honest sales tactic.</p>

<p>About 3 days before the initial wave of user trials ends, I wrap up my engineering sprint and send out a newsletter detailing the latest features and improvements. This re-enforces to new users that the product is under continuous development.</p>

<p>Finally, I have a drip campaign that automatically notifies users a few days before their trial expires and on the actual date of expiry. This period of time is when I typically see most users convert 🤞.</p>



<p>I used to think that if I were to hire someone, it would first be a customer support agent but I’ve since been moving away from that idea. Users are constantly surprised (in a good way!) when they realize the founder is responding to their bug reports or feature requests directly.</p>

<p><strong>Consistently providing great customer support is a long-term investment for Lunch Money</strong> as it is undoubtedly a great way to turn customers into champions. As I receive and respond to support tickets, I’m also able to identify and overhaul the common sources of trouble for users.</p>

<p>I’ve always believed that customer support would be the most important and the hardest part of the business to scale, especially if I have the goal of staying a company of one.</p>

<p>Corroborating data from <a href="https://emailmeter.com/">EmailMeter</a> and my database, it seems my changes are making a difference. Despite a growing user base, inbound support emails remain fairly steady!</p>

<p><img src="https://lunchbag.ca/uploads/screen-shot-2020-05-21-at-4-20-15-pm.png"><span>Handling support on my own should be sustainable at least for the foreseeable future!</span></p>

<h2 id="how-do-support-tickets-work-at-lunch-money">How do support tickets work at Lunch Money?</h2>

<p>A feedback button is located at the bottom right corner of every page. Clicking on it opens up a text area wherein users can submit feature requests, questions, bug …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://lunchbag.ca/company-of-one/">https://lunchbag.ca/company-of-one/</a></em></p>]]>
            </description>
            <link>https://lunchbag.ca/company-of-one/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24931633</guid>
            <pubDate>Thu, 29 Oct 2020 15:36:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Compiling a Lisp to x86-64: Labelled procedure calls]]>
            </title>
            <description>
<![CDATA[
Score 87 | Comments 13 (<a href="https://news.ycombinator.com/item?id=24931577">thread link</a>) | @tekknolagi
<br/>
October 29, 2020 | https://bernsteinbear.com/blog/compiling-a-lisp-11/ | <a href="https://web.archive.org/web/*/https://bernsteinbear.com/blog/compiling-a-lisp-11/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <p><span data-nosnippet="">
<em><a href="https://bernsteinbear.com/blog/compiling-a-lisp-0/">first</a></em> – <em><a href="https://bernsteinbear.com/blog/compiling-a-lisp-10/">previous</a></em>
</span></p>

<p>Welcome back to the Compiling a Lisp series. Last time, we learned about Intel
instruction encoding. This time, we’re going to use that knowledge to compile
procedure calls.</p>

<p>The usual function expression in Lisp is a <code>lambda</code> — an anonymous function
that can take arguments and close over variables. Procedure calls are <em>not</em>
this. They are simpler constructs that just take arguments and return values.</p>

<p>We’re adding procedure calls first as a stepping stone to full closure support.
This will help us get some kind of internal calling convention established and
stack manipulation figured out before things get too complicated.</p>

<p>After this post, we will be able to support programs like the following:</p>

<div><div><pre><code><span>(</span><span>labels</span> <span>((</span><span>add</span> <span>(</span><span>code</span> <span>(</span><span>x</span> <span>y</span><span>)</span> <span>(</span><span>+</span> <span>x</span> <span>y</span><span>)))</span>
         <span>(</span><span>sub</span> <span>(</span><span>code</span> <span>(</span><span>x</span> <span>y</span><span>)</span> <span>(</span><span>-</span> <span>x</span> <span>y</span><span>))))</span>
    <span>(</span><span>labelcall</span> <span>sub</span> <span>4</span> <span>(</span><span>labelcall</span> <span>add</span> <span>1</span> <span>2</span><span>)))</span>
<span>; =&gt; 1</span>
</code></pre></div></div>

<p>and even this snazzy factorial function:</p>

<div><div><pre><code><span>(</span><span>labels</span> <span>((</span><span>factorial</span> <span>(</span><span>code</span> <span>(</span><span>x</span><span>)</span> 
            <span>(</span><span>if</span> <span>(</span><span>&lt;</span> <span>x</span> <span>2</span><span>)</span> <span>1</span> <span>(</span><span>*</span> <span>x</span> <span>(</span><span>labelcall</span> <span>factorial</span> <span>(</span><span>-</span> <span>x</span> <span>1</span><span>)))))))</span>
    <span>(</span><span>labelcall</span> <span>factorial</span> <span>5</span><span>))</span>
<span>; =&gt; 120</span>
</code></pre></div></div>

<p>These are fairly pedestrian snippets of code but they demonstrate some new
features we are adding, like:</p>

<ul>
  <li>A new <code>labels</code> form that all programs will now have to look like</li>
  <li>A new <code>code</code> form for describing procedures and their parameters</li>
  <li>A new <code>labelcall</code> expression for calling procedures</li>
</ul>

<p>Ghuloum does not explain why he does this, but I imagine that the <code>labels</code> form
was chosen over allowing multiple separate top-level bindings because it is
easier to parse and traverse.</p>

<h3 id="big-ideas">Big ideas</h3>

<p>In order to compile a program, we are going to traverse every binding in the
<code>labels</code>. For each binding, we will generate code for each <code>code</code> object.</p>

<p>Compiling <code>code</code> objects requires making an environment for their parameters.
We’ll establish a calling convention later so that our compiler knows where to
find the parameters.</p>

<p>Then, once we’ve emitted all the code for the bindings, we will compile the
body. The body may, but is not required to, contain a <code>labelcall</code> expression.</p>

<p>In order to compile a <code>labelcall</code> expression, we will compile all of the
arguments provided, save them in consecutive locations on the stack, and then
emit a <code>call</code> instruction.</p>

<p>When all of these pieces come together, the resulting machine code will look
something like this:</p>

<div><div><pre><code>mov rsi, rdi  # prologue
label0:
  label0_code
label1:
  label1_code
main:
  main_code
</code></pre></div></div>

<p>You can see that all of the <code>code</code> objects will be compiled in sequence,
followed by the body of the <code>labels</code> form.</p>

<s>
Because I have not yet figured out how to start executing at somewhere other
than the beginning of the generated code, and because I don't store generated
code in any intermediate buffers, and because we don't know the sizes of any
code in advance, I do this funky thing where I emit a `jmp` to the body code.

If you, dear reader, have a better solution, please let me know.
</s>

<p><strong>Edit:</strong> <em>jsmith45</em> gave me the encouragement I needed to work on this again.
It turns out that storing the code offset of the beginning of the <code>main_code</code>
(the <code>labels</code> body) adding that to the <code>buf-&gt;address</code> works just fine. I’ll
explain more below.</p>

<h3 id="a-calling-convention">A calling convention</h3>

<p>We’re not going to use the System V AMD64 ABI. That calling convention requires
that parameters are passed first in certain registers, and then on the stack.
Instead, we will pass all parameters on the stack.</p>

<p>This makes our code simpler, but it also means that at some point later on, we
will have to add a different kind of calling convention so that we can call
foreign functions (like <code>printf</code>, or <code>exit</code>, or something). Those functions
expect their parameters in registers. We’ll worry about that later.</p>

<p>If we borrow and adapt the excellent diagrams from the Ghuloum tutorial, this
means that right before we make a procedure call, our stack will look like
this:</p>

<blockquote>
  <p>Stack illustration courtesy of <a href="https://leonardschuetz.ch/">Leonard</a>.</p>
</blockquote>

<p>You can see the first return point at <code>[rsp]</code>. This is the return point placed
by the caller of the <em>current</em> function.</p>

<p>Above that are whatever local variables we have declared with <code>let</code> or perhaps
are intermediate values from some computation.</p>

<p>Above that is a blank space reserved for the second return point. This is the
return point for the <em>about-to-be-called</em> function. The <code>call</code> instruction will
fill in after evaluating all the arguments.</p>

<p>Above the return point are all the outgoing arguments. They will appear as
locals for the procedure being called.</p>

<p>Finally, above the arguments, is untouched free stack space.</p>

<p>The <code>call</code> instruction decrements <code>rsp</code> and then writes to <code>[rsp]</code>. This means
that if we just emitted a <code>call</code>, the first local would be overwritten. No
good. Worse, the way the stack would be laid out would mean that the locals
would look like arguments.</p>

<p>In order to solve this problem, we need to first adjust <code>rsp</code> to point to the
last local. That way the decrement will move it below the local and the return
address will go between the locals and the arguments.</p>

<p>After the <code>call</code> instruction, the stack will look different. Nothing will have
actually changed, except for <code>rsp</code>. This change to <code>rsp</code> means that the callee
has a different view:</p>

<blockquote>
  <p>Stack illustration courtesy of <a href="https://leonardschuetz.ch/">Leonard</a>.</p>
</blockquote>

<p>The empty colored in spaces below the return point indicate that the values on
the stack are “hidden” from view, since they are above (higher addresses than)
<code>[rsp]</code>. The called function will <em>not</em> be able to access those values.</p>

<p>If the called function wants to use one of its arguments, it can pull it off
the stack from its designated location.</p>

<blockquote>
  <p>One unfortunate consequence of this calling convention is that Valgrind does
not understand it. Valgrind cannot understand that the caller has placed data
on the stack specifically for the callee to read it, and thinks this is a
move/jump of an uninitialized value. This means that we get some errors now
on these labelcall tests.</p>
</blockquote>

<p>Eventually, when the function returns, the <code>ret</code> instruction will pop the
return point off the stack and jump to it. This will bring us back to the
previous call frame.</p>

<p>That’s that! I have yet to find a good tool that will let me visualize the
stack as a program is executing. GDB probably has a mode hidden away somewhere
undocumented that does exactly this. Cutter sort of does, but it’s finicky in
ways I don’t really understand. Maybe one day <a href="http://akkartik.name/">Kartik</a>’s
x86-64 Mu fork will be able to do this.</p>

<h3 id="building-procedure-calls-in-small-pieces">Building procedure calls in small pieces</h3>

<p>In order for this set of changes to make sense, I am going to explain all of
the pieces one at a time, top-down.</p>

<p>First, we’ll look at the new-and-improved <code>Compile_entry</code>, which has been
updated to handle the <code>labels</code> form. This will do the usual Lisp entrypoint
setup and some checks about the structure of the AST.</p>

<p>Then, we’ll actually look at compiling the <code>labels</code>. This means going through
the bindings one-by-one and compiling their <code>code</code> objects.</p>

<p>Then, we’ll look at what it means to compile a <code>code</code> object. Hint: it’s very
much like <code>let</code>.</p>

<p>Last, we’ll tie it all together when compiling the body of the <code>labels</code> form.</p>

<h3 id="compiling-the-entrypoint">Compiling the entrypoint</h3>

<p>Most of this code is checking. What used to just compile an expression now
validates that what we’ve passed in at least vaguely looks like a well-formed
<code>labels</code> form before picking it into its component parts: the <code>bindings</code> and
the <code>body</code>.</p>

<div><div><pre><code><span>int</span> <span>Compile_entry</span><span>(</span><span>Buffer</span> <span>*</span><span>buf</span><span>,</span> <span>ASTNode</span> <span>*</span><span>node</span><span>)</span> <span>{</span>
  <span>assert</span><span>(</span><span>AST_is_pair</span><span>(</span><span>node</span><span>)</span> <span>&amp;&amp;</span> <span>"program must have labels"</span><span>);</span>
  <span>// Assume it's (labels ...)</span>
  <span>ASTNode</span> <span>*</span><span>labels_sym</span> <span>=</span> <span>AST_pair_car</span><span>(</span><span>node</span><span>);</span>
  <span>assert</span><span>(</span><span>AST_is_symbol</span><span>(</span><span>labels_sym</span><span>)</span> <span>&amp;&amp;</span> <span>"program must have labels"</span><span>);</span>
  <span>assert</span><span>(</span><span>AST_symbol_matches</span><span>(</span><span>labels_sym</span><span>,</span> <span>"labels"</span><span>)</span> <span>&amp;&amp;</span>
         <span>"program must have labels"</span><span>);</span>
  <span>ASTNode</span> <span>*</span><span>args</span> <span>=</span> <span>AST_pair_cdr</span><span>(</span><span>node</span><span>);</span>
  <span>ASTNode</span> <span>*</span><span>bindings</span> <span>=</span> <span>operand1</span><span>(</span><span>args</span><span>);</span>
  <span>assert</span><span>(</span><span>AST_is_pair</span><span>(</span><span>bindings</span><span>)</span> <span>||</span> <span>AST_is_nil</span><span>(</span><span>bindings</span><span>));</span>
  <span>ASTNode</span> <span>*</span><span>body</span> <span>=</span> <span>operand2</span><span>(</span><span>args</span><span>);</span>
  <span>return</span> <span>Compile_labels</span><span>(</span><span>buf</span><span>,</span> <span>bindings</span><span>,</span> <span>body</span><span>,</span> <span>/*labels=*/</span><span>NULL</span><span>);</span>
<span>}</span>
</code></pre></div></div>
<p><code>Compile_entry</code> dispatches to <code>Compile_labels</code> for iterating over all of the
labels. <code>Compile_labels</code> is a recursive function that keeps track of all the
labels so far in its arguments, so we start it off with an empty <code>labels</code>
environment.</p>

<h3 id="compiling-labels">Compiling labels</h3>

<p>In <code>Compile_labels</code>, we have first a base case: if there are no labels we
should just emit the body.</p>

<div><div><pre><code><span>int</span> <span>Compile_labels</span><span>(</span><span>Buffer</span> <span>*</span><span>buf</span><span>,</span> <span>ASTNode</span> <span>*</span><span>bindings</span><span>,</span> <span>ASTNode</span> <span>*</span><span>body</span><span>,</span>
                   <span>Env</span> <span>*</span><span>labels</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>AST_is_nil</span><span>(</span><span>bindings</span><span>))</span> <span>{</span>
    <span>buf</span><span>-&gt;</span><span>entrypoint</span> <span>=</span> <span>Buffer_len</span><span>(</span><span>buf</span><span>);</span>
    <span>// Base case: no bindings. Compile the body</span>
    <span>Buffer_write_arr</span><span>(</span><span>buf</span><span>,</span> <span>kEntryPrologue</span><span>,</span> <span>sizeof</span> <span>kEntryPrologue</span><span>);</span>
    <span>_</span><span>(</span><span>Compile_expr</span><span>(</span><span>buf</span><span>,</span> <span>body</span><span>,</span> <span>/*stack_index=*/</span><span>-</span><span>kWordSize</span><span>,</span> <span>/*varenv=*/</span><span>NULL</span><span>,</span>
                   <span>labels</span><span>));</span>
    <span>Buffer_write_arr</span><span>(</span><span>buf</span><span>,</span> <span>kFunctionEpilogue</span><span>,</span> <span>sizeof</span> <span>kFunctionEpilogue</span><span>);</span>
    <span>return</span> <span>0</span><span>;</span>
  <span>}</span>
  <span>// ...</span>
<span>}</span>
</code></pre></div></div>

<p>We also set the buffer entrypoint location to the position where we’re going to
emit the body of the <code>labels</code>. We’ll use this later when executing, or later in
the series when we emit ELF binaries. You’ll have to add a field <code>word
entrypoint</code> to your <code>Buffer</code> struct.</p>

<p>We pass in an empty <code>varenv</code>, since we are not accumulating any locals along
the way; only labels. For the same reason, we give a <code>stack_index</code> of
<code>-kWordSize</code> — the first slot.</p>

<p>If we <em>do</em> have labels, on the other hand, we should deal with the first label.
This means:</p>

<ul>
  <li>pulling out the name and the code object</li>
  <li>binding the name to the <code>code</code> location (the current location)</li>
  <li>compiling the <code>code</code></li>
</ul>

<p>And then from there we deal with the others recursively.</p>

<div><div><pre><code><span>int</span> <span>Compile_labels</span><span>(</span><span>Buffer</span> <span>*</span><span>buf</span><span>,</span> <span>ASTNode</span> <span>*</span><span>bindings</span><span>,</span> <span>ASTNode</span> <span>*</span><span>body</span><span>,</span>
                   <span>Env</span> <span>*</span><span>labels</span><span>)</span> <span>{</span>
  <span>// ....</span>
  <span>assert</span><span>(</span><span>AST_is_pair</span><span>(</span><span>bindings</span><span>));</span>
  <span>// Get the next binding</span>
  <span>ASTNode</span> <span>*</span><span>binding</span> <span>=</span> <span>AST_pair_car</span><span>(</span><span>bindings</span><span>);</span>
  <span>ASTNode</span> <span>*</span><span>name</span> <span>=</span> <span>AST_pair_car</span><span>(</span><span>binding</span><span>);</span>
  <span>assert</span><span>(</span><span>AST_is_symbol</span><span>(</span><span>name</span><span>));</span>
  <span>ASTNode</span> <span>*</span><span>binding_code</span> <span>=</span> <span>AST_pair_car</span><span>(</span><span>AST_pair_cdr</span><span>(</span><span>binding</span><span>));</span>
  <span>word</span> <span>function_location</span> <span>=</span> <span>Buffer_len</span><span>(</span><span>buf</span><span>);</span>
  <span>// Bind the name to the location in the instruction stream</span>
  <span>Env</span> <span>entry</span> <span>=</span> <span>Env_bind</span><span>(</span><span>AST_symbol_cstr</span><span>(</span><span>name</span><span>),</span> <span>function_location</span><span>,</span> <span>labels</span><span>);</span>
  <span>// Compile the binding function</span>
  <span>_</span><span>(</span><span>Compile_code</span><span>(</span><span>buf</span><span>,</span> <span>binding_code</span><span>,</span> <span>&amp;</span><span>entry</span><span>));</span>
  <span>return</span> <span>Compile_labels</span><span>(</span><span>buf</span><span>,</span> <span>AST_pair_cdr</span><span>(</span><span>binding…</span></code></pre></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://bernsteinbear.com/blog/compiling-a-lisp-11/">https://bernsteinbear.com/blog/compiling-a-lisp-11/</a></em></p>]]>
            </description>
            <link>https://bernsteinbear.com/blog/compiling-a-lisp-11/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24931577</guid>
            <pubDate>Thu, 29 Oct 2020 15:31:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Page Load Time Comparison of Raspberry Pi 3 and 4 Web Servers]]>
            </title>
            <description>
<![CDATA[
Score 45 | Comments 20 (<a href="https://news.ycombinator.com/item?id=24929444">thread link</a>) | @sT370ma2
<br/>
October 29, 2020 | https://cheapskatesguide.org/articles/raspberry-pi-3-4-web-server.html | <a href="https://web.archive.org/web/*/https://cheapskatesguide.org/articles/raspberry-pi-3-4-web-server.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://cheapskatesguide.org/articles/raspberry-pi-3-4-web-server.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24929444</guid>
            <pubDate>Thu, 29 Oct 2020 11:43:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[  You will need a subscription license to access Qt 6 (non-LGPL)]]>
            </title>
            <description>
<![CDATA[
Score 32 | Comments 12 (<a href="https://news.ycombinator.com/item?id=24928720">thread link</a>) | @deng
<br/>
October 29, 2020 | https://www.qt.io/faq/what-happens-to-my-perpetual-licenses-once-i-convert-to-subscription | <a href="https://web.archive.org/web/*/https://www.qt.io/faq/what-happens-to-my-perpetual-licenses-once-i-convert-to-subscription">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p><span><span>Yes. If your </span><span>Qt licenses are perpetual, you may continue to use the product in perpetuity after your maintenance expires.&nbsp; Access to product and technical support will only be available via the purchase of an Extended Maintenance Contract for software releases that are end-of-life. If you opt not to renew, please note that Qt will not guarantee to support software versions acquired with a perpetual license.</span></span></p>
<p><span><span>Please note, you will need a subscription license to access Qt 6.</span></span></p>
<!--more-->
<p><span><span>Qt versions can be viewed <a href="https://wiki.qt.io/QtReleasing" rel="noopener">here</a>.</span></span></p>
</span></p></div>]]>
            </description>
            <link>https://www.qt.io/faq/what-happens-to-my-perpetual-licenses-once-i-convert-to-subscription</link>
            <guid isPermaLink="false">hacker-news-small-sites-24928720</guid>
            <pubDate>Thu, 29 Oct 2020 09:44:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Image Scaling Attacks]]>
            </title>
            <description>
<![CDATA[
Score 416 | Comments 67 (<a href="https://news.ycombinator.com/item?id=24927655">thread link</a>) | @wendythehacker
<br/>
October 28, 2020 | https://embracethered.com/blog/posts/2020/husky-ai-image-rescaling-attacks/ | <a href="https://web.archive.org/web/*/https://embracethered.com/blog/posts/2020/husky-ai-image-rescaling-attacks/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
    <p>This post is part of a series about machine learning and artificial intelligence. Click on the blog tag “huskyai” to see related posts.</p>
<ul>
<li><a href="https://embracethered.com/blog/posts/2020/husky-ai-walkthrough/">Overview</a>: How Husky AI was built, threat modeled and operationalized</li>
<li><a href="https://embracethered.com/blog/posts/2020/husky-ai-threat-modeling-machine-learning/">Attacks</a>: Some of the attacks I want to investigate, learn about, and try out</li>
</ul>
<p>A few weeks ago while preparing demos for my GrayHat 2020 - Red Team Village presentation I ran across “Image Scaling Attacks” in <a href="https://www.usenix.org/system/files/sec20-quiring.pdf">Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks in Machine Learning</a> by Erwin Quiring, et al.</p>
<p>I thought that was so cool!</p>
<h2 id="what-is-an-image-scaling-attack">What is an image scaling attack?</h2>
<p>The basic idea is to hide a smaller image inside a larger image (it should be about 5-10x the size). The attack is easy to explain actually:</p>
<ol>
<li>Attacker crafts a malicious input image by hiding the desired target image inside a benign image</li>
<li>The image is loaded by the server</li>
<li>Pre-processing resizes the image</li>
<li>The server acts and makes decision based on a different image then intended</li>
</ol>
<p>My goal was to hide a husky image inside another image:</p>
<p><a href="https://embracethered.com/blog/images/2020/image-rescale-attack.gif"><img src="https://embracethered.com/blog/images/2020/image-rescale-attack.gif" alt="Image Rescaling Attack"></a></p>
<p>Here are the two images I used - before and after the modification:
<a href="https://embracethered.com/blog/images/2020/image-rescaling-attack-schoenbrunn.png"><img src="https://embracethered.com/blog/images/2020/image-rescaling-attack-schoenbrunn.png" alt="Image Rescaling Attack"></a></p>
<p>If you look closely, you can see that the second image does have some strange dots all around. But this is not noticable when viewed in smaller version.</p>
<p>You can find the code on <a href="https://github.com/EQuiw/2019-scalingattack">Github</a>. I used Google Colab to run it, and there were some errors initialy but it worked - let me know if interested and I can clean up and share the Notebook also.</p>
<h2 id="rescaling-and-magic-happens">Rescaling and magic happens!</h2>
<p>Now, look what happens when the image is loaded and resized with <code>OpenCV</code> using default settings:</p>
<p><a href="https://embracethered.com/blog/images/2020/image-rescaling-attack.png"><img src="https://embracethered.com/blog/images/2020/image-rescaling-attack.png" alt="Image Rescaling Attack"></a></p>
<p>On the left you can see the original sized image, and on the left the same image downsized to 128x128 pixels.</p>
<p><strong>That’s amazing!</strong></p>
<p>The downsized image is an entirely different picture now! Of course I picked a husky, since I wanted to attack “Husky AI” and find another bypass.</p>
<h2 id="implications">Implications</h2>
<p>This can have a set of implications:</p>
<ol>
<li><strong>Training process:</strong> Images that poisen the training data (as pre-processing rescales images)</li>
<li><strong>Model queries:</strong> The model might predict on a different image than the one the user uploaded</li>
<li><strong>Non ML related attacks:</strong> This can also be an issue in other, non machine learning areas.</li>
</ol>
<p>I guess security never gets boring, there is always something new to learn.</p>
<h2 id="mitigations">Mitigations</h2>
<p>Turns out that Husky AI uses PIL and that was not vulnerable to this attack by default.</p>
<p>I got lucky, because initially Husky AI did use <code>OpenCV</code> and it’s default settings to resize images. But for some reason I changed that early on (not knowing it would also mitigate this attack).</p>
<p>If you use <code>OpenCV</code> the issue can be fixed by using the <code>interpolation</code> argument when calling the <code>resize</code> API to not have it use the default.</p>
<p>Hope that was useful and interesting.</p>
<p>Cheers,
Johann.</p>
<p><a href="https://twitter.com/wunderwuzzi23">@wunderwuzzi23</a></p>
<h2 id="references">References</h2>
<ul>
<li>Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks in Machine Learning (<a href="https://www.usenix.org/system/files/sec20-quiring.pdf">https://www.usenix.org/system/files/sec20-quiring.pdf</a>) (Erwin Quiring, TU Braunschweig)</li>
<li><a href="https://github.com/EQuiw/2019-scalingattack">https://github.com/EQuiw/2019-scalingattack</a></li>
</ul>

  </section></div>]]>
            </description>
            <link>https://embracethered.com/blog/posts/2020/husky-ai-image-rescaling-attacks/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24927655</guid>
            <pubDate>Thu, 29 Oct 2020 05:48:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Falsehoods programmers believe about addresses (2013)]]>
            </title>
            <description>
<![CDATA[
Score 57 | Comments 62 (<a href="https://news.ycombinator.com/item?id=24926417">thread link</a>) | @gk1
<br/>
October 28, 2020 | https://www.mjt.me.uk/posts/falsehoods-programmers-believe-about-addresses/ | <a href="https://web.archive.org/web/*/https://www.mjt.me.uk/posts/falsehoods-programmers-believe-about-addresses/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p>Perhaps you've read posts like <a href="http://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/">Falsehoods Programmers Believe About Names</a>
and <a href="http://infiniteundo.com/post/25326999628/falsehoods-programmers-believe-about-time">Falsehoods programmers believe about time</a>.
Maybe you've also read <a href="http://wiesmann.codiferes.net/wordpress/?p=15187&amp;lang=en">Falsehoods programmers believe about geography</a>.</p>

<p>Addressing is a fertile ground for incorrect assumptions, because everyone's used to dealing with addresses and 99% of the time they seem so simple.
Below are some incorrect assumptions I've seen made, or made myself, or had reported to me.
(If you want to look up an address for a UK postcode or vice-versa to confirm what I'm telling you, try the <a href="http://www.royalmail.com/postcode-finder/">Royal Mail Postcode Finder</a>)</p>

<!-- Composition of building numbers -->

<ul>
<li><p><strong>An address will start with, or at least include, a building number.</strong></p>

<p>Counterexample: Royal Opera House, Covent Garden, London, WC2E 9DD, United Kingdom.</p></li>
<li><p><strong>When there is a building number, it will be all-numeric.</strong></p>

<p>Counterexample: 1A Egmont Road, Middlesbrough, TS4 2HT</p>

<p>4-5 Bonhill Street, London, EC2A 4BX</p></li>
<li><p><strong>No buildings are numbered zero</strong></p>

<p>Counterexample: 0 Egmont Road, Middlesbrough, TS4 2HT</p></li>
<li><p><strong>Well, at the very least no buildings have negative numbers</strong></p>

<p>Guy Chisholm provided this counterexample: Minusone Priory Road, Newbury, RG14 7QS</p>

<p>(none of the databases I've checked render this as -1)</p></li>
<li><p><strong>We can put those funny numbers into the building name field, as no buildings have both a name and a funny number</strong></p>

<p>Counterexample: Idas Court, 4-6 Princes Road, Hull, HU5 2RD</p></li>
<li><p><strong>When there's a building name, there won't be a building number (or vice-versa)</strong></p>

<p>Counterexample: Flat 1.4, Ziggurat Building, 60-66 Saffron Hill, London, EC1N 8QX, United Kingdom</p></li>
<li><p><strong>A building number will only be used once per street</strong></p>

<p>The difference between 50 Ammanford Road, Tycroes, Ammanford, SA18 3QJ and 50 Ammanford Road, Llandybie, Ammanford, SA18 3YF is about 4 miles (<a href="https://maps.google.co.uk/maps?q=SA18+3QJ+to+SA18+3YF">Google Maps</a>).</p></li>
<li><p><strong>When there's line with a number in an address, it's the building number.</strong></p>

<p>Counterexample: Flat 18, Da Vinci House, 44 Saffron Hill, London, EC1N 8FH, United Kingdom</p>

<p>You also get suite numbers, floor numbers, unit numbers, and organisations with numbers in their names.</p>

<p>Adrien Piérard contributes an address from Japan with fifteen digits in six separate numbers (five if you count the zip code as a single number). The format is: 980-0804 (zip code), Miyagi-ken (prefecture) Sendai-shi (city) Aoba-ku (ward) Kokubuncho (district) 4-10-20 (sub-district-number block-number lot-number) Sendai (building name) 401 (flat number).</p></li>
<li><p><strong>OK, the first line starting with a number then</strong></p>

<p>Counterexample: 3 Store, 311-318 High Holborn, London, WC1V 7BN</p></li>
<li><p><strong>A building will only have one number</strong></p>

<p>Benton Lam offers this address from the Hong Kong Special Administrative Region - it has both a number on its road (14) and in its group of buildings (3): 15/F, Cityplaza 3, 14 TaiKoo Wan Road, Island East, HKSAR</p></li>
<li><p><strong>The number of buildings is the difference between the highest and lowest building numbers</strong></p>

<p>Tibor Schütz points out building numbers may be skipped - for example, on a street where even-numbered buildings are on one side, odd numbers on the other; multiple buildings sharing the same number (such as where a new house has been built) and buildings with more than one number.</p>

<p>Cyrille Chépélov and Sami Lehtinen tell me in Antibes, France and rural Finland some buildings are numbered based on the distance from the start of the road - such as Longroad 65 for the building 750m from the start of longroad.</p></li>
<li><p><strong>If the addresses on the left of the road are even, the addresses on the right must be odd</strong></p>

<p>Cyrille Chépélov points out that in places, <a href="https://maps.google.fr/maps?q=48.857415,2.467167">Boulevard Théophile Sueur, Montreuil, Seine-Saint-Denis, France</a> has evens-only on both sides. The two sides are also in different cities and Départements.</p></li>
<li><p><strong>A building name won't also be a number</strong></p>

<p>Ben Tilly reports on Ten Post Office Sq, Boston MA 02109 USA - which is not, reportedly, the same as 10 Post Office Sq, Boston MA 02109 USA.</p></li>
<li><p><strong>Well, at least you can omit leading zeros</strong></p>

<p>Shaun Crampton reports living at 101 Alma St, Apartment 001, Palo Alto - where apartments 1 and 001 were on different floors.</p></li>
<li><p><strong>A street with a building A will not also have a building Alpha</strong></p>

<p>Douglas Perreault reports he lived in a block within a condo association; it was a large association, with blocks A through Z then Alpha, Beta, Gamma, Delta, and Theta. Mail and deliveries were often misrouted from block Alpha to block A and vice-versa. His address at the time was: 14100 N 46th St., Alpha 39, Tampa, FL 33613</p></li>
</ul>

<!-- Composition of street names -->

<ul>
<li><p><strong>A street name won't include a number</strong></p>

<p>8 Seven Gardens Burgh, WOODBRIDGE, IP13 6SU (pointed out by Raphael Mankin)</p></li>
<li><p><strong>OK, but numbers in street names are expressed as words, not digits</strong></p>

<p>Jan Jongboom reports streets can be numbered in the Netherlands - for example, Plein 1944 in Nijmegen.</p></li>
<li><p><strong>When there's a numbered street and a house number, there will be a separator between them</strong></p>

<p>Another from Jan Jongboom: Gondel 2695, Lelystad, means area Gondel, street 26, number 95</p></li>
<li><p><strong>Street names always end in descriptors like 'street', 'avenue', 'drive', 'square', 'hill' or 'view'</strong></p>

<p>They don't always - for example: Piccadilly, London, W1J 9PN</p></li>
<li><p><strong>OK, but when they do have a descriptor there will only be one</strong></p>

<p>A street name can be entirely descriptors: 17 Hill Street, London, W1J 5LJ or <a href="https://en.wikipedia.org/wiki/Avenue_Road">Avenue Road, Toronto, Ontario</a>.</p></li>
<li><p><strong>OK, but when they do have a descriptor it will be at the end</strong></p>

<p>French addresses use prefix descriptors like 'rue', 'avenue', 'place' and 'allee'.</p></li>
<li><p><strong>OK, but if there's a descriptor it'll be at the start or end of the street name.</strong></p>

<p>Or the middle, like 3 Bishops Square Business Park, Hatfield, AL10 9NA</p></li>
<li><p><strong>OK, but at the very least you wouldn't name a town Street</strong></p>

<p><a href="https://maps.google.co.uk/maps?q=Street,+Somerset">Actually there's a town called Street in Somerset, UK</a>.</p></li>
<li><p><strong>Street numbers (and building numbers) don't contain fractions</strong></p>

<p>Dan, Fred Kroon, David Underwood and Daniel Dickison submitted examples of fractional street numbers like <a href="https://maps.google.com/maps?q=43rd%20%C2%BD%20st,%20Pittsburgh,%20PA">43rd ½ St, Pittsburgh, PA</a>, and of fractional building numbers. These can be written in unicode (43rd ½ St), as a fraction with a slash (43 1/2) or as a decimal (43.5)</p>

<p>Gene Wirchenko reports a fractional building number: 1313 1/2 Railroad Ave Bellingham WA 98225-4729</p></li>
<li><p><strong>Street names don't recurr in the same city</strong></p>

<p><a href="https://maps.google.co.uk/maps?q=from:W3+6LJ+to:W5+5DB+to:N8+7PB+to:SE25+6EP+to:E13+0AJ+to:E17+7LD+to:NW10+4LX+to:N1+9TR+to:E1+6PG+to:NW1+0JH+to:W14+8NL+to:SE13+6AD+to:SW19+5DX+to:E11+2AJ+to:SW19+2AE+to:E6+2HJ+&amp;saddr=W3+6LJ&amp;daddr=W5+5DB+to:N8+7PB+to:SE25+6EP+to:E13+0AJ+to:E17+7LD+to:NW10+4LX+to:N1+9TR+to:E1+6PG+to:NW1+0JH+to:W14+8NL+to:SE13+6AD+to:SW19+5DX+to:E11+2AJ+to:SW19+2AE+to:E6+2HJ">Here's a map of the following addresses:</a></p>

<ul>
<li>High Street, London, W3 6LJ</li>
<li>High Street, London, W5 5DB</li>
<li>High Street, London, N8 7PB</li>
<li>High Street, London, SE25 6EP</li>
<li>High Street, London, E13 0AJ</li>
<li>High Street, London, E17 7LD</li>
<li>High Street, London, NW10 4LX</li>
<li>Islington High Street, London, N1 9TR</li>
<li>Shoreditch High Street, London, E1 6PG</li>
<li>Camden High Street, London, NW1 0JH</li>
<li>Kensington High Street, London, W14 8NL</li>
<li>Lewisham High Street, London, SE13 6AD</li>
<li>High Street Wimbledon, London, SW19 5DX</li>
<li>High Street Wanstead, London, E11 2AJ</li>
<li>High Street Colliers Wood, London, SW19 2AE</li>
<li>High Street North, London, E6 2HJ </li>
</ul></li>
<li><p><strong>But street names don't recurr in close proximity</strong></p>

<p>Julian Fleischer provides an example from Bocholt in Germany showing several roads in close proximity all called <a href="https://maps.google.com/maps?q=51.853945,6.615334">Up de Welle</a>.</p></li>
<li><p><strong>An address will be comprised of road names</strong></p>

<p>Kirk Kerekes spent several years using an address of the form "2 mi N then 3 mi W of Jennings, OK 74038" which regularly got successful deliveries. Mike Riley used to mail the Very Large Array radio telescope at "50 miles (80 km) West of Socorro, New Mexico, USA"</p>

<p>Sam pointed me to <a href="http://www.menomoneefallsnow.com/news/99857214.html">Menomonee Falls</a> where houses are addressed using Milwaukee County's grid system instead of house numbers - giving addresses like N88 W16541 Foobar St.</p>

<p>Andy Monat sent the following address example, from a <a href="http://ciapa.tulane.edu/uploads/1_EE_2012_Acceptance_Packet_INFORMATION-1340749206.pdf">semester abroad program at Tulane University </a>: CIAPA, 50 meters north of the Hypermas/Walmart of Curridabat, San Jose, Costa Rica. Adrien Piérard and Luke Allardyce point out street names are seldom used in Japan - instead, districts and blocks and lot numbers are used (more info on the <a href="https://en.wikipedia.org/wiki/Japanese_addressing_system">Wikipedia entry for the Japanese addressing system</a>).  A <a href="http://www.worldpress.org/Americas/592.cfm">2002 World Press Review report</a> gave this sample address: From where the Chinese restaurant used to be, two blocks down, half a block toward the lake, next door to the house where the yellow car is parked, Managua, Nicaragua. Shaun Crampton sent <a href="https://vianica.com/nicaragua/practical-info/14-addresses.html">an article with more details and examples of the Nicaraguan system</a>. Stig Brautaset pointed out <a href="http://www.bbc.co.uk/news/magazine-14806350">a BBC article about post in Kabul</a> gives this example: "Hamid Jaan, behind Darul-Aman palace". Nathan Fellman reports similar addressing is used in Nicaragua and Costa Rica.</p>

<p>Paul Puschmann and Tibor Schütz pointed out the city of <a href="http://de.wikipedia.org/wiki/Quadratestadt">Mannheim in Germany is sometimes called Quadratestadt (City of Squares)</a> as the city centre is arranged in a grid, with blocks assigned a letter (along the north-south axis) and a number (along the east-west axis) then buildings numbered by block number. So an example address at numbers 6 to 13 on block R 5 would be: Institut für Deutsche Sprache, R 5, 6-13, D-68161 Mannheim </p>

<p>Leoni Lubbinge gives an example of a South African address: Part 84, Strydfontein 306 JR, Pretoria which means the 84th plot of the farm Strydfontein 306 JR.</p></li>
</ul>

<!-- Elements being present or absent -->

<ul>
<li><p><strong>A road will have a name</strong></p>

<p>Plenty of roads like driveways, onramps and the aisles of carparks don't have names. Some roads in Japan also don't have names, as <a href="https://en.wikipedia.org/wiki/Japanese_addressing_system">the prevalent addressing system works on districts, subdistricts, blocks, lots and lot numbers</a>.</p>

<p>Peter Kenway points out in America some homes are addressed as Rural Routes, where numbers are allocated to boxes on a route covering multiple roads. For example: Box 1234, R.R. 1, Winthrop, ME 04364.</p></li>
<li><p><strong>A road will only have one name</strong></p>

<p>Many different roads, from Goswell Road in London to Regent Road in Edinburgh, make up the 410 mile <a href="https://en.wikipedia.org/wiki/A1_road_%28Great_Britain%29">A1</a>. And while there may only be one "1 Goswell Road" and only one "1 Regent Road" there are multiple buildings numbered 1 on the road designated A1.</p>

<p>Roads may also be named in multiple languages. For example, in Ireland roads may be named in both English and Irish</p></li>
<li><p><strong>Addresses will only have one street</strong></p>

<p>The Royal Mail have what they call a 'dependent street' - for example: 6 Elm Avenue, Runcorn Road, Birmingham, B12 8QX, United Kingdom (Runcorn Road is the street, Elm Avenue is the stubby 'dependent street' and isn't unique within the city. <a href="http://maps.google.co.uk/maps?q=B12+8QX">Google Maps</a> )</p>

<p>Another counterexample: Rogue Hair, 1 Hopton Parade, Streatham High Road, London, SW16 …</p></li></ul></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.mjt.me.uk/posts/falsehoods-programmers-believe-about-addresses/">https://www.mjt.me.uk/posts/falsehoods-programmers-believe-about-addresses/</a></em></p>]]>
            </description>
            <link>https://www.mjt.me.uk/posts/falsehoods-programmers-believe-about-addresses/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24926417</guid>
            <pubDate>Thu, 29 Oct 2020 02:25:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I Violated a Code of Conduct]]>
            </title>
            <description>
<![CDATA[
Score 1157 | Comments 879 (<a href="https://news.ycombinator.com/item?id=24926214">thread link</a>) | @tosh
<br/>
October 28, 2020 | https://www.fast.ai/2020/10/28/code-of-conduct/ | <a href="https://web.archive.org/web/*/https://www.fast.ai/2020/10/28/code-of-conduct/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div>

<p><span>Written: 28 Oct 2020 by <i>Jeremy Howard</i></span></p><blockquote>
<p><em>Update Oct 20, 2020</em>: NumFOCUS <a href="https://numfocus.org/blog/jeremy-howard-apology">has apologized</a> to me. I accept their apology. I do not accept their assertion that “At the time of the interview, the committee had not determined that there was a violation of the code of conduct, only that there were two complaints filed and being examined.” The email to set up the call said “We would like to schedule a meeting so that we can discuss the results of our investigation with you” - nothing further. During the call, the committee stated the list of violations, and said “that is what the reporters stated, and what we found”. I asked why they didn’t take a statement from me before that finding, and they said “we all watched the video, so we could see for ourselves the violation”. The committee offered in their apology email to me to have a follow-up discussion, and I declined the offer.</p>
</blockquote>
<blockquote>
<p>Summary: NumFOCUS found I violated their Code of Conduct (CoC) at JupyterCon because my talk was not “kind”, because I said Joel Grus was “wrong” regarding his opinion that Jupyter Notebook is not a good software development environment. Joel (who I greatly respect, and consider an asset to the data science community) was not involved in NumFOCUS’s action, was not told about it, and did not support it. NumFOCUS did not follow their own enforcement procedure and violated their own CoC, left me hanging for over a week not even knowing what I was accused of, and did not give me an opportunity to provide input before concluding their investigation. I repeatedly told their committee that my emotional resilience was low at the moment due to medical issues, which they laughed about and ignored, as I tried (unsuccessfully) to hold back tears. The process has left me shattered, and I won’t be able to accept any speaking requests for the foreseeable future. I support the thoughtful enforcement of Code of Conducts to address sexist, racist, and harassing behavior, but that is not what happened in this case.</p>
</blockquote>
<h2 id="overview">Overview</h2>
<p>In my recent JupyterCon keynote, “I Like Jupyter Notebooks” (re-recording provided at the bottom of this post, if you’re interested in seeing it for yourself), I sought to offer a rebuttal to Joel Grus’ highly influential JupyterCon presentation “<a href="https://www.youtube.com/watch?v=7jiPeIFXb6U">I Don’t Like Notebooks</a>”. Joel claimed in his talk that Jupyter is a poor choice for software development and teaching, and I claimed in my talk that it is a good choice. The NumFOCUS committee found me guilty of violating their code of conduct for having not been “kind” in my disagreement with Joel, and for “insulting” him. The specific reasons given were that:</p>
<ul>
<li>I said that Joel Grus was “wrong”</li>
<li>I used some of his slides (properly attributed) and a brief clip from one of his videos to explain why I thought he was wrong</li>
<li>That I made “a negative reference” to his prior talk</li>
<li>I was also told that “as a keynote speaker” I would “be held to a higher standard than others” (although this was not communicated to me prior to my talk, nor what that higher standard is)</li>
</ul>
<p>Code of Conducts can be a useful tool, when thoughtfully created and thoughtfully enforced, to address sexism, racism, and harassment, all of which have been problems at tech conferences. Given the <a href="https://medium.com/tech-diversity-files/if-you-think-women-in-tech-is-just-a-pipeline-problem-you-haven-t-been-paying-attention-cb7a2073b996">diversity issues in the tech industry</a>, it is important that we continue the work of making conferences more inclusive, particularly to those from marginalized backgrounds. Having a code of conduct with explicit rules against violent threats, unwelcome sexual attention, repeated harassment, sexually explicit pictures, and other harmful behavior is the first step towards addressing and stopping those behaviors. The JupyterCon code provides the following examples of unacceptable behavior, none of which are at all similar to what I did (i.e. saying that someone was wrong on a technical topic, and explaining how and why):</p>
<ul>
<li>Violent threats or violent language directed against another person</li>
<li>Discriminatory jokes and language</li>
<li>Posting sexually explicit or violent material</li>
<li>Posting (or threatening to post) other people’s personally identifying information (“doxing”)</li>
<li>Personal insults, especially those using racist or sexist terms</li>
<li>Unwelcome sexual attention</li>
<li>Advocating for, or encouraging, any of the above behavior</li>
<li>Repeated harassment of others. In general, if someone asks you to stop, then stop</li>
</ul>
<p>My experience with the NumFOCUS code of conduct raises a few key issues:</p>
<ul>
<li>The CoC enforcement process involved conflicting &amp; changing information, no opportunity for me to give input, the stress of a long wait of unknown duration with no information about what I was accused of or what would happen next, and the committee members violated their own CoC during the process</li>
<li>There were two totally different Codes of Conduct with different requirements linked in different places</li>
<li>I was held to a different, undocumented and uncommunicated standard</li>
<li>The existence of, or details about, the CoC were not communicated prior to confirmation of the engagement</li>
<li>CoC experts recommend avoiding requirements of politeness or other forms of “proper” behavior, but should focus on a specific list of unacceptable behaviors. The JupyterCon CoC, however, is nearly entirely a list of “proper” behaviors (such as “Be welcoming”, “Be considerate”, and “Be friendly”) that are vaguely defined</li>
<li>CoC experts recommend using a CoC that focuses on a list of unacceptable behaviors. Both the codes linked to JupyterCon have such a link, and none of the unacceptable behavior examples are in any way related or close to what happened in this case. But NumFOCUS nonetheless found me in violation.</li>
</ul>
<p>I would rather not have to write this post at all. However I know that people will ask about why my talk isn’t available on the JupyterCon site, so I felt that I should explain exactly what happened. In particular, I was concerned that if only partial information became available, the anti-CoC crowd might jump on this as an example of problems with codes of conduct more generally, or might point at this as part of “cancel culture” (a concept I vehemently disagree with, since what is referred to as “cancellation” is often just “facing consequences”). Finally, I found that being on the “other side” of a code of conduct issue gave me additional insights into the process, and that it’s important that I should share those insights to help the community in the future.</p>
<h2 id="details">Details</h2>
<p>The rest of this post is a fairly detailed account of what happened, for those that are interested.</p>
<h3 id="my-talk-at-jupytercon">My talk at JupyterCon</h3>
<p>I recently gave a talk at <a href="https://jupytercon.com/">JupyterCon</a>. My partner Rachel gave a <a href="https://www.youtube.com/watch?v=frc7FgheUj4">talk at JupyterCon</a> a couple of years ago, and had a wonderful experience, and I’m a huge fan of Jupyter, so I wanted to support the project. The conference used to be organized by O’Reilly, who have always done a wonderful job of conferences I’ve attended, but this year the conference was instead handled by <a href="https://numfocus.org/">NumFOCUS</a>.</p>
<p>For my talk, I decided to focus on Jupyter as a literate and <a href="https://www.fast.ai/2019/12/02/nbdev/">exploratory programming environment</a>, using <a href="https://nbdev.fast.ai/">nbdev</a>. One challenge, however, is that two years earlier Joel Grus had given a brilliant presentation called <a href="https://www.youtube.com/watch?v=7jiPeIFXb6U">I Don’t Like Notebooks</a> which had been so compelling that I have found it nearly impossible to talk about programming in Jupyter without being told “you should watch this talk which explains why programming in Jupyter is a terrible idea”.</p>
<p>Joel opened and closed his presentation with some light-hearted digs at me, since I’d asked him ahead of time <em>not</em> to do such a presentation. So I thought I’d kill two birds with one stone, and take the opportunity to respond directly to him. Not only was his presentation brilliant, but his slides were hilarious, so I decided to directly parody his talk by using (with full credit of course) some of his slides directly. That way people that hadn’t seen his talk could both get to enjoy the fantastic content, and also understand just what I was responding to. For instance, here’s how Joel illustrated the challenge of running cells in the right order:</p>
<figure>
<img srcset="https://www.fast.ai/images/numfocus/joel-order.png 2w" sizes="1px" src="https://www.fast.ai/images/numfocus/joel-order.png">
</figure>
<p>I showed that slide, explaining that it’s Joel’s take on the issue, and then followed up with a slide showing how easy it actually is to run all cells in order:</p>
<figure>
<img srcset="https://www.fast.ai/images/numfocus/jeremy-order.png 2w" sizes="1px" src="https://www.fast.ai/images/numfocus/jeremy-order.png">
</figure>
<p>Every slide included a snippet from Joel’s title slide, which, I explained, showed which slides were directly taken from his presentation. I was careful to ensure I did not modify any of his slides in any way. When first introducing his presentation, I described Joel as “a brilliant communicator, really funny, and wrong”. I didn’t make any other comments about Joel (although, for the record, I think he’s awesome, and highly recommend <a href="https://www.amazon.com/Data-Science-Scratch-Principles-Python/dp/1492041130">his book</a>.</p>
<h3 id="the-code-of-conduct-violation-notice">The Code of Conduct violation notice</h3>
<p>A week later, I received an email telling me that two CoC reports were filed regarding my JupyterCon keynote presentation. I was told that “The Code of Conduct Enforcement Team is meeting tomorrow to review the incident and will be contacting you to inform you of the nature of the report and to understand your perspective”.</p>
<p>The CoC wasn’t mentioned at all until after I’d been invited to speak, had accepted, and had completed the online registration. I had reviewed it at that time, and had been a bit confused. The email I received linked to a <a href="https://jupytercon.com/codeofconduct/">JupyterCon Code of Conduct</a>, but that in turn didn’t provide much detail about what is and isn’t OK, and that in turn linked to a different <a href="https://numfocus.org/code-of-conduct">NumFOCUS Code of Conduct</a>. A link was also provided to <a href="https://numfocus.typeform.com/to/ynjGdT">report violations</a>, which also linked to and named the NumFOCUS CoC.</p>
<p>I was concerned that I had done something which might be viewed as a violation, and looked forward to hearing about the nature of the report and having a chance to share my perspective. I was heartened that JupyterCon documented that they follow the <a href="https://numfocus.org/code-of-conduct/response-and-enforcement-events-meetups">NumFOCUS Enforcement Manual</a>. I was also heartened that the manual has a section “Communicate with the Reported Person about the Incident” which says they will “Let the reported person tell someone on the CoC response team their side of the story; the person who receives their side of the story should be prepared to …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.fast.ai/2020/10/28/code-of-conduct/">https://www.fast.ai/2020/10/28/code-of-conduct/</a></em></p>]]>
            </description>
            <link>https://www.fast.ai/2020/10/28/code-of-conduct/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24926214</guid>
            <pubDate>Thu, 29 Oct 2020 01:47:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How I used GPT-3 to hit Hacker News front page 5 times in 3 weeks]]>
            </title>
            <description>
<![CDATA[
Score 21 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24924035">thread link</a>) | @masterrr
<br/>
October 28, 2020 | https://vasilishynkarenka.com/gpt-3/ | <a href="https://web.archive.org/web/*/https://vasilishynkarenka.com/gpt-3/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://vasilishynkarenka.com/content/images/size/w300/2020/10/Screen-Shot-2020-10-28-at-8.24.48-AM.png 300w,
                            https://vasilishynkarenka.com/content/images/size/w600/2020/10/Screen-Shot-2020-10-28-at-8.24.48-AM.png 600w,
                            https://vasilishynkarenka.com/content/images/size/w1000/2020/10/Screen-Shot-2020-10-28-at-8.24.48-AM.png 1000w,
                            https://vasilishynkarenka.com/content/images/size/w2000/2020/10/Screen-Shot-2020-10-28-at-8.24.48-AM.png 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://vasilishynkarenka.com/content/images/size/w2000/2020/10/Screen-Shot-2020-10-28-at-8.24.48-AM.png" alt="How I used GPT-3 to hit Hacker News front page 5 times in 3 weeks">
            </figure>

            <section>
                <div>
                    <p>Posting on Hacker News is like a box of chocolates – you never know what you’re gonna get. I’ve been submitting since 2017 and never had more than one point. So I stopped trying.</p><p>A month ago, I got access to OpenAI API. After ten minutes of tinkering, I’ve got an idea: what if I could make it write good Hacker News titles for my blog posts? I quickly looked up the most favorited posts, designed the prompt in plain English, and generated my first title. It looked weird. I even doubted for a few minutes if it’s worth sharing. But I was curious – so I closed my eyes and hit submit.</p><p>The post went to the moon with <a href="https://news.ycombinator.com/item?id=24547098">229 points and 126 comments</a> in one day. Fascinated, I continued generating titles (and what would you do?). In three weeks, I got to the front page five times, received 1054 upvotes, and had 37k people come to my site.</p><p>Below is everything I’ve learned building a Hacker News post titles generator with OpenAI API, designing GPT-3 prompts, and figuring out how to apply GPT-3 to problems ranging from sales emails to SEO-optimized blog posts. At the end of the post, I cover the broader implications of GPT-3 that became obvious only after a month of working with the tool. If you’re an entrepreneur or an investor who wants to understand the change this tech will drive, you can read my speculations there.</p><p>If you have no idea what I’m talking about, read more about GPT-3 in <a href="https://www.gwern.net/newsletter/2020/05">Gwern’s post</a> first, or check <a href="https://beta.openai.com/">OpenAI’s website</a> with demo videos. In my work, I assume you’re already familiar with the API on some basic level.</p><p><em>Oct 31 update: After I published the post, 48 people asked how to apply GPT-3 to their problems. To help them get started with the OpenAI API, I started building the first GPT-3 course that covers everything I learned – from use cases to prompt design. If you’re interested, </em><a href="mailto:vasilishynkarenka@gmail.com?subject=GPT-3%20course"><em>email me here</em></a><em>.</em></p><hr><p>After I got an idea for an HN titles app, I needed to understand how to do it with GPT-3. As there were no tutorials on approaching the problem, I went to the playground and began experimenting.</p><h2 id="generating-new-titles">Generating new titles</h2><h3 id="1-finding-the-data">1. Finding the data</h3><p>First, I wanted to see if I could make GPT-3 generate engaging post titles at all. To do that, I needed two things:</p><ol><li>Write a prompt that concisely describes the problem that I want to solve.</li><li>Feed the API some sample data to stimulate the completion.</li></ol><p>I went ahead and searched for the most upvoted HN posts of all time. In a few minutes, I’ve got <a href="https://hn.algolia.com/?query=&amp;sort=byPopularity&amp;prefix&amp;page=0&amp;dateRange=all&amp;type=story">an Algolia page</a> with a list of links. But after skimming through them, I figured out that upvotes wouldn’t work. They are mostly news and poorly reflect what kind of content the community values.</p><figure><img src="https://vasilishynkarenka.com/content/images/2020/10/Screen-Shot-2020-10-28-at-9.18.27-AM.png" alt="" srcset="https://vasilishynkarenka.com/content/images/size/w600/2020/10/Screen-Shot-2020-10-28-at-9.18.27-AM.png 600w, https://vasilishynkarenka.com/content/images/size/w1000/2020/10/Screen-Shot-2020-10-28-at-9.18.27-AM.png 1000w, https://vasilishynkarenka.com/content/images/size/w1600/2020/10/Screen-Shot-2020-10-28-at-9.18.27-AM.png 1600w, https://vasilishynkarenka.com/content/images/size/w2400/2020/10/Screen-Shot-2020-10-28-at-9.18.27-AM.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><a href="https://hn.algolia.com/?query=&amp;sort=byPopularity&amp;prefix&amp;page=0&amp;dateRange=all&amp;type=story">Most upvoted HN posts of all time.</a></figcaption></figure><p>Disappointed, I discarded upvotes as a metric. I needed something that would describe the value people get from the post. Something like... bookmarks?</p><p>I quickly looked up <a href="https://news.ycombinator.com/item?id=24351073">the most favorited HN posts</a>. The idea was simple: people don’t bookmark news. They favorite things they want to explore later.</p><figure><img src="https://vasilishynkarenka.com/content/images/2020/10/Screen-Shot-2020-10-28-at-9.21.11-AM.png" alt="" srcset="https://vasilishynkarenka.com/content/images/size/w600/2020/10/Screen-Shot-2020-10-28-at-9.21.11-AM.png 600w, https://vasilishynkarenka.com/content/images/size/w1000/2020/10/Screen-Shot-2020-10-28-at-9.21.11-AM.png 1000w, https://vasilishynkarenka.com/content/images/size/w1600/2020/10/Screen-Shot-2020-10-28-at-9.21.11-AM.png 1600w, https://vasilishynkarenka.com/content/images/size/w2400/2020/10/Screen-Shot-2020-10-28-at-9.21.11-AM.png 2400w" sizes="(min-width: 720px) 720px"><figcaption><a href="https://observablehq.com/@tomlarkworthy/hacker-favourites-analysis">Most favorited HN posts of all time.</a></figcaption></figure><p>The next step was to grab the data from the list, insert it into the playground, and write a clear and concise prompt.</p><p><em>I actually grab data from the <a href="https://news.ycombinator.com/item?id=24351073">dang’s comment</a> rather than the original post; his list was a global one.</em></p><h3 id="2-designing-a-prompt">2. Designing a prompt</h3><p>The best way to program the API is to write a direct and straightforward task description, as you would do if you were delegating this problem to a human assistant. </p><p>Here’s how my first prompt looked like:</p><blockquote>Generate viral titles for Hacker News (<a href="https://news.ycombinator.com/">https://news.ycombinator.com/</a>) posts. The titles have to be provocative and incentivize users to click on them.</blockquote><p>Data-wise, I needed to clean up the list slightly, get rid of irrelevant stuff like IDs, and choose up to five titles to use as a sample – OpenAI team suggests that selecting three to five titles works best for text generation. If you feed the API more examples, it picks up wrong intents and generates irrelevant completions.</p><p>In a few minutes of Google Sheets work, the cleanup was done, and I had a data set of the most favorited HN post titles of all time. I put together my first prompt and clicked “Generate.”</p><figure><img src="https://vasilishynkarenka.com/content/images/2020/10/Screen-Shot-2020-10-28-at-9.29.16-AM.png" alt="" srcset="https://vasilishynkarenka.com/content/images/size/w600/2020/10/Screen-Shot-2020-10-28-at-9.29.16-AM.png 600w, https://vasilishynkarenka.com/content/images/size/w1000/2020/10/Screen-Shot-2020-10-28-at-9.29.16-AM.png 1000w, https://vasilishynkarenka.com/content/images/size/w1600/2020/10/Screen-Shot-2020-10-28-at-9.29.16-AM.png 1600w, https://vasilishynkarenka.com/content/images/size/w2400/2020/10/Screen-Shot-2020-10-28-at-9.29.16-AM.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Cleaned up titles of most favorited HN posts of all time.</figcaption></figure><h3 id="3-tinkering-with-completions">3. Tinkering with completions</h3><p>The first completion was unpromising. The list of titles had too many Ask HNs, and GPT-3 picked up questions as a pattern:</p><figure><img src="https://vasilishynkarenka.com/content/images/2020/10/Screen-Shot-2020-10-28-at-9.40.59-AM.png" alt="" srcset="https://vasilishynkarenka.com/content/images/size/w600/2020/10/Screen-Shot-2020-10-28-at-9.40.59-AM.png 600w, https://vasilishynkarenka.com/content/images/size/w1000/2020/10/Screen-Shot-2020-10-28-at-9.40.59-AM.png 1000w, https://vasilishynkarenka.com/content/images/size/w1600/2020/10/Screen-Shot-2020-10-28-at-9.40.59-AM.png 1600w, https://vasilishynkarenka.com/content/images/size/w2400/2020/10/Screen-Shot-2020-10-28-at-9.40.59-AM.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>My first attempt to generate HN titles.</figcaption></figure><p>To fix that, I cut out half Ask HNs from the dataset and began tinkering.</p><p>If there was one thing I could tell someone about GPT-3, it’d be that getting a perfect completion from the first try is a dream. The best stuff comes after dozens of experiments, sometimes after generating completions with the same parameters many times with the Best Of parameter and writing another GPT-3 classifier to discover a good one. Moreover, you need to test the prompt’s quality, data samples, and temperature (“creativity” of responses) individually to understand what you need to improve. </p><p><em>If you’re looking for prompt design tips, head on to chapter 2 of the post.</em></p><p>Here’s a list of experiments I’ve done:</p><ul><li>Edited the prompt many times, including and excluding adjectives from the task description. I tried “catchy,” “provocative,” “thought-provoking,” and many others. The best configuration I’ve got was “Write a short, thought-provoking, and eye-catching post title for a Hacker News (https://news.ycombinator.com/) submission.”</li><li>Split the prompt into two sentences, separating the task and its description. Discovered that one-sentence long prompts work best for simple tasks.</li><li>Played with data samples. I added and removed Ask HNs, randomly sampled from the list of most favorited posts, and tried picking more subjectively thoughtful titles.</li><li>Changed the temperature. The best results came at .9, while anything less than .7 was repetitive and very similar to the samples. Titles generated with temperature 1 were too random and didn’t look like good HN titles at all.</li></ul><p>To judge the quality of completions, I’ve come up with a question: “If I saw this on HN, would I click on it?” This helped me move quickly through experiments because I knew what bad results looked like.</p><p>After half an hour of tinkering, I’ve got the following completion: &nbsp; &nbsp; &nbsp;</p><figure><img src="https://vasilishynkarenka.com/content/images/2020/10/Screen-Shot-2020-10-28-at-9.54.17-AM.png" alt="" srcset="https://vasilishynkarenka.com/content/images/size/w600/2020/10/Screen-Shot-2020-10-28-at-9.54.17-AM.png 600w, https://vasilishynkarenka.com/content/images/size/w1000/2020/10/Screen-Shot-2020-10-28-at-9.54.17-AM.png 1000w, https://vasilishynkarenka.com/content/images/size/w1600/2020/10/Screen-Shot-2020-10-28-at-9.54.17-AM.png 1600w, https://vasilishynkarenka.com/content/images/size/w2400/2020/10/Screen-Shot-2020-10-28-at-9.54.17-AM.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>My first good GPT-3 completions for HN titles.</figcaption></figure><p>That’s when I realized that I’m onto something. From the list of generated titles above, I’d click on at least three links just out of curiosity. Especially on “A developer’s guide to getting in shape.”</p><p>What’s even more interesting, the completion above was a result of fine-tuning the API. Title8, What You Love Is Not Your Life’s Work, was originally a part of another completion that was lame. So I cut out the bad stuff, added Title8 to my data sample, and continued generating from there.</p><p>The next step was to see if I could make GPT-3 create a custom title for my blog post.</p><h2 id="generating-custom-titles">Generating custom titles</h2><h3 id="1-changing-the-method">1. Changing the method</h3><p>To make GPT-3 generate custom titles, I needed to change my approach. I was no longer exploring new, potentially interesting headlines but figuring out how to make a good one for a post that was already written. To do that, I couldn’t just tell the API, “hey, generate me a good one.” I needed to show what a good one actually is and give GPT-3 some idea of what the post is about.</p><p>The first thing I changed is the prompt. This was relatively easy because I applied the same model of thinking again – “What would I tell a human assistant if I had to delegate this problem?”</p><p>Here’s a prompt that I used:</p><blockquote>Write a short, thought-provoking, and eye-catching post title for Hacker News (https://news.ycombinator.com/) submission based on a blog post’s provided description.</blockquote><p>One unexpected benefit from tinkering with the prompt is the clarity of thought. When you’re dealing with GPT-3, there’s no way to make it work if you don’t know what you’re doing. You need to express your thought in clear terms. It’s a brilliant natural constraint and one of the most underrated benefits of GPT-3: when you must state clearly what you want to do, you begin to better understand the problem.</p><h3 id="2-updating-the-data">2. Updating the data</h3><p>After I designed a new prompt, I had to update my data set. I went back to my previous experiment, selected five titles from the list of most favorited posts of all time, and wrote a clear and concise description for each of them. Lastly, I wrote a description of my own work that I wanted to generate the title for. I left the title field blank and hit “Generate.”</p><figure><img src="https://vasilishynkarenka.com/content/images/2020/10/Screen-Shot-2020-10-28-at-10.35.59-AM.png" alt="" srcset="https://vasilishynkarenka.com/content/images/size/w600/2020/10/Screen-Shot-2020-10-28-at-10.35.59-AM.png 600w, https://vasilishynkarenka.com/content/images/size/w1000/2020/10/Screen-Shot-2020-10-28-at-10.35.59-AM.png 1000w, https://vasilishynkarenka.com/content/images/size/w1600/2020/10/Screen-Shot-2020-10-28-at-10.35.59-AM.png 1600w, https://vasilishynkarenka.com/content/images/size/w2400/2020/10/Screen-Shot-2020-10-28-at-10.35.59-AM.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Updated data set for generating custom HN titles.</figcaption></figure><h3 id="3-experimenting-with-titles">3. Experimenting with titles</h3><p>This time, the completions were shockingly good. Because I’ve already learned how to design a prompt and get the data right, I didn’t have to spend much time tuning the parameters. The only thing I changed was the temperature – the best results came at .92.</p><p>Here’s a list of titles I generated for my post on <a href="https://vasilishynkarenka.com/3-weeks-of-shaolin-kung-fu-in-china/">studying kung fu in China</a>:</p><ul><li>How a Shaolin monk trains Western students</li><li>I went to a Kung Fu school for a month, and I learned 3 lessons that I will never forget</li><li>How I got a Shaolin monk to teach me Kung Fu in China</li><li>What it’s like to study Kung Fu from a Shaolin monk</li><li>21 days in the Shaolin Temple</li><li>An edited transcript of my conversation with my Kung Fu master</li><li>How I learned Kung Fu from a monk in China: The full story</li><li>I Spent a Month with a Kung Fu Master in China</li><li>The Shaolin warrior in the cubicle</li><li>Kamikaze training – A brief study in Kung Fu</li><li>Why I traveled to China to study Kung Fu</li><li>I went to a Shaolin Monastery for 6 months to learn Kung Fu: a true story</li><li>I went to the Shaolin Temple to study Kung Fu, and all I got was this lousy blog post </li><li>The monk and the engineer</li><li>What it’s like studying Kung Fu from Shaolin monks for 6 months</li><li>I went to Shaolin and all I got were Kung Fu lessons</li></ul><p>And, of course, my all-time favorite one:</p><blockquote>When a Shaolin monk met a programmer from the Silicon Valley</blockquote><p>But the best titles didn’t come from completions. The most interesting headlines were the ones I ended up thinking of <em>after</em> I saw the …</p></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://vasilishynkarenka.com/gpt-3/">https://vasilishynkarenka.com/gpt-3/</a></em></p>]]>
            </description>
            <link>https://vasilishynkarenka.com/gpt-3/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24924035</guid>
            <pubDate>Wed, 28 Oct 2020 20:55:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Building a Functional Email Server]]>
            </title>
            <description>
<![CDATA[
Score 27 | Comments 1 (<a href="https://news.ycombinator.com/item?id=24923242">thread link</a>) | @panic
<br/>
October 28, 2020 | https://signalsandthreads.com/building-a-functional-email-server/ | <a href="https://web.archive.org/web/*/https://signalsandthreads.com/building-a-functional-email-server/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<h2 id="000004">00:00:04</h2>

<div><p>Welcome to Signals and Threads, in-depth conversations about every layer of the tech stack from Jane Street. I’m Ron Minsky. </p><p> All right, so, it’s my pleasure today to sit down and have a conversation with Dominick LoBraico about email. In particular, we’re going to talk about a system that Dominick architected and led the development of called Mailcore, which is Jane Street’s own homegrown mail server. </p><p> And I think this is interesting on its own because email is an interesting topic and the whole architecture behind it, but I think it’s also a lens into some interesting questions about software design and how you manage infrastructure, some questions about how you make this choice of when you build your own thing and when you use standard, existing tools, and also some interesting questions about how programming language plays a role in systems design.</p></div>

<h2 id="000048">00:00:48</h2>

<p>Hi, Ron.</p>

<h2 id="000049">00:00:49</h2>

<p>Hey, DLo. So, to get started, can you tell us a little bit about how email works?</p>

<h2 id="000054">00:00:54</h2>

<div><p>Sure. Yeah. So email is based on an old and venerable protocol on the Internet called the Simple Mail Transfer Protocol, SMTP, and SMTP… you can kind of think of it as playing the role that the Postal Service plays in delivering regular mail. It is a way for one server that wants to deliver a message somewhere, to hand that message off to another party, who can get it to its final destination, whether that is the eventual destination server itself or some intermediary who can help you get a little bit closer.
</p><p>Email itself came into fruition, as we know today, in the early days of the Internet, and the protocol itself is very simple. You basically have the actual body of the message itself, which has its own separate format and specification, and then you have a set of instructions for expressing who that message is destined for and who it’s coming from, and so one server connects to another.
</p><p>And it says, “I’ve got a message. It’s coming from so and so, and it’s meant to be delivered to some other person. Here’s the body of the message,” and the receiving server can do with that what it will. It can either say, “Great, I’ll take that, and I’ll be responsible for it from here on out.” It can say, “No, I don’t know anything about that person. You have to find somebody else to deliver that to” or reject it for any number of other reasons, like, “This looks like it has a virus,” or “You’re not allowed to connect to me,” or “I’m not available for receiving mail right now.”</p></div>

<h2 id="000217">00:02:17</h2>

<p>And one thing that always strikes me about email is it’s this kind of wondrous artifact from the early Internet, which is a <em>truly</em> open social network. There’s lots of things that people talk about, right? Could we make existing social networks better and more open and all of that, and email, just <em>is</em> from its initial design; and its complete history has been this very open thing, and as you point out, the core protocols and transports are relatively simple, although there is actually a surprising amount of complexity in the RFCs that tell you how to parse a particular email. The overall system is pretty simple, but there’s a lot of complexity in all of the different players who build systems that actually manage and transfer email around and how they deal with the various problems that happen, like spam and people attacking systems via email and all of that. So the foundations are relatively simple, but the emergent complexity of the system is actually pretty high.</p>

<h2 id="000310">00:03:10</h2>

<p>Like with many protocols of the old Internet, it was designed in a time where the world was much simpler than it is today, especially the Internet-connected world. You know, there were probably 50 institutions that had Internet connections or ARPANET connections at the time, and you didn’t really have to worry that anybody was going to be spamming because barely anybody even know what email was in the first place.</p>

<h2 id="000330">00:03:30</h2>

<div><p>When you start and build a new thing, the early properties of the thing that you build can often be really sticky and really matter in a way that’s kind of hard to predict. So this one early property of being open has stayed there. Email is a thing that anyone can participate in. Organizations can build their own infrastructure to connect to it, and through all the rather large transformations that the email system has gone through, that openness remains as a core property.
</p><p>This is the horrible thing about designing to build a new thing,  when you want to design something new, you have to make a bunch of choices, and clearly, you shouldn’t worry about them that much, because probably the thing you build is going to fail and isn’t going to work out, and even if it does, you’re going to learn more about the problem later, and so you shouldn’t worry too much about the early decisions. But also, some of the early decisions, you don’t know which ones are going to turn out to be very hard to change.</p></div>

<h2 id="000413">00:04:13</h2>

<p>That’s right.</p>

<h2 id="000414">00:04:14</h2>

<p>And you’ll be stuck with them until the end of time.</p>

<h2 id="000416">00:04:16</h2>

<p>And in fact, you know, the big players in email today – obviously Google and Gmail, are a really large percentage of the email sending and receiving on the Internet – but they’re still wrestling with some of those early decisions and some of that openness that are architected in, as they try to figure out how they can make email more secure and how they can protect their users and rein in some of the malicious actors on the Internet, and that’s just a hard thing to do while trying to maintain the existing openness that email has; it cuts both ways I guess.</p>

<h2 id="000445">00:04:45</h2>

<p>That openness, in the end, has a lot of value.</p>

<h2 id="000446">00:04:46</h2>

<p>Absolutely. Yeah.</p>

<h2 id="000448">00:04:48</h2>

<p>So the story here is about how you ended up building the system called Mailcore. What did email at Jane Street look like when you first ran into the problem?</p>

<h2 id="000456">00:04:56</h2>

<div><p>So you might think that there’s really not much special about the way Jane Street uses email compared to any other company, and largely, that’s true. I think we have a few special requirements by dint of the fact that we are in a regulated industry, so we have some requirements around logging for compliance purposes every message that is sent or received by somebody at Jane Street.
</p><p>But other than that, our email system looks pretty similar, or has looked, in the past, pretty similar, to the way an email system in any organization might look, and the rough summary is we have some mail gateways that sit on the outside of our network for receiving email from foreign servers, you know, from external parties, and then we have some mail server, or a set of servers, inside of our network that handle all of the complicated business logic around what to do with those messages.
</p><p>So, in some cases, it’s as simple as receive the message and deliver it into the mailbox of the user if we are the intended recipient. In other cases, it is apply filtering for things like spam and viruses and other things that we might want to extract from messages before we deliver them, do expansion for mailing lists. So if you send an email to some group at Jane Street, you want to be able to expand that group name to the actual list of recipient mailboxes to make sure that it actually ends up in the inboxes of the recipients who it’s destined for. And then this extra compliance implication of making sure that we’re logging all of the right messages with all of the right metadata. </p><p>And at the time that I started, the mail infrastructure here was all based on an open source mail server that has its own config language and is pretty widely used on the Internet at large, and we had about 400 or 500 lines of configuration in the most complex case, I think, for this system to get it to do all of these different things that we wanted it to be able to do.</p></div>

<h2 id="000644">00:06:44</h2>

<p>Great. So that sounds like a reasonable approach in terms of how to build oneself a mail system. What problems did we run into with it?</p>

<h2 id="000650">00:06:50</h2>

<div><p>Yeah, so the biggest problem here, at the end of the day, was the complexity required for configuring this system to do all of the things that we needed it to do. So, now, I said 400 or 500 lines of configuration – that probably doesn’t sound like a huge number, but when it’s in a kind of bespoke configuration language that’s unlike the configuration of any other system and unlike any programming language that a developer or engineer at Jane Street would be familiar with, the complexity of 400 or 500 lines in a foreign language is pretty large and can be a little bit imposing to deal with.
</p><p>In particular, we had some scary near-misses where we realized that we had done the wrong thing in terms of archiving some email for compliance purposes that we were supposed to archive, and luckily, in each of those cases, there were mitigating factors such that it didn’t end up being a big deal, but that near-miss gave us a little bit of a scare because we went and looked at the configuration and wanted to understand how we had gotten ourselves into this position, and it was harder than it felt like it should be to understand what had gone wrong and how to fix it.</p></div>

<h2 id="000750">00:07:50</h2>

<p>It’s maybe also worth mentioning that the problem of logging all of your messages for compliance purposes may sound easy, but it’s made more complicated by the fact that Jane Street is a company that operates in lots of different regulatory regimes and has actually different rules for some of the different places it operates. So even the sort of seemingly simple, “Let’s just write everything down” is more complicated than it might appear at first.</p>

<h2 id="000811">00:08:11</h2>

<p>That’s right, yeah. We have different requirements in terms of what has to be written down and what kinds of metadata we need to store and where the extra copies need to be physically located around the world and things like that, which are reasonable sounding when you think about the human aspects of it, you know, when you reason about, okay, yeah, you need a copy for this and a …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://signalsandthreads.com/building-a-functional-email-server/">https://signalsandthreads.com/building-a-functional-email-server/</a></em></p>]]>
            </description>
            <link>https://signalsandthreads.com/building-a-functional-email-server/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24923242</guid>
            <pubDate>Wed, 28 Oct 2020 19:40:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Learnings from Running a Longevity Startup]]>
            </title>
            <description>
<![CDATA[
Score 38 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24923116">thread link</a>) | @sarthakjshetty
<br/>
October 28, 2020 | https://www.celinehh.com/year-1-learnings | <a href="https://web.archive.org/web/*/https://www.celinehh.com/year-1-learnings">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main" role="main">

      

        

        <div data-content-field="main-content">
          <div data-type="page" data-updated-on="1603911174701" id="page-5f98dba740e38c75e7b2c208"><div><div><div data-block-type="2" id="block-5f98dba740e38c75e7b2c209"><div><p>Oct 27, 2020</p><p>I incorporated Celevity in October 2019. Since then, we’ve gone from a deck and a provisional patent to a team of 12+, a lead drug, and a head brimming with new knowledge and experiences. These are some of my biggest learnings in the last year. </p><h3>Small decisions compound exponentially, especially around team &amp; culture</h3><p>One of the most important things I ever did for our culture was a mistake. The week before a key proof-of-concept study, I decided to completely rework the study design and re-question all of our assumptions. This delayed our timelines and made most of our previous work moot. It also saved the study. </p><p>That was a show not tell moment with our values - it showed the team that truth seeking is the most important thing, and no one has to be scared to tell me if they want to change up something last minute (because I did it myself!). This paved the way to many other close saves. </p><h3>For key relationships, make sure motivations are aligned</h3><p>From investors and advisors to contractors and employees, taking time to understand to fully empathize with their priorities and worries, and not automatically assuming that their motivations are the same as yours, was a hard lesson to embody. As a stereotypical bombastic founder, it was hard for me to comprehend someone <em>not </em>being mission-motivated and not wanting this in the world as badly as I do.</p><p>In hiring, I’ve found that even the most talented person will not perform well if they don’t deeply and truly care about your mission (at least at this stage of the company). In professional relationships, it’s unlikely their motivation is the same as yours, but often motivations can be complimentary. Everything is so much easier when they are. </p><h3>Most catchy startup advice has layers of nuance and conditionals that you wont realize until you’re in the thick of it</h3><p>I hit this one a lot this year, a likely consequence from listening to a few too many startup podcasts. A good example is ‘hire fast, fire fast’. Generally good advice, in my opinion. But no one tells you how difficult it is to execute on that advice, the accusations you’ll face from the person or others, the emotional toll of making a decision that affects someone’s wellbeing, and the ripple effects a firing, even if necessary, can have on the team and how to deal with that. Correct advice? Totally. But far from the full picture. One of my biggest learnings this year was becoming aware of and starting to learn how to predict the nuance and conditionals around things that seem simple from the outside. </p><h3>The line between optimism and naïveté is a wide as a hair </h3><p>To do novel hard things requires optimism that you have some key insight or ability that means you can achieve what others haven’t. You need to be optimistic to trudge through the sludge and to recruit others to your crazy idea long before it’s proven. Naive optimism is so important for building a company and telling a story. But you also need to be paranoid and see around corners. </p><p>Balancing these is difficult - too optimistic and you’ll fall into a trap, too paranoid and you’ll never make a decision. Over the last year, I’ve gained a healthy respect for the challenges ahead of us, moving my natural tendency of optimism a few inches closer to paranoia. I think this will serve us well on this next stage of our journey. </p><p>For me, the hardest part of this was realizing that the optimal balance between optimism and realism differs depending on the situation; what works when facing investors doesn’t work when recruiting, and these two situations differ significantly to the balance you need to hold internally. </p></div></div></div></div></div>
        </div>
      
    </div></div>]]>
            </description>
            <link>https://www.celinehh.com/year-1-learnings</link>
            <guid isPermaLink="false">hacker-news-small-sites-24923116</guid>
            <pubDate>Wed, 28 Oct 2020 19:31:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Transhumanism and Augmented Reality – A brave new augmented world]]>
            </title>
            <description>
<![CDATA[
Score 28 | Comments 13 (<a href="https://news.ycombinator.com/item?id=24922309">thread link</a>) | @brna
<br/>
October 28, 2020 | https://qaautomation.dev/a-brave-new-agumented-world/ | <a href="https://web.archive.org/web/*/https://qaautomation.dev/a-brave-new-agumented-world/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">
    <article>

        

        <section>
            <!--kg-card-begin: markdown--><h4 id="preface">Preface</h4>
<p>This post is a bundle of thoughts on the near future of Augmented Reality implementations, ways of integration, and introducing new layers of cognition. We are going to talk about AR as a way of enabling transhumanism by broadening the human cognitive and sensory toolbox.<br>
We are not going to be concerned with what is and what is not technologically possible at the moment.</p>
<h4 id="introduction">Introduction</h4>
<p>Have you ever taken a walk and thought about all the accessible information on animals, plants, objects, or even people? How much information is available on the internet? How many facts have you encountered and already forgotten?</p>
<p>Ancient Greeks put an active effort into training and maintaining their memorizing capabilities, and already at this stage of progress, human memory is shifting – or has shifted from remembering facts to remembering how and where to find facts. We are keeping less and less information hardcoded in our permanent memory and are in turn learning how to<br>
consume, process, and find useful data in an endless stream of information and misinformation.</p>
<p>A technologically proficient user has access to a vast volume and spectrum of data at his/her disposal, sitting right in the pocket. The human approach to holding knowledge is evolving as we speak. Heck, it started evolving with the invention of speech. But another significant<br>
change is on the cusp of happening. Our mobile phone interfaces, while serving all the information we need, are still detached from us, having considerable latency when we need to find, show, or input data. With the introduction of AR, AI, and human-computer interfaces, everything is about to change. AR or human augmentation in general will make that data<br>
more accessible and drastically lessen the latency needed for the interaction, making the tools feel more and more as part of ourselves.</p>
<p>Imagine walking down the street. You look around to observe people on the street and notifications start popping up alongside people’s heads. For one person the message says:<br>
"You have seen this person three times this week already"; for another one it states:<br>
"You have five mutual connections and common interests".<br>
Those notifications could also tell you if the person is willing to make new connections, based on previously shared data or facial microexpression analysis. Maybe we have gone too far down the rabbit hole for now, but you<br>
get the idea.</p>
<h3 id="definingarandtranshumanism">Defining AR and Transhumanism</h3>
<p>Merriam-Webster defines Augmented Reality as "an enhanced version of reality created by the use of technology to overlay digital information on an image of something being viewed through a device (such as a smartphone camera)". For the purposes of this post, I would like us not to limit reality to human vision alone.</p>
<p>Merriam-Webster has no definition for Transhumanism, but Wikipedia defines it as a philosophical movement that advocates for the transformation of the human condition by developing and making widely available sophisticated technologies to greatly enhance human intellect and physiology.</p>
<p>The two are getting more and more intertwined as technology advances, so I would like us to redefine AR, if only for this post, as "any technology that enhances or expands the human experience of reality", or maybe even as "practical transhumanism".</p>
<h3 id="augmentationtechnologymilestones">Augmentation technology milestones</h3>
<p>These are some freely set milestones for AR technology.</p>
<ul>
<li>LVL 0 – environment emulation via VR – for developmental fast-tracking</li>
<li>LVL 1 – external IO devices – headsets, glasses, gloves</li>
<li>LVL 2 – embedded IO devices – lenses, external neural signal readers</li>
<li>LVL 3 – tapping into existing sensory and neural networks</li>
<li>LVL 4 – registering new input/output devices in neural networks</li>
</ul>
<h3 id="degreesofaugmentation">Degrees of augmentation</h3>
<p>For the time being, and at least until we master the LVL 4 milestone described above, we are not creating new sensory or actuation systems, we are only hijacking the existing ones. Hi, Neuralink ;). This means that with every bit of information we gain, we are blocking the bandwidth and use for our existing IO mechanisms.In all fairness, people recovering mobility after a severe head trauma are evidence that the brain could possess enough plasticity to use whatever is thrown at it, given we connect the right dots, have enough time and practice. User age could also be a part of the equation.<br>
Sounds easier than it is, I know.</p>
<p>So, the more we augment our existing senses, the more we obstruct our basic physical-world data flow, the need to set some design restriction guidelines will grow. We still have accidents while using mobile phones or headphones, so the question arises: how could we manage a society full of people that overlap and fully cover their field of vision and hearing?<br>
Some safety rules integrated into the tech would be quite handy. Here is an example of defining degrees of augmentation:</p>
<ul>
<li>No augmentation</li>
<li>Info - can be an overlay, but should not block much sensory bandwidth</li>
<li>Safe communication – headset equivalent</li>
<li>Immersive experience – not safe for driving, safe for outdoors</li>
<li>Fully overlaying sensory input with enabled alerts – public safe zones</li>
<li>Fully overlaying sensory input – only for safe zones at home</li>
</ul>
<h3 id="augmentationusagecategories">Augmentation usage categories</h3>
<h4 id="knowledgemanagement">Knowledge management</h4>
<p>At some point, AR should enable access to data from the internet, which should be retrieved and presented without too much user effort, based on surrounding situations and context. Also, the useful data should be categorized and stored for retrieval at a later time.<br>
The interface should be able to combine relevant data and present solutions, statistics, and probabilities.</p>
<p>This could also enable saving verifiable event recordings, snapshots, and transcripts, real-time language translation, or seeing blueprints and instructions while fixing devices.</p>
<h4 id="extendingsensoryreach">Extending sensory reach</h4>
<p>The human sensory reach is already drastically extended with the use of IoT and the internet in general, but there is so much more of existing tech to be integrated into an AR system to extend human senses. Users could see and feel if their home or possessions are safe, or more generally, sense physical-world information streams in real-time.<br>
For example, physical-world data can be processed in parallel with the user experiencing the immediate surroundings. Thus, the user would, for instance, return home and detect certain objects have been moved, etc.</p>
<h4 id="extendingphysicalactuationactionreach">Extending physical actuation/action reach</h4>
<p>Besides our physical reach, most of us have already experienced controlling or affecting the physical-world at a distance. AR could help make the world around us feel more like an extension of our physical bodies.<br>
We could run automated processes upon visual triggers, i.e. unlock the front door without thinking, or control devices, drones, and robotics with ease.</p>
<h4 id="communicationandindividuality">Communication and individuality</h4>
<p>Communication could also be streamlined. We could have an instant connection with anyone around the world, almost as if standing side by side. Communication could play out on several levels, depending on the level of technological advancement, and could be:</p>
<ul>
<li>Restrained – messages and recordings with a delay and opportunity for curation</li>
<li>Unrestrained – conservative – real-time audio-visual communication</li>
<li>Unrestrained – progressive – direct thought transfer in a Neuralink-like brain-computer interface manner</li>
</ul>
<p>We could socialize, play games, or work like never before:<br>
imagine coding while sharing your thought process with a colleague, and also having an "AI" assisted IDE.</p>
<h4 id="extendingthecognitiontoolbox">Extending the cognition toolbox</h4>
<p>Given having a portable AR system and a smart enough underlying OS that replicates our surroundings and anticipates our needs OR an efficient enough way of selecting and using computational tools, we could:</p>
<ul>
<li>see object trajectories</li>
<li>have perfect math at our disposal</li>
<li>know objects, plant life, animals, etc.</li>
</ul>
<p>AI or General AI could, if available, also be used as an intermediary to the available cognition toolbox, to serve the necessary tools and results to the end-user.</p>
<h4 id="pastimeandentertainment">Pastime and entertainment</h4>
<p>I won’t go into great detail here, VR games are already immersive and fun, but try to imagine what they would be like if some freedom is added to the mix. As games and entertainment are presumably most likely to require as much immersion as possible, it will be necessary to consider immersion safety.</p>
<h5 id="vrgamesandapsfullyoverlayingsensoryinput">VR games and APS - fully overlaying sensory input</h5>
<p>Cars, stairs, sudden drops, and obstacles are here to stay, so for any device that can write over your sensory input, a way to risk mitigation must be in place.</p>
<h5 id="endusersafetyzonecalibration">End user safety zone calibration</h5>
<p>Imagine you want to play a full-on VR game somewhere. How could you stay safe and not trip over something or fall over some curb? It is simple, you could just walk around your playground to define its contours and limits. A certain amount of space can even be taken out of the outer limits of the playground, any obstacles are reproduced in-game so you can navigate past them. You start the game and play, and in case you move to the outer limits, the game simply starts to fade away.</p>
<h5 id="safesurfaces">Safe surfaces</h5>
<p>OK, but where else could you use 100% augmentation opacity in the real world? The first thing that comes to mind as safe would be walls. You can project stuff in front of almost any wall and be sure that nothing you can’t see will hit you from that direction, or at least as sure as you are now.</p>
<h5 id="contentkillswitch">Content killswitch</h5>
<p>Another useful feature would be to have the immersive content stop when quiet time or rest is needed, as well as if stress levels are critical.</p>
<h3 id="pitfallsofarandhumanaugmentation">Pitfalls of AR and Human augmentation</h3>
<ul>
<li>Having no signal or battery</li>
<li>Developing a dependence on AR in everyday life</li>
<li>Losing sense of the physical self</li>
<li>Losing individuality</li>
<li>Identifying the AR OS as a part of the personality</li>
</ul>
<h3 id="wecanworkondevelopingartoday">We can work on developing AR today</h3>
<p>I guess we are all well aware of the current limitations of the AR systems, but looking past the non-existing or low tech ways of input and output that we nowadays possess, by emulating non-existing augmentation pathways and …</p></section></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://qaautomation.dev/a-brave-new-agumented-world/">https://qaautomation.dev/a-brave-new-agumented-world/</a></em></p>]]>
            </description>
            <link>https://qaautomation.dev/a-brave-new-agumented-world/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24922309</guid>
            <pubDate>Wed, 28 Oct 2020 18:24:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Pyston v2: Faster Python]]>
            </title>
            <description>
<![CDATA[
Score 260 | Comments 206 (<a href="https://news.ycombinator.com/item?id=24921790">thread link</a>) | @kmod
<br/>
October 28, 2020 | https://blog.pyston.org/2020/10/28/pyston-v2-20-faster-python/ | <a href="https://web.archive.org/web/*/https://blog.pyston.org/2020/10/28/pyston-v2-20-faster-python/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">
			
		<div>
		<main id="main" role="main">

					<div>
				
<article id="post-895">
	<!-- .entry-header -->

	<div>
		
<p>We’re very excited to release Pyston v2, a faster and highly compatible implementation of the Python programming language.  Version 2 is 20% faster than stock Python 3.8 on our macrobenchmarks.  More importantly, it is likely to be faster on your code.  Pyston v2 can reduce server costs, reduce user latencies, and improve developer productivity.</p>



<p>Pyston v2 is easy to deploy, so if you’re looking for better Python performance, we encourage you to take five minutes and <a href="#availability">try Pyston</a>. Doing so is one of the easiest ways to speed up your project.</p>



<h2>Performance</h2>



<p>Pyston v2 provides a noticeable speedup on many workloads while having few drawbacks.  Our focus has been on web serving workloads, but Pyston v2 is also faster on other workloads and popular benchmarks.</p>



<p>Our team put together a new <a href="https://github.com/pyston/python-macrobenchmarks/">public Python macrobenchmark suite</a> that measures the performance of several commonly-used Python projects.  The benchmarks in this suite are larger than those found in other Python suites, making them more likely to be representative of real-world applications.  Even though this gives us a lower headline number than other projects, we believe it translates to better speedups for real use cases.  Pyston v2 still shows sped-up performance on microbenchmarks, being twice as fast as standard Python on tests like chaos.py and nbody.py.</p>



<p>Here are our performance results:</p>



<figure><table><tbody><tr><td></td><td>CPython 3.8.5</td><td>Pyston 2.0</td><td>PyPy 7.3.2</td></tr><tr><td>flaskblogging warmup time [1]</td><td>n/a</td><td>n/a</td><td>85s</td></tr><tr><td>flaskblogging mean latency</td><td>5.1ms</td><td>4.1ms</td><td>2.5ms</td></tr><tr><td>flaskblogging p99 latency</td><td>6.3ms</td><td>5.2ms</td><td>5.8ms</td></tr><tr><td>flaskblogging memory usage</td><td>47MB</td><td>54MB</td><td>228MB</td></tr><tr><td>djangocms warmup time [1]</td><td>n/a</td><td>n/a</td><td>105s</td></tr><tr><td>djangocms mean latency</td><td>14.1ms</td><td>11.8ms</td><td>15.9ms</td></tr><tr><td>djangocms p99 latency</td><td>15.0ms</td><td>12.8ms</td><td>179ms</td></tr><tr><td>djangocms memory usage</td><td>84MB</td><td>91MB</td><td>279MB</td></tr><tr><td>Pylint speedup</td><td>1x</td><td>1.16x</td><td>0.50x</td></tr><tr><td>mypy speedup</td><td>1x</td><td>1.07x [2]</td><td>unsupported</td></tr><tr><td>PyTorch speedup</td><td>1x</td><td>1.00x [2]</td><td>unsupported</td></tr><tr><td>PyPy benchmark suite [3]</td><td>1x</td><td>1.36x</td><td>2.48x</td></tr></tbody></table><figcaption>Results were collected on an m5.large EC2 instance running Ubuntu 20.04</figcaption></figure>



<p>[1] Warmup time is defined as time until the benchmark reached 95% of peak performance; if it was not distinguishable from noise it is marked “n/a”.  Only post-warmup behavior is considered for latency measurement.<br>[2] mypy and PyTorch don’t support automatically building their C extensions from source, so these Pyston numbers use our unsafe compatibility mode<br>[3] The PyPy benchmark suite was modified to only run the benchmarks that are compatible with Python 3.8</p>



<h2>Results analysis</h2>



<p>In our targeted benchmarks (djangocms + flaskblogging), Pyston v2 provides an average 1.22x speedup for mean latency and an 1.18x improvement for p99 latency while using a just few more megabytes per process.  We have not yet invested time in optimizing the other benchmarks.</p>



<p>“p99 latency” is the upper 99th percentile of the response-time distribution, and is a common metric used in web serving contexts since it can provide insight into user experience that is lost by taking an average.  PyPy’s high p99 latency on djangocms comes from periodic latency spikes, presumably from garbage collection pauses.  CPython and Pyston both exhibit periodic spikes, presumably from their cycle collectors, but they are both less frequent and much smaller in magnitude.</p>



<p>The mypy and PyTorch benchmarks show a natural boundary of Pyston v2. These benchmarks both do the bulk of their work in C extensions which are unaffected by our Python speedups.  We natively support the C API and do not have an emulation layer, so we are still able to provide a small boost to mypy performance and do not degrade pytorch or numpy performance.  Your benefit will depend on your mix of Python and C extension work.</p>



<h2>Technical approach</h2>



<p>We’re planning on going into more detail in future blog posts, but some of the techniques we use in Pyston v2 include:</p>



<ul><li>A very-low-overhead JIT using <a href="https://luajit.org/dynasm.html">DynASM</a></li><li><a href="https://bugs.python.org/issue14757">Quickening</a></li><li>General CPython optimizations</li><li>Build process improvements</li></ul>



<h2>Compatibility</h2>



<p>Since Pyston is a fork of CPython, we believe it is one of the most compatible alternative Python implementations available today.  It supports all the same features and C API that CPython does.</p>



<p>While Pyston is identically functional in theory, in practice there are some temporary compatibility hurdles for any new Python implementation.  Please see <a href="https://github.com/pyston/pyston/wiki">our wiki</a> for details.</p>



<h2 id="availability">Availability</h2>



<p>Pyston v2.0 is <a href="https://github.com/pyston/pyston/releases">immediately available</a> as a pre-built package.  Currently, we have packages for Ubuntu 18.04 and 20.04 x86_64.  If you would like support for a different OS, let us know by filing an issue in our <a href="https://github.com/pyston/pyston/issues">issue tracker</a>.</p>



<p>Trying out Pyston is as simple as installing our package, replacing <code>python3</code> with <code>pyston3</code>, and reinstalling your dependencies with <code>pip-pyston3 install</code> (though see our <a href="https://github.com/pyston/pyston/wiki">wiki</a> for a known issue about setuptools). If you already have an automated build set up, the change should be just a few lines.</p>



<p>Our plan is to open-source the code in the future, but since compiler projects are expensive and we no longer have benevolent corporate sponsorship, it is currently closed-source while we iron out our business model.</p>



<h2>Reaching us</h2>



<p>We are designing Pyston for developers and love to hear about your needs and experiences.  So, we’ve set up a <a href="https://discord.gg/S7gsqnb">Discord server</a> where you can chat with us.  If you’d like a commercially-supported version of Pyston, please <a href="mailto:business@pyston.org">send us an email</a>.</p>



<p>We’ve optimized Pyston for several use cases but are eager to hear about new ones so that we can make it even more beneficial.  If you run into any problems or instances where Pyston does not help as much as expected, please let us know!</p>



<h2>Background</h2>



<p>We designed Pyston v1 at Dropbox to speed up Python for its web serving workloads.  After the project ended, some of us from the team brainstormed how we would do it differently if we were to do it again.  In early 2020, enough pieces were in place for us to start a company and work on Pyston full-time.</p>



<p>Pyston v2 is inspired by but is technically unrelated to the original Pyston v1 effort.</p>



<h2>Moving forward</h2>



<p>We’re on a mission to make Python faster and have plenty of ideas to do so.  That means we’re actively looking for people to join the team.  <a href="https://discord.gg/S7gsqnb">Let us know</a> if you’d like to get involved.  Otherwise stay tuned for future releases and reach out if you have any questions!</p>
			</div><!-- .entry-content -->

	<!-- .entry-meta -->
</article><!-- #post-## -->
			</div>

				<nav role="navigation" id="nav-below">
		

	
				
	
	</nav><!-- #nav-below -->
	
			
<!-- #comments -->

		
		</main><!-- #main -->
	</div><!-- #primary -->

					<!-- #secondary -->
	
	</div></div>]]>
            </description>
            <link>https://blog.pyston.org/2020/10/28/pyston-v2-20-faster-python/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24921790</guid>
            <pubDate>Wed, 28 Oct 2020 17:42:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Ruby's Proposed STM]]>
            </title>
            <description>
<![CDATA[
Score 152 | Comments 33 (<a href="https://news.ycombinator.com/item?id=24921657">thread link</a>) | @chrisseaton
<br/>
October 28, 2020 | https://chrisseaton.com/truffleruby/ruby-stm/ | <a href="https://web.archive.org/web/*/https://chrisseaton.com/truffleruby/ruby-stm/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>

<header>

<h2><a href="https://chrisseaton.com/">Chris Seaton</a>, 28 October 2020</h2>


</header>

<p>There’s a proposal to add <em>Software Transactional Memory</em>, or <em>STM</em>, to the Ruby programming language. This is part of a wider effort to add better support for concurrency and parallelism in Ruby, and in particular the idea of <em>ractors</em>. A concept has been <a href="https://bugs.ruby-lang.org/issues/17261">proposed</a> and <a href="https://github.com/ruby/ruby/pull/3652">implemented</a> by Koichi Sasada.</p>

<figure>
<img src="https://chrisseaton.com/truffleruby/ruby-stm/testBoard.gif" width="50%">
<figcaption>An animation of the algorithm we're going to use as an example of STM - we'll explain this later on</figcaption>
</figure>

<p>This article gives some context on what STM is, how you use it, and why you might want to use it. We’ll show an application which is well-suited to STM and we’ll use this to talk about the benefits, issues, and some open questions.</p>

<p>We’ll finish by setting a challenge for STM in Ruby.</p>

<p>I wrote the first half of my PhD on STM, and the second half on Ruby, so I’ve got quite a bit of experience with both and the idea of their combination is very interesting to me.</p>

<h2 id="why-might-we-want-an-stm">Why might we want an STM?</h2>

<p>Let’s say we’re a bank managing many bank accounts. Each account has a total. We get a never-ending stream of requests to move a sum of money <code>m</code> from an account <code>a</code> to account <code>b</code>.</p>

<div><div><pre><code><span>loop</span> <span>do</span>
  <span>a</span><span>,</span> <span>b</span><span>,</span> <span>m</span> <span>=</span> <span>get_next_transfer</span>
  <span>accounts</span><span>[</span><span>a</span><span>]</span> <span>-=</span> <span>m</span>
  <span>accounts</span><span>[</span><span>b</span><span>]</span> <span>+=</span> <span>m</span>
<span>end</span>
</code></pre></div></div>

<p>Something not everyone may know about Ruby is that <code>x += y</code> is equivalent to writing <code>t = x; x = t + y</code>. We’ll write that out in full to make that clear to ourselves.</p>

<div><div><pre><code><span>loop</span> <span>do</span>
  <span>a</span><span>,</span> <span>b</span><span>,</span> <span>m</span> <span>=</span> <span>get_next_transfer</span>
  <span>a_balance</span> <span>=</span> <span>accounts</span><span>[</span><span>a</span><span>]</span>
  <span>accounts</span><span>[</span><span>a</span><span>]</span> <span>=</span> <span>a_balance</span> <span>-</span> <span>m</span>
  <span>b_balance</span> <span>=</span> <span>accounts</span><span>[</span><span>b</span><span>]</span>
  <span>accounts</span><span>[</span><span>b</span><span>]</span> <span>=</span> <span>b_balance</span> <span>+</span> <span>m</span>
<span>end</span>
</code></pre></div></div>

<p>We’ve got a lot of transfers to run through, so we’ll have multiple threads processing these transfers.</p>

<div><div><pre><code><span>n</span><span>.</span><span>times</span> <span>do</span>
  <span>Thread</span><span>.</span><span>new</span>
    <span>loop</span> <span>do</span>
      <span>a</span><span>,</span> <span>b</span><span>,</span> <span>m</span> <span>=</span> <span>get_next_transfer</span>
      <span>a_balance</span> <span>=</span> <span>accounts</span><span>[</span><span>a</span><span>]</span>
      <span>accounts</span><span>[</span><span>a</span><span>]</span> <span>=</span> <span>a_balance</span> <span>-</span> <span>m</span>
      <span>b_balance</span> <span>=</span> <span>accounts</span><span>[</span><span>b</span><span>]</span>
      <span>accounts</span><span>[</span><span>b</span><span>]</span> <span>=</span> <span>b_balance</span> <span>+</span> <span>m</span>
    <span>end</span>
  <span>end</span>
<span>end</span>
</code></pre></div></div>

<p>We’ve got a few problems here now. With all these threads running at the same time, what happens if two threads are putting money into your account concurrently?</p>

<div><div><pre><code><span>accounts</span><span>[</span><span>a</span><span>]</span> <span>=</span> <span>100</span>

<span># thread 1                        # thread 2</span>
<span>balance</span> <span>=</span> <span>accounts</span><span>[</span><span>a</span><span>]</span>
  <span># balance = 100</span>
                                  <span>balance</span> <span>=</span> <span>accounts</span><span>[</span><span>a</span><span>]</span>
                                    <span># balance = 100</span>
<span>accounts</span><span>[</span><span>a</span><span>]</span> <span>=</span> <span>balance</span> <span>+</span> <span>10</span>
  <span># accounts[a] = 110</span>
                                  <span>accounts</span><span>[</span><span>a</span><span>]</span> <span>=</span> <span>balance</span> <span>+</span> <span>10</span>
                                    <span># accounts[a] = 110</span>
</code></pre></div></div>

<p>The two transfers have run, but your balance is 110. The other 10 has been lost - this is called a <em>lost update</em>, meaning it’s as if the update was never made.</p>

<p>Also consider what happens if the thread crashes after taking money from <code>a</code> but before putting it into <code>b</code>? The transfer would be applied partially and again we’d lose money.</p>

<p>We need to use some kind of <em>synchronization</em> on our accounts. Ruby has <em>mutual exclusion locks</em> or <em>mutexes</em>, so we can try using those.</p>

<div><div><pre><code><span>n</span><span>.</span><span>times</span> <span>do</span>
  <span>Thread</span><span>.</span><span>new</span>
    <span>loop</span> <span>do</span>
      <span>a</span><span>,</span> <span>b</span><span>,</span> <span>m</span> <span>=</span> <span>get_next_transfer</span>
      <span>locks</span><span>[</span><span>a</span><span>].</span><span>synchronize</span> <span>do</span>
        <span>locks</span><span>[</span><span>b</span><span>].</span><span>synchronize</span> <span>do</span>
          <span>accounts</span><span>[</span><span>a</span><span>]</span> <span>-=</span> <span>m</span>
          <span>accounts</span><span>[</span><span>b</span><span>]</span> <span>+=</span> <span>m</span>
        <span>end</span>
      <span>end</span>
    <span>end</span>
  <span>end</span>
<span>end</span>
</code></pre></div></div>

<p>Does this work? What if we process a transfer from account 1001 to account 1002 on one thread at the same time as processing a transfer from account 1002 to 1001, so the other way around, at the same time?</p>

<p>The first thread will try to lock 1001 and then 1002. The second thread will try to lock 1002 and then 1001. If the first thread gets as far as locking 1001, and the second as far as locking 1002, then both will be waiting for the opposite lock and will never release the lock they already have. We will be in <em>deadlock</em>.</p>

<p>If we always acquired locks in the same order, by collecting them up first and sorting them, we could fix this.</p>

<div><div><pre><code><span>n</span><span>.</span><span>times</span> <span>do</span>
  <span>Thread</span><span>.</span><span>new</span>
    <span>loop</span> <span>do</span>
      <span>a</span><span>,</span> <span>b</span><span>,</span> <span>m</span> <span>=</span> <span>get_next_transfer</span>
      <span>x</span><span>,</span> <span>y</span> <span>=</span> <span>[</span><span>a</span><span>,</span> <span>b</span><span>].</span><span>sort</span>
      <span>locks</span><span>[</span><span>x</span><span>].</span><span>synchronize</span> <span>do</span>
        <span>locks</span><span>[</span><span>y</span><span>].</span><span>synchronize</span> <span>do</span>
          <span>accounts</span><span>[</span><span>a</span><span>]</span> <span>-=</span> <span>m</span>
          <span>accounts</span><span>[</span><span>b</span><span>]</span> <span>+=</span> <span>m</span>
        <span>end</span>
      <span>end</span>
    <span>end</span>
  <span>end</span>
<span>end</span>
</code></pre></div></div>

<p>Now in both transfers account 1001 is locked first and 1002 is locked second. That will work.</p>

<p>We have to make up a somewhat artificial requirement to explain the next issue, but consider if for some good reason we wanted to transfer to one account if we had a lot of money, and a different account if we only had a little money. Maybe if we’re rich this month we donate to charity, otherwise we unfortunately need to save for ourselves.</p>

<div><div><pre><code>if account balance &gt; 1000
  transfer 10 to charity
else
  transfer 10 to savings
end
</code></pre></div></div>

<p>We’ll talk about accounts <code>a</code>, <code>b</code>, and <code>c</code>, now, and a threshold of money <code>t</code>.</p>

<div><div><pre><code><span>n</span><span>.</span><span>times</span> <span>do</span>
  <span>Thread</span><span>.</span><span>new</span>
    <span>loop</span> <span>do</span>
      <span>a</span><span>,</span> <span>b</span><span>,</span> <span>c</span><span>,</span> <span>t</span><span>,</span> <span>m</span> <span>=</span> <span>get_next_transfer</span>
      <span>x</span><span>,</span> <span>y</span><span>,</span> <span>z</span> <span>=</span> <span>[</span><span>a</span><span>,</span> <span>b</span><span>,</span> <span>c</span><span>].</span><span>sort</span>
      <span>locks</span><span>[</span><span>x</span><span>].</span><span>synchronize</span> <span>do</span>
        <span>locks</span><span>[</span><span>y</span><span>].</span><span>synchronize</span> <span>do</span>
          <span>locks</span><span>[</span><span>z</span><span>].</span><span>synchronize</span> <span>do</span>
            <span>if</span> <span>accounts</span><span>[</span><span>a</span><span>]</span> <span>&gt;</span> <span>t</span>
              <span>accounts</span><span>[</span><span>a</span><span>]</span> <span>-=</span> <span>m</span>
              <span>accounts</span><span>[</span><span>b</span><span>]</span> <span>+=</span> <span>m</span>
            <span>else</span>
              <span>accounts</span><span>[</span><span>a</span><span>]</span> <span>-=</span> <span>m</span>
              <span>accounts</span><span>[</span><span>c</span><span>]</span> <span>+=</span> <span>m</span>
            <span>end</span>
          <span>end</span>
        <span>end</span>
      <span>end</span>
    <span>end</span>
  <span>end</span>
<span>end</span>
</code></pre></div></div>

<p>It’s starting to get very complicated. And this locks more than it needs to - it locks both <code>b</code> and <code>c</code> but then only uses one of them. If you use <code>b</code> in the end, ideally another thread could be serving a transfer to <code>c</code> at the same time, but you’ve locked it and it can’t. Imagine if instead of two potential accounts it was thousands and you had to lock them all. Imagine if you couldn’t work out at all which account you’d be transferring to until you started the transfer - then you’d never be able to process two transfers at the same time.</p>

<p>At this point as well we’re likely to start to make errors trying to do all this locking and ordering of locks and things.</p>

<p>Stepping back and taking it all in, we can draw up some requirements for what we need.</p>

<ul>
  <li><em>atomicity</em> - that all writes in the transfer are applied or none are applied</li>
  <li><em>consistency</em> - meaning that our data structures are always valid - the total sum of money never changes</li>
  <li><em>isolation</em> - meaning one transfer does not interfere with another</li>
  <li><em>durability</em> - meaning that when applied the transfer is available to all subsequent transactions</li>
</ul>

<p>Ideally a library or the language could do this all for us. We’d like to be able to write almost what we originally wrote, but with just an annotation to make the code inside a block atomic, consistent, isolated, and the result durable.</p>

<div><div><pre><code><span>n</span><span>.</span><span>times</span> <span>do</span>
  <span>Thread</span><span>.</span><span>new</span>
    <span>loop</span> <span>do</span>
      <span>a</span><span>,</span> <span>b</span><span>,</span> <span>m</span> <span>=</span> <span>get_next_transfer</span>
      <span>atomically</span> <span>do</span>
        <span>accounts</span><span>[</span><span>a</span><span>]</span> <span>-=</span> <span>m</span>
        <span>accounts</span><span>[</span><span>b</span><span>]</span> <span>+=</span> <span>m</span>
      <span>end</span>
    <span>end</span>
  <span>end</span>
<span>end</span>
</code></pre></div></div>

<p>This is what a <em>transactional</em> memory can let us do. It will automatically monitor what you read and write inside the <code>atomically</code> block, which is a <em>transaction</em>, and will make sure it is either applied fully or not, that the balance of the whole system is always consistent, that transactions do not see the result of each other partially applied, and that writes appear and stay.</p>

<p>It may be implemented using the code we eventually arrived at ourselves, or it could do something else instead. In practice how it is often implemented is that
reads and writes are stored in a log, then at the end the transaction works out if anyone else has written locations that you’ve read. If they have then the values you read are no longer valid, so your transaction <em>conflicts</em> with another, is <em>aborted</em> and retries, reading the locations again. When it eventually does not conflict with any other transactions it is <em>committed</em> and succeeds. This means you don’t need to lock everything up-front, which means you avoid the problem of what happens if you may potentially need every account. Locking everything up-front is called <em>pessimistic locking</em>. We’re moving to <em>optimistic locking</em></p>

<h2 id="the-proposed-stm">The proposed STM</h2>

<p>Koichi’s <a href="https://bugs.ruby-lang.org/issues/17261">proposed</a> STM for Ruby, in combination with his proposed <em>ractors</em> (similar to <em>actors</em>) would look like this.</p>

<div><div><pre><code><span>accounts</span> <span>=</span> <span>9999</span><span>.</span><span>times</span><span>.</span><span>map</span> <span>{</span> <span>Thread</span><span>::</span><span>TVar</span><span>.</span><span>new</span><span>(</span><span>100</span><span>)</span> <span>}</span>

<span>n</span><span>.</span><span>times</span> <span>do</span>
  <span>Ractor</span><span>.</span><span>new</span> <span>*</span><span>accounts</span> <span>do</span> <span>|*</span><span>accounts</span><span>|</span>
    <span>loop</span> <span>do</span>
      <span>a</span><span>,</span> <span>b</span><span>,</span> <span>m</span> <span>=</span> <span>get_next_transfer</span>
      <span>Thread</span><span>.</span><span>atomically</span> <span>do</span>
        <span>accounts</span><span>[</span><span>a</span><span>].</span><span>value</span> <span>-=</span> <span>m</span>
        <span>accounts</span><span>[</span><span>b</span><span>].</span><span>value</span> <span>+=</span> <span>m</span>
      <span>end</span>
    <span>end</span>
  <span>end</span>
<span>end</span>
</code></pre></div></div>

<p>He’s using a <code>Ractor</code> but you can think of it as a thread for the purposes of this article. Instead of an array of account balances, we now have an array of <code>TVar</code> objects that contain values. A <code>TVar</code> is a <em>transactional variable</em>. Only these variables are transactional - not any other Ruby value you read or write. His design requires that the <code>TVar</code> objects you’re going to use are passed into the <code>Ractor</code>, due to rules about sharing that aren’t relevant for this article.</p>

<p>This looks good, doesn’t it!</p>

<h2 id="a-more-complex-application">A more complex application</h2>

<p>Let’s consider a larger application, in order to illustrate further and to talk about some issues and open questions. The <a href="https://github.com/chrisseaton/ruby-stm-lee-demo">code is available on GitHub</a>.</p>

<p>Let’s say it’s our job to lay out the wires on a circuit board. We get a board with <em>pads</em> (connections to components mounted on the board) and a list of <em>routes</em> that we need to draw between these pads. There are a great many pads and routes, there isn’t much space on the tiny board, and another catch is that it’s very expensive to have wires crossing each other. Let’s say it’s exponentially more expensive for more deeply stacked wires.</p>

<figure>
<img src="https://chrisseaton.com/truffleruby/ruby-stm/minimal.svg" width="25%">
<figcaption>A minimal board and a solution</figcaption>
</figure>

<p>In this minimal example we we can see two routes, and how they have to cross each other.</p>

<figure>
<img src="https://chrisseaton.com/truffleruby/ruby-stm/mainboard.svg" width="50%">
<figcaption>A processor module board and a solution</figcaption>
</figure>

<p>This example is a processor module and shows what kind of scale we might want to be working at. This board has many longer routes which are more likely to conflict.</p>

<figure>
<img src="https://chrisseaton.com/truffleruby/ruby-stm/memboard.svg" width="50%">
<figcaption>A memory module board and a solution</figcaption>
</figure>

<p>This example is a memory module. It has many shorter routes which we may expect to conflict less.</p>

<figure>
<img src="https://chrisseaton.com/truffleruby/ruby-stm/testBoard.svg" width="50%">
<figcaption>The test board we'll use and a solution</figcaption>
</figure>

<p>We’ll use this test board, which is somewhere between all these extremes.</p>

<p>There’s an algorithm to lay each routes, and it actually produces an optimal solution for an individual route, but not for all routes. It’s called <em>Lee’s algorithm</em> and was published back in 1960. We’ll …</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://chrisseaton.com/truffleruby/ruby-stm/">https://chrisseaton.com/truffleruby/ruby-stm/</a></em></p>]]>
            </description>
            <link>https://chrisseaton.com/truffleruby/ruby-stm/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24921657</guid>
            <pubDate>Wed, 28 Oct 2020 17:32:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Uncover how your funds spend money in politics]]>
            </title>
            <description>
<![CDATA[
Score 44 | Comments 14 (<a href="https://news.ycombinator.com/item?id=24921199">thread link</a>) | @mushufasa
<br/>
October 28, 2020 | https://www.yourstake.org/politics/ | <a href="https://web.archive.org/web/*/https://www.yourstake.org/politics/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">

<section>
  <p>
    <span>Are your Investments</span>
    <br>
    <span>Democrat or Republican?</span>
  </p>
  <p><img src="https://stake-assets.s3.amazonaws.com/static/sillyimages/Donkey-.svg">
    <img src="https://stake-assets.s3.amazonaws.com/static/sillyimages/Indian-elephant.svg">
  </p>
  <br>


  <!--
    to implement when we setup plaid
    <div class="tabs is-centered is-toggle">
    <ul>
      <li class="is-active">
        <a>
          <span>Select</span>
        </a>
      </li>
      <li>
        <a>
          <span>Sync</span>
        </a>
      </li>
    </ul>
  </div> -->
  
  <p>
    Lookup your Fund
  </p>
  <!-- field -->
<!-- myform-->

<p>
  <em>Find out how much money the companies in your mutual fund donate to Democratic and Republican candidates.</em>
</p>

<p><a href="https://www.yourstake.org/yourimpact/">
    <img src="https://stake-assets.s3.amazonaws.com/static/images/yourstakelogo.png" alt="Stake: Your Voice through Your Investments">
  </a>
</p>






</section></div>]]>
            </description>
            <link>https://www.yourstake.org/politics/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24921199</guid>
            <pubDate>Wed, 28 Oct 2020 17:01:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Chess Captcha]]>
            </title>
            <description>
<![CDATA[
Score 45 | Comments 30 (<a href="https://news.ycombinator.com/item?id=24920945">thread link</a>) | @chadash
<br/>
October 28, 2020 | https://elioair.github.io/chesscaptcha/ | <a href="https://web.archive.org/web/*/https://elioair.github.io/chesscaptcha/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section id="main_content">
          <h3>
ChessCaptcha.</h3>

<p>This is a captcha system where the user either recreates the position of the pieces on the board - <em>non chess savvy users</em> - or she solves a mate-in-one puzzle by putting the piece on the square where it gives the checkmate - <em>chess savvy users only</em>. There is also a no-js fallback that exists mostly as a placeholder for future iterations; don't use it.</p>
        </section><section id="usage">
          <h3>
Usage.</h3>

<h4>Copy the position mode. - Default</h4>
<p>
  In this mode which happens to be the one where the user needs no chess knowledge at all to use, he simply drags the pieces
  into the board trying to replicate the position shown in the image.
</p>
<p>
  <img src="https://elioair.github.io/chesscaptcha/images/chesscaptchausage.gif">
</p>
<h4>Mate in One mode</h4>
<p>
  Here the user is given a position and where he has to move a piece to create a mating position on the board.
  Only chess players -and maybe computer engines. cough..!- will be able to solve this. It can be used for example in chess sites
  to determine if the user is really a chess player or a simple spammer.
</p>
<p>
  <img src="https://elioair.github.io/chesscaptcha/images/chesscaptchamatemode.gif">
</p>
<h4>Color Tolerance</h4>
<p>
  If needed you can turn on color tolerance. In this case the validation will be color agnostic and for example the white king will be 
  considered equal to the black king.
</p>
<p>
  <img src="https://elioair.github.io/chesscaptcha/images/chesscaptchacolortolerance.gif">
</p>
        </section></div>]]>
            </description>
            <link>https://elioair.github.io/chesscaptcha/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24920945</guid>
            <pubDate>Wed, 28 Oct 2020 16:44:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[If Not SPAs, What?]]>
            </title>
            <description>
<![CDATA[
Score 341 | Comments 426 (<a href="https://news.ycombinator.com/item?id=24920702">thread link</a>) | @todsacerdoti
<br/>
October 28, 2020 | https://macwright.com/2020/10/28/if-not-spas.html | <a href="https://web.archive.org/web/*/https://macwright.com/2020/10/28/if-not-spas.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>A few months ago, I <a href="https://macwright.com/2020/05/10/spa-fatigue.html">wrote an article about how the SPA pattern has failed to simplify web development</a>. The <em>SPA pattern</em> (Single-Page Apps), I tried to define, was about the React model, which also covers, to a large extent, the model of Vue, Angular, and other frontend frameworks.</p><p>Like any critique, it begs for a prescription and I didn’t give one, other than gesturing toward server-side frameworks like Rails and Django. But I think there are some trends starting to form. I had queued up some time to <em>really dive into the frameworks</em>, but things like <a href="https://macwright.com/2020/10/12/holly-park.html">walking in parks</a> have taken priority, so here’s just a grand tour.</p><h3 id="opinionated-full-stack-javascript-frameworks">Opinionated full-stack JavaScript frameworks</h3><p>Primarily I’m talking about <a href="https://remix.run/features">Remix</a>, <a href="https://redwoodjs.com/">RedwoodJS</a>, and <a href="https://blitzjs.com/">Blitz.js</a>, though I’m sure there are similar efforts in the non-React world that are relevant. <a href="https://nextjs.org/">Next.js</a> <em>almost</em> falls into this category, but as far as I can tell, it’s still unopinionated about the data layer and most sites that use Next.js are still going to use a separate API stack. But that’s subject to change, because all of these are moving fast.</p><p>It’s interesting to note that Remix, Redwood, and Next are all backed by companies or foundations, and that Blitz is aiming early on to be a <a href="https://github.com/sponsors/blitz-js">sponsor-funded project</a>. These projects, I think, are trying to sidestep the “tragedy of the commons” failures of earlier open source, wherein overworked and unpaid maintainers service a large userbase and eventually burn out and abandon the project.</p><p>To take Remix as an example, it re-ties data loading with routes, and then gives the pretty amazing promise of <em>no client side data fetching by default</em>. These frameworks are also <em>opinionated about status codes and caching strategies</em>. RedwoodJS automatically creates an <a href="https://redwoodjs.com/tutorial/side-quest-how-redwood-works-with-data">ORM-like interface using GraphQL and Prisma</a>.</p><p>As context, Remix is backed by the folks from <a href="https://reacttraining.com/">React Training</a>, who are also the folks from <a href="https://reactrouter.com/">React Router</a>, which is as much React pedigree as you can get without joining the team at Facebook. Redwood is run by <a href="https://prestonwernerventures.com/">Preston-Werner Ventures</a>, of <a href="https://en.wikipedia.org/wiki/Tom_Preston-Werner">Tom Preston-Werner</a>, a GitHub founder. Next.js is sponsored and heavily promoted by <a href="https://vercel.com/">Vercel</a>, née Zeit.</p><h3 id="turbolinks">Turbolinks</h3><p>It’s worthwhile to just mention <a href="https://github.com/turbolinks/turbolinks">Turbolinks</a>. I didn’t use it until this year, and apparently there were issues with it before, but the pitch for Turbolinks 5 is: <em>what is the bare minimum you need to do to get the SPA experience without any cooperation from your application?</em></p><p>So it’s a tiny JavaScript library that sits on top of an existing server-rendered application and replaces full pageloads with SPA-like partial pageloads. Instead of loading a page from scratch, pages are loaded with AJAX, page contents are replaced, and client-side navigation updates your URLs. Basically, it prevents the ‘blink’ of real page transitions and saves on all othe sorts of costs of fully loading a new page. Turbolinks was spawned from the <a href="https://rubyonrails.org/">Ruby on Rails</a> project, and works great with Rails but doesn’t require it.</p><p>In terms of power-to-weight for user experience improvements, Turbolinks is a standout: it adds very little complexity and a tiny size impact for a big user experience improvement.</p><h3 id="server-side-state-frameworks">Server-side-state frameworks</h3><p>These are the spiciest new solution. The main contenders are <a href="https://laravel-livewire.com/">Laravel Livewire</a> (in PHP), <a href="https://docs.stimulusreflex.com/">Stimulus Reflex</a> (for Ruby on Rails), and <a href="https://github.com/phoenixframework/phoenix_live_view">Phoenix LiveView</a> (on Phoenix, in Elixir).</p><p>The pitch here is: <em>what if you didn’t have to write any JavaScript?</em> It sort of hearkens back to the critique of JavaScript in <a href="https://vimeo.com/5047563">_why’s ART &amp;&amp; CODE talk</a>, that web development is the only kind where you normally have to write in three (or more) languages. These languages also most remnants of “client-side” logic, putting it all on the server side.</p><p>How do they do this? Well, a lot of WebSockets, in the case of Reflex and LiveView, as well as very tightly coupled server interactions. As you can see in the <a href="https://www.phoenixframework.org/blog/build-a-real-time-twitter-clone-in-15-minutes-with-live-view-and-phoenix-1-5">LiveView demo</a>, which I highly recommend, these frameworks tend to operate sort of like reactive DOM libraries on the front end – in which the framework figures out minimal steps to transform from one state to another - except those steps are computed on the server side and then generically applied on the client side. They also do a lot more data storage &amp; state management on the server-side, because a lot of those interactions which wouldn’t be persisted to the server are now at least communicated to the server.</p><p>These frameworks are exciting, and also extremely contrarian, because they are the polar opposite of the “frontend plus agnostic API layer” pattern, and they also wholeheartedly embrace the thing everyone tries to avoid: mutable state on the server.</p><h3 id="modest-progressive-enhancement-javascript-frameworks">Modest progressive-enhancement JavaScript frameworks</h3><p>These are typically used “in addition” to the above, but they certainly deserve a shout-out because I think a wide swath of frontend-programming concerns actually only need a tiny hint of JavaScript. But the main caveat is that <em>they assume that you know JavaScript and the DOM</em>, which are not necessarily universal skills anymore. A lot of developers growing up on React have acquired a real blind spot for native browser APIs.</p><p>The main ones I’ve looked at are <a href="https://stimulusjs.org/">Stimulus</a> (out of the Ruby on Rails camp), <a href="https://github.com/alpinejs/alpine/">Alpine</a>, and <a href="https://htmx.org/">htmlx</a>. They’re all tiny, and work great in <em>existing pages</em>. I think – and here come the flames – <a href="https://developer.mozilla.org/en-US/docs/Web/Web_Components">Web Components</a> also fit into this sphere of progressive enhancement! If you just use good web components - <a href="https://github.com/search?q=topic%3Aweb-components+org%3Agithub&amp;type=Repositories">only ones that GitHub writes is a good rule of thumb</a> - then they can fit the role of just improving an existing static UI. It’s where you start to use Web Components as an apples-to-apples replacement for full-fledged frontend frameworks is where things seem to get dicey.</p><p>These frameworks have the luxury of operating on a deeply improved web stack, one with fundamental components like <a href="https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API">fetch()</a> and <a href="https://developer.mozilla.org/en-US/docs/Web/API/MutationObserver">MutationObserver</a>. These things were previously at the core of the utility of progressive enhancement frameworks, and now they can just be the utilities that those frameworks build on.</p><hr><p>I’m sure that there are additional patterns out there! But these currents all seem strong right now, and it’s fascinating to see some really divergent and adventurous – and common-sense – approaches start to crop up.</p></div></div>]]>
            </description>
            <link>https://macwright.com/2020/10/28/if-not-spas.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24920702</guid>
            <pubDate>Wed, 28 Oct 2020 16:27:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Bizarre Design Choices in Zoom's End-to-End Encryption]]>
            </title>
            <description>
<![CDATA[
Score 44 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24920125">thread link</a>) | @some_furry
<br/>
October 28, 2020 | https://soatok.blog/2020/10/28/bizarre-design-choices-in-zooms-end-to-end-encryption/ | <a href="https://web.archive.org/web/*/https://soatok.blog/2020/10/28/bizarre-design-choices-in-zooms-end-to-end-encryption/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

		<div>

			
<p>Zoom recently announced that they were going to make end-to-end encryption available to all of their users–not just customers.</p>



<figure><div>

</div></figure>



<p>This is a good move, especially for people living in countries with <a href="https://soatok.blog/2020/07/02/how-and-why-america-was-hit-so-hard-by-covid-19/">inept leadership that failed to address the COVID-19 pandemic</a> and therefore need to conduct their work and schooling remotely through software like Zoom. I enthusiastically applaud them for making this change.</p>



<div><figure><img data-attachment-id="1333" data-permalink="https://soatok.blog/soatoktelegrams2020-08/" data-orig-file="https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png" data-orig-size="512,512" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="SoatokTelegrams2020-08" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png?w=512" src="https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png?w=512" alt="" srcset="https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png 512w, https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png?w=150 150w, https://soatok.files.wordpress.com/2020/09/soatoktelegrams2020-08.png?w=300 300w" sizes="(max-width: 512px) 100vw, 512px"><figcaption>End-to-end encryption, on by default, is a huge win for everyone who uses Zoom. (Art by <a href="https://twitter.com/lynxvsjackalope">Khia</a>.)</figcaption></figure></div>



<p>The end-to-end encryption capability arrives on the heels of their acquisition of <a href="https://keybase.io/">Keybase</a> in earlier this year. Hiring a team of security experts and cryptography engineers seems like a good move overall.</p>



<p>Upon hearing this news, I decided to be a good neighbor and take a look at their source code, with the reasoning, “If so many people’s privacy is going to be dependent on Zoom’s security, I might as well make sure they’re not doing something ridiculously bad.”</p>



<p>Except I couldn’t find their source code anywhere online. But they did publish <a href="https://github.com/zoom/zoom-e2e-whitepaper">a white paper on Github</a>…</p>







<h2>Disclaimers</h2>



<p>What follows is the opinion of some guy on the Internet with a fursona–so whether or not you choose to take it seriously should be informed by this context. It is not the opinion of anyone’s employer, nor is it endorsed by Zoom, etc. Tell your lawyers to calm their nips.</p>



<p>More importantly, I’m not here to hate on Zoom for doing a good thing, nor on the security experts that worked hard on making Zoom better for their users. The responsibility of security professionals is to the users, after all.</p>



<p>Also, these aren’t zero-days, so don’t try to lecture me about “responsible” disclosure. (That term is also <a href="https://adamcaudill.com/2015/11/19/responsible-disclosure-is-wrong/">problematic</a>, by the way.)</p>



<p>Got it? Good. Let’s move on.</p>







<h2>Bizarre Design Choices in Version 2.3 of Zoom’s E2E White Paper</h2>



<p>Note: I’ve altered the screenshots to be white text on a black background, since my blog’s color scheme is darker than a typical academic PDF. You can find the source <a href="https://github.com/zoom/zoom-e2e-whitepaper/blob/d3be2a5a3e16be04f1199b92630f180ba79cb51c/zoom_e2e.pdf">here</a>.</p>



<h3>Cryptographic Algorithms</h3>



<div><figure><img data-attachment-id="1744" data-permalink="https://soatok.blog/zoom-e2e-02/" data-orig-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-02.png" data-orig-size="784,652" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zoom-e2e-02" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-02.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-02.png?w=580" src="https://soatok.files.wordpress.com/2020/10/zoom-e2e-02.png" alt=""></figure></div>



<p>It’s a little weird that they’re calculating a signature over SHA256(Context) || SHA256(M), considering Ed25519 uses SHA512 internally.</p>



<p>It would make just as much sense to sign Context || M directly–or, if pre-hashing large streams is needed, SHA512(Context || M).</p>



<div><figure><img data-attachment-id="1740" data-permalink="https://soatok.blog/zoom-e2e-01/" data-orig-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-01.png" data-orig-size="1039,788" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zoom-e2e-01" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-01.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-01.png?w=580" src="https://soatok.files.wordpress.com/2020/10/zoom-e2e-01.png" alt=""></figure></div>



<p>At the top of this section, it says it uses libsodium’s <code>crypto_box</code> interface. But then they go onto… not actually use it.</p>



<p>Instead, they wrote their own protocol using HKDF, two SHA256 hashes, and XChaCha20-Poly1305.</p>



<p>While secure, this isn’t <em>really</em> using the crypto_box interface.</p>



<p>The only part of the libsodium interface that’s being used is <code><a href="https://github.com/jedisct1/libsodium/blob/927dfe8e2eaa86160d3ba12a7e3258fbc322909c/src/libsodium/crypto_box/curve25519xsalsa20poly1305/box_curve25519xsalsa20poly1305.c#L35-L46">crypto_box_beforenm()</a></code>, which could easily have been a call to <code>crypto_scalarmult()</code>instead (since they’re passing the output of the scalar multiplication to HKDF anyway).</p>







<p>Also, the SHA256(a) || SHA256(b) pattern returns. Zoom’s engineers must love SHA256 for some reason.</p>



<p>This time, it’s in the additional associated data for the XChaCha20-Poly1305. </p>



<p>Binding the ciphertext and the signature to the same context string is a sensible thing to do, it’s just the concatenation of SHA256 hashes is a bit weird when SHA512 exists.</p>



<h3>Meeting Leader Security Code</h3>



<div><figure><img data-attachment-id="1746" data-permalink="https://soatok.blog/zoom-e2e-03/" data-orig-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-03.png" data-orig-size="760,733" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zoom-e2e-03" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-03.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-03.png?w=580" src="https://soatok.files.wordpress.com/2020/10/zoom-e2e-03.png" alt=""></figure></div>



<p>Here we see Zoom using the a SHA256 of a constant string (“<code>Zoombase-1-ClientOnly-MAC-SecurityCode</code>“) in a construction that tries but fails to be HMAC.</p>



<p>And then they concatenate it with the SHA256 hash of the public key (which is already a 256-bit value), and then they hash the whole thing again.</p>



<p>It’s redundant SHA256 all the way down. The redundancy of “MAC” and “SecurityCode” in their constant string is, at least, consistent with the rest of their design philosophy.</p>



<p>It would be a real shame if double-hashing carried the risk of <a href="https://eprint.iacr.org/2013/382">invalidating security proofs</a>, or if <a href="https://cseweb.ucsd.edu/~mihir/papers/kmd5.pdf">the security proof for HMAC</a> required a high Hamming distance of padding constants and this design decision also later <a href="https://eprint.iacr.org/2012/684.pdf">saved HMAC from related-key attacks</a>.</p>



<h3>Hiding Personal Details</h3>



<figure><img data-attachment-id="1750" data-permalink="https://soatok.blog/zoom-e2e-04/" data-orig-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png" data-orig-size="739,603" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="zoom-e2e-04" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png?w=580" src="https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png?w=739" alt="" srcset="https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png 739w, https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png?w=150 150w, https://soatok.files.wordpress.com/2020/10/zoom-e2e-04.png?w=300 300w" sizes="(max-width: 739px) 100vw, 739px"></figure>



<p>Wait, you’re telling me Zoom was aware of HMAC’s existence this whole time?</p>



<div><figure><img data-attachment-id="1202" data-permalink="https://soatok.blog/soatoktelegrams2020-02/" data-orig-file="https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png" data-orig-size="512,512" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="SoatokTelegrams2020-02" data-image-description="" data-medium-file="https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png?w=300" data-large-file="https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png?w=512" src="https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png?w=512" alt="" srcset="https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png 512w, https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png?w=150 150w, https://soatok.files.wordpress.com/2020/08/soatoktelegrams2020-02.png?w=300 300w" sizes="(max-width: 512px) 100vw, 512px"><figcaption>I give up!</figcaption></figure></div>



<h2>Enough Pointless Dunking, What’s the Takeaway?</h2>



<p>None of the design decisions Zoom made that I’ve criticized here are security vulnerabilities, but they do demonstrate an early lack of cryptography expertise in their product design.</p>



<p>After all, the weirdness is almost entirely contained in section 3 of their white paper, which describes the “Phase I” of their rollout. So what I’ve pointed out here appears to be mostly legacy cruft that wasn’t risky enough to bother changing in their final design.</p>



<p>The rest of their paper is pretty straightforward and pleasant to read. Their design makes sense in general, and each phase includes an “Areas to Improve” section.</p>



<p>All in all, if you’re worried about the security of Zoom’s E2EE feature, the only thing they can really do better is to publish the source code (and link to it from the whitepaper repository for ease-of-discovery) for this feature so independent experts can publicly review it.</p>



<p>However, they seem to be getting a lot of mileage out of the experts on their payroll, so I wouldn’t count on that happening.</p>

		</div><!-- .entry-content -->

	</div></div>]]>
            </description>
            <link>https://soatok.blog/2020/10/28/bizarre-design-choices-in-zooms-end-to-end-encryption/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24920125</guid>
            <pubDate>Wed, 28 Oct 2020 15:45:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Dutch flying car gets permission to drive on European roads]]>
            </title>
            <description>
<![CDATA[
Score 181 | Comments 106 (<a href="https://news.ycombinator.com/item?id=24917841">thread link</a>) | @Bologo
<br/>
October 28, 2020 | https://www.psychnewsdaily.com/dutch-flying-car-pal-v-gets-permission-to-drive-on-european-roads/ | <a href="https://web.archive.org/web/*/https://www.psychnewsdaily.com/dutch-flying-car-pal-v-gets-permission-to-drive-on-european-roads/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-4802" role="main"><div><div><div><p>The Dutch company <a rel="noreferrer noopener" href="https://www.pal-v.com/en/" target="_blank">PAL-V</a> today announced that its Liberty flying car <a href="https://www.pal-v.com/en/press/worlds-first-flying-car-hits-the-road" target="_blank" rel="noreferrer noopener">has received permission</a> from the <a rel="noreferrer noopener" href="https://www.rdw.nl/over-rdw/information-in-english" target="_blank">Netherlands Vehicle Authority</a>&nbsp;to drive on public roads.<span data-ez-name="psychnewsdaily_com-medrectangle-3"></span></p><p>The PAL-V Liberty, the flying car’s full name, is a gyro-copter.&nbsp;The rotor can be folded so that the vehicle can also drive on the road like a regular car.&nbsp;</p><p>It needs a runway of between 180 – 330 meters for takeoff, but only 30 meters for landings.&nbsp;Both in the air and on the road, its maximum speed is 180 km/hour (112 mph). Converting from road to air mode (or vice versa) takes between five and ten minutes.</p><p>The Liberty runs on normal gasoline, and has a range of 1315 km (817 miles) on the road. In the air, it can fly 400 – 500 km (250 – 310 milles), and can remain airborne for 4.3 hours.</p><p>The PAL-V Liberty weighs 664 kg (1464 lbs) when empty. Its fuel tank holds 100 liters. The tank of a Honda Accord, just by way of comparison, holds about 53 liters.<span data-ez-name="psychnewsdaily_com-medrectangle-4"></span></p><h2>Already 30 orders for this Dutch flying car</h2><p>According to the company, about thirty Dutch residents have already ordered and paid for the Liberty. The list price is just under €500,000 ($587,000).</p><p>At the moment, the granted permission is only for a single vehicle. That means PAL-V cannot yet put their car into full production.&nbsp;The Netherlands Vehicle Authority first needs to ensure that the company can produce every vehicle according to the same quality standards.</p><p>PAL-V has been working on the Liberty since 2007. The European Aviation Safety Agency is still examining the company’s request to have the vehicles certified to fly. The company expects this permission to arrive in 2022.</p><p>The US state of New Hampshire <a href="https://www.timesnownews.com/auto/features/article/this-is-the-first-state-in-us-to-allow-flying-cars-on-public-roads/634621" target="_blank" rel="noreferrer noopener">made it legal to drive flying cars on public roads</a> in August of this year.<span data-ez-name="psychnewsdaily_com-box-4"></span></p><p>See a video of the PAL-V Liberty driving <a rel="noreferrer noopener" href="https://youtu.be/yIjSaEeO2l0" target="_blank">here</a>, and flying (briefly) <a rel="noreferrer noopener" href="https://youtu.be/fFW_0C7yFCI" target="_blank">here</a>.</p><figure><img loading="lazy" width="1024" height="512" src="https://www.psychnewsdaily.com/wp-content/uploads/2020/10/Flying-Car-PAL-V-Liberty-1024x512.jpg" alt="" srcset="https://www.psychnewsdaily.com/wp-content/uploads/2020/10/Flying-Car-PAL-V-Liberty-1024x512.jpg 1024w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/Flying-Car-PAL-V-Liberty-300x150.jpg 300w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/Flying-Car-PAL-V-Liberty-768x384.jpg 768w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/Flying-Car-PAL-V-Liberty-1536x768.jpg 1536w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/Flying-Car-PAL-V-Liberty-360x180.jpg 360w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/Flying-Car-PAL-V-Liberty-1320x660.jpg 1320w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/Flying-Car-PAL-V-Liberty.jpg 1560w" sizes="(max-width: 1024px) 100vw, 1024px"></figure><figure><img loading="lazy" width="1024" height="537" src="https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-flying-car-1024x537.jpg" alt="" srcset="https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-flying-car-1024x537.jpg 1024w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-flying-car-300x157.jpg 300w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-flying-car-768x403.jpg 768w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-flying-car-360x189.jpg 360w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-flying-car-1320x693.jpg 1320w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-flying-car.jpg 1408w" sizes="(max-width: 1024px) 100vw, 1024px"></figure><figure><img loading="lazy" width="1024" height="538" src="https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-fying-car-interior-1-1024x538.jpg" alt="" srcset="https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-fying-car-interior-1-1024x538.jpg 1024w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-fying-car-interior-1-300x158.jpg 300w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-fying-car-interior-1-768x403.jpg 768w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-fying-car-interior-1-360x189.jpg 360w, https://www.psychnewsdaily.com/wp-content/uploads/2020/10/pal-v-liberty-fying-car-interior-1.jpg 1200w" sizes="(max-width: 1024px) 100vw, 1024px"></figure><p>For a weekly summary of the latest psychology news, subscribe to our <a href="https://www.psychnewsdaily.com/the-psych-news-weekly-newsletter/" target="_blank" rel="noreferrer noopener">Psych News Weekly newsletter</a>.</p></div></div></div></article></div>]]>
            </description>
            <link>https://www.psychnewsdaily.com/dutch-flying-car-pal-v-gets-permission-to-drive-on-european-roads/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24917841</guid>
            <pubDate>Wed, 28 Oct 2020 12:17:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Are we losing our ability to remember?]]>
            </title>
            <description>
<![CDATA[
Score 238 | Comments 157 (<a href="https://news.ycombinator.com/item?id=24917721">thread link</a>) | @scotthtaylor
<br/>
October 28, 2020 | https://st.im/are-we-losing-our-ability-to-remember/ | <a href="https://web.archive.org/web/*/https://st.im/are-we-losing-our-ability-to-remember/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <p>The other day I had a bit of a crisis, I was worried that I was starting to have trouble with my memory. Something had to be wrong! I started to notice (increasingly!) my inability to recall trivial things; for example, the action points from a Zoom call, or a quote from a book that I had read a couple of months ago. Surely this can’t be normal?</p><p>Before calling the doctor’s office I did what any decent hypochondriac would do, and started googling. After clicking through a few pages, I began to feel a bit better. It was normal. Short term (or working) memory <em>is</em> inefficient, and unless I revisit the thing I’m trying to remember a few times, I’m most likely going to forget it. And no, it’s not a side effect of turning thirty. Phew.</p><p>It’s a “feature, not a bug” of how our memory systems are designed. </p><p>Our memory is made up of not one, nor two, but three components: 1) a sensory register, 2) working memory, and 3) long-term memory.</p><p>When I look back at my childhood or I remember some basic words from French, I'm drawing on portions of my brain involved in long-term memory. But when I'm trying to hold a few ideas in mind to connect them together so I can understand a concept or solve a problem, I'm using my working memory.</p><p>With my health crisis averted, I got thinking about technology and its impact (positive and negative) on the way the brain, and memory, function. With all the knowledge I could ever need at my fingertips, alongside note taking apps and the smartphones that are now an extension of our physical being -- am I being lazy or efficient, or a mixture of the two? Much like our overactive fight-or-flight response, has evolution not had a chance to adapt or catch-up with the mind of today versus our ancestors’? </p><h3 id="four-chunks-of-information">Four chunks of information</h3><p>We ‘can’t remember’ things because there is a limit to what we can hold in our working memory. Researchers used to think that it could hold around seven items or chunks, but now it’s widely believed that the working memory only holds about four chunks of information.</p><p>If you’re anything like me you’ll have to repeat something to yourself until you have a chance to write it down. Repetitions are needed so that natural dissipating processes don’t suck the memories away. How many times have you found yourself shutting your eyes to keep other things from intruding into the limited slots of your working memory as you concentrate?</p><p>I believe that we need to offload from our working memory as soon as possible.</p><p>With the goal historically being, before computers, to move it to long-term memory. If this didn’t happen -- you’d essentially be waving goodbye to that memory. </p><p>Moving a memory from ‘working’ to ‘long-term’ takes time and practice. </p><p>There’s a steep drop in what you remember, anyway. The ‘forgetting curve’, as it’s called, is steepest during the first twenty-four hours after you learn something. Exactly how much you forget, percentage-wise, varies, but unless you review the material, much of it slips down the drain. What you remember after day one has a good chance of still being retained after thirty. </p><h3 id="spaced-repetition">Spaced repetition</h3><p>To improve retention, spaced repetition is typically used. This technique involves repeating what you're trying to retain, ensuring to space the repetition out. Repeating a new vocabulary word or a problem solving technique for example over a number of days.</p><p>The good news is, our long-term memory has room for billions of items. In fact there can be so many items they can bury each other. It can be difficult for you to find the information you need unless you practice and repeat at least a few times. This allows the synoptic connections in the brain to form and strengthen into a lasting structure.</p><p>Long-term memory is important because it's where you store fundamental concepts and techniques that are often involved in whatever you're learning about.</p><p>Having strong foundations in your long-term memory also makes the working memory more efficient, and able to connect dots from wider, more abstract, fields. It gives our thinking ‘richness’ and ‘associative access’. </p><p>Richness refers to the theory that a large number of things we have apparently forgotten all about are still there, somewhere, and add depth to our thinking. Associative access means that your thoughts can be accessed in a number of different ways by semantic or perceptual associations  — memories can be triggered by related words, by category names, by a smell, an old song or photograph, or even seemingly random neural firings that bring them up to consciousness.</p><h3 id="offloading-memory">Offloading memory</h3><p>But now, of course, we don’t bother to do all of the hard work of committing many things to our long-term memory. We have devices -- and the internet -- to remember stuff for us.</p><p>When I think back to when I started journaling on my iPad and laptop, as well as using apps like <a href="https://obsidian.md/">Obsidian</a> that are focused on ‘<a href="https://st.im/ive-become-obsessed-with-networked-thought/">networked thought</a>’, it is interesting<strong> </strong>to hypothesise how they have potentially impacted the fundamental chemistry or feedback loops in my brain. </p><p>For me, they have helped reduce my cognitive load by letting me off-load much of what goes on in my brain to an external entity. In this day and age, this recall memory has become less necessary. Recognition memory is more important (i.e. the ability to judge that a currently present object, person, place, or event, has previously been encountered or experienced).</p><p>Research has shown that the internet functions as a sort of externalised memory. “When people expect to have future access to information, they have lower rates of recall of the information itself,” <a href="https://www.ncbi.nlm.nih.gov/pubmed/21764755">as one study puts it</a>. </p><p>If you know that you ‘know’ something, and you know how to retrieve it (thank you Google) that performs pretty much the same function as having a brain stuffed with lots of long-term memories. And the new ‘networked thought’ apps allow us to make interesting connections between these various bits of stored knowledge in much the same way that a well-stocked memory does.</p><h3 id="using-our-second-brain">Using our second brain </h3><p>So I wouldn’t say we are losing our ability to remember, as I posed at the start of this post. I think people (me included) just don’t do enough work to move stuff from our working memory into our long-term memory. </p><p>Our decreased reliance on recall memory and our &nbsp;ever-decreasing attention spans may not be the disaster that I first feared.</p><p>I think that the internet and apps focused on network thinking do assist us. They do act as a second brain. And I think that &nbsp;makes us more efficient.</p><hr><p>Cover photo by <a href="https://unsplash.com/@sarandywestfall_photo?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Sarandy Westfall</a> on <a href="https://unsplash.com/s/photos/memory?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a><br>Thanks to <a href="https://jonathangifford.com/">Jonathan</a> for reading through early drafts of this post.<br>For discussion, check out this post on <a href="https://news.ycombinator.com/item?id=24917721">Hacker News</a>. &nbsp;</p>
              <section>
                <h2>Enjoying these posts? Join for more</h2>
                <a href="https://st.im/join/">Join now</a>
                <br>
                <a href="https://st.im/signin/">Already have an account? Sign in</a>
              </section>
  </div></div>]]>
            </description>
            <link>https://st.im/are-we-losing-our-ability-to-remember/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24917721</guid>
            <pubDate>Wed, 28 Oct 2020 11:58:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What Colour are your bits? (2004)]]>
            </title>
            <description>
<![CDATA[
Score 63 | Comments 23 (<a href="https://news.ycombinator.com/item?id=24917679">thread link</a>) | @dredmorbius
<br/>
October 28, 2020 | https://ansuz.sooke.bc.ca/entry/23 | <a href="https://web.archive.org/web/*/https://ansuz.sooke.bc.ca/entry/23">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="contentsidewrap">
    <div id="content">
      <p>
        « <a href="https://ansuz.sooke.bc.ca/entry/22">Yon and Tinu</a> | <a href="https://ansuz.sooke.bc.ca/">Home</a> | <a href="https://ansuz.sooke.bc.ca/entry/24">Colour, social beings, and und...</a> »
      </p>    

      <h2><a href="https://ansuz.sooke.bc.ca/entry/23">What Colour are your bits?</a></h2>

      <p><span>
        Thu 10 Jun 2004 by <a href="mailto:mskala@ansuz.sooke.bc.ca" title="">mskala</a>
        Tags used:  <a rel="tag" href="https://ansuz.sooke.bc.ca/tag/colour" title="Tag: colour">colour</a>, <a rel="tag" href="https://ansuz.sooke.bc.ca/tag/copyright" title="Tag: copyright">copyright</a>, <a rel="tag" href="https://ansuz.sooke.bc.ca/tag/philosophy" title="Tag: philosophy">philosophy</a>
      </span></p><p>There's a classic adventure game called <a href="http://www.paranoia-rpg.com/players/intro.php">Paranoia</a> which is
set in an extremely <s>repressive</s> Utopian futuristic world
run by The Computer, who
is Your Friend.&nbsp; Looking at a recent <a href="http://research.yale.edu/lawmeme/modules.php?name=News&amp;file=article&amp;sid=1487">LawMeme
posting</a> and related discussion, it occurred to me that the concept of
colour-coded security clearances in Paranoia provides a good metaphor for
a lot of copyright and intellectual freedom issues, and it may illuminate
why we sometimes have difficulty communicating and understanding the
ideologies in these areas.</p>

<p><i>An article based on this one and its follow-ups, by me, Brett Bonfield,
and Mary Fran Torpey, appeared in the 15 February 2008 issue of
<a href="http://www.libraryjournal.com/article/CA6529387.html">LJ, Library
Journal</a>.</i></p>

<p>In Paranoia, everything has a colour-coded security level (from Infrared
up to Ultraviolet) and everybody has a clearance on the same scale.&nbsp; You
are not allowed to touch, or have any dealings with, anything that exceeds
your clearance.&nbsp; If you're a Red Troubleshooter, you're not allowed to
walk through an Orange door.&nbsp; Formally, you're not really supposed to even
know about the existence of anything above your clearance.&nbsp; Anyone who
breaks the rules is a Commie Mutant Traitor, subject to the death penalty.</p>

<p>Much of the game revolves around the consequences of the security levels.&nbsp;
For instance, Friend Computer might assign a team of Red Troubleshooters
to re-paint a hallway that ought to be Orange but was painted Yellow by
<s>mistake</s> the Commie Mutant Traitors.&nbsp; It's quite likely in
such a case that the Troubleshooters will all end up shooting each other
for treason against Friend Computer, since none of them are allowed to
touch the paint, go near the hallway, or talk about their mission, and
they're all charged with enforcing the rules on one another.</p>

<p>In intellectual property and some other fields we're very interested in
information, data, artistic works, a whole lot of things that I'll
summarize with the term "bits".&nbsp; Bits are all the things you can (at
least in principle) represent with binary ones and zeroes.&nbsp; And very much
of intellectual property law comes down to rules regarding intangible
attributes of bits - Who created the bits?&nbsp; Where did they come from?&nbsp;

Where are they going?&nbsp; Are they copies of other bits?&nbsp; Those questions are
perhaps answerable by "metadata", but metadata suggests to me additional
bits attached to the bits in question, and I'd like to emphasize that I'm
talking here about something that is not properly captured by bits at all
and actually cannot be, ever.&nbsp; Let's call it "Colour", because it turns
out to behave a lot like the colour-coded security clearances of the
Paranoia universe.</p>

<p>Bits do not naturally have Colour.&nbsp; Colour, in this sense, is not part of
the natural universe.&nbsp; Most importantly, you cannot look at bits and
observe what Colour they are.&nbsp; I encountered an amusing example of bit
Colour recently:&nbsp; one of my friends was talking about how he'd
performed John Cage's famous silent musical composition <i>4'33"</i> for
MP3.&nbsp; Okay, we said, (paraphrasing the conversation here) so you took an
appropriate-sized file of zeroes out of /dev/zero and compressed that with
an MP3 compressor?&nbsp; No, no, he said.&nbsp; If I did that, it wouldn't really be

<i>4'33"</i> because to perform the composition, you have to make the
silence in a certain way, according to the rules laid down by the
composer.&nbsp; It's not just four minutes and thirty-three seconds of any old
silence.</p>

<p>My friend had gone through an elaborate process that basically amounted to
performing some other piece of music four minutes and thirty-three seconds
long, with a software synthesizer and the volume set to zero.&nbsp; The result
was an appropriate-sized file of zeroes - which he compressed with an MP3
compressor.&nbsp; The MP3 file was bit-for-bit identical to one that would have
been produced by compressing /dev/zero...&nbsp; but this file was (he claimed)
legitimately a recording of <i>4'33"</i> and the other one wouldn't have
been.&nbsp; The difference was the Colour of the bits.&nbsp; He was asserting
that the bits in his copy of 433.mp3 had a different Colour from those
in a copy of 433.mp3 I might make by means of the /dev/zero procedure,
even though the two files would contain exactly the same bits.</p>

<p>Now, the preceding paragraph is basically nonsense to computer scientists
or anyone with a mathematical background.&nbsp; (My friend is one; he'd done
this as a sort of elaborate joke.)  Numbers are numbers, right?&nbsp;
If I add 39 plus 3 and get 42, and you do the same thing, there is no way
that "my" 42 can be said to be different from "your" 42.&nbsp; Given two
bit-for-bit identical MP3 files, there is no meaningful (to a computer
scientist) way to say that one is a recording of the Cage composition and
the other one isn't.&nbsp; There would be no way to test one of the files and
see which one it was, because they are actually the same file.&nbsp; Having
identical bits means by definition that there can be no difference.&nbsp; Bits
don't have Colour; computer scientists, like computers, are Colour-blind.&nbsp;
That is not a mistake or deficiency on our part:&nbsp; rather, we have worked
hard to become so.&nbsp; Colour-blindness on the part of computer scientists
helps us understand the fact that computers are also Colour-blind, and we
need to be intimately familiar with that fact in order to do our jobs.</p>

<p>The trouble is, human beings are not in general Colour-blind.&nbsp; The law is
not Colour-blind.&nbsp; It makes a difference not only what bits you have, but
where they came from.&nbsp; There's a very interesting Web page illustrating
the Coloured nature of bits in law <a href="http://aa.usno.navy.mil/faq/docs/lawyers.html">on the US Naval
Observatory Web site</a>.&nbsp; They provide information on that site about
when the Sun rises and sets and so on...&nbsp; but they also provide it under a
disclaimer saying that this information is not suitable for use in court.&nbsp;
If you need to know when the Sun rose or set for use in a court case, then
you need an expert witness - because you don't actually just need the bits
that say when the Sun rose.&nbsp; You need those bits to be Coloured with the
Colour that allows them to be admissible in court, and the USNO doesn't
provide that.&nbsp; It's not just a question of accuracy - we all know
perfectly well that the USNO's numbers are good.&nbsp; It's a question of where
the numbers came from.&nbsp; It makes perfect sense to a lawyer that where the
information came from is important, in fact maybe more important than the
information itself.&nbsp; The law sees Colour.</p>

<p>Suppose you publish an article that happens to contain a sentence
identical to one from this article, like "The law sees Colour."  That's
just four words, all of them common, and it might well occur by random
chance.&nbsp; Maybe you were thinking about similar ideas to mine and happened
to put the words together in a similar way.&nbsp; If so, fine.&nbsp; But maybe you
wrote "your" article by cutting and pasting from "mine" - in that case,
the words have the Colour that obligates you to follow quotation
procedures and worry about "derivative work" status under copyright law
and so on.&nbsp; Exactly the same words - represented on a computer by the same
bits - can vary in Colour and have differing consequences.&nbsp; When you
use those words without quotation marks, either you're an author or a
plagiarist depending on where you got them, even though they are the
same words.&nbsp; It matters where the bits came from.</p>

<p>I think Colour is what the designers of <a href="http://monolith.sourceforge.net/">Monolith</a> are trying to
challenge, although I'm afraid I think their understanding of the
issues is superficial on both the legal and computer-science sides.&nbsp; The
idea of Monolith is that it will mathematically combine two files with the
exclusive-or operation.&nbsp; You take a file to which someone claims
copyright, mix it up with a public file, and then the result, which is
mixed-up garbage supposedly containing no information, is supposedly free
of copyright claims even though someone else can later undo the mixing
operation and produce a copy of the copyright-encumbered file you started
with.&nbsp; Oh, happy day!&nbsp; The lawyers will just have to all go away now,
because we've demonstrated the absurdity of intellectual property!</p>

<p>The fallacy of Monolith is that it's playing fast and loose with Colour,
attempting to use legal rules one moment and math rules another moment as
convenient.&nbsp; When you have a copyrighted file at the start, that file
clearly has the "covered by copyright" Colour, and you're not cleared for
it, Citizen.&nbsp; When it's scrambled by Monolith, the claim is that the
resulting file has no Colour - how could it have the copyright Colour?&nbsp;
It's just random bits!&nbsp; Then when it's descrambled, it still can't have
the copyright Colour because it came from public inputs.&nbsp; The problem is
that there are two conflicting sets of rules there.&nbsp; Under the lawyer's
rules, Colour is not a mathematical function of the bits that you can
determine by examining the bits.&nbsp; <i>It matters where the bits came
from.</i> The scrambled file still has the copyright Colour because it
came from the copyrighted input file.&nbsp; It doesn't matter that it looks
like, or maybe even is bit-for-bit identical with, some other file that
you could get from a random number generator.&nbsp; It happens that you didn't
get it from a random number generator.&nbsp; You got it from copyrighted
material; it is copyrighted.&nbsp; The randomly-generated file, even if
bit-for-bit identical, would have a different Colour.&nbsp; The Colour inherits
through all scrambling and descrambling operations and you're distributing
a copyrighted work, you Commie Mutant Traitor.</p>

<p>To a computer scientist, on the other hand, bits are bits are bits and it
is absolutely fundamental that two identical chunks of bits cannot be
distinguished.&nbsp; Colour does not exist.&nbsp; I've seen computer people claim
(indeed, one did this to me just today in the very discussion that
inspired this posting) that copyright law …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ansuz.sooke.bc.ca/entry/23">https://ansuz.sooke.bc.ca/entry/23</a></em></p>]]>
            </description>
            <link>https://ansuz.sooke.bc.ca/entry/23</link>
            <guid isPermaLink="false">hacker-news-small-sites-24917679</guid>
            <pubDate>Wed, 28 Oct 2020 11:53:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Markov Chain Monte Carlo (MCMC) Sampling, Part 1: The Basics (2019)]]>
            </title>
            <description>
<![CDATA[
Score 145 | Comments 20 (<a href="https://news.ycombinator.com/item?id=24917011">thread link</a>) | @vonadz
<br/>
October 28, 2020 | https://www.tweag.io/blog/2019-10-25-mcmc-intro1/ | <a href="https://web.archive.org/web/*/https://www.tweag.io/blog/2019-10-25-mcmc-intro1/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><p>This is part 1 of a series of blog posts about MCMC techniques:</p>
<ul>
<li><a href="https://www.tweag.io/posts/2020-01-09-mcmc-intro2.html">Part II: Gibbs sampling</a></li>
<li><a href="https://www.tweag.io/blog/2020-08-06-mcmc-intro3/">Part III: Hamiltonian Monte Carlo</a></li>
<li><a href="https://www.tweag.io/blog/2020-10-28-mcmc-intro-4/">Part IV: Replica Exchange</a></li>
</ul>
<p><a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Markov chain Monte Carlo</a> (MCMC) is a powerful class of methods to sample from probability distributions known only up to an (unknown) normalization constant. But before we dive into MCMC, let’s consider why you might want to do sampling in the first place.</p>
<p>The answer to that is: whenever you’re either interested in the samples themselves (for example, inferring unknown parameters in Bayesian inference) or you need them to approximate expected values of functions w.r.t. to a probability distribution (for example, calculating thermodynamic quantities from the distribution of microstates in statistical physics).
Sometimes, only the mode of a probability distribution is of primary interest. In this case, it’s obtained by numerical optimization so full sampling is not necessary.</p>
<p>It turns out that sampling from any but the most basic probability distributions is a difficult task.
<a href="https://en.wikipedia.org/wiki/Inverse_transform_sampling">Inverse transform sampling</a> is an elementary method to sample from probability distributions, but requires the cumulative distribution function, which in turn requires knowledge of the, generally unknown, normalization constant.
Now in principle, you could just obtain the normalization constant by numerical integration, but this quickly gets infeasible with an increasing number of dimensions.
<a href="https://en.wikipedia.org/wiki/Rejection_sampling">Rejection sampling</a> does not require a normalized distribution, but efficiently implementing it requires a good deal of knowledge about the distribution of interest, and it suffers strongly from the curse of dimension, meaning that its efficiency decreases rapidly with an increasing number of variables.
That’s when you need a smart way to obtain representative samples from your distribution which doesn’t require knowledge of the normalization constant.</p>
<p>MCMC algorithms are a class of methods which do exactly that.
These methods date back to a <a href="https://pdfs.semanticscholar.org/7b3d/c9438227f747e770a6fb6d7d7c01d98725d6.pdf">seminal paper by Metropolis et al.</a>, who developed the first MCMC algorithm, correspondingly called <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolis algorithm</a>, to calculate the equation of state of a two-dimensional system of hard spheres. In reality, they were looking for a general method to calculate expected values occurring in statistical physics.</p>
<p>In this blog post, I introduce the basics of MCMC sampling; in subsequent posts I’ll cover several important, increasingly complex and powerful MCMC algorithms, which all address different difficulties one frequently faces when using the Metropolis-Hastings algorithm. Along the way, you will gain a solid understanding of these challenges and how to address them.
Also, this serves as a reference for MCMC methods in the context of the <a href="https://www.tweag.io/posts/2019-09-20-monad-bayes-1.html">monad-bayes</a> series.
Furthermore, I hope the provided notebooks will not only spark your interest in exploring the behavior of MCMC algorithms for various parameters/probability distributions, but also serve as a basis for implementing and understanding useful extensions of the basic versions of the algorithms I present.</p>
<h2>Markov chains</h2>
<p>Now that we know why we want to sample, let’s get to the heart of MCMC: Markov chains.
What is a Markov chain? Without all the technical details, a Markov chain is a random sequence of states in some state space in which the probability of picking a certain state next depends only on the current state in the chain and not on the previous history: it is memory-less.
Under certain conditions, a Markov chain has a unique stationary distribution of states to which it converges after a certain number of states. From that number on, states in the Markov chain are distributed according to the invariant distribution.</p>
<p>In order to sample from a distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi(x)</annotation></semantics></math></span></span>, a MCMC algorithm constructs and simulates a Markov chain whose stationary distribution is <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi(x)</annotation></semantics></math></span></span>, meaning that, after an initial “burn-in” phase, the states of that Markov chain are distributed according to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi(x)</annotation></semantics></math></span></span>.
We thus just have to store the states to obtain samples from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi(x)</annotation></semantics></math></span></span>.</p>
<p>For didactic purposes, let’s for now consider both a discrete state space and discrete “time”.
The key quantity characterizing a Markov chain is the transition operator <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">T(x_{i+1}|x_i)</annotation></semantics></math></span></span> which gives you the probability of being in state <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{i+1}</annotation></semantics></math></span></span> at time <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">i+1</annotation></semantics></math></span></span> given that the chain is in state <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span></span> at time <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span></span>.</p>
<p>Now just for fun (and for illustration), let’s quickly whip up a Markov chain which has a unique stationary distribution.
We’ll start with some imports and settings for the plots:</p>
<div data-language="python"><pre><code><span>%</span>matplotlib notebook
<span>%</span>matplotlib inline
<span>import</span> numpy <span>as</span> np
<span>import</span> matplotlib<span>.</span>pyplot <span>as</span> plt
plt<span>.</span>rcParams<span>[</span><span>'figure.figsize'</span><span>]</span> <span>=</span> <span>[</span><span>10</span><span>,</span> <span>6</span><span>]</span>
np<span>.</span>random<span>.</span>seed<span>(</span><span>42</span><span>)</span></code></pre></div>
<p>The Markov chain will hop around on a discrete state space which is made up from three weather states:</p>
<div data-language="python"><pre><code>state_space <span>=</span> <span>(</span><span>"sunny"</span><span>,</span> <span>"cloudy"</span><span>,</span> <span>"rainy"</span><span>)</span></code></pre></div>
<p>In a discrete state space, the transition operator is just a matrix.
Columns and rows correspond, in our case, to sunny, cloudy, and rainy weather.
We pick more or less sensible values for all transition probabilities:</p>
<div data-language="python"><pre><code>transition_matrix <span>=</span> np<span>.</span>array<span>(</span><span>(</span><span>(</span><span>0.6</span><span>,</span> <span>0.3</span><span>,</span> <span>0.1</span><span>)</span><span>,</span>
                              <span>(</span><span>0.3</span><span>,</span> <span>0.4</span><span>,</span> <span>0.3</span><span>)</span><span>,</span>
                              <span>(</span><span>0.2</span><span>,</span> <span>0.3</span><span>,</span> <span>0.5</span><span>)</span><span>)</span><span>)</span></code></pre></div>
<p>The rows indicate the states the chain might currently be in and the columns the states the chains might transition to.
If we take one “time” step of the Markov chain as one hour, then, if it’s sunny, there’s a 60% chance it stays sunny in the next hour, a 30% chance that in the next hour we will have cloudy weather, and only a 10% chance of rain immediately after it had been sunny before.
This also means that each row has to sum up to one.</p>
<p>Let’s run our Markov chain for a while:</p>
<div data-language="python"><pre><code>n_steps <span>=</span> <span>20000</span>
states <span>=</span> <span>[</span><span>0</span><span>]</span>
<span>for</span> i <span>in</span> <span>range</span><span>(</span>n_steps<span>)</span><span>:</span>
    states<span>.</span>append<span>(</span>np<span>.</span>random<span>.</span>choice<span>(</span><span>(</span><span>0</span><span>,</span> <span>1</span><span>,</span> <span>2</span><span>)</span><span>,</span> p<span>=</span>transition_matrix<span>[</span>states<span>[</span><span>-</span><span>1</span><span>]</span><span>]</span><span>)</span><span>)</span>
states <span>=</span> np<span>.</span>array<span>(</span>states<span>)</span></code></pre></div>
<p>We can monitor the convergence of our Markov chain to its stationary distribution by calculating the empirical probability for each of the states as a function of chain length:</p>
<div data-language="python"><pre><code><span>def</span> <span>despine</span><span>(</span>ax<span>,</span> spines<span>=</span><span>(</span><span>'top'</span><span>,</span> <span>'left'</span><span>,</span> <span>'right'</span><span>)</span><span>)</span><span>:</span>
    <span>for</span> spine <span>in</span> spines<span>:</span>
        ax<span>.</span>spines<span>[</span>spine<span>]</span><span>.</span>set_visible<span>(</span><span>False</span><span>)</span>

fig<span>,</span> ax <span>=</span> plt<span>.</span>subplots<span>(</span><span>)</span>
width <span>=</span> <span>1000</span>
offsets <span>=</span> <span>range</span><span>(</span><span>1</span><span>,</span> n_steps<span>,</span> <span>5</span><span>)</span>
<span>for</span> i<span>,</span> label <span>in</span> <span>enumerate</span><span>(</span>state_space<span>)</span><span>:</span>
    ax<span>.</span>plot<span>(</span>offsets<span>,</span> <span>[</span>np<span>.</span><span>sum</span><span>(</span>states<span>[</span><span>:</span>offset<span>]</span> <span>==</span> i<span>)</span> <span>/</span> offset
            <span>for</span> offset <span>in</span> offsets<span>]</span><span>,</span> label<span>=</span>label<span>)</span>
ax<span>.</span>set_xlabel<span>(</span><span>"number of steps"</span><span>)</span>
ax<span>.</span>set_ylabel<span>(</span><span>"likelihood"</span><span>)</span>
ax<span>.</span>legend<span>(</span>frameon<span>=</span><span>False</span><span>)</span>
despine<span>(</span>ax<span>,</span> <span>(</span><span>'top'</span><span>,</span> <span>'right'</span><span>)</span><span>)</span>
plt<span>.</span>show<span>(</span><span>)</span></code></pre></div>
<p><span>
      <a href="https://www.tweag.io/static/970ffb50b65a63406121b8ed07f5e4b1/d0d8c/mcmc-intro1-weatherchain.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="png" title="png" src="https://www.tweag.io/static/970ffb50b65a63406121b8ed07f5e4b1/fcda8/mcmc-intro1-weatherchain.png" srcset="https://www.tweag.io/static/970ffb50b65a63406121b8ed07f5e4b1/12f09/mcmc-intro1-weatherchain.png 148w,
https://www.tweag.io/static/970ffb50b65a63406121b8ed07f5e4b1/e4a3f/mcmc-intro1-weatherchain.png 295w,
https://www.tweag.io/static/970ffb50b65a63406121b8ed07f5e4b1/fcda8/mcmc-intro1-weatherchain.png 590w,
https://www.tweag.io/static/970ffb50b65a63406121b8ed07f5e4b1/d0d8c/mcmc-intro1-weatherchain.png 609w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span></p>
<h2>The mother of all MCMC algorithms: Metropolis-Hastings</h2>
<p>So that’s lots of fun, but back to sampling an arbitrary probability distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span></span>.
It could either be discrete, in which case we would keep talking about a transition matrix <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span></span>, or be continuous, in which case <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span></span> would be a transition <em>kernel</em>.
From now on, we’re considering continuous distributions, but all concepts presented here transfer to the discrete case.</p>
<p>If we could design the transition kernel in such a way that the next state is already drawn from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span></span>, we would be done, as our Markov chain would… well… immediately sample from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span></span>.
Unfortunately, to do this, we need to be able to sample from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span></span>, which we can’t.
Otherwise you wouldn’t be reading this, right?</p>
<p>A way around this is to split the transition kernel <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">T(x_{i+1}|x_i)</annotation></semantics></math></span></span> into two parts:
a proposal step and an acceptance/rejection step.
The proposal step features a proposal distribution <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">q(x_{i+1}|x_i)</annotation></semantics></math></span></span>, from which we can sample possible next states of the chain.
In addition to being able to sample from it, we can choose this distribution arbitrarily. But, one should strive to design it such that samples from it are both as little correlated with the current state as possible and have a good chance of being accepted in the acceptance step.
Said acceptance/rejection step is the second part of the transition kernel and corrects for the error introduced by proposal states drawn from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo mathvariant="normal">≠</mo><mi>π</mi></mrow><annotation encoding="application/x-tex">q \neq \pi</annotation></semantics></math></span></span>.
It involves calculating an acceptance probability <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_\mathrm{acc}(x_{i+1}|x_i)</annotation></semantics></math></span></span> and accepting the proposal <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{i+1}</annotation></semantics></math></span></span> with that probability as the next state in the chain.
Drawing the next state <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{i+1}</annotation></semantics></math></span></span> from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">T(x_{i+1}|x_i)</annotation></semantics></math></span></span> is then done as follows:
first, a proposal state <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{i+1}</annotation></semantics></math></span></span> is drawn from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">q(x_{i+1}|x_i)</annotation></semantics></math></span></span>.
It is then accepted as the next state with probability <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_\mathrm{acc}(x_{i+1}|x_i)</annotation></semantics></math></span></span> or rejected with probability <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub><mo stretchy="false">(</mo><mi>x</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">1-p_\mathrm{acc}(x\_{i+1}|x_i)</annotation></semantics></math></span></span>, in which case the current state is copied as the next state.</p>
<p>We thus have</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>×</mo><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mtext>&nbsp;.</mtext></mrow><annotation encoding="application/x-tex">T(x_{i+1}|x_i)=q(x_{i+1} | x_i) \times p_\mathrm{acc}(x_{i+1}|x_i) \ \text .</annotation></semantics></math></span></span></span></p><p>A sufficient condition for a Markov chain to have <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span></span> as its stationary distribution is the transition kernel obeying <em>detailed balance</em> or, in the physics literature, <em>microscopic reversibility</em>:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mi>T</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>π</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mi>T</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi(x_i) T(x_{i+1}|x_i) = \pi(x_{i+1}) T(x_i|x_{i+1})</annotation></semantics></math></span></span></span></p><p>This means that the probability of being in a state <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span></span> and transitioning to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{i+1}</annotation></semantics></math></span></span> must be equal to the probability of the reverse process, namely, being in state <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">x\_{i+1}</annotation></semantics></math></span></span> and transitioning to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span></span>.
Transition kernels of most MCMC algorithms satisfy this condition.</p>
<p>For the two-part transition kernel to obey detailed balance, we need to choose <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub></mrow><annotation encoding="application/x-tex">p_\mathrm{acc}</annotation></semantics></math></span></span> correctly, meaning that is has to correct for any asymmetries in probability flow from / to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">x_{i+1}</annotation></semantics></math></span></span> or <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span></span>.
Metropolis-Hastings uses the Metropolis acceptance criterion:</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi></mrow><mrow><mo fence="true">{</mo><mn>1</mn><mo separator="true">,</mo><mfrac><mrow><mi>π</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>×</mo><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><mrow><mi>π</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>×</mo><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mfrac><mo fence="true">}</mo></mrow><mtext>&nbsp;.</mtext></mrow><annotation encoding="application/x-tex">p_\mathrm{acc}(x_{i+1}|x_i) = \mathrm{min} \left\{1, \frac{\pi(x_{i+1}) \times q(x_i|x_{i+1})}{\pi(x_i) \times q(x_{i+1}|x_i)} \right\} \ \text .</annotation></semantics></math></span></span></span></p><p>Now here’s where the magic happens:
we know <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span></span> only up to a constant, but it doesn’t matter, because that unknown constant cancels out in the expression for <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub></mrow><annotation encoding="application/x-tex">p_\mathrm{acc}</annotation></semantics></math></span></span>!
It is this property of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub></mrow><annotation encoding="application/x-tex">p_\mathrm{acc}</annotation></semantics></math></span></span> which makes algorithms based on Metropolis-Hastings work for unnormalized distributions.
Often, symmetric proposal distributions with <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><mi>q</mi><mo stretchy="false">(</mo><mi>x</mi><mi mathvariant="normal">_</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">q(x_i|x_{i+1})=q(x\_{i+1}|x_i)</annotation></semantics></math></span></span> are used, in which case the Metropolis-Hastings algorithm reduces to the original, but less general Metropolis algorithm developed in 1953 and for which</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi></mrow><mrow><mo fence="true">{</mo><mn>1</mn><mo separator="true">,</mo><mfrac><mrow><mi>π</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><mrow><mi>π</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mfrac><mo fence="true">}</mo></mrow><mtext>&nbsp;.</mtext></mrow><annotation encoding="application/x-tex">p_\mathrm{acc}(x_{i+1}|x_i) = \mathrm{min} \left\{1, \frac{\pi(x_{i+1})}{\pi(x_i)} \right\} \ \text .</annotation></semantics></math></span></span></span></p><p>We can then write the complete Metropolis-Hastings transition kernel as</p>
<p><span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>×</mo><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>:</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo mathvariant="normal">≠</mo><msub><mi>x</mi><mi>i</mi></msub><mtext>;</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>1</mn><mo>−</mo><mo>∫</mo><mi mathvariant="normal">d</mi><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mtext>&nbsp;</mtext><mi>q</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>×</mo><msub><mi>p</mi><mrow><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">c</mi></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>:</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>x</mi><mi>i</mi></msub><mtext>.</mtext></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">T(x_{i+1}|x_i) = \begin{cases}
                   q(x_{i+1}|x_i) \times p_\mathrm{acc}(x_{i+1}|x_i) &amp;: x_{i+1} \neq x_i \text ; \\\\
                   1 - \int \mathrm{d}x_{i+1} \ q(x_{i+1}|x_i) \times p_\mathrm{acc}(x_{i+1}|x_i) &amp;: x_{i+1} = x_i\text .
                 \end{cases}</annotation></semantics></math></span></span></span></p><h2>Implementing the Metropolis-Hastings algorithm in Python</h2>
<p>All right, now that we know how Metropolis-Hastings works, let’s go ahead and implement it.
First, we set the log-probability of the distribution we want to sample from—without normalization constants, as we pretend we don’t know them. Let’s work for now with a standard normal distribution:</p>
<div data-language="python"><pre><code><span>def</span> <span>log_prob</span><span>(</span>x<span>)</span><span>:</span>
     <span>return</span> <span>-</span><span>0.5</span> <span>*</span> np<span>.</span><span>sum</span><span>(</span>x <span>**</span> <span>2</span><span>)</span></code></pre></div>
<p>Next, we choose a symmetric proposal distribution. Generally, including information you have about the distribution …</p></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.tweag.io/blog/2019-10-25-mcmc-intro1/">https://www.tweag.io/blog/2019-10-25-mcmc-intro1/</a></em></p>]]>
            </description>
            <link>https://www.tweag.io/blog/2019-10-25-mcmc-intro1/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24917011</guid>
            <pubDate>Wed, 28 Oct 2020 09:55:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Defining Data Intuition]]>
            </title>
            <description>
<![CDATA[
Score 39 | Comments 7 (<a href="https://news.ycombinator.com/item?id=24916868">thread link</a>) | @headalgorithm
<br/>
October 28, 2020 | https://blog.harterrt.com/data_intuition.html | <a href="https://web.archive.org/web/*/https://blog.harterrt.com/data_intuition.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p>Last week, one of my peers asked me to explain what I meant by "Data Intuition",
and I realized I really didn't have a good definition.
That's a problem! I refer to data intuition all the time!</p>
<p>Data intuition is one of the three skills I interview new data scientists for
(along with statistics and technical skills).
In fact, I just spent the first nine months of 2020
building Mozilla's data intuition.
I'm really surprised to realize I can't point to
a good explanation of what I'm trying to cultivate.</p>
<p>So - I'll make one up. I propose the following definition for Data Intuition:</p>
<blockquote>
<p><strong>Data Intuition is a resilience to misleading data and analyses.</strong></p>
</blockquote>
<p>In other words, it's harder to mislead someone with data
if they have strong data intuition.
Think of this as <strong>a defense against the dark data arts</strong>.</p>
<p>So what does that look like in practice?</p>
<h2>Data Stink</h2>
<p>Someone with strong data intuition can quickly spot "data-stink"
(a close cousin to "<a href="https://en.wikipedia.org/wiki/Code_smell">code smell</a>").
These are data issues that don't necessarily invalidate an analysis,
but certainly draw suspicion on the results.
For example:</p>
<ul>
<li>An analysis prominently reports a seemingly <strong>arbitrary metric</strong> -
  4-day retention increased by 0.5%!
  Where did 4-day retention come from? Don't we usually track 7-day retention?
  This needs more attention before I trust the results.</li>
<li>An analysis reports <strong>extraordinary results</strong> where nominal results are expected -
  this feature increased retention by 10%!
  But, past efforts were trying to increase retention by 0.5% - 
  and isn't retention already 90%? How'd we get and increase of 10%?</li>
</ul>
<p>These are extreme examples. 
Usually the problems are more subtle
and result in a general sense of uneasiness with the results
(that's why it's called "intuition").</p>
<p>It's clear to me that data intuition is <em>related</em> to product intuition,
though these <em>are</em> different skills.
Product intuition can contextualize our results
and make it easier to identify extraordinary claims in analyses.
To know a 10% gain in retention is ridiculous
we need to know that users retain pretty well already.</p>
<h2>Methods issues</h2>
<p>Strong data intuition can also help you 
spot issues with how the analysis was designed.
Things like: how did the author collect data? Is it a representative sample?
Do they need to have an experiment to establish causation?</p>
<p>Here's an example -
say an analysis reports that Firefox users who create a Firefox account
retain 10% higher than users who don't.
By default, a lot of folks interpret this to mean that
if we invest some time in helping users open accounts
we'll see an increase in retention.
Folks with stronger data intuition will instead 
recognize these results are just correlational (not causational).</p>
<p>Users who use the product a lot tend to stick around longer.
Users who open an account are more active users, thus they retain better.
Users who <em>crash</em> Firefox are more active users, and also retain <em>better</em>.</p>
<p>I think this intuition is more than just understanding statistics well.
A strong stats background can help me identify issues
when reading the <em>methods section of a white paper</em>.
Strong data intuition helps me determine how much I trust
results I hear about in a <em>news headline</em>.
Data intuition helps me establish whether results are
<a href="https://blog.harterrt.com/pub-true.html">true-enough</a>.</p>
<h2>More than Skepticism</h2>
<p>I almost defined data intuition as a type of skepticism,
but I think this is a bad characterization.
Skepticism over-focuses on disregarding results.</p>
<p>Intuition is more than being skeptical.
It's <strong>incorporating new data as part of a body of existing knowledge</strong>.
A lot of times, that means deciding new incoming data are inconsistent
and need more investigating before we can trust them.
But other times, it means changing our opinions in the face of new data
that are more authoritative than our existing body of knowledge.</p>
<h2>What do you think?</h2>
<p>I want to hear your thoughts on this.
I'm posting this definition publicly in part because I want to invoke
<a href="https://meta.wikimedia.org/wiki/Cunningham%27s_Law">Cunningham's Law</a>.
The best way to get to the right answer is to post the wrong answer!</p>
<p>Does this definition for data intuition resonate with you?
Am I missing something important? Let me know! 
My email is at the bottom of this page.</p>
<p>I'm spending the next few month building some self-service trainings
to help non-data people at Mozilla build data intuition.
I'd rather be wrong now than next year!</p>
  </div><p>
        Feel free to share any feedback via email!
        You can reach me at <code>harterrt</code> on gmail.
        Look forward to hearing from you!
    </p></div>]]>
            </description>
            <link>https://blog.harterrt.com/data_intuition.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24916868</guid>
            <pubDate>Wed, 28 Oct 2020 09:33:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Draw on a PDF Online]]>
            </title>
            <description>
<![CDATA[
Score 43 | Comments 27 (<a href="https://news.ycombinator.com/item?id=24916721">thread link</a>) | @perrys
<br/>
October 28, 2020 | https://www.goodannotations.com/tools/draw-on-pdf | <a href="https://web.archive.org/web/*/https://www.goodannotations.com/tools/draw-on-pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><h2>Replace pen and paper by drawing on PDF online</h2><p>Pen and papers are slow and inefficient. Leave the slow school tools in the past, quickly jot down, circle, and draw pointers on PDF documents. Draw your thoughts and markups on the screen as fast as they come to mind. The free pdf editor lets you use any stylus to draw on PDF online better and more effectively. Simply pick the right tool and the right color and draw online. </p><h2>Avoid friction and draw directly on your PDF document</h2><p>Less friction means more accomplishment. Taking your PDFs online and drawing with the right tools is fast, convenient, and easy to use. Don’t waste time with bulky software and confusing tool manuals. Focus on the message, not the technicals, and draw on PDF like a professional. </p><h2>Draw with a better markup and annotation PDF editor </h2><p>Take the lead on your project and stay in charge. Indicate and suggest what should be changed, improved, and deleted from any PDF document. Circle the problematic text, draw a pointer, and explain why that part of the PDF document should get more attention than the rest. Your team members will appreciate better digital communication.</p><h2>Deliver product guides and how-to material in PDF</h2><p>Create product guides that look and feel professional and trustworthy. Nothing says we’re a serious project better than a nicely done PDF catalog of how-to product guides. Take screenshots of your digital product, and draw on PDF online. Simply choose the PDF option when you download the file or share it with a link.</p><h2>Most PDF tools don’t let you draw in PDF documents </h2><p>Regular PDF software lets you only open and read the file. And even when you find an inexpensive and reasonably understandable pdf editor, you probably can’t add pointers, circles, squares, and other elements. Avoid the trouble, and upload your PDF to the best online PDF editor and simply jot the details down on PDF.</p><h2>Delete, change or reverse any PDF drawing online</h2><p>Never lose track of your work by having to start all over again after a careless mistake. Click on the bent arrow to undo any drawings on your document, and try again without damaging your design. Backspace and the trash bin icon will help you delete a specific element. Use the pointer to select a troublesome drawing and remove it with one click on the trash bin icon.</p><h2>Draw on PDF from any internet devices and gadget</h2><p>Tablets, mobile phones, and MacBooks are all fantastic devices to draw on PDF online. As long as you have an internet connection, you can draw your stylus, fingers, trackpad, and mouse on any PDF file. Jot down your ideas on the move, and share them digitally from anywhere and at any time. </p><h2>Draw for free with no trial accounts and data retention</h2><div><p>Don’t start a free month because you don’t need to pay to edit PDF documents. Enjoy smooth and frictionless PDF drawings wholly free and online. You can transform your PDF files into professional-looking project guides and give classy feedback to other project members with Good Annotations.</p><p>We’re thinking about premium features for different tools. Come back soon to discover optimized teamwork, collaborative editing, and organized file libraries. </p></div></div></div></div></div>]]>
            </description>
            <link>https://www.goodannotations.com/tools/draw-on-pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-24916721</guid>
            <pubDate>Wed, 28 Oct 2020 09:04:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Emerging JavaScript pattern: multiple return values]]>
            </title>
            <description>
<![CDATA[
Score 27 | Comments 20 (<a href="https://news.ycombinator.com/item?id=24916683">thread link</a>) | @loige
<br/>
October 28, 2020 | https://loige.co/emerging-javascript-pattern-multiple-return-values | <a href="https://web.archive.org/web/*/https://loige.co/emerging-javascript-pattern-multiple-return-values">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><p>In this article, I want to explore an interesting pattern that I am seeing more and more in JavaScript code which allows you to return multiple values from a function.</p>
<p>You probably know already that JavaScript does not support multiple return values natively, so this article will actually explore some ways to “simulate” this behavior.</p>
<p>One of the most famous usages of this pattern I have seen recently is within <a href="https://reactjs.org/docs/hooks-overview.html">React Hooks</a>, but before delving into that, let’s see what I mean with “multiple return values” by exploring this concept in other languages.</p>
<h2 id="multiple-return-values-in-other-languages"><a href="#multiple-return-values-in-other-languages" aria-label="multiple return values in other languages permalink"></a>Multiple return values in other languages</h2>
<p>Two languages that come to my mind which natively support multiple return values are Lua and Go. Let’s implement a simple <em>integer division</em> function that returns both the <em>quotient</em> and the <em>remainder</em>.</p>
<h3 id="lua"><a href="#lua" aria-label="lua permalink"></a>Lua</h3>
<p>Let’s start with a simple implementation in Lua. It’s definitely worth mentioning that <a href="https://www.lua.org/pil/5.1.html">Lua’s official documentation</a> defines multiple return values as <em>“An unconventional, but quite convenient feature”</em>:</p>
<div data-language="lua"><pre><code><span>function</span> <span>intDiv</span> <span>(</span>dividend<span>,</span> divisor<span>)</span>
  <span>local</span> quotient <span>=</span> math<span>.</span><span>floor</span><span>(</span>dividend <span>/</span> divisor<span>)</span>
  <span>local</span> remainder <span>=</span> dividend <span>%</span> divisor
  <span>return</span> quotient<span>,</span> remainder
<span>end</span>

<span>print</span><span>(</span><span>intDiv</span><span>(</span><span>10</span><span>,</span><span>3</span><span>)</span><span>)</span> </code></pre></div>
<h3 id="go"><a href="#go" aria-label="go permalink"></a>Go</h3>
<p>Here’s some equivalent code in Go:</p>
<div data-language="go"><pre><code><span>package</span> main

<span>import</span> <span>"fmt"</span>

<span>func</span> <span>intDiv</span><span>(</span>dividend<span>,</span> divisor <span>int</span><span>)</span> <span>(</span><span>int</span><span>,</span> <span>int</span><span>)</span> <span>{</span>
  quotient <span>:=</span> dividend <span>/</span> divisor
  remainder <span>:=</span> dividend <span>%</span> divisor
  <span>return</span> quotient<span>,</span> remainder
<span>}</span>

<span>func</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  fmt<span>.</span><span>Println</span><span>(</span><span>intDiv</span><span>(</span><span>10</span><span>,</span><span>3</span><span>)</span><span>)</span> 
<span>}</span></code></pre></div>
<p>As you can see in these 2 code snippets, functions can return more than 1 value and this can be very convenient in cases where you logically have produce multiple outputs in a computation.</p>
<p><strong>Note</strong>: a more realistic implementation in Go, would take into account errors (e.g. division by 0) and add an extra return value to propagate potential errors. We shouldn’t worry too much about this for the sake of this article, but it is definitely worth mentioning that multiple return values in Go shine when it comes to error propagation and error handling. We will touch a bit more on this later in this article to see how this idea can be applied to JavaScript as well, especially in the context of Async/Await.</p>
<h2 id="simulating-multiple-return-values-in-javascript"><a href="#simulating-multiple-return-values-in-javascript" aria-label="simulating multiple return values in javascript permalink"></a>Simulating multiple return values in JavaScript</h2>
<p>So, as we said early on, JavaScript does not natively support a syntax to return more than one value from a function. We can workaround this limitation by using <em>composite values</em> like arrays or objects.</p>
<h3 id="multiple-return-values-with-arrays"><a href="#multiple-return-values-with-arrays" aria-label="multiple return values with arrays permalink"></a>Multiple return values with arrays</h3>
<p>Let’s implement our <code>intDiv</code> in JavaScript by using arrays as return types:</p>
<div data-language="javascript"><pre><code><span>intDiv</span> <span>=</span> <span>(</span><span>dividend<span>,</span> divisor</span><span>)</span> <span>=&gt;</span> <span>{</span>
  <span>const</span> quotient <span>=</span> Math<span>.</span><span>floor</span><span>(</span>dividend <span>/</span> divisor<span>)</span>
  <span>const</span> remainder <span>=</span> dividend <span>%</span> divisor
  <span>return</span> <span>[</span>quotient<span>,</span> remainder<span>]</span>
<span>}</span>

console<span>.</span><span>log</span><span>(</span><span>intDiv</span><span>(</span><span>10</span><span>,</span> <span>3</span><span>)</span><span>)</span> </code></pre></div>
<p>Here we are just printing the result of a division, but let’s assume we want to handle the two return values individually, how do we <em>reference</em> those?</p>
<p>Well, the return value is an array so we can simply access the two elements in the array using the indices <code>0</code> and <code>1</code>:</p>
<div data-language="javascript"><pre><code><span>const</span> result <span>=</span> <span>intDiv</span><span>(</span><span>10</span><span>,</span> <span>3</span><span>)</span>
<span>const</span> quotient <span>=</span> result<span>[</span><span>0</span><span>]</span>
<span>const</span> remainder <span>=</span> result<span>[</span><span>1</span><span>]</span>
console<span>.</span><span>log</span><span>(</span><span><span>`</span><span>Quotient = </span><span><span>${</span>quotient<span>}</span></span><span>`</span></span><span>)</span> 
console<span>.</span><span>log</span><span>(</span><span><span>`</span><span>Remainder = </span><span><span>${</span>remainder<span>}</span></span><span>`</span></span><span>)</span> </code></pre></div>
<p>This syntax is arguably verbose and definitely not very elegant. Thankfully, <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Destructuring_assignment">ES2015 array destructuring assignment</a> can help us here:</p>
<div data-language="javascript"><pre><code><span>const</span> <span>[</span>quotient<span>,</span> remainder<span>]</span> <span>=</span> <span>intDiv</span><span>(</span><span>10</span><span>,</span> <span>3</span><span>)</span>
console<span>.</span><span>log</span><span>(</span><span><span>`</span><span>Quotient = </span><span><span>${</span>quotient<span>}</span></span><span>`</span></span><span>)</span> 
console<span>.</span><span>log</span><span>(</span><span><span>`</span><span>Remainder = </span><span><span>${</span>remainder<span>}</span></span><span>`</span></span><span>)</span> </code></pre></div>
<p>This is much nicer to read and we also trimmed away 2 out 3 lines of code, big win!</p>
<p>As nice as it is, this implementation has an important shortcoming: return values are positional, so you need to be careful and respect the order while destructuring.</p>
<h3 id="multiple-return-values-with-objects"><a href="#multiple-return-values-with-objects" aria-label="multiple return values with objects permalink"></a>Multiple return values with objects</h3>
<p>An alternative implementation could use objects as return value, let’s see how:</p>
<div data-language="javascript"><pre><code><span>intDiv</span> <span>=</span> <span>(</span><span>dividend<span>,</span> divisor</span><span>)</span> <span>=&gt;</span> <span>{</span>
  <span>const</span> quotient <span>=</span> Math<span>.</span><span>floor</span><span>(</span>dividend <span>/</span> divisor<span>)</span>
  <span>const</span> remainder <span>=</span> dividend <span>%</span> divisor
  <span>return</span> <span>{</span> quotient<span>,</span> remainder <span>}</span>
<span>}</span></code></pre></div>
<p>Note that here we are using another syntactic sugar from ES2015 (Enhanced object literal syntax) that allows us to define objects very concisely. Prior to ES2015, we would have defined the return statement as <code>{quotient: quotient, remainder: remainder}</code>.</p>
<p>With this approach we will be able to use our <code>intDiv</code> function as follows:</p>
<div data-language="javascript"><pre><code><span>const</span> result <span>=</span> <span>intDiv</span><span>(</span><span>10</span><span>,</span> <span>3</span><span>)</span>
<span>const</span> quotient <span>=</span> result<span>.</span>quotient
<span>const</span> remainder <span>=</span> result<span>.</span>remainder
console<span>.</span><span>log</span><span>(</span><span><span>`</span><span>Quotient = </span><span><span>${</span>quotient<span>}</span></span><span>`</span></span><span>)</span> 
console<span>.</span><span>log</span><span>(</span><span><span>`</span><span>Remainder = </span><span><span>${</span>remainder<span>}</span></span><span>`</span></span><span>)</span> </code></pre></div>
<p>Again, this is a bit too verbose and ES2015 has another fantastic syntactic sugar to make this nicer:</p>
<div data-language="javascript"><pre><code><span>const</span> <span>{</span> quotient<span>,</span> remainder <span>}</span> <span>=</span> <span>intDiv</span><span>(</span><span>10</span><span>,</span> <span>3</span><span>)</span>
console<span>.</span><span>log</span><span>(</span><span><span>`</span><span>Quotient = </span><span><span>${</span>quotient<span>}</span></span><span>`</span></span><span>)</span> 
console<span>.</span><span>log</span><span>(</span><span><span>`</span><span>Remainder = </span><span><span>${</span>remainder<span>}</span></span><span>`</span></span><span>)</span> </code></pre></div>
<p>This syntactic sugar is called <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Destructuring_assignment#Object_destructuring">Object Destructuring Assignment</a>. With this approach we are now independent from the position of return values (we can swap the position of <code>quotient</code> and <code>remainder</code> without side effects). This syntax also lets you rename the destructured variables, which can very useful to avoid name collisions with other local variables, or just to make variable names shorter or more descriptive as we please. Let’s see how this works:</p>
<div data-language="javascript"><pre><code><span>const</span> <span>{</span> remainder<span>:</span> r<span>,</span> quotient<span>:</span> q <span>}</span> <span>=</span> <span>intDiv</span><span>(</span><span>10</span><span>,</span> <span>3</span><span>)</span>
console<span>.</span><span>log</span><span>(</span><span><span>`</span><span>Quotient = </span><span><span>${</span>q<span>}</span></span><span>`</span></span><span>)</span> 
console<span>.</span><span>log</span><span>(</span><span><span>`</span><span>Remainder = </span><span><span>${</span>r<span>}</span></span><span>`</span></span><span>)</span> </code></pre></div>
<p>Here we are not dependent by values position, but by their names in the returned object. If you are designing an API with multiple return values, it’s up to you to figure out which trade off will be the best to guarantee a proper developer experience.</p>
<p>Ok, now you should have a good idea on how to simulate multiple return values in JavaScript. In the next section we will see some more realistic examples that take advantage of this pattern.</p>
<h2 id="some-more-realistic-use-cases"><a href="#some-more-realistic-use-cases" aria-label="some more realistic use cases permalink"></a>Some more realistic use cases</h2>
<p>As mentioned early on, this technique has been recently popularized by React hooks, so we are gonna explore this use case first. Later we will see other two cases related to Async/Await.</p>
<h3 id="react-hooks"><a href="#react-hooks" aria-label="react hooks permalink"></a>React Hooks</h3>
<p>React hooks are a <a href="https://reactjs.org/docs/hooks-overview.html">new feature proposal</a> available from <em>React v16.7.0-alpha</em> that lets you use state and other React features without having to write a class.</p>
<p>The first and most famous React hook present is called <strong>State Hook</strong>.</p>
<p>Let’s see how it works with an example, let’s build a CSS color viewer component.</p>
<p>Here’s how our component is going to look like:</p>
<p><img src="https://loige.co/content/4c8fe2a31135971902611822ca2f3df2/css-color-viewer-demo.gif" alt="CssColorViewer React component demo"></p>
<p>And here’s the code used to implement this:</p>
<div data-language="javascript"><pre><code><span>import</span> <span>{</span> useState <span>}</span> <span>from</span> <span>'react'</span>

<span>function</span> <span>CssColorViewer</span><span>(</span><span>)</span> <span>{</span>
  <span>const</span> <span>[</span>cssColor<span>,</span> setCssColor<span>]</span> <span>=</span> <span>useState</span><span>(</span><span>'Blue'</span><span>)</span>
  

  <span>const</span> <span>onCssColorChange</span> <span>=</span> <span>e</span> <span>=&gt;</span> <span>{</span>
    <span>setCssColor</span><span>(</span>e<span>.</span>target<span>.</span>value<span>)</span>
  <span>}</span>

  <span>return</span> <span>(</span>
    <span>&lt;</span>div<span>&gt;</span>
      <span>&lt;</span>input value<span>=</span><span>{</span>cssColor<span>}</span> onChange<span>=</span><span>{</span>onCssColorChange<span>}</span> <span>/</span><span>&gt;</span>
      <span>&lt;</span>div
        style<span>=</span><span>{</span><span>{</span>
          width<span>:</span> <span>100</span><span>,</span>
          height<span>:</span> <span>100</span><span>,</span>
          background<span>:</span> cssColor<span>,</span>
        <span>}</span><span>}</span>
      <span>/</span><span>&gt;</span>
    <span>&lt;</span><span>/</span>div<span>&gt;</span>
  <span>)</span>
<span>}</span></code></pre></div>
<p>You can see this component in action and play with the code on <a href="https://codesandbox.io/s/9lzyov54lr">CodeSandbox</a>.</p>
<p>For the sake of this article, we are going to focus only on the <code>useState</code> call, but if you are curious to understand better how the hook itself works internally I really recommend you read the <a href="https://reactjs.org/docs/hooks-state.html">official State Hook documentation</a>. I was personally curious to understand how multiple <code>useState</code> calls could maintain the relationship with the specific state attribute (since there’s no explicit labelling or reference). If you are curious about that too, well you should read the <a href="https://reactjs.org/docs/hooks-faq.html#how-does-react-associate-hook-calls-with-components">Hooks FAQ</a> and <a href="https://medium.com/@dan_abramov/making-sense-of-react-hooks-fdbde8803889">Dan Abramov’s recent article about Hooks</a>.</p>
<p>Back to the <code>useState</code> call in our example, now!</p>
<p>The <code>useState</code> hook acts like a factory: given a default value for the state property (<code>'Blue'</code> in our case), it will need to instantiate for you 2 things:</p>
<ul>
<li>the current value for the specific state property (<code>cssColor</code> in our case)</li>
<li>a function that allows you to alter the specific property (<code>setCssColor</code> in our case)</li>
</ul>
<p>React developers decided to handle this requirement by simulating multiple return values with an array.</p>
<p>Combining this with array destructuring and proper variable naming, the result is an API that is very nice to read and to use.</p>
<p>This React feature is still very experimental and subject to change at the time of writing, but it already sounds like a big deal for the React community to make the code more expressive and reduce the barrier to entry to start adopting React.</p>
<p>The point I want to make is that, in this specific case, the multiple return values pattern plays a big role towards this goal.</p>
<h3 id="converting-callbacks-api-to-asyncawait"><a href="#converting-callbacks-api-to-asyncawait" aria-label="converting callbacks api to asyncawait permalink"></a>Converting callbacks API to Async/Await</h3>
<p>Recently I found another great use case for the multiple return values pattern while trying to convert a callback oriented API into an equivalent Async/Await API.</p>
<p>To make this part clear, I am going to explain very quickly an approach I use to convert callback based APIs into functions that I can use with Async/Await.</p>
<p>Let’s take this generic example:</p>
<div data-language="javascript"><pre><code><span>function</span> <span>doSomething</span><span>(</span><span>input<span>,</span> callback</span><span>)</span> <span>{</span>
  
  
  
  <span>callback</span><span>(</span>error<span>,</span> response<span>)</span>
<span>}</span></code></pre></div>
<p>To convert this function into something that can be used with Async/Await we have to essentially <a href="https://loige.co/to-promise-or-to-callback-that-is-the-question/"><em>promisify</em></a> it. There are libraries to do it and, if you are using Node.js you can even use the builtin <a href="https://nodejs.org/api/util.html#util_util_promisify_original"><code>util.promisify</code></a>, but that’s something we can do ourselves by just creating a <em>wrapper</em> function like the following one:</p>
<div data-language="javascript"><pre><code><span>const</span> <span>doSomethingPromise</span> <span>=</span> <span>(</span><span>input</span><span>)</span> <span>=&gt;</span> <span>new</span> <span>Promise</span><span>(</span><span>(</span><span>resolve<span>,</span> reject</span><span>)</span> <span>=&gt;</span> <span>{</span>
  <span>doSomething</span><span>(</span>input<span>,</span> <span>(</span><span>error response</span><span>)</span> <span>=&gt;</span> <span>{</span>
    <span>if</span> <span>(</span>error<span>)</span> <span>{</span>
      <span>return</span> <span>reject</span><span>(</span>error<span>)</span>
    <span>}</span>

    <span>return</span> <span>resolve</span><span>(</span>response<span>)</span>
  <span>}</span><span>)</span>
<span>}</span><span>)</span></code></pre></div>
<p>In short, our wrapper function <code>doSomethingPromise</code> is immediately returning a <code>Promise</code>. Inside the body of the promise we are invoking the original <code>doSomething</code> function with a callback that will be resolving or rejecting the promise based on whether there’s an <code>error</code> or not.</p>
<p>Now we can finally take advantage of Async/Await:</p>
<div data-language="javascript"><pre><code>
<span>const</span> response <span>=</span> <span>await</span> <span>doSomethingPromise</span><span>(</span>input<span>)</span></code></pre></div>
<p><strong>Note</strong>: this will throw in case of error, so make sure you have it in a <code>try/catch</code> block to handle the error correctly.</p>
<blockquote>
<p>If you are curious about <em>promisifying</em> callback-based functions, I have <a href="https://loige.co/to-promise-or-to-callback-that-is-the-question/">an entire article</a> dedicated to this topic.</p>
</blockquote>
<p>In my specific use case, I was using a <a href="https://www.npmjs.com/package/twitter">twitter client</a> library that follows this conventions:</p>
<div data-language="javascript"><pre><code>
client<span>.</span><span>get</span><span>(</span><span>'statuses/user_timeline'</span><span>,</span> params<span>,</span> <span>function</span> <span>callback</span>…</code></pre></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://loige.co/emerging-javascript-pattern-multiple-return-values">https://loige.co/emerging-javascript-pattern-multiple-return-values</a></em></p>]]>
            </description>
            <link>https://loige.co/emerging-javascript-pattern-multiple-return-values</link>
            <guid isPermaLink="false">hacker-news-small-sites-24916683</guid>
            <pubDate>Wed, 28 Oct 2020 08:56:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What Was Remote YC Like?]]>
            </title>
            <description>
<![CDATA[
Score 81 | Comments 7 (<a href="https://news.ycombinator.com/item?id=24916629">thread link</a>) | @crowdhailer
<br/>
October 28, 2020 | https://www.richardesigns.co.uk/2020/10/27/what-was-remote-yc-like.html | <a href="https://web.archive.org/web/*/https://www.richardesigns.co.uk/2020/10/27/what-was-remote-yc-like.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
  <nav>
    <a href="https://www.richardesigns.co.uk/">Home</a>
  </nav>
  
  <p>Summer 2020 saw the first completely remote YC batch in history.  My co-founder Peter and I were fortunate enough to participate with our company <a href="https://plummail.co/">Plum Mail</a> and we did so entirely remotely from our homes in South West England.</p>

<p>What was it like you ask?  Essentially, it was like taking an e-learning course. This had a number of benefits for us as a team, which are totally valid, but a number of disadvantages surface as you progress through the process.</p>

<p>The whole YC team did a genuinely amazing job of making the experience hugely valuable, exciting and accessible.  As a founder, your challenge is to step up to the plate and embrace the process if you want to get the most out of it.  Being aware of the disadvantages of the remote batch means you can come up with creative ways to get around them.</p>

<p>Hopefully, this blog will help you feel ready to embrace a remote YC batch, so here’s what doing a remote YC batch is really like.</p>

<h2 id="1-the-interview">1. The Interview</h2>

<p>This is your first remote YC experience.  It’s on a Zoom call that will last around fifteen minutes.  You’re probably feeling nervous and it may be a very weird time of day depending on your time zone.  For us, the interview was at 5pm BST / 9am PST so I had all day to get worked up about it!</p>

<p>For some of our batchmates, calls were at even less convenient times (try midnight) although YC are sensitive to this and will try to accommodate speaking to you at a time that suits everyone on the call.</p>

<p>This combination of timezone conflict and the usual pre-interview nerves are kinda toxic on a video call. You have to work hard to come across as calm, confident and competent.</p>

<p>It felt like sitting in a TV studio waiting to go on air. There is no time for pleasantries as you might expect in person. There is no time for technical errors either. In fact, my internet dropped out at the very end of the call but I only missed a few seconds of the conversation.</p>

<p>I wish deeply that I had gotten used to doing video calls before the interview. Weird as that sounds mid-pandemic, before Covid I had barely done 10 video calls my entire life and now I was doing arguably the most valuable Zoom call I might ever do.</p>

<p>All in all it was quite a stressful experience that I could have aced with some simple steps. Having said that, we were accepted into the batch so it can’t have been that bad.</p>

<h3 id="yc-interview-takeaways">YC Interview Takeaways</h3>

<ol>
  <li>
    <p>Get comfortable on video calls.  Take a moment to figure out where best for a clean background.  Sit or stand facing a window or other source of light so you don’t look like a shawdow puppet.  Aim for getting your whole head, shoulders and upper body into the shot roughly square on.</p>
  </li>
  <li>
    <p>Chill before the call.  The interviewer just wants to have a chat with you about you and your product.  Nervous people tend to ramble.  If you can come across as calm and confident you’ll have a more natural, authentic conversation.</p>
  </li>
  <li>
    <p>Smile. Be yourself.</p>
  </li>
  <li>
    <p>If the interview slot is at 2am local time ask them to consider re-arranging it.  No one is their best at 2am, unless you are your best at 2am in which case, I guess 2am is fine. Don’t be afraid to ask nicely if the interview time is massively inconvenient for you.</p>
  </li>
  <li>
    <p>Jazz up your internet connection.  You’ll be doing a lot of video calls if you get into the batch and at remote demo day, a sturdy, fast internet connection is essential so why not start now and have a decent connection for the interview too.</p>
  </li>
</ol>

<h2 id="2-your-remote-working-arrangements">2. Your remote working arrangements</h2>

<p>Peter and I are used to working remote with meetings every so often. However, local restrictions meant we couldn’t always meet up when we needed to.  This definitely created issues for us in terms of communication.  We would try to use instant messenger but found this quickly descended into hours of wasted typing when a phone call or meeting would have been much faster.</p>

<p>Working remote for me also equalled working alone most of the day.  I started out with great focus but would often go astray as my thoughts outpaced my ability to deliver.</p>

<p>This process was compounded during the batch because of the pressure to deliver impressive progress. During the batch you are expected to pull out all the stops to get progress made and I think this is reasonable and exciting.  YC is a once (sometimes twice, rarely three times) in a lifetime opportunity.</p>

<p>Overtime, we got good at working remotely in an efficient and productive way but it took a couple of tries to get it right.</p>

<p>What works for us is a daily catch up call at the same time every day. There is always plenty to talk about and it’s something to hold onto if you’re feeling adrift.  Every Friday we travel and get together (assuming local restrictions allow it) for a strategy day.</p>

<p>Time-boxing strategy talk to Fridays is a very efficient way to timebox ‘big chat’ so that the other four days a week your focus is on actions.</p>

<p>This aspect of the batch experience is probably the most different to an in-person batch where you’re living in a flat with your founders and you eat, sleep, breathe your company.</p>

<p>I’d say it’s totally worth optimising your remote working arrangements so you can make the most of the productive time you have during the batch. The more progress you make, the more you’ll be able to leverage out of the process as the weeks go by.</p>

<h2 id="3-bootcamp">3. Bootcamp</h2>

<p>The first two weeks of remote YC are intense.  Somehow it seemed more intense from the comfort of my own home.  Bootcamp introduces you to an almost overwhelming number of talks and presentations that cover pretty much every aspect of running a startup.</p>

<p>It’s really exciting!  It’s also all on-screen.  Pretty much everything is on video call from here on in.</p>

<p>Each day of bootcamp typically starts with a rousing introduction by a YC partner followed by a series of talks by YC Alum, well-known entrepreneurs or investors and after all of that you are thrust into a video call with a random selection of batchmates to discuss what you just heard.</p>

<p>It all takes around three hours.</p>

<p>The content was all hugely valuable.  Somehow it felt harder to access the value remotely. When you’re watching someone speak live it’s somehow more immersive, over video call I felt disconnected and at times distracted.</p>

<p>However, it is much easier to make notes if you’re sat at a desk listening to the talk rather than trying to jot onto a notepad perched on your knee in a lecture theatre.</p>

<p>And that’s what made the remote bootcamp awesome for me.  I started each talk with a plan to make notes throughout.  This gave me a task to do to keep me interested and the output remains a valuable resource on which to draw.</p>

<p>Some of the insights you pick up might not seem relevant at the time but six months in they suddenly make perfect sense.</p>

<p>If you’re in a non Pacific time zone, like I was, be kind to yourself and take a little time off-screen before the video calls kick in.  Bootcamp will be much easier if you’re feeling fresh and don’t have eye-strain already from an 8 hour day coding.</p>

<p>For me, bootcamp started at 5pm BST so I typically took an hour or so from 4pm away from the computer to get some air, a little food and exercise before the talks began.  A few times I worked through and hopped into bootcamp last minute.  That’s when I felt least engaged largely because I was through with working for the day before the bootcamp had even started.</p>

<h2 id="4-serendipity">4. Serendipity</h2>

<p>It’s not what you know, it’s who you know.  This is true as far as I’m concerned and to be candid, a remote YC batch does not lend itself to chance-encounters and lunch-queue friendships.</p>

<p>Those ‘water cooler moments’ obviously don’t happen so, YC tried to create virtual equivalents of this magic sauce.  This is so important because your batchmates are potential customers, employees, introducers, a shoulder to cry on and someone to celebrate with.</p>

<p>After bootcamp and dinner talks, YC hosts video calls with up to seven participants in each one and you’re dropped randomly into one.  What you get are seven people who don’t know each other but have two things in common: 1, they are a company founder in your batch and 2, they just watched the same talk as you.</p>

<p>At times these were extremely awkward with no one speaking or maybe one person dominating the conversation completely.  However, some of the calls were really engaging and it’s a good chance to get to know a few folks.   Soon enough you make friends and you’ll be setting up video calls for virtual coffee before you can say pumpkin latte.</p>

<p>YC also invited us to participate in random founder pairings in which two founders were randomly introduced to each other and invited to have a 30 minute call.  This resulted in some awesome friendships that continue to add value on a personal and professional level.</p>

<p>These moments of serendipity aren’t really serendipitous at all, they are forced, however, they do a good job of replacing the serendipity where it just isn’t practical to have 400 people in the same room at the moment.</p>

<p>So, embrace it.  Yes it’s slightly forced but approach it with an open mind and an open heart and you’ll be pleasantly surprised.</p>

<h2 id="5-office-hours">5. Office Hours</h2>

<p>Every two weeks we were invited to participate in a video call always with the same group of people for what YC call Group Office Hours.  It’s like group therapy.  You get to know the folks in your group probably more intimately than any of your other batchmates.</p>

<p>Since you’ll be spending a lot of time with these guys on video calls, I would definitely reach out to them outside of office hours to say hello and to allow yourself time to get to know each other.</p>

<p>This really helped me get more out of the office hours.  We’re all probably a little shy around new people but if you get to know everyone better, you’ll be able to access more value from the process faster.  It just requires more effort remotely because you can’t read people so easily.</p>

<h2 id="6-success-and-failure">6. Success and Failure</h2>

<p>Any startup journey is a rollercoaster.  When we enjoyed success …</p></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.richardesigns.co.uk/2020/10/27/what-was-remote-yc-like.html">https://www.richardesigns.co.uk/2020/10/27/what-was-remote-yc-like.html</a></em></p>]]>
            </description>
            <link>https://www.richardesigns.co.uk/2020/10/27/what-was-remote-yc-like.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24916629</guid>
            <pubDate>Wed, 28 Oct 2020 08:47:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Qt 6 will provide additional libraries via Conan package manager]]>
            </title>
            <description>
<![CDATA[
Score 42 | Comments 35 (<a href="https://news.ycombinator.com/item?id=24916321">thread link</a>) | @alaenix
<br/>
October 28, 2020 | https://www.qt.io/blog/qt-6-additional-libraries-via-package-manager | <a href="https://web.archive.org/web/*/https://www.qt.io/blog/qt-6-additional-libraries-via-package-manager">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                            

                            <p>
                                Tuesday October 27, 2020 by <a href="https://www.qt.io/blog/author/iikka-eklund">Iikka Eklund</a> | <a href="#commento">Comments</a>
                            </p>
                            
                            <p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p><span data-contrast="auto">With Qt 6 we want to provide more flexibility via </span><span data-contrast="auto">leveraging</span><span data-contrast="auto"> a package manager in addition to Qt </span><span data-contrast="auto">Online </span><span data-contrast="auto">Installer. The new package manager functionality, based on conan.io (</span><a href="https://conan.io/"><span data-contrast="none">https://conan.io</span></a><span data-contrast="auto">), allows provi</span><span data-contrast="auto">ding more </span><span data-contrast="auto">packages </span><span data-contrast="auto">to the users without increasing the complexity of the baseline Qt. In addition to the </span><span data-contrast="auto">packages </span><span data-contrast="auto">provided by Qt, the package manager can be used for getting content from other sources.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></p>
<!--more-->
<p><span data-contrast="auto">Initially</span><span>,</span><span data-contrast="auto"> we have three </span><span data-contrast="auto">Additional Li</span><span data-contrast="auto">b</span><span data-contrast="auto">raries </span><span data-contrast="auto">provided via the package manager: Qt Network Authorization, Qt Image Formats</span><span>,</span><span data-contrast="auto"> and Qt 3D. More </span><span data-contrast="auto">Additional Libraries </span><span data-contrast="auto">will be available </span><span data-contrast="auto">in forthcoming</span><span data-contrast="auto"> Qt 6 releases. We are currently leveraging the exis</span><span data-contrast="auto">ting Qt delivery system as</span><span data-contrast="auto"> the</span> <span data-contrast="auto">backend for the </span><span data-contrast="auto">Additional Libraries</span><span data-contrast="auto"> available via the package manager.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></p>
<p><strong>How the packages are managed?&nbsp;</strong></p>
<p><span data-contrast="auto">The required </span><span data-contrast="auto">tools, Conan, </span><span data-contrast="auto">CMake</span><span>,</span><span data-contrast="auto"> and Ninja, can be easily installed u</span><span data-contrast="auto">sing</span><span data-contrast="auto"> the</span> <span><strong>Qt </strong></span><strong><span data-contrast="auto">O</span></strong><strong><span data-contrast="auto">nline installer </span></strong><strong><span data-contrast="auto">4.0</span></strong><span data-contrast="auto">, which is going to be released </span><span data-contrast="auto">soon.</span> <span data-contrast="auto">The</span><span data-contrast="auto"> Conan</span><span data-contrast="auto"> build </span><span data-contrast="auto">recipes</span> <span data-contrast="auto">for </span><span data-contrast="auto">Additional Libraries</span><span data-contrast="auto"> require </span><span data-contrast="auto">CMake</span><span data-contrast="auto"> and Ninja to build the module</span><span data-contrast="auto">.</span> <span data-contrast="auto">The project linking to the module</span> <span data-contrast="auto">can be </span><span data-contrast="auto">qmake</span><span data-contrast="auto">-</span><span data-contrast="auto">based as well.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></p>
<p><span data-contrast="auto">Once installed</span><span data-contrast="auto">, </span><span data-contrast="auto">the selected </span><span data-contrast="auto">Add</span><span data-contrast="auto">itional Libraries</span> <span data-contrast="auto">can be built </span><span data-contrast="auto">once </span><span data-contrast="auto">by </span><span data-contrast="auto">using</span><span data-contrast="auto"> Conan</span><span data-contrast="auto"> per selected target configuration</span><span data-contrast="auto">. After the build</span><span data-contrast="auto">,</span><span data-contrast="auto"> the binary package is available in </span><span data-contrast="auto">user’s</span><span data-contrast="auto"> local</span><span data-contrast="auto"> Conan</span><span data-contrast="auto"> cache</span><span data-contrast="auto">, and can be </span><span data-contrast="auto">linked to any other project</span><span data-contrast="auto">.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;<br></span><span></span></p>
<p><span><strong>How to get and build the packages?&nbsp;</strong><br></span><span data-contrast="auto"></span></p>
<p><span data-contrast="auto">An example build call</span> <span data-contrast="auto">look</span><span data-contrast="auto">s</span><span data-contrast="auto"> like this:</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></p>
<pre>$conan.exe install qtnetworkauth/6.0.0@qt/beta --build=missing <br>--profile=&lt;QtSdk&gt;/Tools/Conan/profiles/qt-6.0.0-msvc2019_64 -s <br>build_type=Release -g cmake_paths -g=cmake -g deploy&nbsp;</pre>

<p><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">Now, let's look what that contains:</span></p>
<ul>
<li data-leveltext="" data-font="Symbol" data-listid="2" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><span data-contrast="auto">qtnetworkauth</span><span data-contrast="auto">/6.0.0@qt/beta</span><span data-ccp-props="{&quot;134233279&quot;:true,&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span>
<ul>
<li data-leveltext="" data-font="Symbol" data-listid="2" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><span data-contrast="auto">This is the</span><span data-contrast="auto"> Conan</span><span data-contrast="auto"> reference for the package</span></li>
<li data-leveltext="" data-font="Symbol" data-listid="2" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><span data-contrast="auto">You can search available packages in your </span><span data-contrast="auto">C</span><span data-contrast="auto">onan cache by:</span><span data-ccp-props="{&quot;134233279&quot;:true,&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;&nbsp;</span><span data-contrast="auto">$conan</span><span data-contrast="auto">.exe </span><span data-contrast="auto">search</span><span data-ccp-props="{&quot;134233279&quot;:true,&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></li>
</ul>
</li>
<li data-leveltext="" data-font="Symbol" data-listid="2" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><span data-contrast="none">--profile</span><span data-ccp-props="{&quot;134233279&quot;:true,&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span>
<ul>
<li data-leveltext="" data-font="Symbol" data-listid="2" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><span data-contrast="none">This file is installed by the Qt installer. Each </span><span data-contrast="none">Qt 6 E</span><span data-contrast="none">ssential package installed by the Qt </span><span data-contrast="none">I</span><span data-contrast="none">nstaller</span><span data-contrast="none"> installs also a matching profile file. This tells</span><span data-contrast="none"> Conan</span><span data-contrast="none"> the target build configuration.</span><span data-ccp-props="{&quot;134233279&quot;:true,&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></li>
<li data-leveltext="" data-font="Symbol" data-listid="2" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><span data-contrast="none">The user needs to select</span><span data-contrast="none"> a </span><span data-contrast="none">suitable profile</span><span data-contrast="none">, that is </span><span data-contrast="none">the target build configuration.</span><span data-ccp-props="{&quot;134233279&quot;:true,&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></li>
</ul>
</li>
<li data-leveltext="" data-font="Symbol" data-listid="2" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><span data-contrast="none">-g</span><span data-ccp-props="{&quot;134233279&quot;:true,&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span>
<ul>
<li data-leveltext="" data-font="Symbol" data-listid="2" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><span data-contrast="none">If your consuming project is a </span><span data-contrast="none">CMake</span> <span data-contrast="none">project</span><span data-contrast="none"> the</span><span data-contrast="none">n</span><span data-contrast="none"> use the</span> <span data-contrast="none">CMake</span><span data-contrast="none"> generators</span><span data-ccp-props="{&quot;134233279&quot;:true,&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></li>
<li data-leveltext="" data-font="Symbol" data-listid="2" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><span data-contrast="none">If your consuming project is</span><span data-contrast="none"> a</span> <span data-contrast="none">qmake</span> <span data-contrast="none">project</span><span data-contrast="none"> then you can pass: -g </span><span data-contrast="none">qmake</span><span data-ccp-props="{&quot;134233279&quot;:true,&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></li>
<li data-leveltext="" data-font="Symbol" data-listid="2" aria-setsize="-1" data-aria-posinset="1" data-aria-level="1"><span data-contrast="none">The “deploy” generator deploys the buil</span><span data-contrast="none">t</span><span data-contrast="none"> A</span><span data-contrast="none">dditional Library</span><span data-contrast="none"> from </span><span data-contrast="none">the</span><span data-contrast="none"> Conan</span><span data-contrast="none"> cache to your working environment</span><span data-contrast="none">. This is useful if you </span><span data-contrast="none">want to bundle your application files together.</span><span data-ccp-props="{&quot;134233279&quot;:true,&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></li>
</ul>
</li>
</ul>
<p><span data-contrast="none">For detailed steps see the <a data-insert="true" href="https://wiki.qt.io/Qt6_Add-on_src_package_build_using_Conan_package_manager" rel="noopener">instructions</a>.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></p>
<p><span data-contrast="none">Currently</span><span data-contrast="none">,</span><span data-contrast="none"> Qt </span><span data-contrast="none">Online </span><span data-contrast="none">Installer exports the A</span><span data-contrast="none">dditional Library packages (sources and build recipes)</span><span data-contrast="none"> into the</span><span data-contrast="none"> Conan</span><span data-contrast="none"> cache. There is no </span><span data-contrast="none">C</span><span data-contrast="none">onan remote that hosts the Add</span><span data-contrast="none">itional Library</span> <span data-contrast="none">C</span><span data-contrast="none">onan packages</span><span data-contrast="none">.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></p>
<p aria-level="2"><strong>Next steps&nbsp;</strong></p>
<p><span data-contrast="none">Like Qt 6.0, the current work is still in beta phase and </span><span data-contrast="none">all </span><span data-contrast="none">feedback is welcome</span><span data-contrast="none">.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;Note that currently the Conan profile files and build recipies for&nbsp;</span><span data-contrast="none">Android and iOS targets are being worked on</span><span data-contrast="none">.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;Also, t</span><span data-contrast="none">he build recipes of the </span><span data-contrast="none">Additional Libraries</span><span data-contrast="none"> are not part of the repositories yet. </span><span>&nbsp;</span><span data-contrast="none">Once the build recipes</span><span data-contrast="none"> are mature</span><span data-contrast="none"> the plan is to move those into module repositories.</span><span>&nbsp;</span></p>
<p><span></span><span data-contrast="none">If you want to have a look already now</span><span data-contrast="none">,</span><span data-contrast="none"> how</span><span data-contrast="none"> the conanfile.py recipes </span><span data-contrast="none">look like</span><span data-contrast="none">,</span> <span data-contrast="none">those </span><span data-contrast="none">can be found </span><span data-contrast="none">in</span> <span data-contrast="none">the </span><span data-contrast="none">Qt installation, under each module in “</span><span data-contrast="none">AdditionalLibraries</span><span data-contrast="none">/Qt</span><span data-contrast="none">/”</span><span data-contrast="none">.</span><span data-ccp-props="{&quot;201341983&quot;:0,&quot;335559739&quot;:160,&quot;335559740&quot;:259}">&nbsp;</span></p>
&nbsp;</span></p>
                            
                            
                                <hr>
                          
                                <h6>Blog Topics:</h6>        
                                
                            


                        </div></div>]]>
            </description>
            <link>https://www.qt.io/blog/qt-6-additional-libraries-via-package-manager</link>
            <guid isPermaLink="false">hacker-news-small-sites-24916321</guid>
            <pubDate>Wed, 28 Oct 2020 07:55:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Tracing Kernel Functions: FBT stack() and arg]]>
            </title>
            <description>
<![CDATA[
Score 25 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24915942">thread link</a>) | @moks
<br/>
October 27, 2020 | https://zinascii.com/2020/fbt-args-and-stack.html | <a href="https://web.archive.org/web/*/https://zinascii.com/2020/fbt-args-and-stack.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

      
      <p>Oct 27, 2020</p>

      <p>
	In my <a href="https://zinascii.com/2020/the-amd64-fbt-handler.html">previous
	post</a> I described how FBT intercepts function calls and
	vectors them into the DTrace framework. That laid the
	foundation for what I want to dicuss in this post: the
	implementation of the <code>stack()</code> action and
	built-in <code>arg</code> variables. These features rely on
	the precise layout of the stack, the details of which I
	touched on previously. In this post I hope to illuminate those
	details a bit more with the help of some visuals, and then
	guide you through the implentation of these two DTrace
	features as they relate to the FBT provider.
      </p>

      <h2>A Correction</h2>

      <p>
	But first I must make a correction to my last post. It turns
	out the FBT handler <b>does not</b> execute on the IST stack.
	It runs on either the thread’s stack or the CPU’s high-level
	interrupt stack depending on the context of the kernel
	function call, but never on the IST.
	Rather, <a href="https://en.wikipedia.org/wiki/Kernel_page-table_isolation">KPTI</a>
	uses the IST stack as a scratch space to perform its
	trampoline into the real handler. This little detail is
	important. Functions like <code>dtrace_getpcstack()</code>
	have zero chance of working if run with the IST stack, for
	reasons which become obvious later. This also explains why the
	AMD64 handler pulls down the stack
	during <code>pushq&nbsp;%RBP</code> emulation: if it’s working
	on the same stack as the thread/interrupt, then it must make
	room for <code>RBP</code>. I can explain better with a visual.
	First, the diagram from the last post.
      </p>

      <figure>
	  
	  <figcaption><a name="fig1-int3-pre-handler">Figure 1. INT3 thread/interrupt state pre-handler</a></figcaption>
      </figure>

      <p>
	On the left we have a kernel thread, interrupt thread, or
	high-level interrupt running on CPU. On the right we have the
	“interrupt context” of the breakpoint exception, using the
	IST. The image is correct in that there are two different
	stacks in play, but what’s running on the right-hand side is
	not the <code>brktrap</code> handler. The right-hand side is
	running the KPTI trampoline, ensuring a CR3 switch when moving
	between the user/kernel boundary. The trampoline also provides
	a facsimile of the processor frame to the interrupted thread’s
	stack, making it none the wiser that KPTI was ever on the
	scene. So all the action happens on the left side, but what
	does the stack look like as we transition through the #BP
	handler on our way to <code>dtrace_invop()</code>?
      </p>

      <figure>
	  
	  <figcaption><a name="fig2-pre-fbt-stack">Figure 2. stack state from #BP to pre dtrace_invop()</a></figcaption>
      </figure>

      <p>
	In phase ① <code>mac_provider_tx()</code> is
	calling <code>mac_ring_tx()</code> while it is under FBT entry
	instrumentation. The last thing on the thread’s stack is the
	return address, and the CPU is about to execute
	the <code>int3</code> instruction.
      </p>

      <p>
	Phase ② is immediately after the CPU has finished execution of
	the <code>int3</code> instruction. The processor (via the
	spectre of the KPTI trampoline) has pushed a 16-byte aligned
	processor frame on the stack and has vectored into
	the <code>brktrap()</code> handler.
      </p>

      <p>
	Phase ③ is after some amount of execution of
	the <code>brktrp()</code> and <code>invoptrap()</code>
	handlers—remember, the #BP handler for DTrace mimics a #UD.
	This last phase shows the state just before the call
	to <code>dtrace_invop()</code>. At this point we’ve grown an
	entire <code>regs</code> structure on the stack and stashed a
	copy of the return address on top of this. The later used to
	populate <code>cpu_dtrace_caller</code>, a variable which
	becomes important later.
      </p>

      <h2>The stack() Action</h2>

      <p>
	The separation of probes and actions is a vital aspect of
	DTrace’s architecture. A firm boundary between these two makes
	DTrace more powerful than it ever could be if they were
	tightly coupled. Think about it, I can ask for the call stack
	in any probe, not just the probes that deem that information
	useful. The probes give you access to a context, and the
	actions give you access to data in that context. To limit the
	execution of actions to specific probes would limit the
	questions you can ask about the system. With this design the
	number of questions you can ask is virtually endless. And it
	turns out one of the more useful questions to ask is: “what
	the hell is running on my CPU”?
      </p>

      <p>
	The <code>stack()</code> action allows you to record the call
	stack that lead to the probe site. In the context of FBT this
	will record the call stack of the kernel thread or interrupt
	executing an entry or return from this kernel function. You
	can also access the userland stack of a thread
	via <code>ustack()</code>, but I don’t cover that here.
      </p>

      <p>
	The <code>stack()</code> action is implemented by
	the <code>dtrace_getpcstack()</code> function. To get there
	from <code>dtrace_invop()</code> requires a couple of more
	calls in the DTrace framework. Ultimately, the call stack to
	get there looks like this.
      </p>

      <figure>
	<div>
	  <pre><code>dtrace_getpcstack()
dtrace_probe()
fbt_invop()
dtrace_invop()
dtrace_invop_callsite() &lt;aka invoptrap&gt;
&lt;rest of call stack that lead here&gt;</code></pre>
	</div>
	<figcaption>call stack between dtrace_getpcstack() and dtrace_invop()</figcaption>
      </figure>

      <p>
	The implementation of <code>stack()</code> really starts
	with <code>DTRACEACT_STACK</code> inside
	of <code>dtrace_probe()</code>.
      </p>

      <figure>
	<figcaption><a href="https://github.com/illumos/illumos-gate/blob/007ca33219ffdc49281657f5f8a9ee1bbfc367ab/usr/src/uts/common/dtrace/dtrace.c#L7184-L7191">usr/src/uts/common/dtrace/dtrace.c</a></figcaption>
	<figure>
	  <div>
<pre><span><span></span><code>			case DTRACEACT_STACK:</code></span>
<span><span></span><code>				if (!dtrace_priv_kernel(state))</code></span>
<span><span></span><code>					continue;</code></span>
<span><span></span><code></code></span>
<span><span></span><code>				dtrace_getpcstack((pc_t *)(tomax + valoffs),</code></span>
<span><span></span><code>				    size / sizeof (pc_t), probe-&gt;dtpr_aframes,</code></span>
<span><span></span><code>				    DTRACE_ANCHORED(probe) ? NULL :</code></span>
<span><span></span><code>				    (uint32_t *)arg0);</code></span></pre>
	  </div>
	  <figcaption>stack() action implementation found in dtrace_probe()</figcaption>
	</figure>
      </figure>

      <p>
	The first argument is the address of the array used to store
	program counter values (aka function pointers). This array
	starts at some offset into the current DTrace buffer. The
	second argument if the size of that array. The third argument
	is the number of “artificial frames” on the stack, more on
	this later. The fourth argument is used to determine if the
	first (topmost) program counter in the call stack is the value
	passed in <code>arg0</code> to <code>dtrace_probe()</code>. An
	“anchored” probe is one that has a function name specified
	when calling <code>dtrace_probe_create()</code>. For example,
	the FBT provider uses the name of the kernel function as the
	probe’s function name, thus it is anchored on the kernel
	function. The profile provider, however, specifies no probe
	function name; it is not anchored and is a bit of a special
	case. I address this at the end of the post.
      </p>

      <p>
	This brings us to the <code>dtrace_getpcstack()</code>
	function. But first I’ll expand
	on <a href="#fig2-pre-fbt-stack">figure 2</a> to show our
	stack state as of source line 60 of the function.
      </p>

      <figure>
	
	<figcaption>
	  <a name="fig3-start-of-dtrace_getpcstack">Figure 3. start of dtrace_getpcstack()</a>
	</figcaption>
      </figure>

      <figure>
	<figcaption><a href="https://github.com/illumos/illumos-gate/blob/007ca33219ffdc49281657f5f8a9ee1bbfc367ab/usr/src/uts/intel/dtrace/dtrace_isa.c#L43-L60">usr/src/uts/intel/dtrace/dtrace_isa.c</a></figcaption>
	<figure>
	  <div>
<pre><span><span></span><code>void</code></span>
<span><span></span><code>dtrace_getpcstack(pc_t *pcstack, int pcstack_limit, int aframes,</code></span>
<span><span></span><code>    uint32_t *intrpc)</code></span>
<span><span></span><code>{</code></span>
<span><span></span><code>	struct frame *fp = (struct frame *)dtrace_getfp();</code></span>
<span><span></span><code>	struct frame *nextfp, *minfp, *stacktop;</code></span>
<span><span></span><code>	int depth = 0;</code></span>
<span><span></span><code>	int on_intr, last = 0;</code></span>
<span><span></span><code>	uintptr_t pc;</code></span>
<span><span></span><code>	uintptr_t caller = CPU-&gt;cpu_dtrace_caller;</code></span>
<span><span></span><code></code></span>
<span><span></span><code>	if ((on_intr = CPU_ON_INTR(CPU)) != 0)</code></span>
<span><span></span><code>		stacktop = (struct frame *)(CPU-&gt;cpu_intr_stack + SA(MINFRAME));</code></span>
<span><span></span><code>	else</code></span>
<span><span></span><code>		stacktop = (struct frame *)curthread-&gt;t_stk;</code></span>
<span><span></span><code>	minfp = fp;</code></span>
<span><span></span><code></code></span>
<span><span></span><code>	aframes++;</code></span></pre>
	  </div>
	  <figcaption>dtrace_getpcstack()</figcaption>
	</figure>
      </figure>

      <p>
	To build the call stack we first need to be able to walk the
	stack. Luckily, illumos keeps frame pointers in the kernel,
	making this easy. But in this particular situation there is
	more to consider. First, we might have two stacks in play: the
	high-level interrupt’s stack as well as the stack of the
	thread it interrupted. Second, the DTrace framework and FBT
	provider have put their own frames between this code and the
	function that tripped this probe; we must exclude these
	“artificial” frames from the result. Finally, we need to make
	sure not to walk off the stack and into space, both for
	correctness and safety. Speaking of the stack,
	the <code>stacktop</code> variable is pointing to the “top” of
	the stack in terms of memory (on x86 stacks grow downwards).
	Logically speaking, <code>stacktop</code> is the bottom of the
	stack and the <code>dtrace_getpcstack()</code> frame is the
	top.
      </p>

      <figure>
	<figcaption><a href="https://github.com/illumos/illumos-gate/blob/007ca33219ffdc49281657f5f8a9ee1bbfc367ab/usr/src/uts/intel/dtrace/dtrace_isa.c#L62-L63">usr/src/uts/intel/dtrace/dtrace_isa.c</a></figcaption>
	<figure>
	  <div>
<pre><span><span></span><code>	if (intrpc != NULL &amp;&amp; depth &lt; pcstack_limit)</code></span>
<span><span></span><code>		pcstack[depth++] = (pc_t)intrpc;</code></span></pre>
	  </div>
	  <figcaption>dtrace_getpcstack()</figcaption>
	</figure>
      </figure>

      <p>
	If <code>intrpc</code> is set, then that’s our first program counter.
      </p>

      <figure>
	<figcaption><a href="https://github.com/illumos/illumos-gate/blob/007ca33219ffdc49281657f5f8a9ee1bbfc367ab/usr/src/uts/intel/dtrace/dtrace_isa.c#L65-L85">usr/src/uts/intel/dtrace/dtrace_isa.c</a></figcaption>
	<figure>
	  <div>
<pre><span><span></span><code>	while (depth &lt; pcstack_limit) {</code></span>
<span><span></span><code>		nextfp = (struct frame *)fp-&gt;fr_savfp;</code></span>
<span><span></span><code>		pc = fp-&gt;fr_savpc;</code></span>
<span><span></span><code></code></span>
<span><span></span><code>		if (nextfp &lt;= minfp || nextfp &gt;= stacktop) {</code></span>
<span><span></span><code>			if (on_intr) {</code></span>
<span><span></span><code>				/*</code></span>
<span><span></span><code>				 * Hop from interrupt stack to thread stack.</code></span>
<span><span></span><code>				 */</code></span>
<span><span></span><code>				stacktop = (struct frame *)curthread-&gt;t_stk;</code></span>
<span><span></span><code>				minfp = (struct frame *)curthread-&gt;t_stkbase;</code></span>
<span><span></span><code>				on_intr = 0;</code></span>
<span><span></span><code>				continue;</code></span>
<span><span></span><code>			}</code></span>
<span><span></span><code></code></span>
<span><span></span><code>			/*</code></span>
<span><span></span><code>			 * This is the last frame we can process; indicate</code></span>
<span><span></span><code>			 * that we should return after processing this frame.</code></span>
<span><span></span><code>			 */</code></span>
<span><span></span><code>			last = 1;</code></span>
<span><span></span><code>		}</code></span></pre>

	  </div>
	  <figcaption>dtrace_getpcstack()</figcaption>
	</figure>
      </figure>

      <p>
	The main loop walks the call stack and fills in program
	counters as long as there are slots remaining in
	<code>pcstack</code>. If we were in the context of a
	high-level interrupt and we’ve walked off its stack, then hop
	to the thread stack. Otherwise, we’ve walked off the thread
	stack, leaving just this last frame to record.
      </p>

      <figure>
	<figcaption><a href="https://github.com/illumos/illumos-gate/blob/007ca33219ffdc49281657f5f8a9ee1bbfc367ab/usr/src/uts/intel/dtrace/dtrace_isa.c#L87-L98">usr/src/uts/intel/dtrace/dtrace_isa.c</a></figcaption>
	<figure>
	  <div>
<pre><span><span></span><code>		if (aframes &gt; 0) {</code></span>
<span><span></span><code>			if (--aframes == 0 &amp;&amp; caller != 0) {</code></span>
<span><span></span><code>				/*</code></span>
<span><span></span><code>				 * We've just run out of artificial frames,</code></span>
<span><span></span><code>				 * and we have a valid caller -- fill it in</code></span>
<span><span></span><code>				 * now.</code></span>
<span><span></span><code>				 */</code></span>
<span><span></span><code>				ASSERT(depth &lt; pcstack_limit);</code></span>
<span><span></span><code>				pcstack[depth++] = (pc_t)caller;</code></span>
<span><span></span><code>				caller = 0;</code></span>
<span><span></span><code>			}</code></span>
<span><span></span><code>		} else {</code></span></pre>
	  </div>
	  <figcaption>dtrace_getpcstack()</figcaption>
	</figure>
      </figure>

      <p>
	Make sure to skip over any artificial frames.
	The <code>aframes</code> value is based on information given
	by the provider at probe creation time
	(<code>dtrace_probe_create()</code>/<code>dtpr_aframes</code>)
	as well as knowledge inherent to the DTrace framework. These
	two know how many frames they have each injected between
	the <code>stack()</code> action and the first real frame; we
	sum the values to know how many total frames to skip.
      </p>

      <p>
	The <code>caller</code> variable is a bit more subtle; and
	this is another thing I got wrong in
	my <a href="https://zinascii.com/2020/the-amd64-fbt-handler.html">last post</a> while
	discussing the return probe. The <code>caller</code> value
	…</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://zinascii.com/2020/fbt-args-and-stack.html">https://zinascii.com/2020/fbt-args-and-stack.html</a></em></p>]]>
            </description>
            <link>https://zinascii.com/2020/fbt-args-and-stack.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24915942</guid>
            <pubDate>Wed, 28 Oct 2020 06:39:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Two Paradigms of Personal Computing]]>
            </title>
            <description>
<![CDATA[
Score 42 | Comments 9 (<a href="https://news.ycombinator.com/item?id=24915683">thread link</a>) | @riverlong
<br/>
October 27, 2020 | https://jayriverlong.github.io/2020/10/27/machines.html | <a href="https://web.archive.org/web/*/https://jayriverlong.github.io/2020/10/27/machines.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main" role="main"> <article role="article">  <p>A frequently recurring staple of Hacker News is the <em>personal computing rant</em>: modern computers are black boxes, far too locked down.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup> We long for our desktop computers, the early-2000s computing paradigm, and we want to take back control of our data.</p> <p>I sympathize greatly with this view. For the past five years, I have exclusively run Arch Linux. I love the early-2000s style of personal computing: text-heavy interfaces, words rather than icons, uniform keyboard shortcuts everywhere. I do nearly all my computing in emacs. It is feature rich, and no product manager is ever going to change the experience, interface, or shortcuts. My workspace allows me to be <em>fast</em>. Contrary to much modern software, emacs responds instantaneously to my keystrokes. Every action has an immediate effect, clear on the page, with no background process ambiguity. This is enormously satisfying. I would characterize this computing environment as a spiritual descendant of the typewriter: on its own, it is a dumb machine, but when you command it, it becomes a powerful extension of you. The user experience is given by speed, smoothness, reliability, and nothing else.</p> <p>On the other hand, you’ve got iPhones and iPads. These are not machines that you command – on the contrary, you might argue that all their notifications command you. With touch interfaces, facial recognition, and voice control, these are machines that are meant to merge with you. Instead of becoming an <em>extension</em> of you, it becomes <em>part</em> of you. Instead of you commanding it, it is meant to anticipate your commands. Where is the data? How does it work? Who knows. They’re total black boxes, effectively indistinguishable from magic, whereas your Linux Desktop is a DIYable simple machine.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2">2</a></sup></p> <p>Many contemporary writers think of personal computing as one paradigm that is gradually evolving. I disagree: I think there are two, deeply different paradigms of personal computing, reflecting deeply different desires.</p> <ul> <li>Machine as extension of the self (your Windows XP Desktop).</li> <li>Machine as part of the self (your iPhone).</li> </ul> <p>Plenty of devices exist in the space in-between. Android phones try to give you some control back over your device, by which they necessarily preclude themselves from truly being <em>part</em> of you. Modern versions of OS X, with the touchbar, Siri, and integrations into all your other Apple devices, are starting to make the leap from extending the self to being part of the self.</p> <p>As a genre, the personal computing rant comes out of the correct view that these are all forms of personal computing, but it fails to recognize that within this broad umbrella, we’ve seen paradigms arise that differ in both intent and the human desire they’re meant to meet. It’s not that one class of devices is replacing the other. They are fundamentally different, and one paradigm succeeds where the other falls short of meeting the consumers’ needs. The terminal never met the needs of those who want Amazon Alexa, and vice-versa.</p> <p>I live in the in-between. I happily imagine a world where my Apple Watch monitors my glucose and blood oxygen, my Eight Sleep quantifies my rest, I mark up documents on giant touchscreens, but I sit on a clacky IBM keyboard to write code and blog posts in a terminal that hasn’t changed in twenty years. While your iPhone might meld with your mind by quantifying and anticipating your every need, my terminal melds with my mind by being fast, constant, and <em>always</em> correct. Each has its place, and I would never trade one for the other.</p>  <hr>  <br> </article> </div></div>]]>
            </description>
            <link>https://jayriverlong.github.io/2020/10/27/machines.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24915683</guid>
            <pubDate>Wed, 28 Oct 2020 05:53:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Bayesian Perspective on Q-Learning]]>
            </title>
            <description>
<![CDATA[
Score 64 | Comments 4 (<a href="https://news.ycombinator.com/item?id=24915662">thread link</a>) | @jonbaer
<br/>
October 27, 2020 | https://brandinho.github.io/bayesian-perspective-q-learning/ | <a href="https://web.archive.org/web/*/https://brandinho.github.io/bayesian-perspective-q-learning/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  <d-contents>
    <nav>
      <h3>Contents</h3>
      
      
      
      
      
      
    </nav>
  </d-contents>

  <p>
    Recent work by Dabney et al. suggests that the brain represents reward predictions as probability distributions
    <d-footnote>
      Experiments were conducted on mice using single-unit recordings from the ventral tegmental area.
    </d-footnote><d-cite key="dabney2020distribtionmice"></d-cite>.
    This contrasts against the widely adopted approach in reinforcement learning (RL) of modelling single scalar
    quantities (expected values).
    In fact, by using distributions we are able to quantify uncertainty in the decision-making process.
    Uncertainty is especially important in domains where making a mistake can result in the inability to recover
    <d-footnote>
      Examples of such domains include autonomous vehicles, healthcare, and the financial markets.
    </d-footnote>. Research in risk-aware reinforcement learning has emerged to address such problems
    <d-cite key="morimura2010risksensitive,chow2018riskconstrainedrl"></d-cite>.
    However, another important application of uncertainty, which we focus on in this article, is efficient exploration
    of the state-action space.
  </p>

  <h2>Introduction</h2>

  <p>
    The purpose of this article is to clearly explain Q-Learning from the perspective of a Bayesian.
    As such, we use a small grid world and a simple extension of tabular Q-Learning to illustrate the fundamentals.
    Specifically, we show how to extend the deterministic Q-Learning algorithm to model
    the variance of Q-values with Bayes' rule. We focus on a sub-class of problems where it is reasonable to assume that Q-values
    are normally distributed
    and derive insights when this assumption holds true. Lastly, we demonstrate that applying Bayes' rule to update
    Q-values comes with a challenge: it is vulnerable to early exploitation of suboptimal policies.
  </p>

  <p>
    This article is largely based on the seminal work from Dearden et al. <d-cite key="dearden1998bayesianqlearning"></d-cite>.
    Specifically, we expand on the assumption that Q-values are normally distributed and evaluate various Bayesian exploration
    policies. One key distinction is that we model $$\mu$$ and $$\sigma^2$$, while the
    authors of the original Bayesian Q-Learning paper model a distribution over these parameters. This allows them to quantify
    uncertainty in their parameters as well as the expected return - we only focus on the latter.
  </p>

  <div><h4>Epistemic vs Aleatoric Uncertainty</h4></div>
  <div><p>
    Since Dearden et al. model a distribution over the parameters, they can sample from this distribution and the resulting
    dispersion in Q-values is known as <b>epistemic</b> uncertainty. Essentially, this uncertainty is representative of the
    "knowledge gap" that results from limited data (i.e. limited observations). If we close this gap, then we are left with
    irreducible uncertainty (i.e. inherent randomness in the environment), which is known as <b>aleatoric</b> uncertainty
    </p><d-cite key="kiureghian2007aleatoric"></d-cite><p>.

    </p><p>
    One can argue that the line between epistemic and aleatoric uncertainty is rather blurry. The information that
    you feed into your model will determine how much uncertainty can be reduced. The more information you incorporate about
    the underlying mechanics of how the environment operates (i.e. more features), the less aleatoric uncertainty there will be.

    </p><p>

    It is important to note that inductive bias also plays an important role in determining what is categorized as
    epistemic vs aleatoric uncertainty for your model.
    </p><p>

    <b>Important Note about Our Simplified Approach:</b></p><p>

    Since we only use $$\sigma^2$$ to represent uncertainty, our approach does not distinguish between epistemic and aleatoric uncertainty.

    Given enough interactions, the agent will close the knowledge gap and $$\sigma^2$$ will only represent aleatoric uncertainty. However, the agent still
    uses this uncertainty to explore.

    This is problematic because the whole point of exploration is to gain
    knowledge, which indicates that we should only explore using epistemic uncertainty.
    </p></div>


  

  <p>
    Since we are modelling $$\mu$$ and $$\sigma^2$$, we begin by evaluating the conditions under which it is appropriate
    to assume Q-values are normally distributed.
  </p>

  <a href="#section-1" id="section-1"></a>
  <h2>When Are Q-Values Normally Distributed?</h2>

  <p>
    The readers who are familiar with Q-Learning can skip over the collapsible box below.
  </p>

  <div><h4>Temporal Difference Learning</h4></div>
  <div>
    <p>
      Temporal Difference (TD) learning is the dominant paradigm used to learn value functions in reinforcement learning
      <d-cite key="sutton1988tempdiff"></d-cite>.
      Below we will quickly summarize a TD learning algorithm for Q-values,
      which is called Q-Learning. First, we will write Q-values as follows <d-cite key="Sutton2017ReinforcementIntroduction"></d-cite>:
    </p>

    <d-math block="">
      \overbrace{Q_\pi(s,a)}^\text{current Q-value} =
      \overbrace{R_s^a}^\text{expected reward for (s,a)} +
      \overbrace{\gamma Q_\pi(s^{\prime},a^{\prime})}^\text{discounted Q-value at next timestep}
    </d-math>

    <p>
      We will precisely define Q-value as the expected value of the total return from taking action $$a$$ in state $$s$$ and following
      policy $$\pi$$ thereafter. The part about $$\pi$$ is important because the agent's view on how good an action is
      depends on the actions it will take in subsequent states. We will discuss this further when analyzing our agent in
      the game environment.
    </p>

    <p>
      For the Q-Learning algorithm, we sample a reward $$r$$ from the environment, and estimate the Q-value for the current
      state-action pair $$q(s,a)$$ and the next state-action pair $$q(s^{\prime},a^{\prime})$$
      <d-footnote>
        For Q-Learning, the next action $$a^{\prime}$$ is the action with the largest Q-value in that state:
        $$\max_{a^{\prime}} q(s^{\prime}, a^{\prime})$$.
      </d-footnote>. We can represent the sample as:
    </p>

    <d-math block="">
      q(s,a) = r + \gamma q{(s^\prime,a^\prime)}
    </d-math>

    <p>
      The important thing to realize is that the left side of the equation is an estimate (current Q-value), and the right side
      of the equation is a combination of information gathered from the environment (the sampled reward) and another estimate
      (next Q-value). Since the right side of the equation contains more information about the true Q-value than the left side,
      we want to move the value of the left side closer to that of the right side. We accomplish this by minimizing the squared
      Temporal Difference error ($$\delta^2_{TD}$$), where $$\delta_{TD}$$ is defined as:
    </p>

    <d-math block="">
      \delta_{TD} = r + \gamma q(s^\prime,a^\prime) - q(s,a)
    </d-math>

    <p>
      The way we do this in a tabular environment, where $$\alpha$$ is the learning rate, is with the following update rule:
    </p>

    <d-math block="">
      q(s,a) \leftarrow \alpha(r_{t+1} + \gamma q(s^\prime,a^\prime)) + (1 - \alpha) q(s,a)
    </d-math>

    <p>
      Updating in this manner is called bootstrapping because we are using one Q-value to update another Q-value.
    </p>
  </div>

  

  <p>
    We will use the Central Limit Theorem (CLT) as the foundation to understand when Q-values are normally
    distributed. Since Q-values are sample sums, then they should look more and more normally distributed as the sample size
    increases <d-cite key="lecam1986clt"></d-cite>.
    However, the first nuance that we will point out is that rewards must be sampled from distributions with finite variance.
    Thus, if rewards are sampled distributions such as Cauchy or Lévy, then we cannot assume Q-values are normally distributed.
  </p>

  

  <p>
    Otherwise, Q-values are approximately normally distributed when the number of <b><i>effective timesteps</i></b>
    $$\widetilde{N}$$ is large
    <d-footnote>
      We can think of effective timesteps as the number of <b>full</b> samples.
    </d-footnote>.
    This metric is comprised of three factors:
  </p>

  <ul>
    <li>
      $$N$$ - <b>Number of timesteps</b>: As $$N$$ increases, so does $$\widetilde{N}$$.
    </li>

    <li>
      $$\xi$$ - <b>Sparsity</b>: We define sparsity as the number of timesteps,
      on average, a reward of zero is deterministically received in between receiving non-zero rewards
      <d-footnote>
        In the Google Colab notebook, we ran simulations to show that $$\xi$$ reduces the effective number of timesteps by $$\frac{1}{\xi + 1}$$:
        <a href="https://colab.research.google.com/github/brandinho/bayesian-perspective-q-learning/blob/main/Sparsity.ipynb">
          Experiment in a <span>Notebook</span>
        </a>
      </d-footnote>.
      When sparsity is present, we lose samples (since they are always zero).
      <!-- As sparsity increases, we essentially lose samples since they are always zero. -->
      Therefore, as $$\xi$$ increases, $$\widetilde{N}$$ decreases.
    </li>

    <li>
      $$\gamma$$ - <b>Discount Factor</b>:
      As $$\gamma$$ gets smaller, the agent places more weight on immediate rewards relative to distant ones, which means
      that we cannot treat distant rewards as full samples. Therefore, as $$\gamma$$ increases, so does $$\widetilde{N}$$.
    </li>

    <div><h4>Discount Factor and Mixture Distributions</h4></div>
    <div>
      <p>
        We will define the total return as the sum of discounted future
        rewards, where the discount factor $$\gamma$$ can take on any value between $$0$$ (myopic) and $$1$$ (far-sighted).
        It helps to think of the resulting distribution $$G_t$$ as a weighted mixture distribution.
      </p>

      <d-math block="">
        G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + ... + \gamma^{N-1} r_{t+N}
      </d-math>

      <p>
        When we set $$\gamma \lt 1$$, the mixture weights for the underlying distributions change from equal weight
        to time-weighted, where immediate timesteps have a higher weight. When $$\gamma = 0$$, then this is
        equivalent to sampling from only one timestep and CLT would not hold. Use the slider
        to see the effect $$\gamma$$ has on the mixture weights, and ultimately the mixture distribution.
      </p>

      <div>
        <div>
          <p>
              $$$$<br>
              $$$$
          </p>
          

          <p>
              $$$$<br>
              $$$$
          </p>
          

          <p>
              $$$$<br>
              $$$$
          </p>

          
          
        </div>

        
      </div>

      <p><label for="barGammaMixture">$$\gamma$$ = <span></span></label></p>
    </div>
  </ul>

  We combine the factors above to formally define the number of effective timesteps:

  <d-math block="">
    \widetilde{N} = \frac{1}{\xi + 1}\sum_{i=0}^{N-1}\gamma^{i}
  </d-math>

  <p>
    Below we visually demonstrate how each factor affects the normality of Q-values
    <d-footnote>
      We scale the Q-values by $$\widetilde{N}$$ because otherwise the distribution of Q-values
      moves farther and farther to the right as the number of effective timesteps increases, which distorts the visual.
    </d-footnote>:
  </p>

  <p>
    <label for="skew">Skew-Normal</label>
    
    <label for="bernoulli">Bernoulli</label>
  </p>

  

  

  <figcaption>
    Select whether the underlying distribution follows a skew-normal or a Bernoulli distribution.
    In the Google Colab notebook we also include three statistical tests of normality for the Q-value distribution.
    </figcaption>
 …</div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://brandinho.github.io/bayesian-perspective-q-learning/">https://brandinho.github.io/bayesian-perspective-q-learning/</a></em></p>]]>
            </description>
            <link>https://brandinho.github.io/bayesian-perspective-q-learning/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24915662</guid>
            <pubDate>Wed, 28 Oct 2020 05:49:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Theory of Software Architecture]]>
            </title>
            <description>
<![CDATA[
Score 490 | Comments 238 (<a href="https://news.ycombinator.com/item?id=24915497">thread link</a>) | @nreece
<br/>
October 27, 2020 | https://danuker.go.ro/the-grand-unified-theory-of-software-architecture.html | <a href="https://web.archive.org/web/*/https://danuker.go.ro/the-grand-unified-theory-of-software-architecture.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <div>
        <div>
    <section id="content">
        <article>
            
            <div>
                
                <p>Take <strong>Uncle Bob's</strong> Clean Architecture and map its correspondences with <strong>Gary Bernhardt's</strong> thin imperative shell around a functional core, and you get an understanding of how to cheaply maintain and scale software!</p>
<p>This is what <a href="https://rhodesmill.org/brandon/">Mr. Brandon Rhodes</a> did. It's not every day that I find such clear insight.</p>
<p>I am honored to have found his <a href="https://rhodesmill.org/brandon/talks/#clean-architecture-python">presentation</a> and <a href="https://rhodesmill.org/brandon/slides/2014-07-pyohio/clean-architecture/">slides</a> explaining  <a href="https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html">Uncle Bob's Clean Architecture</a> and Gary Bernhardt's PyCon talks of <a href="https://archive.org/details/pyvideo_422___units-need-testing-too">2011</a>, <a href="https://pycon-2012-notes.readthedocs.io/en/latest/fast_tests_slow_tests.html">2012</a>, and <a href="https://www.destroyallsoftware.com/talks/boundaries">2013</a>.</p>
<p>Mr. Rhodes offers such a distilled view, that he can show you these crucial concepts in 3 slides of code. I will go ahead and summarize what he said and add a tiny bit of my insight.</p>
<p>Copyright of all Python code on this page belongs to <a href="https://rhodesmill.org/brandon/">Mr. Brandon Rhodes</a>, and copyright of the diagram belongs to <a href="https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html">Robert C. Martin (Uncle Bob)</a>. I use these under (hopefully) fair use (nonprofit and educational).</p>


<p>First of all, we need to be on the same page, in order to be able to understand each other. Here are the words I'll use:</p>
<ul>
<li>Function: I use "function" or "pure function" to refer to a Python "function" that only uses its parameters for input, returns a result as output, and does not cause any other side-effects (such as I/O). <ul>
<li>A pure function returns the same output given the same inputs.</li>
<li>A pure function may be called any number of times without changing the system state - it should have no influence on DB, UI, other functions or classes.</li>
<li>This is very similar to a mathematical function: takes you from <em>x</em> to <em>y</em> and nothing else happens.</li>
<li>Sadly we can't have only pure functions; software has a <strong>purpose</strong> of causing side-effects.</li>
</ul>
</li>
<li>Procedure, Routine, or Subroutine: A piece of code that executes, that may or may not have side effects. This is a "function" in Python, but might not be a "pure function".</li>
<li>Tests: automated unit tests. By "unit" I mean not necessarily just a class, but a behavior. If you want, see more details in <a href="https://danuker.go.ro/tdd-revisited-pytest-updated-2020-09-03.html#update-2020-09-03-keep-coupling-low">the coupling chapter of my previous post</a>.</li>
</ul>

<div><pre><span></span><code><span>import</span> <span>requests</span>                      <span># Listing 1</span>
<span>from</span> <span>urllib</span> <span>import</span> <span>urlencode</span>

<span>def</span> <span>find_definition</span><span>(</span><span>word</span><span>):</span>
    <span>q</span> <span>=</span> <span>'define '</span> <span>+</span> <span>word</span>
    <span>url</span> <span>=</span> <span>'http://api.duckduckgo.com/?'</span>
    <span>url</span> <span>+=</span> <span>urlencode</span><span>({</span><span>'q'</span><span>:</span> <span>q</span><span>,</span> <span>'format'</span><span>:</span> <span>'json'</span><span>})</span>
    <span>response</span> <span>=</span> <span>requests</span><span>.</span><span>get</span><span>(</span><span>url</span><span>)</span>     <span># I/O</span>
    <span>data</span> <span>=</span> <span>response</span><span>.</span><span>json</span><span>()</span>           <span># I/O</span>
    <span>definition</span> <span>=</span> <span>data</span><span>[</span><span>u</span><span>'Definition'</span><span>]</span>
    <span>if</span> <span>definition</span> <span>==</span> <span>u</span><span>''</span><span>:</span>
        <span>raise</span> <span>ValueError</span><span>(</span><span>'that is not a word'</span><span>)</span>
    <span>return</span> <span>definition</span>
</code></pre></div>
<p>Here, we have a piece of code that prepares a URL, then gets some data over the network (I/O), then validates the result (a word definition) and returns it.</p>
<p>This is a bit much: a procedure should ideally do one thing only. While this small-ish procedure is quite readable still, it is a metaphor for a more developed system - where it could be arbitrarily long.</p>
<p>The current knee-jerk reaction is to <em>hide</em> the I/O operations somewhere far away. Here is the same code after extracting the I/O lines:</p>

<div><pre><span></span><code><span>def</span> <span>find_definition</span><span>(</span><span>word</span><span>):</span>           <span># Listing 2</span>
    <span>q</span> <span>=</span> <span>'define '</span> <span>+</span> <span>word</span>
    <span>url</span> <span>=</span> <span>'http://api.duckduckgo.com/?'</span>
    <span>url</span> <span>+=</span> <span>urlencode</span><span>({</span><span>'q'</span><span>:</span> <span>q</span><span>,</span> <span>'format'</span><span>:</span> <span>'json'</span><span>})</span>
    <span>data</span> <span>=</span> <span>call_json_api</span><span>(</span><span>url</span><span>)</span>
    <span>definition</span> <span>=</span> <span>data</span><span>[</span><span>u</span><span>'Definition'</span><span>]</span>
    <span>if</span> <span>definition</span> <span>==</span> <span>u</span><span>''</span><span>:</span>
        <span>raise</span> <span>ValueError</span><span>(</span><span>'that is not a word'</span><span>)</span>
    <span>return</span> <span>definition</span>

<span>def</span> <span>call_json_api</span><span>(</span><span>url</span><span>):</span>
    <span>response</span> <span>=</span> <span>requests</span><span>.</span><span>get</span><span>(</span><span>url</span><span>)</span>     <span># I/O</span>
    <span>data</span> <span>=</span> <span>response</span><span>.</span><span>json</span><span>()</span>           <span># I/O</span>
    <span>return</span> <span>data</span>
</code></pre></div>
<p>In Listing #2, the I/O is extracted from the top-level procedure. </p>
<p>The problem is, the code is still <strong>coupled</strong> - <code>call_json_api</code> is called whenever you want to test anything - even the building of the URL or the parsing of the result.</p>
<p><strong>Coupling kills software.</strong></p>
<p>A good rule of thumb to spot coupling is this: Can you test a piece of code without having to mock or dependency inject like Frankenstein?</p>
<p>Here, we can't test <code>find_definition</code> without somehow replacing <code>call_json_api</code> from inside it, in order to avoid making HTTP requests.</p>
<p>Let's find out what a better solution looks like.</p>

<div><pre><span></span><code><span>def</span> <span>find_definition</span><span>(</span><span>word</span><span>):</span>           <span># Listing 3</span>
    <span>url</span> <span>=</span> <span>build_url</span><span>(</span><span>word</span><span>)</span>
    <span>data</span> <span>=</span> <span>requests</span><span>.</span><span>get</span><span>(</span><span>url</span><span>)</span><span>.</span><span>json</span><span>()</span>  <span># I/O</span>
    <span>return</span> <span>pluck_definition</span><span>(</span><span>data</span><span>)</span>

<span>def</span> <span>build_url</span><span>(</span><span>word</span><span>):</span>
    <span>q</span> <span>=</span> <span>'define '</span> <span>+</span> <span>word</span>
    <span>url</span> <span>=</span> <span>'http://api.duckduckgo.com/?'</span>
    <span>url</span> <span>+=</span> <span>urlencode</span><span>({</span><span>'q'</span><span>:</span> <span>q</span><span>,</span> <span>'format'</span><span>:</span> <span>'json'</span><span>})</span>
    <span>return</span> <span>url</span>

<span>def</span> <span>pluck_definition</span><span>(</span><span>data</span><span>):</span>
    <span>definition</span> <span>=</span> <span>data</span><span>[</span><span>u</span><span>'Definition'</span><span>]</span>
    <span>if</span> <span>definition</span> <span>==</span> <span>u</span><span>''</span><span>:</span>
        <span>raise</span> <span>ValueError</span><span>(</span><span>'that is not a word'</span><span>)</span>
    <span>return</span> <span>definition</span>
</code></pre></div>
<p>Here, the procedure at the top (aka. the <span><strong>imperative shell</strong></span> of the program) is handling the I/O, and everything else is moved to <span><strong>pure functions</strong></span> (<code>build_url</code>, <code>pluck_definition</code>). The <span><strong>pure functions</strong></span> are easily testable by just calling them on made-up data structures; no Frankenstein needed.</p>
<p>This separation into an <span><strong>imperative shell</strong></span> and <span><strong>functional core</strong></span> is an encouraged idea by Functional Programming.</p>
<p>Ideally, though, in a real system, you wouldn't test elements as small as these routines, but integrate more of the system. See <a href="https://danuker.go.ro/tdd-revisited-pytest-updated-2020-09-03.html#update-2020-09-03-keep-coupling-low">the coupling chapter of my previous post</a> to understand the trade-offs.</p>

<p>Look at <a href="https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html">Uncle Bob's Clean Architecture chart</a> (Copyright Robert C. Martin aka. Uncle Bob) :
<img alt="The Clean Architecture" src="https://danuker.go.ro/images/CleanArchitecture.jpg"></p>
<p>Uncle Bob's <span><strong>Use Cases</strong></span> and <span><strong>Entities</strong></span> (red and yellow circles of the chart) map to the <span><strong>pure functions</strong></span> we saw earlier - <code>build_url</code> and <code>pluck_definition</code> from Listing 3, and the <span><strong>plain objects</strong></span> they receive as parameters and send as outputs. <em>(updated 2020-10-28)</em></p>
<p>Uncle Bob's <span><strong>Interface Adapters</strong></span> (green circle) map to the top-level <span><strong>imperative shell</strong></span>  from earlier - <code>find_definition</code> from Listing 3, handling only I/O to the outside (Web, DB, UI, other frameworks).</p>
<p><a href="https://www.reddit.com/r/programming/comments/jj7ave/the_grand_unified_theory_of_software_architecture/gabst6z/?context=3">Update 2020-10-28</a>: A "Model" object in today's MVC frameworks is a poisoned apple: it is not a <a href="https://khanlou.com/2014/12/pure-objects/">"pure" object</a> or <a href="http://xunitpatterns.com/Humble%20Object.html">"humble" object</a>, but one that can produce side effects like saving or loading from the database. Their "save" and "read" methods litter your code with untestable side-effects all over. Avoid them, or confine them to the periphery of your system and reduce their influence accordingly (they are actually a hidden <span><strong>Interface Adapter</strong></span>) due to interacting with the DB.</p>
<p>Notice the arrows on the left side of the circles, pointing inwards to more and more abstract parts. These are procedure or function calls. Our code is called by the outside. <strong>This has some exceptions. Whatever you do, the database won't call your app. But the web can, a user can through a UI, the OS can through STDIN, and a timer can, at regular intervals (such as in a game).</strong> <em>(updated 2020-10-28)</em></p>
<p>The top-level procedure:</p>
<ol>
<li>gets the input, </li>
<li>adapts it to simple objects acceptable to the system,</li>
<li>pushes it through the functional core,</li>
<li>gets the returned value from the functional core,</li>
<li>adapts it for the output device,</li>
<li>and pushes it out to the output device.</li>
</ol>
<p>This lets us easily test the functional core. Ideally, most of a production system should be pure-functional.</p>

<p>If you reduce the <span><strong>imperative shell</strong></span> and move code into the <span><strong>functional core</strong></span>, each test can verify almost the entire (now-functional) stack, but stopping short of actually performing external actions.</p>
<p>You can then test the imperative shell using <strong>fewer integration tests</strong>: you only need to check that it is <strong>correctly connected</strong> to the functional core.</p>
<p>Having two users for the system - the real user and the unit tests - and listening to both, lets you guide your architecture so as to <strong>minimize coupling</strong> and build a more <strong>flexible system</strong>.</p>
<p>Having a flexible system lets you implement new features and change existing ones <strong>quickly and cheaply</strong>, in order to <strong>stay competitive as a business</strong>.</p>
<p>Comments are much appreciated. I am yet to apply these insights, and I may be missing something!</p>
<p><strong>Edit 2020-10-28:</strong> I have tried out this methodology in some small TDD Katas, and together with TDD, it works great. But I am not employed right now, so I can't say I've <em>really</em> tried it.</p>
            </div>
            <!-- /.entry-content -->


        </article>
    </section>

        </div>
        
    </div>
</div></div>]]>
            </description>
            <link>https://danuker.go.ro/the-grand-unified-theory-of-software-architecture.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24915497</guid>
            <pubDate>Wed, 28 Oct 2020 05:19:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Dual-Boot Ubuntu 20.04 and Windows 10 with Encryption]]>
            </title>
            <description>
<![CDATA[
Score 144 | Comments 150 (<a href="https://news.ycombinator.com/item?id=24914573">thread link</a>) | @Fiveplus
<br/>
October 27, 2020 | https://www.mikekasberg.com/blog/2020/04/08/dual-boot-ubuntu-and-windows-with-encryption.html | <a href="https://web.archive.org/web/*/https://www.mikekasberg.com/blog/2020/04/08/dual-boot-ubuntu-and-windows-with-encryption.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  
  <p><img src="https://www.mikekasberg.com/images/posts/dual-boot-encryption-full.jpg" alt="Image for "></p>
  <p><span>08 Apr 2020</span></p><p>When you run the Ubuntu installer, there’s an option to dual-boot Ubuntu with an
existing Windows installation. There’s also an option to encrypt your Ubuntu
installation, but <em>only if you erase everything and install ubuntu</em>. There’s no
automatic way to install Ubuntu alongside Windows 10 with encryption. And while
there are plenty of tutorials for dual-booting Ubuntu and Windows, many of them
are outdated – often referencing an MBR partition table – and almost none of
them seem to address encrypting your Ubuntu partition.</p>

<blockquote>
  <p>Dual-booting with encrypted storage should not be this hard in 2020.</p>

  <p>–Me, while figuring out how to do this.</p>
</blockquote>

<p>In reality, once you figure it out, it’s not that hard. The tricky thing is that
this isn’t well-documented <strong>anywhere</strong>! So I’m hoping to fix that with this
tutorial blog post. Honestly, if you know enough about Ubuntu to set up a
dual-boot with Windows, it’s only a little bit harder to do it with encryption.
I prepared this tutorial on a Dell Latitude e7450, and I fine-tuned it when I
tested it on my Dell Precision 5510. So it should work with almost no
modification on most Dell systems, and with only minor modifications
(particularly around BIOS setup) on most other types of computers.</p>

<h2 id="references">References</h2>

<p>To write this guide, I compiled information from several sources. Here are some
of the most useful references I found:</p>

<ul>
  <li><a href="https://gist.github.com/luispabon/db2c9e5f6cc73bb37812a19a40e137bc">XPS 15 9560 Dual-Boot with Encryption Notes</a>,
by <a href="https://gist.github.com/luispabon">luispabon</a>. I followed these notes pretty
closely, but modified some partition sizes and names based on other guides.</li>
  <li><a href="https://gist.github.com/mdziekon/221bdb597cf32b46c50ffab96dbec08a">XPS 15 9570 Dual-Boot with Encryption Notes</a>,
by <a href="https://gist.github.com/mdziekon">mdziekon</a>, upon which the above is based.</li>
  <li><a href="https://help.ubuntu.com/community/Full_Disk_Encryption_Howto_2019">Full Disk Encryption HowTo 2019</a>,
from the Ubuntu Community Wiki. This is a great resource, but deals with
encryption without dual-booting.</li>
  <li><a href="https://help.ubuntu.com/community/ManualFullSystemEncryption">Manual Full System Encryption</a>,
from the Ubuntu Community Wiki. This is longer, and isn’t focused on
dual-booting, but provides great details on the way certain things work.</li>
</ul>

<p>It is worth noting that this method doesn’t encrypt <code>/boot</code>. While there are
valid reasons for encrypting /boot, the graphical installer does not encrypt it
when you do a graphical install with LUKS. As such, I’m matching that precedent,
and keeping the simplicity of an unencrypted /boot partition. Thus, the guide
I’ve compiled below is just about the <strong>simplest way to have a LUKS encryption
with dual-boot.</strong></p>

<h2 id="why-encryption-is-important">Why encryption is important</h2>

<p>I began using encrypted storage on all my personal computers 5 or 6 years ago
after noticing that all the companies I’d worked for required it, and had good
reason to. Laptops get lost and stolen all the time. They’re high-value items
that are small and easy to carry. And when a thief gets your laptop, there’s
tons of valuable information on it that they can use or sell. Even if you use a
password to log in, it’s easy for an attacker to gain access to your data if
your disk isn’t encrypted – for example, by using a live USB stick. And once
they have that data, they might get access to online accounts, bank statements,
emails, and tons of other data. For me, an encrypted hard disk isn’t optional
anymore – its a necessity.</p>

<h2 id="an-overview">An Overview</h2>

<p>So what are we going to do? This tutorial will help you set up a system to
<strong>dual-boot Ubuntu 20.04 and Windows 10</strong>. (I haven’t tested it, but it should
work with most other modern versions (~16.04+) of Ubuntu or Windows.) The system
will use a GPT hard disk with UEFI (your BIOS must support UEFI). The Ubuntu
partition will be encrypted with LUKS.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup> The Windows partition can optionally
be encrypted with BitLocker. I’m going to keep the Ubuntu installation as close
to a “default” installation as possible – no fancy tricks like a separate
<code>/home</code> partition, but it should be somewhat easy to add that yourself if you
really want to.</p>

<p>I’m going to start with a blank hard disk, installing both Windows 10 and Ubuntu
from scratch. If you already have Windows installed and you want to keep it, you
should be able to shrink your windows partition and join us in phase 3 (though
you might want to skim phases 1 and 2 to understand what we did).</p>

<p>To give you a broad overview of where we’re headed, here’s what we’re going to
do:</p>

<ol>
  <li>Prepare the installation media and computer</li>
  <li>Install Windows 10</li>
  <li>Create an encrypted partition for Ubuntu</li>
  <li>Install Ubuntu</li>
</ol>

<p>Of course, as with any new OS installation, you should back up any important
data before proceeding. <strong>The instructions below will erase all the data on your
hard disk.</strong> Proceed at your own risk; I’m not responsible for any damage or
data loss.</p>



<p>Since we’re installing both Windows 10 and Ubuntu from scratch, we’ll need a USB
stick for each. If you don’t already have a computer running Ubuntu or Windows,
making the installation media will be a little harder – but there are tutorials
for that and I’ll let you figure it out on your own.</p>

<ol>
  <li>Create a Windows Installer USB stick.  The easiest way is to use the <a href="https://www.microsoft.com/software-download/">Windows
10 Media Creation Tool</a> from a
computer that’s already running Windows.</li>
  <li>Create an Ubuntu 20.04 USB stick. The easiest way is to <a href="https://ubuntu.com/download/desktop">download the
ISO</a> and use the Startup Disk Creator on a
computer that’s already running Ubuntu.</li>
</ol>

<p>Great! We’ve got our USB sticks ready to go! One final thing before we get
started – we need to make sure our BIOS is set up correctly. In particular, we
want to make sure we’re using UEFI to boot our OS.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2">2</a></sup></p>

<p><img src="https://www.mikekasberg.com/images/dual-boot-encryption/dell-bios.jpg" alt="The Dell BIOS"></p>

<ol start="3">
  <li>Ensure your computer is running the latest BIOS available. This is important
because an out-of-date BIOS can have bugs, and those bugs sometimes affect
things like UEFI, non-Windows operating systems, or other components we’ll be
touching.</li>
  <li>Edit your BIOS settings. The following names are probably specific to Dell
BIOS, but other manufacturers will have similar settings.
    <ol>
      <li>Under <code>General</code> and <code>Boot Sequence</code>, make sure your <code>Boot
List Option</code> is set to <code>UEFI</code>.</li>
      <li>Under <code>General</code> and <code>Advanced Boot Options</code>, I disabled
<code>Legacy Option ROMs</code>. It’s important that both OSes install in UEFI mode.
(You can probably enable this when installation is complete if you care).</li>
      <li>Under <code>Security</code>, <code>TPM Security</code> must be enabled if you
want to easily set up BitLocker in Windows.</li>
      <li>I disabled <code>Secure Boot</code>. I’m not sure if this is absolutely required, and
you can try leaving it on or re-enabling it when you’re done if you want.</li>
    </ol>
  </li>
</ol>

<p>Now that our BIOS is configured for UEFI, we’re going to set up our hard disk.</p>

<div>
<p><b>For this tutorial, your BIOS must support UEFI!</b></p>
<p>Most modern computers support this, but if yours doesn't this tutorial won't
work for you. You should consider:</p>

<ul>
  <li>Installing only Linux with encryption using the graphical installer.</li>
  <li>OR Installing only Windows with encryption.</li>
  <li>OR Dual-booting Linux and Windows without encryption using Ubuntu's graphical installer.</li>
  <li>OR Finding another tutorial or figuring out how to do this with an MBR disk.</li>
</ul>
</div>

<ol start="5">
  <li><strong>Completely erase</strong> your hard disk and set it up for UEFI by doing the
following.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3">3</a></sup>
    <ol>
      <li>Boot your Ubuntu USB stick and use <code>Try without installing</code>.</li>
      <li>Open a terminal. Make it fullscreen while you’re at it.</li>
      <li>Figure out what your primary hard disk is called. It will probably be
either <code>/dev/sda</code> or <code>/dev/nvme0n1</code>. Importantly, it’s <strong>not</strong> <code>/dev/sda1</code> or
<code>/dev/nvme0n1p1</code> – those are partitions of the disk. One way to figure out what
yours is called is to run <code>lsblk</code> and look at the disk size. Throughout the rest
of this guide, I’m going to refer to <code>/dev/sda</code>. <strong>If yours is not
<code>/dev/sda</code>, replace <code>/dev/sda</code> with your own (perhaps <code>/dev/sdb</code> or
<code>/dev/nvme0n1</code>) for the rest of this guide.</strong></li>
      <li>
        <p>Run the following commands. This will initialize the drive as a GPT drive
and create a 550M EFI system partition formatted as FAT32.</p>

        <div><div><pre><code>$ sudo su
# sgdisk --zap-all /dev/sda
# sgdisk --new=1:0:+550M /dev/sda
# sgdisk --change-name=1:EFI /dev/sda
# sgdisk --typecode=1:ef00 /dev/sda
# mkfs.fat -F 32 /dev/sda1
</code></pre></div>        </div>
      </li>
    </ol>
  </li>
</ol>

<p>OK, phase 1’s complete. We have our installation media ready to go and the
computer’s BIOS and hard drive is set up correctly. Next, we’ll install Windows.</p>

<h2 id="phase-2-install-windows">Phase 2: Install Windows</h2>

<p>In this phase, we’re going to install Windows. Note that when we do this, we’re
going to leave some unallocated space to install Linux later. This is a good
approach because the Windows installer will mess with our partitions a little
bit, and its easier to let it do so before finalizing our Linux partitions.</p>

<p><img src="https://www.mikekasberg.com/images/dual-boot-encryption/windows-installer.jpg" alt="The Windows installer"></p>

<ol>
  <li>Boot from your Windows Installer USB stick.</li>
  <li>Choose a <code>Custom (advanced)</code> install to get to the Windows partitioning tool.</li>
  <li>Create a new partition. The size of this partition should be the amount of
disk space you want to use for Windows. In this example, I did 80G since the SSD
on my computer is relatively small. If unsure, do about half of your hard
disk.</li>
  <li>Windows will warn you that it is going to create an extra system partition.
This is good.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4">4</a></sup></li>
  <li>Install Windows onto the partition you just made. There’s no need to format
any partitions – the Windows installer will take care of that for you.</li>
  <li>When the Windows installation is finished, log in and enable BitLocker on
drive <code>C:</code>. This will automatically create yet another partition on your disk
(a Windows recovery partition) - which is why we’re doing it before
partitioning for Ubuntu.</li>
</ol>

<p>At this point, you can start using Windows. But I’d avoid doing too much setup
or personalization yet so you don’t have to do it again if something goes wrong
below. If you want to double check your partitions, this is what you’ll be left
with after installing Windows and enabling BitLocker:</p>

<div><div><pre><code>ubuntu@ubuntu:~$ sudo sgdisk --print /dev/sda
Disk /dev/sda: 500118192 sectors, 238.5 GiB

Number  Start (sector)    End (sector)  Size       Code  Name
   1            2048         1128447   550.0 MiB   EF00  EFI
   2         1128448         1161215   16.0 MiB    0C01  Microsoft reserved ...
   3         1161216       167825076   79.5 GiB    0700  Basic data partition
   4       167825408       168900607   525.0 MiB   2700
</code></pre></div></div>

<h2 id="phase-3-partition-the-drive-for-ubuntu">Phase 3: Partition the drive for Ubuntu</h2>

<p>This is the trickiest phase since this is where we need to manually set up our
encrypted disks for Ubuntu. We’re going to make it work very similar to …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.mikekasberg.com/blog/2020/04/08/dual-boot-ubuntu-and-windows-with-encryption.html">https://www.mikekasberg.com/blog/2020/04/08/dual-boot-ubuntu-and-windows-with-encryption.html</a></em></p>]]>
            </description>
            <link>https://www.mikekasberg.com/blog/2020/04/08/dual-boot-ubuntu-and-windows-with-encryption.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24914573</guid>
            <pubDate>Wed, 28 Oct 2020 02:56:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[When is no-code useful?]]>
            </title>
            <description>
<![CDATA[
Score 87 | Comments 81 (<a href="https://news.ycombinator.com/item?id=24914062">thread link</a>) | @thesephist
<br/>
October 27, 2020 | https://linus.coffee/note/no-code/ | <a href="https://web.archive.org/web/*/https://linus.coffee/note/no-code/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
        <p>To talk about what no-code is good for, we need to first take a digression on what makes no-code fundamentally different from “yes-code” software.</p>
<h2 id="the-grain-of-abstractions">The grain of abstractions</h2>
<p>Software – yes-code software – has been around for a while. One of the things we’ve learned as an industry is how to write software <em>that gracefully evolves</em>. We’re not perfect – sad, legacy systems still proliferate – but we as a technical industry have learned how to build and evolve software systems against changing requirements and constraints that span years and decades.</p>
<p>When we first solve a problem with software, we write some code against the constraints of that particular day. We don’t necessarily know how the problem is going to change. Maybe there will be different customers or stakeholders tomorrow, or maybe the product will expand to serve a related, but different, problem space. We need to be able to change software to accommodate changing circumstances without rewriting it, and that is fundamentally what <em>software engineering</em> is: how to change software systems. <em>Change</em> is the name of the game.</p>
<p>We’ve gotten decent at change. We’ve built tools like Git and patterns like continuous integration and code autoformatting to make it easier to change code and remain stable. We’ve learned how to operate large software teams, especially in open source. We’ve also learned to use better abstractions. Abstractions are conceptual wrappers that isolate different parts of a codebase – say, a data source from a user interface – so that one part may change while another doesn’t. In general we have started to figure out how to make the DNA of software systems resilient against the changing tides of time.</p>
<p>No-code seems to reject a lot of those learnings, for better or worse. I haven’t seen any no-code company or product that allows source control (and I’ve seen many no-code companies, but you’re welcome to prove me wrong.) I have yet to find no-code products that allow for natural construction of abstraction between layers of a no-code workflow. No-code software is also scarily ill-prepared for large scale development: we have software systems being worked on by tens of thousands of engineers – what does it look like for a team of 1000 engineers to be working on a set of thousands of no-code workflows? Chaos.</p>
<blockquote>
<p>Traditional software has learned the abstractions and patterns that make software resilient and adaptable to change and scale. No-code software is not ready for changing constraints nor development scale.</p>
</blockquote>
<p>Despite these limitations, I think no-code has a few niches where making tradeoffs in adaptability and scale allows no-code tools to be much, much better. So, given this, <em>when is no-code useful?</em></p>
<h2 id="1-transitionary-ephemeral-software">1. Transitionary, ephemeral software</h2>
<p>The obvious answer, and one I had before our conversation, was <em>transitionary</em> software, software with <em>a defined lifetime</em>. If your software system has a finite, pre-defined lifetime and team, it doesn’t need to worry about changing constraints or team growth. It just needs to worry about solving a problem well, now.</p>
<p>Lots of software has predictably finite lifetime: a product prototype for an early-stage company, a game or app used as a part of an interactive online ad, a quick sketch or solution to patch a particularly urgent problem in a product, an app built for an event or a conference or a recruiting cycle or a quarterly goal tracker… all of these are projects with a pre-defined, maximum lifetime. They don’t need to last or grow or change – they just need to work now, and by giving up some of the adaptability of software abstractions of code, no-code software benefits from way faster prototyping speed. This is a plus.</p>
<p>I think we see lots of finite-time software in transitions. Transitions from having no product to having a product, in a prototype. Transitions in the process of brainstorming a solution and trying multiple designs. Software with a finite shelf life is a good fit for no-code tools.</p>
<h2 id="2-high-churn-code">2. High-churn code</h2>
<p>There’s another category of software for which long-term maintainability matters little – code with high churn.</p>
<p>By high churn, I mean that requirements are changing almost daily, and very little of the code written today will exist in a month or a quarter’s time. If the code you write today doesn’t have to last and evolve, because something new is going to take its place tomorrow, what matters is the speed to build, not resilience to change.</p>
<p>There’s lots of high-churn code in businesses. Marketing websites and landing pages, data pipelines for analytics, e-commerce storefronts, marketing campaigns, payment portals – requirements for these kinds of solutions change quickly enough that code is constantly being rewritten, and if code needs to <em>be replaced</em> more than it needs to <em>last</em>, no-code tools are a great fit.</p>
<h2 id="avoiding-the-same-mistakes">Avoiding the same mistakes</h2>
<p>I think “no-code” is a misnomer. It leads us to think that no-code software is the start of a trend in which general software will involve less coding, and software engineering will become easier. This is not the case. Software engineering is not about building solutions, it’s about evolving them. But change resiliency over time is not the focus of no-code tools, and I think that’s ok.</p>
<p>I think no-code tools are instead an extension of a different trend: <a href="https://thesephist.com/posts/text/">reifying workflows</a>. Business processes and workflows used to be documented in Word docs strewn about the office or on a shared folder, or even just passed down by oral tradition in companies. Now, we have tools that allow us to build these workflows, talk about them, edit them, and share them more concretely. This is a huge boon for more repeatable business processes and for getting things done quickly! I think this is the true win of no-code tools: concretizing workflows.</p>
<p>If no-code wants to be a serious competitor against “traditional” software – though I don’t think it should try – no-code needs to learn from the mistakes of early software. No-code tools need to understand that products and software systems need to live on for decades against changing teams and requirements, and against products and companies and standards that die out and get replaced. This requires a cultural shift, a tooling shift, and a new class of abstractions in our toolbelt as no-code engineers. Anytime we try to introduce more tooling and abstraction to no-code, I think no-code gets just a little more “code” in it. And perhaps that’ll bring us right back to where we started, discovering that code is good.</p>
<p>After all, the world is complex. And when we build software against the complexity of the world, that <a href="https://thesephist.com/posts/complexity-conservation/">complexity needs to go somewhere</a>. Software is complex, but only as much as the world it attempts to make sense of.</p>
<p>It feels like we’re getting off the edge of a discovery phase of no-code, and into a time when we’re starting to understand what problems no-code tools are great for. I think it’s important that no-code tool builders focus on those strengths, or risk falling into the trap of repeating the software industry’s mistakes from the ground up.</p>

        <hr>
        <p>
            
            Next:
            <a href="https://linus.coffee/note/scannability/"><em>Scannability is king</em></a>
            
        </p>
    </article></div>]]>
            </description>
            <link>https://linus.coffee/note/no-code/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24914062</guid>
            <pubDate>Wed, 28 Oct 2020 01:36:07 GMT</pubDate>
        </item>
    </channel>
</rss>
