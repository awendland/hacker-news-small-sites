<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 20]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 20. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Wed, 08 Jul 2020 12:19:59 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-20.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Wed, 08 Jul 2020 12:19:59 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[CoreBGP – Plugging in to BGP]]>
            </title>
            <description>
<![CDATA[
Score 80 | Comments 8 (<a href="https://news.ycombinator.com/item?id=23744167">thread link</a>) | @jordanwhited
<br/>
July 5, 2020 | https://www.jordanwhited.com/posts/corebgp-plugging-in-to-bgp/ | <a href="https://web.archive.org/web/*/https://www.jordanwhited.com/posts/corebgp-plugging-in-to-bgp/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<hr>
<p><img src="https://www.jordanwhited.com/posts/corebgp-plugging-in-to-bgp/cover.png" alt="cover"></p>
<hr>

<p><a href="https://tools.ietf.org/html/rfc4271" target="_blank">BGP</a> is one of many protocols that powers the Internet. Chances are you have heard of it, even if you don’t work in or around the computer networking space. If you aren’t familiar, I’ll try to provide some quick background:</p>
<ul>
<li>BGP is a <a href="https://en.wikipedia.org/wiki/Distance-vector_routing_protocol" target="_blank">distance-vector routing protocol</a> used to disseminate routing information.</li>
<li>A BGP speaker implements a <a href="https://en.wikipedia.org/wiki/Finite-state_machine" target="_blank">finite state machine</a> with 6 states:
<ul>
<li>Idle</li>
<li>Active</li>
<li>Connect</li>
<li>OpenSent</li>
<li>OpenConfirm</li>
<li>Established</li>
</ul>
</li>
<li>Inputs to the BGP FSM include messages, timer events, and administrative events.</li>
<li>Routing information is exchanged via UPDATE messages in the Established state.</li>
<li>BGP is extensible; speakers communicate their capabilities via OPEN messages.</li>
</ul>
<p>Expanding on that last bullet point, it’s difficult to summarize exactly how/where BGP is used due to its flexibility and extensibility. Various <a href="https://ietf.org/about/" target="_blank">IETF</a> Working Groups continue to publish BGP-related RFCs for a protocol that took shape in the early 90s. As the BGP landscape and application widens, we need software that enables us to keep up.</p>
<p>In this post I’ll provide some of my personal experience and history working with BGP, and introduce a new BGP library, <a href="https://github.com/jwhited/corebgp" target="_blank">CoreBGP</a>, which can be used to build the next generation of BGP-enabled applications.</p>

<p>In October of 2010 I attended my first <a href="https://www.nanog.org/" target="_blank">NANOG</a> meeting in Atlanta, GA after accidentally falling into the position of Network Operations Engineer at work. I worked for a modest-sized hosting provider at the time, and was intrigued with BGP. Upon arriving in Atlanta, I vaguely remember some confusion after telling a cab driver that the hotel I needed to be dropped at was on Peachtree St. I later learned that there are 71 streets in Atlanta with a variant of “Peachtree” in their name, according to <a href="https://en.wikipedia.org/wiki/Peachtree_Street#Nomenclature" target="_blank">Wikpedia</a>.</p>
<p>I got where I needed to go, eventually, and the first talk I attended was <a href="https://archive.nanog.org/meetings/nanog50/presentations/Sunday/NANOG50.Talk33.NANOG50-BGP-Techniques.pdf" target="_blank">BGP techniques for Internet Service Providers</a> by <a href="http://www.bgp4all.com.au/" target="_blank">Philip Smith</a>. Philip started with the basics before getting into the techniques used at ISPs. So many light bulbs went off for me during this talk. I have yet to see any other BGP presentation cover such a breadth of information but still do it in a way that is beginner-friendly, useful as a refresher for any expert, and just downright interesting.</p>
<p>Fast-forward 10 years and I’ve gained a fair share of experience operating networks that use BGP. In more recent years I’ve shifted to software engineering where I’ve had the opportunity to implement various BGP-enabled applications for network observability, data analytics, and SDN purposes.</p>
<p>Each time I started a new BGP-enabled app, I had to answer the following question – which existing BGP implementation should be its foundation?</p>

<p>Of the handful of open source BGP implementations out there, I’ve had hands-on experience with projects making use of:</p>
<ul>
<li><a href="https://bird.network.cz/" target="_blank">BIRD</a></li>
<li><a href="https://osrg.github.io/gobgp/" target="_blank">GoBGP</a></li>
<li><a href="https://www.opendaylight.org/what-we-do/odl-platform-overview" target="_blank">OpenDaylight</a></li>
<li><a href="https://www.quagga.net/" target="_blank">Quagga</a></li>
</ul>
<p>BIRD shines where a <a href="https://bird.network.cz/?get_doc&amp;v=20&amp;f=bird-5.html" target="_blank">rich policy language</a> is needed. GoBGP has a <a href="https://github.com/osrg/gobgp/tree/master/api" target="_blank">feature-rich gRPC API</a>, and can be embedded as a library. OpenDaylight’s BGP implementation is part of a larger SDN controller solution and has extensive support for <a href="https://docs.opendaylight.org/en/stable-oxygen/user-guide/bgpcep-guide/bgp/bgp-user-guide-linkstate-family.html" target="_blank">BGP-LS</a>. Quagga can reliably produce <a href="https://tools.ietf.org/html/rfc6396" target="_blank">MRT</a> dumps and has been around a long time, though I believe <a href="https://frrouting.org/" target="_blank">FRRouting</a> is now considered its successor.</p>
<p>These are all mature, established implementations. Some of them are in production at large ISPs, <a href="https://www.digitalocean.com/blog/scaling-droplet-public-networking/" target="_blank">Cloud Providers</a>, and <a href="https://joinup.ec.europa.eu/collection/open-source-observatory-osor/document/bird-manages-routing-worlds-largest-internet-exchanges-bird" target="_blank">Internet Exchange Points</a>. They are purpose-built and make various tradeoffs to suit their use cases (programming language, threading model, data structures, API, etc…).</p>
<p>But what if we are building something that doesn’t line up with the primary use cases of these widely used implementations? We may be locked in to decisions that are ultimately burdensome if we choose to build around them. Swapping in our own data structures for routing tables, or adding a new NLRI is non-trivial. Even if an implementation is intended to be embedded as library, it can still back us into a corner with resource consumption. There’s clearly a need to plug in or hook into specific parts of the BGP FSM, without inheriting decisions that went into a full-blown BGP daemon.</p>
<p>At the 27th IEEE International Conference On Network Protocols (ICNP), a group from the Université catholique de Louvain presented a paper on <code>The Case for Pluginized Routing Protocols</code>:</p>
<blockquote>
<p>Abstract—Routing protocols such as BGP and OSPF are key components of Internet Service Provider (ISP) networks. These protocols and the operator’s requirements evolve over time, but it often takes many years for network operators to convince their different router vendors and the IETF to extend routing protocols. Some network operators, notably in enterprise and datacenters have adopted Software Defined Networking (SDN) with its centralised control to be more agile. We propose a new approach to implement routing protocols that enables network operators to innovate while still using distributed routing protocols and thus keeping all their benefits compared to centralised routing approaches. We extend a routing protocol with a virtual machine that is capable of executing plugins. These plugins extend the protocol or modify its underlying algorithms through a simple API to meet the specific requirements of operators. We modify the OSPF and BGP implementations provided by FRRouting and demonstrate the applicability of our approach with several use cases.</p>
<p>— <!-- raw HTML omitted -->The Case for Pluginized Routing Protocols<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup><!-- raw HTML omitted --></p>
</blockquote>
<p>In their paper they present a method for plugging into a previously mentioned open-source BGP implementation, FRRouting. Plugins exist at a function level, either prior to invocation (PRE), as a replacement (REPLACE), or just before returning (POST). Much of their BGP plugin focus is around the reception of messages, and decisions made shortly after:</p>
<blockquote>
<p>The BGP daemon is also extended similarly. We add insertion points on functions receiving BGP messages from neighbours, on filters and inside the decision process. We also expose specific functions to the plugins that are executed by the uBPF VM.</p>
<p>— <!-- raw HTML omitted -->The Case for Pluginized Routing Protocols<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup><!-- raw HTML omitted --></p>
</blockquote>
<p>They take a clever approach with plugin sandboxing by leveraging a user space eBPF VM (<a href="https://github.com/iovisor/ubpf" target="_blank">uBPF</a>) linked to the FRRouting protocol implementation. Each plugin compiles to eBPF bytecode and runs inside of said VM. Plugins can be loaded and unloaded without impacting the primary protocol implementation. Using an eBPF VM also allowed them to utilise all the pre-existing Linux Kernel tooling.</p>
<p>I found this approach inspiring, but still not quite a match for my use cases:</p>
<ul>
<li>Plugins appear to be built around “incoming” events, or messages. What if I want to inject an UPDATE message to a peer irrespective of what FRRouting wants to send?</li>
<li>FRRouting was not built with this plugin model in mind. Changes/Updates to FRRouting will result in a maintenance headache for the VM hook points.</li>
<li>eBPF bytecode is typically compiled from C. Writing C can be time-consuming in comparison to more modern languages.</li>
<li>I need to be an FRRouting expert to do anything non-trivial.</li>
</ul>
<p>This experience and research led me to create CoreBGP, a BGP library that I could re-use across my BGP-enabled applications.</p>

<p>CoreBGP is a BGP library written in Go that implements the BGP FSM with an event-driven, pluggable model. It exposes an API that empowers the user to:</p>
<ul>
<li>send and validate OPEN message capabilities</li>
<li>handle “important” state transitions</li>
<li>handle incoming UPDATE messages</li>
<li>send outgoing UPDATE messages</li>
</ul>
<p>CoreBGP does not decode UPDATE messages (besides header validation), manage a routing table, or send its own UPDATE messages. These responsibilities are all passed down to the user. Therefore, the intended user is someone who wants that responsibility.</p>
<p>The primary building block of CoreBGP is a Plugin, defined by the following interface:</p>
<div><pre><code data-lang="go"><span>// Plugin is a BGP peer plugin.
</span><span></span><span>type</span> Plugin <span>interface</span> {
	<span>// GetCapabilities is fired when a peer's FSM is in the Connect state prior
</span><span></span>	<span>// to sending an Open message. The returned capabilities are included in the
</span><span></span>	<span>// Open message sent to the peer.
</span><span></span>	<span>GetCapabilities</span>(peer <span>*</span>PeerConfig) []<span>*</span>Capability

	<span>// OnOpenMessage is fired when an Open message is received from a peer
</span><span></span>	<span>// during the OpenSent state. Returning a non-nil Notification will cause it
</span><span></span>	<span>// to be sent to the peer and the FSM will transition to the Idle state.
</span><span></span>	<span>//
</span><span></span>	<span>// Per RFC5492 a BGP speaker should only send a Notification if a required
</span><span></span>	<span>// capability is missing; unknown or unsupported capabilities should be
</span><span></span>	<span>// ignored.
</span><span></span>	<span>OnOpenMessage</span>(peer <span>*</span>PeerConfig, capabilities []<span>*</span>Capability) <span>*</span>Notification

	<span>// OnEstablished is fired when a peer's FSM transitions to the Established
</span><span></span>	<span>// state. The returned UpdateMessageHandler will be fired when an Update
</span><span></span>	<span>// message is received from the peer.
</span><span></span>	<span>//
</span><span></span>	<span>// The provided writer can be used to send Update messages to the peer for
</span><span></span>	<span>// the lifetime of the FSM's current, established state. It should be
</span><span></span>	<span>// discarded once OnClose() fires.
</span><span></span>	<span>OnEstablished</span>(peer <span>*</span>PeerConfig, writer UpdateMessageWriter) UpdateMessageHandler

	<span>// OnClose is fired when a peer's FSM transitions out of the Established
</span><span></span>	<span>// state.
</span><span></span>	<span>OnClose</span>(peer <span>*</span>PeerConfig)
}
</code></pre></div><p>Here’s an example Plugin that logs when a peer enters/leaves an established state and when an UPDATE message is received:</p>
<div><pre><code data-lang="go"><span>type</span> plugin <span>struct</span>{}

<span>func</span> (p <span>*</span>plugin) <span>GetCapabilities</span>(c <span>*</span>corebgp.PeerConfig) []<span>*</span>corebgp.Capability {
	caps <span>:=</span> <span>make</span>([]<span>*</span>corebgp.Capability, <span>0</span>)
	<span>return</span> caps
}

<span>func</span> (p <span>*</span>plugin) <span>OnOpenMessage</span>(peer <span>*</span>corebgp.PeerConfig, capabilities []<span>*</span>corebgp.Capability) <span>*</span>corebgp.Notification {
	<span>return</span> <span>nil</span>
}

<span>func</span> (p <span>*</span>plugin) <span>OnEstablished</span>(peer <span>*</span>corebgp.PeerConfig, writer corebgp.UpdateMessageWriter) corebgp.UpdateMessageHandler {
	log.<span>Println</span>(<span>"peer established"</span>)
	<span>// send End-of-Rib
</span><span></span>	writer.<span>WriteUpdate</span>([]<span>byte</span>{<span>0</span>, <span>0</span>, <span>0</span>, <span>0</span>})
	<span>return</span> p.handleUpdate
}

<span>func</span> (p <span>*</span>plugin) <span>OnClose</span>(peer <span>*</span>corebgp.PeerConfig) {
	log.<span>Println</span>(<span>"peer closed"</span>)
}

<span>func</span> (p <span>*</span>plugin) <span>handleUpdate</span>(peer <span>*</span>corebgp.PeerConfig, u []<span>byte</span>) <span>*</span>corebgp.Notification {
	log.<span>Printf</span>(<span>"got update message of len: %d"</span>, <span>len</span>(u))
	<span>return</span> <span>nil</span>
}
</code></pre></div><p>Plugins are attached to peers when they are added to the Server, which manages their lifetime:</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.jordanwhited.com/posts/corebgp-plugging-in-to-bgp/">https://www.jordanwhited.com/posts/corebgp-plugging-in-to-bgp/</a></em></p>]]>
            </description>
            <link>https://www.jordanwhited.com/posts/corebgp-plugging-in-to-bgp/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23744167</guid>
            <pubDate>Mon, 06 Jul 2020 02:39:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Rust for JavaScript Developers – Functions and Control Flow]]>
            </title>
            <description>
<![CDATA[
Score 101 | Comments 39 (<a href="https://news.ycombinator.com/item?id=23743363">thread link</a>) | @rkwz
<br/>
July 5, 2020 | http://www.sheshbabu.com/posts/rust-for-javascript-developers-functions-and-control-flow/ | <a href="https://web.archive.org/web/*/http://www.sheshbabu.com/posts/rust-for-javascript-developers-functions-and-control-flow/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>This is the third part in a series about introducing the Rust language to JavaScript developers. Here are the past chapters:</p>
<ol>
<li><a href="http://www.sheshbabu.com/posts/rust-for-javascript-developers-tooling-ecosystem-overview/">Tooling Ecosystem Overview</a></li>
<li><a href="http://www.sheshbabu.com/posts/rust-for-javascript-developers-variables-and-data-types/">Variables and Data Types</a></li>
</ol>
<h2 id="Functions"><a href="#Functions" title="Functions"></a>Functions</h2><p>Rust’s function syntax is pretty much similar to the one in JavaScript.</p>
<pre><code><span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> income <span>=</span> <span>100</span><span>;</span>
  <span>let</span> tax <span>=</span> <span>calculate_tax</span><span>(</span>income<span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"{}"</span><span>,</span> tax<span>)</span><span>;</span>
<span>}</span>

<span>fn</span> <span>calculate_tax</span><span>(</span>income<span>:</span> i32<span>)</span> <span>-&gt;</span> i32 <span>{</span>
  <span>return</span> income <span>*</span> <span>90</span> <span>/</span> <span>100</span><span>;</span>
<span>}</span></code></pre>
<p>The only difference you might see above is the type annotations for arguments and return values.</p>
<p>The <code>return</code> keyword can be skipped and it’s very common to see code without an explicit return. If you’re returning implicitly, make sure to remove the semicolon from that line. The above function can be refactored as:</p>
<pre><code>fn main() {
  let income = 100;
  let tax = calculate_tax(income);
  println!("{}", tax);
}

fn calculate_tax(income: i32) -&gt; i32 {
<span>- return income * 90 / 100;</span>
<span>+ income * 90 / 100</span>
}</code></pre>
<h2 id="Arrow-Functions"><a href="#Arrow-Functions" title="Arrow Functions"></a>Arrow Functions</h2><p>Arrow functions are a popular feature in modern JavaScript - they allow us to write functional code in a concise way.</p>
<p>Rust has something similar and they are called “Closures”. The name might be a bit confusing and would require getting used to because in JavaScript, closures can be created using both normal and arrow functions.</p>
<p>Rust’s closure syntax is very similar to JavaScript’s arrow functions:</p>
<p><strong>Without arguments:</strong></p>
<pre><code>
<span>let</span> greet <span>=</span> <span>(</span><span>)</span> <span>=</span><span>&gt;</span> console<span>.</span><span>log</span><span>(</span><span>"hello"</span><span>)</span><span>;</span>

<span>greet</span><span>(</span><span>)</span><span>;</span> </code></pre>
<pre><code>
<span>let</span> greet <span>=</span> <span>||</span> <span>println!</span><span>(</span><span>"hello"</span><span>)</span><span>;</span>

<span>greet</span><span>(</span><span>)</span><span>;</span> </code></pre>
<p><strong>With arguments:</strong></p>
<pre><code>
<span>let</span> greet <span>=</span> <span>(</span>msg<span>)</span> <span>=</span><span>&gt;</span> console<span>.</span><span>log</span><span>(</span>msg<span>)</span><span>;</span>

<span>greet</span><span>(</span><span>"good morning!"</span><span>)</span><span>;</span> </code></pre>
<pre><code>
<span>let</span> greet <span>=</span> <span>|</span>msg<span>:</span> <span>&amp;</span>str<span>|</span> <span>println!</span><span>(</span><span>"{}"</span><span>,</span> msg<span>)</span><span>;</span>

<span>greet</span><span>(</span><span>"good morning!"</span><span>)</span><span>;</span> </code></pre>
<p><strong>Returning values:</strong></p>
<pre><code>
<span>let</span> add <span>=</span> <span>(</span>a<span>,</span> b<span>)</span> <span>=</span><span>&gt;</span> a <span>+</span> b<span>;</span>

<span>add</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span><span>;</span> </code></pre>
<pre><code>
<span>let</span> add <span>=</span> <span><span>|</span>a<span>:</span> i32<span>,</span> b<span>:</span> i32<span>|</span></span> <span>-&gt;</span> i32 <span>{</span> a <span>+</span> b <span>}</span><span>;</span>

<span>add</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span><span>;</span> </code></pre>
<p><strong>Multiline:</strong></p>
<pre><code>
<span>let</span> add <span>=</span> <span>(</span>a<span>,</span> b<span>)</span> <span>=</span><span>&gt;</span> <span>{</span>
  <span>let</span> sum <span>=</span> a <span>+</span> b<span>;</span>
  <span>return</span> sum<span>;</span>
<span>}</span><span>;</span>

<span>add</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span><span>;</span> </code></pre>
<pre><code>
<span>let</span> add <span>=</span> <span><span>|</span>a<span>:</span> i32<span>,</span> b<span>:</span> i32<span>|</span></span> <span>-&gt;</span> i32 <span>{</span>
  <span>let</span> sum <span>=</span> a <span>+</span> b<span>;</span>
  <span>return</span> sum<span>;</span>
<span>}</span><span>;</span>

<span>add</span><span>(</span><span>1</span><span>,</span> <span>2</span><span>)</span><span>;</span> </code></pre>
<p>Here’s a cheatsheet:<br><img src="http://www.sheshbabu.com/images/2020-rust-for-javascript-developers-3/image-2.png" alt=""></p>
<p>Closures don’t need the type annotations most of the time, but I’ve added them here for clarity.</p>
<h2 id="If-Else"><a href="#If-Else" title="If Else"></a>If Else</h2><pre><code><span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> income <span>=</span> <span>100</span><span>;</span>
  <span>let</span> tax <span>=</span> <span>calculate_tax</span><span>(</span>income<span>)</span><span>;</span>
  <span>println!</span><span>(</span><span>"{}"</span><span>,</span> tax<span>)</span><span>;</span>
<span>}</span>

<span>fn</span> <span>calculate_tax</span><span>(</span>income<span>:</span> i32<span>)</span> <span>-&gt;</span> i32 <span>{</span>
  <span>if</span> income <span>&lt;</span> <span>10</span> <span>{</span>
    <span>return</span> <span>0</span><span>;</span>
  <span>}</span> <span>else</span> <span>if</span> income <span>&gt;=</span> <span>10</span> <span>&amp;&amp;</span> income <span>&lt;</span> <span>50</span> <span>{</span>
    <span>return</span> <span>20</span><span>;</span>
  <span>}</span> <span>else</span> <span>{</span>
    <span>return</span> <span>50</span><span>;</span>
  <span>}</span>
<span>}</span></code></pre>
<h2 id="Loops"><a href="#Loops" title="Loops"></a>Loops</h2><p>While loops:</p>
<pre><code><span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> <span>mut</span> count <span>=</span> <span>0</span><span>;</span>

  <span>while</span> count <span>&lt;</span> <span>10</span> <span>{</span>
    <span>println!</span><span>(</span><span>"{}"</span><span>,</span> count<span>)</span><span>;</span>
    count <span>+=</span> <span>1</span><span>;</span>
  <span>}</span>
<span>}</span></code></pre>
<p>Normal <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/for" target="_blank" rel="noopener">for loops</a> don’t exist in Rust, we need to use <code>while</code> or <code>for..in</code> loops. <code>for..in</code> loops are similar to the <code>for..of</code> loops in JavaScript and they loop over an iterator.</p>
<pre><code><span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> numbers <span>=</span> <span>[</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>,</span> <span>4</span><span>,</span> <span>5</span><span>]</span><span>;</span>

  <span>for</span> n <span>in</span> numbers<span>.</span><span>iter</span><span>(</span><span>)</span> <span>{</span>
    <span>println!</span><span>(</span><span>"{}"</span><span>,</span> n<span>)</span><span>;</span>
  <span>}</span>
<span>}</span></code></pre>
<p>Notice that we’re not iterating directly over the array but instead using the <code>iter</code> method of the array.</p>
<p>We can also loop over <a href="https://doc.rust-lang.org/reference/expressions/range-expr.html" target="_blank" rel="noopener">ranges</a>:</p>
<pre><code><span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>for</span> n <span>in</span> <span>1</span><span>..</span><span>5</span> <span>{</span>
    <span>println!</span><span>(</span><span>"{}"</span><span>,</span> n<span>)</span><span>;</span>
  <span>}</span>
<span>}</span></code></pre>
<h2 id="Iterators"><a href="#Iterators" title="Iterators"></a>Iterators</h2><p>In JavaScript, we can use array methods like map/filter/reduce/etc instead of <code>for</code> loops to perform calculations or transformations on an array.</p>
<p>For example, here we take an array of numbers, double them and filter out the elements that are less than 10:</p>
<pre><code><span>function</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> numbers <span>=</span> <span>[</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>,</span> <span>4</span><span>,</span> <span>5</span><span>]</span><span>;</span>

  <span>let</span> double <span>=</span> <span>(</span>n<span>)</span> <span>=</span><span>&gt;</span> n <span>*</span> <span>2</span><span>;</span>
  <span>let</span> less_than_ten <span>=</span> <span>(</span>n<span>)</span> <span>=</span><span>&gt;</span> n <span>&lt;</span> <span>10</span><span>;</span>

  <span>let</span> result <span>=</span> numbers<span>.</span><span>map</span><span>(</span>double<span>)</span><span>.</span><span>filter</span><span>(</span>less_than_ten<span>)</span><span>;</span>

  console<span>.</span><span>log</span><span>(</span>result<span>)</span><span>;</span> 
<span>}</span></code></pre>
<p>In Rust, we can’t directly use map/filter/etc over vectors, we need to follow these steps:</p>
<ol>
<li>Convert the vector into an iterator using <code>iter</code>, <code>into_iter</code> or <code>iter_mut</code> methods</li>
<li>Chain <code>adapters</code> such as map/filter/etc on the iterator</li>
<li>Finally convert the iterator back to a vector using <code>consumers</code> such as <code>collect</code>, <code>find</code>, <code>sum</code> etc</li>
</ol>
<p>Here’s the equivalent Rust code:</p>
<pre><code><span>fn</span> <span>main</span><span>(</span><span>)</span> <span>{</span>
  <span>let</span> numbers <span>=</span> <span>vec!</span><span>[</span><span>1</span><span>,</span> <span>2</span><span>,</span> <span>3</span><span>,</span> <span>4</span><span>,</span> <span>5</span><span>]</span><span>;</span>

  <span>let</span> double <span>=</span> <span><span>|</span>n<span>:</span> <span>&amp;</span>i32<span>|</span></span> <span>-&gt;</span> i32 <span>{</span> n <span>*</span> <span>2</span> <span>}</span><span>;</span>
  <span>let</span> less_than_10 <span>=</span> <span><span>|</span>n<span>:</span> <span>&amp;</span>i32<span>|</span></span> <span>-&gt;</span> bool <span>{</span> <span>*</span>n <span>&lt;</span> <span>10</span> <span>}</span><span>;</span>

  <span>let</span> result<span>:</span> Vec<span>&lt;</span>i32<span>&gt;</span> <span>=</span> numbers<span>.</span><span>iter</span><span>(</span><span>)</span><span>.</span><span>map</span><span>(</span>double<span>)</span><span>.</span><span>filter</span><span>(</span>less_than_10<span>)</span><span>.</span><span>collect</span><span>(</span><span>)</span><span>;</span>

  <span>println!</span><span>(</span><span>"{:?}"</span><span>,</span> result<span>)</span><span>;</span> 
<span>}</span></code></pre>
<p>You should be able to understand most of the code above but you might notice few things off here:</p>
<ul>
<li>The usage of <code>&amp;</code> and <code>*</code> in the closure</li>
<li>The <code>Vec&lt;i32&gt;</code> type annotation for the <code>result</code> variable</li>
</ul>
<p>The <code>&amp;</code> is the reference operator and the <code>*</code> is the dereference operator. The <code>iter</code> method instead of copying the elements in the vector, it passes them as references to the next adapter in the chain. This is why we use <code>&amp;i32</code> in the map’s closure (double). This closure returns <code>i32</code> but <a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.filter" target="_blank" rel="noopener">filter</a> calls its closure (less_than_10) with reference so that’s why we need to use <code>&amp;i32</code> again. To dereference the argument, we use the <code>*</code> operator. We’ll cover this in more detail in future chapters.</p>
<p>Regarding <code>Vec&lt;i32&gt;</code>, so far we haven’t added type annotations to variables as Rust can infer the types automatically, but for <code>collect</code>, we need to be explicitly tell Rust that we expect a <code>Vec&lt;i32&gt;</code> output.</p>
<p>Aside from map and filter, there are ton of other <a href="https://doc.rust-lang.org/std/iter/trait.Iterator.html" target="_blank" rel="noopener">useful adapters</a> that we can use in iterators.</p>
<p>Thanks for reading! Feel free to follow me in <a href="https://twitter.com/sheshbabu" target="_blank" rel="noopener">Twitter</a> for updates :)</p>
</div></div>]]>
            </description>
            <link>http://www.sheshbabu.com/posts/rust-for-javascript-developers-functions-and-control-flow/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23743363</guid>
            <pubDate>Mon, 06 Jul 2020 00:07:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why I’m Writing a Book on Cryptography]]>
            </title>
            <description>
<![CDATA[
Score 171 | Comments 23 (<a href="https://news.ycombinator.com/item?id=23743218">thread link</a>) | @gedigi
<br/>
July 5, 2020 | https://www.cryptologie.net/article/504/why-im-writing-a-book-on-cryptography/ | <a href="https://web.archive.org/web/*/https://www.cryptologie.net/article/504/why-im-writing-a-book-on-cryptography/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>

<p>I’ve now been writing a book on <strong>applied cryptography</strong> for a year and a half.
I’m nearing the end of my journey, as I have one last ambitious chapter left to write: next-generation cryptography (a chapter that I’ll use to talk about cryptography that will become more and more practical: post-quantum cryptography, homomorphic encryption, multi-party computation, and zk-SNARKs).</p>
<p>I’ve been asked multiple times <strong>why write a new book about cryptography?</strong> and <strong>why should I read your book?</strong>.
To answer this, you have to understand when it all started…</p>
<h2>Diagrams are everything</h2>
<p>Today if you want to learn about almost anything, you just google it.
Yet, for cryptography, and depending on what you're looking for, resources can be quite lacking.</p>
<p>It all started a long time ago.
For a class, I had to implement a <a href="https://www.paulkocher.com/doc/DifferentialPowerAnalysis.pdf">differential power analysis attack</a>, a breakthrough in cryptanalysis as it was the first side-channel attack to be published.
A differential power analysis uses the power consumption of a device during an encryption to leak its private key.
At the time, I realized that great papers could convey great ideas with very little emphasis on understanding.
I remember banging my head against the wall trying to figure out what the author of the white paper was trying to say.
Worse, I couldn’t find a good resource that explained the paper.
So I banged my head a bit more, and finally I got it.
And then I thought I would help others.
So I drew some diagrams, animated them, and recorded myself going over them.
That was <a href="https://www.youtube.com/watch?v=gbqNCgVcXsM">my first screencast</a>.</p>
<p>This first step in education was enough to make me want to do more.
I started making more of these videos, and started writing more articles about cryptography on this blog (today totaling more than 500 articles).</p>
<p><img alt="we want to know" src="https://www.cryptologie.net/upload/we_want_to_know.png"></p>
<p>I realized early that diagrams were extremely helpful to understand complicated concepts, and that strangely most resources in the field shied away from them.</p>
<p>For example, anyone in cryptography who thinks about AES-CBC would immediately think about the following wikipedia diagram:</p>
<p><img alt="aes cbc" src="https://www.cryptologie.net/upload/600px-CBC_encryption.svg_.png"></p>
<p>So here I was, trying to explain everything I learned, and thinking hard about what sorts of simple diagrams could easily convey these complex ideas.
That’s when I started thinking about a book, years and years before <a href="https://manning.com/">Manning Publications</a> would reach out to me with a book deal.</p>
<h2>The applied cryptographer curriculum</h2>
<p> I hadn’t started cryptography due to a long-life passion.
I had finished a bachelor in theoretical mathematics and didn’t know what was next for me.
I had also been programming my whole life, and I wanted to reconcile the two.
Naturally, I got curious about cryptography, which seemed to have the best of both world, and started reading the different books at my disposal.
I quickly discovered my life's calling.</p>
<p>Some things were annoying me though. In particular, the long introductions that would start with history.
I was only interested in the technicalities, and always had been.
I swore to myself, if I ever wrote a book about cryptography, I would not write a single line on Vigenère ciphers, Caesar ciphers, and others.</p>
<p>And so after applying to the masters of Cryptography at the university of Bordeaux, and obtaining a degree in the subject, I thought I was ready for the world.
Little did I know.
What I thought was a very applied degree actually lacked a lot on the real world protocols I was about to attack.
I had spent a lot of time learning about the mathematics of elliptic curves, but nothing about how they were used in cryptographic algorithms.
I had learned about LFSRs, and ElGamal, and DES, and a series of other cryptographic primitives that I would never see again.</p>
<p>When I started working in the industry at Matasano, which then became NCC Group, my first gig was to audit <a href="https://www.openssl.org/">OpenSSL</a> (the most popular TLS implementation).
Oh boy, did it hurt my brain.
I remember coming back home every day with a strong headache.
What a clusterfuck of a library.
I had no idea at the time that I would years later become a co-author of TLS 1.3.</p>
<p><img alt="sign" src="https://www.cryptologie.net/upload/7._Note_that_digital_signatures_are_specified_with_a_hash_function,_allowing_you_to_.png"></p>
<p>But at that point I was already thinking: this is what I should have learned in school.
The knowledge I’m getting now is what would have been useful to prepare me for the real world.
After all, I was now a security practitioner specialized in cryptography.
I was reviewing real-world cryptographic applications.
I was doing the job that one would wish they had after finishing a cryptography degree.
I implemented, verified, used, and advised on what cryptographic algorithms to use.</p>
<p>This is the reason I’m the first reader of the book I’m writing.
This is what I would have written to my past self in order to prepare me for the real world.</p>
<h2>The use of cryptography is where most of the bugs are</h2>
<p>My consulting job led me to audit many real world cryptographic applications like the <a href="https://www.nccgroup.com/us/about-us/newsroom-and-events/blog/2015/may/openssl-audit/">OpenSSL</a>, the <a href="https://www.nccgroup.trust/globalassets/our-research/us/public-reports/2018/final_public_report_ncc_group_google_encryptedbackup_2018-10-10_v1.0.pdf">encrypted backup system of Google</a>, the <a href="https://blog.cloudflare.com/ncc-groups-cryptography-services-audit-of-tls-1-3/">TLS 1.3 implementation of Cloudflare</a>, the <a href="https://letsencrypt.org/2015/04/14/ncc-group-audit.html">certificate authority protocol of Let’s Encrypt</a>, the <a href="https://www.nccgroup.com/us/our-research/zcash-overwinter-consensus-and-sapling-cryptography-review/">sapling protocol of Zcash</a>, the <a href="https://blog.nucypher.com/security-audits--round-1--3/">threshold proxy re-encryption scheme of NuCypher</a> and dozens and dozens of other real-world cryptographic applications that I unfortunately cannot mention publicly.</p>
<p>Early in my job, I was tasked to audit the custom protocol a big corporation (that I can’t name) had written to encrypt their communications.
It turns out that, they were signing everything but the ephemeral keys, which completely broke the whole protocol (as one could have easily replaced the ephemeral keys).
A rookie mistake from anyone with some experience with secure transport protocols, but something that was missed by people who thought they were experienced enough to roll their own crypto.
I remember explaining the vulnerability at the end of the engagement, and a room full of engineers turning silent for a good 30 seconds.</p>
<p>This story repeated itself many times during my career.
There was this time where while auditing a cryptocurrency for another client, I found a way to forge transactions from already existing ones (due to some ambiguity of what was being signed).
Looking at TLS implementations for another client, I found some subtle ways to break an RSA implementation, which in turned transformed into a white paper (with one of the inventor of RSA) leading to a number of <a href="https://eprint.iacr.org/2018/1173">Common Vulnerabilities and Exposures (CVEs) reported to a dozen of open source projects</a>.
More recently, reading about Matrix as part of writing my book, I realized that their authentication protocol was completely broken, <a href="https://matrix.org/security-disclosure-policy/">leading to a complete break of their end-to-end encryption</a>.</p>
<p><img alt="comic" src="https://www.cryptologie.net/upload/HEY_MERE_S_AN.png"></p>
<p>There’s so many details that can unfortunately collapse under you, when making use of cryptography.
At that point, I knew I had to write something about it.
This is why my book contains many of these anecdotes.</p>
<p>As part of the job, I would review cryptography libraries and applications in a multitude of programming languages.
I discovered bugs (for example <a href="https://cryptologie.net/article/347/my-first-cve-o/?utm_content=buffer5c408&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer">CVE-2016-3959</a> in Golang’s standard library), I researched ways that libraries could fool you into misusing them (for example see my paper <a href="https://eprint.iacr.org/2016/644">How to Backdoor Diffie-Hellman</a>), and I advised on what libraries to use.
Developers never knew what library to use, and I always found the answer to be tricky.</p>
<p>I went on to invent the <a href="https://discocrypto.com/">disco protocol</a>, and wrote a fully-featured cryptographic library in less than 1,000 lines of code in several languages.
Disco only relied on two cryptographic primitives: the permutation of SHA-3 and curve25519.
Yes, from only these two things in 1,000 lines of code a developer could do any type of authenticated key exchange, signatures, encryption, MACs, hashing, key derivation, etc.
This gave me a unique perspective as to what a good cryptographic library was supposed to be.</p>
<p>I wanted my book to contain these kind of practical insights.
So naturally, the different chapters contain examples on how to do crypto in different programming languages, using well-respected cryptographic libraries.</p>
<h2>A need for a new book?</h2>
<p>As I was giving <a href="https://www.blackhat.com/us-17/training/beyond-the-beast-a-broad-survey-of-crypto-vulnerabilities.html">one of my annual cryptography training at Black Hat</a>, one student came to me and asked if I could recommend a good book or online course on cryptography.
I remember advising the student to read <a href="http://toc.cryptobook.us/">the book from Boneh &amp; Shoup</a> and <a href="https://crypto.stanford.edu/~dabo/courses/OnlineCrypto/">Cryptography I from Boneh on Coursera</a>.</p>
<p>The student told me “<em>Ah, I tried, it’s too theoretical!</em>”.
This answer stayed with me.
I disagreed at first, but slowly realized that they were right.
Most of these resources were pretty heavy in math, and most developers interacting with cryptography don’t want to deal with math.
 What else was there for them?
The other two somewhat respected resources at the time were Applied Cryptography and Cryptography Engineering (both from Schneier).
But these books were starting to be quite outdated.
Applied Cryptography spent 4 chapters on block ciphers, with a whole chapter on cipher modes of operation but none on authenticated encryption.
Cryptography Engineering had a single mention of elliptic curve cryptography (in a footnote).</p>
<p>On the other hand, many of my videos or blog posts were becoming good primary references for some cryptographic concepts.</p>
<p><strong>I knew I could do something special</strong>.</p>
<p>Gradually, many of my students started becoming interested in cryptocurrencies, asking more and more questions on the subject.
At the same time, I started to audit more and more cryptocurrency applications.
I finally moved to a job at Facebook to work on <a href="https://libra.org/">Libra</a>.
Cryptocurrency was now one of the hottest field to work on, mixing a multitude of extremely interesting cryptographic primitives that so far had seen no real-world use case (zero knowledge proofs, aggregated signatures, threshold cryptography, multi-party computations, consensus protocols, cryptographic accumulators, verifiable random functions, verifiable delay functions, ... the list goes on)</p>
<p><strong>I was now in a unique position</strong>.</p>
<p>I knew I could write something that would tell students, developers, consultants, security engineers, and others, what modern applied cryptography was all about.</p>
<p><img alt="book" src="https://www.cryptologie.net/upload/needs_to_send_a_let.png"></p>
<p>This was going to be a book with very little …</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.cryptologie.net/article/504/why-im-writing-a-book-on-cryptography/">https://www.cryptologie.net/article/504/why-im-writing-a-book-on-cryptography/</a></em></p>]]>
            </description>
            <link>https://www.cryptologie.net/article/504/why-im-writing-a-book-on-cryptography/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23743218</guid>
            <pubDate>Sun, 05 Jul 2020 23:45:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Modern Object Pascal Introduction for Programmers]]>
            </title>
            <description>
<![CDATA[
Score 96 | Comments 36 (<a href="https://news.ycombinator.com/item?id=23742999">thread link</a>) | @eatonphil
<br/>
July 5, 2020 | http://newpascal.org/assets/modern_pascal_introduction.html | <a href="https://web.archive.org/web/*/http://newpascal.org/assets/modern_pascal_introduction.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">
<div>
<h2 id="_why">1. Why</h2>
<div>
<div>
<table>
<tbody><tr>
<td>
<p>Note</p>
</td>
<td>
This is a modified version of the original document from Michalis, because we (authors of <a href="http://newpascal.org/">http://newpascal.org/</a> and <a href="http://synopse.info/">http://synopse.info/</a> ) prefer the "mode delphi" without the "generic" / "specialize" keywords.
</td>
</tr>
</tbody></table>
</div>
<p>There are many books and resources about Pascal out there, but too many of them talk about the old Pascal, without classes, units or generics.</p>
<p>So I wrote this quick introduction to what I call <strong>modern Object Pascal</strong>. Most of the programmers using it don’t really call it <em>"modern Object Pascal"</em>, we just call it  <em>"our Pascal"</em>. But when introducing the language, I feel it’s important to emphasize that it’s a modern, object-oriented language. It evolved a <strong>lot</strong> since the old (Turbo) Pascal that many people learned in schools long time ago. Feature-wise, it’s quite similar to C++ or Java or C#.</p>
<div>
<ul>
<li>
<p>It has all the modern features you expect — classes, units, interfaces, generics…​</p>
</li>
<li>
<p>It’s compiled to a fast, native code,</p>
</li>
<li>
<p>It’s very type safe,</p>
</li>
<li>
<p>High-level but can also be low-level if you need it to be.</p>
</li>
</ul>
</div>
<p>It also has excellent, portable and open-source compiler called the <em>Free Pascal Compiler</em>, <a href="http://freepascal.org/">http://freepascal.org/</a> . And an accompanying IDE (editor, debugger, a library of visual components, form designer) called <em>Lazarus</em> <a href="http://lazarus.freepascal.org/">http://lazarus.freepascal.org/</a> . Myself, I’m the creator of <em>Castle Game Engine</em>, <a href="https://castle-engine.io/">https://castle-engine.io/</a> , which is a cool portable 3D and 2D game engine using this language to create games on many platforms (Windows, Linux, MacOSX, Android, iOS, web plugin).</p>
<p>This introduction is mostly directed at programmers who already have experience in other languages. We will not cover here the meanings of some universal concepts, like <em>"what is a class"</em>, we’ll only show how to do them in Pascal.</p>
</div>
</div>
<div>
<h2 id="_basics">2. Basics</h2>
<div>
<div>
<h3 id="__hello_world_program">2.1. "Hello world" program</h3>
<div>
<div>
<pre><code data-lang="pascal"><span>{$mode delphi}</span> 

<span>program</span> MyProgram; 
<span>begin</span>
  Writeln(<span><span>'</span><span>Hello world!</span><span>'</span></span>);
<span>end</span>.</code></pre>
</div>
</div>
<p>This is a complete program that you can <em>compile</em> and <em>run</em>.</p>
<div>
<ul>
<li>
<p>If you use the command-line FPC, just create a new file <code>myprogram.lpr</code> and execute <code>fpc myprogram.lpr</code>.</p>
</li>
<li>
<p>If you use <em>Lazarus</em>, create a new project (menu <em>Project</em> → <em>New Project</em> → <em>Simple Program</em>). Save it as <code>myprogram</code> and paste this source code as the main file. Compile using the menu item <em>Run → Compile</em>.</p>
</li>
<li>
<p>This is a command-line program, so in either case — just run the compiled executable from the command-line.</p>
</li>
</ul>
</div>
<p>The rest of this article talks about the Object Pascal language, so don’t expect to see anything more fancy than the command-line stuff. If you want to see something cool, just create a new GUI project in <em>Lazarus</em> (<em>Project</em> → <em>New Project</em> → <em>Application</em>).
Voila — a working GUI application, cross-platform, with native look everywhere, using a comfortable visual component library. The <em>Lazarus</em> and <em>Free Pascal Compiler</em> come with lots of ready units for networking, GUI, database, file formats (XML, json, images…​), threading and everything else you may need. I already mentioned my cool <em>Castle Game Engine</em> earlier:)</p>
</div>
<div>
<h3 id="_functions_procedures_primitive_types">2.2. Functions, procedures, primitive types</h3>
<div>
<div>
<pre><code data-lang="pascal"><span>{$mode delphi}</span>

<span>program</span> MyProgram;

<span>procedure</span> MyProcedure(<span>const</span> A: Integer);
<span>begin</span>
  Writeln(<span><span>'</span><span>A + 10 is: </span><span>'</span></span>, A + <span>10</span>);
<span>end</span>;

<span>function</span> MyFunction(<span>const</span> S: <span>string</span>): <span>string</span>;
<span>begin</span>
  Result := S + <span><span>'</span><span>strings are automatically managed</span><span>'</span></span>;
<span>end</span>;

<span>var</span>
  X: Single;
<span>begin</span>
  Writeln(MyFunction(<span><span>'</span><span>Note: </span><span>'</span></span>));
  MyProcedure(<span>5</span>);

  
  X := <span>15</span> / <span>5</span>;
  Writeln(<span><span>'</span><span>X is now: </span><span>'</span></span>, X); 
  Writeln(<span><span>'</span><span>X is now: </span><span>'</span></span>, X:<span>1</span>:<span>2</span>); 
<span>end</span>.</code></pre>
</div>
</div>
<p>To return a value from a function, assign something to the magic <code>Result</code> variable. You can read and set the <code>Result</code> freely, just like a local variable.</p>
<div>
<div>
<pre><code data-lang="pascal"><span>function</span> MyFunction(<span>const</span> S: <span>string</span>): <span>string</span>;
<span>begin</span>
  Result := S + <span><span>'</span><span>something</span><span>'</span></span>;
  Result := Result + <span><span>'</span><span> something more!</span><span>'</span></span>;
  Result := Result + <span><span>'</span><span> and more!</span><span>'</span></span>;
<span>end</span>;</code></pre>
</div>
</div>
<p>You can also treat the function name (like <code>MyFunction</code> in example above) as the variable, to which you can assign. But I would discourage it in new code, as it looks "fishy" when used on the right side of the assignment expression. Just use <code>Result</code> always when you want to read or set the function result.</p>
<p>If you want to call the function itself recursively, you can of course do it. If you’re calling a parameter-less function recursively, be sure to specify the parenthesis (even though in Pascal you can usually omit the parentheses for a parameter-less function), this makes a recursive call to a parameter-less function different from accessing this function’s current result. Like this:</p>
<div>
<div>
<pre><code data-lang="pascal"><span>function</span> ReadIntegersUntilZero: <span>string</span>;
<span>var</span>
  I: Integer;
<span>begin</span>
  Readln(I);
  Result := IntToStr(I);
  <span>if</span> I &lt;&gt; <span>0</span> <span>then</span>
    Result := Result + <span><span>'</span><span> </span><span>'</span></span> + ReadIntegersUntilZero();
<span>end</span>;</code></pre>
</div>
</div>
<p>You can call <code>Exit</code> to end the execution of the procedure or function before it reaches the final <code>end;</code>. If you call parameter-less <code>Exit</code> in a function, it will return the last thing you set as <code>Result</code>. You can also use <code>Exit(X)</code> construct, to set the function result and exit <strong>now</strong> — this is just like <code>return X</code> construct in C-like languages.</p>
<div>
<div>
<pre><code data-lang="pascal"><span>function</span> AddName(<span>const</span> ExistingNames, NewName: <span>string</span>): <span>string</span>;
<span>begin</span>
  <span>if</span> ExistingNames = <span><span>'</span><span>'</span></span> <span>then</span>
    Exit(NewName);
  Result := ExistingNames + <span><span>'</span><span>, </span><span>'</span></span> + NewName;
<span>end</span>;</code></pre>
</div>
</div>
</div>
<div>
<h3 id="_testing_if">2.3. Testing (if)</h3>
<p>Use <code>if .. then</code> or <code>if .. then .. else</code> to run some code when some condition is satisfied. Unlike in the C-like languages, in Pascal you don’t have to wrap the condition in parenthesis.</p>
<div>
<div>
<pre><code data-lang="pascal"><span>var</span>
  A: Integer;
  B: boolean;
<span>begin</span>
  <span>if</span> A &gt; <span>0</span> <span>then</span>
    DoSomething;

  <span>if</span> A &gt; <span>0</span> <span>then</span>
  <span>begin</span>
    DoSomething;
    AndDoSomethingMore;
  <span>end</span>;

  <span>if</span> A &gt; <span>10</span> <span>then</span>
    DoSomething
  <span>else</span>
    DoSomethingElse;

  
  B := A &gt; <span>10</span>;
  <span>if</span> B <span>then</span>
    DoSomething
  <span>else</span>
    DoSomethingElse;
<span>end</span>;</code></pre>
</div>
</div>
<p>The <code>else</code> is paired with the last <code>if</code>. So this works as you expect:</p>
<div>
<div>
<pre><code data-lang="pascal"><span>if</span> A &lt;&gt; <span>0</span> <span>then</span>
  <span>if</span> B &lt;&gt; <span>0</span> <span>then</span>
    AIsNonzeroAndBToo
  <span>else</span>
    AIsNonzeroButBIsZero;</code></pre>
</div>
</div>
<p>While the example with nested <code>if</code> above is correct, it is often better to place the nested <code>if</code> inside a <code>begin</code> …​ <code>end</code> block in such cases. This makes the code more obvious to the reader, and it will remain obvious even if you mess up the indentation. The improved version of the example is below. When you add or remove some <code>else</code> clause in the code below, it’s obvious to which condition it will apply (to the <code>A</code> test or the <code>B</code> test), so it’s less error-prone.</p>
<div>
<div>
<pre><code data-lang="pascal"><span>if</span> A &lt;&gt; <span>0</span> <span>then</span>
<span>begin</span>
  <span>if</span> B &lt;&gt; <span>0</span> <span>then</span>
    AIsNonzeroAndBToo
  <span>else</span>
    AIsNonzeroButBIsZero;
<span>end</span>;</code></pre>
</div>
</div>
</div>
<div>
<h3 id="_logical_relational_and_bit_wise_operators">2.4. Logical, relational and bit-wise operators</h3>
<p>The <em>logical operators</em> are called <code>and</code>, <code>or</code>, <code>not</code>, <code>xor</code>. Their meaning is probably obvious (search for <em>"exclusive or"</em> if you’re unsure what <em>xor</em> does:). They take <em>boolean arguments</em>, and return a <em>boolean</em>. They can also act as <em>bit-wise operators</em> when both arguments are integer values, in which case they return an integer.</p>
<p>The <em>relational (comparison)</em> operators are <code>=</code>, <code>&lt;&gt;</code>, <code>&gt;</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;=</code>. If you’re accustomed to C-like languages, note that in Pascal you compare two values (check are they equal) using a single equality character <code>A = B</code> (unlike in C where you use <code>A == B</code>). The special <em>assignment</em> operator in Pascal is <code>:=</code>.</p>
<p>The <em>logical (or bit-wise) operators have a higher precedence than relational operators</em>. So you may need to use parenthesis around some expressions.</p>
<p>For example this is a compilation error:</p>
<div>
<div>
<pre><code data-lang="pascal"><span>var</span>
  A, B: Integer;
<span>begin</span>
  <span>if</span> A = <span>0</span> <span>and</span> B &lt;&gt; <span>0</span> <span>then</span> ... </code></pre>
</div>
</div>
<p>The above fails to compile, because the compiler sees the bit-wise <code>and</code> inside: <code>(0 and B)</code>.</p>
<p>This is correct:</p>
<div>
<div>
<pre><code data-lang="pascal"><span>var</span>
  A, B: Integer;
<span>begin</span>
  <span>if</span> (A = <span>0</span>) <span>and</span> (B &lt;&gt; <span>0</span>) <span>then</span> ...</code></pre>
</div>
</div>
<p>The <em>short-circuit evaluation</em> is used. Consider this expression:</p>
<div>
<div>
<pre><code data-lang="pascal"><span>if</span> MyFunction(X) <span>and</span> MyOtherFunction(Y) <span>then</span>...</code></pre>
</div>
</div>
<div>
<ul>
<li>
<p>It’s guaranteed that <code>MyFunction(X)</code> will be evaluated first.</p>
</li>
<li>
<p>And if <code>MyFunction(X)</code> returns <code>false</code>, then the value of expression is known (the value of <code>false and whatever</code> is always <code>false</code>), and <code>MyOtherFunction(Y)</code> will not be executed at all.</p>
</li>
<li>
<p>Analogous rule is for <code>or</code> expression. There, if the expression is known to be <code>true</code> (because the 1st operand is <code>true</code>), the 2nd operand is not evaluated.</p>
</li>
<li>
<p>This is particularly useful when writing expressions like</p>
<div>
<div>
<pre><code data-lang="pascal"><span>if</span> (A &lt;&gt; <span>nil</span>) <span>and</span> A.IsValid <span>then</span>...</code></pre>
</div>
</div>
<p>This will work OK, even when <code>A</code> is <code>nil</code>.</p>
</li>
</ul>
</div>
</div>
<div>
<h3 id="_testing_single_expression_for_multiple_values_case">2.5. Testing single expression for multiple values (case)</h3>
<p>If a different action should be executed depending on the value of some expression, then the <code>case .. of .. end</code> statement is useful.</p>
<div>
<div>
<pre><code data-lang="pascal"><span>case</span> SomeValue <span>of</span>
  <span>0</span>: DoSomething;
  <span>1</span>: DoSomethingElse;
  <span>2</span>: <span>begin</span>
       IfItsTwoThenDoThis;
       AndAlsoDoThis;
     <span>end</span>;
  <span>3</span>..<span>10</span>: DoSomethingInCaseItsInThisRange;
  <span>11</span>, <span>21</span>, <span>31</span>: AndDoSomethingForTheseSpecialValues;
  <span>else</span> DoSomethingInCaseOfUnexpectedValue;
<span>end</span>;</code></pre>
</div>
</div>
<p>The <code>else</code> clause is optional. When no condition matches, and there’s no <code>else</code>, then nothing happens.</p>
<p>In you come from C-like languages, and compare this with <code>switch</code> statement in these languages, you will notice that there is no automatic <em>fall-through</em>. This is a deliberate blessing in Pascal. You don’t have to remember to place <code>break</code> instructions. In every execution, <em>at most one</em> branch of the <code>case</code> is executed, that’s it.</p>
</div>
<div>
<h3 id="_enumerated_and_ordinal_types_and_sets_and_constant_length_arrays">2.6. Enumerated and ordinal types and sets and constant-length arrays</h3>
<p>Enumerated type in Pascal is a very nice, opaque type. You will probably use it much more often than enums in other languages:)</p>
<div>
<div>
<pre><code data-lang="pascal"><span>type</span>
  TAnimalKind = (akDuck, akCat, akDog);</code></pre>
</div>
</div>
<p>The convention is to prefix the enum names with a two-letter shortcut of type name, hence <code>ak</code> = shortcut for <em>"Animal Kind"</em>. This is a useful convention, since the enum names are in the unit (global) namespace. So by prefixing them with <code>ak</code> prefix, you minimize the chances of collisions with other identifiers.</p>
<div>
<table>
<tbody><tr>
<td>
<p>Note</p>
</td>
<td>
The collisions in names are not a show-stopper. It’s Ok for different units to define the same identifier. But it’s a good idea to try to avoid the collisions anyway, to keep code simple to understand and grep.
</td>
</tr>
</tbody></table>
</div>
<div>
<table>
<tbody><tr>
<td>
<p>Note</p>
</td>
<td>
You can avoid placing enum names in the global namespace by directive <code>{$scopedenums on}</code>. This means you will have to access them qualified by a type name, like <code>TAnimalKind.akDuck</code>. The need for <code>ak</code> prefix disappears in this situation, and you will probably just call the enums <code>Duck, Cat, Dog</code>. This is …</td></tr></tbody></table></div></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://newpascal.org/assets/modern_pascal_introduction.html">http://newpascal.org/assets/modern_pascal_introduction.html</a></em></p>]]>
            </description>
            <link>http://newpascal.org/assets/modern_pascal_introduction.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-23742999</guid>
            <pubDate>Sun, 05 Jul 2020 23:15:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Writing a winning 4K intro in Rust]]>
            </title>
            <description>
<![CDATA[
Score 266 | Comments 63 (<a href="https://news.ycombinator.com/item?id=23742870">thread link</a>) | @Dowwie
<br/>
July 5, 2020 | https://www.codeslow.com/2020/07/writing-winning-4k-intro-in-rust.html | <a href="https://web.archive.org/web/*/https://www.codeslow.com/2020/07/writing-winning-4k-intro-in-rust.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="post-body-4371838969983872321" itemprop="description articleBody">
<div><p><span>I recently wrote my first 4K intro in Rust and released it at the Nova 2020 where it took first place in the new school intro competition. Writing a 4K intro is quite involved and requires you to master many different areas at the same time. Here I will focus on what I learned about making Rust code as small as possible.</span></p><p><iframe allowfullscreen="" height="322" src="https://www.youtube.com/embed/SIkkYRQ07tU" width="387" youtube-src-id="SIkkYRQ07tU"></iframe></p><p>You can view the demo on<span>&nbsp;</span><a href="https://www.youtube.com/watch?v=SIkkYRQ07tU">youtube</a>, download the executable at<span>&nbsp;</span><a href="https://www.pouet.net/prod.php?which=85924">pouet</a><span>&nbsp;</span>or get the source code from<span>&nbsp;</span><a href="https://github.com/janiorca/sphere_dance">github</a></p><p>A 4K intro is a demo where the entire program ( including any data ) has two be 4096 bytes or less so it is important that the code is as space efficient as possible. Rust has a bit of a reputation for creating bloated executables so I wanted to find out if is possible to create very space efficient code with it.</p><p>The entire intro is written in a combination of Rust and glsl. Glsl is used for rendering everything on screen but Rust does everything else; world creation, camera and object control, creating instruments and playing music etc.</p><p>Some of the features I depend on, such as xargo, are not yet part of stable Rust so I use the nightly rust toolchain. To install and use the nightly toolchain as default you need the following rustup commands.</p><pre data-info="" data-role="codeBlock"><code>rustup toolchain install nightly
rustup default nightly
</code></pre><p>I use<span>&nbsp;</span><a href="http://crinkler.net/">crinkler</a><span>&nbsp;</span>to compress the object file generated by the rust compiler.</p><p>I also used<span>&nbsp;</span><a href="https://github.com/laurentlb/Shader_Minifier">shader minifier</a><span>&nbsp;</span>for pre-processing the<span>&nbsp;</span><code>glsl</code><span>&nbsp;</span>shader to make it smaller and more crinkler friendly. The shader minifier doesn't support output into<span>&nbsp;</span><code>.rs</code><span>&nbsp;</span>files so I ended up using its raw output and manually copying it into my<span>&nbsp;</span><a href="http://shader.rs/">shader.rs</a><span>&nbsp;</span>file. (In hindsight, I should have written something to automate that stage. Or even created a PR for shader minifier)</p><p>The starting point was the proof of concept code I developed earlier (<a href="https://www.codeslow.com/2020/01/writing-4k-intro-in-rust.html">https://www.codeslow.com/2020/01/writing-4k-intro-in-rust.html</a>) which I thought was pretty lean at the time. That article also goes into but more detail about setting up the<span>&nbsp;</span><code>toml</code><span>&nbsp;</span>file and how to use xargo for compiling tiny executable.</p><p>Many of the most effective size optimizations have nothing to do with clever hacks but are the result of rethinking the design.</p><p>My initial design had one part of the code creating the world, including placing the spheres and another part was responsible for moving the spheres. At some point I realized that the sphere placement and sphere moving code were doing very similar things and I could merge them into one sightly more complicated function that did both. Unfortunately, this type of optimization can make the code less elegant and readable.</p><p>At some point you have to look at the compiled assembly code to understand what the code gets compiled into and what size optimizations are worth it. The Rust compiler has a very useful option,<span>&nbsp;</span><code>--emit=asm</code><span>&nbsp;</span>for outputting assembler code. The following command creates a<span>&nbsp;</span><code>.s</code><span>&nbsp;</span>assembly file;</p><pre data-info="" data-role="codeBlock"><code>xargo rustc --release --target i686-pc-windows-msvc -- --emit=asm
</code></pre><p>It is not necessary to be an expert in assembler to benefit from studying the assembler output but it definitely helps to have a basic understanding assembler syntax. The release version uses<span>&nbsp;</span><code>opt-level = "z</code><span>&nbsp;</span>which causes the compiler to optimize for the smallest possible size. This can make it a bit tricky to work out which part of the assembly code corresponds to which part of the Rust code.</p><p>I discovered that the Rust compiler can be surprisingly good at minimizing code; getting rid of unused code and unnecessary parameters and folding code. It can also do some strange things which is why it is essential to occasionally study the resulting assembly code.</p><p>I worked with two versions of the code; one version does logging and allows the viewer to manipulate the camera which is used for creating interesting camera paths. Rust allows you to define<span>&nbsp;</span><strong>features</strong><span>&nbsp;</span>that you can use to optionally include bits of functionality. The<span>&nbsp;</span><code>toml</code><span>&nbsp;</span>file has a<span>&nbsp;</span><strong>[features]</strong><span>&nbsp;</span>section that lets you declare the available features and their dependencies. My 4K intro has the following section in the<span>&nbsp;</span><code>toml</code><span>&nbsp;</span>file;</p><pre data-info="toml" data-role="codeBlock"><span>[</span><span>features</span><span>]</span>
<span>logger</span> <span>=</span> <span>[</span><span>]</span>
<span>fullscreen</span> <span>=</span> <span>[</span><span>]</span>
</pre><p>Neither of the optional features has dependencies so they effectively work as being conditional compilation flags. The conditional blocks of code are preceded by<span>&nbsp;</span><code>#[cfg(feature)]</code><span>&nbsp;</span>statement. Using features in itself does not make the code smaller but it makes development process much nicer when you easily switch between different feature sets.</p><pre data-info="rust" data-role="codeBlock">        <span>#[cfg(feature = "fullscreen")]</span>
        <span>{</span>
            
        <span>}</span>

        <span>#[cfg(not(feature = "fullscreen"))]</span>
        <span>{</span>
            
        <span>}</span>
</pre><p>Having inspected the compiled code I am certain that only the selected features get included in the compiled code.</p><p>One of the main uses of<span>&nbsp;</span><strong>features</strong><span>&nbsp;</span>was to enable logging and error checking for the debug build. The code loading and compiling the glsl shader failed frequently and without useful error messages it would have been extremely painful to find the problems.</p><p>When putting code inside an<span>&nbsp;</span><code>unsafe{}</code><span>&nbsp;</span>block I sort of assumed that all safety checks would be disabled within this block but this is not true, all the usual checks are still applied and these checks can be expensive.</p><p>By default Rust range checks all array accesses. Take the following Rust code</p><pre data-info="rust" data-role="codeBlock">    delay_counter <span>=</span> sequence<span>[</span> play_pos <span>]</span><span>;</span>
</pre><p>Before doing the table look up the compiler would insert code that checks that play_pos is not indexing past the end of sequence and panic if that was the case. This adds considerable size to the code as there can be a lot of table look-ups like this.</p><p>Converting the above code into</p><pre data-info="rust" data-role="codeBlock">    delay_counter <span>=</span> <span>*</span>sequence<span>.</span><span>get_unchecked</span><span>(</span> play_pos <span>)</span><span>;</span>
</pre><p>tells the compiler to not perform any range checks and just do the table look-up. This is clearly a potentially dangerous operation and can thus only be performed within an<span>&nbsp;</span><code>unsafe</code><span>&nbsp;</span>code block</p><p>Initially all my loops used the idiomatic rust way of doing loops, using the<span>&nbsp;</span><code>for x in 0..10</code><span>&nbsp;</span>syntax which I just assumed would be compiled into tightest possible loop. Surprisingly, this was not the case. The simplest case;</p><pre data-info="rust" data-role="codeBlock"><span>for</span> x <span>in</span> <span>0</span><span>..</span><span>10</span> <span>{</span>
    
<span>}</span>
</pre><p>would get translated into assembly code that did the following;</p><pre data-info="" data-role="codeBlock"><code>    setup loop variable
loop:
    check for loop condition    
    if loop finished, jump to end
    // do code inside loop
    unconditionally jump to loop
end:
</code></pre><p>whereas if did the following rust code</p><pre data-info="rust" data-role="codeBlock"><span>let</span> x <span>=</span> <span>0</span><span>;</span>
<span>loop</span><span>{</span>
    
    x <span>+=</span> <span>1</span><span>;</span>
    <span>if</span> i <span>==</span> <span>10</span> <span>{</span>
        <span>break</span><span>;</span>
    <span>}</span>
<span>}</span>
</pre><p>would get directly compiled into;</p><pre data-info="" data-role="codeBlock"><code>    setup loop variable
loop:
    // do code inside loop
    check for loop condition    
    if loop not finished, jump to loop
end:
</code></pre><p>Note that the loop condition is checked at the end of each loop which makes the unconditional jump unnecessary. This is small space saving for one loop but they do add up when there are 30 loops in the program.</p><p>The other, much harder to understand, problem with the idiomatic Rust loop is that in some cases it the compiler would add some additional iterator setup code that really bloated the code. I never fully understood what triggered this additional iterator setup as it was always trivial to replace the<span>&nbsp;</span><code>for {}</code><span>&nbsp;</span>constructs with a<span>&nbsp;</span><code>loop{}</code><span>&nbsp;</span>construct.</p><p>I spent a lot of time optimizing the<span>&nbsp;</span><code>glsl</code><span>&nbsp;</span>code and one of the best class of optimizations ( which also usually made the code run faster) was to operate on an entire vector at a time instead of operating at a component at a time.</p><p>For example, the ray tracing code use a fast<span>&nbsp;</span><a href="http://www.cse.yorku.ca/~amana/research/grid.pdf">grid traversal algorithm</a><span>&nbsp;</span>to check which parts of the map each ray visits. The original algorithm considers each axis separately but it is possible to rewrite the algorithm so it considers all axes at the same time and does not need any branches. Rust doesn't really have a native vector type like glsl but you can use intrinsics to tell it to use SIMD instructions.</p><p>To use intrinsics I would convert the following code</p><pre data-info="rust" data-role="codeBlock">        global_spheres<span>[</span> CAMERA_ROT_IDX <span>]</span><span>[</span> <span>0</span> <span>]</span> <span>+=</span> camera_rot_speed<span>[</span> <span>0</span> <span>]</span><span>*</span>camera_speed<span>;</span>
        global_spheres<span>[</span> CAMERA_ROT_IDX <span>]</span><span>[</span> <span>1</span> <span>]</span> <span>+=</span> camera_rot_speed<span>[</span> <span>1</span> <span>]</span><span>*</span>camera_speed<span>;</span>
        global_spheres<span>[</span> CAMERA_ROT_IDX <span>]</span><span>[</span> <span>2</span> <span>]</span> <span>+=</span> camera_rot_speed<span>[</span> <span>2</span> <span>]</span><span>*</span>camera_speed<span>;</span>
</pre><p>into</p><pre data-info="rust" data-role="codeBlock">        <span>let</span> <span>mut</span> dst<span>:</span>x86<span>:</span><span>:</span>__m128 <span>=</span> core<span>:</span><span>:</span>arch<span>:</span><span>:</span>x86<span>:</span><span>:</span><span>_mm_load_ps</span><span>(</span>global_spheres<span>[</span> CAMERA_ROT_IDX <span>]</span><span>.</span><span>as_mut_ptr</span><span>(</span><span>)</span><span>)</span><span>;</span>
        <span>let</span> <span>mut</span> src<span>:</span>x86<span>:</span><span>:</span>__m128 <span>=</span> core<span>:</span><span>:</span>arch<span>:</span><span>:</span>x86<span>:</span><span>:</span><span>_mm_load_ps</span><span>(</span>camera_rot_speed<span>.</span><span>as_mut_ptr</span><span>(</span><span>)</span><span>)</span><span>;</span>
        dst <span>=</span> core<span>:</span><span>:</span>arch<span>:</span><span>:</span>x86<span>:</span><span>:</span><span>_mm_add_ps</span><span>(</span> dst<span>,</span> src<span>)</span><span>;</span>
        core<span>:</span><span>:</span>arch<span>:</span><span>:</span>x86<span>:</span><span>:</span><span>_mm_store_ss</span><span>(</span> <span>(</span><span>&amp;</span><span>mut</span> global_spheres<span>[</span> CAMERA_ROT_IDX <span>]</span><span>)</span><span>.</span><span>as_mut_ptr</span><span>(</span><span>)</span><span>,</span> dst <span>)</span><span>;</span>
</pre><p>which would be quite a bit smaller ( but a lot less readable ). Sadly, for some reason this broke the debug build while working perfectly on the release build. Clearly, this is a problem with my intrinsics knowledge and not a problem with Rust. This is something I would spend more time on for my next 4K intro as the space saving were significant.</p><p>There are lot of standard Rust crates for loading OpenGL functions but by default they all load a very large set of OpenGL functions. Each loaded function takes up some space because the loader has to know its name. Crinkler does a very good job of compressing this kind of code but it is not able to completely get rid of the overhead so I had to create my own version<span>&nbsp;</span><code>gl.rs</code><span>&nbsp;</span>that only includes the OpenGL functions that are used in the code.</p><p>My first objective was to write a competitive proper 4K intro to prove that language was suitable for scenarios where every single byte counts and you really need low level control. Typically this has been the sole domain of assembler and C. The secondary objective was to write it using idiomatic Rust as much possible.</p><p>I think I was fairly successful on the first objective. At no point during the development did I feel that Rust was holding me back in any way or that I was sacrificing performance or capabilities because I was using Rust rather than C.</p><p>I was less successful on the second objective. There is far too much<span>&nbsp;</span><code>unsafe</code><span>&nbsp;</span>code that doesn't really need to be there.<span>&nbsp;</span><code>Unsafe</code><span>&nbsp;</span>has a corrupting effect; it is very easy to use<span>&nbsp;</span><code>unsafe</code><span>&nbsp;</span>code to quickly accomplish something (like using mutable statics) but once the unsafe code is there it begets more unsafe code and suddenly it …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.codeslow.com/2020/07/writing-winning-4k-intro-in-rust.html">https://www.codeslow.com/2020/07/writing-winning-4k-intro-in-rust.html</a></em></p>]]>
            </description>
            <link>https://www.codeslow.com/2020/07/writing-winning-4k-intro-in-rust.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-23742870</guid>
            <pubDate>Sun, 05 Jul 2020 23:00:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Poor Man's Cluster]]>
            </title>
            <description>
<![CDATA[
Score 24 | Comments 2 (<a href="https://news.ycombinator.com/item?id=23742683">thread link</a>) | @FilingTrader
<br/>
July 5, 2020 | http://www.regressionist.com/2020/07/05/poor-mans-cluster/ | <a href="https://web.archive.org/web/*/http://www.regressionist.com/2020/07/05/poor-mans-cluster/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			
<figure><img src="http://www.regressionist.com/wp-content/uploads/2020/06/cluster.jpg" alt="" srcset="http://www.regressionist.com/wp-content/uploads/2020/06/cluster.jpg 765w, http://www.regressionist.com/wp-content/uploads/2020/06/cluster-300x229.jpg 300w, http://www.regressionist.com/wp-content/uploads/2020/06/cluster-624x476.jpg 624w" sizes="(max-width: 765px) 100vw, 765px"></figure>



<p>This part might just be me cargo-culting, but I feel like even a startup quant fund needs a compute cluster. I once even heard a joke that any self-respecting quant should be able expand their computational needs to fill an arbitrarily large number of servers. The cluster I’ve just built is a low-budget clunker, made of a motley bunch of leftover and refurbished servers, linked together with parts off eBay. But I’m very proud of it!</p>



<h2>Maximum compute per dollar</h2>



<div><p>New servers cost a fortune, and only a small fraction of the cost is for the actual CPU. These servers are designed for use cases that cannot tolerate downtime, where the administrators are remote, and where all the hardware and even software must be supported by some company with expensive contracts. In contrast, my cluster is designed only for research. Downtime is ok, as long as no data gets lost and I get get back up and running easily. So, my focus is only on maximizing performance given a limited budget. I’m optimizing for compute per dollar. (Incidentally, I’ve found the <a href="https://www.cpubenchmark.net/cpu_list.php">PassMark CPU mark</a> to accurately reflect how well each CPU can handle my workload.)</p><p>There is a stigma around buying refurbished enterprise grade equipment that I don’t understand. Basic compute servers that cost $25k three years ago now cost only $2.5k, refurbished at places like <a href="https://www.metservers.com/">metservers.com</a> or <a href="https://www.stalliontek.com/">stalliontek.com</a>. Both of these companies provide warranties, too. Even better, these are real servers that already exist and can be shipped to you immediately, rather than waiting months for new ones due to things like worldwide memory shortages. New <a href="https://store.mellanox.com/products/mellanox-mcx515a-ccat-connectx-5-en-network-interface-card-100gbe-single-port-qsfp28-pcie3-0-x16-tall-bracket-rohs-r6.html">Mellanox 100GbE</a> infiniband cards cost $795 each, but on <a href="https://www.ebay.com/sch/i.html?_nkw=Mellanox+ConnectX-3+56GbE&amp;_sacat=0&amp;LH_TitleDesc=0&amp;_osacat=0&amp;_odkw=Mellanox+ConnectX-3+56">eBay 56GbE cards</a> can be bought for $40 each.</p></div>



<figure><img src="http://www.regressionist.com/wp-content/uploads/2020/07/R630_front.jpg" alt="" srcset="http://www.regressionist.com/wp-content/uploads/2020/07/R630_front.jpg 598w, http://www.regressionist.com/wp-content/uploads/2020/07/R630_front-300x87.jpg 300w" sizes="(max-width: 598px) 100vw, 598px"></figure>



<figure><img src="http://www.regressionist.com/wp-content/uploads/2020/07/R630_back.jpg" alt="" srcset="http://www.regressionist.com/wp-content/uploads/2020/07/R630_back.jpg 594w, http://www.regressionist.com/wp-content/uploads/2020/07/R630_back-300x107.jpg 300w" sizes="(max-width: 594px) 100vw, 594px"></figure>



<h2>NVMe vs. memory</h2>



<p>Memory can really drive up the cost of a server, doubling or tripling the price. I don’t think loading up on RAM is cost-effective at scale. Instead, I recommend NVMe drives as an affordable alternative. Typical RAM for a refurbished Dell R630 server would be DDR4-2133, which has bandwidth of 136Gbps. The Samsung 970 EVO Plus 2TB NVMe drive has a read speed of 28Gbps. With the right software, an old infiniband card can max out its 56Gbps bandwidth by reading simultaneously from NVMe drives on only 2-3 other boxes in the cluster. For my workload, this is close enough to RAM speed that I/O ceases to be a bottleneck, and I can focus on just getting the computations done.</p>



<figure><img src="http://www.regressionist.com/wp-content/uploads/2020/07/nvme_vs_ram.png" alt="" srcset="http://www.regressionist.com/wp-content/uploads/2020/07/nvme_vs_ram.png 828w, http://www.regressionist.com/wp-content/uploads/2020/07/nvme_vs_ram-300x200.png 300w, http://www.regressionist.com/wp-content/uploads/2020/07/nvme_vs_ram-768x512.png 768w, http://www.regressionist.com/wp-content/uploads/2020/07/nvme_vs_ram-624x416.png 624w" sizes="(max-width: 828px) 100vw, 828px"></figure>



<p>I have also chosen to go with retail NVMe drives. They cost far less than enterprise NVMe drives, and they have the same speed (PCIe Gen 3.0 x4) as all but the very newest enterprise drives. The advantage of enterprise drives is the longer lifetime, measured in hundreds or thousands of Terabytes written). But I tend to read far more than I write. Another advantage is that some enterprise drives are dual-port. This is a high-availability feature that allows two hosts to access the same drive, keeping it connected in case of host failure. But as I’ve said, I don’t need expensive high-availability features.</p>



<h2>Distributed storage</h2>



<p>Having a distributed filesystem simplifies coding on a cluster. It makes it feel almost like just working on one big machine. Each job reads and writes to a shared filesystem that is mounted locally, using traditional posix system calls.</p>



<figure><img src="http://www.regressionist.com/wp-content/uploads/2020/07/mfs_and_beegfs.png" alt="" srcset="http://www.regressionist.com/wp-content/uploads/2020/07/mfs_and_beegfs.png 901w, http://www.regressionist.com/wp-content/uploads/2020/07/mfs_and_beegfs-300x142.png 300w, http://www.regressionist.com/wp-content/uploads/2020/07/mfs_and_beegfs-768x364.png 768w, http://www.regressionist.com/wp-content/uploads/2020/07/mfs_and_beegfs-624x296.png 624w" sizes="(max-width: 901px) 100vw, 901px"></figure>



<p>After searching out <a href="http://www.regressionist.com/2020/06/20/reviews-of-distributed-filesystems/">filesystem reviews</a>, I decided to use <strong>MooseFS</strong> for my robust storage. It is easy to configure. It can handle my collection of drives of all sizes, and is robust to the failure of a drive, or even an entire server. It also has a nice browser-based monitoring tool. I have set it up to store one copy of each data chunk on an SSD, and the replicated chunk on a regular spinning disk. The clients are configured to prefer SSD chunkservers, which makes reading reasonably fast. Note: chunkserver labels apply to the whole server, so don’t mix SSDs and HDDs in one server if you want to explicitly prioritize reading from SSDs.</p>



<p>I considered paying for MooseFS Pro, but decided it was too expensive. For a 20TB lifetime license for versions 3.x and 4.x, I was quoted $1,620, or $810 if it was for non-commercial use. The main two benefits of getting a Pro license are 1) getting the high-availability of multiple online master servers instead of just metaloggers, and 2) getting erasure coding for more efficient use of storage space. The erasure coding is interesting to me, but for slow storage, big disks are really cheap. So, storing multiple full copies of a file isn’t such a big deal.</p>



<p>For serious speed, I’ve chosen <strong>BeeGFS</strong> with NVMe drives. BeeGFS supports RDMA (remote direct memory access) with infiniband, so it can move data between boxes without involving the CPUs. It is very fast. It is also relatively easy to configure. I am treating this sort of like volatile storage, and I have not set up “buddy mirrors.” Since I will lose data if my hardware fails, I frequently rsync with the robust storage. I was disappointed to find out that even Pro BeeGFS doesn’t support erasure coding. It would make more sense to use with these expensive NVMe drives. However, erasure coding also slows down both reading and writing. So, I’m ok with giving up robustness in order to have one blazing fast filesystem.</p>



<h2>Conclusion</h2>



<p>Benchmarking a distributed filesystem is complicated and workload-dependent. But everything is working as I hoped. My cluster is mostly hyperconverged, with CPU and storage combined in each server. However, I do have some servers that are clients/CPU only. They are less powerful, so I keep them powered off until needed, to conserve energy. I got an APC AP7911A rack-mount PDU on eBay, so controlling power to the different ports is easy.</p>



<figure><img src="http://www.regressionist.com/wp-content/uploads/2020/07/apc-1.png" alt="" srcset="http://www.regressionist.com/wp-content/uploads/2020/07/apc-1.png 488w, http://www.regressionist.com/wp-content/uploads/2020/07/apc-1-300x225.png 300w" sizes="(max-width: 488px) 100vw, 488px"></figure>



<p>Building a cluster has been a lot of fun, and previously slow processes are now excitingly fast. But I’m anxious to begin real research now, and stop messing around with infrastructure.</p>



<h2>Appendix A, configuring infiniband on CentOS 7</h2>



<p>As a non-HPC guy, learning about infiniband, and getting the network functioning was the hardest part of building the cluster. It took me a long time, and lots of reading and trial-and-error. For that reason, I think it’s worth posting detailed instructions on what eventually worked for me. I don’t believe I’m getting all that is possible out of my infiniband network, but I’m still very pleased with it.</p>



<h3>The cards</h3>



<p>I went with 79DJ3 Mellanox ConnectX-3 CX353A FDR InfiniBand + 56GbE/ 40GbE Single QSFP+ RDMA cards. The most recent ones I ordered on eBay were $35 each. I believe the PCIe lanes cannot handle the full bandwidth of the dual-port cards, which is why I stayed with the simpler single-port card/setup. I did have to order replacement brackets for a couple of my high-profile-PCIe computers.</p>



<figure><img src="http://www.regressionist.com/wp-content/uploads/2020/06/79DJ3.png" alt="" srcset="http://www.regressionist.com/wp-content/uploads/2020/06/79DJ3.png 785w, http://www.regressionist.com/wp-content/uploads/2020/06/79DJ3-300x186.png 300w, http://www.regressionist.com/wp-content/uploads/2020/06/79DJ3-768x476.png 768w, http://www.regressionist.com/wp-content/uploads/2020/06/79DJ3-624x387.png 624w" sizes="(max-width: 785px) 100vw, 785px"></figure>



<h3>The cables</h3>



<p>I went with Mellanox MC2207130-0A1 1.5M IB FDR QSFP copper cables for about $20 each. Fiber optic cables are better for long distances, but these passive cables have worked perfectly.</p>



<figure><img src="http://www.regressionist.com/wp-content/uploads/2020/06/FDR_cable-1024x768.jpg" alt="" srcset="http://www.regressionist.com/wp-content/uploads/2020/06/FDR_cable-1024x768.jpg 1024w, http://www.regressionist.com/wp-content/uploads/2020/06/FDR_cable-300x225.jpg 300w, http://www.regressionist.com/wp-content/uploads/2020/06/FDR_cable-768x576.jpg 768w, http://www.regressionist.com/wp-content/uploads/2020/06/FDR_cable-1536x1152.jpg 1536w, http://www.regressionist.com/wp-content/uploads/2020/06/FDR_cable-624x468.jpg 624w, http://www.regressionist.com/wp-content/uploads/2020/06/FDR_cable.jpg 1600w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<h3>The switch</h3>



<p>There are two switches that will work. The first is a small unmanaged switch, the Mellanox SX6005. It runs about $90 used:</p>



<figure><img src="http://www.regressionist.com/wp-content/uploads/2020/06/mellanox_SX6005.png" alt="" srcset="http://www.regressionist.com/wp-content/uploads/2020/06/mellanox_SX6005.png 807w, http://www.regressionist.com/wp-content/uploads/2020/06/mellanox_SX6005-300x201.png 300w, http://www.regressionist.com/wp-content/uploads/2020/06/mellanox_SX6005-768x514.png 768w, http://www.regressionist.com/wp-content/uploads/2020/06/mellanox_SX6005-624x418.png 624w" sizes="(max-width: 807px) 100vw, 807px"></figure>



<p>The second is the larger, managed, Mellanox SX6036. It runs about $300 used:</p>



<figure><img src="http://www.regressionist.com/wp-content/uploads/2020/06/mellanox_SX6036-1024x353.jpg" alt="" srcset="http://www.regressionist.com/wp-content/uploads/2020/06/mellanox_SX6036-1024x353.jpg 1024w, http://www.regressionist.com/wp-content/uploads/2020/06/mellanox_SX6036-300x103.jpg 300w, http://www.regressionist.com/wp-content/uploads/2020/06/mellanox_SX6036-768x264.jpg 768w, http://www.regressionist.com/wp-content/uploads/2020/06/mellanox_SX6036-1536x529.jpg 1536w, http://www.regressionist.com/wp-content/uploads/2020/06/mellanox_SX6036-624x215.jpg 624w, http://www.regressionist.com/wp-content/uploads/2020/06/mellanox_SX6036.jpg 1600w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>If you have more than one switch involved, you can daisy chain them together. You can even run multiple cables between them, which will reduce the bottleneck between the switches. No special configuration is necessary, just plug in multiple cables, and it will spread the load among them to some degree.</p>



<h3>Subnet manager</h3>



<p>There needs to be exactly one subnet manager for the infiniband network. The managed switch can provide this service, but you need to enable it in the configuration interface. The unmanaged switch cannot provide this service. In that case, you need to run a subnet manager on one server. <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/networking_guide/sec-configuring_the_subnet_manager">It is trivial to install on CentOS 7:</a></p>



<pre><code>yum install opensm
systemctl enable opensm
systemctl start opensm</code></pre>



<h3>Software</h3>



<p>I’m using CentOS 7 on my cluster, because at this time neither MooseFS nor BeeGFS supports CentOS 8. When I first played around with Ubuntu, it was much more difficult to get infiniband working. And I also had to downgrade its kernel to get BeeGFS working. I don’t think it is worth all that hassle, and CentOS 7 is working great.</p>



<pre><code># install packages
yum groupinstall "Infiniband Support"
yum install net-tools mstflint infiniband-diags iperf

# disable firewall
systemctl status firewalld
systemctl stop firewalld
systemctl disable firewalld

# disable SELINUX
nano /etc/selinux/config
# set, then reboot
SELINUX=disabled

# start the RDMA service
systemctl start rdma
systemctl enable rdma</code></pre>



<h3>Updating the card firmware</h3>



<p>After installing the infiniband card, find out the PCI address:</p>



<pre><code># Check the device’s PCI address
lspci | grep Mellanox
# 04:00.0 Network controller: Mellanox Technologies MT27500 Family [ConnectX-3]
# so "04:00.0" is the address</code></pre>



<p>Next, use the PCI address to find the card’s PSID, and note the current firmware version:</p>



<pre><code># Identify the adapter card's PSID (last line of the output)
mstflint -d 04:00.0 q
#Image type:            FS2
#FW Version:            2.32.5100
#FW Release Date:       3.9.2014
#Rom Info:              type=PXE version=3.4.306 proto=IB
#Device ID:             4099
#Description:           Node             Port1            Port2            Sys image
#GUIDs:                 e41d2d0300b2bdc0 e41d2d0300b2bdc1 e41d2d0300b2bdc2 e41d2d0300b2bdc3 
#MACs:                                       e41d2db2bdc1     e41d2db2bdc2
#VSD:                   
#PSID:                  DEL1100001019</code></pre>



<p>Now use the PSID to find the latest firmware version:</p>



<pre><code># Download the firmware BIN file from the Mellanox website that matches your card's PSID:
http://www.mellanox.com/page/firmware_table_dell?mtag=oem_firmware_download
Adapters
Dell EMC ConnectX-3 Firmware Download Center
2.42.5000
079DJ3
DEL1100001019
http://www.mellanox.com/downloads/firmware/fw-ConnectX3-rel-2_42_5000-079DJ3-FlexBoot-3.4.752.bin.zip</code></pre>
</div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://www.regressionist.com/2020/07/05/poor-mans-cluster/">http://www.regressionist.com/2020/07/05/poor-mans-cluster/</a></em></p>]]>
            </description>
            <link>http://www.regressionist.com/2020/07/05/poor-mans-cluster/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23742683</guid>
            <pubDate>Sun, 05 Jul 2020 22:33:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Triplebyte data download doesn’t give you all your data]]>
            </title>
            <description>
<![CDATA[
Score 53 | Comments 0 (<a href="https://news.ycombinator.com/item?id=23742473">thread link</a>) | @wolfgang42
<br/>
July 5, 2020 | https://www.linestarve.com/blog/post/triplebyte-data-download/ | <a href="https://web.archive.org/web/*/https://www.linestarve.com/blog/post/triplebyte-data-download/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="<%= page.layout %>-<%= page.slug %>" itemscope="" itemprop="blogPost">
	<div>
		<div>
			<div>
				<p>In May of last year I decided to start looking for a new job, and started by taking <a href="https://triplebyte.com/">Triplebyte</a>’s quiz. Having passed that, I spent the next three months going through the rest of their process, from a <a href="https://triplebyte.com/interview_guide">two-hour remote interview</a> all the way through to final negotiations with the company whose offer I selected. Throughout the process they were extremely competent and helpful, and at the end of it all I had only good things to say about them. They made the whole process go extremely smoothly, answered all the questions I had and gave me a ton of advice on the whole process, and their screening process was not only great from the my perspective but also gave me confidence in the quality of all their candidates.</p>

<p>Then, a little over a month ago, I got an email announcing the upcoming launch of Triplebyte’s new public profiles. I thought they were was a neat idea, and made a note that I should turn mine on next time I started a job hunt. Then someone <a href="https://news.ycombinator.com/item?id=23279837">posted the email on Hacker News</a>, pointing out that buried in the middle of the email was the fact that these new profiles were going to be opt-<em>out,</em> and unless I turned it off in the next week my profile would become public. This understandably caused an uproar, which the Triplebyte CEO Ammon <a href="https://news.ycombinator.com/item?id=23280460">completely misinterpreted</a>, posting a series of <a href="https://news.ycombinator.com/item?id=23280120">inflammatory comments</a> that <a href="https://news.ycombinator.com/item?id=23280472">misunderstood what people were upset about</a> before vanishing. A few days later he came back with a very apologetic email explaining that they weren’t going to go through with it after all, though it received <a href="https://news.ycombinator.com/item?id=23303037">mixed reactions</a>, with a lot of people being concerned that the idea had been considered at all.</p>

<p>In the midst of all this, I submitted a request through the <a href="https://triplebyte.com/privacy-center">Triplebyte privacy center</a> to download my data. (I considered deleting my account, but decided to give them the benefit of the doubt until things settled down.) After approving the request by clicking an email link, I was informed that it might take up to 30 days to complete my request, so I settled down to wait. As the weeks passed, I thought that the sudden influx of requests must have overwhelmed whoever was responsible for gathering the data from all the systems it was stored in.</p>

<p>Then, 36 days after I first submitted the request, I got an email informing me that my data was now ready to be downloaded. I clicked the link in the email, and then another link on the next page, and finally I got—</p>

<p>A 2,917 byte JSON blob.</p>

<p><em>Odd,</em> I thought, <em>that seems like an awfully long time for so little data.</em> (It’s just over 81 bytes per day, in fact, though I realize that’s a silly metric.) Still, I was relieved to know that they hadn’t been gathering reams of data about me behind my back. Scanning over the minified data, it looked like all they had was my address, some information I’d given them about my past jobs and preferred languages, and a couple of recent IP addresses. Seemingly they hadn’t even kept any information at all about my job search with them.</p>

<p>Then I opened up the file in a JSON viewer and gradually realized: <em>this was not all the information they had.</em> It wasn’t even all of the information they were <em>willing to admit</em> they had—it was missing some obvious things, like the text descriptions on the <code>education</code> and <code>work experience</code> objects, which were prominently displayed on my profile page. As far as I can tell, all I got was a sloppy attempt at making it look at a casual glance like they’d given me what I asked for.</p>

<p>This raises serious concerns for me about Triplebyte, even more so than their plan to make profiles public by default, which started this all. That may well have been born of overenthusiastic naïvité, and was quickly rescinded after being exposed to public comment. After that fiasco, though, I would have expected them to double down on making sure that they were taking privacy seriously. They had over a month before sending me this data to fix any issues with the system, and instead they sent me some slapdash attempt at maybe giving me a whiff of my data.</p>

<p>Triplebyte (as they explain in their privacy center) “care deeply about how your personal information is used and shared,” but apparently not enough to actually put effort into getting it right when you ask for it.</p>

<p>(I’ve sent them an email asking what happened to the rest of the data, and will update this post when I get a response. As it’s the weekend I may not hear back for a few days.)</p>

			</div>
		</div>
	</div>
</article></div>]]>
            </description>
            <link>https://www.linestarve.com/blog/post/triplebyte-data-download/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23742473</guid>
            <pubDate>Sun, 05 Jul 2020 22:02:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Contrarian view on closing files]]>
            </title>
            <description>
<![CDATA[
Score 80 | Comments 90 (<a href="https://news.ycombinator.com/item?id=23742390">thread link</a>) | @coady
<br/>
July 5, 2020 | https://coady.github.io/posts/closing-files/ | <a href="https://web.archive.org/web/*/https://coady.github.io/posts/closing-files/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody text">
    <div>
<div>

<div>
<div>
<h2 id="Contrarian-view-on-closing-files.">Contrarian view on closing files.<a href="#Contrarian-view-on-closing-files.">¶</a>
</h2>
<p>It has become conventional wisdom to always explicitly close file-like objects, via context managers.
The <a href="https://google.github.io/styleguide/pyguide.html#311-files-and-sockets">google style guide</a>
is representative:</p>
<blockquote>
<p>Explicitly close files and sockets when done with them.
Leaving files, sockets or other file-like objects open unnecessarily has many downsides, including:</p>
<p>They may consume limited system resources, such as file descriptors.</p>
<ul>
<li>Code that deals with many such objects may exhaust those resources unnecessarily if they're not returned to the system promptly after use.</li>
<li>Holding files open may prevent other actions being performed on them, such as moves or deletion.</li>
<li>Files and sockets that are shared throughout a program may inadvertantly be read from or written to after logically being closed. If they are actually closed, attempts to read or write from them will throw exceptions, making the problem known sooner.</li>
</ul>
<p>Furthermore, while files and sockets are automatically closed when the file object is destructed, tying the life-time of the file object to the state of the file is poor practice, for several reasons:</p>
<ul>
<li>There are no guarantees as to when the runtime will actually run the file's destructor. Different Python implementations use different memory management techniques, such as delayed Garbage Collection, which may increase the object's lifetime arbitrarily and indefinitely.</li>
<li>Unexpected references to the file may keep it around longer than intended (e.g. in tracebacks of exceptions, inside globals, etc).</li>
</ul>
<p>The preferred way to manage files is using the "with" statement:</p>

<pre><code>with open("hello.txt") as hello_file:
    for line in hello_file:
        print line</code></pre>
</blockquote>
<h3 id="In-theory">In theory<a href="#In-theory">¶</a>
</h3>
<p>Good points, and why limit this advice to file descriptors?  Any resource may be limited or require exclusivity;  that's why they're called resources.  Similarly one should always explicitly call <code>dict.clear</code> when finished with a <code>dict</code>.  After all, "there are no guarantees as to when the runtime will actually run the &lt;object's&gt; destructor.  And "code that deals with many such objects may exhaust those resources unnecessarily", such as memory, or whatever else is in the <code>dict</code>.</p>
<p>But in all seriousness, this advice is applying a notably higher standard of premature optimization to file descriptors than to any other kind of resource.  There are plenty of Python projects that are guaranteed to run on CPython for a variety of reasons, where destructors are immediately called.  And there are plenty of Python projects where file descriptor usage is just a non-issue.  It's now depressingly commonplace to see this in <code>setup.py</code> files:</p>

</div>
</div>
</div>
<div>
<div>
<p>In&nbsp;[&nbsp;]:</p>
<div>
    <div>
<div><pre><span></span><span>with</span> <span>open</span><span>(</span><span>"README.md"</span><span>)</span> <span>as</span> <span>readme</span><span>:</span>
    <span>long_description</span> <span>=</span> <span>readme</span><span>.</span><span>read</span><span>()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div>

<div>
<p>Let's consider a practical example: a <code>load</code> function which is supposed to read and parse data given a file path.</p>
</div>
</div>
<div>
<div>
<p>In&nbsp;[&nbsp;]:</p>
<div>
    <div>
<div><pre><span></span><span>import</span> <span>csv</span>
<span>import</span> <span>json</span>

<span>def</span> <span>load</span><span>(</span><span>filepath</span><span>):</span>
    <span>"""the supposedly bad way"""</span>
    <span>return</span> <span>json</span><span>.</span><span>load</span><span>(</span><span>open</span><span>(</span><span>filepath</span><span>))</span>

<span>def</span> <span>load</span><span>(</span><span>filepath</span><span>):</span>
    <span>"""the supposedly good way"""</span>
    <span>with</span> <span>open</span><span>(</span><span>filepath</span><span>)</span> <span>as</span> <span>file</span><span>:</span>
        <span>return</span> <span>json</span><span>.</span><span>load</span><span>(</span><span>file</span><span>)</span>

<span>def</span> <span>load</span><span>(</span><span>filepath</span><span>):</span>
    <span>"""with a different file format"""</span>
    <span>with</span> <span>open</span><span>(</span><span>filepath</span><span>)</span> <span>as</span> <span>file</span><span>:</span>
        <span>return</span> <span>csv</span><span>.</span><span>reader</span><span>(</span><span>file</span><span>)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div>

<div>
<div>
<p>Which versions work correctly?  Are you sure?  If it's not immediately obvious why one of these is broken, that's the point.  In fact, it's worth trying out before reading on.</p>
<p>...</p>
<p>The <code>csv</code> version returns an iterator over a closed file.  It's a violation of procedural abstraction to know whether the result of <code>load</code> is lazily evaluated or not; it's just supposed to implement an interface.  Moreover, according to this best practice, it's <em>impossible</em> to write the <code>csv</code> version correctly.  As absurd as it sounds, it's just an abstraction that can't exist.</p>
<p>Defiantly clever readers are probably already trying to fix it.  Maybe like this:</p>

</div>
</div>
</div>
<div>
<div>
<p>In&nbsp;[&nbsp;]:</p>
<div>
    <div>
<div><pre><span></span><span>def</span> <span>load</span><span>(</span><span>filepath</span><span>):</span>
    <span>with</span> <span>open</span><span>(</span><span>filepath</span><span>)</span> <span>as</span> <span>file</span><span>:</span>
        <span>yield from</span> <span>csv</span><span>.</span><span>reader</span><span>(</span><span>file</span><span>)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div>

<div>
<div>
<p>No, it will not be fixed.  This version only appears to work by <em>not</em> closing the file until the generator is exhausted or collected.</p>
<p>This trivial example has deeper implications.  If one accepts this practice, then one must also accept that storing a file handle anywhere, such as on an instance, is also disallowed.  Unless of course that object then virally implements it owns context manager, ad infinitum.</p>
<p>Furthermore it demonstrates that often the context is not being managed locally.  If a file object is passed another function, then it's being used outside of the context.  Let's revisit the <code>json</code> version, which works because the file is fully read.  Doesn't a json parser have some expensive parsing to do after it's read the file?  It might even throw an error.  And isn't it desirable, trivial, <a href="https://github.com/python/cpython/blob/master/Lib/json/__init__.py#L274">and likely</a> that the implementation releases interest in the file as soon as possible?</p>
<p>So in reality there are scenarios where the supposedly good way could keep the file open <em>longer</em> than the supposedly bad way.  The original inline version does exactly what it's supposed to do: close the file when all interested parties are done with it.  Python uses garbage collection to manage shared resources.  Any attempt to pretend otherwise will result in code that is broken, inefficient, or reinventing reference counting.</p>
<p>A true believer now has to accept that <code>json.load</code> is a useless and dangerous wrapper, and that the only correct implementation is:</p>

</div>
</div>
</div>
<div>
<div>
<p>In&nbsp;[&nbsp;]:</p>
<div>
    <div>
<div><pre><span></span><span>def</span> <span>load</span><span>(</span><span>filepath</span><span>):</span>
    <span>with</span> <span>open</span><span>(</span><span>filepath</span><span>)</span> <span>as</span> <span>file</span><span>:</span>
        <span>contents</span> <span>=</span> <span>file</span><span>.</span><span>read</span><span>()</span>
    <span>return</span> <span>json</span><span>.</span><span>loads</span><span>(</span><span>contents</span><span>)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div>

<div>
<div>
<p>This line of reasoning reduces to the absurd: a file should never be passed or stored anywhere.  Next an example where the practice has caused real-world damage.</p>
<h3 id="In-practice">In practice<a href="#In-practice">¶</a>
</h3>
<p><a href="https://requests.readthedocs.io/en/master/">Requests</a> is one of the most popular python packages, and <a href="https://docs.python.org/3/library/http.client.html#module-http.client">officially recommended</a>.  It includes a <a href="http://requests.readthedocs.org/en/latest/user/advanced/#session-objects">Session</a> object which supports closing via a context manager.  The vast majority of real-world code uses the the top-level functions or single-use sessions.</p>

</div>
</div>
</div>
<div>
<div>
<p>In&nbsp;[&nbsp;]:</p>
<div>
    <div>
<div><pre><span></span><span>response</span> <span>=</span> <span>requests</span><span>.</span><span>get</span><span>(</span><span>...</span><span>)</span>

<span>with</span> <span>requests</span><span>.</span><span>Session</span><span>()</span> <span>as</span> <span>session</span><span>:</span>
    <span>response</span> <span>=</span> <span>session</span><span>.</span><span>get</span><span>(</span><span>...</span><span>)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div>

<div>
<div>
<p>Sessions manage the connection pool, so this pattern of usage is establishing a new connection every time.  There are popular standard API clients which seriously do this, for every single request to the same endpoint.</p>
<p>Requests' documentation prominently states that "Keep-alive and HTTP connection pooling are 100% automatic".  So part of the blame may lay with that phrasing, since it's only "automatic" if sessions are reused.  But surely a large part of the blame is the dogma of closing sockets, and therefore sessions, explicitly.
The whole point of a connection pool is that it may leave connections open, so users who genuinely need this granularity are working at the wrong abstraction layer.  <code>http.client</code> is already builtin for that level of control.</p>
<p>Tellingly, requests' own top-level functions didn't always close sessions.  There's a long history to that code, including a <a href="https://github.com/kennethreitz/requests/commit/3155bc99362a8c6ab136b6a3bb999732617cd2e5">version that only closed sessions on success</a>.  An older version was <a href="https://github.com/kennethreitz/requests/issues/1882">causing warnings</a>, when run to check for such warnings, and was being blamed for the <em>appearance</em> of <a href="https://github.com/kennethreitz/requests/issues/1685">leaking memory</a>.  Those threads are essentially debating whether a resource pool is "leaking" resources.</p>

</div>
</div>
</div>
<div>

<div>
<div>
<h3 id="Truce">Truce<a href="#Truce">¶</a>
</h3>
<p>Prior to <code>with</code> being introduced in Python 2.5, it was <em>not</em> recommended that inlined reading of a file required a <code>try... finally</code> block.  Far from it, in the past idioms like <code>open(...).read()</code> and <code>for line in open(...)</code> were lauded for being succinct and expressive.  But if all this orphaned file descriptor paranoia was well-founded, it would have been a problem back then too.</p>
<p>Finally, let's address readability.  It could be argued (though it rarely is) that showing the reader when the file is closed has inherent value.  Conveniently, that tends to align with having opened the file for writing anyway, thereby needing an reference to it.  In which case, the readability is approximately equal, and potential pitfalls are more realistic.  But readability is genuinely lost when the file would have been opened in a inline expression.</p>
<p>The best practice is unjustifiably special-casing file descriptors, and not seeing its own reasoning through to its logical conclusion.  This author proposes advocating for <em>anonymous read-only</em> <code>open</code> expressions.  Your setup script is not going to run out of file descriptors because you wrote <code>open("README.md").read()</code>.</p>

</div>
</div>
</div>
</div>
    </div></div>]]>
            </description>
            <link>https://coady.github.io/posts/closing-files/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23742390</guid>
            <pubDate>Sun, 05 Jul 2020 21:50:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The key points of Software Design X-Rays]]>
            </title>
            <description>
<![CDATA[
Score 21 | Comments 4 (<a href="https://news.ycombinator.com/item?id=23741261">thread link</a>) | @nicoespeon
<br/>
July 5, 2020 | https://understandlegacycode.com/blog/key-points-of-software-design-x-rays/ | <a href="https://web.archive.org/web/*/https://understandlegacycode.com/blog/key-points-of-software-design-x-rays/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>How do you analyze a very large Legacy codebase?</p><p>Where do you start when your system is distributed across dozens of micro-services?</p><p>How do you identify development bottlenecks and prioritize refactoring?</p><p>In his book <!-- -->[Software Design X-Rays]<!-- -->(<a href="https://www.google.com/search?q=software+design+x-rays&amp;oq=soft">https://www.google.com/search?q=software+design+x-rays&amp;oq=soft</a>, Adam Tornhill presents a very unique approach to answer these questions. It’s a mix of software architecture and human psychology that generates powerful techniques to tackle large codebases.</p><p>Yet, I realized it’s not a very known book.</p><blockquote><p>I’ve read <a href="https://www.google.com/search?q=your+code+as+a+crime+scene">Your Code as a Crime Scene</a> from the same guy. How is this different?</p></blockquote><p>Well, “Software Design X-Rays” was written after “Your Code as a Crime Scene”. While the forensics flavor of the book was fun and all, Adam stopped referring it too much to avoid getting the reader distracted. The content is much more polished!</p><p>Let me give you my summary of what’s inside the book and why I think it can help you:</p><h2 id="tackle-technical-debt-with-behavioral-code-analysis"><a href="#tackle-technical-debt-with-behavioral-code-analysis" aria-label="tackle technical debt with behavioral code analysis permalink"></a>Tackle Technical Debt with Behavioral Code Analysis</h2><p>The book focuses on giving you the answers to these 3 questions:</p><ol><li>Where’s the code with the higher interest rate?</li><li>Does your architecture support the way your system evolves?</li><li>Are there any productivity bottlenecks for inter-team coordination?</li></ol><p>To do so, Adam presents a technique called <strong>Behavioral Code Analysis</strong>. It uses the information contained in your Version Control System (VCS) to help you make smart decisions on large codebases.</p><h3 id="identify-your-system-hotspots"><a href="#identify-your-system-hotspots" aria-label="identify your system hotspots permalink"></a>Identify your system Hotspots</h3><p>Technical Debt isn’t really a problem if you don’t have to maintain it.</p><p>Static analysis tools consider all debt to be equivalent. They report countless of code smells that you have no choice but to focus on the critical ones. Still, that leaves plenty of things to clean up!</p><p>That’s why you should use the time dimension to identify <strong>Hotspots</strong>: places where you should focus the Refactor efforts in a large codebase if you want to be super effective.</p><p><undefined>
  <a href="https://understandlegacycode.com/static/019e5f99d9ed31fd581770a1835f8ba1/4cf67/hotspots.png" target="_blank" rel="noopener">
  
  <span>
    <span>
      <img alt="hotspots" title="" src="https://understandlegacycode.com/static/019e5f99d9ed31fd581770a1835f8ba1/4cf67/hotspots.png" srcset="https://understandlegacycode.com/static/019e5f99d9ed31fd581770a1835f8ba1/00d96/hotspots.png 148w,https://understandlegacycode.com/static/019e5f99d9ed31fd581770a1835f8ba1/0b23c/hotspots.png 295w,https://understandlegacycode.com/static/019e5f99d9ed31fd581770a1835f8ba1/4cf67/hotspots.png 572w" sizes="(max-width: 572px) 100vw, 572px">
    </span>
  </span>
  
  </a>
    </undefined></p><p><undefined>
  <a href="https://understandlegacycode.com/static/00cdf44baa120d7c01e6f6c9536df456/11d19/code-that-matters.png" target="_blank" rel="noopener">
  
  <span>
    <span>
      <img alt="code that matters" title="" src="https://understandlegacycode.com/static/00cdf44baa120d7c01e6f6c9536df456/799d3/code-that-matters.png" srcset="https://understandlegacycode.com/static/00cdf44baa120d7c01e6f6c9536df456/00d96/code-that-matters.png 148w,https://understandlegacycode.com/static/00cdf44baa120d7c01e6f6c9536df456/0b23c/code-that-matters.png 295w,https://understandlegacycode.com/static/00cdf44baa120d7c01e6f6c9536df456/799d3/code-that-matters.png 590w,https://understandlegacycode.com/static/00cdf44baa120d7c01e6f6c9536df456/11d19/code-that-matters.png 800w" sizes="(max-width: 590px) 100vw, 590px">
    </span>
  </span>
  
  </a>
    </undefined></p><p>If you want to learn how to generate and use these, I presented the technique in details:</p><ul><li><a href="https://understandlegacycode.com/blog/focus-refactoring-with-hotspots-analysis">Focus refactoring on what matters with Hotspots Analysis</a></li><li><a href="https://understandlegacycode.com/blog/convince-management-to-address-tech-debt-with-enclosure-diagrams">Convince managers to address Tech Debt with Enclosure Diagrams</a></li></ul><p>Interestingly, hotspots tend to stay here because people are afraid to tackle them. So they attract even more complexity and become problematic bottlenecks.</p><h3 id="loc-a-simple-and-efficient-indicator-of-code-complexity"><a href="#loc-a-simple-and-efficient-indicator-of-code-complexity" aria-label="loc a simple and efficient indicator of code complexity permalink"></a>LOC: a simple and efficient indicator of code complexity</h3><p>When it comes to evaluating the complexity of the code, many metrics compete. The most popular is probably Cyclomatic Complexity. Yet, it’s fascinating to see that the count of Lines Of Code (LOC) is often a <em>good enough</em> indicator!</p><p>As it’s a language-neutral metric, it’s very easy to generate regardless of your language tooling. You can use <a href="http://cloc.sourceforge.net/">cloc</a> for that:</p><pre data-language="bash" data-index="0"><p><code><span><span>cloc </span><span>.</span><span> --csv --quiet --report-file=your_project.csv</span></span></code></p></pre><p>Another language-neutral metric that works well is the <strong>Indentation Level</strong>. Indentation carries the meaning of logical splits. That’s a good indicator that code is complex.</p><p>The limit of using these is when you have a change in the coding style in the history of the project. But because these metrics are simple, it makes no sense to look at specific values and thresholds. <strong>It’s the trend that matters</strong>. That’s usually enough.</p><h3 id="evaluate-hotspots-with-complexity-trends"><a href="#evaluate-hotspots-with-complexity-trends" aria-label="evaluate hotspots with complexity trends permalink"></a>Evaluate Hotspots with Complexity Trends</h3><p>If you analyze the evolution of complexity of a file over time, you get the story of that file:</p><p><undefined>
  <a href="https://understandlegacycode.com/static/cb402f296f7492bb21a7edeee345b91e/48606/complexity-trend.png" target="_blank" rel="noopener">
  
  <span>
    <span>
      <img alt="complexity trend" title="" src="https://understandlegacycode.com/static/cb402f296f7492bb21a7edeee345b91e/799d3/complexity-trend.png" srcset="https://understandlegacycode.com/static/cb402f296f7492bb21a7edeee345b91e/00d96/complexity-trend.png 148w,https://understandlegacycode.com/static/cb402f296f7492bb21a7edeee345b91e/0b23c/complexity-trend.png 295w,https://understandlegacycode.com/static/cb402f296f7492bb21a7edeee345b91e/799d3/complexity-trend.png 590w,https://understandlegacycode.com/static/cb402f296f7492bb21a7edeee345b91e/48606/complexity-trend.png 701w" sizes="(max-width: 590px) 100vw, 590px">
    </span>
  </span>
  
  </a>
    </undefined></p><p>That’s helpful to show the impact of refactoring to non-technical managers. That helps them visually see the effects of such work, and the results on team productivity.</p><h3 id="perform-x-rays-analysis-to-narrow-even-deeper"><a href="#perform-x-rays-analysis-to-narrow-even-deeper" aria-label="perform x rays analysis to narrow even deeper permalink"></a>Perform X-Rays analysis to narrow even deeper</h3><p>Once you identified Hotspots, you can apply the same logic at the file level to find the complex functions:</p><p><undefined>
  <a href="https://understandlegacycode.com/static/2e63aa78e40f6496f17d737b9ead73c8/11d19/x-ray.png" target="_blank" rel="noopener">
  
  <span>
    <span>
      <img alt="x ray" title="" src="https://understandlegacycode.com/static/2e63aa78e40f6496f17d737b9ead73c8/799d3/x-ray.png" srcset="https://understandlegacycode.com/static/2e63aa78e40f6496f17d737b9ead73c8/00d96/x-ray.png 148w,https://understandlegacycode.com/static/2e63aa78e40f6496f17d737b9ead73c8/0b23c/x-ray.png 295w,https://understandlegacycode.com/static/2e63aa78e40f6496f17d737b9ead73c8/799d3/x-ray.png 590w,https://understandlegacycode.com/static/2e63aa78e40f6496f17d737b9ead73c8/11d19/x-ray.png 800w" sizes="(max-width: 590px) 100vw, 590px">
    </span>
  </span>
  
  </a>
    </undefined></p><p>This is what Adam calls “X-Ray analysis”. Here’s the rough recipe:</p><ol><li>Fetch the source code of the file for each revision from Git</li><li>Run a <code>git diff</code> for every revision to list the modifications</li><li>Match the <code>diff</code> results to the functions that existed in this version (parsing the code is necessary here)</li><li>Perform a Hotspot calculation for the functions<ul><li>Change Frequency = times a function was changed</li><li>Complexity = length or indentation level of the function</li><li>Combine them to calculate the score</li></ul></li></ol><p><undefined>
  <a href="https://understandlegacycode.com/static/2dd2ea002e1ba78bc3e7234740b86379/e7347/x-ray-results.png" target="_blank" rel="noopener">
  
  <span>
    <span>
      <img alt="x ray results" title="" src="https://understandlegacycode.com/static/2dd2ea002e1ba78bc3e7234740b86379/799d3/x-ray-results.png" srcset="https://understandlegacycode.com/static/2dd2ea002e1ba78bc3e7234740b86379/00d96/x-ray-results.png 148w,https://understandlegacycode.com/static/2dd2ea002e1ba78bc3e7234740b86379/0b23c/x-ray-results.png 295w,https://understandlegacycode.com/static/2dd2ea002e1ba78bc3e7234740b86379/799d3/x-ray-results.png 590w,https://understandlegacycode.com/static/2dd2ea002e1ba78bc3e7234740b86379/e7347/x-ray-results.png 849w" sizes="(max-width: 590px) 100vw, 590px">
    </span>
  </span>
  
  </a>
    </undefined></p><p>With the Hotspot + X-Ray techniques, you can take a 400kLOC codebase and focus on the few hundred lines of code that will have the most impact if they are refactored.</p><p>It’s good to know you can perform a cheap X-Ray with git log, using the <code>-L</code> option:</p><pre data-language="bash" data-index="1"><p><code><span><span>git log -L:intel_crtc_page_flip:drivers/gpu/drm/i915/intel_display.c</span></span></code></p></pre><h2 id="coupling-in-time-where-surprises-happen"><a href="#coupling-in-time-where-surprises-happen" aria-label="coupling in time where surprises happen permalink"></a>Coupling in Time: where surprises happen</h2><p>You generally forget about the time dimension when you analyze the code to evaluate its design. That’s a mistake! <strong>Change Coupling</strong> is when files change together over time.</p><p><undefined>
  <a href="https://understandlegacycode.com/static/772eb927f6f8cbe7cc9a4f933ac7d6fd/3af27/change-coupling.png" target="_blank" rel="noopener">
  
  <span>
    <span>
      <img alt="change coupling" title="" src="https://understandlegacycode.com/static/772eb927f6f8cbe7cc9a4f933ac7d6fd/3af27/change-coupling.png" srcset="https://understandlegacycode.com/static/772eb927f6f8cbe7cc9a4f933ac7d6fd/00d96/change-coupling.png 148w,https://understandlegacycode.com/static/772eb927f6f8cbe7cc9a4f933ac7d6fd/0b23c/change-coupling.png 295w,https://understandlegacycode.com/static/772eb927f6f8cbe7cc9a4f933ac7d6fd/3af27/change-coupling.png 545w" sizes="(max-width: 545px) 100vw, 545px">
    </span>
  </span>
  
  </a>
    </undefined></p><p>2 files might change together accidentally. But if they changed together in many commits, with a high degree of coupling, then there’s a high chance these 2 files are coupled!</p><p>This allows you to identify things that empirically belong together. If these files are not co-located, then there might be a problem with the current design. Maybe there’s a bad abstraction or maybe there’s copy-pasted code that keeps evolving together.</p><p>Expected coupling:</p><ul><li>highly-cohesive files (same module)</li><li>code &amp; tests</li></ul><p>Unexpected coupling:</p><ul><li>low-cohesive files (different modules)</li><li>surprising relationships</li></ul><p>Since you’re using git metadata to determine these coupling, it’s <strong>language agnostic</strong>. Therefore, you can detect coupling across stacks, like between front-end and back-end.</p><p>A limit of this technique is the commit patterns developers use. If a developer always commits tests and code independently, you can adapt the technique and consider commits from the same author in a 24h sliding window as “coupled together”. Usually, that’s good enough.</p><h3 id="identify-actual-code-duplication"><a href="#identify-actual-code-duplication" aria-label="identify actual code duplication permalink"></a>Identify actual code duplication</h3><p>Copy-paste is not bad in itself.</p><p>It’s only bad if you need to keep changing all occurrences together. Hence, if you integrate a metric of <em>Similarity</em> in your Change Coupling analysis, you can detect problematic copy-paste:</p><p><undefined>
  <a href="https://understandlegacycode.com/static/33313f1bcd8d42c2f66ef8fd55ac3875/388a9/change-coupling-copy-paste.png" target="_blank" rel="noopener">
  
  <span>
    <span>
      <img alt="change coupling copy paste" title="" src="https://understandlegacycode.com/static/33313f1bcd8d42c2f66ef8fd55ac3875/799d3/change-coupling-copy-paste.png" srcset="https://understandlegacycode.com/static/33313f1bcd8d42c2f66ef8fd55ac3875/00d96/change-coupling-copy-paste.png 148w,https://understandlegacycode.com/static/33313f1bcd8d42c2f66ef8fd55ac3875/0b23c/change-coupling-copy-paste.png 295w,https://understandlegacycode.com/static/33313f1bcd8d42c2f66ef8fd55ac3875/799d3/change-coupling-copy-paste.png 590w,https://understandlegacycode.com/static/33313f1bcd8d42c2f66ef8fd55ac3875/388a9/change-coupling-copy-paste.png 681w" sizes="(max-width: 590px) 100vw, 590px">
    </span>
  </span>
  
  </a>
    </undefined></p><p>Fixing code duplication is often a quick win. It helps getting started in refactoring a Hotspot.</p><p>As a rule of thumb: <strong>things that are coupled should be co-located</strong>.</p><h2 id="the-principles-of-code-age"><a href="#the-principles-of-code-age" aria-label="the principles of code age permalink"></a>The Principles of Code Age</h2><p>Code is only desirable in 2 states:</p><ol><li>Very recent, because it’s fresh in your mind</li><li>Very old, because it means it has stabilized</li></ol><p>When you meet a very old code, you can encapsulate that into a library and extract it from your codebase. That’s less code to deal with, which is good for developers and onboarding!</p><p><strong>Old code usually has no bugs.</strong></p><p>A code that doesn’t stabilize is problematic. It usually means you need to patch it. Because you don’t know it very well, there’s a high chance of creating bugs by ignorance. By creating more bugs, you need to update the code again: it doesn’t stabilize.</p><h3 id="calculate-the-age-of-code"><a href="#calculate-the-age-of-code" aria-label="calculate the age of code permalink"></a>Calculate the age of code</h3><p>The algorithm is simple:</p><ol><li>List modified files with <code>git ls-files</code></li><li>Get the last modification date for each file with <code>git log -l --format="%ad" --date=short -- path/to/file</code></li><li>Calculate the age of the file</li></ol><p>If the codebase was not maintained for some time, consider the youngest one to be 0.</p><h3 id="refactor-towards-code-of-similar-age"><a href="#refactor-towards-code-of-similar-age" aria-label="refactor towards code of similar age permalink"></a>Refactor towards code of similar age</h3><p>Within the same packages, you can identify the code of different ages (very old AND very recent). Try to understand why some code fails to stabilize.</p><p>Maybe you’ll be able to extract parts of it that would actually stabilize.</p><p>Maybe you’ll identify different concepts that are mixed. So you can refactor the structure of the code:</p><p><undefined>
  <a href="https://understandlegacycode.com/static/54ecf644a3270956bb2619a3ed374ec2/fed2b/architecture.png" target="_blank" rel="noopener">
  
  <span>
    <span>
      <img alt="architecture" title="" src="https://understandlegacycode.com/static/54ecf644a3270956bb2619a3ed374ec2/fed2b/architecture.png" srcset="https://understandlegacycode.com/static/54ecf644a3270956bb2619a3ed374ec2/00d96/architecture.png 148w,https://understandlegacycode.com/static/54ecf644a3270956bb2619a3ed374ec2/0b23c/architecture.png 295w,https://understandlegacycode.com/static/54ecf644a3270956bb2619a3ed374ec2/fed2b/architecture.png 564w" sizes="(max-width: 564px) 100vw, 564px">
    </span>
  </span>
  
  </a>
    </undefined></p><h2 id="beyond-conways-law"><a href="#beyond-conways-law" aria-label="beyond conways law permalink"></a>Beyond Conway’s Law</h2><h3 id="development-congestion"><a href="#development-congestion" aria-label="development congestion permalink"></a>Development Congestion</h3><p>When you put too many developers on the same code, it’s hard to keep productive. That’s because the code constantly changes: the code you wrote three days ago is now different, so you have to constantly re-discover what it does. The risk of bug is high.</p><p>This is <em>Development Congestion</em>.</p><p>That’s why if you put more people on a late project, the project will be even later.</p><p>Code reviews and automated tests can mitigate the risk of bugs.</p><h3 id="the-problem-of-having-too-many-contributors"><a href="#the-problem-of-having-too-many-contributors" aria-label="the problem of having too many contributors permalink"></a>The problem of having too many contributors</h3><p><strong>Many minor contributors you have in the last 3 months = higher chances to have bugs</strong>.</p><p>That’s because contributors don’t have the full context of what they change.</p><p>With many contributors, <em>diffusion of responsibility</em> makes the codebase rot because each developer thinks someone else will take care of refactoring.</p><p>Also, many contributors lead to <em>process loss</em> (waste) due to communication overhead.</p><p>Thus, you need to introduce <strong>areas of responsibility</strong> to give teams full ownership. Other teams may contribute through PRs, but one team should own their part, be involved in reviews, and have the final word.</p><p>Finally, <strong>teams should have a broader knowledge boundary (what they know) than their operational boundary (what they change)</strong>. You can make that happen with:</p><ul><li>Teams demoing what they’re working on</li><li>Inter-teams code reviews to spread knowledge</li><li>Make people move between teams</li></ul><h3 id="calculating-the-diffusion-score"><a href="#calculating-the-diffusion-score" aria-label="calculating the diffusion score permalink"></a>Calculating the Diffusion score</h3><p>You can count the number of developers on a specific part of the code from git:</p><pre data-language="bash" data-index="2"><p><code><span><span>git shortlog -s --after=2020-01-12 -- some/module/path | wc -l</span></span></code></p></pre><p>If you analyze the distribution of contributions on a part of the code, you get a <em>Diffusion</em> score:</p><p><undefined>
  <a href="https://understandlegacycode.com/static/02bdece8d042cd9e25d429bfdb5b2c9e/4ba4a/diffusion-formula.png" target="_blank" rel="noopener">
  
  <span>
    <span>
      <img alt="diffusion formula" title="" src="https://understandlegacycode.com/static/02bdece8d042cd9e25d429bfdb5b2c9e/4ba4a/diffusion-formula.png" srcset="https://understandlegacycode.com/static/02bdece8d042cd9e25d429bfdb5b2c9e/00d96/diffusion-formula.png 148w,https://understandlegacycode.com/static/02bdece8d042cd9e25d429bfdb5b2c9e/0b23c/diffusion-formula.png 295w,https://understandlegacycode.com/static/02bdece8d042cd9e25d429bfdb5b2c9e/4ba4a/diffusion-formula.png 497w" sizes="(max-width: 497px) 100vw, 497px">
    </span>
  </span>
  
  </a>
    </undefined>
<undefined>
  <a href="https://understandlegacycode.com/static/cbb6e1f1ec465eacf6ddc572c3787682/b3297/diffusion-results.png" target="_blank" rel="noopener">
  
  <span>
    <span>
      <img alt="diffusion results" title="" src="https://understandlegacycode.com/static/cbb6e1f1ec465eacf6ddc572c3787682/b3297/diffusion-results.png" srcset="https://understandlegacycode.com/static/cbb6e1f1ec465eacf6ddc572c3787682/00d96/diffusion-results.png 148w,https://understandlegacycode.com/static/cbb6e1f1ec465eacf6ddc572c3787682/0b23c/diffusion-results.png 295w,https://understandlegacycode.com/static/cbb6e1f1ec465eacf6ddc572c3787682/b3297/diffusion-results.png 474w" sizes="(max-width: 474px) 100vw, 474px">
    </span>
  </span>
  
  </a>
    </undefined></p><p>You can generate an enclosure diagram to identify bottlenecks in your large codebase:</p><p><undefined>
  <a href="https://understandlegacycode.com/static/9552981d4e14928c6b9520a0f25fee0c/c5652/diffusion-diagram.png" target="_blank" rel="noopener">
  
  <span>
    <span>
      <img alt="diffusion diagram" title="" src="https://understandlegacycode.com/static/9552981d4e14928c6b9520a0f25fee0c/c5652/diffusion-diagram.png" srcset="https://understandlegacycode.com/static/9552981d4e14928c6b9520a0f25fee0c/00d96/diffusion-diagram.png 148w,https://understandlegacycode.com/static/9552981d4e14928c6b9520a0f25fee0c/0b23c/diffusion-diagram.png 295w,https://understandlegacycode.com/static/9552981d4e14928c6b9520a0f25fee0c/c5652/diffusion-diagram.png 466w" sizes="(max-width: 466px) 100vw, 466px">
    </span>
  </span>
  
  </a>
    </undefined></p><h3 id="keep-a-decision-log"><a href="#keep-a-decision-log" aria-label="keep a decision log permalink"></a>Keep a decision log</h3><p><a href="https://understandlegacycode.com/blog/earn-maintainers-esteem-with-adrs">Architecture Decision Record (ADR)</a> are very useful to simply keep track of architectural decisions in the project.</p><p>They help people understand why and how decisions were taken in the past. This is useful to re-evaluate them later in the project, as well as spreading knowledge.</p><h3 id="a-few-management-pitfalls"><a href="#a-few-management-pitfalls" aria-label="a few management pitfalls permalink"></a>A few management pitfalls</h3><p>Adam gives a few pieces of advice to managers, referring to human psychology. Whether you’re a Tech Lead or a non-technical manager, these are gold.</p><p>First, you should never …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://understandlegacycode.com/blog/key-points-of-software-design-x-rays/">https://understandlegacycode.com/blog/key-points-of-software-design-x-rays/</a></em></p>]]>
            </description>
            <link>https://understandlegacycode.com/blog/key-points-of-software-design-x-rays/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23741261</guid>
            <pubDate>Sun, 05 Jul 2020 19:37:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[‘Collapse of civilisation is the most likely outcome’: top climate scientists]]>
            </title>
            <description>
<![CDATA[
Score 39 | Comments 35 (<a href="https://news.ycombinator.com/item?id=23741179">thread link</a>) | @1qazxsw23edc
<br/>
July 5, 2020 | https://voiceofaction.org/collapse-of-civilisation-is-the-most-likely-outcome-top-climate-scientists/ | <a href="https://web.archive.org/web/*/https://voiceofaction.org/collapse-of-civilisation-is-the-most-likely-outcome-top-climate-scientists/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
              
<p>Australia’s top climate scientist says “we are already deep into the trajectory towards collapse” of civilisation, which may now be inevitable because 9 of the 15 known global climate tipping points that regulate the state of the planet have been activated.</p>



<p>Australian National University emeritus professor Will Steffen (pictured) told <em>Voice of Action</em> that there was already a chance we have triggered a “global tipping cascade” that would take us to a less habitable “Hothouse Earth” climate, regardless of whether we reduced emissions.</p>



<p>Steffen says it would take 30 years at best (more likely 40-60 years) to transition to net zero emissions, but when it comes to tipping points such as Arctic sea ice we could have already run out of time. </p>



<p>Evidence shows we will also lose control of the tipping points for the <a rel="noreferrer noopener" href="https://www.theguardian.com/world/2019/jul/25/amazonian-rainforest-near-unrecoverable-tipping-point?CMP=share_btn_tw" data-type="https://www.theguardian.com/world/2019/jul/25/amazonian-rainforest-near-unrecoverable-tipping-point?CMP=share_btn_tw" target="_blank">Amazon rainforest</a>, the West Antarctic ice sheet, and the Greenland ice sheet in much less time than it’s going to take us to get to net zero emissions, Steffen says.</p>



<p>“Given the momentum in both the Earth and human systems, and the growing difference between the ‘reaction time’ needed to steer humanity towards a more sustainable future, and the ‘intervention time’ left to avert a range of catastrophes in both the physical climate system (e.g., melting of Arctic sea ice) and the biosphere (e.g., loss of the Great Barrier Reef), we are already deep into the trajectory towards collapse,” said Steffen.</p>



<p>“That is, the intervention time we have left has, in many cases, shrunk to levels that are shorter than the time it would take to transition to a more sustainable system.</p>



<p>“The fact that many of the features of the Earth System that are being damaged or lost constitute ‘tipping points’ that could well link to form a ‘tipping cascade’ raises the ultimate question: Have we already lost control of the system? Is collapse now inevitable?”</p>



<p>This is not a unique view – leading Stanford University biologists, who were first to reveal that we are already experiencing the sixth mass extinction on Earth, released <a href="https://www.theguardian.com/environment/2020/jun/01/sixth-mass-extinction-of-wildlife-accelerating-scientists-warn" data-type="https://www.theguardian.com/environment/2020/jun/01/sixth-mass-extinction-of-wildlife-accelerating-scientists-warn" target="_blank" rel="noreferrer noopener">new research this week</a> showing species extinctions are accelerating in an unprecedented manner, which may be a tipping point for the collapse of human civilisation.</p>



<p>Also in the past week <a rel="noreferrer noopener" href="https://www.smh.com.au/environment/climate-change/australia-among-global-hot-spots-as-droughts-worsen-in-warming-world-20200601-p54ydh.html?btis" data-type="https://www.smh.com.au/environment/climate-change/australia-among-global-hot-spots-as-droughts-worsen-in-warming-world-20200601-p54ydh.html?btis" target="_blank">research emerged</a> showing the world’s major food baskets will experience more extreme droughts than previously forecast, with southern Australia among the worst hit globally.</p>



<p>Steffen used the metaphor of the Titanic in one of his recent talks to describe how we may cross tipping points faster than the time it would take us to react to get our impact on the climate under control.</p>



<p>“If the Titanic realises that it’s in trouble and it has about 5km that it needs to slow and steer the ship, but it’s only 3km away from the iceberg, it’s already doomed,” he said.</p>



<h3>‘This is an existential threat to civilization’</h3>



<p>Steffen, along with some of the world’s most eminent climate scientists, laid out our predicament in the starkest possible terms in a <a href="https://www.nature.com/articles/d41586-019-03595-0" data-type="https://www.nature.com/articles/d41586-019-03595-0" target="_blank" rel="noreferrer noopener">piece for the journal Nature</a> at the end of last year.</p>



<p>They found that 9 of the 15 known Earth tipping elements that regulate the state of the planet had been activated, and there was now scientific support for declaring a state of planetary emergency. These tipping points can trigger abrupt carbon release back into the atmosphere, such as the release of carbon dioxide and methane caused by the irreversible thawing of the Arctic permafrost.</p>



<figure><img src="https://voiceofaction.org/wp-content/uploads/2020/06/tipping-points-climate-change-nature-comment-1.jpg" alt="" srcset="https://voiceofaction.org/wp-content/uploads/2020/06/tipping-points-climate-change-nature-comment-1.jpg 907w, https://voiceofaction.org/wp-content/uploads/2020/06/tipping-points-climate-change-nature-comment-1-300x209.jpg 300w, https://voiceofaction.org/wp-content/uploads/2020/06/tipping-points-climate-change-nature-comment-1-768x536.jpg 768w, https://voiceofaction.org/wp-content/uploads/2020/06/tipping-points-climate-change-nature-comment-1-143x100.jpg 143w, https://voiceofaction.org/wp-content/uploads/2020/06/tipping-points-climate-change-nature-comment-1-500x349.jpg 500w, https://voiceofaction.org/wp-content/uploads/2020/06/tipping-points-climate-change-nature-comment-1-690x482.jpg 690w" sizes="(max-width: 907px) 100vw, 907px"><figcaption>9 of 15 known Earth tipping points have been activated</figcaption></figure>



<p>“If damaging tipping cascades can occur and a global tipping point cannot be ruled out, then this is an existential threat to civilization,” they wrote.</p>



<p>“No amount of economic cost–benefit analysis is going to help us. We need to change our approach to the climate problem.</p>



<p>“The evidence from tipping points alone suggests that we are in a state of planetary emergency: both the risk and urgency of the situation are acute.”</p>



<p>Steffen is also the lead author of the heavily cited 2018 paper, <a rel="noreferrer noopener" href="https://www.pnas.org/content/115/33/8252" data-type="https://www.pnas.org/content/115/33/8252" target="_blank">Trajectories of the Earth System in the Anthropocene</a>, where he found that “even if the Paris Accord target of a 1.5°C to 2°C rise in temperature is met, we cannot exclude the risk that a cascade of feedbacks could push the Earth System irreversibly onto a ‘Hothouse Earth’ pathway.”</p>



<p>Steffen is a global authority on the subject of tipping points, which are prone to sudden shifts if they get pushed hard enough by a changing climate, and could take the trajectory of the system out of human control. Further warming would become self-sustaining due to system feedbacks and their mutual interaction.</p>



<p>Steffen describes it like a row of dominos and his concern is we are already at the point of no return, knocking over the first couple of dominos which could lead to a cascade knocking over the whole row.</p>



<p>“Some of these we think are vulnerable in the temperature range we’re entering into now,” said Steffen.</p>



<p>“If we get those starting to tip we could get the whole row of dominos tipping and take us to a much hotter climate even if we get our emissions down.”</p>



<p>Even the notoriously conservative United Nations Intergovernmental Panel on Climate Change (IPCC) has found that already with the 1.1°C of warming we have had to date, there was a moderate risk of tipping some of these – and the risk increased as the temperatures increased.</p>



<p>Steffen believes we are committed to at least a 1.5°C temperature rise given the momentum in the economic and climate system, but we still have a shot at staying under 2°C with urgent action.</p>



<h3>+4°C world would support &lt; 1 billion people</h3>



<p>Professor Hans Joachim Schellnhuber, director emeritus and founder of the Potsdam Institute for Climate Impact Research, believes if we go much above 2°C we will quickly get to 4°C anyway because of the tipping points and feedbacks, which would spell the end of human civilisation.</p>



<figure><img src="https://voiceofaction.org/wp-content/uploads/2020/06/15341907941_4ecef6f665_k-1024x680.jpg" alt="" srcset="https://voiceofaction.org/wp-content/uploads/2020/06/15341907941_4ecef6f665_k-1024x680.jpg 1024w, https://voiceofaction.org/wp-content/uploads/2020/06/15341907941_4ecef6f665_k-300x199.jpg 300w, https://voiceofaction.org/wp-content/uploads/2020/06/15341907941_4ecef6f665_k-768x510.jpg 768w, https://voiceofaction.org/wp-content/uploads/2020/06/15341907941_4ecef6f665_k-1536x1020.jpg 1536w, https://voiceofaction.org/wp-content/uploads/2020/06/15341907941_4ecef6f665_k-151x100.jpg 151w, https://voiceofaction.org/wp-content/uploads/2020/06/15341907941_4ecef6f665_k-500x332.jpg 500w, https://voiceofaction.org/wp-content/uploads/2020/06/15341907941_4ecef6f665_k-690x458.jpg 690w, https://voiceofaction.org/wp-content/uploads/2020/06/15341907941_4ecef6f665_k.jpg 2048w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>“There is a very big risk that we will just end our civilisation”: Professor Schellnhuber</figcaption></figure>



<p>Johan Rockström, the head of one of Europe’s leading research institutes, warned in 2019 that in a 4°C-warmer world it would be “difficult to see how we could accommodate a billion people or even half of that … There will be a rich minority of people who survive with modern lifestyles, no doubt, but it will be a turbulent, conflict-ridden world”.</p>



<p>Schellnhuber, one of the world’s leading authorities on climate change, said that if we continue down the present path “there is a very big risk that we will just end our civilisation. The human species will survive somehow but we will destroy almost everything we have built up over the last two thousand years.”</p>



<p>Schellnhuber said in a <a rel="noreferrer noopener" href="https://youtu.be/4PTRTwn3wrg" data-type="https://youtu.be/4PTRTwn3wrg" target="_blank">recent interview</a> that the IPCC report stating we could stay below 1.5°C of warming was “slightly dishonest” because it relies on immense negative emissions (pulling CO2 out of the air) which was not viable at global scale. He said 1.5°C was no longer achievable but it was still possible to stay under 2°C with massive changes to society.</p>



<p>If we don’t bend the emissions curve down substantially before 2030 then keeping temperatures under 2°C becomes unavoidable. The “carbon law” <a rel="noreferrer noopener" href="https://science.sciencemag.org/content/355/6331/1269" data-type="https://science.sciencemag.org/content/355/6331/1269" target="_blank">published in the journal Science</a> in 2017 found that, to hold warming below 2°C, emissions would need to be cut in half between 2020 and 2030.</p>



<p>Steffen told <em>Voice of Action</em> that the three main challenges to humanity – climate change, the degradation of the biosphere and the growing inequalities between and among countries – were “just different facets of the same fundamental problem”.</p>



<p>This problem was the “neoliberal economic system” that spread across the world through globalisation, underpinning “high production high consumption lifestyles” and a “religion built not around eternal life but around eternal growth”.</p>



<p>“It is becoming abundantly clear that (i) this system is incompatible with a well-functioning Earth System at the planetary level; (ii) this system is eroding human- and societal-well being, even in the wealthiest countries, and (iii) collapse is the most likely outcome of the present trajectory of the current system, as prophetically modelled in 1972 in the Limits to Growth work,” Steffen told <em>Voice of Action</em>.</p>



<h3>Eternal growth is not possible</h3>



<p>The <a href="https://www.clubofrome.org/report/the-limits-to-growth/" data-type="https://www.clubofrome.org/report/the-limits-to-growth/" target="_blank" rel="noreferrer noopener">Limits to Growth model</a> released by the Club of Rome in 1972 looked at the interplay between food production, industry, population, non-renewable resources and pollution.</p>



<p>The basic findings were that you can’t grow the system indefinitely as you will cause environmental and resource issues that will ultimately cause the whole global system to collapse (ABC’s This Day Tonight program covered it <a rel="noreferrer noopener" href="https://www.youtube.com/watch?v=cCxPOqwCr1I" data-type="https://www.youtube.com/watch?v=cCxPOqwCr1I" target="_blank">here</a>). At the time of the model’s release it accurately reproduced the historical data from 1900 to 1970.</p>



<p>A <a rel="noreferrer noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0959378008000435" data-type="https://www.sciencedirect.com/science/article/abs/pii/S0959378008000435" target="_blank">2008 study</a> by Graham Turner, then a senior CSIRO research scientist, used three decades of real-world historical data to conclude that the Limits to Growth model’s predictions were coming to pass: “30 years of historical data compare favourably with key features of a business-as-usual [BAU] scenario called the ‘standard run’ scenario, which results in collapse of the global system midway through the 21st century.”</p>



<figure><img src="https://voiceofaction.org/wp-content/uploads/2020/06/grahamturner-1024x547.jpg" alt="" srcset="https://voiceofaction.org/wp-content/uploads/2020/06/grahamturner-1024x547.jpg 1024w, https://voiceofaction.org/wp-content/uploads/2020/06/grahamturner-300x160.jpg 300w, https://voiceofaction.org/wp-content/uploads/2020/06/grahamturner-768x410.jpg 768w, https://voiceofaction.org/wp-content/uploads/2020/06/grahamturner-187x100.jpg 187w, https://voiceofaction.org/wp-content/uploads/2020/06/grahamturner-500x267.jpg 500w, https://voiceofaction.org/wp-content/uploads/2020/06/grahamturner-690x369.jpg 690w, https://voiceofaction.org/wp-content/uploads/2020/06/grahamturner.jpg 1226w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Former CSIRO scientist Graham Turner has been warning about collapse for decades</figcaption></figure>



<p>Turner ran updated figures through the model <a rel="noreferrer noopener" href="https://www.ingentaconnect.com/contentone/oekom/gaia/2012/00000021/00000002/art00010" data-type="https://www.ingentaconnect.com/contentone/oekom/gaia/2012/00000021/00000002/art00010" target="_blank">again in 2012</a> for another peer-reviewed paper, and <a rel="noreferrer noopener" href="https://sustainable.unimelb.edu.au/__data/assets/pdf_file/0005/2763500/MSSI-ResearchPaper-4_Turner_2014.pdf" data-type="https://sustainable.unimelb.edu.au/__data/assets/pdf_file/0005/2763500/MSSI-ResearchPaper-4_Turner_2014.pdf" target="_blank">again in 2014</a> when he had joined the University of Melbourne’s Sustainable Society Institute.</p>



<p>“Data from the forty years or so since the LTG study was completed indicates that the world is closely tracking the BAU scenario,” Turner concluded in the 2014 paper.</p>



<p>“It is notable that there does not appear to be other economy-environment models that have demonstrated such comprehensive and long-term data agreement.”</p>



<p>Turner semi-retired in 2015 but runs a small organic market garden on a rural property in the NSW south coast’s Bega Valley.</p>



<p>He and his wife grow most of their own food and live off grid powered by a …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://voiceofaction.org/collapse-of-civilisation-is-the-most-likely-outcome-top-climate-scientists/">https://voiceofaction.org/collapse-of-civilisation-is-the-most-likely-outcome-top-climate-scientists/</a></em></p>]]>
            </description>
            <link>https://voiceofaction.org/collapse-of-civilisation-is-the-most-likely-outcome-top-climate-scientists/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23741179</guid>
            <pubDate>Sun, 05 Jul 2020 19:27:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[UBC quietly changes references to Taiwan amid sensitive political climate]]>
            </title>
            <description>
<![CDATA[
Score 22 | Comments 6 (<a href="https://news.ycombinator.com/item?id=23741119">thread link</a>) | @abc-xyz
<br/>
July 5, 2020 | https://www.ubyssey.ca/news/taiwan-references-changed/ | <a href="https://web.archive.org/web/*/https://www.ubyssey.ca/news/taiwan-references-changed/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      
        <p>UBC has quietly made a significant change in the way it refers to Taiwan in its annual enrolment report.</p><p>In <a href="https://academic.ubc.ca/sites/vpa.ubc.ca/files/documents/2018-19%20Enrolment%20Report.pdf" target="_blank"><u>past reports</u></a>, the university simply listed the island as “Taiwan,” but in the recent <a href="https://bog3.sites.olt.ubc.ca/files/2020/01/4_2020.02_Enrolment-Annual-Report.pdf" target="_blank"><u>2019/20</u></a> enrolment report, it was lengthier: “Taiwan (Province of China).”</p><p>In a written statement from Kurt Heinrich, UBC Media Relations senior communications director, he said this is because in 2018, UBC’s data governance steering committee adopted International Organization for Standardization (ISO) data standards.</p><p>The ISO, which is recognized by the United Nations, has referred to Taiwan as “Province of China” <a href="https://www.taiwannews.com.tw/en/news/3812381" target="_blank"><u>since 1974</u></a> under ISO 3166, and the UN switched its recognition of China from the Republic of China (Taiwan) to the People’s Republic of China (Mainland China) in 1971.</p><p>In a later email, the university stated that the adoption of ISO standards was “necessary for the university’s successful transition to Workday,” UBC’s partner for its software overhaul that will replace aging systems. Elsewhere on UBC websites, however, the island is still referred to as “Taiwan.”</p><p>The nature of Taiwan’s sovereignty is a deeply political debate. Taiwan, which boasts its own democratically elected government, claims to be an independent nation. Mainland China, on the other hand, claims Taiwan to be an integral province of China.</p>
<p>“To put ‘Province of China’ after the name is to politicize the name,” said Dr. Timothy Brook, a UBC professor and an expert in Chinese history.</p><p>Many countries have <a href="https://www.newsweek.com/who-recognizes-taiwan-two-change-china-1460559" target="_blank"><u>switched their allegiance</u></a> from Taipei to Beijing in recent decades. In 1970, Canada severed diplomatic ties with Taipei in favour of Beijing, but Canada and Taiwan maintain strong trade and informal ties.</p><p>UBC’s decision to make the change to label Taiwan as a “Province of China” came at a low point in Chinese–Canadian relations, following the <a href="https://www.ctvnews.ca/world/trudeau-says-china-made-obvious-link-between-meng-and-two-michaels-1.4994128" target="_blank"><u>arrests</u></a> of Huawei CFO Meng Wanzhou in Vancouver, and the <a href="https://www.theglobeandmail.com/politics/article-china-suggests-it-will-free-two-michaels-if-canada-allows-huawei/" target="_blank"><u>two Michaels</u></a> in China, which Brook described as “political hostage taking.”</p><p>For UBC, the stakes for appeasing China are high. Huawei has granted <a href="https://nationalpost.com/news/meng-wanzhou-arrest-caused-ubc-leaders-concern-over-enrolment-fundraising-internal-documents-show" target="_blank"><u>$9.5 million</u></a> in funding for research projects at UBC in recent years, which continued even after Meng’s December 2018 arrest.</p><p>Chinese students make up more than one third of all international students at UBC. In 2019/20, international student tuition made up <a href="https://bog3.sites.olt.ubc.ca/files/2020/04/2.1_2020.04_Budget-Fiscal-2020-2021.pdf" target="_blank"><u>$507 million</u></a> in revenue compared to $386 million from domestic students. Rising Canadian–Chinese tensions have made UBC administrators fear potential impacts on Chinese student enrolment and funding.</p><p>In 2019, Vice-Provost International Murali Chandrashekaran sent an <a href="https://nationalpost.com/news/meng-wanzhou-arrest-caused-ubc-leaders-concern-over-enrolment-fundraising-internal-documents-show" target="_blank"><u>email</u></a> to colleagues calling for a campus-wide meeting to address this, “given our significant reliance on China for students/$.”</p><p>If diplomatic tensions reach a point where China restricts students from going to UBC, as<a href="https://www.ubyssey.ca/news/ubc-urges-saudi-arabia-students-to-contact-enrolment-services/" target="_blank"><u> Saudi Arabia did in 2018</u></a>, UBC could face a significant <a href="https://theprovince.com/pmn/news-pmn/canada-news-pmn/credit-agency-warns-big-risk-to-canadian-schools-if-china-pulls-students/wcm/268ed61c-89fd-41e0-8a2d-3aba815152e3" target="_blank"><u>credit risk</u></a>, according to prominent credit agency Moody’s.</p><p>Anxieties surrounding Chinese interference also exist at other Canadian universities. At McMaster University, a Chinese student group had its club status <a href="https://www.scmp.com/news/china/diplomacy/article/3036309/chinese-student-association-mcmaster-university-loses-appeal" target="_blank"><u>revoked</u></a> after allegations it reported a talk by a Uyghur–Canadian woman to the Chinese consulate. At the University of Toronto, Chemi Lhamo, student union president and Canadian of Tibetan origins, was met with widespread <a href="https://www.cbc.ca/news/canada/toronto/china-tibet-student-election-1.5019648" target="_blank"><u>backlash</u></a> by Chinese students after her election win.</p>
<p>Yves Tiberghien, a UBC political science professor focusing on China and the Asia-Pacific Region, said he “can’t imagine” that the change in recognition of Taiwan in the recent enrolment report was due to Chinese financial influence.</p><p>“If I had to guess,” he added, the technical committee that decided this likely “did not have the full knowledge” of the sensitive political nature of the Taiwan–China relationship.</p><p>According to Brook, however, it is “entirely possible” that there were some Chinese pressures. “I wouldn’t be surprised if every Chinese consulate in Canada was going around and looking at things like universities to see how [they] refer to Taiwan, but I have no evidence of this,” he said. “It would be entirely in keeping with the kind of broader diplomatic initiatives that the [People’s Republic of China]’s been making over the last five years.”</p><p>While the university claims the decision was made for technical purposes, it did not respond to a follow-up email asking whether the committee responsible for the change was aware of the political context around the name.</p><p>“I wish they hadn’t done it,” said Tiberghien. “I don’t think it’s a good idea because it’s stepping into something that’s raw right now … the last thing you want is to step into this now.”</p>
      
      </div></div>]]>
            </description>
            <link>https://www.ubyssey.ca/news/taiwan-references-changed/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23741119</guid>
            <pubDate>Sun, 05 Jul 2020 19:15:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Transport makes up only 6% of the greenhouse gas emissions from food]]>
            </title>
            <description>
<![CDATA[
Score 77 | Comments 69 (<a href="https://news.ycombinator.com/item?id=23741040">thread link</a>) | @shafyy
<br/>
July 5, 2020 | https://blog.yeticheese.com/eating-local-has-tiny-environmental-impact/ | <a href="https://web.archive.org/web/*/https://blog.yeticheese.com/eating-local-has-tiny-environmental-impact/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
				<p>There's a common misconception that eating locally produced foods is important from an environmental point of view. Even the <a href="https://twitter.com/UN/status/1188622911080415235">UN tweeted about it.</a> This is wrong.</p><p>Transport makes up only 6% of the greenhouse gas emissions from food:</p><figure><img src="https://blog.yeticheese.com/content/images/2020/07/How-much-of-GHGs-come-from-food-1-.png" alt="" width="3889" height="3935" srcset="https://blog.yeticheese.com/content/images/size/w600/2020/07/How-much-of-GHGs-come-from-food-1-.png 600w, https://blog.yeticheese.com/content/images/size/w1000/2020/07/How-much-of-GHGs-come-from-food-1-.png 1000w, https://blog.yeticheese.com/content/images/size/w1600/2020/07/How-much-of-GHGs-come-from-food-1-.png 1600w, https://blog.yeticheese.com/content/images/size/w2400/2020/07/How-much-of-GHGs-come-from-food-1-.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Source: <a href="https://ourworldindata.org/environmental-impacts-of-food">Our World in Data</a>.</figcaption></figure><p>The reason for this is that most foods are transported by ship and not plane. Only about 0.16% of food miles are done by plane:</p><figure><img src="https://blog.yeticheese.com/content/images/2020/07/share-food-miles-by-method.png" alt="" width="3400" height="2400" srcset="https://blog.yeticheese.com/content/images/size/w600/2020/07/share-food-miles-by-method.png 600w, https://blog.yeticheese.com/content/images/size/w1000/2020/07/share-food-miles-by-method.png 1000w, https://blog.yeticheese.com/content/images/size/w1600/2020/07/share-food-miles-by-method.png 1600w, https://blog.yeticheese.com/content/images/size/w2400/2020/07/share-food-miles-by-method.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>Source: <a href="https://ourworldindata.org/grapher/share-food-miles-by-method">Our World in Data</a>.</figcaption></figure><p>It makes sense to try and avoid foods that are transported by air. Typically, those are foods which are highly perishable, such as asparagus, green beans and berries.</p><p>In some cases, eating local food even has a more negative impact on the environment than buying something that has been produced half way around the world. For example, heated greenhouses are energy intensive and can produce more greenhouse gases than transporting something for thousands of kilometers by water or road.</p><p>It's clear that avoiding meat and dairy has a much bigger impact on reducing greenhouse gas emissions.</p><p>So, why do people keep saying that we should eat local?</p><p>It could just be ignorance. However, I think that it's often a straw man argument pushed by interest groups that want to keep selling meat and dairy. It is something that is easy to do and seems to make sense on the surface to many people. Let's take another look at that UN tweet from before:</p><figure><img src="https://blog.yeticheese.com/content/images/2020/07/Screenshot-2020-07-05-at-7.18.44-PM.png" alt="" width="1194" height="634" srcset="https://blog.yeticheese.com/content/images/size/w600/2020/07/Screenshot-2020-07-05-at-7.18.44-PM.png 600w, https://blog.yeticheese.com/content/images/size/w1000/2020/07/Screenshot-2020-07-05-at-7.18.44-PM.png 1000w, https://blog.yeticheese.com/content/images/2020/07/Screenshot-2020-07-05-at-7.18.44-PM.png 1194w" sizes="(min-width: 720px) 720px"><figcaption>Source: <a href="https://twitter.com/UN/status/1188622911080415235">Tweet from @UN</a> on Oct 28, 2019.</figcaption></figure><p>In addition to eating local food, they also recommend unplugging unused appliances and using less hot water. Like avoiding plastic bags or plastic straws, this is good advice but a long shot from making a meaningful impact on climate change.</p><p>Arguments like these try to shift away the spot light from big companies who collectively make up a large chunk of the greenhouse gas emissions to individuals. People think that they did something meaningful by buying local food, which, as we have seen, is not the case.</p><p>I'm not saying that we shouldn't do those things. We absolutely should, but it shouldn't be the main talking points of organizations like the UN or WWF.</p><p>To make real change, we must eat less meat and dairy, move to more renewable energy sources and reduce air and road travel significantly.</p><p>PS: I'm only talking about the impact on climate change in this article. Eating local and organic food has other benefits such as supporting the local economy and in most cases it's a good idea to do it.</p><p>Comments or questions? Join in on the discussion on <a href="https://twitter.com/yeticheeseparty/status/1279850824378781697?s=20">this Twitter thread</a>. </p><!--kg-card-begin: html--><!-- Begin Mailchimp Signup Form -->


<div id="mc_embed_signup">
<p>
    Our plant-based Yeti Feta will be available to order soon. Leave your email below and we'll let you know when it's ready. (No newsletters or other shenanigans)
</p>

</div>

<!--End mc_embed_signup--><!--kg-card-end: html-->
			</section></div>]]>
            </description>
            <link>https://blog.yeticheese.com/eating-local-has-tiny-environmental-impact/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23741040</guid>
            <pubDate>Sun, 05 Jul 2020 19:05:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Self Organising WS2811 LEDs]]>
            </title>
            <description>
<![CDATA[
Score 29 | Comments 6 (<a href="https://news.ycombinator.com/item?id=23741036">thread link</a>) | @iamflimflam1
<br/>
July 5, 2020 | https://blog.cmgresearch.com/2020/06/05/self-organising-ws2811-leds.html | <a href="https://web.archive.org/web/*/https://blog.cmgresearch.com/2020/06/05/self-organising-ws2811-leds.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <p>Lots of LEDs? It’s not Christmas yet!</p>

<p>I had a big bundle of addressable WS2811 LED strings and an ESP-CAM board (an ESP32 dev board with a camera). There’s only one possible project that you can do with these components. Turn the disorganised chaos of lights into something a bit more organised.</p>

<p>As an added bonus I’ve ended up duplicating the image processing code in JavaScript so you don’t even need a camera on your ESP32 board - you can just use a plain dev board to drive the LEDs.</p>

<p>You can see the results of my efforts in the video below and I’ll run through a bit more detail of the code in the following text.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Ueim2Ko8VWo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>The full sample code can be found here: <a href="https://github.com/atomic14/self-organising-leds">https://github.com/atomic14/self-organising-leds</a></p>

<p>If you want to do this yourself then you will need an ESP32 dev board of some kind and of course you’ll need some kind of addressable LEDs. I’m using the <a href="https://github.com/FastLED/FastLED">FastLED</a> library for driving the LEDs so with some small code changes you can probably support pretty much any addressable LEDs.</p>

<p><img src="https://blog.cmgresearch.com/assets/article_images/2020-06-05-ws2811/boards.jpg" alt="ESP32 and ESP-CAM Boards"></p>

<p>Our challenge comes down to a very basic problem, given access to a stream of images from a camera identity the approximate locations of each LED in 2D space. Once you’ve done that it’s a simple problem to map from each LED’s x and y location onto a frame buffer containing the pattern you want to show.</p>

<p>There’s a bunch of boiler plate code to initialise the ESP-CAM - I took inspiration from the sample code <a href="https://randomnerdtutorials.com/esp32-cam-video-streaming-face-recognition-arduino-ide/">here</a> and copied the bits I needed to get the camera up and running.</p>

<p>An important change I’ve made is only capture greyscale images at the lowest framesize:</p>

<div><div><pre><code><span>config</span><span>.</span><span>pixel_format</span> <span>=</span> <span>PIXFORMAT_GRAYSCALE</span><span>;</span>
<span>config</span><span>.</span><span>frame_size</span> <span>=</span> <span>FRAMESIZE_QQVGA</span><span>;</span>
</code></pre></div></div>

<p>And then to grab a frame from the camera we simply do:</p>

<div><div><pre><code>    <span>camera_fb_t</span> <span>*</span><span>fb</span> <span>=</span> <span>esp_camera_fb_get</span><span>();</span>
    <span>Frame</span> <span>*</span><span>frame</span> <span>=</span> <span>new</span> <span>Frame</span><span>(</span><span>fb</span><span>);</span>
    <span>esp_camera_fb_return</span><span>(</span><span>fb</span><span>);</span>
</code></pre></div></div>

<p>With our Frame class grabbing a copy of the pixels along with the width and the height of the image.</p>

<div><div><pre><code><span>pixels</span> <span>=</span> <span>(</span><span>uint8_t</span> <span>*</span><span>)</span><span>malloc</span><span>(</span><span>fb</span><span>-&gt;</span><span>height</span> <span>*</span> <span>fb</span><span>-&gt;</span><span>width</span><span>);</span>
<span>memcpy</span><span>(</span><span>pixels</span><span>,</span> <span>fb</span><span>-&gt;</span><span>buf</span><span>,</span> <span>fb</span><span>-&gt;</span><span>height</span> <span>*</span> <span>fb</span><span>-&gt;</span><span>width</span><span>);</span>
<span>width</span> <span>=</span> <span>fb</span><span>-&gt;</span><span>width</span><span>;</span>
<span>height</span> <span>=</span> <span>fb</span><span>-&gt;</span><span>height</span><span>;</span>
<span>length</span> <span>=</span> <span>fb</span><span>-&gt;</span><span>width</span> <span>*</span> <span>fb</span><span>-&gt;</span><span>height</span><span>;</span>
</code></pre></div></div>

<p>The image below shows a frame grabbed from the ESP-CAM sensor.</p>

<p><img src="https://blog.cmgresearch.com/assets/article_images/2020-06-05-ws2811/grabbed-frame.png" alt="Grabbed Frame"></p>

<p>For the JavaScript version of this code it’s a bit more complicated. One of the biggest problems is that we need to be running over HTTPS to have access to the camera - more on this later….</p>

<div><div><pre><code><span>const</span> <span>stream</span> <span>=</span> <span>await</span> <span>navigator</span><span>.</span><span>mediaDevices</span><span>.</span><span>getUserMedia</span><span>({</span>
  <span>video</span><span>:</span> <span>{</span> <span>facingMode</span><span>:</span> <span>"</span><span>environment</span><span>"</span> <span>},</span>
  <span>audio</span><span>:</span> <span>false</span><span>,</span>
<span>});</span>
<span>const</span> <span>canPlayListener</span> <span>=</span> <span>()</span> <span>=&gt;</span> <span>{</span>
  <span>// the video is loaded and we can grab frames</span>
  <span>onVideoReady</span><span>(</span><span>video</span><span>);</span>
  <span>video</span><span>.</span><span>removeEventListener</span><span>(</span><span>"</span><span>canplay</span><span>"</span><span>,</span> <span>canPlayListener</span><span>);</span>
<span>};</span>
<span>video</span><span>.</span><span>addEventListener</span><span>(</span><span>"</span><span>canplay</span><span>"</span><span>,</span> <span>canPlayListener</span><span>);</span>
<span>video</span><span>.</span><span>srcObject</span> <span>=</span> <span>stream</span><span>;</span>
<span>video</span><span>.</span><span>play</span><span>();</span>
</code></pre></div></div>

<p>Once we have a video stream coming from the camera we can grab a frame by drawing the video to a canvas context and then getting the imageData from it.</p>

<div><div><pre><code><span>function</span> <span>getVideoFrame</span><span>(</span><span>video</span><span>:</span> <span>HTMLVideoElement</span><span>,</span> <span>canvas</span><span>:</span> <span>HTMLCanvasElement</span><span>)</span> <span>{</span>
  <span>const</span> <span>width</span> <span>=</span> <span>video</span><span>.</span><span>videoWidth</span><span>;</span>
  <span>const</span> <span>height</span> <span>=</span> <span>video</span><span>.</span><span>videoHeight</span><span>;</span>
  <span>const</span> <span>context</span> <span>=</span> <span>canvas</span><span>.</span><span>getContext</span><span>(</span><span>"</span><span>2d</span><span>"</span><span>);</span>
  <span>// draw the video to the canvas</span>
  <span>context</span><span>!</span><span>.</span><span>drawImage</span><span>(</span><span>video</span><span>,</span> <span>0</span><span>,</span> <span>0</span><span>,</span> <span>width</span><span>,</span> <span>height</span><span>);</span>
  <span>// get the raw image bytes</span>
  <span>const</span> <span>imageData</span> <span>=</span> <span>context</span><span>!</span><span>.</span><span>getImageData</span><span>(</span><span>0</span><span>,</span> <span>0</span><span>,</span> <span>width</span><span>,</span> <span>height</span><span>);</span>
  <span>// convert to greyscale</span>
  <span>const</span> <span>bytes</span> <span>=</span> <span>new</span> <span>Uint8Array</span><span>(</span><span>width</span> <span>*</span> <span>height</span><span>);</span>
  <span>for</span> <span>(</span><span>let</span> <span>y</span> <span>=</span> <span>0</span><span>;</span> <span>y</span> <span>&lt;</span> <span>height</span><span>;</span> <span>y</span><span>++</span><span>)</span> <span>{</span>
    <span>for</span> <span>(</span><span>let</span> <span>x</span> <span>=</span> <span>0</span><span>;</span> <span>x</span> <span>&lt;</span> <span>width</span><span>;</span> <span>x</span><span>++</span><span>)</span> <span>{</span>
      <span>const</span> <span>r</span> <span>=</span> <span>imageData</span><span>.</span><span>data</span><span>[(</span><span>y</span> <span>*</span> <span>width</span> <span>+</span> <span>x</span><span>)</span> <span>*</span> <span>4</span><span>];</span>
      <span>const</span> <span>g</span> <span>=</span> <span>imageData</span><span>.</span><span>data</span><span>[(</span><span>y</span> <span>*</span> <span>width</span> <span>+</span> <span>x</span><span>)</span> <span>*</span> <span>4</span> <span>+</span> <span>1</span><span>];</span>
      <span>const</span> <span>b</span> <span>=</span> <span>imageData</span><span>.</span><span>data</span><span>[(</span><span>y</span> <span>*</span> <span>width</span> <span>+</span> <span>x</span><span>)</span> <span>*</span> <span>4</span> <span>+</span> <span>2</span><span>];</span>
      <span>// https://en.wikipedia.org/wiki/Grayscale#Converting_color_to_grayscale</span>
      <span>const</span> <span>grey</span> <span>=</span> <span>Math</span><span>.</span><span>min</span><span>(</span><span>255</span><span>,</span> <span>0.299</span> <span>*</span> <span>r</span> <span>+</span> <span>0.587</span> <span>*</span> <span>g</span> <span>+</span> <span>0.114</span> <span>*</span> <span>b</span><span>);</span>
      <span>bytes</span><span>[</span><span>y</span> <span>*</span> <span>width</span> <span>+</span> <span>x</span><span>]</span> <span>=</span> <span>grey</span><span>;</span>
    <span>}</span>
  <span>}</span>
  <span>return</span> <span>bytes</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p><img src="https://blog.cmgresearch.com/assets/article_images/2020-06-05-ws2811/phone-interface.png" alt="Phone Interface"></p>

<p>Now we can grab frames we just need to grab a frame with no LEDs lit, light one LED, grab another frame and then compare the two. The difference should tell us where the LED is. To avoid noise or small movements of the camera having a bit impact we apply a guassian blur to the captured frames before taking the difference.</p>

<p>This is a of course a very naive and simple algorithm and could easily be improved on.</p>

<p>In the C++ code of the ESP32 we do all this directly in code. In the JavaScript version we call API functions on the web interface of the ESP32 to turn LEDs on and off and once we’ve finished the processing send up the calculated positions of the LEDs to the board.</p>

<p>In our ESP32 code we create a framebuffer and draw patterns into it. We then use the locations of each LED to work out what color it should be.</p>

<p><img src="https://blog.cmgresearch.com/assets/article_images/2020-06-05-ws2811/organised.jpg" alt="Organised"></p>

<p>To solve the issue of needing HTTPS to access the camera and also needing the API calls to be HTTPS as well (we can’t mix content nowadays!) we need a way of serving both the UI and the API from the ESP32 web server over HTTPS. There are web servers that support HTTPS and self signed certificates available for the ESP32 but this leads to other problems and would require a rewrite of the device code. An easy workaround to this problem is to use a service such as <a href="https://ngrok.com/">ngrok</a> to provide a secure URL from the cloud through our computer to the ESP32 device. Slightly convoluted, but it works!</p>

<p>This in only needed if you are not using an ESP-CAM and have to use your phone’s camera for calibrating the LEDs. Sign up for a free acount with ngrok and then find the IP address of your ESP32 board:</p>

<div><div><pre><code>ping espcam.local
PING espcam.local (10.0.1.17): 56 data bytes
64 bytes from 10.0.1.17: icmp_seq=0 ttl=255 time=14.343 ms
64 bytes from 10.0.1.17: icmp_seq=1 ttl=255 time=6.493 ms
</code></pre></div></div>

<p>Take the IP-Address and ask ngrok to start proxying requests for us:</p>

<div><div><pre><code>ngrok http 10.0.1.17 -inspect=false
</code></pre></div></div>

<p>You’ll need steady hands - there’s quite a lot of latency going on so the space between turning an LED on and off and grabbing a frame can be quite large. I slightly moved as I was taking the frame to show the locations so the positions are slightly off.</p>

<p><img src="https://blog.cmgresearch.com/assets/article_images/2020-06-05-ws2811/located.png" alt="LED locations"></p>

<p>Checkout the video to see how well it works - surprising for such a simple algorithm.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Ueim2Ko8VWo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

          </div></div>]]>
            </description>
            <link>https://blog.cmgresearch.com/2020/06/05/self-organising-ws2811-leds.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-23741036</guid>
            <pubDate>Sun, 05 Jul 2020 19:04:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Purpose of Persuasion]]>
            </title>
            <description>
<![CDATA[
Score 78 | Comments 40 (<a href="https://news.ycombinator.com/item?id=23740669">thread link</a>) | @apsec112
<br/>
July 5, 2020 | https://www.persuasion.community/p/the-purpose-of-persuasion | <a href="https://web.archive.org/web/*/https://www.persuasion.community/p/the-purpose-of-persuasion">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><a target="_blank" href="https://cdn.substack.com/image/fetch/c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F76311586-b35a-4d2b-82b1-d8a8b6ca9b18_4522x3019.jpeg"><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F76311586-b35a-4d2b-82b1-d8a8b6ca9b18_4522x3019.jpeg" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/76311586-b35a-4d2b-82b1-d8a8b6ca9b18_4522x3019.jpeg&quot;,&quot;height&quot;:972,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:6649732,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null}" alt=""></a></p><p>Friends,</p><p>I'm floored by the response of the past three days.</p><p>Once I hit <em>send</em>, this article will land in the inboxes of over 15,000 people. When we launched, my main fear was that the world would not be interested in a community that pledges to defend the values of a free society; now, my main fear is that we won't be able to live up to the hype.</p><p>So, here's my promise: We will do our very best to earn your trust. Some great articles will be coming your way soon. We are getting ready to announce more events and high-level members of the community. I hope you will join us for our inaugural townhall, which will take place next Sunday, July 12th, at 4pm EST. (Watch this space for the invite.) I'm very, very excited about what lies ahead. But I also know that it is hard to build this kind of community from the ground up, and that we will undoubtedly make mistakes along the way. Please bear with us when we do.</p><p>In the meanwhile, I want to take you a little deeper into the thinking that went into creating <em>Persuasion</em>. Why this project? Why now? And how can just a bunch of us—even if we are a much larger bunch than I could possibly have dreamed a few days ago—really make a difference to the future of free societies in the United States and around the world? The key to an answer lies in a short (and necessarily schematic) history of American intellectual life over the past half century. </p><p>Fifty years ago, the most important American institutions enjoyed a degree of legitimacy that is now hard to fathom. Nearly every American watched the news on one of the three network television stations. Nearly every American had a positive opinion of Princeton and Stanford. Nearly every Member of Congress believed that the advice of the Brookings Institution or the Council on Foreign Relations was to be taken seriously.&nbsp;</p><p>These institutions had much to recommend them: They gave the public a shared set of facts and assumptions, which could form the basis of political debate. And, though they never thought of their primary goal as fighting for the ideals of a free society, their operating system was philosophically liberal: From CBS to Harvard to Brookings, senior decision makers instinctively believed in values like free speech and due process.</p><p>However, these institutions also suffered from two important shortcomings. First, the people they admitted into their gilded halls only represented a small slice of America's population: sexism, racism and homophobia were <em>far</em> more prevalent in these institutions than they are today. The views they considered serious sometimes included the morally abhorrent.&nbsp;</p><p>Second, the realm of the “reasonable" was rather narrow. And, though this narrowness of debate constituted the lesser injustice, it was—at least in the short term—the cause of greater instability: Having come to believe that they could never quite speak in their own voices in the halls of the Brookings Institution or the column inches of the <em>New York Times</em>, a few assorted bands of malcontents started to cast around for an alternative. </p><p>Of these, the group that had the biggest impact on public life in America was a band of devoted conservatives, determined to create an ideological counter-establishment that could rival the mainstream.&nbsp;What started with <em>National Review</em>, an ideological fighting magazine, quickly grew into a sprawling and immensely powerful network of conservative institutions. The Heritage Foundation was set up to rival the influence of Brookings. The Federalist Society sought to change the ideological composition of America's judiciary. Fox News did its dismal best to spread the ideas of the conservative movement beyond the Beltway. A whole network of activist groups provided conservatives with an ideological foundation, a group of friends, and a professional home. Measured by its own ambitions,&nbsp;the movement was a staggering success.</p><p>Other minoritarian ideological movements took a page out of the same playbook. In 1960, a libertarian was a person with idiosyncratic views and no obvious political home. Then, the Institute for Humane Studies started to advocate libertarian ideas on college campuses, <em>Reason </em>took up their public defense, and a reinvigorated American Enterprise Institute set out to influence legislators on Capitol Hill. By 1980, the influence and intellectual self-confidence of libertarians had increased enormously.</p><p>The further left has always had its share of counter-establishment institutions. <em>The Nation</em>, after all, is one of the oldest magazines in the country, and some academic disciplines have long been at the forefront of leftist thought. But the left, too, has of late succeeded in building a more cohesive network of fighting institutions, as universities have become much more progressive, movements like the Democratic Socialists of America have awakened from decades of peaceful slumber, and publications like <em>Jacobin </em>have infused the movement with fresh intellectual energy.</p><p>Five or ten years ago, our potted history might have concluded here. Ideological movements from conservative to libertarian to leftist had fighting institutions of their own. Though philosophical liberals did not have a comparable home, they could confidently express their views within mainstream institutions.&nbsp;</p><p>But then those institutions started to change.</p><p>The story of that change has attracted an immense amount of attention over the past months. I won't bore you with a detailed recap of its most worrying manifestations, from the firing of James Bennet to the uncritical celebration of Robin DiAngelo. Nor do I want to suggest that these changes have completely delegitimized the mainstream: These institutions have not yet become wholly illiberal, and the advocates of a free society would be foolish to stop fighting for them.</p><p>But the erosion of values like free speech and due process within mainstream institutions does put philosophical liberals at a unique disadvantage. It is difficult to convey just how many amazing writers, journalists, and think-tankers—some young and some old, some relatively obscure and others very famous—have privately told me that they can no longer write in their own voices; that they are counting the days until they get fired; and that they don't know where to turn if they do. (Astonishingly, a number of them are far enough to the left to have supported Bernie Sanders in the primaries.)</p><p>This, to me, is a huge part of the reason why the defenders of the free society have seemed to lack conviction in recent months and years. Feeling, at best, begrudgingly tolerated by the institutions that employ them, they are always on the back foot: writing and speaking with one eye on Twitter, one eye on a hostile editor, and one eye on the attacks being shared on their own company’s Slack channel. (As you may have noticed, that requires too many eyes.)</p><p>But, if this situation helps to explain the collective lack of confidence among the advocates of a free society, it also points the way to an obvious solution. <strong>Instead of lamenting our loss of control over the establishment, we should follow the lead of other movements that have successfully built their own counter-establishment institutions.</strong><em>&nbsp;</em></p><p><em>That </em>is the goal I had in mind in starting <em>Persuasion</em>.</p><p>One core element of this project is a publishing platform explicitly devoted to debating, articulating, and defending the values of a free society. Emulating what <em>Reason</em>, <em>Jacobin,</em> and the <em>National Review</em> have accomplished within their own ideological traditions, I hope to create a space in which philosophical liberals can ask hard questions and come up with compelling answers. This requires both a commitment to a set of shared aspirations and enough diversity of opinion to force us to think very hard about how we can make the world a better place. This is a space for people who are open to changing their minds, but not their fundamental values.</p><p>But creating a modern reinvention of a fighting magazine, devoted to defending the ideals of a free society, is not my only ambition. If places like the <em>National Review</em> had a tremendous influence on our society, it is also because they became the nucleus of a cohesive community, which seeded a much wider archipelago of allied institutions. This is why I take the community element of <em>Persuasion</em>—all the live events, book clubs and social gatherings we'll experiment with over the coming months—so seriously. And it is also why I hope that this particular venture will spawn many formally independent organizations that share our founding values.</p><p>Before I close, let me say two quick words about some of the establishment institutions whose recent fate I have been lamenting. The first is that we must do what we can to preserve those universities, publications, and think tanks that still operate with fundamentally (small l) liberal assumptions. For example, I deeply love <em>The Atlantic</em>, and will continue to write for it. A small fighting institution that primarily addresses a devoted crowd of philosophical liberals neither is nor should be in competition with a large general interest magazine whose readership will always span a much broader ideological range. Part of the reason why we should articulate these values as clearly, forcefully, and persuasively as possible within these pages is to maximize the likelihood that they will continue to form the implicit operating system of vitally important publications like <em>The Atlantic</em>.&nbsp;</p><p>The second thing is that our ambition needs to extend beyond nostalgia. There is much to lament about the changes that have taken place in some of the country's most important institutions over the past years. But there is also much to criticize in what these institutions looked like at their supposed best. Our goal is not to return to a golden age that has, sadly, never existed; it is to build societies that live up to the noble and ambitious values of freedom and justice better than any society of the past.</p><p>The examples I have used here&nbsp;are very …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.persuasion.community/p/the-purpose-of-persuasion">https://www.persuasion.community/p/the-purpose-of-persuasion</a></em></p>]]>
            </description>
            <link>https://www.persuasion.community/p/the-purpose-of-persuasion</link>
            <guid isPermaLink="false">hacker-news-small-sites-23740669</guid>
            <pubDate>Sun, 05 Jul 2020 18:16:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Deep Introduction to JIT Compilers: JITs are not very Just-in-time]]>
            </title>
            <description>
<![CDATA[
Score 278 | Comments 89 (<a href="https://news.ycombinator.com/item?id=23740655">thread link</a>) | @chrisseaton
<br/>
July 5, 2020 | https://carolchen.me/blog/jits-intro/ | <a href="https://web.archive.org/web/*/https://carolchen.me/blog/jits-intro/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
  <header>
    
    
  </header>

  <main id="main">
    
<article>
  <header>
    
    
  </header>
  <section id="js-article">
    
<p><em>If you are familiar with how JITs generally work (if you get what the title is referring to), I recommend skimming this or going straight to reading <a href="https://carolchen.me/blog/jits-impls">How JIT Compilers are Implemented and Fast: Julia, Pypy, LuaJIT, Graal and More</a></em> </p>
<p>My mentor, <a href="https://chrisseaton.com/">Chris</a>, who took me from “what is a JIT” to where I am now once told me that compilers were just bytes in bytes out and not at all low-level and scary. This is actually fairly true, and it's fun to learn about compiler internals and often useful for programmers everywhere!</p>
<p>This blog post gives background on how programming languages are implemented and how JITs work. It'll introduce the implementation details of the Julia language, though it won't talk about specific implementation details or optimizations made by more traditional JITs. Check out <a href="https://carolchen.me/blog/jits-impls">How JIT Compilers are Implemented and Fast: Julia, Pypy, LuaJIT, Graal and More</a> to read about how meta-tracing is implemented, how Graal supports C extensions, the relationship of JITs with LLVM and more!</p>
<h2 id="how-programming-languages-are-implemented">How Programming Languages are Implemented<a href="#how-programming-languages-are-implemented" aria-label="Anchor link for: how-programming-languages-are-implemented"> <i></i></a>
</h2>
<p>When we run a program, it’s either interpreted or compiled in some way. The compiler/interpreter is sometimes referred to as the "implementation" of a language, and one language can have many implementations. You may have heard things like "Python is interpreted", but that really means the reference(standard/default) implementation of Python is an interpreter. Python is a language specification and <em>CPython</em> is the interpreter and implementation of Python. </p>
<p>An interpreter is a program that directly executes your code. Well-known interpreters are usually written in C. Ruby, Python and PHP are written in C. Below is a function that loosely models how an interpreter might work:</p>
<pre><code><span>func </span><span>interpret</span><span>(</span><span>code </span><span>string</span><span>) {
  </span><span>if </span><span>code </span><span>== </span><span>"print('Hello, World!')" </span><span>{
    </span><span>print</span><span>(</span><span>"Hello, World"</span><span>);
  } </span><span>else if </span><span>code </span><span>==</span><span> “</span><span>x </span><span>= </span><span>0</span><span>; </span><span>x </span><span>+= </span><span>4</span><span>; </span><span>print</span><span>(</span><span>x</span><span>)” {
    variable_x </span><span>:= </span><span>0 
    </span><span>variable_x </span><span>+= </span><span>4
    </span><span>print</span><span>(</span><span>x</span><span>)
  }
}
</span></code></pre>
<p>A compiler is a program that translates code from some language to another language, though it usually refers to a destination language that is a machine code. Examples of compiled languages are C, Go and Rust.</p>
<pre><code><span>func </span><span>compile</span><span>(</span><span>code </span><span>string</span><span>) {
  []</span><span>byte </span><span>compiled_code </span><span>= </span><span>get_machine_code</span><span>(</span><span>code</span><span>);
  </span><span>write_to_executable</span><span>(</span><span>compiled_code</span><span>);
}
</span></code></pre>
<p>The difference between a compiled and interpreted language is actually much more nuanced. C, Go and Rust are clearly compiled, as they output a machine code file - which can be understood natively by the computer. The compile and run steps are fully distinct.</p>
<p>However, compilers can translate to any target language (this is sometimes called transpiling). Java for example, has a two-step implementation. The first is compiling Java source to bytecode, which is an Intermediate Representation (IR). The bytecode is then JIT compiled - which involves interpretation.</p>
<p>Python and Ruby also execute in two steps. Despite being known as interpreted languages, their reference implementations actually compile the source down to a bytecode. You may have seen .pyc files (not anymore in Python3) which contain Python bytecode! The bytecode is then interpreted by a virtual machine. These interpreters use bytecode because programmers tend to care less about compile time, and creating a bytecode language allows the engineers to specify a bytecode that is as efficient to interpret as possible. </p>
<p>Having bytecode is how languages check syntax before execution (though they could technically just do a pass before starting the interpreter). An example below shows why you would want to check syntax before runtime.</p>
<pre><code><span>sleep</span><span>(</span><span>1000</span><span>)
bad syntax beep boop beep boop
</span></code></pre>
<p>Another important note is that interpreted languages are typically slower for various reasons, the most obvious being that they're executed in a higher level language that has overhead execution time. The main reason is that the dynamic-ness of the languages they tend to implement means that they need many extra instructions to decide what to do next and how to route data. People still choose to build interpreters over compilers because they're easier to build and are more suited to handle things like dynamic typing, scopes etc (though you could build a compiler that has the same features). </p>
<h3 id="so-what-is-a-jit">So What is a JIT?<a href="#so-what-is-a-jit" aria-label="Anchor link for: so-what-is-a-jit"> <i></i></a>
</h3>
<p>A JIT compiler doesn't compile code Ahead-Of-Time (AOT), but still compiles source code to machine code and therefore is not an interpreter. JITs compile code at runtime, while your program is executing. This gives the JITs flexibility for dynamic language features, while maintaining speed from optimized machine code output. JIT-compiling C would make it slower as we'd just be adding the compilation time to the execution time. JIT-compiling Python would be fast, as compilation + executing machine code can often be faster than interpreting, especially since the JIT has no need to write to a file (disk writing is expensive, memory/RAM/register writing is fast). JITs also improve in speed by being able to optimize on information that is only available at runtime.</p>
<h3 id="julia-a-jit-compiler-that-s-just-in-time">Julia: a JIT Compiler that's Just-in-time<a href="#julia-a-jit-compiler-that-s-just-in-time" aria-label="Anchor link for: julia-a-jit-compiler-that-s-just-in-time"> <i></i></a>
</h3>
<p>A common theme between compiled languages is that they're statically typed. That means when the programmer creates or uses a value, they’re telling the computer what type it is and that information is guaranteed at compile time.</p>
<p>Julia is dynamically typed, but internally Julia is much closer to being statically typed.</p>
<pre><code><span>function </span><span>multiply</span><span>(x, y)
  x </span><span>*</span><span> y
</span><span>end
</span></code></pre>
<p>Here is an example of a Julia function, which could be used to multiply integers, floats, vectors, strings etc (Julia allows operator overloading). Compiling out the machine code for <em>all</em> these cases is not very productive for a variety of reasons, which is what we'd have to do if we wanted Julia to be a compiled language. Idiomatic programming means that the function will probably only be used by a few combinations of types and we don't want to compile something that we don't use yet since that's not very jitty (this is not a real term).</p>
<p>If I were to code <code>multiply(1, 2)</code>, then Julia will compile a function that multiplies integers. If I then wrote <code>multiply(2, 3)</code>, then the already-compiled code will be used. If I then added <code>multiply(1.4, 4)</code>, another version of the function will be compiled. We can observe what the compilation does with <code>@code_llvm multiply(1, 1)</code>, which generates LLVM Bitcode (not quite machine code, but a lower-level Intermediate Representation).</p>
<pre><code><span>define i64 @julia_multiply_17232(i64, i64) {
top</span><span>:</span><span>
; ┌ @ int</span><span>.</span><span>jl</span><span>:</span><span>54</span><span> within `*'
   </span><span>%</span><span>2 </span><span>=</span><span> mul i64 </span><span>%</span><span>1</span><span>, </span><span>%</span><span>0</span><span>
; └
  ret i64 </span><span>%</span><span>2</span><span>
}
</span></code></pre>
<p>And with <code>multiply(1.4, 4)</code>, you can see how complicated it can get to compile even one more function. In AOT compiled Julia, all (some optimizations can be made to reduce) of these combinations would have to live in the compiled code even if only one was used, along with the control flow to delegate. </p>
<pre><code><span>define double @julia_multiply_17042(double, i64) {
top</span><span>:</span><span>
; ┌ @ promotion</span><span>.</span><span>jl</span><span>:</span><span>312</span><span> within `*'
; │┌ @ promotion</span><span>.</span><span>jl</span><span>:</span><span>282</span><span> within `promote'
; ││┌ @ promotion</span><span>.</span><span>jl</span><span>:</span><span>259</span><span> within `_promote'
; │││┌ @ number</span><span>.</span><span>jl</span><span>:</span><span>7</span><span> within `convert'
; ││││┌ @ float</span><span>.</span><span>jl</span><span>:</span><span>60</span><span> within `</span><span>Float64</span><span>'
       </span><span>%</span><span>2 </span><span>=</span><span> sitofp i64 </span><span>%</span><span>1</span><span> to double
; │└└└└
; │ @ promotion</span><span>.</span><span>jl</span><span>:</span><span>312</span><span> within `*' @ float</span><span>.</span><span>jl</span><span>:</span><span>405
   </span><span>%</span><span>3 </span><span>=</span><span> fmul double </span><span>%</span><span>2</span><span>, </span><span>%</span><span>0</span><span>
; └
  ret double </span><span>%</span><span>3</span><span>
}
</span></code></pre>
<p>The general strategy of “assume a type and compile/behave based on that” is called type inferencing, which Julia mildly uses in the examples above. There are a lot of other compiler optimizations that are made, though none of them are very specific to JITs as Julia may be better described as a lazy AOT compiler.</p>
<p>The simplicity of this kind of jitting makes it easy for Julia to also supply AOT compilation. It also helps Julia to benchmark very well, definitely a tier above languages like Python and comparable to C (I'd cite numbers, but those are always nuanced and I don't want to get into that).</p>
<h3 id="so-what-is-a-jit-take-two">So What is a JIT? Take Two.<a href="#so-what-is-a-jit-take-two" aria-label="Anchor link for: so-what-is-a-jit-take-two"> <i></i></a>
</h3>
<p>Julia is actually the jittiest JIT I'll discuss, but not the most interesting as a "JIT". It actually compiles code right before the code needs to be used -- just in time. Most JITs however (Pypy, Java, JS Engines), are not actually about compiling code just-in-time, but compiling <em>optimal code</em> at an optimal time. In some cases that time is actually never. In other cases, compilation occurs more than once. In a vast majority of the cases compilation doesn't occur until after the source code has been executed numerous times, and the JIT will stay in an interpreter as the overhead to compilation is too high to be valuable.</p>
<p><img src="https://carolchen.me/blog/img/jits/jitbrr.jpg" alt=""></p>
<p>The other aspect at play is generating <em>optimal code</em>. Assembly instructions are not created equal, and compilers will put a lot of effort into generating well-optimized machine code. Usually, it is possible for a human to write better assembly than a compiler (though it would take a fairly smart and knowledgeable human), because the compiler cannot dynamically analyze your code. By that, I mean things like knowing the possible range of your integers or what keys are in your map, as these are things that a computer could only know after (partially) executing your program. A JIT compiler can actually do those things because it interprets your code first and gathers data from the execution. Thus, JITs are expensive in that they interpret, and add compilation time to execution time, but they make it up in highly optimised compiled code. With that, the timing of compilation is also dependent on whether the JIT has gathered enough valuable information.</p>
<p>The cool part about JITs is that I was sort of lying when I said a JIT implementation of C could not be faster than existing compiled implementations. It would not be feasible to try, but jit-compiling C in the way I just described is not a strict superset of compiling a language and thus it is not logically impossible to compile code fast enough to make up for the compile+profile+interpreting time. If I "JIT compiled" C similarly to how Julia does it (statically compile each function as it's called), it would be impossible to …</p></section></article></main></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://carolchen.me/blog/jits-intro/">https://carolchen.me/blog/jits-intro/</a></em></p>]]>
            </description>
            <link>https://carolchen.me/blog/jits-intro/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23740655</guid>
            <pubDate>Sun, 05 Jul 2020 18:14:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Multiple Dispatch in Julia]]>
            </title>
            <description>
<![CDATA[
Score 39 | Comments 8 (<a href="https://news.ycombinator.com/item?id=23740622">thread link</a>) | @wikunia
<br/>
July 5, 2020 | https://opensourc.es/blog/basics-multiple-dispatch/ | <a href="https://web.archive.org/web/*/https://opensourc.es/blog/basics-multiple-dispatch/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>From time to time I hear that my posts are a little bit too deep and complicated for beginners. I totally agree: especially in my newest series on <a href="https://opensourc.es/blog/constraint-solver-1">"How to build a constraint solver from scratch?"</a> which is probably a never ending series. I do like to blog about it and will continue but this series is different.</p>
<p>Julia is a relatively young language and not too many people are using it. It thrives in scientific computing but I believe that it can be used for general computing (probably where start up time is not that relevant) i.e. I do use it for creating this blog with <a href="https://franklinjl.org/">Franklin</a> which I blogged a bit about as well <a href="https://opensourc.es/blog/Franklin.jl">here</a>.</p>
<p>This series tries to explain some of the core concepts of Julia and maybe some packages to beginners. I, the explainer of this stuff here, am by no means an expert in any of these. If you know this blog then you might know that I'm trying to explain stuff directly after I've learned them to hopefully be able to communicate better than some people who do this their entire life and are experts in it. Blogs from experts are sometimes hard to follow for me, so maybe also for you. I'll let them proof read my blog post just to be sure that there is nothing wrong with what I explain ;)</p>
<p>Posts in this series can be reached over the side bar. </p>
<p>Post 2: <a href="https://opensourc.es/blog/basics-repl-revise">REPL &amp; Revise</a></p>
<p>Okay everyone ready?</p>

<h2 id="what_is_dispatch"><a href="#what_is_dispatch">What is dispatch?</a></h2>
<p>In every programming language there are <code>functions</code> which have the purpose of providing structure and reusability of code. The functions have a <code>name</code> and some <code>arguments</code>.  They are defined like</p>
<pre><code>def add(x, y):
    return x + y</code></pre>
<p>In Python this can be called with <code>add(2,3)</code> which gives the expected <code>5</code> but also <code>add("2","3")</code> gives <code>'23'</code> which might make sense or not depending on who you ask :D</p>
<p>Now how about <code>add("2", 3)</code> ?</p>
<p>This results in an error <code>TypeError: can only concatenate str (not "int") to str</code> which again can make sense. Well I would say it does but Javascript does something different:</p>
<pre><code>function add(x,y) {
    return x+y
}</code></pre>
<p>will produce <code>"23"</code> for that.</p>
<p>Then there are of course languages like <code>C++</code> where this all is not that simple because we need to introduce types:</p>
<pre><code>int add(int x, int y) {
  return x+y;
}</code></pre>
<p>There it is clear for everyone that it takes two <code>int</code> and returns one <code>int</code>. Because it is a compiled language we would get an error directly when trying to call it with <code>add("2", 3)</code>.</p>
<p>Now what is the point? We can see different concepts here with having static types as in C++ and dynamic typing in JS and Python. They both have pros and cons which is probably obvious.  The one is easier to reason about and the others are maybe easier to hack around with.</p>
<p>Let's go into dispatching on those three languages:</p>
<p>In Python it is: Ah okay the user wants to call <code>add</code>:</p>
<ul>
<li><p>We have the <code>add</code> function</p>
</li>
<li><p>Lets call it with the arguments.</p>
</li>
<li><p>Good that works as it expects two arguments and we have two.</p>
</li>
</ul>
<p>Then we might get to the point where the function doesn't work like for <code>add("2", 3)</code> and throw an error or it works out and the answer is returned. </p>
<p>It is basically the same in JS. There are possibility some "extensions" like Typescript where things might happen differently. I haven't programmed with any of those languages lately so keep that in mind.</p>
<p>In <code>C++</code> more things are going on as each variable has a static type and one can check directly whether this fits or not. This means as we can later see that there can be more functions with the same name. I'll explain the difference between function overloading and multiple dispatch in that part ;)</p>
<p>In all of these languages we can have classes such that we might have a class <code>Manufacturer</code> and we can define <code>add(self, thing)</code> or something like that and can call the function with  <code>manufacturer.add(thing)</code>. This can be kind of seen as single dispatch. Dispatch is basically the process of deciding which function to call. Here it depends on the type of <code>manufacturer</code>. Is it a <code>Manufacturer</code> or a <code>Box</code>? For a <code>Box</code> we might have defined a class <code>Box</code> and <code>add(self, box)</code> inside of it.</p>
<div><p>⚠ Note</p> <p>These examples are more from the Python world but hopefully convey the point.</p></div>
<p>For people coding in Julia for longer this might sound like a weird concept. Actually writing about it, I am thinking: How would I do some things, where I use multiple dispatch all the time (like in the ConstraintSolver) in one of those languages?</p>
<p>Before I explain multiple dispatch I want to note:</p>
<p>Yes there are ways in Python for single dispatching like <a href="https://www.blog.pythonlibrary.org/2016/02/23/python-3-function-overloading-with-singledispatch/">@singledispatch</a> but given that the main posts I found when searching are 3-4 years old I doubt that a lot are using it :D</p>
<p>And it is still single dispatch.</p>
<h2 id="how_does_dispatch_work_in_julia"><a href="#how_does_dispatch_work_in_julia">How does dispatch work in Julia?</a></h2>
<p>Now that there is that out of our way let us have a look at one example in Julia:</p>
<pre><code>add(x, y) = x+y</code></pre>
<p>just to show a neat little way of defining one-line functions... (or as they are called in Julia: Methods)</p>
<p>The interesting things is when you type this in the Julia REPL (Read-eval-print loop) you get:</p>
<pre><code>julia&gt; add(x, y) = x+y
add (generic function with 1 method)</code></pre>
<p>calling that function works basically like in Python (from the user perspective for now). It doesn't work for strings though.</p>
<div><p>⚠ Note</p> <p>Julia and <code>+</code> for strings: Julia is a mathematical language and <code>+</code> is commutative whereas concatenating strings is not. So <code>"2"+"3"</code> is not <code>"3"+"2"</code>. Therefore Julia decided to use <code>*</code> instead. Which is commutative for numbers but not matrices for example.</p></div>
<p>Okay where did I interrupt myself? ... Ah yeah so we have an <code>add</code> function with one method now in the REPL. That looks like we might be able to add a new one, right?</p>
<div><p>⚠ Note</p> <p>As correctly pointed out on <a href="https://www.reddit.com/r/Julia/comments/hfk3u3/basics_multiple_dispatch_start_of_a_new_series/fvykyl3?utm_source=share&amp;utm_medium=web2x">Reddit</a>: I use mostly the word function. In Julia there is actually a difference between functions and methods. There is one <code>+</code> function with a lot of different implementations: called methods.</p></div>
<pre><code>julia&gt; add(x,y) = 2x+y
add (generic function with 1 method)</code></pre>
<p>okay that did not work because it still allows all types of inputs (and throws an error later when it doesn't work) because the compiler had no way to decide which <strike> function</strike>method to call. </p>
<p>Should it guess? It just overwrites the old method.</p>
<p>A small side step again: Let's check what happens when we call <code>add("2", "3")</code>
</p><pre><code>julia&gt; add("a", "b")
ERROR: MethodError: no method matching *(::Int64, ::String)
Closest candidates are:
  *(::Any, ::Any, ::Any, ::Any...) at operators.jl:529
  *(::Missing, ::AbstractString) at missing.jl:174
  *(::T, ::T) where T&lt;:Union{Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8} at int.jl:54
  ...
Stacktrace:
 [1] add(::String, ::String) at ./REPL[3]:1
 [2] top-level scope at REPL[4]:1</code></pre>
<p>That error occurs when calling <code>2x</code> where it figured that <code>2</code> is an integer and <code>x = "2"</code> is a string. It gives us information of what kind of types it can multiply.</p>
<p>Let's pick one: </p><pre><code>*(::T, ::T) where T&lt;:Union{Int128, Int16, Int32, Int64, Int8, UInt128, UInt16, UInt32, UInt64, UInt8} at int.jl:54</code></pre>
<p>This tells us that we can multiply two numbers of those types when they are the same. So <code>UInt8</code> with <code>UInt8</code> but I come to that syntax later.</p>
<p>You might wonder how many of those <code>*</code> <strike> functions</strike>methods there are: <code>357</code> is the answer which you get when typing</p>
<pre><code>julia&gt; methods(*)
...</code></pre>
<p>which gives you a long list with all kind of weird types where it sometimes spreads over several lines. I mean what is this? :D</p>
<pre><code>[345] *(A::LinearAlgebra.LQ{TA,S} where S&lt;:AbstractArray{TA,2}, B::Union{DenseArray{TB,1}, DenseArray{TB,2}, Base.ReinterpretArray{TB,1,S,A} where S where A&lt;:Union{SubArray{T,N,A,I,true} where I&lt;:Union{Tuple{Vararg{Real,N} where N}, Tuple{AbstractUnitRange,Vararg{Any,N} where N}} where A&lt;:DenseArray where N where T, DenseArray}, Base.ReinterpretArray{TB,2,S,A} where S where A&lt;:Union{SubArray{T,N,A,I,true} where I&lt;:Union{Tuple{Vararg{Real,N} where N}, Tuple{AbstractUnitRange,Vararg{Any,N} where N}} where A&lt;:DenseArray where N where T, DenseArray}, Base.ReshapedArray{TB,1,A,MI} where MI&lt;:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N} where N} where A&lt;:Union{Base.ReinterpretArray{T,N,S,A} where S where A&lt;:Union{SubArray{T,N,A,I,true} where I&lt;:Union{Tuple{Vararg{Real,N} where N}, Tuple{AbstractUnitRange,Vararg{Any,N} where N}} where A&lt;:DenseArray where N where T, DenseArray} where N where T, SubArray{T,N,A,I,true} where I&lt;:Union{Tuple{Vararg{Real,N} where N}, Tuple{AbstractUnitRange,Vararg{Any,N} where N}} where A&lt;:DenseArray where N where T, DenseArray}, Base.ReshapedArray{TB,2,A,MI} where MI&lt;:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N} where N} where A&lt;:Union{Base.ReinterpretArray{T,N,S,A} where S where A&lt;:Union{SubArray{T,N,A,I,true} where I&lt;:Union{Tuple{Vararg{Real,N} where N}, Tuple{AbstractUnitRange,Vararg{Any,N} where N}} where A&lt;:DenseArray where N where T, DenseArray} where N where T, SubArray{T,N,A,I,true} where I&lt;:Union{Tuple{Vararg{Real,N} where N}, Tuple{AbstractUnitRange,Vararg{Any,N} where N}} where A&lt;:DenseArray where N where T, DenseArray}, SubArray{TB,1,A,I,L} where L where I&lt;:Tuple{Vararg{Union{Int64, AbstractRange{Int64}, Base.AbstractCartesianIndex},N} where N} where A&lt;:Union{Base.ReinterpretArray{T,N,S,A} where S where A&lt;:Union{SubArray{T,N,A,I,true} where I&lt;:Union{Tuple{Vararg{Real,N} where N}, Tuple{AbstractUnitRange,Vararg{Any,N} where N}} where A&lt;:DenseArray where N where T, DenseArray} where N where T, Base.ReshapedArray{T,N,A,MI} where MI&lt;:Tuple{Vararg{Base.MultiplicativeInverses.SignedMultiplicativeInverse{Int64},N} where N} where A&lt;:Union{Base.ReinterpretArray{T,N,S,A} where S where A&lt;:Union{SubArray{T,N,A,I,true} where I&lt;:Union{Tuple{Vararg{Real,N} where N}, Tuple{AbstractUnitRange,Vararg{Any,N} where N}} where A&lt;:DenseArray where N where T, DenseArray} where N where T, SubArray{T,N,A,I,true} where I&lt;:Union{Tuple{Vararg{Real,N} where N}, Tuple{AbstractUnitRange,Vararg{Any,N} where N}} where A&lt;:DenseArray where N where T, DenseArray} where N where T, DenseArray}, SubArray{TB,2,A,I,L} where L where I&lt;:Tuple{Vararg{Union{Int64, AbstractRange{Int64}, Base.AbstractCartesianIndex},N} where N} where A&lt;:Union{Base.ReinterpretArray{T,N,S,A} where S where A&lt;:Union{SubArra…</code></pre></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://opensourc.es/blog/basics-multiple-dispatch/">https://opensourc.es/blog/basics-multiple-dispatch/</a></em></p>]]>
            </description>
            <link>https://opensourc.es/blog/basics-multiple-dispatch/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23740622</guid>
            <pubDate>Sun, 05 Jul 2020 18:10:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Choosing a Rust web framework]]>
            </title>
            <description>
<![CDATA[
Score 89 | Comments 30 (<a href="https://news.ycombinator.com/item?id=23740028">thread link</a>) | @LukeMathWalker
<br/>
July 5, 2020 | https://www.lpalmieri.com/posts/2020-07-04-choosing-a-rust-web-framework-2020-edition/ | <a href="https://web.archive.org/web/*/https://www.lpalmieri.com/posts/2020-07-04-choosing-a-rust-web-framework-2020-edition/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    

<blockquote>
<p><em>This post was originally meant as a section of <a href="https://www.lpalmieri.com/posts/2020-05-24-zero-to-production-0-foreword/"><strong>Zero To Production</strong></a> to explain the reasoning behind our technology choice. It eventually grew so large to be its own article!</em></p>

<p><em>You can discuss the article on <a href="https://news.ycombinator.com/item?id=23740028">HackerNews</a> or <a href="https://www.reddit.com/r/rust/comments/hlpsw5/choosing_a_rust_web_framework_2020_edition/">r/rust</a></em>.</p>
</blockquote>

<p>As of July 2020, the main web frameworks in the Rust ecosystem are:</p>

<ul>
<li><a href="https://actix.rs/"><code>actix-web</code></a>;<br></li>
<li><a href="https://rocket.rs/"><code>rocket</code></a>;<br></li>
<li><a href="https://github.com/http-rs/tide"><code>tide</code></a>;<br></li>
<li><a href="https://github.com/seanmonstar/warp"><code>warp</code></a>.</li>
</ul>

<p>Which one should you pick if you are about to start building a new <strong>production-ready</strong> API in Rust?</p>

<p>I will break down where each of those web frameworks stands when it comes to:</p>

<ul>
<li><a href="#1-comprehensiveness">Comprehensiveness</a>;<br></li>
<li><a href="#2-community-and-adoption">Community and adoption</a>;<br></li>
<li><a href="#3-sync-vs-async">Sync vs Async</a>, as well as their choice of <a href="#3-1-futures-runtime">futures runtime</a>;<br></li>
<li><a href="#4-documentation-tutorials-and-examples">Documentation, tutorials and examples</a>;<br></li>
<li><a href="#5-api-and-ergonomics">API and ergonomics</a>.</li>
</ul>

<p>I will in the end make <a href="#6-our-choice">my recommendation</a>.<br>
Worth remarking that there are no absolutes: different circumstances (and taste) might lead you to a different pick.</p>

<h2 id="1-comprehensiveness">1. Comprehensiveness</h2>

<p><code>actix-web</code>, <code>tide</code> and <code>warp</code> are <em>slim</em> web frameworks: they offer you an HTTP web server, routing logic, middleware infrastructure and basic building blocks and abstractions to parse, manipulate and respond to HTTP requests.</p>

<p><code>rocket</code> takes a different approach - it aims to be batteries-included: the most common needs should be covered by functionality provided out-of-the-box by <code>rocket</code> itself, with hooks for you to extend <code>rocket</code> if your usecase needs it.<br>
It should not come as a surprise then that <code>rocket</code> ships an easy-to-use <a href="https://rocket.rs/v0.4/guide/state/#databases">integration to manage connection pools</a> for several popular database (e.g. Postgres, Redis, Memcache, etc.) as well as its own <a href="https://rocket.rs/v0.4/guide/configuration/">configuration system</a> in <a href="https://api.rocket.rs/v0.4/rocket_contrib/"><code>rocket-contrib</code></a>, an ancillary crate hosted in <code>rocket</code>’s own repository.</p>

<p>We can compare them to frameworks available in other ecosystems:</p>

<ul>
<li><code>actix-web</code>, <code>tide</code> and <code>warp</code> are closer in spirit to <a href="https://palletsprojects.com/p/flask/"><code>Flask</code></a> from Python or <a href="https://expressjs.com/"><code>Express</code></a> from Javascript - they might be opinionated, but they do not ship a configuration management system or an ORM integration out of the box. You are in charge of structuring your API as you deem appropriate, bringing all the necessary crates and patterns into the picture;<br></li>
<li><code>rocket</code> is closer to <a href="https://www.djangoproject.com/"><code>Django</code></a> from Python or <a href="https://symfony.com/"><code>Symphony</code></a> from PHP: a stable and solid core with a set of high-quality in-tree components to fulfill your every day needs when building a solid web application. <code>rocket</code> has still a long way to go to match its peers in breadth and scope, but it is definitely off to a good start.</li>
</ul>

<p>Of course this is a snapshot of the landscape as of today, but the situation is continuously shifting according to the maintainers’ intentions - e.g. <code>actix-web</code> has slowly been accumulating more and more supporting functionality (from security to session management) in <a href="https://github.com/actix/actix-extras"><code>actix-extras</code></a>, under the umbrella of the <code>actix</code> GitHub organization.<br>
Furthermore, using a slim web framework does not force you to write everything from scratch as soon as the framework is falling short of your needs: you can leverage the ecosystem built by the community around it to avoid re-inventing the wheel on every single project.</p>

<h2 id="2-community-and-adoption">2. Community and adoption</h2>

<p>Numbers can be misleading, but they are a good conversation starting point. Looking at <a href="https://crates.io/">crates.io</a>, we have:</p>

<table>
<thead>
<tr>
<th>Framework</th>
<th>Total Downloads</th>
<th>Daily Downloads</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>actix-web</code></td>
<td>~1250k</td>
<td>~3000</td>
</tr>

<tr>
<td><code>rocket</code></td>
<td>~525k</td>
<td>~1000</td>
</tr>

<tr>
<td><code>warp</code></td>
<td>~435k</td>
<td>~3000</td>
</tr>

<tr>
<td><code>tide</code></td>
<td>~47k</td>
<td>~300</td>
</tr>
</tbody>
</table>

<p>The number of total downloads is obviously influenced by how long a framework has been around (e.g. <code>actix-web:0.1.0</code> came out at the end of 2017!) while daily downloads are a good gauge for the current level of interest around it.</p>

<p>You should care about adoption and community size for a couple of reasons:</p>

<ul>
<li>consistent production usage over years makes it way less likely that you are going to be the first one to spot a major defect. Others cried so that you could smile (most of the time);<br></li>
<li>it correlates with the number of supporting crates for that framework;<br></li>
<li>it correlates with the amount of tutorials, articles and helping hands you are likely to find if you are struggling.</li>
</ul>

<p>The second point is particularly important for slim frameworks.<br>
You can get a feel of the impact of community size, once again, by looking at the number of results popping up on <a href="https://crates.io/">crates.io</a> when searching a framework name:</p>

<table>
<thead>
<tr>
<th>Framework</th>
<th># results</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>rocket</code></td>
<td>178</td>
</tr>

<tr>
<td><code>actix-web</code></td>
<td>113</td>
</tr>

<tr>
<td><code>warp</code></td>
<td>57</td>
</tr>

<tr>
<td><code>tide</code></td>
<td>20</td>
</tr>
</tbody>
</table>

<p>Will all those crates be relevant? Unlikely.<br>
Will a fair share of them be outdated or unproven? Definitely.</p>

<p>Nonetheless it is a good idea, before starting a project, to have a quick look for functionality you know for a fact you will need. Let’s make a couple of quick examples with features we will be relying on in the email newsletter implementation we are building in <em>Zero To Production</em>:</p>

<ul>
<li>if you need to add Prometheus’ metrics to your API you can get off the ground in a couple of minutes with <a href="https://crates.io/crates/actix-web-prom"><code>actix-web-prom</code></a> or <a href="https://crates.io/crates/rocket_prometheus"><code>rocket-prometheus</code></a>, both with thousands of downloads. If you are using <code>warp</code> or <code>tide</code> you will have to write the integration from scratch;<br></li>
<li>if you want to add distributed tracing, <a href="https://crates.io/crates/actix-web-opentelemetry"><code>actix-web-opentelemetry</code></a> has your back. You will have to re-implement it if you choose any other framework.</li>
</ul>

<p>Most of these features are not too much work to implement, but the effort (especially maintenance) compounds over time. You need to choose your framework with your eyes wide open on the level of commitment it is going to require.</p>

<h2 id="3-sync-vs-async">3. Sync vs Async</h2>

<p>Rust landed its <code>async</code>/<code>await</code> syntax in version <code>1.39</code> - a game changer in terms of ergonomics for asynchronous programming.<br>
It took some time for the whole Rust ecosystem to catch up and adopt it, but it’s fair to say that crates dealing with IO-bound workloads are now generally expected to be async-first (e.g. <code>reqwest</code>).</p>

<p>What about web frameworks?<br>
<code>actix-web</code> adopted <code>async</code>/<code>await</code> with its <code>0.2.x</code> release, same as <code>warp</code>, while <code>tide</code> was using <code>async</code>/<code>await</code> before its stabilisation relying on the <code>nightly</code> Rust compiler.<br>
<code>rocket</code>, instead, still exposes a synchronous interface. <code>async</code>/<code>await</code> support is expected as part of its next <code>0.5</code> release, <a href="https://github.com/SergioBenitez/Rocket/issues/1065">in the making since last summer</a>.</p>

<p>Should you rule out <code>rocket</code> as a viable option because it does not yet support asynchronous programming?<br>
It depends.<br>
If you are implementing an application to handle high volumes of traffic with strict performance requirements it might be better to opt for an async web framework.<br>
If that is not the case, the lack of async support in <code>rocket</code> should not be one of your primary concerns.</p>

<h3 id="3-1-futures-runtime">3.1. Futures runtime</h3>

<p><code>async</code>/<code>await</code> is not all sunshine and roses.<br>
Asynchronous programming in Rust is built on top of the <code>Future</code> trait: a future exposes a <code>poll</code> method which has to be called to allow the future to make progress. You can think of Rust’s futures as <em>lazy</em>: unless polled, there is no guarantee that they will execute to completion.<br>
This is often been described as a <em>pull</em> model compared to the <em>push</em> model adopted by other languages<sup id="fnref:async-announcement"><a href="#fn:async-announcement">1</a></sup>, which has some interesting implications when it comes to performance and task cancellation.</p>

<p>Wait a moment though - if futures are lazy and Rust does not ship a runtime in its standard library, who is in charge to call the <code>poll</code> method?<br>
<strong>BYOR</strong> - <strong>B</strong>ring <strong>Y</strong>our <strong>O</strong>wn <strong>R</strong>untime!<br>
The async runtime is literally a dependency of your project, brought in as a crate.<br>
This provides you with a great deal of flexibility: you could indeed implement your own runtime optimised to cater for the specific requirements of your usecase (see <a href="http://smallcultfollowing.com/babysteps/blog/2019/12/09/async-interview-2-cramertj/#async-interview-2-cramertj">the Fuchsia project</a> or <a href="https://github.com/bastion-rs/bastion"><code>bastion</code></a>’s actor framework) or simply choose the most suitable on a case-by-case basis according to the needs of your application.<br>
That sounds amazing on paper, but reality is a bit less glamorous: interoperability between runtimes is quite poor at the moment; mixing runtimes can be painful, often causing issues that are not straight-forward either to triage, detect or solve.<br>
While most libraries should not depend on runtimes directly, relying instead on the interfaces exposed by the <a href="https://docs.rs/futures/0.3.5/futures/"><code>futures</code></a> crate, this is often not the case due to historical baggage (e.g. <code>tokio</code> was for a long time the only available runtime in the ecosystem), practical needs (e.g. a framework has to be able to spawn tasks) or lack of standardisation (e.g. the ongoing discussion on the <code>AsyncRead</code>/<code>AsyncWrite</code> traits - see <a href="http://smallcultfollowing.com/babysteps/blog/2020/01/20/async-interview-5-steven-fackler/">here</a> and <a href="http://smallcultfollowing.com/babysteps/blog/2020/03/10/async-interview-7-withoutboats/#async-interview-7-withoutboats">here</a>).<br>
Therefore picking an async web framework goes beyond the framework itself: you are choosing an ecosystem of crates, suddenly making it much more cumbersome to consume libraries relying on a different async runtime.</p>

<p>The current state of affairs is far from ideal, but if you are writing async Rust today I’d recommend you to make a <em>deliberate</em> choice when it comes to your async runtime.</p>

<p>The two main general-purpose async runtimes currently available in Rust are <a href="https://tokio.rs/"><code>tokio</code></a> and <a href="https://github.com/async-rs/async-std"><code>async-std</code></a>.<br>
<code>tokio</code> has been around for quite some time and it has seen extensive production usage. It is fairly tunable, although this results in a larger and more complex API surface.<br>
<code>async-std</code> was released almost a year ago, around the time of <code>async</code>/<code>await</code> stabilization. It provides great ergonomics, while leaving less room for configuration knobs.</p>

<p><a href="https://crates.io/">crates.io</a> can once again be used as a gauge for adoption and readiness:</p>

<table>
<thead>
<tr>
<th>Runtime</th>
<th>Total Downloads</th>
<th>Daily Downloads</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>tokio</code></td>
<td>~9600k</td>
<td>~30k</td>
</tr>

<tr>
<td><code>async-std</code></td>
<td>~600k</td>
<td>~4k</td>
</tr>
</tbody>
</table>

<p>How do frameworks map to runtimes?</p>

<table>
<thead>
<tr>
<th>Framework</th>
<th>Runtime</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>actix-web</code></td>
<td><code>tokio</code></td>
</tr>

<tr>
<td><code>rocket</code> (<code>0.5.x</code>)</td>
<td><code>tokio</code></td>
</tr>

<tr>
<td><code>tide</code></td>
<td><code>async-std</code></td>
</tr>

<tr>
<td><code>warp</code></td>
<td><code>tokio</code></td>
</tr>
</tbody>
</table>

<h2 id="4-documentation-tutorials-and-examples">4. Documentation, tutorials and examples</h2>

<p>Having to dive into the source code to understand how something works can be fun (and educational!), but it should be a choice, not a necessity.<br>
In most situations I’d rather rely on the framework being well-documented, including non-trivial examples of relevant usage patterns.<br>
Good documentation, tutorials and fully-featured examples are <strong>mission-critical</strong> if you are working as part of a team, especially if one or more teammates are not experienced Rust developers.</p>

<p>Rust’s tooling treats documentation as a first class concept (just run <code>cargo doc --open</code> to get auto-generated docs for your project!) and it grew to be part of the culture of the Rust community itself. Library authors generally take it seriously and web frameworks are no exception to the general tendency: what you can find on <a href="https://docs.rs/">docs.rs</a> is quite thorough, with contextual examples …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.lpalmieri.com/posts/2020-07-04-choosing-a-rust-web-framework-2020-edition/">https://www.lpalmieri.com/posts/2020-07-04-choosing-a-rust-web-framework-2020-edition/</a></em></p>]]>
            </description>
            <link>https://www.lpalmieri.com/posts/2020-07-04-choosing-a-rust-web-framework-2020-edition/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23740028</guid>
            <pubDate>Sun, 05 Jul 2020 16:49:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Scaling Pandas: Comparing Dask, Ray, Modin, Vaex, and Rapids]]>
            </title>
            <description>
<![CDATA[
Score 84 | Comments 30 (<a href="https://news.ycombinator.com/item?id=23740012">thread link</a>) | @FHMS
<br/>
July 5, 2020 | https://datarevenue.com/en-blog/pandas-vs-dask-vs-vaex-vs-modin-vs-rapids-vs-ray | <a href="https://web.archive.org/web/*/https://datarevenue.com/en-blog/pandas-vs-dask-vs-vaex-vs-modin-vs-rapids-vs-ray">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><p>Python and its most popular data wrangling library, Pandas, are soaring in popularity. Compared to competitors like Java, Python and Pandas make data exploration and transformation <strong>simple</strong>.</p><p>But both Python and Pandas are known to have issues around <strong>scalability</strong> and <strong>efficiency</strong>.</p><p>Python loses some efficiency right off the bat because it’s an interpreted, dynamically typed language. But more importantly, Python has always focused on simplicity and readability over raw power. Similarly, Pandas focuses on offering a simple, high-level API, largely ignoring performance. In fact, the creator of Pandas wrote “<a href="https://wesmckinney.com/blog/apache-arrow-pandas-internals/">The 10 things I hate about pandas</a>,” which summarizes these issues:</p><figure id="w-node-412b9aecdea3-e2612716"><p><img src="https://global-uploads.webflow.com/5d3ec351b1eba4332d213004/5ef62e9007b2509635bd1ba2_image3.png" alt="Ten things Wes McKinney hates about Pandas."></p><figcaption>Performance issues and lack of flexibility are the main things Pandas’ own creator doesn’t like about the library. (<a href="https://wesmckinney.com/blog/apache-arrow-pandas-internals/">source</a>)</figcaption></figure><p>So it’s no surprise that many developers are trying to add more power to Python and Pandas in various ways. Some of the most notable projects are:</p><ul role="list"><li><a href="https://www.datarevenue.com/ml-tools/dask"><strong>Dask</strong></a><strong>:</strong> a low-level scheduler and a high-level partial Pandas replacement, geared toward running code on compute clusters.</li><li><strong>Ray:</strong> a low-level framework for parallelizing Python code across processors or clusters.</li><li><a href="https://www.datarevenue.com/ml-tools/modin"><strong>Modin</strong></a><strong>:</strong> a drop-in replacement for Pandas, powered by either <strong>Dask</strong> or <strong>Ray</strong>.</li><li><a href="https://www.datarevenue.com/ml-tools/vaex"><strong>Vaex</strong></a><strong>:</strong> a partial Pandas replacement that uses lazy evaluation and memory mapping to allow developers to work with large datasets on standard machines.</li><li><a href="https://www.datarevenue.com/ml-tools/rapids"><strong>RAPIDS</strong></a><strong>: </strong>a collection of data-science libraries that run on GPUs and include <a href="https://github.com/rapidsai/cudf">cuDF</a>, a partial replacement for Pandas.</li></ul><p>There are others, too. Below is an overview of the Python data wrangling landscape:</p><figure id="w-node-78c43e6cecae-e2612716"><p><img src="https://global-uploads.webflow.com/5d3ec351b1eba4332d213004/5ef62eb85c7038610cea20d0_image2.png" alt="A graph showing how often popular data wrangling libraries are compared in Google searches."></p><figcaption>Dask, Modin, Vaex, Ray, and CuDF are often considered potential alternatives to each other. Source: Created with <a href="https://anvaka.github.io/vs/?query=Dask">this tool</a></figcaption></figure><p>So if you’re working with a lot of data and need faster results, which should you use?</p><h2><strong>Just tell me which one to try</strong></h2><p>Before you can make a decision about which tool to use, it’s good to have some more context about each of their approaches. We’ll compare each of them closely, but you’ll probably want to try them out in the following order:</p><ul role="list"><li><strong>Modin</strong>, with <strong>Ray</strong> as a backend. By installing these, you might see significant benefit by changing just a single line (`import pandas as pd` to `import modin.pandas as pd`). Unlike the other tools, Modin aims to reach full compatibility with Pandas.</li><li><strong>Dask</strong>,<strong> </strong>a larger and hence more complicated project. But Dask also provides <a href="https://docs.dask.org/en/latest/dataframe.html">Dask.dataframe</a>, a higher-level, Pandas-like library that can help you deal with <a href="https://en.wikipedia.org/wiki/External_memory_algorithm">out-of-core</a> datasets.</li><li><strong>Vaex, </strong>which is designed to help you work with large data on a standard laptop. Its Pandas replacement covers some of the Pandas API, but it’s more focused on exploration and visualization.</li><li><strong>RAPIDS, </strong>if you have access to NVIDIA graphics cards<strong>.</strong></li></ul><h2><strong>Quick comparison</strong></h2><p>Each of the libraries we examine has different strengths, weaknesses, and scaling strategies. The following table gives a broad overview of these. Of course, as with many things, most of the scores below are heavily dependent on your exact situation.&nbsp;</p><figure id="w-node-3fc1cb6579be-e2612716"><p><img src="https://global-uploads.webflow.com/5d3ec351b1eba4332d213004/5ef62ef85090a97b0c469fa9_image5.png" alt="A table comparing the tools across maturity, popularity, ease of adoption, and other metrics."></p><figcaption>Dask and Ray are more mature, but Modin and Vaex are easier to get started with. Rapids is useful if you have access to GPUs.</figcaption></figure><p>These are subjective grades, and they may vary widely given your specific circumstances. When assigning these grades, we considered:</p><ul role="list"><li><strong>Maturity: </strong>The time since the first commit and the number of commits.</li><li><strong>Popularity: </strong>The number of GitHub stars.</li><li><strong>Ease of Adoption: </strong>The amount of knowledge expected from users, presumed hardware resources, and ease of installation.</li><li><strong>Scaling ability: </strong>The broad dataset size limits for each tool, depending on whether it relies mainly on RAM, hard drive space on a single machine, or can scale up to clusters of machines.&nbsp;</li><li><strong>Use case: </strong>Whether the libraries are designed to speed up Python software in general (“<strong>General</strong>”), are focused on data science and machine learning (“<strong>Data science</strong>”), or are limited to simply replacing Pandas’ ‘DataFrame’ functionality (“<strong>DataFrame</strong>”).</li></ul><h2><strong>CPUs, GPUs, Clusters, or Algorithms?</strong></h2><p>If your dataset is too large to work with efficiently on a single machine, your main options are to run your code across…</p><ul role="list"><li><strong>...multiple threads or processors:</strong> Modern CPUs have several independent cores, and each core can run many threads. Ensuring that your program uses all the potential processing power by parallelizing across cores is often the easiest place to start.</li><li><strong>...GPU cores: </strong>Graphics cards were originally designed to efficiently carry out basic operations on millions of pixels in parallel. However, developers soon saw other uses for this power, and “GP-GPU” (general processing on a graphics processing unit) is now a popular way to speed up code that relies heavily on matrix manipulations.</li><li><strong>...compute clusters: </strong>Once you hit the limits of a single machine, you need a networked cluster of machines, working cooperatively.</li></ul><p>Apart from adding more hardware resources, clever algorithms can also improve efficiency. Tools like Vaex rely heavily on <a href="https://en.wikipedia.org/wiki/Lazy_evaluation"><strong>lazy evaluation</strong></a><strong> </strong>(not doing any computation until it’s certain the results are needed) and <a href="https://en.wikipedia.org/wiki/Memory-mapped_file"><strong>memory mapping</strong></a><strong> </strong>(treating files on hard drives as if they were loaded into RAM).</p><p>None of these strategies is inherently better than the others, and you should choose the one that suits your specific problem.</p><p>Parallel programming (no matter whether you’re using threads, CPU cores, GPUs, or clusters) offers many benefits, but it’s also quite complex, and it makes tasks such as debugging far more difficult.</p><p>Modern libraries can hide some – but not all – of this added complexity. No matter which tools you use, you’ll run the risk of expecting everything to work out neatly (below left), but getting chaos instead (below right).</p><figure id="w-node-7b8872b99c95-e2612716"><p><img src="https://global-uploads.webflow.com/5d3ec351b1eba4332d213004/5efef852495a4ce0972910e9_image4_s.jpg" alt="Puppies in a row eating food from different bowls – and then chaos ensues."></p><figcaption>Parallel processing doesn’t always work out as neatly as you expect. (<a href="https://www.reddit.com/r/aww/comments/2oagj8/multithreaded_programming_theory_and_practice/">Source</a>)</figcaption></figure><h2><strong>Dask vs. Ray vs. Modin vs. Vaex vs. RAPIDS</strong></h2><p>While not all of these libraries are direct alternatives to each other, it’s useful to compare them each head-to-head when deciding which one(s) to use for a project.</p><p>Before getting into the details, note that:</p><ul role="list"><li>RAPIDS is a collection of libraries. For this comparison, we consider only the <strong>cuDF</strong> component, which is the RAPIDS equivalent of Pandas.</li><li>Dask is better thought of as two projects: a low-level Python scheduler (similar in some ways to Ray) and a higher-level Dataframe module (similar in many ways to Pandas).</li></ul><h3><strong>Dask vs. Ray</strong></h3><p>Dask (as a lower-level scheduler) and Ray overlap quite a bit in their goal of making it easier to execute Python code in parallel across clusters of machines. Dask focuses more on the data science world, providing higher-level APIs that in turn provide partial replacements for Pandas, NumPy, and scikit-learn, in addition to a low-level scheduling and cluster management framework.</p><p>The creators of Dask and Ray discuss how the libraries compare in <a href="https://github.com/ray-project/ray/issues/642">this GitHub thread</a>, and they conclude that the scheduling strategy is one of the key differentiators. Dask uses a centralized scheduler to share work across multiple cores, while Ray uses distributed bottom-up scheduling.</p><h3><strong>Dask vs. Modin</strong></h3><p>Dask (the higher-level Dataframe) acknowledges the limitations of the Pandas API, and while it partially emulates this for familiarity, it doesn’t aim for full Pandas compatibility. If you have complicated existing Pandas code, it’s unlikely that you can simply switch out Pandas for Dask.Dataframe and have everything work as expected. By contrast, this is exactly the goal Modin is working toward: 100% coverage of Pandas. Modin can run on top of Dask but was originally built to work with Ray, and that integration remains more mature.</p><h3><strong>Dask vs. Vaex</strong></h3><p>Dask (Dataframe) is not fully compatible with Pandas, but it’s pretty close. These close ties mean that Dask also carries some of the baggage inherent to Pandas. Vaex deviates more from Pandas (although for basic operations, like reading data and computing summary statistics, it’s very similar) and therefore is also less constrained by it.</p><p>Ultimately, Dask is more focused on letting you scale your code to compute clusters, while Vaex makes it easier to work with large datasets on a single machine. Vaex also provides features to help you easily visualize and plot large datasets, while Dask focuses more on data processing and wrangling.</p><h3><strong>Dask vs. RAPIDS (cuDF)</strong></h3><p>Dask and RAPIDS play nicely together via an integration <a href="https://rapids.ai/dask.html">provided by</a> RAPIDS. If you have a compute cluster, you should use Dask. If you have an NVIDIA graphics card, you should use RAPIDS. If you have a compute cluster of NVIDIA GPUs, you should use both.</p><h3><strong>Ray vs. Modin or Vaex or RAPIDS</strong></h3><p>It’s not that meaningful to compare Ray to Modin, Vaex, or RAPIDS. Unlike the other libraries, Ray doesn’t offer high-level APIs or a Pandas equivalent. Instead, Ray powers Modin and <a href="https://docs.ray.io/en/latest/tune.html">integrates with RAPIDS</a> in a similar way to Dask.</p><h3><strong>Modin vs. Vaex</strong></h3><p>As with the Dask and Vaex comparison, Modin’s goal is to provide a full Pandas replacement, while Vaex deviates more from Pandas. Modin should be your first port of call if you’re looking for a quick way to speed up existing Pandas code, while Vaex is more likely to be interesting for new projects or specific use cases (especially visualizing large datasets on a single machine).</p><h3><strong>Modin vs. RAPIDS (cuDF)</strong></h3><p>Modin scales Pandas code by using many CPU cores, via Ray or Dask. RAPIDS scales Pandas code by running it on GPUs. If you have GPUs available, give RAPIDS a try. But the easiest win is likely to come from Modin, and you should probably turn to RAPIDS only after you’ve tried Modin first.</p><h3><strong>Vaex vs. RAPIDS (cuDF)</strong></h3><p>Vaex and RAPIDS are similar in that they can both provide performance boosts on a single machine: Vaex by better utilizing your computer’s hard drive and processor cores, and RAPIDS by using your computer’s GPU (if it’s available and compatible). The RAPIDS project as a whole aims to be much broader than Vaex, letting you do machine learning end-to-end without the data leaving your GPU. Vaex is better for prototyping and data exploration, letting you explore large datasets on consumer-grade machines.</p><h2><strong>Final remarks: Premature optimization is the root of all evil</strong></h2><p>It’s fun to play with new, specialized tools. That said, many …</p></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://datarevenue.com/en-blog/pandas-vs-dask-vs-vaex-vs-modin-vs-rapids-vs-ray">https://datarevenue.com/en-blog/pandas-vs-dask-vs-vaex-vs-modin-vs-rapids-vs-ray</a></em></p>]]>
            </description>
            <link>https://datarevenue.com/en-blog/pandas-vs-dask-vs-vaex-vs-modin-vs-rapids-vs-ray</link>
            <guid isPermaLink="false">hacker-news-small-sites-23740012</guid>
            <pubDate>Sun, 05 Jul 2020 16:47:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Here Is What You Need To Know Before Learning Code. Bookmark This Guide]]>
            </title>
            <description>
<![CDATA[
Score 27 | Comments 1 (<a href="https://news.ycombinator.com/item?id=23739809">thread link</a>) | @yassinerajallah
<br/>
July 5, 2020 | https://devhypercharged.com/here-is-what-you-need-to-know-before-learning-how-to-code-bookmark-this-guide/ | <a href="https://web.archive.org/web/*/https://devhypercharged.com/here-is-what-you-need-to-know-before-learning-how-to-code-bookmark-this-guide/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			
			
<p>Before starting out, I’d like to take a minute to thank you for reading &amp; sharing my last article<span>&nbsp;</span><em><a href="https://devhypercharged.com/the-7-traps-that-make-your-software-unusable/">“You Need To Know These 7 Traps That Make Your Software Useless”</a></em></p>
<p>Since we have numerous new members joining the club, feel free to send me your requests, topics, comments, advice, and I’ll make sure to reply to each one of your emails!</p>
<p>Without further ado, let’s dive in.</p>
<p>Learning how to code can be daunting, the progress is slow, the concepts are unique, and the positive feedback loop that will keep you motivated is hard to maintain. Countless are the articles that tell you should learn some framework X because it’s the future or master a language Y because it’s robust. Personally, I wouldn’t have learned multiple stacks (Deep learning, iOS dev, Android dev, Game dev, Cloud services) had I found the right source to guide me. And this is what I wished I had known:</p>
<p>Before choosing what programming language/framework to learn, let’s establish these first principles:</p>
<h3><strong>Programming is an investment&nbsp;</strong></h3>
<p>Learning programming and taking it to the next level are two different things, this is what you’ll be working with for years to come, the market is competitive but you definitely have a spot – if you work hard+smart enough. You don’t need to be a genius but willing to sit down and work. Had I told anyone I wanted to learn deep learning when I was still a freshman with 0 experience in coding, they’d have laughed their life off.</p>
<p>When you pick a programming language, you’ll start building “assets”. Think of assets as utilities that help you in video games. The harder the level, the more you need them to win fast. Typically, after 2 or 3 side projects, you’ll be having a folder full of code snippets that will save you hours! (I’m constantly taking code from projects I’ve finished 3 years ago)</p>
<p>No, you don’t need to stick with your programming language and you can always bounce off to something else. That might sound counter-intuitive, but at least before switching, you’ll have an important cognitive asset:</p>
<ul>
<li>The ability to make complex decisions fast.</li>
<li>Knowing how to learn the new programming language/framework faster &amp; more reliably</li>
<li>Apply the same general concepts onto the new PL</li>
</ul>
<p>However, I don’t recommend switching areas frequently unless you have a valid technical reason for doing so.</p>
<h3><strong>You can’t skip the basics</strong></h3>
<p>Not fun to hear, I’m fully aware. But learning the basics is your first step into programming. Here, you build your tools, learn facts, and polish your skills. Many people struggle with this phase, and when stuck, they think programming isn’t for them.</p>
<p>Think of this step as learning how to reason in sequence. The human brain is a supercomputer on steroids, 1 + 1 appears trivial since you are looking at an equation from a top view. However, the machine only gets to look at 1 operand at a time, so you have to declare your intentions first then tell it what to do with your intentions.</p>
<p>Finally, you elevate your reasoning by solving a real-world problem using an algorithm (a set of instructions).</p>
<p>Again, it doesn’t have to be daunting or scary, as I always say, take an hour or two a day and learn at your own pace. Don’t compare with others because chances are you’ll feel overtaken. And there is no room for intelligence or stupidity in learning code. Only actions and results.</p>
<p>To close off this first part, I invite you to experiment and try whatever works for you. This isn’t the absolute rule on how to learn how to code. If the analogies aren’t that practical for you then great! set your own. If my logic is flawed, then also great, rethink yours! It’s always great to experiment.</p>
<p>Now let’s get you started with choosing the right platform, here is what you should know:</p>
<h2><strong>Gaming<span>&nbsp;</span></strong></h2>
<ul>
<li>Unity is your friend. I still remember the day I decided to make games and become a game dev. The language is C# and unbelievably easy to learn.</li>
<li>Some people prefer using Xcode, but the software is platform restricted, and from my experience, I found it a thousand times easier to learn game dev on Unity than Xcode</li>
<li>The game engine (Unity) does the heavy lifting, you tell the objects to move by a given speed and whether they can collide or not, and there you go, the embedded physics kick in</li>
<li>You’ll have a huge boost by learning design using Blender (An open-source software for design and animation) or similar. Nevertheless, it’s not required, but there will be instances when you wish you’d known how to design</li>
<li>You can compile your game for whatever platform! (Desktop, consoles, web, mobile, etc..) Check out<span>&nbsp;</span><a href="https://unity.com/features/multiplatform">this link</a><span>&nbsp;</span>for more info</li>
<li>The industry is competitive and requires a lot of discipline since you’ll be working long hours. Some like it some don’t, it’s up to you to decide</li>
<li>You are restricted a bit in terms of employment. There are only as many game dev companies out there, and if you don’t like the domain anymore, you’ll need to learn another skill instead of reusing what you already know</li>
<li>Game physics are annoying sometimes, however, the more you learn, the easier it gets – Classic debugging scenario. Game dev is inherently time-consuming given the small details to address. Also, often you’ll get unwanted guests: Bugs. The combinations of physics are near unlimited, sometimes you’ll find yourself debugging for 5 hours a small bug that you simply can’t fix. In this scenario, you can get help from someone you know or revert back to online forums.</li>
<li>No, you won’t make “easy” money with game development. It will take a bit of time to learn how to make smooth, flawless mechanics. So if you are doing it for the money only, maybe you want to save yourself the frustration.</li>
<li>If you are doing it by passion, by no means try it out! you’ll have so much fun creating weird games. Let your imagination go wild, it only gets better from there!</li>
</ul>
<h2><strong>Competitive Programming</strong></h2>
<ul>
<li>Simply practice and resilience: Back in freshman year, I decided to dive into competitive programming. It sounded nerdy and cool. I finished the “cracking the coding interview” by Google. Which contains 150 programming interview questions. I noticed slight progress given the giant amount of work I had to endure for a full summer.</li>
<li>If you want to make money through this, you sure love making money the hard way, but it’s doable. Nevertheless, this is a great way to get into big techs if you rank top in programming contests.</li>
<li>You only remember as much as you practice. The hidden trump card about Competitive programming isn’t the difficulty of the problems but how prepared are you. You don’t have to remember how you solved a problem but where you missed. If you stop practicing, you’ll feel you lost that ‘cognitive prowess’ that helped you draw links between multiple parts of the question.</li>
<li>More often than not, the standard programming language is C++. Python is on the rise too.</li>
</ul>
<h2><strong>Mobile Development</strong></h2>
<ul>
<li>Android or iOS, it’s up to you to choose. I prefer coding for iOS (Swift) because it feels much cleaner. Back in Android (Java/Kotlin), I had to deal with preliminary problems like fixing “Gradle” after an update to just get the project going. Also, I felt that Android was a bit messy to code for in a native language. So I switched to iOS.</li>
<li>iOS is more restricted, you need to pay $100/year for the Apple Developer Program. Also, you need the membership if you want to include advanced features in your app such as Deep Links, notifications, background activity, etc…</li>
<li>Android on the other side, you pay $25 for a<span>&nbsp;</span><strong>lifetime.</strong><span>&nbsp;</span>You submit as many apps as you want, and no one is restricting you in any way. You have a wide audience of developers and potential users waiting for your app. But on the other side, you are competing with more people.</li>
<li>Why not both? Use a framework! I learned both Android &amp; iOS dev and now I’m switching completely to Flutter (more on this in the last paragraph). Using a framework helps with coding for multiple platforms using the same code base. You don’t want to do double the work, and maintaining just 1 app needs<span>&nbsp;</span><strong>many!<span>&nbsp;</span></strong>people. It’s not a matter of “competency” but resources. If I judge by my skills, I don’t need anyone technical on my team, but often than not, I need many people to help me out because there are many small details to take care of.</li>
<li>App developers have a favorable edge in the job market. Mobile users are on the rise. According to BankMyCell, 3.5B people are using their smartphone and it’s only been growing to date</li>
<li>Career-wise, you can work anywhere from the comfort of your home. Also, you ‘ll progress quite fast if you know what you’re doing. Salaries are on the rise too, and the benefits are staggering. Finally, you’re not limited to some predefined companies, you can work for startups, freelance, build your own project, and so on.</li>
<li>This area is getting more &amp; more competitive, and there is a slight switch that is happening, which is the move to software development kits (aka Flutter, React native, etc…). Instead of paying a full-stack mobile dev to write code for 1 platform only, you can have the same code base work for multiple platforms.</li>
</ul>
<h2><strong>Web Development</strong></h2>
<ul>
<li>Web dev is in some serious demand as well. Here you don’t have a predefined programming language because you can literally combine multiple ones, for example: Html + CSS + JS. I never wanted to learn web dev because of javascript. Spaghetti language (sorry!) Also the idea of learning multiple languages to do 1 thing never made sense to me. However, in your case, this isn’t a problem anymore. If you are passionate about creating websites or Web apps, you can go for frameworks such as React Js or Flutter (in Beta). And you’re sorted for life.</li>
<li>No, you don’t need to worry about the “no-code tools” and the “website builders”. These can only help as much. If you want to focus on the backend (servers, technical logic, business logic, etc…) you are in a much better position given that each use case is different. And for a no-code tool to handle that, it’s pretty unreasonable.</li>
<li>Web Development is a dimension in itself. I’m not a web developer and don’t want to snap off someone …</li></ul></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://devhypercharged.com/here-is-what-you-need-to-know-before-learning-how-to-code-bookmark-this-guide/">https://devhypercharged.com/here-is-what-you-need-to-know-before-learning-how-to-code-bookmark-this-guide/</a></em></p>]]>
            </description>
            <link>https://devhypercharged.com/here-is-what-you-need-to-know-before-learning-how-to-code-bookmark-this-guide/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23739809</guid>
            <pubDate>Sun, 05 Jul 2020 16:23:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[JIT Compilers Are Implemented and Fast: Julia, PyPy, LuaJIT, Graal and More]]>
            </title>
            <description>
<![CDATA[
Score 23 | Comments 4 (<a href="https://news.ycombinator.com/item?id=23739416">thread link</a>) | @kipply
<br/>
July 5, 2020 | https://carolchen.me/blog/jits-impls/ | <a href="https://web.archive.org/web/*/https://carolchen.me/blog/jits-impls/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
  <header>
    
    
  </header>

  <main id="main">
    
<article>
  <header>
    
    
  </header>
  <section id="js-article">
    
<p>This post goes into details of 5+ JITs and various optimization strategies and discuss how they work with different JITs. Information in this blog post is more <em>depth-first</em>, thus there are many important concepts that may be skipped.</p>
<p>For background on JIT compilers see <a href="https://carolchen.me/blog/jits-intro">A Deep Introduction to JIT Compilers: JITs are not very Just-in-time</a>. If the title does not make sense to you then it may be worth a skim. </p>
<blockquote>
<p><em>Mild Disclaimers, can be skipped</em>.</p>
</blockquote>
<blockquote>
<p>I will often describe an optimization behaviour and claim that it probably exists in some other compiler. Though I don't always check if an optimization exists in another JIT (it's sometimes ambiguous), I'll always state explicitly if I know it’s there. 
I will also provide code examples to show where an optimization might occur, however the optimization may not necessarily occur for that code because another optimization will take precedence. There may also be some general oversimplifications, but not more than I think exists in most posts like these. </p>
</blockquote>
<h2 id="table-of-contents-highlights">Table of Contents / Highlights<a href="#table-of-contents-highlights" aria-label="Anchor link for: table-of-contents-highlights"> <i></i></a>
</h2>
<ul>
<li><a href="https://carolchen.me/blog/jits-impls/#wait-but-you-said-meta-tracing">Meta-tracing in Pypy works</a></li>
<li><a href="https://carolchen.me/blog/jits-impls/#interpreting-c">How GraalVM languages support C extensions</a></li>
<li><a href="https://carolchen.me/blog/jits-impls/#go-back-to-the-interpreted-code-it-ll-be-faster">Deoptimisation</a></li>
<li><a href="https://carolchen.me/blog/jits-impls/#wet-code-is-fast-code-inlining-and-osr">Inlining and OSR</a></li>
<li><a href="https://carolchen.me/blog/jits-impls/#what-if-instead-of-instruction-based-ir-like-everyone-else-we-had-a-big-graph-and-also-it-modifies-itself">Seas of Nodes</a></li>
<li><a href="https://carolchen.me/blog/jits-impls/#yay-jit-compiled-code-let-s-compile-it-again-and-again">Tiering JITs</a></li>
</ul>

<p>LuaJIT employs a method called tracing. Pypy does meta-tracing, which involves using a system to generate tracing interpreters and JITs. Pypy and LuaJIT are not the reference implementations of Python or Lua, but a projects on their own. I would describe LuaJIT as shockingly fast, and it describes itself as one of the fastest dynamic language implementations -- which I buy fully.</p>
<p>To determine when to start tracing, the interpreting loop will look for "hot" loops to trace (the concept of "hot" code is universal to JITS!). Then, the compiler will "trace" the loop, recording executed operations to compile well optimized machine code. In LuaJIT, the compilation is performed on the traces with an instruction-like IR that is unique to LuaJIT. </p>
<h3 id="how-pypy-implements-tracing"><strong>How Pypy Implements Tracing</strong><a href="#how-pypy-implements-tracing" aria-label="Anchor link for: how-pypy-implements-tracing"> <i></i></a>
</h3>
<p>Pypy will start tracing a function after 1619 executions, and will compile it after another 1039 executions, meaning a function has to execute around 3000 times for it to start gaining speed. These constants were carefully tuned by the Pypy team (lots of constants are tuned for compilers in general!).</p>
<p>Dynamic languages make it hard to optimize things away. The following code could be statically eliminated by a stricter language, as <code>False</code> will always be falsy. However, in Python 2, that could not have been guaranteed before runtime.</p>
<pre><code><span>if </span><span>False</span><span>:
  </span><span>print</span><span>(</span><span>"FALSE"</span><span>)
</span></code></pre>
<p>For any sane program, the conditional will always be false. Unfortunately, the value of <code>False</code> could be reassigned and thus if the statement were in a loop, it could be redefined somewhere else. For this case, Pypy would build a "guard". When a guard fails, the JIT will fall back to the interpreting loop. Pypy then uses another constant (200), called <em>trace eagerness</em> to decide whether to compile the rest of the new path till the end of the loop. That sub-path is called a <em>bridge</em>.</p>
<p>Pypy also exposes all those constants as arguments that can be tweaked at execution, along with configuration for unrolling (expanding loops) and inlining! It also exposes some hooks so we can see when things are compiled. </p>
<pre><code><span>def </span><span>print_compiler_info</span><span>(</span><span>i</span><span>):
  </span><span>print</span><span>(i</span><span>.</span><span>type)
pypyjit</span><span>.</span><span>set_compile_hook</span><span>(print_compiler_info)

</span><span>for </span><span>i </span><span>in </span><span>range</span><span>(</span><span>10000</span><span>):
  </span><span>if </span><span>False</span><span>:
    </span><span>pass

</span><span>print</span><span>(pypyjit</span><span>.</span><span>get_stats_snapshot</span><span>()</span><span>.</span><span>counters)
</span></code></pre>
<p>Above, I set up a plain python program with a compile hook to print the type of compilation made. It also prints some data at the end, where I can see the number of guards. For the above I get one compilation of a loop and 66 guards. When I replaced the if statement with just a pass under the for-loop, I was left with 59 guards.</p>
<pre><code><span>for </span><span>i </span><span>in </span><span>range</span><span>(</span><span>10000</span><span>):
  </span><span>pass </span><span># removing the `if False` saved 7 guards!
</span></code></pre>
<p>With these two lines added to the for loop, I will get two compilations, with the new one being of type 'bridge'!</p>
<pre><code><span>if </span><span>random</span><span>.</span><span>randint</span><span>(</span><span>1</span><span>, </span><span>100</span><span>) </span><span>&lt; </span><span>20</span><span>:
  </span><span>False </span><span>= </span><span>True
</span></code></pre><h3 id="wait-but-you-said-meta-tracing">Wait, but you said Meta-tracing!<a href="#wait-but-you-said-meta-tracing" aria-label="Anchor link for: wait-but-you-said-meta-tracing"> <i></i></a>
</h3>
<p>The concept behind meta-tracing is “write an interpreter, get a compiler for free!” or more magically, “turn your interpreter into a JIT-compiler!”. This is just obviously a great thing, since writing compilers is hard so if we can get a great compiler for free that’s just a good deal. Pypy "has" an interpreter and a compiler, but there’s no explicit implementation of a traditional compiler.</p>
<p>Pypy has a toolchain called RPython (which was built for Pypy). It is a framework program for implementing interpreters. It is a language in that it specifies a subset of the Python language, namely to force things like static typing. It is a language to write an interpreter in. It is not a language to code in typed-Python, since it doesn’t care or have things like standard libraries or packages. Any RPython program is a valid Python program. RPython programs are transpiled to C and then compiled. Thus, the RPython meta-compiler exists as a compiled C program.</p>
<p>The “meta” in meta-tracing comes from the fact that the trace is on the execution of the interpreter rather than the execution of the program. The interpreter more or less behaves as any interpreter, with the added capability of tracing its own operations, and being engineered to optimize those traces by updating the path of the interpreter (itself). With further tracing, the path that the interpreter takes becomes more optimized. With a very optimized interpreter taking a specific, optimized path, the compiled machine code being used in that path from the compiled RPython can be used as the compilation. </p>
<p>In short, the “compiler” in Pypy is compiling your interpreter, which is why Pypy is sometimes referred to as a meta-compiler. The compiler is less for the program you're trying to execute, but rather for compiling the trace of the optimizing interpreter!</p>
<p>Metatracing might be confusing, so I wrote a very bad metatracing program that can only understand <code>a = 0</code> and <code>a++</code>to illustrate.</p>
<pre><code><span># interpreter written with RPython
</span><span>for </span><span>line </span><span>in </span><span>code:
  </span><span>if </span><span>line </span><span>== </span><span>"a = 0"</span><span>:
    </span><span>alloc</span><span>(a, </span><span>0</span><span>)
  </span><span>elif </span><span>line </span><span>== </span><span>"a++"</span><span>:
    </span><span>guard</span><span>(a, </span><span>"is_int"</span><span>) </span><span># notice how in Python, the type is unknown, but after being interpreted by RPython, the type is known
    </span><span>guard</span><span>(a, </span><span>"&gt; 0"</span><span>)
    </span><span>int_add</span><span>(a, </span><span>1</span><span>)
</span></code></pre>
<p>If I ran the following in a hot loop;</p>
<pre><code><span>a </span><span>= </span><span>0
</span><span>a</span><span>++
</span><span>a</span><span>++
</span></code></pre>
<p>Then the traces may look something like:</p>
<pre><code><span># Trace from numerous logs of the hot loop
</span><span>a </span><span>= </span><span>alloc</span><span>(</span><span>0</span><span>) </span><span># guards can go away
</span><span>a </span><span>= </span><span>int_add</span><span>(a, </span><span>1</span><span>)
a </span><span>= </span><span>int_add</span><span>(a, </span><span>2</span><span>)

</span><span># optimize trace to be compiled
</span><span>a </span><span>= </span><span>alloc</span><span>(</span><span>2</span><span>) </span><span># the section of code that executes this trace _is_ the compiled code
</span></code></pre>
<p>But the compiler isn't some special standalone unit, it's built into the interpreter! So the interpreter loop would actually look something like this</p>
<pre><code><span>for </span><span>line </span><span>in </span><span>code:
  </span><span>if </span><span>traces</span><span>.</span><span>is_compiled</span><span>(line):
    </span><span>run_compiled</span><span>(traces</span><span>.</span><span>compiled</span><span>(line))
    </span><span>continue
  elif </span><span>traces</span><span>.</span><span>is_optimized</span><span>(line):
    </span><span>compile</span><span>(traces</span><span>.</span><span>optimized</span><span>(line))
      </span><span>continue
  elif </span><span>line </span><span>== </span><span>"a = 0"
  </span><span># ....
</span></code></pre><h2 id="an-introduction-to-jvms">An Introduction to JVMs<a href="#an-introduction-to-jvms" aria-label="Anchor link for: an-introduction-to-jvms"> <i></i></a>
</h2>
<p>Disclaimer: I worked on/with a Graal-based language, <a href="https://github.com/oracle/truffleruby">TruffleRuby</a> for four months and loved it.</p>
<p>Hotspot (named after looking for <em>hot</em> spots) is the VM that ships with standard installations of Java, and there are actually multiple compilers in it for a tiered strategy. Hotspot is open source, with 250,000 lines of code which contains the compilers, and three garbage collectors. It does an <em>awesome</em> job at being a good JIT, there are some benchmarks that have Hotspot on par with C++ impls (oh my gosh so many asterisks on this, you can Google to find all the debate). Though Hotspot is not a tracing JIT, it employs a similar approach of having an interpreter, profiling and then compiling. There is not a specific name for what Hotspot does, though the closest categorization would probably be a Tiering JIT. </p>
<p>Strategies used in Hotspot inspired many of the subsequent JITs, the structure of language VMs and especially the development of Javascript engines. It also created a wave of JVM languages such as Scala, Kotlin, JRuby or Jython. JRuby and Jython are fun implementations of Ruby and Python that compile the source code down to the JVM bytecode and then have Hotspot execute it. These projects have been relatively successful at speeding up languages like Python and Ruby (Ruby more so than Python) without having to implement an entire toolchain like Pypy did. Hotspot is also unique in that it's a JIT for a less dynamic language (though it's technically it's a JIT for JVM bytecode and not Java). </p>
<p><img src="https://carolchen.me/blog/img/jits/vms.png" alt=""></p>
<p>GraalVM is a JavaVM and then some, written in Java. It can run any JVM language (Java, Scala, Kotlin, etc). It also supports a Native Image, to allow AOT compiled code through something called Substrate VM. Twitter runs a significant portion of their Scala services with Graal, so it must be pretty good, and better than the JVM in some ways despite being written in Java. </p>
<p>But wait, there's more! GraalVM also provides Truffle, a framework for implementing languages through building Abstract Syntax Tree (AST) interpreters. With Truffle, there’s no explicit step where JVM bytecode is created as with a conventional JVM language, rather Truffle will just use the interpreter and communicate with Graal to create machine code directly with profiling and a technique called partial evaluation. Partial evaluation is out of scope for this blog post, tl;dr it follows metatracing’s “write an interpreter, get a compiler for free” philosophy but is approached differently.</p>
<blockquote>
<p>TruffleJS, the Truffle implementation of Javascript outperforms the JavaScript V8 engine on select benchmarks which is really impressive since V8 has had numerous more years of development, Google money+resources poured in and some crazy skilled people working on it. TruffleJS is still by no means “better” than V8 (or other JS engines) on most measures but it is a sign of promise for Graal. </p>
</blockquote>

<h3 id="interpreting-c">Interpreting C<a href="#interpreting-c" aria-label="Anchor link for: interpreting-c"> <i></i></a>
</h3>
<p>A common problem with JIT implementations is support for C Extensions. Standard interpreters such …</p></section></article></main></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://carolchen.me/blog/jits-impls/">https://carolchen.me/blog/jits-impls/</a></em></p>]]>
            </description>
            <link>https://carolchen.me/blog/jits-impls/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23739416</guid>
            <pubDate>Sun, 05 Jul 2020 15:31:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Andrew Wilkinson and Tiny Capital]]>
            </title>
            <description>
<![CDATA[
Score 77 | Comments 16 (<a href="https://news.ycombinator.com/item?id=23739381">thread link</a>) | @colinkeeley
<br/>
July 5, 2020 | https://colinkeeley.com/blog/andrew-wilkinson-tiny-capital-operating-manual | <a href="https://web.archive.org/web/*/https://colinkeeley.com/blog/andrew-wilkinson-tiny-capital-operating-manual">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-block-type="2" id="block-db1d94e86a488296a48d"><div><blockquote><p><em>“Let someone else run the marathon and incentivize them.”&nbsp;</em></p><p><em>-Andrew Wilkinson</em></p></blockquote><p><strong>What is Tiny?</strong></p><p><a href="http://tinycapital.com/">Tiny</a>&nbsp;is a long term holding company for internet businesses started by&nbsp;<a href="https://twitter.com/awilkinson">Andrew Wilkinson</a>&nbsp;and&nbsp;<a href="https://twitter.com/_sparling_?lang=en">Chris Sparling</a>. They take majority, generally whole, stakes in "profitable, simple, and often boring” internet businesses.&nbsp;</p><p><strong>Why are holding companies and micro private equity interesting?&nbsp;</strong></p><p>I suspect this is the most dependable way to become very wealthy. It isn’t as glamorous or as quick (potentially) as founding or investing in the next multi-billion dollar startup. This is a longer-term grind it out approach.&nbsp;</p><p>Starting companies is fun, but anyone who has done it knows it is a lot of work. Buying established businesses with existing cash flow isn’t as sexy so I suspect it is wildly underrated as a way of building wealth.&nbsp;</p><p>The reality is that it is easier to buy and improve businesses than to start them. It is easier to go from 3 to 10 than from 0 to 1. Even for the folks that have done it before.&nbsp;</p><p>There isn’t much info on how holding companies or micro-PEs like Tiny actually operate. I’ve listened to every podcast Andrew has been on and compiled these notes from them.&nbsp;</p><p>Here is what they are doing behind the scenes.</p><p><strong>How Andrew got started? Where the capital comes from?</strong></p><p>In 2006, Andrew founded&nbsp;<a href="http://metalab.co/">MetaLab</a>, a Victoria, Canada-based design agency shortly after high school. After rapid growth, he used the profits to diversify into a variety of businesses, which today form Tiny, a holding company he owns fully with his business partner Chris Sparling.&nbsp;</p><p>Agencies traditionally aren’t very profitable, but MetaLab is able to charge San Francisco agency rates and only pay Victoria, Canada wages.&nbsp;</p><p>Tiny shifted its focus from starting businesses to buying them in 2013 when MetaLab and all their other businesses combined were doing $7M/year in profit. Tiny is fully self-funded today.</p><p><strong>What’s the scale of Tiny now?</strong></p><p>Comfortably not tiny. It sounds like somewhere around $80-95M revenue per year (double-digit millions is what Andrew says) with highly profitable businesses. They have around 350-400 employees across 20ish companies.&nbsp;</p><p><strong>What Tiny looks for in businesses to buy?</strong></p><p>From their site:</p><blockquote><p><em>3-5+ years of operating history</em></p><p><em>Profits. A minimum $500k/year in annual profit, as high as $15MM.</em></p><p><em>A high-quality team in place. This is negotiable if the business is simple to operate and the team wants to leave.</em></p><p><em>We are open to owners sticking around, leaving cold turkey, or transitioning out over time. We'll work with you to transition.</em></p><p><em>Simple internet businesses that have high margins, don't require tons of people or complex technology, and have a competitive advantage that protects them from competitors. For example: A dominant brand, a large and loyal community, a niche vertical, or something similar.</em></p></blockquote><p>Andrew describes these businesses as "New Zealand companies.”</p><p>What is a New Zealand company?</p><ul data-rte-list="default"><li><p>It is in the middle of nowhere, nobody is paying attention to it, but it is quietly growing. It is not at risk of nuclear war.&nbsp;</p></li><li><p>It is self-sufficient and thriving. It’s food &amp; energy independent. A "safe" business isn't beholden to benevolent gatekeepers like Google or Facebook to reach their customer.&nbsp;</p></li></ul><p>Andrew is always worried about staying power.&nbsp;</p><p>An example of one of his New Zealand business is Dribbble:</p><ul data-rte-list="default"><li><p>Top 1,000 site on the internet&nbsp;</p></li><li><p>A huge community of designers</p></li><li><p>Profitable</p></li><li><p>Few competitors. Big companies are not trying to kill it or compete.&nbsp;</p></li><li><p>Not dependent on Facebook or Google for traffic. People type Dribbble.com into the address bar to visit.&nbsp;</p></li></ul><p><strong>Types of businesses Tiny has bought/started?</strong></p><p>I don’t know if this is by design, but it seems like Andrew has progressed from services to tools/products to platforms/communities to digital marketplaces.&nbsp;</p><ul data-rte-list="default"><li><p>Agencies: MetaLab (design agency), Double Up (podcast growth agency), 8020 (no-code agency)</p></li><li><p>SaaS tools:&nbsp;<a href="https://www.getflow.com/">Flow</a>&nbsp;(product management), Castro (podcast player), Supercast (podcast subscriptions)</p></li><li><p>Products: Caramba</p></li><li><p>Communities: Dribbble&nbsp;</p></li><li><p>Media: Designer News, RideHome (podcast network)</p></li><li><p>Job Boards:&nbsp;<a href="https://weworkremotely.com/">We Work Remotely</a></p></li><li><p>Digital goods marketplaces: Creative Market, Pixel Union</p></li></ul><p><strong>How Tiny companies operate?</strong></p><p>Tiny companies have fewer information responsibilities than typical PE-owned companies. There are no formal board meetings for example.&nbsp;</p><p>Once a month companies send Tiny a finance-only update with the P&amp;L, balance sheet, and KPIs. No operational info is included.&nbsp;</p><p>Once a quarter companies send Tiny a SWOT (strengths, weaknesses, opportunities, and threats) analysis.&nbsp;</p><p>Companies contact Tiny ASAP for emergencies, major news, or decisions.&nbsp;</p><p>Some CEOs will go 6 months or more without speaking with Andrew.&nbsp;</p><p><strong>How Tiny launches new businesses?</strong></p><p>Tiny’s primary business is buying majority stakes in businesses, not starting them. For a while Andrew would start a new business in any niche he was interested in. He tries to avoid that now and thinks it’s a lot better to buy something that is already good.</p><p>When Andrew does start a new business now, he delegates almost all aspects of it. He recently said he only spent something like 4 hours on each of the new businesses he has launched.&nbsp;</p><p>Andrew will pay for all the work to be done and the investment will form his stake in the business. He will find a CEO to run the business and pay the new CEOs a month or two of salaries to get things going. Then he’ll help with intros, but otherwise, he’ll be hands-off. All in he said it takes $10-50k to get off the ground with a great operator.</p><p><strong>Why do Founders sell to Tiny?</strong></p><p>Tiny is positioned as the good guys of private equity. The Berkshire Hathway of internet businesses.</p><p>They have become known for doing simple acquisitions. Andrew didn’t like the traditional acquisition process: long due diligence, and renegotiation of terms. Warren Buffet does deals in seven days and those are larger, more complex businesses. Smaller deals should be even quicker.</p><p>A challenge with this model is that it is difficult to acquire tech companies at reasonable prices. Acquiring boring traditional businesses is easier because the valuations are so much lower than tech companies. To successfully use this approach you need discipline around what you’re willing to pay for a business and a reputation for being easy to work with. Andrew gets deals by being a nice guy and offering a good home for businesses to live on. Contrast this with the typical PE approach of dramatically cutting costs (ie firing everyone) and squeezing as much profit out as possible. Some founders are looking more for freedom and an easy process than maximizing their financial outcome. </p><p>These smaller PE opportunities are underserved relative to the typical VC businesses. The lifestyle businesses that VC shuns are Andrew’s ideal companies. He is fishing in a less crowded pond. </p><p>Andrew will occasionally pay 10x for an amazing business, but that is rare.&nbsp;</p><p><strong>What happens to businesses after the sale?&nbsp;</strong></p><p>For the employees, it is business as usual for the most part. The goal is for the employees to not even notice.&nbsp; </p><p>The biggest difference is that Tiny becomes the bank. Cash is kept in the company based on historical working capital needs and any extra goes to the head office for new acquisitions.&nbsp; </p><p>Often Tiny buys product or designer-led startups that have grown organically. They will put standard best-practice marketing and sales processes in place and sometimes raise prices. Each company has its own CEO with a few exceptions like all job boards (5+) are under one CEO.&nbsp; </p><p>Tiny has a preference for remote companies where they can hire more affordably. Andrew estimates the cost of running a business in Canada can be 60-65% the cost of in California. Struggling American companies with inflated cost structures can reduce costs by moving to Canada. Canadian arbitrage includes lower salaries, not needing to pay medical benefits, SRED, and cheaper currency.</p><p><strong>Who runs the business after a sale?&nbsp;</strong></p><p>Often Andrew is buying from bootstrapped founders that have been at it for 5-10 years and want to move on.</p><p>Finding great people to run these companies is one of the hardest aspects of this model.&nbsp;</p><p>Andrew deals with this by paying up and hiring CEOs that have managed similar businesses at larger scales already before instead of trying to find underpriced less-experienced talent.&nbsp;</p><p>Months before closing on a deal Andrew works to identify opportunities for the business and a new leader to come in.&nbsp;</p><p>He finds these new CEOs through his existing CEOs by asking “we’re about to buy a business who’s the smartest person you know in the space."</p><p><strong>What does the operating company and Andrew do day-to-day?</strong></p><blockquote><p><em>“Entrepreneurship is just delegation”&nbsp;</em></p><p><em>-Andrew Wilkinson</em></p></blockquote><p>Andrew spends time looking for new deals and looking at their existing portfolio and thinking "how they could get fucked”.&nbsp;</p><p>Andrew says his strengths are:</p><ul data-rte-list="default"><li><p>Laser focused on problems for a short period of time. Moves fast.&nbsp;</p></li><li><p>Very good at 0 to 1. Burns bright for 15 days.&nbsp;</p></li><li><p>Inch deep and a mile wide</p></li><li><p>Not good at execution or day to day details</p></li></ul><p>Being comfortable with delegation is key to this model. Andrew is the owner, not the CEO. The owner can’t constantly be delegating what can or can’t be done or the CEO grows resentful. Some comfort with decisions being made that you don’t agree with comes with the territory. Large decisions that require more capital than usual are a discussion.&nbsp;</p><p><strong>How connected are businesses in the holding company?</strong></p><p>Tiny companies are not at all connected. They each operate independently.&nbsp;</p><p>CEOs will take calls and give advice on best practices, but nothing beyond small favors. Real work gets paid for. Tiny pays all companies for the work they do for the holding company and all work between companies is paid at the full rate.&nbsp;</p><p>Synergies are appealing, but they generally just make the CEOs resentful so they are avoided entirely.&nbsp;</p><p><strong>How much debt do they use?</strong></p><p>Tiny uses little debt for acquisitions (less than Berkshire Hathaway) and they like to pay off debt within 6 months. Debt comes from&nbsp;<a href="https://en.wikipedia.org/wiki/Business_Development_Bank_of_Canada"><strong>BDB of Canada</strong></a>, or traditional banks.</p><p><em>If you know of anything I should add to this please reach …</em></p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://colinkeeley.com/blog/andrew-wilkinson-tiny-capital-operating-manual">https://colinkeeley.com/blog/andrew-wilkinson-tiny-capital-operating-manual</a></em></p>]]>
            </description>
            <link>https://colinkeeley.com/blog/andrew-wilkinson-tiny-capital-operating-manual</link>
            <guid isPermaLink="false">hacker-news-small-sites-23739381</guid>
            <pubDate>Sun, 05 Jul 2020 15:26:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The most logical explanation is that it comes from a laboratory]]>
            </title>
            <description>
<![CDATA[
Score 24 | Comments 18 (<a href="https://news.ycombinator.com/item?id=23738545">thread link</a>) | @markdog12
<br/>
July 5, 2020 | https://www.minervanett.no/corona/the-most-logical-explanation-is-that-it-comes-from-a-laboratory/361860 | <a href="https://web.archive.org/web/*/https://www.minervanett.no/corona/the-most-logical-explanation-is-that-it-comes-from-a-laboratory/361860">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>

            <section>

                
                <div>

    

            <h2>NYHET</h2>



        

        

            <h3 itemprop="description">
            The well-known Norwegian virologist Birger Sørensen and his colleagues have examined the corona virus. They believe it has certain properties which would not evolve naturally. These conclusions are politically controversial, but in this interview he shares the findings behind the headlines.
            </h3>

    

</div>


                <div><p>“I understand that this is controversial, but the public has a legitimate need to know, and it is important that it is possible to freely discuss alternate hypotheses on how the virus originated” Birger Sørensen starts to explain when Minerva visits him in his office one morning in Oslo.</p><p>Despite the explosiveness of his statements and research, Sørensen remains calm and collected.</p><p>Sørensen has been a point of controversy ever since former MI6 director Richard Dearlove cited a yet to be published article by Sørensen and his colleagues in an interview with The Daily Telegraph. The article claims that the virus that causes Covid-19 most likely has not emerged naturally.</p><p>“It’s a shame that there has already been so much talk about this, because I have yet to publish the article where I put forward my analysis”, Sørensen says in the form of an exasperated sigh.</p><p>Together with his colleagues, Angus Dalgleish and Andres Susrud have authored an article that looks into the most plausible explanations regarding the origins of the novel coronavirus. The article builds upon an already published article in the Quarterly Review of Biophysics that describes newly discovered properties in the virus spike protein. The authors are still in dialogue with scientific journals regarding an upcoming publication of the article.</p><p>News outlets are thus confronted with a difficult question: Are the findings and arguments Sørensen and his colleagues put forward of a sufficiently high quality to be presented and discussed in the public sphere? Sørensen explains that they in their dialogue with scientific journals are encountering a certain reluctance to publishing the article – without, however, proper scientific objections. Minerva has read a draft of the article, and has after an overall assessment decided that the findings and arguments do deserve public debate, and that this discussion cannot depend entirely on the publication process of scientific journals.</p><p>In this interview with Minerva, Sørensen therefore puts forward his hypothesis on why it is highly unlikely that the coronavirus emerged naturally.</p><p>On May 18th, WHO decided to conduct an inquiry into the coronavirus epidemic in China. Sørensen believes that it is important that this inquiry looks into new and alternate explanations for how the virus originated, beyond the already well-known suggestion that the virus originated in the Wuhan Seafood Market.</p><p>“There are very few who still believe that the epidemic started there, so as of today we have no good answers on how the epidemic started. Then we must also dare to look at more controversial, alternative explanations for the origin,” Sørensen says.</p><p>Birger Sørensen and one of his co-authors, Angus Dalgleish, are already known as HIV researchers par excellence.</p><p>In 2008, Sørensen’s work came to international <a href="https://www.dagensperspektiv.no/2008/norsk-firma-med-hiv-gjennombrudd">attention</a> when he launched a new immunotherapy for HIV. <a href="https://www.nature.com/search?author=%22Angus%20G.+Dalgleish%22">Angus Dalgleish</a> is the professor at St. George’s Medical School in London who became world famous in 1984 after having <a href="https://www.nature.com/articles/312763a0">discovered</a> a novel receptor that the HIV virus uses to enter human cells.</p><p>The purpose of the work Sørensen and his colleagues have done on the novel coronavirus, has been to produce a vaccine. And they have taken their experience in trialling HIV vaccines with them to analyse the coronavirus more thoroughly, in order to make a vaccine that can protect against Covid-19 without major side effects.</p><h2>Exceptionally well adjusted</h2><p>“The difference between our approach and other vaccine manufacturers is that we have a chemistry background, and we analyse the virus in detail as if we were making a drug,” Sørensen starts to explain.</p><p>“Biology is also chemistry, so by considering the virus from a chemistry perspective, we carry out more detailed analysis, zooming in on certain components.”</p><p>Sørensen takes us through the basic elements of their approach:</p><p>“The first thing you need to establish is which parts of the virus are changing, and which parts are stable. If you want to make a vaccine that lasts, you must stimulate the immune system to react against those parts of the virus that are constant, otherwise the effect will disappear and, in the worst-case scenario, lead to increased illness.</p><p>“Once we know this, we can try to make a vaccine. Where we differ is that we are trying to make a vaccine that uses elements that have as little in common with the body’s natural components as possible, so that the immune system is taught to recognise exactly what the vaccine should protect against”, Sørensen elaborates.</p><p>Sørensen believes this is an important insight which will prevent the immune system from being falsely stimulated in a way that could lead the vaccine to create too many dangerous side effects in the vaccinated person.</p><p>“When we have not succeeded in creating an HIV vaccine, despite the enormous efforts put into that endeavour for the past 30 years, it is because we haven’t understood this,” Sørensen continues.</p><p>He believes that there has not been enough interaction between the part of the pharmaceutical industry that makes HIV medicines and the part that runs the vaccine research. As a consequence, the knowledge you need to make a successful vaccine against HIV in the big pharmaceutical companies has not been adequately exploited by the big, international HIV preventing vaccine studies that have been carried out.”</p><p>Asked about what significance his approached has had when he has analyzed the coronavirus, Sørensen explains:</p><p>“We have examined which components of the virus are especially well suited to attach themselves to cells in humans. And we have done this by comparing the properties of the virus with human genetics. What we found was that this virus was exceptionally well adjusted to infect humans.”</p><p>He pauses for a second.</p><p>“So well that it was suspicious,” he adds.</p><h2>Perfected to infect humans</h2><p>It is already known that the novel coronavirus, like the virus that caused the SARS epidemic in Southeast Asia in 2002-2003, could attach itself to the ACE-2 receptors in the lower respiratory tract.</p><p>“But what we have discovered is that there are properties in this new virus which enables it to use an additional receptor, and create a binding to human cells in the upper respiratory tract and the intestines which is strong enough to produce an infection,” Sørensen elaborates.</p><p>Sørensen says that it is the use of this additional receptor that most likely results in a different illness in Covid-19 patients than the one resulting from SARS.</p><p>“This is what enables the virus to transmit to a greater degree between humans, without the virus having attached itself to the ACE-2 receptors in the lower respiratory tract, where it causes deep pneumonia.</p><p>“That is also why so many of the Covid-19 patients have mild symptoms at the start of the illness, and are contagious before they develop severe symptoms,” he adds.</p><p>It might also explain why some people are ‘super spreaders’ without being ill themselves, Sørensen says.</p><p>In the already published article Sørensen and his colleagues Angus Dalgleish and Andres Susrud describe what they claim is curious about the spike protein of the coronavirus, which makes it especially well suited to infect humans. These findings are the foundation for the hypothesis Sørensen and his colleagues develop in the new article, where they claim that the virus is not natural in origin.</p><div id="factbox-361864">
    <div>
        
        <h2>FACT BOX – Spike Protein</h2>
        <p>A spike protein is a part of the virus attached to the surface of the virus. The spike protein is used by the virus when it enters cells, enabling it to stick in humans. The properties of the spike determines which receptors a virus can utilise and thus which cells the virus can enter to create illness.</p>
        
    </div>
    
</div><p>“There are several factors that point towards this,” says Sørensen. “Firstly, this part of the virus is very stable; it mutates very little. That points to this virus as a fully developed, almost perfected virus for infecting humans.</p><p>“Secondly, this indicates that the structure of the virus cannot have evolved naturally. When we compare the novel coronavirus with the one that caused SARS, we see that there are altogether six inserts in this virus that stand out compared to other known SARS viruses,” he goes on explaining.</p><p>Sørensen says that several of these changes in the virus are unique, and that they do not exist in other known SARS coronaviruses.</p><p>“Four of these six changes have the property that they are suited to infect humans. This kind of aggregation of a type of property can be done simply in a laboratory, and helps to substantiate such an origin,” Sørensen points out.</p><h2>An artificially created virus</h2><p>Asked about whether this implies that the virus is not natural, Sørensen goes on to explain the laboratory process that leads to the creation of new viruses.</p><p>“In a sense it is natural. But the natural processes have most likely been accelerated in a laboratory,” he explains. “It’s also possible for a virus to attain these properties in nature, but it’s not likely. If the mutations had happened in nature, we would have most likely seen that the virus had attracted other properties through mutations, not just properties that help the virus to attach itself to human cells.”</p><p>Sørensen vividly explains this argument:</p><p>“Imagine that you have cultivated a billion coronaviruses you have gathered from nature, then you take this mass of viruses and inject them into a human cell culture from for example the upper respiratory tract. As a result, a few of these viruses will change in order to better attach themselves to …</p></div></section></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.minervanett.no/corona/the-most-logical-explanation-is-that-it-comes-from-a-laboratory/361860">https://www.minervanett.no/corona/the-most-logical-explanation-is-that-it-comes-from-a-laboratory/361860</a></em></p>]]>
            </description>
            <link>https://www.minervanett.no/corona/the-most-logical-explanation-is-that-it-comes-from-a-laboratory/361860</link>
            <guid isPermaLink="false">hacker-news-small-sites-23738545</guid>
            <pubDate>Sun, 05 Jul 2020 13:38:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[New revamped version of the AnyMeal recipe management software]]>
            </title>
            <description>
<![CDATA[
Score 43 | Comments 29 (<a href="https://news.ycombinator.com/item?id=23738543">thread link</a>) | @wedesoft
<br/>
July 5, 2020 | https://www.wedesoft.de/software/2020/06/30/anymeal/ | <a href="https://web.archive.org/web/*/https://www.wedesoft.de/software/2020/06/30/anymeal/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://www.wedesoft.de/software/2020/06/30/anymeal/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23738543</guid>
            <pubDate>Sun, 05 Jul 2020 13:37:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Rust on the ESP32 (2019)]]>
            </title>
            <description>
<![CDATA[
Score 128 | Comments 42 (<a href="https://news.ycombinator.com/item?id=23737451">thread link</a>) | @lnyan
<br/>
July 5, 2020 | https://mabez.dev/blog/posts/esp32-rust/ | <a href="https://web.archive.org/web/*/https://mabez.dev/blog/posts/esp32-rust/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
	  <p>About six months ago, I made a <a href="https://www.reddit.com/r/rust/comments/ar2d3r/espressif_have_finally_released_an_llvm_fork_this/">post on reddit</a> highlighting the launch of Espressif's llvm xtensa fork, not too long after, I had a working <code>rustc</code> toolchain capable of generating xtensa assembly. At this point I had to put this project to the side to finish my final year of university. Funnily enough I didn't stray too far, my final year project used Rust to create a <a href="https://github.com/MWatch">'smartwatch'</a> (I may write about this in the future, if anyone is interested). </p>
<p>Since then I have seen a few posts utilising my fork to run Rust on the <a href="https://www.espressif.com/en/products/hardware/esp32/overview">ESP32</a> (<a href="https://dentrassi.de/2019/06/16/rust-on-the-esp-and-how-to-get-started/">see this great write up</a> by ctron, if you haven't already), most of which are building on top of <a href="https://github.com/espressif/esp-idf">esp-idf</a> which is written in C. In this post I'll be discussing the steps I took to generate valid binaries for the xtensa architecture with <code>rustc</code> and then write some <code>no_std</code> code to build a blinky program for the ESP32 only using Rust!</p>
<h2 id="hacking-the-compiler">Hacking the compiler</h2>
<p>In March of 2019, Espressif released their first run at an <a href="https://github.com/espressif/llvm-xtensa">llvm fork</a> to support the xtensa architecure. Shortly after I got to work bootstrapping Rust to use this newly created fork. Prior to this project, I'd had no experience with the compiler, fortunately I came across the <a href="https://github.com/rust-lang/rust/pull/52787">RISCV PR</a> which gave me a rough idea of what was required. After <em>many</em> build attempts I finally got it working; I was now able to generate xtensa assembly from Rust source code!</p>
<p>The next step was to assemble and link the generated assembly. The llvm fork in it's current state cannot perform object generation, so we must use an external assembler. Luckily Rust allows us to do so by specifying the <code>linker_flavor</code> as <code>gcc</code> and providing a path to the linker with the <code>linker</code> target option, in this case <code>xtensa-esp32-elf-gcc</code>. After that I created a few built-in targets (which you can see <a href="https://github.com/MabezDev/rust-xtensa/blob/ad570c5cb999f62a03156286fdb5d3d1bbd0fb8b/src/librustc_target/spec/xtensa_esp32_none_elf.rs">here</a>); <code>xtensa-esp32-none-elf</code> for the ESP32; <code>xtensa-esp8266-none-elf</code> for the ESP8266; finally the <code>xtensa-unknown-none-elf</code> target for a generic xtensa target.</p>
<h2 id="blinky-code">Blinky code</h2>
<p>Now lets try and get a ESP32 board to blink the onboard LED using just Rust. First off, we need our basic program structure. The <code>xtensa_lx6_rt</code> crate does most of the heavy lifting in this respect, we simply need to define an entry point and the panic handler. Some of this may look vaguely familiar if you have any experience with <code>cortex-m</code> development on Rust, I've tried to mirror the API as best as I can.</p>
<pre><span>#![</span><span>no_std</span><span>]
#![</span><span>no_main</span><span>]


</span><span>use</span><span> xtensa_lx6_rt as _;

</span><span>use </span><span>core::panic::PanicInfo;

</span><span>/// Entry point - called by xtensa_lx6_rt after initialisation
</span><span>#[</span><span>no_mangle</span><span>]
</span><span>fn </span><span>main</span><span>() -&gt; ! {
    </span><span>loop </span><span>{}
}

</span><span>/// Simple panic handler
</span><span>#[</span><span>panic_handler</span><span>]
</span><span>fn </span><span>panic</span><span>(</span><span>_info</span><span>: &amp;PanicInfo) -&gt; ! {
    </span><span>loop </span><span>{}
}
</span></pre>
<p>Now lets add some register definitions for the peripherals we want to use. For our blinky program, we will need to control the GPIO peripheral. In the ESP32 (and most modern processors) peripherals are mapped to memory adresses, commonly refered to as memory mapped peripherals. To control a peripheral we simply need to write values to the right addresses in memory, with respect to the reference manual supplied by the chip manufacturer.</p>
<pre><span>/// GPIO output enable reg
</span><span>const </span><span>GPIO_ENABLE_W1TS_REG</span><span>: </span><span>u32 </span><span>= </span><span>0x3FF44024</span><span>;

</span><span>/// GPIO output set register
</span><span>const </span><span>GPIO_OUT_W1TS_REG</span><span>: </span><span>u32 </span><span>= </span><span>0x3FF44008</span><span>;
</span><span>/// GPIO output clear register
</span><span>const </span><span>GPIO_OUT_W1TC_REG </span><span>: </span><span>u32 </span><span>= </span><span>0x3FF4400C</span><span>;

</span><span>/// The GPIO hooked up to the onboard LED
</span><span>const </span><span>BLINKY_GPIO</span><span>: </span><span>u32 </span><span>= </span><span>2</span><span>;

</span><span>/// GPIO function mode
</span><span>const </span><span>GPIO_FUNCX_OUT_BASE</span><span>: </span><span>u32 </span><span>= </span><span>0x3FF44530</span><span>;
</span><span>const </span><span>GPIO_FUNCX_OUT_SEL_CFG</span><span>: </span><span>u32 </span><span>= </span><span>GPIO_FUNCX_OUT_BASE </span><span>+ (</span><span>BLINKY_GPIO </span><span>* </span><span>4</span><span>);
</span></pre>
<p>Using these definitions it should be possible to change the gpio for your board<sup><a href="#gpio_pin">1</a></sup> by changing the <code>BLINKY_GPIO</code>; for my board (NODEMCU ESP-32S) it was GPIO2.</p>
<h3 id="initialisation">Initialisation</h3>
<p>Next lets setup the pin as a GPIO output. For the ESP32, this is a two step process<sup><a href="#gpio_pin">1</a></sup>. Firstly, its simply a case of setting a bit in the GPIO ouput enable register. Secondly the pin has to be configured in GPIO mode. There are not enough pins for all the possible peripherals in the chip, to combat this each pin can have multiple function modes. In the case of the ESP32, each pin has up to 256 different functions, although not all are mapped. To put the pin in GPIO mode, we need to put in mode 256 (0x100), we do this by writing to the function select register. After issuing those two register writes, we should be able to turn on the GPIO by setting the relevant bit inside the GPIO set register<sup><a href="#2">2</a></sup>.</p>
<pre><span>#[</span><span>no_mangle</span><span>]
</span><span>fn </span><span>main</span><span>() -&gt; ! {

    </span><span>// configure the pin as an output
    </span><span>unsafe </span><span>{
        core::ptr::write_volatile(</span><span>GPIO_ENABLE_W1TS_REG </span><span>as </span><span>*mut </span><span>_, </span><span>0x1 </span><span>&lt;&lt; </span><span>BLINKY_GPIO</span><span>);
        </span><span>// 0x100 makes this pin a simple gpio pin - see the technical reference for more info
        </span><span>core::ptr::write_volatile(</span><span>GPIO_FUNCX_OUT_SEL_CFG </span><span>as </span><span>*mut </span><span>_, </span><span>0x100</span><span>); 
    }
    </span><span>// turn on the LED
    </span><span>unsafe </span><span>{
        core::ptr::write_volatile(</span><span>GPIO_OUT_W1TS_REG </span><span>as </span><span>*mut </span><span>_, </span><span>0x1 </span><span>&lt;&lt; idx);           
    }
    </span><span>loop </span><span>{}
}
</span></pre><h3 id="delaying">Delaying</h3>
<p>For the next stage of our blinky program, we need a way to delay; a simple approach could use <code>for</code> loop like so.</p>
<pre><span>pub fn </span><span>delay</span><span>(</span><span>clocks</span><span>: </span><span>u32</span><span>) {
    </span><span>let</span><span> dummy_var: </span><span>u32 </span><span>= </span><span>0</span><span>;
    </span><span>for </span><span>_ in </span><span>0</span><span>..clocks {
        </span><span>unsafe </span><span>{ core::ptr::read_volatile(&amp;dummy_var) };
    }
}
</span></pre>
<p>We add the volatile read so that the compiler doesn't optimise our delay away. The problem with this approach is that depending of the optimisation level, the number of clock cycles each iteration of the loop changes. We need a cycle accurate way of delaying, fortunately the ESP32 has an internal clock counting register which can be accessed with the read special register <code>rsr</code> instruction. Now are delay function looks like this.</p>
<pre><span>/// cycle accurate delay using the cycle counter register
</span><span>pub fn </span><span>delay</span><span>(</span><span>clocks</span><span>: </span><span>u32</span><span>) {
    </span><span>// NOTE: does not account for rollover
    // ommitted: the asm to read the ccount
    </span><span>let</span><span> target = </span><span>get_ccount</span><span>() + clocks;
    </span><span>loop </span><span>{
        </span><span>if </span><span>get_ccount</span><span>() &gt; target {
            </span><span>break</span><span>;
        }
    }
}
</span></pre>
<p>Now we have cycle accurate counting we can delay for one second by waiting for the number of cycles the processor will do in one second. The default clock speed on most ESP boards is 40mhz, hence waiting for 40 million cycles equates to a one second delay.</p>
<p>Bringing the snippets together and cleaning up the code into functions, we now have <code>main</code> that looks like this.</p>
<pre><span>#[</span><span>no_mangle</span><span>]
</span><span>fn </span><span>main</span><span>() -&gt; ! {
    </span><span>// configure the pin as an output
    </span><span>configure_pin_as_output</span><span>(</span><span>BLINKY_GPIO</span><span>);

    </span><span>loop </span><span>{
        </span><span>set_led</span><span>(</span><span>BLINKY_GPIO</span><span>, </span><span>true</span><span>);
        </span><span>delay</span><span>(</span><span>CORE_HZ</span><span>);
        </span><span>set_led</span><span>(</span><span>BLINKY_GPIO</span><span>, </span><span>false</span><span>);
        </span><span>delay</span><span>(</span><span>CORE_HZ</span><span>);
    }
}
</span></pre>
<p>After flashing to the board, and firing up our JTAG debugger<sup><a href="#1">3</a></sup>, we are greeted with a blinking LED!</p>

<p>The full source can be found in the <a href="https://github.com/MabezDev/xtensa-rust-quickstart">the xtensa quickstart repo</a> if you wish to try it for yourself.</p>
<p>Now I know what most of you are thinking at this point, it's not very Rusty; it contains bundles of unsafe and there are no real abstractions here, and you are right; but it's something to get the ball rolling.</p>
<h2 id="limitations">Limitations</h2>
<p>There are a few small teething issues, but by far the biggest being issue is that the fork struggles with generating debug info; the external assembler does not support <a href="https://sourceware.org/binutils/docs-2.24/as/CFI-directives.html#CFI-directives">CFI directives</a> something that all llvm targets need to support. CFI directives can easily be removed with some preprocessing, but does of course add an extra step. After pushing past that issue, I was still getting relocation linker errors. I opened <a href="https://github.com/espressif/llvm-xtensa/issues/10">an issue</a> to document my findings in the hopes it can be sorted in the next iteration of the llvm fork.</p>
<h2 id="future-work">Future work</h2>
<p>Once the debuginfo issue is sorted, I hope to start developing an ecosystem of HAL's and drivers similar to the <a href="https://github.com/stm32-rs">stm32-rs</a> and <a href="https://github.com/nrf-rs">nrf-rs</a>; I've already started the <a href="https://github.com/esp-rs">esp-rs</a> organization which is where <code>xtensa-lx6-rt</code> currently resides. Espressif has started the upstream process, the first ten patches are now in review, there should be an update coming to their fork moving from the older llvm6 to llvm8 (and hopefully some other additions and fixes too!).</p>
<h2 id="links">Links</h2>
<ul>
<li><a href="https://github.com/MabezDev/xtensa-rust-quickstart">xtensa-quickstart</a> - A quickstart project for using Rust on xtensa</li>
<li><a href="https://github.com/MabezDev/rust-xtensa">rust-xtensa</a> - The xtensa fork of Rust</li>
<li><a href="https://github.com/MabezDev">github</a> - My github</li>
</ul>
<br>
<hr>
<br>




	</div></div>]]>
            </description>
            <link>https://mabez.dev/blog/posts/esp32-rust/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23737451</guid>
            <pubDate>Sun, 05 Jul 2020 09:00:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Beginner's Guide to Abstraction]]>
            </title>
            <description>
<![CDATA[
Score 237 | Comments 79 (<a href="https://news.ycombinator.com/item?id=23735991">thread link</a>) | @jesseduffield
<br/>
July 4, 2020 | https://jesseduffield.com/beginners-guide-to-abstraction/ | <a href="https://web.archive.org/web/*/https://jesseduffield.com/beginners-guide-to-abstraction/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-75">
	<!-- .entry-header -->

	
	
	<div>
		<p>In <em>The Pragmatic Programmer</em>, Andrew Hunt and David Thomas introduced the DRY (Don't Repeat Yourself) principle. The rationale being that if you see the same code copy+pasted 10 times you should probably factor that code into its own method/class.</p>
<p>But then Sandi Metz came along and <a href="https://www.sandimetz.com/blog/2016/1/20/the-wrong-abstraction">said</a>:</p>
<blockquote>
<p>Duplication is far cheaper than the wrong abstraction.</p>
</blockquote>
<p>And so the eternal war began.</p>
<h3>What is abstraction?</h3>
<p>For the purposes of this post I'm referring to the kind of abstraction as described in the <a href="https://en.wikipedia.org/wiki/Abstraction_(computer_science)#Abstraction_in_object_oriented_programming">Abstraction Principle</a>, which Wikipedia describes like so:</p>
<blockquote>
<p>In software engineering and programming language theory, the abstraction principle (or the principle of abstraction) is a basic dictum that aims to reduce duplication of information in a program (usually with emphasis on code duplication) whenever practical by making use of abstractions provided by the programming language or software libraries</p>
</blockquote>
<p>This post has nothing to say about the conceptual kind of abstraction where from the concrete examples of 'Parrot' and 'Sparrow' you create an abstraction of 'Bird'. This post is about duplicated code, how to respond to it, and how to respond to other people's responses to it.</p>
<p>I define the verb 'abstraction' to be an <em>attempt</em> to reduce complexity by combining repeated commonality into some generalisation. And so, the noun 'abstraction' is the result of that attempt. If you're somebody who believes abstraction is by definition a <em>successful</em> attempt, feel free to substitute the term 'wrong abstraction' with 'failure to abstract' throughout this post.</p>
<p>The process of abstraction typically goes like this:<br>
1) you identify different chunks of code that you think are all essentially doing the same thing<br>
2) you create a method or a class with a narrow interface which can be substituted in for all the chunks of code you found<br>
3) you go and swap out the chunks of code with a call to your method/class</p>
<h3>Abstraction is always a gamble</h3>
<p>In the world of software engineering, when requirements are always changing, every abstraction is a gamble. When you make an abstraction over some concrete things, you're making a bet that the concrete things are more similar than they are different, and that their similarities are not mere coincidences: that there is a common purpose shared by the concrete things which would lead them to evolve in lockstep as requirements evolve. If you win the bet, your codebase will be easier to work in and adding new use cases via your abstraction will be trivially easy. If you lose, you'll see a flash of fear in your colleague's eyes whenever they're assigned a ticket to make yet another extension to the misfigured monster that the once-innocent abstraction has now become</p>
<p>But risk abounds everywhere, and leaving duplicated code unabstracted is its own gamble. You're betting that the chunks of code will evolve in separate directions as requirements change and that their current similarities are more coincidence than a reflection of their common purpose. Win the bet and your colleague gets to sleep soundly at night knowing they won't be facing the abstraction monster at work the next day. Lose, and code that should have evolved in lockstep is now implemented in completely different ways across different files, where a developer fixes a bug identified in one place, only for the same bug to be reported days later in a completely different file.</p>
<p>Your job is to get good at making the right bets.</p>
<h3>The right/wrong abstraction</h3>
<p>You'll know that you've made the <em>right</em> abstraction when a long time passes and you haven't needed to expand the interface (an example of expanding the interface is adding an optional flag argument). You'll also know you've made the right abstraction when another developer doesn't find it that much harder to understand how the code behaves for a given use case than if somebody had written the code to satisfy the use case without the abstraction.</p>
<p>You'll know you've made the <em>wrong</em> abstraction when after a while the interface has been expanded to support various optional flags, each for a different use case, and you need to be a genius to reason about what the code will actually do for a given use case. By the way, if you have a string arg that merely gets fed into a switch statement inside a method and for each new use case you come up with a new accepted value for it, you <em>are</em> expanding the implicit interface, even if that fact isn't captured in your type system.</p>
<p>There is plenty of daylight between the perfect abstraction and the completely wrong abstraction (perhaps the interface needs to be fundamentally changed but afterwards you're back to having a good abstraction), and so the point of this section isn't to prescribe how much you should be abstracting, but to encourage you to think about both perspectives and be able to make a case in a PR review for why you think an abstraction should/should-not exist.</p>
<h3>Do you over or under-abstract?</h3>
<p>Given it is impossible to make the right decision with regards to abstraction every time, you are probably either somebody who over-abstracts or somebody who under-abstracts.</p>
<p>If common feedback on your PR reviews is that you should DRY up your code, you could probably benefit from doing a scan for duplicated code before submitting a PR and considering whether it belongs in its own method/class.</p>
<p>If you commonly get feedback that your methods are hard to understand because they support too many disparate use cases at once, you are probaby over-abstracting and should consider whether you should increase your tolerance for duplication.</p>
<p>Note that it's not always as simple as under-abstracting vs over-abstracting. Sometimes abstraction is appropriate, but you might take the wrong approach. If an abstraction is deemed wrong by the team, that doesn't mean no abstraction is necessarily the best alternative.</p>
<h3>Under-abstraction examples</h3>
<p>The main sign that you could be under-abstracting is that you have a heap of code doing the exact same thing called in a heap of places with no obvious reason why anybody would want the code to diverge.</p>
<h4>Example: Hard-coded formulas</h4>
<h5>Bad:</h5>
<pre><code># sphere has radius of 11
sphere_volume = 4*Math::PI/3*11**3
puts "the volume of the sphere is #{sphere_volume} cm^3"
...

radius = calculate_radius
volume = 4*Math::PI/3*radius**3
sphere.volume = volume</code></pre>
<h5>Good:</h5>
<pre><code>def sphere_volume(radius)
  4*Math::PI/3*radius**3
end

# sphere has radius of 11
sphere_volume = sphere_volume(11)
puts "the volume of the sphere is #{sphere_volume} cm^3"
...

radius = calculate_radius
volume = sphere_volume(radius)
sphere.volume = volume</code></pre>
<p>Why is it a good idea to abstract the formula for a sphere's volume into its own method? Because if mathematicians ever found out they got the formula wrong, you would want to go through all the places in your code that you used the formula and update it to be correct. That is, we know ahead of time that we want the code to be in lockstep. This is as safe a gamble as you can get.</p>
<h3>Over-abstraction examples</h3>
<p>The main sign that you're over-abstracting is that your method accepts a bunch of optional args:</p>
<h4>Example: Bloated method</h4>
<h5>Bad:</h5>
<pre><code>def average(arr, type = Integer, ignore_nulls = false)
  if arr.any?(&amp;:nil?)
    if ignore_nulls
      arr = arr.compact
    else
      return nil
    end
  end

  if type == String
    arr = arr.map(&amp;:to_i)
  end

  arr.sum / arr.size
end

puts average([1,2,3])
=&gt; 2

puts average(['1','2','3'], String)
=&gt; 2

puts average(['1','2','3', nil], String, true)
=&gt; 2

puts average([1, 2, 3, nil], Integer, false)
=&gt; nil</code></pre>
<p>If you want to know how the <code>average</code> method behaves when you're dealing with an array of strings with no <code>nil</code> values, you have to read through the first if condition which has nothing to do with your use case before reaching the code that does. Likewise if you want to know how the <code>average</code> method behaves when the array contains either nils or integers, the second if condition is irrelevant, but you'll still need to read through that to understand how the whole thing works.</p>
<p>If each of the use cases came up dozens or hundreds of times, maybe then it would make sense to retain the abstraction, but when the number of optional arguments is roughly equal to the number of different use cases, chances are you've got the wrong abstraction.</p>
<h5>Good:</h5>
<pre><code>def average(arr)
  arr.sum / arr.size
end

puts average([1,2,3])
=&gt; 2

arr = ['1','2','3'].map(&amp;:to_i)
puts average(arr)
=&gt; 2

arr = ['1','2','3', nil].compact.map(&amp;:to_i)
puts average(arr)
=&gt; 2

arr = [1, 2, 3, nil]
if arr.any?(&amp;:nil?)
  puts nil
else
  puts average(arr)
end
=&gt; nil</code></pre>
<p>In this case we're not removing the abstraction altogether: we're just keeping the part that actually applies to all cases. Now understanding the logic of any one invocation of our <code>average</code> method is trivial.</p>
<p>We now have <code>.map(&amp;:to_i)</code> being duplicated whereas it only appeared once in the <code>Bad</code> alternative, but it's a small cost for a vast improvement.</p>
<p>Note that looking at the <code>Good</code> variant, it's clear that the behaviour is quite different from one use case to the next, but that is not at all clear in the <code>Bad</code> variant because the method calls all look so simple and it was anybody's guess how much code inside <code>average</code> applied to each use case.</p>
<p>This is why abstractions go bad over time: because as you expand the interface more and more, it becomes harder and harder to judge how appropriate the abstraction is to any given use case, and developers end up assuming that all that convoluted code is vaguely relevant to the majority of use cases when in fact it's not.</p>
<h4>Example: Awkward class</h4>
<h5>Bad:</h5>
<pre><code>class Shape
  def initialize(radius: nil, width: nil, type:)
    @radius = radius
    @width = width
    @type = type
  end

  def area
    case @type
    when :square
      @width ** 2
    when :circle
      (@radius ** 2) * Math::PI
    end
  end

  def perimeter
    case @type
    when :square
      @width * 4
    when :circle
      @radius * 2 * Math::PI
    end
  end

  def diameter
    case @type
    when :square
      nil
    when :circle
 …</code></pre></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://jesseduffield.com/beginners-guide-to-abstraction/">https://jesseduffield.com/beginners-guide-to-abstraction/</a></em></p>]]>
            </description>
            <link>https://jesseduffield.com/beginners-guide-to-abstraction/</link>
            <guid isPermaLink="false">hacker-news-small-sites-23735991</guid>
            <pubDate>Sun, 05 Jul 2020 01:12:40 GMT</pubDate>
        </item>
    </channel>
</rss>
