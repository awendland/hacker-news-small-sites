<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 20]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 20. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Mon, 07 Sep 2020 08:25:10 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-20.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Mon, 07 Sep 2020 08:25:10 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[What Satoshi Did Not Know (2015) [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 138 | Comments 139 (<a href="https://news.ycombinator.com/item?id=24378055">thread link</a>) | @MrXOR
<br/>
September 4, 2020 | https://www.ifca.ai/pub/fc15/89750001.pdf | <a href="https://web.archive.org/web/*/https://www.ifca.ai/pub/fc15/89750001.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.ifca.ai/pub/fc15/89750001.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-24378055</guid>
            <pubDate>Fri, 04 Sep 2020 19:21:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Build a State]]>
            </title>
            <description>
<![CDATA[
Score 55 | Comments 52 (<a href="https://news.ycombinator.com/item?id=24377232">thread link</a>) | @tnorthcutt
<br/>
September 4, 2020 | https://www.worksinprogress.co/issue/how-to-build-a-state/ | <a href="https://web.archive.org/web/*/https://www.worksinprogress.co/issue/how-to-build-a-state/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
					<div>
						<p>It’s easy to imagine that governments were always as bureaucratic as they are today. Certain policies, like the widespread granting of monopolies in the seventeenth century, or the presence of a powerful landed aristocracy, seem like archaic products of a past that was simply more corrupt. The fact that governments rarely got involved with healthcare or education before the mid-nineteenth century seems the product of a lack of imagination, or perhaps yet another product of our ancestors’ venality – simply what happens when you put the war-hungry knights and nobles in charge.</p>
<p>But the bureaucratic state of today, with its officials involving themselves with every aspect of modern life, is a relatively recent invention. In a world without bureaucracy, when state capacity was relatively lacking, it’s difficult to see what other options monarchs would have had. Suppose yourself transported to the throne of England in 1500, and crowned monarch. Once you bored of the novelty and luxuries of being head of state, you might become concerned about the lot of the common man and woman. Yet even if you wanted to create a healthcare system, or make education free and universal to all children, or even create a police force (London didn’t get one until 1829, and the rest of the country not til much later), there is absolutely no way you could succeed.</p>
<p>For a start, you would struggle to maintain your hold on power. Fund schools you say? Somebody will have to pay. The nobles? Well, try to tax them — in many European states they were exempt from taxation — and you might quickly lose both your throne and your head. And supposing you do manage to tax them, after miraculously stamping out an insurrection without their support, how would you even begin to go about collecting it? There was simply no central government agency capable of raising it. Working out how much people should pay, chasing up non-payers, and even the physical act of collection, not to mention protecting that treasure once collected, all takes substantial manpower. Not to mention the fact that the collecting agents will likely siphon most of it off to line their own pockets.</p>
<p>As a monarch in 1500, you would be forced to rely heavily on delegation. As the economic historian Jared Rubin emphasizes, every ruler requires agents to propagate their rule. These can take the form of big burly blokes with heavy weapons — your enforcers — and they can take the form of people spreading the ideology of your right to rule, lending your orders legitimacy and more generally spreading social norms of obedience — the local officials, jurists, and clerics. Crucially, these propagating agents needed to be kept on-side at all costs. Hence the tax exemptions for nobles, many of whom were rich enough to support their own private armies, and whose ancestors might have been granted such a privilege by one of your predecessors.</p>
<p>Joan of Arc’s village, for example, was exempted from taxation <i>forever,</i> because of her role in restoring the French king’s military fortunes. And hence the control often exerted over policy by religious figures, who could give one’s rule divine sanction in the eyes of the people. It is no surprise that the emperors of China carefully cultivated themselves at the head of as many religions as possible (one of the reasons that Christianity, with its rather awkward stress on Jesus being the king of kings, was occasionally banned from the empire). As economic historians Mark Koyama and Noel Johnson argue, this focus on appeasing religious legitimisers was one of the reasons rulers so often sought to stamp out heretics and heathens.</p>
<p>Even beyond satisfying the propagating agents, however, core state activities like tax collection were heavily reliant on delegation of authority. Absent an expensive bureaucracy to do it on your behalf, it was much easier to farm out the activity to others. When taxing a predominantly agrarian society — in 1500, only 3.6% of England’s population lived in cities — it made most sense to appoint a local noble to be tax farmer for an area. As a local, they would know the area well, and the products of the land are relatively straightforward to tax. It’s not exactly easy to hide. And as for the tax farmer, they could be remunerated by taking a cut of what they collected. Corrupt, yes. But inevitable and even seen as legitimate, given the lack of alternatives.</p>
<p>Your capacity as a ruler was thus entirely governed by the <em>quid pro quo</em>, and heavily restricted by the deals made by your predecessors. Cities were granted charters, giving them various rights of self-governance, in return for collecting taxes from among their own members (commerce and industry being much harder to tax than land, and thus requiring more consent). And, of course, there were occasionally times when the defence of the realm might require extra taxes — for such instances, rulers convened parliaments. Taxes raised by parliaments were referred to, somewhat confusingly, as “subsidies”. But that is exactly what they were: special subsidies granted to the monarch, by the collected representatives of the nobles and the various other propagating agents, for a one-off extraordinary purpose – albeit giving those collected agents some say over policy.</p>
<p>It was not until 1689, when there was a coup, that an incoming ruler allowed the English parliament to sit whenever it pleased. Before that, it was convened only at the whim of the ruler, and dispersed even at the slightest provocation. In 1621, for example, when James I was planning to marry his heir to a Spanish princess, Parliament sent him a petition asserting their right to debate the matter. Upon hearing of it, he called for the official record of parliamentary proceedings, personally ripped out the page with the offending vote, and promptly dissolved the Parliament. The downside, of course, was that James could not then acquire any parliamentary subsidies.</p>
<p>Ruling was thus an intensely personal affair, of making deals and finding ways to circumvent deals you had inherited. Increasing your capabilities as a ruler – state capacity – was thus no easy task, as the typical ruler was stuck in an essentially medieval equilibrium. Imposing a policy costs money, but raising money involves imposing policy. Breaking out of this chicken-and-egg problem took centuries of canny leadership. The rulers who achieved it most would today seem hopelessly corrupt.</p>
<p>To gain extra cash without interference from Parliament, successive monarchs first asserted and then abused their ancient prerogative rights to grant monopolies over trades and industries. They eventually granted them to whomever was willing to pay, establishing monopolies over industries like gambling cards or alehouses under the guise of regulating unsavoury activities. They also sold off knighthoods and titles, and in 1670 Charles II even made a secret deal with the French that he would convert to Catholicism and attack the Protestant Dutch, all in exchange for cash. Anything to not have to call a potentially pesky Parliament. At times, the most effective rulers even resembled mob bosses. Take Elizabeth I’s anger when a cloth-laden merchant fleet bound for an Antwerp fair in 1559 was allowed to depart. Her order to stop them had not arrived in time, thus preventing her from extracting “loans” from the merchants while she still had their goods within her power.</p>
<p>Yet, ironically, it was when monarchs lost control that they did most to boost the capabilities of the centralised state. It was under Parliament, first in the 1650s when it briefly overthrew the monarchy, and then from the late 1680s following its deal with the usurping William III, that British state capacity began to most rapidly and inexorably grow. Likewise, in France, it was following the French Revolution that the steady rise of state capacity was boosted — it was then, over three centuries after the fact, that the perpetual tax exemptions for Joan of Arc’s village were finally rescinded.</p>
<p>Parliaments, as bodies of legitimising agents, despite their lack of representation in any modern democratic sense, had the unquestioned legitimacy with which to raise taxes, change policy, and undo the deals of previous monarchs. In the process, they often trampled on the ancient liberties of citizens and subjects. But, unlike monarchs, they found it much easier to force the changes through. When motivated by the needs of war — often the one thing members could agree on — parliaments in the eighteenth century were able to raise cash that would have been unfathomable to the monarchs of even a few decades before. And it was parliaments, also, that were eventually susceptible in the nineteenth century to the lobbying of those who wished the state to involve itself in areas like education, health, and policing.</p>
<p>The route to state capacity thus took centuries of blood, dirty dealing, corruption and campaigning to make the states we have to today. But it is difficult to see how it could have been done in any other way.</p>
					</div>
				</div></div>]]>
            </description>
            <link>https://www.worksinprogress.co/issue/how-to-build-a-state/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24377232</guid>
            <pubDate>Fri, 04 Sep 2020 17:50:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Algorithmic Trading Using Logistic Regression]]>
            </title>
            <description>
<![CDATA[
Score 37 | Comments 11 (<a href="https://news.ycombinator.com/item?id=24377199">thread link</a>) | @cshad
<br/>
September 4, 2020 | https://handsoffinvesting.com/an-algorithmic-trading-strategy-using-logistic-regression/ | <a href="https://web.archive.org/web/*/https://handsoffinvesting.com/an-algorithmic-trading-strategy-using-logistic-regression/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-87">
      <div>
    <section>
            <div>
        
<h3>Learn how to implement an automated machine learning strategy with the goal of finding the optimal stocks for algorithmic trading.</h3>



<hr>



<figure><img src="https://miro.medium.com/max/4096/0*Qin1Uu6TF6EGY9Qe" alt="Image for post" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption>Photo by&nbsp;<a href="https://unsplash.com/@stephenleo1982?utm_source=medium&amp;utm_medium=referral" target="_blank" rel="noreferrer noopener">Stephen Leonardi</a>&nbsp;on&nbsp;<a href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" target="_blank" rel="noreferrer noopener">Unsplash</a></figcaption></figure>



<p id="761b">With the increasing popularity of machine learning, many traders are looking for ways in which they can “teach” a computer to trade for them. This process is called algorithmic trading (sometimes called algo-trading).</p>



<figure><blockquote><p>Algorithmic trading is a hands off strategy for buying and selling stocks that leverages technical indicators instead of human intuition.</p></blockquote></figure>



<p id="8e73">In order to implement an algorithmic trading strategy though, you have to first narrow down a list of stocks that you want to analyze. This walk-through provides an automated process (using python and logistic regression) for determining the best stocks to algo-trade.</p>



<p id="45b0">I will dive deeper into the logic and code below, but here is a high-level overview of the process:</p>



<ol><li>Import the historical data of every stock using yahoo finance.</li><li>Pull in over 32 technical indicators for each stock using the&nbsp;<a href="https://github.com/bukosabino/ta" target="_blank" rel="noreferrer noopener">technical analysis library</a>.</li><li>Perform a logistic regression on each stock using 5, 30, and 60 day observation time periods.</li><li>Interpret the results.</li></ol>



<hr>



<h2 id="7355">Import Necessary Libraries</h2>



<p id="9268">Running this program in python requires these libraries:</p>



<p id="64b9">Libraries:</p>



<ul><li><a href="https://pypi.org/project/yfinance/" target="_blank" rel="noreferrer noopener">Yfinance</a>: Gather the historical data of each stock.</li><li><a href="https://pandas.pydata.org/docs/reference/index.html" target="_blank" rel="noreferrer noopener">Pandas</a>: Work with large datasets.</li><li><a href="https://docs.python.org/3/library/shutil.html" target="_blank" rel="noreferrer noopener">Shutil</a>,&nbsp;<a href="https://docs.python.org/3/library/glob.html" target="_blank" rel="noreferrer noopener">Glob</a>, and&nbsp;<a href="https://docs.python.org/3/library/os.html" target="_blank" rel="noreferrer noopener">OS</a>: Access folders/files on your computer.</li><li><a href="https://docs.python.org/3/library/time.html" target="_blank" rel="noreferrer noopener">Time</a>: Forcing the program to pause for a period of time.</li><li><a href="https://github.com/shilewenuw/get_all_tickers" target="_blank" rel="noreferrer noopener">Get_All_Tickers</a>: Filter through all stocks to get the list you desire.</li><li><a href="https://numpy.org/" target="_blank" rel="noreferrer noopener">Numpy</a>: Work with arrays.</li><li><a href="https://scikit-learn.org/stable/" target="_blank" rel="noreferrer noopener">Sklearn</a>: To run our logistic regression model.</li><li><a href="https://github.com/bukosabino/ta" target="_blank" rel="noreferrer noopener">TA</a>: To import the technical indicators.</li></ul>



<div><pre data-file="https://gist.github.com/cameronShadmehry/dfc993bae9f6192a88f43c22c0461302#file-atlr_lib-py" data-lang="Python"><code>import yfinance as yf, pandas as pd, shutil, os, time, glob
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import load_digits
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from get_all_tickers import get_tickers as gt
from ta import add_all_ta_features
from ta.utils import dropna</code></pre></div>



<hr>



<h2 id="8a50">Collecting Historical Stock Data</h2>



<p id="b37a">Obtaining historical data on the stocks that we want to observe is a two-step process.</p>



<ol><li>Narrow down a list of stocks that we want to observe.</li><li>Make individual calls to the&nbsp;<a href="https://pypi.org/project/yfinance/" target="_blank" rel="noreferrer noopener">Yfinance API</a>&nbsp;in order to import data about each company.</li></ol>



<p id="d925"><strong>Choosing the Stocks We Want to Target</strong></p>



<p id="b1b8">The library&nbsp;<a href="https://github.com/shilewenuw/get_all_tickers" target="_blank" rel="noreferrer noopener">get-all-tickers</a>&nbsp;allows us to compile a list of stock tickers by filtering companies on aspects like market cap or exchange. For this example, I am looking at companies that have a market cap between $150,000 and $10,000,000 (in millions).</p>



<p id="b0b7">You will notice that I also included a line of code to print the number of tickers we are using. This is very important. You will need to be sure that you are not targeting more than 2,000 tickers, because the Yfinance API has a 2,000 API calls per hour limit.</p>



<p id="a75a">We also need to remove and create a folder on our local machine to hold historical stock data. Note that before this step runs for the first time, you will need to manually create this folder. This step is important because it will get rid of the data from past iterations and allow the program to start new.</p>



<div><pre data-file="https://gist.github.com/cameronShadmehry/7fd146a70470186482eadefde6dec7ea#file-atlr_gettickers-py" data-lang="Python"><code># List of the stocks we are interested in analyzing. At the time of writing this, it narrows the list of stocks down to 44.
# If you have a list of your own you would like to use just create a new list instead of using this, for example: tickers = ["FB", "AMZN", ...] 
tickers = gt.get_tickers_filtered(mktcap_min=150000, mktcap_max=10000000)

# Check that the amount of tickers isn't more than 2000
print("The amount of stocks chosen to observe: " + str(len(tickers)))

# These two lines remove the Stocks folder and then recreate it in order to remove old stocks. Make sure you have created a Stocks Folder the first time you run this.
shutil.rmtree("&lt;Your Path&gt;\\Bayesian_Logistic_Regression\\Stocks\\")
os.mkdir("&lt;Your Path&gt;\\Bayesian_Logistic_Regression\\Stocks\\")</code></pre></div>



<hr>



<h2 id="c45e">Importing Historical Stock Data</h2>



<p id="1c9d">We will now obtain the historical price data of each stock in our tickers list by making independent calls to Yahoo Finance. After receiving the data, the program will save each company’s information in a new CSV file that will be located in the folder you created beforehand.</p>



<div><pre data-file="https://gist.github.com/cameronShadmehry/6ca432c1d94e5ae7f44553c5288b1804#file-atlr_importdata-py" data-lang="Python"><code># Holds the amount of API calls we executed
Amount_of_API_Calls = 0

# This while loop is reponsible for storing the historical data for each ticker in our list. Note that yahoo finance sometimes incurs json.decode errors and because of this we are sleeping for 2
# seconds after each iteration, also if a call fails we are going to try to execute it again.
# Also, do not make more than 2,000 calls per hour or 48,000 calls per day or Yahoo Finance may block your IP. The clause "(Amount_of_API_Calls &lt; 1800)" below will stop the loop from making
# too many calls to the yfinance API.
# Prepare for this loop to take some time. It is pausing for 2 seconds after importing each stock.

# Used to make sure we don't waste too many API calls on one Stock ticker that could be having issues
Stock_Failure = 0
Stocks_Not_Imported = 0

# Used to iterate through our list of tickers
i=0
while (i &lt; len(tickers)) and (Amount_of_API_Calls &lt; 1800):
    try:
        stock = tickers[i]  # Gets the current stock ticker
        temp = yf.Ticker(str(stock))
        Hist_data = temp.history(period="max")  # Tells yfinance what kind of data we want about this stock (In this example, all of the historical data)
        Hist_data.to_csv("&lt;Your Path&gt;\\Bayesian_Logistic_Regression\\Stocks\\"+stock+".csv")  # Saves the historical data in csv format for further processing later
        time.sleep(2)  # Pauses the loop for two seconds so we don't cause issues with Yahoo Finance's backend operations
        Amount_of_API_Calls += 1 
        Stock_Failure = 0
        i += 1  # Iteration to the next ticker
        print("Importing stock data:" + str(i))
    except ValueError:
        print("Yahoo Finance Backend Error, Attempting to Fix")  # An error occured on Yahoo Finance's backend. We will attempt to retreive the data again
        if Stock_Failure &gt; 5:  # Move on to the next ticker if the current ticker fails more than 5 times
            i+=1
            Stocks_Not_Imported += 1
        Amount_of_API_Calls += 1
        Stock_Failure += 1
print("The amount of stocks we successfully imported: " + str(i - Stocks_Not_Imported))</code></pre></div>



<hr>



<h2 id="2687">Importing Technical Indicators</h2>



<p id="a921">In order to run a logistic regression on each stock, we need to substantiate the independent variables. These 32+ technical indicators will act as predictor variables for the dependent variable.</p>



<p id="6e7c">To bring in the technical indicators we make use of the&nbsp;<a href="https://github.com/bukosabino/ta" target="_blank" rel="noreferrer noopener">technical analysis library</a>.</p>



<p id="1d04">You will also notice that we are creating three extra columns. These will act as the value we are trying to predict (dependent variable) with the model. The three variables are 5, 30, and 60 day observations of the closing prices of each stock (1 if it increased and 0 if it did not).</p>



<div><pre data-file="https://gist.github.com/cameronShadmehry/741582a3aa0a86986773f1a47cbc5bc8#file-atlr_ti-py" data-lang="Python"><code># These two lines remove the Stocks folder and then recreate it in order to remove old stocks. Make sure you have created a Stocks Folder the first time you run this.
shutil.rmtree("&lt;Your Path&gt;\\Bayesian_Logistic_Regression\\Stocks_Sub\\")
os.mkdir("&lt;Your Path&gt;\\Bayesian_Logistic_Regression\\Stocks_Sub\\")

# Get the Y values
list_files = (glob.glob("&lt;Your Path&gt;\\Bayesian_Logistic_Regression\\Stocks\\*.csv")) # Creates a list of all csv filenames in the stocks folder
for interval in list_files:
    Stock_Name = ((os.path.basename(interval)).split(".csv")[0])
    data = pd.read_csv(interval)
    dropna(data)
    data = add_all_ta_features(data, open="Open", high="High", low="Low", close="Close", volume="Volume")
    data = data.iloc[100:]
    close_prices = data['Close'].tolist()
    Five_Day_Obs = []
    thirty_Day_Obs = []
    sixty_Day_Obs = []
    x = 0
    while x &lt; (len(data)):
        if x &lt; (len(data)-5):
            if ((close_prices[x+1] + close_prices[x+2] + close_prices[x+3] + close_prices[x+4] + close_prices[x+5])/5) &gt; close_prices[x]:
                Five_Day_Obs.append(1)
            else:
                Five_Day_Obs.append(0)
        else:
            Five_Day_Obs.append(0)
        x+=1
    y = 0
    while y &lt; (len(data)):
        if y &lt; (len(data)-30):
            ThirtyDayCalc = 0
            y2 = 0
            while y2 &lt; 30:
                ThirtyDayCalc = ThirtyDayCalc + close_prices[y+y2]
                y2 += 1
            if (ThirtyDayCalc/30) &gt; close_prices[y]:
                thirty_Day_Obs.append(1)
            else:
                thirty_Day_Obs.append(0)
        else:
            thirty_Day_Obs.append(0)
        y+=1
    z = 0
    while z &lt; (len(data)):
        if z &lt; (len(data)-60):
            SixtyDayCalc = 0
            z2 = 0
            while z2 &lt; 60:
                SixtyDayCalc = SixtyDayCalc + close_prices[z+z2]
                z2 += 1
            if (SixtyDayCalc/60) &gt; close_prices[z]:
                sixty_Day_Obs.append(1)
            else:
                sixty_Day_Obs.append(0)
        else:
            sixty_Day_Obs.append(0)
        z+=1
    data['Five_Day_Observation_Outcome'] = Five_Day_Obs
    data['Thirty_Day_Observation_Outcome'] = thirty_Day_Obs
    data['Sixty_Day_Observation_Outcome'] = sixty_Day_Obs
    data.to_csv("&lt;Your Path&gt;\\Bayesian_Logistic_Regression\\Stocks_Sub\\"+Stock_Name+".csv")
    print("Data for " + Stock_Name + " has been substantiated with technical features.")</code></pre></div>



<hr>



<h2 id="c892">Run the Logistic Regression Model</h2>



<p id="d59a">At this point, we have the historical data as well as over 32 technical indicators for each stock we chose to observe. We also created three different time-interval observations for which we want to predict the stock’s future price.</p>



<p id="1c52">Now, all that is left is to clean the data (remove infinite and …</p></div></section></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://handsoffinvesting.com/an-algorithmic-trading-strategy-using-logistic-regression/">https://handsoffinvesting.com/an-algorithmic-trading-strategy-using-logistic-regression/</a></em></p>]]>
            </description>
            <link>https://handsoffinvesting.com/an-algorithmic-trading-strategy-using-logistic-regression/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24377199</guid>
            <pubDate>Fri, 04 Sep 2020 17:46:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: AnyLeaf – PH Sensor for Microcontrollers]]>
            </title>
            <description>
<![CDATA[
Score 79 | Comments 48 (<a href="https://news.ycombinator.com/item?id=24376990">thread link</a>) | @the__alchemist
<br/>
September 4, 2020 | https://www.anyleaf.org/ph-module | <a href="https://web.archive.org/web/*/https://www.anyleaf.org/ph-module">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

        <div>
            <p><img src="https://www.anyleaf.org/static/logo.png" width="300" alt="AnyLeaf"></p>
            <h2>For Arduino, Raspberry Pi, embedded</h2>
        </div>

        <p>
            <h2>Accurate</h2>
            <h2>Easy to use</h2>
            <h2>Affordable</h2>
        </p>

        <div>
            <div>
                <p><img src="https://www.anyleaf.org/static/ph_with_probe_cropped.jpg" width="640px">
                </p>
                
                <p onclick="addToCart(1)">
                    <h2>Buy with pH probe for $60</h2>
                </p>
            </div>

            <div>
                <p><img src="https://www.anyleaf.org/static/ph_cropped.jpg" width="640px">
                </p>

                <p onclick="addToCart(2)">
                    <h2>Buy without probe for $20</h2>
                </p>
            </div>

        </div>

        <p><img src="https://www.anyleaf.org/static/ph_probe_cropped.jpg" width="640px">
        </p>

        <h2>For laboratory, hydroponics, aquarium, and brewing</h2>

        <p>Available with lab-grade pH probe. Connects directly to your
            device (Arduino, Raspberry Pi, or embedded) - no other parts required. Our
            open-source software makes measuring fast and easy.</p>

        <h2>Suitable for continuous immersion</h2>

        <h3>Beyond pH</h3>
        <p>Includes temperature compensation, with built in sensor. Can be used with
            Oxidation Reduction Potential(ORP) , and other ion-selective electrodes.
            (Nitrate, Potassium, etc)

            probes.</p>

        <p>This module can be used with any probe with a BNC connection.
            The I2C interface can send data to nearly any microcontroler.</p>

        <h3>Fully tested on <a href="https://www.arduino.cc/">Arduino</a>,
            <a href="https://www.raspberrypi.org/">Raspberry Pi</a>, and
            <a href="https://www.st.com/en/microcontrollers-microprocessors/stm32f3-series.html">STM32</a></h3>

        <h4>Output pins are labeled on the board, as follows:</h4>
        <ul>
            <li>+VCC: 2V – 5V power input</li>
            <li>SDA: I²C digital output</li>
            <li>SCL: I²C digital output</li>
            <li>GND: Ground</li>
        </ul>

        <h2>Arduino wiring</h2>
        <p><img src="https://www.anyleaf.org/static/arduino_diagram.png" width="500">
        </p>

        <h2>Raspberry Pi wiring</h2>
        <p><img src="https://www.anyleaf.org/static/pi_diagram.png" width="600">
        </p>

        
        <p><b>Arduino: </b> From the Arduino IDE: <i>Sketch</i> → <i>Include Library</i> →
            <i>Manage Libraries</i> → select <i><a href="https://www.arduino.cc/reference/en/libraries/anyleaf/">Anyleaf</a></i></p>
        <p><b>C++: </b>Download <i>Anyleaf.cpp</i>
            and <i>Anyleaf.h</i> from the C++ Github link below, and place them in your project directory
        </p>
        <p><b>Python: </b>Run <i>sudo apt install python3-scipy</i>, then
            <i>pip3 install <a href="https://pypi.org/project/anyleaf/">anyleaf</a></i>

        </p>
        <ul>
            <li><h3><a href="https://github.com/anyleaf/ph-cpp" target="_blank">Arduino and C++</a></h3></li>
            <li><h3><a href="https://github.com/anyleaf/anyleaf-python" target="_blank">Python</a></h3></li>
            <li><h3><a href="https://github.com/anyleaf/anyleaf-rust" target="_blank">Rust</a></h3></li>
        </ul>

        <h2>A miminal Arduino example</h2>

        <!--
        Syntax highlighting by http://hilite.me/


#include <Anyleaf.h>

PhSensor ph_sensor;

void setup(void) {
    Serial.begin(9600);

    ph_sensor = PhSensor();

    ph_sensor.calibrate_all(
        CalPt(0., 7., 25.), CalPt(0.18, 4., 25.)
    );
}

void loop(void) {
    Serial.print("pH: "); Serial.println(ph_sensor.read());

    delay(1000);
}
        !-->



        <!-- HTML generated using hilite.me --><div><pre><span>#include &lt;Anyleaf.h&gt;</span>

PhSensor ph_sensor;

<span>void</span> <span>setup</span>(<span>void</span>) {
    Serial.begin(<span>9600</span>);

    ph_sensor = PhSensor();

    ph_sensor.calibrate_all(
        CalPt(<span>0.</span>, <span>7.</span>, <span>25.</span>), CalPt(<span>0.18</span>, <span>4.</span>, <span>25.</span>)
    );
}

<span>void</span> <span>loop</span>(<span>void</span>) {
    Serial.print(<span>"pH: "</span>); Serial.println(ph_sensor.read());

    delay(<span>1000</span>);
}
</pre></div>

        <!--
import time
import board
import busio
from anyleaf import PhSensor, CalPt, OnBoard

def main():
    i2c = busio.I2C(board.SCL, board.SDA)
    delay = 1  # Time between measurements, in seconds
    ph_sensor = PhSensor(i2c, delay)

    ph_sensor.calibrate_all(
        CalPt(0., 7., 25.), CalPt(0.18, 4., 25.)
    )

    while True:
        print(f"pH: {ph_sensor.read(OnBoard())}")

        time.sleep(delay)


if __name__ == "__main__":
    main()
        -->

        <h2>A miminal Raspberry Pi Python example</h2>

        <!-- HTML generated using hilite.me --><!-- HTML generated using hilite.me --><div><pre><span>import</span> <span>time</span>
<span>import</span> <span>board</span>
<span>import</span> <span>busio</span>
<span>from</span> <span>anyleaf</span> <span>import</span> PhSensor, CalPt, OnBoard

<span>def</span> <span>main</span>():
    i2c = busio.I2C(board.SCL, board.SDA)
    delay = <span>1</span>  <span># Time between measurements, in seconds</span>
    ph_sensor = PhSensor(i2c, delay)

    ph_sensor.calibrate_all(
        CalPt(<span>0.</span>, <span>7.</span>, <span>25.</span>), CalPt(<span>0.18</span>, <span>4.</span>, <span>25.</span>)
    )

    <span>while</span> <span>True</span>:
        <span>print</span>(f<span>"pH: {ph_sensor.read(OnBoard())}"</span>)

        time.sleep(delay)


<span>if</span> __name__ == <span>"__main__"</span>:
    main()
</pre></div>

    <h2><a href="https://www.anyleaf.org/static/ph-module-datasheet.pdf">Datasheet</a></h2>

    <p>Regular calibration is required for accurate measurements.
        The official drivers support 2 or 3 point calibration. 2 point is sufficient for
        hydroponics, aquarium, or brewing use. 3 points, or 2 carefully chosen points are required
        for sufficient accuracy for lab use.</p>

    <h2>Specifications:</h2>
    <ul>
        <li><b>Module dimensions:</b> 50 × 25 × 17(height) mm</li>
        <li><b>Module weight:</b> 14 grams</li>
        <li><b>pH precision:</b> .01, with a suitable sensor</li>
        <li><b>pH Range:</b> 0 - 14</li>
        <li><b>Digital precision:</b> 16 bit</li>
        <li><b>Input voltage:</b> 2 - 5V</li>
        <li><b>Mounting screws:</b> 4 × M3, 6mm len (included)</li>
        <li><b>I2C address:</b> 0×48 or 0x49 (selectable)</li>
        <li><b>Sensor interface:</b> BNC male (Accepts BNC female sensors)</li>
        <li><b>Power consumption:</b> 375μA</li>
        <li><b>UPC / GTIN:</b> 860004179009</li>
    </ul>

    <h2>Specifications pertaining to the pH probe only:</h2>
    <ul>
        <li><b>Design:</b> Double junction glass hydrogen-sensitive electrode</li>
        <li><b>Dimensions:</b> 155 × 12 mm</li>
        <li><b>Weight:</b> 24g</li>
        <li><b>Measurement range:</b> 0 - 14</li>
        <li><b>Outer tube material:</b> Plastic</li>
        <li><b>Bulb shape:</b> Spherical</li>
        <li><b>Cable length:</b> 1 m</li>
        <li><b>Connector:</b> BNC female</li>
        <li><b>Temperature range: </b>0-80°C</li>
    </ul>

    <p>Compatible with the <a href="https://kizniche.github.io/Mycodo/">Mycodo Environmental
        Monitoring and Regulation System</a>. If you're using a Raspberry Pi, this is a simple
        way to get it running with a GUI, charts, and logging. Just follow the installation instructions
        on that page, and add <i>Anyleaf pH</i> as an <i>input</i>.
    </p>

</div></div>]]>
            </description>
            <link>https://www.anyleaf.org/ph-module</link>
            <guid isPermaLink="false">hacker-news-small-sites-24376990</guid>
            <pubDate>Fri, 04 Sep 2020 17:26:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Google taught me to turn Impostor Syndrome into an Advantage]]>
            </title>
            <description>
<![CDATA[
Score 28 | Comments 3 (<a href="https://news.ycombinator.com/item?id=24376950">thread link</a>) | @FactCore
<br/>
September 4, 2020 | https://www.zainrizvi.io/blog/the-impostors-advantage/ | <a href="https://web.archive.org/web/*/https://www.zainrizvi.io/blog/the-impostors-advantage/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
			<!-- .cover -->


				<div>
					<p>My heart was racing. My palms sweating. I was going to be fired.</p><p>Performance reviews had just ended, and it was time to meet my manager and be told my results. Except I knew what it would say. How else do you rate a programmer who doesn’t code?</p><p>As I stood up from my desk, my eyes fell on my Ship-it plaque, congratulating me for helping release Windows 7. I had joined Microsoft a mere two weeks before it was released, a fresh college hire. There wasn’t a single character I had contributed to that code.</p><p>The plaque was a lie. Just like me.</p><figure><img src="https://www.zainrizvi.io/content/images/2020/08/cropped-plaque.png" alt="" srcset="https://www.zainrizvi.io/content/images/size/w600/2020/08/cropped-plaque.png 600w, https://www.zainrizvi.io/content/images/size/w1000/2020/08/cropped-plaque.png 1000w, https://www.zainrizvi.io/content/images/2020/08/cropped-plaque.png 1325w" sizes="(min-width: 720px) 720px"></figure><p>My manager had booked a private conference room to share the results, far away from ears that might overhear anything said. Or begged. The long walk began, each step echoing down the corridor.</p><p>I racked my brains, grasping for an excuse to justify keeping my job. Instead my mind kept going back to the last bug I was supposed to fix. I’d spent all day failing to find the problem, finally giving in and asking a teammate for help. He found it in 10 minutes. I was way out of my league.</p><p>My boss must have seen it too, I bet that was why he assigned me, the kid, to help government auditors analyze our source code. Help <em>them</em>? I barely understood it myself! But this was more of a “people project.” If it didn’t require writing code, why waste real programmers on it? And so it came to me. I barely stayed afloat, constantly asking my manager for explanations and struggling to relay them to the auditors.</p><p>Yeah, I was doomed.</p><p>How would I tell my parents? Would I ever get another job? The only coding I did here was with an obsolete technology that no other company cared about; I didn’t even have the skills to land a new job.</p><p>What made me think I could work at Microsoft?</p><p>I reached the conference room, could I stall any longer? Huh, It’s the same room he interviewed me in two years ago. I doubt he remembers.</p><p>Okay, this is it. Deep breath, poker face on. No matter what, I wouldn’t let him <a href="https://www.zainrizvi.io/blog/the-interviewing-advice-no-one-shares#tip-3-be-open-to-learning-during-the-interview">see me sweat</a>.</p><p>I stepped inside. Scott was sitting at the table, laptop carefully angled to hide the screen.</p><p>“Have a seat” he said, gesturing to his right. As I sat down, Scott looked straight at me. He opened his mouth to give me the news. But it wasn’t what I expected.</p><p>“Congratulations, you’ve been promoted”</p><p>Huh? No way I heard that right. Keep that poker face tight.</p><p>“Keep up the great work! Anything you’d like to ask?”</p><p>Wait but...when did I...what?</p><p>He hadn’t noticed? I wasn’t about to point out his mistake. Can’t let any surprise show.</p><p>“Great, thanks.” That was all I trusted myself to say.</p><p>I was safe. For now.</p><p>He’d catch on in a few months, I was sure. I couldn’t hide forever.</p><p>I spent the next few years preparing for that inevitable day, desperately trying to work on projects that could teach me the skills that would catch a recruiter’s eye. I had to become hireable.</p><p>I needed a stronger resume, with skills people cared about. I switched to a new team which built stuff for the cloud: Azure Web Apps. Companies love the cloud, right? Surely I’ll learn industry relevant skills there.</p><p>Fast forward four years: I still didn’t feel like I was anything special, yet I kept getting promoted. I kept fooling them somehow, the <a href="https://www.zainrizvi.io/blog/hacking-the-bureaucracy-to-get-stuff-done/">bureaucratic</a> review process hiding my flaws. But something else also started happening, hinting that, just maybe, I wasn’t as clueless as I thought.</p><p>What changed?</p><p>People started coming to me for answers.</p><p>I still didn’t feel like I knew that much. I was just telling people about the stuff I’d worked on, occasionally pointing younger engineers towards tactics I had seen work well. That didn’t feel like anything original, but folks were finding it useful.</p><p>It got really weird when the more senior engineers started asking <em>me </em>about the code base. These were brilliant people who had often helped me over the years. Didn’t they already know everything?</p><p>I guess not, but they were still way above my league. It’s not like I knew enough to offer <em>them </em>real advice.</p><p>But still...my team seemed to think I was doing well. Would other companies think so too? Was I finally hireable? Only one way to find out: I started <a href="https://www.zainrizvi.io/blog/the-interviewing-advice-no-one-shares">applying</a>.</p><p>I couldn’t believe the results when multiple job offers came in. And one was from Google! I couldn’t pass that one up. I made the switch.</p><p>During orientation, Google spent a lot of time discussing Impostor Syndrome, the feeling of accomplished people belittling their own talents and constantly being terrified of being discovered as a fraud. That’s a thing?</p><p>“Raise your hand if you have this feeling” Hah, yeah right, and have me be the only one raising my...oh, wow, that’s a lot of hands. Mine joined the crowd.</p><p>As I started working, impostor syndrome came up constantly. It was mentioned at company meetings, folks made memes admitting to it. It was everywhere.</p><figure><img src="https://www.zainrizvi.io/content/images/2020/08/when-someone-comments-on-my-design-doc.gif" alt=""></figure><p>People freely admitted to not knowing stuff. Teammates admitted to not understanding the code, or having no idea how a tool worked. All the stuff I didn’t know, many others didn’t get either.</p><p>Seeing everyone admit their ignorance <strong>freed me</strong> from my own fear. Suddenly, feeling clueless seemed normal. &nbsp;It was a psychological quirk, not the truth.</p><p>My self-confidence grew. And gradually, without quite realizing it, something magical happened.</p><p>Impostor syndrome became a tool. I discovered the <strong>impostor’s advantage</strong>.</p><p>Did I notice feeling intimidated about asking a question? I started pushing myself to ask that question. Turns out other people had felt afraid as well, asking that question helped improve everyone’s understanding. &nbsp;When I started openly admitting to being unfamiliar with a tool or some code, my teammates felt like less of an impostor themselves. Their confidence went up. And they in turn became more likely to admit the same, creating a virtuous cycle boosting the entire team’s morale.</p><p>The impostor’s advantage was a super power.</p><p>And it offered new insights.</p><p>That feeling of being an impostor is your subconscious telling you something: It’s saying you’re about to push yourself past your comfort zone and into the growth zone. Now when an opportunity shows up and impostor syndrome starts twitching in the pit of my stomach, <strong>that’s a sign I should jump at it!</strong> This led me to take on <a href="https://www.zainrizvi.io/blog/whats-it-like-as-a-senior-engineer/#research-like-a-detective">bigger and more ambitious</a> projects, without worrying about being exposed. Somehow I still delivered results, helped by the various people I was no longer afraid to reach out to.</p><p>Every project still started with the thought, “I have no idea what to do here.” But then I’d remind myself, “no one else does either.” That was a surprising lesson about the more senior positions: <strong>Their work is so valuable </strong><em><strong>precisely because </strong></em><strong>no one knows exactly what needs to be done. It’s ambiguous.</strong> And it requires people who can still push through the uncertainty and forge a path forward.</p><p>They embrace the impostor’s advantage.</p><figure><img src="https://www.zainrizvi.io/content/images/2020/08/i-feel-like-an-impostor.png" alt="" srcset="https://www.zainrizvi.io/content/images/size/w600/2020/08/i-feel-like-an-impostor.png 600w, https://www.zainrizvi.io/content/images/2020/08/i-feel-like-an-impostor.png 666w"></figure><p>Looking back, I realize now that in my early days I'd been evaluating myself with biased glasses. &nbsp;<strong>I was comparing myself to people much more senior than me</strong>. Of course there would be a skill gap. <strong>If one person understood something, I assumed everyone knew it</strong>. That was false. As the “systems grow it’s impossible for one person to keep it all in their head”[1]; each person just knew the areas they had personally worked on.</p><p>My biggest mistake: <strong>I didn’t value the soft skills I brought to the table</strong>. Fresh out of college I had taken a significant load off my manager's plate by being the main point of contact with auditors and other teams. The fact that I was not writing code made me think I wasn’t doing anything useful, when in fact the soft skill of being able to work with them was incredibly valuable to the team.</p><p>As I continue working and taking on bigger projects, I suspect impostor syndrome will never completely go away. But now I take that as a good thing. An advantage. It’s a sign that I’m growing and stretching myself past my comfort zone.</p><p>And in thost darkest moments, when self doubt is at its highest, I remind myself:</p><p>I haven’t been fired yet.</p><p>[1] <em><a href="https://www.amazon.com/Managers-Path-Leaders-Navigating-Growth/dp/1491973897https://www.amazon.com/Managers-Path-Leaders-Navigating-Growth/dp/1491973897">The Manager’s Path</a></em>, by &nbsp;Camille Fournier</p>
				</div><!-- .post-content -->
				<!-- .post-footer -->
				<!-- .comments-area -->


		</article></div>]]>
            </description>
            <link>https://www.zainrizvi.io/blog/the-impostors-advantage/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24376950</guid>
            <pubDate>Fri, 04 Sep 2020 17:24:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Massacring C Pointers (2018)]]>
            </title>
            <description>
<![CDATA[
Score 148 | Comments 73 (<a href="https://news.ycombinator.com/item?id=24376622">thread link</a>) | @pcr910303
<br/>
September 4, 2020 | https://wozniak.ca/blog/2018/06/25/1/index.html | <a href="https://web.archive.org/web/*/https://wozniak.ca/blog/2018/06/25/1/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

<p>
I'm taking a break from debugging books to talk about a calamitous shitshow of textbook writing: <i>Mastering C Pointers: Tools for Programming Power</i>, by Robert J. Traister.
</p>

<p>
I learned of the book through a <a href="https://www.youtube.com/watch?v=8SUkrR7ZfTA">talk</a> by Brian Kernighan where he refers to the book as probably “the worst C programming textbook ever written.”<span>[<a id="fnr.1" href="#fn.1">1</a>]</span>
He doesn't name it but <a href="https://twitter.com/johnregehr/status/1003411102791692288">with some help</a> I was able to track down his obliquely accurate reference.
</p>

<p>
This book has become my <a href="https://www.urbandictionary.com/define.php?term=white%20whale">white whale</a>.
Since I started reading debugging books, and especially now that I'm digging through older ones, I find bits of advice that simply don't work today.
While some of it could be construed as useless or idiotic, I've always found the authors come from a position of earnestness, attempting to draw the best conclusions based on decent principles and what they knew at the time they wrote it.
In some cases they may not have known much, but they're honestly and humbly trying to impart some wisdom.
</p>

<p>
When Kernighan put up the following example, I saw what seemed to be the opposite of that.
</p>

<pre>char *combine(s, t)
char *s, *t;
{

      int x, y;
      char r[100];

      strcpy(r, s);
      y = strlen(r);
      for (x = y; *t != '\0'; ++x)
           r[x] = *t++;

      r[x] = '\0';

      return(r);

}
</pre>

<p>
This program (formatting preserved) is taken from the first edition of the book (p.146).
It's horrific (Kernighan calls it “malpractice”).
It does not exhibit the genuineness I have seen with, say, books from the late 1970s on how to debug BASIC programs.
I didn't know what it was.
Deceit?
Laziness?
Unremitting ignorance?
What is the mindset of someone who writes this, presumably thinking it's a good idea?
Can the whole book be that bad?
Kernighan said it was.
I had to know.
</p>

<p>
The book has two editions: the first was published in 1990, the second in 1993.
The fact that there are two editions piqued my curiosity even more.
It sold enough to make another version?
Is that horrible example corrected?
I obtained a copy of each and read them.
</p>

<p>
Reviewing the book is pointless.
Kernighan was right: it's garbage.
And the second edition only makes things worse.
Teaching from either one would be a breach of ethics.
If you follow Traister’s coding practices, even adjusted for today's standards, you are <i>guaranteed</i> to create defects and vicious, latent bugs.
A subtly pernicious aspect of the book is the casual tone of the writing.
It’s informal enough that if you don’t really know much about C what he says sort of makes sense, despite the sloppy terminology and mixed, inaccurate metaphors.
</p>

<p>
Any trained programmer will recognize the lessons as worthless.
His terminology is all over the map and typically inaccurate, if not plainly wrong.
Expressions “return a value.”
Values are “handed” into locations.
Constants are “written directly into the program.”
A union is a “specialized pointer.”
The terminology isn’t even consistent.
Micro-optimizations are stressed at all times, and program efficiency is valued over comprehension.
He can’t even define a pointer correctly: “A pointer is a special variable that returns the address of a memory location.”
It does not take long to realize Traister has no idea what he’s talking about.
</p>

<p>
Although I’m not going to go over the content (that would take far too long since there’s nothing redeeming there), I did take extensive notes.
You can <a href="https://wozniak.ca/blog/2018/06/25/1/notes.html">read them</a>, if you are so inclined.
</p>

<p>
I must, however, take a moment to single out the code.
It is universally bad and much of it is simply wrong.
(Imagine trying to learn programming principles from a book that contains a large number of programs that don’t even compile.)
I’ve <a href="https://wozniak.ca/blog/2018/06/25/1/code.html">transcribed</a> some of the programs and annotated them with comments so that you can get a taste of how inept Traister is as a C programmer.
One thing to keep in mind (both for the programs and the notes) is that C89/C90 was new at the time and that the code was written on (and for) MS-DOS systems of the late 1980s/early 1990s.
Things were a bit different then.
</p>

<p>
Enough about the material.
I want to explore the question of how something so wrong even got written.  It’s not that everything in the book is wrong, but it feels like when it’s right, it’s right by accident.
</p>

<p>
Traister has <a href="https://www.amazon.com/Robert-J.-Traister/e/B001H6UPHY/ref=sr_ntt_srch_lnk_1?qid=1529540256&amp;sr=1-1">written</a> other books, some about electronics and some about programming, one called <i>Going from BASIC to C</i>.
In <i>Mastering C Pointers</i>, he talks about a product he created called CBREEZE that converts BASIC code to C.
Throughout the book he makes passing, roundabout references to BASIC and uses terminology that suggests he’s written a lot of BASIC code.
For example, there is a whole chapter on using pointers to access memory, where reading and writing memory is instead called “peeking” and “poking”, based on the PEEK and POKE instructions in BASIC.
He also says that it took him a couple of tries to learn C coming from BASIC.
In short, I’m convinced he’s knowledgeable about BASIC and has worked on writing software for small, electronic devices.
</p>

<p>
Why is this important?
As I read the book (and if you read my notes, you know where this is going) I started to notice something in the wording and tone.
The further I progressed the more I became convinced of it, and I think it explains how he managed to mangle the explanation of C pointers so badly.
</p>

<p>
I don’t think he understands the call stack.
</p>

<p>
My argument for this interpretation requires a little knowledge of BASIC and embedded devices.
</p>

<p>
With BASIC, the key thing to know about most implementations at the time is that there were no functions and no scope aside from the global scope.<span>[<a id="fnr.2" href="#fn.2">2</a>]</span>
The closest thing to a function in BASIC is the GOSUB command.
The GOSUB command jumps to a line and executes code until it gets to a RETURN statement, where control is transferred back to the line following the GOSUB command.
Within a GOSUB you can jump somewhere else with another GOSUB.
The control follows a stack principle, but no arguments are passed.
GOSUB routines are a way to factor out common code, but that common code has to work on global variables.
(And yes, it’s as terrible as it sounds.)
</p>

<p>
Now, consider the case of simple electronic devices.
Even today some embedded devices, usually programmed using C, do not have a call stack that dynamically allocates space for automatic variables.
There simply isn’t enough memory for it.
Instead, the compiler lays out memory such that each function’s local variables have fixed memory addresses (a “compiled stack” model).
The only stack you have is for return addresses and it is probably handled in hardware.
</p>

<p>
Suppose you’re used to writing BASIC for small memory electronic devices and you learn about C.
You read about pointers and realize something: it’s possible to write a subroutine that can change variables <i>without knowing their names</i>.
It’s manna from heaven!
You don’t have to devote global variables to being the “parameters” of your subroutines anymore.
Life is great.
</p>

<p>
This is the mindset I think Traister had and never got past.
In the book there is one fleeting mention of the stack in reference to excessive (automatic) memory allocation.
(On MS-DOS, if the space for local variables is too large in the program, it might not compile.)
He consistently describes variables as having “exclusive” addresses in the program.
His writing about pointers suggests that he thinks, for each function, space is set aside to hold the local variables for the duration of the program, but you can only access them when inside the function in which they were declared.
So pointers are really powerful because you can provide this address to another function and it can change the value using only a parameter.
</p>

<p>
Further evidence for his lack of understanding is that he frequently cites ridiculous space micro-optimizations within functions, such as avoiding the use of integers for index variables, if possible.
Another one, mentioned often, is local <code>char</code> arrays that have a fixed size.
There are good reasons to not use them but his are not among them.
His admonishment is that they waste space.
Technically, that is true, but they don't exist until they're on the stack.
And he never talks about global or file variables.
He only refers to locals with “exclusive” addresses “set aside” for variables.
</p>

<p>
This interpretation runs into some problems once you start asking how functions with malloc will work, but it's worth pointing out that there is almost no discussion about memory management.
In a book devoted to C pointers, that's a toxic mix of gross negligence and incompetence.
There is literally one short paragraph devoted to talking about the <code>free</code> function—and it's characterized as a “side note.”
</p>

<p>
Another sticking point in this interpretation is Traister’s incomprehensible approach to writing functions that take a variable number of arguments.
He does this by passing an arbitrary number of arguments to a function (the first being the number of arguments) and accessing them using offsets from the address of the first argument.
This suggests he has some idea about parameters being passed in a dynamic fashion, but it is so spectacularly wrong<span>[<a id="fnr.3" href="#fn.3">3</a>]</span> you’re left wondering if he even tried his programs out before publishing them.
</p>

<p>
Honestly, this is the most generous interpretation of the text I could come up with, and it still paints a terrible picture.
Occam's Razor suggests that Traister is just clueless.
But like <a href="https://www.youtube.com/watch?v=01l1WIC9mBo">analyzing a terrible movie</a> that somehow gets made, it's more fun to reason through the “behind the scenes” parts.
</p>

<p>
Given the ineptness of the book, you'd think it was self-published.
You would be wrong.
It was published through Academic Press, which was a division of Harcourt, Brace &amp; World at the time, but is now an “imprint of Elsevier.”
</p>

<p>
In the preface of the second edition it says that the first edition was reviewed “by a professional C programmer hired by the publisher.”
That programmer said it should not be published.
That programmer was right, but the publisher went ahead and …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://wozniak.ca/blog/2018/06/25/1/index.html">https://wozniak.ca/blog/2018/06/25/1/index.html</a></em></p>]]>
            </description>
            <link>https://wozniak.ca/blog/2018/06/25/1/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24376622</guid>
            <pubDate>Fri, 04 Sep 2020 16:49:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Standards for Gloves]]>
            </title>
            <description>
<![CDATA[
Score 52 | Comments 19 (<a href="https://news.ycombinator.com/item?id=24375011">thread link</a>) | @brudgers
<br/>
September 4, 2020 | https://guidegloves.com/en/knowledge/standards | <a href="https://web.archive.org/web/*/https://guidegloves.com/en/knowledge/standards">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <h4>About Guide</h4>
        <p>For over 30 years, our mission has been to protect and improve the performance of working hands. By being the fastest, most sustainable and innovative manufacturer of premium work gloves for a wide range of industries, we aim to make people safer and healthier at work.</p>

    </div></div>]]>
            </description>
            <link>https://guidegloves.com/en/knowledge/standards</link>
            <guid isPermaLink="false">hacker-news-small-sites-24375011</guid>
            <pubDate>Fri, 04 Sep 2020 14:04:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[List of YouTube channels for improving web development and programming skills]]>
            </title>
            <description>
<![CDATA[
Score 268 | Comments 92 (<a href="https://news.ycombinator.com/item?id=24374979">thread link</a>) | @bojanvidanovic
<br/>
September 4, 2020 | https://devandgear.com/posts/the-ultimate-list-of-youtube-channels-to-boost-your-web-development-and-programming-skills/ | <a href="https://web.archive.org/web/*/https://devandgear.com/posts/the-ultimate-list-of-youtube-channels-to-boost-your-web-development-and-programming-skills/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
<header>
<figure>
<img data-srcset="https://source.unsplash.com/4-EeTnaC1S4/480 480w,
                  https://source.unsplash.com/4-EeTnaC1S4/640 640w,
                  https://source.unsplash.com/4-EeTnaC1S4/960 960w,
                  https://source.unsplash.com/4-EeTnaC1S4/1280 1280w" data-sizes="auto" src="https://source.unsplash.com/4-EeTnaC1S4/50x0" alt="A man watching his laptop">
<figcaption>
<a href="https://unsplash.com/@sickhews" target="_blank">
Wes Hicks
</a>
</figcaption>
</figure>
<div>
<p><time>
Sep 4, 2020
</time></p>
<p>These YouTubers are followed by thousands for their high-quality programming tutorials, find your favorite channel in this ultimate list to learn and boost your web development skills today.</p>
<p>
<span>in
<a href="https://devandgear.com/categories/learning"><b>Learning</b></a>
</span>
</p>
</div>
</header>
<div>
<div>
<p>Reading and listening to someone might not be the same, especially when learning
hard technical topics. With the rise of developers turned YouTubers, there is
a vast choice of technical videos from all areas of software development
available on YouTube. With that in mind, it has never been easier to learn the web
development from scratch or just boost the level of your skills thanks to the
enormous YouTube community.</p>
<p>But finding the right YouTube channel might not be easy, so we created an
ultimate list of channels for you to follow and start learning today.</p>
<p>Don’t forget to share, and let us know in the comments if your
favorite YouTube channel is not listed.</p>
<ul>
<li><a href="https://www.youtube.com/c/DevEd">Dev Ed</a></li>
<li><a href="https://www.youtube.com/WebDevSimplified">Web Dev Simplified</a></li>
<li><a href="https://www.youtube.com/c/TheNetNinja">The Net Ninja</a></li>
<li><a href="https://www.youtube.com/c/TraversyMedia">Traversy Media</a></li>
<li><a href="https://www.youtube.com/c/BenAwad97">Ben Awad</a></li>
<li><a href="https://www.youtube.com/c/coreyms">Corey Schafer</a></li>
<li><a href="https://www.youtube.com/aniakubow">Ania Kubów</a></li>
<li><a href="https://www.youtube.com/channel/UCvI5azOD4eDumpshr00EfIw">Codú Community</a></li>
<li><a href="https://www.youtube.com/channel/UCd-EhXGbXSozuzsAAdPIn3A">Tuts+ Code</a></li>
<li><a href="https://www.youtube.com/channel/UCVTlvUkGslCV_h-nSAId8Sw">LearnCode.academy</a></li>
<li><a href="https://www.youtube.com/channel/UCADyUOnhyEoQqrw_RrsGleA">Chris Coyier</a></li>
<li><a href="https://www.youtube.com/c/webcrunch">Web-Crunch</a></li>
<li><a href="https://www.youtube.com/channel/UCyU5wkjgQYGRB0hIHMwm2Sg">LevelUpTuts</a></li>
<li><a href="https://www.youtube.com/channel/UCO1cgjhGzsSYb1rsB4bFe4Q">FunFunFunction</a></li>
<li><a href="https://www.youtube.com/channel/UCxSITxL2JbF229OGCqieVZw">Garrett Love</a></li>
<li><a href="https://www.youtube.com/channel/UCpOIUW62tnJTtpWFABxWZ8g">Codecourse</a></li>
<li><a href="https://www.youtube.com/florinpop">Florin Pop</a></li>
<li><a href="https://www.youtube.com/channel/UCwRXb5dUK4cvsHbx-rGzSgw">Derek Banas</a></li>
<li><a href="https://www.youtube.com/channel/UCoebwHSTvwalADTJhps0emA">Wes Bos</a></li>
<li><a href="https://www.youtube.com/user/thenewboston">thenewboston</a></li>
<li><a href="https://www.youtube.com/user/williamprey">Alessandro Castellani</a></li>
<li><a href="https://www.youtube.com/channel/UCpzRDg0orQBZFBPzeXm1yNg">Adam Khoury</a></li>
<li><a href="https://www.youtube.com/channel/UCt0ya0xGvXu01zfXjNrGqzg">flavio</a></li>
<li><a href="https://www.youtube.com/channel/UCnUYZLuoy1rq1aVMwx4aTzw">Google Chrome Developers</a></li>
<li><a href="https://www.youtube.com/channel/UCRxWW_Ncs308nW4An23Yeig/">Simple Programmer</a></li>
<li><a href="https://www.youtube.com/channel/UChByJR-sX8CooIAc5nkV7Mg">Matt Stauffer</a></li>
<li><a href="https://www.youtube.com/channel/UCvM5YYWwfLwpcQgbRr68JLQ">Adria Twarog</a></li>
<li><a href="https://www.youtube.com/kepowob">Kevin Powell</a></li>
<li><a href="https://www.youtube.com/channel/UC-T8W79DN6PBnzomelvqJYw">James Q Quick</a></li>
<li><a href="https://www.youtube.com/c/SteveGriffith-Prof3ssorSt3v3">Steve Griffith</a></li>
<li><a href="https://www.youtube.com/user/programmingwithmosh">Programmming with Mosh</a></li>
<li><a href="https://www.youtube.com/user/CalebTheVideoMaker2">Caleb Curry</a></li>
<li><a href="https://www.youtube.com/channel/UC4JX40jDee_tINbkjycV4Sg">Tech With Tim</a></li>
<li><a href="https://www.youtube.com/channel/UC8A0M0eDttdB11MHxX58vXQ">Meth Meth Method</a></li>
<li><a href="https://www.youtube.com/channel/UCrqAGUPPMOdo0jfQ6grikZw">Colt Steele</a></li>
<li><a href="https://www.youtube.com/user/shiffman">The Coding Train</a></li>
<li><a href="https://www.youtube.com/user/andrewjosephmead1">Andrew Mead</a></li>
<li><a href="https://www.youtube.com/channel/UCHkqtrnQO2HMyW50ixOtJGw">This Dot Media</a></li>
<li><a href="https://www.youtube.com/c/KentCDodds-vids">Kent C. Dodds</a></li>
<li><a href="https://www.youtube.com/user/Weibenfalk">Weibenfalk</a></li>
<li><a href="https://www.youtube.com/channel/UCtb40EQj2inp8zuaQlLx3iQ">Andre Madarang</a></li>
<li><a href="https://www.youtube.com/channel/UC8butISFwT-Wl7EV0hUK0BQ">freeCodeCamp.org</a></li>
<li><a href="https://www.youtube.com/channel/UCJq6AEgtWeZt7ziQ-fLKOeA">Prismic</a></li>
<li><a href="https://www.youtube.com/channel/UCWr0mx597DnSGLFk1WfvSkQ">Kalle Hallden</a></li>
<li><a href="https://www.youtube.com/channel/UCtxCXg-UvSnTKPOzLH4wJaQ">Coding Tech</a></li>
<li><a href="https://www.youtube.com/channel/UCsBjURrPoezykLs9EqgamOA">Fireship</a></li>
<li><a href="https://www.youtube.com/channel/UCRLEADhMcb8WUdnQ5_Alk7g">Aaron Jack</a></li>
<li><a href="https://www.youtube.com/user/irideabanana">uidotdev</a></li>
<li><a href="https://www.youtube.com/channel/UCDCHcqyeQgJ-jVSd6VJkbCw">codeSTACKr</a></li>
<li><a href="https://www.youtube.com/channel/UCsUalyRg43M8D60mtHe6YcA">Honeypot</a></li>
<li><a href="https://www.youtube.com/unsureprogrammer">Unsure Programmer</a></li>
<li><a href="https://www.youtube.com/channel/UCJUmE61LxhbhudzUugHL2wQ">codedamn</a></li>
<li><a href="https://www.youtube.com/channel/UC80PWRj_ZU8Zu0HSMNVwKWw">Codevolution</a></li>
<li><a href="https://www.youtube.com/channel/UC8n8ftV94ZU_DJLOLtrpORA">Code Explained</a></li>
<li><a href="https://www.youtube.com/user/DesignCourse">DesingCourse</a></li>
<li><a href="https://www.youtube.com/simplesnippets">Simple Snippets</a></li>
<li><a href="https://www.youtube.com/channel/UCxA99Yr6P_tZF9_BgtMGAWA">Faraday Academy</a></li>
<li><a href="https://www.youtube.com/c/SteveSchoger">Steve Schoger</a></li>
<li><a href="https://www.youtube.com/channel/UCDrekHmOnkptxq3gUU0IyfA">Devon Crawford</a></li>
<li><a href="https://www.youtube.com/user/javaboynavin">Telusko</a></li>
<li><a href="https://www.youtube.com/c/ackzell/videos">ackzell</a></li>
<li><a href="https://www.youtube.com/c/Academind">Academind</a></li>
<li><a href="https://www.youtube.com/channel/UCQI-Ym2rLZx52vEoqlPQMdg">iCoder’s Tape</a></li>
<li><a href="https://www.youtube.com/user/IAmTimCorey">IAmTimCorey</a></li>
<li><a href="https://www.youtube.com/user/CodingEntrepreneurs">CodingEntrepreneurs</a></li>
<li><a href="https://www.youtube.com/codinggardenwithcj">Coding Garden with CJ</a></li>
<li><a href="https://www.youtube.com/channel/UCksTNgiRyQGwi2ODBie8HdA">Adam Bien</a></li>
<li><a href="https://www.youtube.com/SimonHoiberg">Simon Hoiberg</a></li>
<li><a href="https://www.youtube.com/channel/UC54NcJvLCvM2CNaBjd5j6HA">RealToughCandy</a></li>
<li><a href="https://www.youtube.com/c/QuickProgramming">Quick programmming</a></li>
<li><a href="https://www.youtube.com/user/TheCharmefis">Dani Krossing</a></li>
<li><a href="https://www.youtube.com/c/RodtheITGuy">Rod the IT guy</a></li>
<li><a href="https://www.youtube.com/channel/UCjX0FtIZBBVD3YoCcxnDC4g">dcode</a></li>
<li><a href="https://www.youtube.com/user/hiteshitube">Hitesh Choudhary</a></li>
<li><a href="https://www.youtube.com/c/Lengstorf">Jason Lengstorf</a></li>
<li><a href="https://www.youtube.com/channel/UCbwXnUipZsLfUckBPsC7Jog">Online Tutorials</a></li>
<li><a href="https://www.youtube.com/c/WrongAkram">Wrong Akram</a></li>
<li><a href="https://www.youtube.com/user/binarythistle">Les Jackson</a></li>
<li><a href="https://www.youtube.com/cleverprogrammer">Clever Programmer</a></li>
<li><a href="https://www.youtube.com/user/Conutant">Tyler Moore</a></li>
<li><a href="https://www.youtube.com/user/GeekyShow1">Geeky Shows</a></li>
<li><a href="https://www.youtube.com/channel/UC-Zcse8tC53G34Uo4kzLeAg">Chris Blakely</a></li>
<li><a href="https://www.youtube.com/channel/UCNFmBuclxQPe57orKiQbyfA">Tanay Pratap</a></li>
<li><a href="https://www.youtube.com/techsithtube">techsith</a></li>
<li><a href="https://www.youtube.com/hswolff">Harry Wolff</a></li>
<li><a href="https://www.youtube.com/c/FKnight">ForrestKnight</a></li>
<li><a href="https://www.youtube.com/user/Tychos1">Joshua Fluke</a></li>
<li><a href="https://www.youtube.com/c/sentdex">sentdex</a></li>
<li><a href="https://www.youtube.com/BeforeSemicolon">Before Semicolon</a></li>
</ul>

</div>

</div>

<section>
<section id="comments">
<h2>Comments</h2>



</section>
</section>
</article></div>]]>
            </description>
            <link>https://devandgear.com/posts/the-ultimate-list-of-youtube-channels-to-boost-your-web-development-and-programming-skills/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24374979</guid>
            <pubDate>Fri, 04 Sep 2020 13:59:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Snake in a QR Code]]>
            </title>
            <description>
<![CDATA[
Score 266 | Comments 79 (<a href="https://news.ycombinator.com/item?id=24374570">thread link</a>) | @phit_
<br/>
September 4, 2020 | https://itsmattkc.com/etc/snakeqr/ | <a href="https://web.archive.org/web/*/https://itsmattkc.com/etc/snakeqr/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p>The above QR code contains a complete Windows executable that will run on Windows 7 and up. It's a simple but fully playable implementation of Snake.</p>
<p>A QR code is a data storage medium like any other, and while it's almost always used to store ASCII text, it can also be used to store binary data. As such, virtually any computer data can theoretically be stored in a QR code provided it can fit within the size limitations.</p>
<p>A QR code comes in various standard sizes, the largest being version 40 which can store up to 2,953 bytes (roughly 2.9 KB) of binary data. The above code is not quite that large, it's only storing around 1.4 KB of data (shrunk down from 3.2 KB with <a href="http://www.crinkler.net/" target="_blank">Crinkler</a>, slightly larger than the executable shown in the video because I decided to add a little more functionality).</p>

<p><b>Method 1: Webcam (automatic)</b></p>
<p>You can treat this QR code much like a QR code in the real world by using your computer's webcam to read it (e.g. after loading this page on a phone or printing the QR code out). For this, I recommend using zbarcam. I've included a download link with everything you need:</p>
<ol>
    <li><a href="https://itsmattkc.com/etc/snakeqr/zbarcam.zip">Download zbarcam</a></li>
    <li>Open "Read Snake From QR.bat". After a moment, your webcam will activate.</li>
    <li>Using either your phone screen or a print-out, hold the QR code up to the camera.</li>
    <li>Once it recognizes the code, it will close and Snake will appear, freshly read from the QR code.</li>
</ol>
<p><b>Method 2: Webcam (manual)</b></p>
<p>This method is the same as the above, however if you'd rather input the commands manually than running my script, go for it:</p>
<ol>
    <li><a href="https://itsmattkc.com/etc/snakeqr/zbarcam.zip">Download zbarcam</a></li>
    <li>Open Command Prompt</li>
    <li><span>cd</span> to the zbarcam folder.</li>
    <li>Run the following command:
        <p>zbarcam --raw --oneshot -Sbinary &gt; snake.exe</p>
        <p>The arguments used are as follows:</p>
        <ul>
            <li><span>--raw</span> - Disables any character encoding conversion.</li>
            <li><span>--oneshot</span> - Stops reading and closes zbarcam after the code has been successfully read.</li>
            <li><span>-Sbinary</span> - Informs zbarcam that the data in the code is binary data.</li>
        </ul>
    </li>
    <li>Using either your phone screen or a print-out, hold the QR code up to the camera.</li>
    <li>Once it recognizes the code, it will close and an executable called "snake.exe" will appear in the same folder.</li>
    <li>Open "snake.exe".</li>
</ol>

<ul>
    <li>The version of zbarcam provided above has been patched to correctly output binary data on Windows. This project exposed a bug in zbarcam where binary data was incorrectly treated as text, inadvertently corrupting it as Windows attempted to convert LF line endings to CRLF. The fix used here has since been merged into zbar's master source code, but as of July 2020 no stable release version has included the fix yet.</li>
    <li>The version of zbarcam provided above has been compiled with DirectShow support for greater compatibility with webcam devices. In my video, I use a build that has not been compiled for DirectShow (using VfW instead) which leads to a black and white webcam feed. This build provides a feed in color.</li>
</ul>

<p>As a proof-of-concept, I wrote an implementation of Snake in JavaScript that, when minified, could also fit into the maximum QR code size which I briefly showcased in the video. Some people were interested in playing this, so <a href="https://itsmattkc.com/etc/snakeqr/snake-minified.html">it is available here</a>.</p>

<p>
    • <a href="https://en.wikipedia.org/wiki/QR_code">QR code on Wikipedia</a><br>
    • <a href="https://github.com/mchehab/zbar">ZBar on GitHub</a><br>
</p>
</div></div>]]>
            </description>
            <link>https://itsmattkc.com/etc/snakeqr/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24374570</guid>
            <pubDate>Fri, 04 Sep 2020 13:08:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Bread, How Did They Make It? Addendum: Rice]]>
            </title>
            <description>
<![CDATA[
Score 100 | Comments 50 (<a href="https://news.ycombinator.com/item?id=24373939">thread link</a>) | @Kednicma
<br/>
September 4, 2020 | https://acoup.blog/2020/09/04/collections-bread-how-did-they-make-it-addendum-rice/ | <a href="https://web.archive.org/web/*/https://acoup.blog/2020/09/04/collections-bread-how-did-they-make-it-addendum-rice/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>As an addendum on to our four-part look at the general structures of the farming of cereal grains (<a href="https://acoup.blog/2020/07/24/collections-bread-how-did-they-make-it-part-i-farmers/">I</a>, <a href="https://acoup.blog/2020/07/31/collections-bread-how-did-they-make-it-part-ii-big-farms/">II</a>, <a href="https://acoup.blog/2020/08/06/collections-bread-how-did-they-make-it-part-iii-actually-farming/">III</a>, <a href="https://acoup.blog/2020/08/21/collections-bread-how-did-they-make-it-part-iv-markets-and-non-farmers/">IV</a>) this post is going to briefly discuss some of the key ways that the structures of <strong><em>rice</em> <em>farming </em></strong>differ from the structures of wheat and barley farming.  We’ll start with some of the key differences in the mechanics of rice farming itself, before moving into how those differences might motivate different social and economic organization in the countryside, before finally discussing some of the theories as to how rice farming might – or might not – impact larger structures like the state.</p>



<p>As always, if you like what you are reading here, please share it. If you want updates whenever a new post appears, you can click the button below for email updates or follow me on twitter (@BretDevereaux) for updates as to new posts, as well as my occasional ancient history, foreign policy or pop-culture thoughts. And if you want to ensure that I can continue to get my rice, you can support me on <a href="https://www.patreon.com/user?u=20122096">Patreon</a>.</p>






<p>(Quick bibliographic aside: I relied primary for this on Hsu, <em>Han Agriculture: The Formation of Early Chinese Agrarian Economy</em> (206 B.C. – A.D. 220) (1980) and F. Bray, <em>The Rice Economies: Technology and Development in Asian Societies </em>(1986) which were recommended to me by specialists in the field.  The latter is a wealth of technical details on rice cultivation, although it is as focused on the transition to mechanization and modern agriculture as to the conditions of pre-modern rice cultivation.)</p>



<h2>Rice Cultivation</h2>



<p>We want to start with the rice cultivation process.  There are a <em>lot</em> of varieties of rice out there, but the key divide we want to make early is between dry-rice and wet-rice.  When we’re talking about ‘rice cultures’ or ‘rice agriculture,’ generally, we mean wet-rice farming, where the rice is partially submerged during its growing.  Wild rice, as far as we can tell, began as a swamp-grass and thus likes to have quite a lot of water around, although precisely controlling the water availability can lead the rice to be a lot more productive than it would be in its natural habitat.  While there are varieties of rice which can be (and are) farmed ‘dry’ (that is, in unflooded fields much like wheat and barley are farmed), the vast majority of rice farming is ‘wet.’  As with grains, this is not merely a matter of different methods of farming, but of different varieties of rice that have been adapted to that farming; varieties of dry-rice and wet-rice have been selectively bred over millennia to perform best in those environments.</p>



<p>Wet-rice is farmed in paddies, small fields (often <em>very</em> small – some Chinese agronomists write that the ideal size for an individual rice field is around 0.1 hectare, which is just 0.24 acres) surrounded by low ‘bunds’ (small earthwork walls or dykes) to keep in the water, typically around two feet high.  Because controlling the water level is crucial, rice paddies must be very precisely flat, leading to even relatively gentle slopes often being terraced to create a series of flat fields.  Each of these rice paddies (and there will be many because they are so small) are then connected by irrigation canals which channel and control the water in what is often a quite complex system.</p>



<figure><img data-attachment-id="4363" data-permalink="https://acoup.blog/535232001/" data-orig-file="https://acoupdotblog.files.wordpress.com/2020/09/535232001.jpg" data-orig-size="1667,2500" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="535232001" data-image-description="" data-medium-file="https://acoupdotblog.files.wordpress.com/2020/09/535232001.jpg?w=200" data-large-file="https://acoupdotblog.files.wordpress.com/2020/09/535232001.jpg?w=683" src="https://acoupdotblog.files.wordpress.com/2020/09/535232001.jpg?w=683" alt="" srcset="https://acoupdotblog.files.wordpress.com/2020/09/535232001.jpg?w=683 683w, https://acoupdotblog.files.wordpress.com/2020/09/535232001.jpg?w=1366 1366w, https://acoupdotblog.files.wordpress.com/2020/09/535232001.jpg?w=100 100w, https://acoupdotblog.files.wordpress.com/2020/09/535232001.jpg?w=200 200w, https://acoupdotblog.files.wordpress.com/2020/09/535232001.jpg?w=768 768w" sizes="(max-width: 683px) 100vw, 683px"><figcaption><a href="https://www.britishmuseum.org/collection/object/A_1915-0823-0-749">Via the British Museum</a>, a Japanese color woodblock print (1855) showing rice farmers working in the fields.</figcaption></figure>



<p>The exact timing of rice production is more complex than wheat because a single paddy often sees two crops in a year and the exact planting times vary between areas;  one common cycle on the Yangtze is for a February planting (with a June harvest) followed by a June planting (with a November harvest).  In other areas, paddies planted with rice during the first planting might be drained and sown with a different plant entirely (sometimes including wheat) in the intervening time.</p>



<p>The cycle runs thusly: after the heavy rains of the monsoons (if available), the field is tilled (or plowed, but as we’ll see, manual tillage is often more common).  The seed is then sown (or transplanted) and the field is, using the irrigation system, lightly flooded, so that the young seedlings grow in standing water.  Sometimes the seed is initially planted in a dedicated seed-bed and then transferred to the field, rather than being sown there directly; doing so has a positive impact on yields, but is substantially more labor intensive.  The water level is raised as the plant grows; agian this is labor intensive, but increases yields.  Just before the harvest the fields are drained out and allowed to dry out, before the crop is harvested and then goes into processing.</p>



<p>Rice is threshed much like grain (more often manually threshed and generally not threshed with flails) to release the seeds, the individual rice grains, from the plant.  That is going to free the endosperm of the speed, along with a hull around it and a layer of bran between the two.  Hulling was traditionally done by hand-pounding, which frees the seed from the hull, leaving just the endosperm and some of the bran; this is how you get <em>brown</em> rice, which is essentially ‘whole-grain’ rice.  While it is generally less <em>tasty</em>, the bran actually has quite a lot of nutrients not present in the calorie-rich endosperm.  Whereas <em>white rice</em> is produced by then milling or polishing away the bran to produce a pure, white kernal of the endosperm; it is very tasty, but lacks many of the vitamins that brown rice has.</p>



<p>Consequently, while a diet of mostly brown rice can be healthy, a diet overwhelmingly of <em>white rice</em> leads to <a href="https://en.wikipedia.org/wiki/Thiamine_deficiency">Thiamine deficiency</a>, known colloquially as <em>beriberi</em>.  My impression from the literature is that this wasn’t as much an issue prior to the introduction of mechanical milling processes for rice. Mechanical milling made producing white rice in quantity cheap and so it came to dominate the diet to the exclusion of brown rice, producing negative health effects for the poor who could not afford to supplement their rice-and-millet diet with other foods, or for soldiers whose ration was in rice.  But prior to that mechanical milling, brown rice was all that was available for the poor, which in turn meant less Thiamine deficiency among the lower classes of society.</p>



<h2>Farmer Terrain</h2>



<p>Because rice is such a different crop than wheat or barley, there are a lot of differences in the way that rice cultivation shapes the countryside.  We’ll move here from the relatively direct impacts on the organization of farmers and then discuss the more speculative impacts on the organization of whole societies.</p>



<p>The thing to note about <strong>rice is that it is both <em>much</em> more productive on a per-acre basis than wheat or barley, but also <em>much</em> more labor intensive</strong>; it also relies on different forms of capital to be productive.  Whole-grain wheat and brown rice have similar calorie and nutritional value (brown rice is somewhat better in most categories) on a unit-weight basis (so, per pound or ton), but the yield difference is fairly large: rice is typically around (very roughly) 50% more productive per acre than wheat.  Moreover, rice plants have a more favorable ratio of seeds-to-plants, meaning that the demand to put away seeds for the next harvest is easier – whereas crop-to-seed ratios on pre-modern wheat range from 3:1 to 10:1, rice can achieve figures as high as 100:1.  As a result, not only is the gross yield higher (that is, more tons of seed per field) but a lower percentage of that seed has to be saved for the next planting.</p>



<figure><img data-attachment-id="4365" data-permalink="https://acoup.blog/1613316772/" data-orig-file="https://acoupdotblog.files.wordpress.com/2020/09/1613316772.jpg" data-orig-size="1565,1406" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="1613316772" data-image-description="" data-medium-file="https://acoupdotblog.files.wordpress.com/2020/09/1613316772.jpg?w=300" data-large-file="https://acoupdotblog.files.wordpress.com/2020/09/1613316772.jpg?w=1024" src="https://acoupdotblog.files.wordpress.com/2020/09/1613316772.jpg?w=1024" alt="" srcset="https://acoupdotblog.files.wordpress.com/2020/09/1613316772.jpg?w=1024 1024w, https://acoupdotblog.files.wordpress.com/2020/09/1613316772.jpg?w=150 150w, https://acoupdotblog.files.wordpress.com/2020/09/1613316772.jpg?w=300 300w, https://acoupdotblog.files.wordpress.com/2020/09/1613316772.jpg?w=768 768w, https://acoupdotblog.files.wordpress.com/2020/09/1613316772.jpg 1565w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><a href="https://www.britishmuseum.org/collection/object/A_1939-0311-0-6-56">Via the British Museum</a>, a map made of Kaliwungi, Java (1807) showing a collection of rice fields.  Of particular note is how small each individual field is (compare the village on the right-hand side!).</figcaption></figure>



<p><strong>At the same time, the irrigation demands for effective production of wet-rice requires a <em>lot</em> of labor to build and maintain</strong>.  Fields need to be flooded and drained; in some cases (particularly pre-modern terrace farming) this may involve moving the water manually, in buckets, from lower fields to higher ones.  Irrigation canals connecting paddies can make this job somewhat easier, as can bucket-lifts (there’s a picture of a simple one below), but that still demands moving quite a lot of water.  In any irrigation system, the bunds need to be maintained and the water level carefully controlled, with also involves potentially quite a lot of labor.</p>



<p>The consequence of all of this is that while the rice farming household seems to be roughly the same size as the wheat-farming household (that is, an extended family unit of variable size, but typically around 8 or so members), <strong>the farm is much smaller</strong>, with common household farm sizes, even in the modern period, clustering around 1 hectare (2.47 acres) in comparison to the standard household wheat farms clustered around 4-6 acres (which, you may note with the yield figures above, lands us right back at around the same subsistence standard).</p>



<p>Moreover, rice cultivation is less soil dependent (but more water dependent) because wet-rice farming both encourages nitrogen fixation in the soil (maintaining the fertility of it generally without expensive manure use) and because rice farming leads naturally to a process known as <a href="https://en.wikipedia.org/wiki/Podzol">pozdolisation</a>, slowly converting the underlying soil over a few years to a set of characteristics which are more favorable for more rice cultivation.  So whereas with wheat cultivation, where you often have clumps of marginal land (soil that is too wet, too dry, too rocky, too acidic, too uneven, too heavily forested, and so on), rice cultivation tends to be able to make use of almost any land where there is sufficient water (although terracing may be needed to level out the land).  The reliance on the rice itself to ‘terraform’ its own fields does mean that new rice fields tended to under-produce for the first few years.</p>



<figure><img data-attachment-id="4357" data-permalink="https://acoup.blog/terrace_field_yunnan_china_denoised/" data-orig-file="https://acoupdotblog.files.wordpress.com/2020/09/terrace_field_yunnan_china_denoised.jpg" data-orig-size="2000,1295" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="terrace_field_yunnan_china_denoised" data-image-description="" data-medium-file="https://acoupdotblog.files.wordpress.com/2020/09/terrace_field_yunnan_china_denoised.jpg?w=300" data-large-file="https://acoupdotblog.files.wordpress.com/2020/09/terrace_field_yunnan_china_denoised.jpg?w=1024" src="https://acoupdotblog.files.wordpress.com/2020/09/terrace_field_yunnan_china_denoised.jpg?w=1024" alt="" srcset="https://acoupdotblog.files.wordpress.com/2020/09/terrace_field_yunnan_china_denoised.jpg?w=1024 1024w, https://acoupdotblog.files.wordpress.com/2020/09/terrace_field_yunnan_china_denoised.jpg?w=150 150w, https://acoupdotblog.files.wordpress.com/2020/09/terrace_field_yunnan_china_denoised.jpg?w=300 300w, https://acoupdotblog.files.wordpress.com/2020/09/terrace_field_yunnan_china_denoised.jpg?w=768 768w, https://acoupdotblog.files.wordpress.com/2020/09/terrace_field_yunnan_china_denoised.jpg 2000w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><a href="https://en.wikipedia.org/wiki/Rice_production_in_China#/media/File:Terrace_field_yunnan_china_denoised.jpg">Via Wikipedia</a>, terraced rice paddies in Yunnan, China, showing how uneven land can be terraced to create level fields …</figcaption></figure></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://acoup.blog/2020/09/04/collections-bread-how-did-they-make-it-addendum-rice/">https://acoup.blog/2020/09/04/collections-bread-how-did-they-make-it-addendum-rice/</a></em></p>]]>
            </description>
            <link>https://acoup.blog/2020/09/04/collections-bread-how-did-they-make-it-addendum-rice/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24373939</guid>
            <pubDate>Fri, 04 Sep 2020 11:21:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[MongoDB History]]>
            </title>
            <description>
<![CDATA[
Score 32 | Comments 47 (<a href="https://news.ycombinator.com/item?id=24373439">thread link</a>) | @jaysonqpt
<br/>
September 4, 2020 | https://www.quickprogrammingtips.com/mongodb/mongodb-history.html | <a href="https://web.archive.org/web/*/https://www.quickprogrammingtips.com/mongodb/mongodb-history.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
							
				<p>When it comes to modern web application development, MongoDB is the king. If you are a full stack programmer, you hear about MERN or MEAN stacks every day. The <b>M</b> is in every one of them and it stands for MongoDB. The free and open source community version of MongoDB powers a large number of Web applications. From its humble beginnings in 2007, MongoDB has come a long way. It is the primary product behind the company MongoDB Inc. with over 10 billion dollars in market capitalisation. Like many other products before and after, online advertising was the key catalyst behind the vision and development of MongoDB. The story of MongoDB is an interesting one and in this article, I will take you on a journey through the MongoDB land and its history.</p>
<h2>The Idea of the Humongous Database – The Beginnings</h2>
<p>The story of MongoDB has its beginnings much earlier than 2007. In 1995, <a href="https://en.wikipedia.org/wiki/Dwight_Merriman" target="_blank" rel="noopener noreferrer">Dwight Merriman</a> and Kevin O’Connor created the famous online advertising company DoubleClick. Kevin Ryan joined the team soon after(Dwight and Kevin later cofounded 5 companies together – Gilt, 10gen, Panther Express, ShopWiki and Business Insider). DoubleClick soon took off and within a few years it was serving as much as 400,000 ads/second. Such large scale traffic was not anticipated by the relational database technologies available at that time. Configuring relational databases for such scale also required substantial amount of money and hardware resources. So Dwight(who was the CTO at the time) and his team wrote custom database implementations to scale DoubleClick for the increased traffic. In one of his <a href="https://youtu.be/hOOQJpGu1kY?t=59" target="_blank" rel="noopener noreferrer">early talks on MongoDB</a>, Dwight talks about getting a network hardware with serial number 7 and wondering whether it will work! This was even before the invention of load balancers.</p>
<p><img src="https://www.quickprogrammingtips.com/wp-content/uploads/2020/08/mongodb-people.jpg" width="300px" alt="People behind MongoDB" title="People behind MongoDB">In 2003, <a href="https://en.wikipedia.org/wiki/Eliot_Horowitz" target="_blank" rel="noopener noreferrer">Eliot Horowitz</a> joined DoubleClick R&amp;D division as a software engineer immediately after his college. Within 2 years he left DoubleClick to start ShopWiki along with Dwight. Both of them realised that they were solving the same horizontal scalability issues again and again. So in 2007, Dwight, Eliot and Kevin Ryan started a new company called 10gen. 10gen was focused on creating a PaaS hosting solution with its own application and database stack. 10gen soon got the attention of the venture capitalist Albert Wenger (Union Square Ventures) and he invested $1.5 million into it. This is what <a href="https://www.usv.com/writing/2008/07/10gen/" target="_blank" rel="noopener noreferrer">Albert Wenger wrote in 2008 about the 10gen investment</a>,</p>
<blockquote><p>
Today we are excited to announce that we are backing a team working on an alternative, the amazingly talented folks at 10gen. They bring together experience in building Internet scale systems, such as DART and the Panther Express CDN, with extensive Open Source involvement, including the Apache Software Foundation. They are building an open source stack for cloud computing that includes an appserver and a database both written from scratch based on the capabilities of modern hardware and the many lessons learned in what it takes to build a web site or service. The appserver initially supports server side Javascript and (experimentally) Ruby. The database stores objects using an interesting design that balances fast random access with efficient scanning of collections.</p></blockquote>
<p>What Albert was refering to as “the database with interesting design” was in fact MongoDB. Rapid development on the new database was undertaken from 2007 to 2009. The <a href="https://github.com/mongodb/mongo/commit/e73188b5512c82290a4070af4afddac20d0b981e" target="_blank" rel="noopener noreferrer">first commit of the MongoDB database server by Dwight can be seen here</a>. The core engine was written in C++. The database was named MongoDB since the idea was to use it to store and serve humongous amount of data required in typical use cases such as content serving. Initially the team had only 4 engineers(including Dwight and Eliot) and decided to focus just on the MongoDB database instead of the initial PaaS product. The business idea was to release the database as an open source free download and offer commercial support and training services on top of it.</p>
<p><a href="https://github.com/mongodb/mongo/tree/v1.0" target="_blank" rel="noopener noreferrer">MongoDB 1.0 was released</a> in February 2009. The initial version focused on providing a usable query language with document model, indexing and basic support for replication. It also had experimental version of sharding, but production ready sharding clusters were available only in version 1.6 released a year later.</p>
<p>Here is <a href="https://groups.google.com/forum/#!topic/mongodb-user/X0Le3mBel2I/discussion" target="_blank" rel="noopener noreferrer">how Dwight responded to a question</a> on the suitability of Mongo for a highly scalable system (MongoDB user group – September 2009),</p>
<blockquote><p>
For horizontal scaling, one would use auto-sharding to build out large MongoDB clusters.  This is in alpha now, but if your project is just getting started, it will be in production by the time you need it.</p></blockquote>
<h2>Early MongoDB Design Philosophy</h2>
<p>In the early years, the basic design principles behind MongoDB development were the following,</p>
<ul>
<li>Quick and easy data model for faster programming – document model with CRUD.</li>
<li>Use of familiar language and format – JavaScript/JSON.</li>
<li>Schema less documents for agile iterative development.</li>
<li>Only essential features for faster development and easy scaling. No join, no transactions across collections.</li>
<li>Support easy horizontal scaling and durability/availability (replication/sharding).</li>
</ul>
<p>In his ZendCon 2011 presentation titled <a href="https://www.youtube.com/watch?v=hOOQJpGu1kY" target="_blank" rel="noopener noreferrer">“NoSQL and why we created MongoDB”</a>, Dwight talks about these principles in detail. Around <a href="https://youtu.be/hOOQJpGu1kY?t=2408" target="_blank" rel="noopener noreferrer">42 minutes mark</a>, there is also an interesting discussion on the difference between replication and sharding. As the database server code matured and once MongoDB hit the mainstream, many of these principles were obviously diluted. Latest MongoDB server versions support joining to some extend and since MongoDB 4.2, even distributed transactions are supported!</p>
<h2>What is MongoDB?</h2>
<p>Before we get into detailed MongoDB history and how it evolved over the years, let us briefly look at what exactly it is!</p>
<p>MongoDB is a document based NoSQL database. It can run on all major platforms (Windows, Linux, Mac) and the open source version is available as a free download. MongoDB stores data entities in a container called collection and each piece of data stored is in a JSON document format. For example, if a customer submits an online order, the entire details of that order (order number, order line items, delivery address etc.) are kept in a single hierarchical document in JSON format. It is then saved to a collection named “customer_order”.</p>
<p>MongoDB also comes with a console client called MongoDB shell. This is a fully functional JavaScript environment using which you can add, remove, edit or query document data in the database.</p>
<h2>MongoDB Architecture</h2>
<p>The following MongoDB architecture diagram provides a high level view of the major components in the MongoDB server.</p>
<p><img src="https://www.quickprogrammingtips.com/wp-content/uploads/2020/08/mongodb-architecture.jpg" width="800px" alt="MongoDB Architecture Diagram" title="MongoDB Architecture Diagram"></p>
<p>MongoDB currently offers <a href="https://docs.mongodb.com/drivers/" target="_blank" rel="noopener noreferrer">drivers for 13 languages</a> including Java, Node.JS, Python, PHP and Swift. The storage engine MMAPv1 is removed since version 4.2. The encrypted storage engine is only supported in the commercial enterprise server.</p>
<p>The beauty of MongoDB is that using the same open source free community server you can,</p>
<ul>
<li>Run a simple single machine instance suitable for most small applications.</li>
<li>Run a multi-machine instance with durability/high availability suitable for most business applications.</li>
<li>Run a large horizontally scaled cluster of machines(shard cluster) handling both very large sets of data and high volume of query traffic. MongoDB provides automatic infrastructure to distribute both data and its processing across machines. A typical use case would be running a popular ad service with thousands of customers and millions of impressions.</li>
</ul>
<p>The following diagrams show various options available for running MongoDB instances.</p>
<h2>Single Server/Fault Tolerant Setup</h2>
<p>For small applications, a single server setup is enough with frequent data backups. For installations that require fault tolerance, a replica set implementation can be done. In the fault tolerant deployment, usually there are 3 or more MongoDB instances. Only one of them work as the primary instance and if it fails, one of the other 2 secondaries takes over as the primary. The data is identical in all instances.</p>
<p><img src="https://www.quickprogrammingtips.com/wp-content/uploads/2020/08/mongodb-deployment1.jpg" width="800px" alt="Single server/fault tolerant setup" title="Single server/fault tolerant setup"></p>
<h2>Shard Cluster for Horizontal Scalability</h2>
<p>For a large database with both horizontal scalability and fault tolerance requirements, a MongoDB shard cluster is configured. As can be seen from the diagram below, minimum recommended number of machines for a fault tolerant shard cluster is 14! Each fault tolerant replica set in this case handles only a subset of the data. This data partitioning is automatically done by MongoDB engine.<br>
<img src="https://www.quickprogrammingtips.com/wp-content/uploads/2020/08/mongodb-deployment2.jpg" width="800px" alt="Shard cluster with partitioned data" title="Shard cluster with partitioned data"></p>
<p>When you download the latest version of MongoDB (4.4) and extract it, you will find that it contains only the following 3 main files,</p>
<ul>
<li><b>mongo</b> – MongoDB shell for interacting with your server using JavaScript based commands.</li>
<li><b>mongod</b> – The MongoDB main executable. This can run as a single database instance, as a database member of a sharded cluster or as a configuration server of a sharded cluster.</li>
<li><b>mongos</b> – A router application only needed for sharded horizontally scaled cluster of database servers.</li>
</ul>
<p>In a mac machine, the total size of these 3 executables is around 150MB. These are the only components you need for any type of MongoDB deployments! In a world of bloated software, this is a welcome change! This simplicity and elegance is what makes MongoDB so powerful and reliable.</p>
<h2>Evolution of MongoDB (2009 to 2020)</h2>
<p>MongoDB 1.0 was released in February 2009 and it had most of the basic query functionalities. MongoDB 1.2 was released in December 2009 and it introduced large scale data processing using map-reduce. Realising that MongoDB has good potential, 10gen quickly ramped up the team. MongoDB 1.4 (March 2010) introduced background index creation and MongoDB 1.6 (August 2010) introduced some major features such as production ready sharding for horizontal scaling, replica sets with automatic failover and IPv6 support.</p>
<p><img src="https://www.quickprogrammingtips.com/wp-content/uploads/2020/08/mongodb-timeline1.jpg" width="800px" alt="MongoDB 2009 to 2013" title="MongoDB 2009 to 2013"></p>
<p>By 2012, 10gen had 100 employees and the company started providing 24/7 support. MongoDB 2.2 release(August 2012) introduced aggregation pipeline enabling multiple data processing steps as a chain of …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.quickprogrammingtips.com/mongodb/mongodb-history.html">https://www.quickprogrammingtips.com/mongodb/mongodb-history.html</a></em></p>]]>
            </description>
            <link>https://www.quickprogrammingtips.com/mongodb/mongodb-history.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24373439</guid>
            <pubDate>Fri, 04 Sep 2020 09:52:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Reading mind and Visualizing thoughts using AI]]>
            </title>
            <description>
<![CDATA[
Score 35 | Comments 9 (<a href="https://news.ycombinator.com/item?id=24372774">thread link</a>) | @umermirzapk
<br/>
September 4, 2020 | https://thinkml.ai/ai-can-read-and-visualize-our-thoughts/ | <a href="https://web.archive.org/web/*/https://thinkml.ai/ai-can-read-and-visualize-our-thoughts/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://thinkml.ai/content/images/size/w300/2020/09/meditation-1000062.jpg 300w,
                            https://thinkml.ai/content/images/size/w600/2020/09/meditation-1000062.jpg 600w,
                            https://thinkml.ai/content/images/size/w1000/2020/09/meditation-1000062.jpg 1000w,
                            https://thinkml.ai/content/images/size/w2000/2020/09/meditation-1000062.jpg 2000w" sizes="(max-width: 800px) 400px,
                            (max-width: 1170px) 700px,
                            1400px" src="https://thinkml.ai/content/images/size/w2000/2020/09/meditation-1000062.jpg" alt="AI can Read and Visualize our thoughts">
            </figure>

            <section>
                <div>
                    <p>Imagine a computer-based system visualizing your thoughts and secret thoughts; yes, it's possible now by artificial intelligence assistance. Recent advancements in hardware innovation have re-energized technology. It becomes more accurate, authentic, can produce better sound, accurate visualization, and understanding of the location. Outstanding computer processors support computer to make a decision, plan outputs and don't repeat the mistake as they learn from it. The four scientists in Kyoto at Kyoto University did an exceptional experiment that exceeds the global expectations about such a dreamy truth. </p><p>Tomoyasu Horikawa, Guohua Shen, Yukiyasu Kamitani, and Kei Majima recently published their <a href="https://www.biorxiv.org/content/biorxiv/early/2017/12/30/240317.full.pdf">results in a paper </a>on a scientific research platform &nbsp;BioRxiv. They have done their experiment in ATR Computational Neuroscience Laboratories. Their study mainly exposes the decoding of human thoughts using an artificially intelligent system. &nbsp;The artificial intelligence system becomes so smart and real to duplicate human minds and show what they're thinking in their minds. &nbsp;Now scientists are successful in developing the next phase of AI machine, which uses human psychology. It is a leading technology from science fiction to merely science phenomena.</p><p>Below video is demonstration of Tomoyasu’s work. </p><figure><iframe width="480" height="270" src="https://www.youtube.com/embed/YrO1v7-KcXs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></figure><h3 id="machine-learning-is-helping-human-beings"><a href="https://thinkml.ai/artificial-intelligence-is-helping-common-people/">Machine Learning is helping human beings</a> </h3><p>Machine learning is in use since many years ago in the magnetic resonance imaging (MRI) technique. It helped generate a visual impact of human thinking in the form of binary mages or geographic shapes. The images obtained are of simple shapes or pixels. However, another prediction that the human brain thinks in unique patterns of image creation; different thinking levels, either with simple or complex components.</p><p>The idea of processing human thoughts is not new as we can see 1950s videos where doctors used to put contraptions on the human brain. It helps them to decipher the thoughts always running in their patient's minds. A British Serial <strong>Quatermass and the Pit </strong>shows a similar technique to translate the alien's thoughts.</p><p>The scientists from the United States, China, and Japan utilize this fantasy to change it into reality. They use <strong>functional magnetic resonance imaging (fMRI) </strong>strategy for better measurement of brain activity. The deep neural networks involved in replicating human brain functioning procedure.</p><h3 id="the-emergence-of-artificial-people-with-machine-learning-system">The emergence of Artificial People with Machine Learning System</h3><p>Machines come with consciousness and result in developing a direct relationship between humans and machines. The day is coming when robots not only talk but can laugh, fight, and be cruel with the power of advancement in technology. Some research laboratories are busy making a force that can sense its surrounding environment and show response according to it. They sense human thoughts running in their minds and visualize to others. It's beneficial to know and see the thoughts of those quality people who are extraordinary but can't speak.</p><p>Alan Turing said:</p><blockquote>‘’If a machine behaves as intelligently as a human being, it is as intelligent as a human being.’’</blockquote><h3 id="is-it-telepathy-or-something-advanced">Is it Telepathy or Something Advanced?</h3><p>The whole world is excited about the news of AI reading minds, but the reality is somewhat not as same as appearing to us. Machine learning is still unable to detect rightly what we think, feel, desire, wish, or need at a specific time. The science is aiming to decipher the images that subjects see for a period. Thus, machine learning uses a reconstruction of a visual filed algorithm for a visual demonstration of thoughts.</p><p>In the last years, scientists strive to create a system that can judge images based on their shapes, characters, and letters when a subject’s mind view them.</p><h3 id="kyoto-university-research-by-four-scientists">Kyoto University Research by Four Scientists</h3><p>The advances in the development of artificial intelligence also use a <strong>deep neural network </strong>to decode human thoughts. &nbsp;They aimed to develop a method that not only deciphers the images but also regenerate the same one. Therefore, the four efficient scientists at Kyoto University utilized the BioRxiv platform and <strong>Deep Neural Networks (DNN)</strong> as a proxy to share their findings of thought decoding. These neural systems help decode images of thoughts in the human brain and produce a visual prediction that resembles a person's thinking.</p><p>More generalized; the system should visualize the image about which its system is not trained previously after decoding the thoughts. &nbsp;It can see or covert human thoughts into images.</p><h3 id="how-does-ai-can-visualize-our-thinking-process">How Does AI Can Visualize our Thinking Process? </h3><p>The basis of this system is to scan the human brain deeply. For this purpose, the scientists introduce fMRI (Functional MRI) that scans the brain's core over traditional MRI that involves only monitoring brain activity. The unusual behavior of fMRI is its ability to track <strong>blood flow </strong>to the brain and <strong>brainwaves. </strong></p><p>It takes data from the scan and interprets what the subject was thinking when data was taken from it? The actual decoding occurs in the <strong>complex neural network </strong>that converts data into image format. But the humans have to rain the system first to get-go everything. They train the machine about the human brain thinking strategy that how it works and make a solution. It gets information from blood flow as it tracks blood's path, way, direction, and speed to reach the brain.</p><p>After it, the system starts producing images related to the information it gets by tracking blood flow. The process is only possible if the system comes with multilayers of <strong>deep neural networks. DNN</strong>involves the processing of information in image form. The information is then carried towards <strong>Deep Generator Network (DGN) Algorithm, </strong>which finally creates the image with high precision and accuracy. If we obtain images without using DGN, we can't get a high resolution or clear images of a different thought. DGN captures faces, eyes, and textual patterns and produces high-quality visual clues. With an efficient DGN algorithm, efficiency can be exceeded to 99%, and highly matching decoded images are obtained.</p><h3 id="the-methodology-of-the-experiment-deep-image-reconstruction">The methodology of the Experiment; Deep Image Reconstruction</h3><p>In simple words, the system consists of two necessary steps:</p><p>1. &nbsp;A subject sees an image, and the same image is shown to the AI system; thus, it can recreate its image.<br>2. &nbsp;After it, the subject’s brain thinks about that image, and AI recreates the same image by showing real-time response.</p><p>The scientists selected three subjects and showed them natural geometric shapes, artificial images, and other objects of varying characters. They repeated the same exposure of showing objects in varying periods for almost ten weeks and recorded their brainwaves. In <strong>experiment number one</strong>, they recorded the brain activity of the subject when seeing the particular object out of 25. In contrast, they measured brain activity by hiding the image and asked the brain to think about it (<strong>experiment number 2</strong>). </p><p>In both scenarios, they recorded the image created by artificial intelligence and then compared them to get staggering results. The filtered data is considered as a template of sorts. Then they used a decoder to identify the images via fMRI. They scanned brain activity and used the reverse-engineered method to produce an image of information obtained from thoughts. The scientists enabled <strong>deep learning networks </strong>to understand the brainwaves and decode its information to get output.</p><p>AI is so fast as it has the power of guessing about a particular problem. It asks and answers its questions at a rapid speed. &nbsp;</p><h3 id="ai-visualizing-static-thoughts">AI Visualizing Static Thoughts </h3><p>Results were outstanding as the computer reconstructed that image but with low accuracy. The quality of images in both experiments was fizzy and comparatively low in the second experiment case. The reason was that it's difficult for a brain to remember an image as identical as it is. It enables scientists to decode ''hieratical'' images with various colors, pixels, and formations. The system establishes, for instance, can predict a picture of a bird, a plant, or an animal. Technology goes on proceeding towards refinement for better implementation of a particular process. </p><h3 id="statement-of-kamitani-about-the-project">Statement of Kamitani About the Project</h3><p>Kamitani was excited about their mutual hard work and getting mind-blowing results. He talked to <a href="https://www.cnbc.com/make-it/">CNBC Make It</a>as:</p><blockquote>‘’We have been studying methods to reconstruct or recreate an image a person sees just by looking at its brain activity. Our previous method was to suppose that an image consists of pixels or simple shapes. But our brain processes visual information hierarchically extracting different levels of features or components of different complexities.''</blockquote><blockquote>'These neural networks or AI models can be used as a proxy for the human brain's hierarchical structure,' he added more. </blockquote><h3 id="ai-decoding-videos-you-re-watching-from-your-brain-activity">AI Decoding Videos You’re Watching from Your Brain Activity</h3><p>There is another group of scientists who published their research on ‘<strong>’Neural Encoding and Decoding with Deep Learning for Dynamic Natural Vision’’ </strong>in <em>Cerebral Cortex</em>. They developed deep learning algorithms to scan the human brain and its working principles to decode its information.</p><p>Their experiment was based on three women who watched videos of hundreds of unique categories for several hours, and the scientists recorded their brain activity. The videos were on birds, airplanes, and other related objects. They obtained images from a popular Neural Encoding and Decoding with Deep Learning for Dynamic Natural Vision. The scientists used a functional MRI machine that measures signals of activity in the visual cortex. As the women watched several clips, the deep learning algorithms predicted activity closely related to the actual brain activity. Dozens of brain parts respond to a specific thought; thus, the artificial system mimics natural brain working strategies and shows visible results. </p><p>It also helped scientists to recognize which part of the cortex is involved in thought …</p></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://thinkml.ai/ai-can-read-and-visualize-our-thoughts/">https://thinkml.ai/ai-can-read-and-visualize-our-thoughts/</a></em></p>]]>
            </description>
            <link>https://thinkml.ai/ai-can-read-and-visualize-our-thoughts/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24372774</guid>
            <pubDate>Fri, 04 Sep 2020 08:01:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Rust testing or verifying: Why not both?]]>
            </title>
            <description>
<![CDATA[
Score 135 | Comments 25 (<a href="https://news.ycombinator.com/item?id=24372760">thread link</a>) | @adreid
<br/>
September 4, 2020 | https://alastairreid.github.io/why-not-both/ | <a href="https://web.archive.org/web/*/https://alastairreid.github.io/why-not-both/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p>Dijkstra famously dissed testing by saying “Program testing can be used to show
the presence of bugs, but never to show their absence!”
As if you should choose one over the other.
I don’t see them as opposites but as complementary techniques that
should both be used to improve the quality of your code.</p>

<p>I am a big fan of formal verification.
Formal verification tools can be the best bug-finding tools you have
ever used.
And they can completely eliminate entire classes of bugs from your code.
But, formal verification won’t find bugs in your
specification; or in your assumptions about your dependencies; or in your
build/CI harness, etc. (See <a href="https://alastairreid.github.io/RelatedWork/papers/fonseca:ecs:2017">Fonseca</a> for more examples
of where testing has found bugs in formally verified systems.)</p>

<p>And, we all reluctantly agree that Dijkstra was right: even a thorough,
perfectly executed test plan can miss bugs.</p>

<p>So, for the last few months, I have been trying to have both.
We (my team and I at Google) have been reimplementing Jason Lingle’s
<a href="https://github.com/AltSysrq/proptest">proptest</a> property-testing library for use with Rust formal verification tools.
The original proptest lets you write test harnesses to test that your code
(probably) satisfies properties.
Today, we are releasing <a href="https://github.com/project-oak/rust-verification-tools">a reimplementation of the proptest interface</a>
that enables you to use exactly the same test harnesses to
formally verify that the properties hold.
So far, we have only tried this with the <a href="https://klee.github.io/">KLEE symbolic execution engine</a>
but our implementation is based on the verification
interface used in <a href="https://alastairreid.github.io/verification-competitions/">verification competitions</a> so it should be
possible to port our library to many other verification tools.<sup id="fnref:for-Rust" role="doc-noteref"><a href="#fn:for-Rust">1</a></sup></p>

<p><em>[Before I go any further, I should mention that what we are <a href="https://github.com/project-oak/rust-verification-tools">releasing this
week</a> is a very early research prototype.  It is not ready for serious use
and it is definitely not an official, supported Google product.  We are
releasing it now, in its current, immature state because we want to have
a conversation about how programmers want to formally verify Rust code
and we think that it is helpful to have something to push against.
We welcome pull requests that add support for other verifiers, or that
push the design in a better direction.]</em></p>

<h2 id="a-proptest-test-harness">A proptest test harness</h2>

<p>To get an idea for what property testing looks like in <code>proptest</code>,
we’ll look at an example from the <a href="https://altsysrq.github.io/proptest-book/proptest/tutorial/macro-proptest.html">proptest book</a>.</p>

<div><div><pre><code>fn add(a: i32, b: i32) -&gt; i32 {
    a + b
}

proptest! {
    #[test]
    fn test_add(a in 0..1000i32, b in 0..1000i32) {
        let sum = add(a, b);
        assert!(sum &gt;= a);
        assert!(sum &gt;= b);
    }
}
</code></pre></div></div>

<p>This example defines a property called <code>test_add</code> that tests a function called
<code>add</code>.  In particular, it checks that, for non-negative values <code>a</code> and <code>b</code>, the
result of <code>sum(a, b)</code> is at least as large as <code>a</code> and as <code>b</code>.</p>

<p>The notation <code>0..1000i32</code> represents the set of all values in the range <code>[0 ..
1000)</code> and the notation <code>a in 0..1000i32</code> says that a value <code>a</code> should be
chosen from that set.</p>

<p>The <code>proptest!</code> macro converts this property into a test function
that repeatedly generates random values for <code>a</code> and <code>b</code> and executes the
body of the property.</p>

<p>(After adding in <a href="https://altsysrq.github.io/proptest-book/proptest/tutorial/macro-proptest.html">some additional glue code</a>) we can run this example with the command</p>



<p>which produces the following output</p>

<div><div><pre><code>running 1 test
test test_add ... ok

test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out
</code></pre></div></div>

<p>and, if we deliberately write a property that doesn’t hold, then <code>cargo test</code>
produces output more like this</p>

<div><div><pre><code>running 1 test
test test_add ... FAILED

failures:

---- test_add stdout ----
thread 'test_add' panicked at 'assertion failed: sum &gt;= a + 100', src/main.rs:11:9
thread 'test_add' panicked at 'Test failed: assertion failed: sum &gt;= a + 100; minimal failing input: a = 0, b = 0
        successes: 0
        local rejects: 0
        global rejects: 0
', src/main.rs:7:1


failures:
    test_add

test result: FAILED. 0 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out
</code></pre></div></div>

<h2 id="verifying-with-propverify">Verifying with propverify</h2>

<p>Proptest checks the above property by generating random values and testing.
But we can also interpret the property as saying that for all values
<code>a</code> and <code>b</code> in the sets, the body of the property will not panic.
That is, we can interpret it as a universally quantified specification.</p>

<p>To verify the above property, we use a script that compiles the Rust
code, invokes the <a href="https://klee.github.io/">KLEE symbolic execution engine</a> and filters
the output of KLEE to determine whether there is any choice of <code>a</code> and
<code>b</code> that can cause the body of the property to panic.</p>



<p>This produces output like this: confirming that the property does hold</p>

<div><div><pre><code>Running 1 test(s)
test test_add ... ok

test result: ok. 1 passed; 0 failed
VERIFICATION_RESULT: VERIFIED
</code></pre></div></div>

<p>And, if we change the example property so that the property does not hold,
<code>cargo-verify</code> produces this output.</p>

<div><div><pre><code>thread 'test_add' panicked at 'assertion failed: sum &gt;= a + 100', src/main.rs:11:9

running 1 test
  Value a = 0
  Value b = 0
test test_add ... FAILED

failures:

failures:
    test_add

test result: FAILED. 0 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out
</code></pre></div></div>

<p>On this example, random testing and formal verification produced similar
results.
I could have chosen an example where formal verification found a bug that
random testing misses.
Or I could have chosen an example where random testing easily finds
a bug but formal verification spins forever.
But, in this post, I wanted to focus on the similarity between the two
approaches, not the differences.</p>

<h2 id="learning-more">Learning more</h2>

<p>In this article, we used an example that was easy to explain but
does not really show off the power of <code>propverify</code>.
Some slightly better examples are</p>

<ul>
  <li>
    <p><a href="https://github.com/project-oak/rust-verification-tools/blob/main/compatibility-test/src/collections.rs#L60">collections.rs</a>
where we check properties involving standard Rust collection
types such as vectors and B-trees.
(This should be read in conjunction with the <a href="https://altsysrq.github.io/rustdoc/proptest/latest/proptest/collection/index.html">proptest
documentation</a>.)</p>
  </li>
  <li>
    <p><a href="https://github.com/project-oak/rust-verification-tools/blob/0a6a0ff2b3880eb4a19d8161bfae8d163b0a65ec/compatibility-test/src/dynamic.rs#L82">dynamic.rs</a>
where we check properties involving trait objects by
quantifying over a subset of the possible instances of the trait.</p>
  </li>
</ul>

<p>And, we have more documentation about</p>

<ul>
  <li>
    <p><a href="https://github.com/project-oak/rust-verification-tools/blob/main/docs/using-propverify.md">Using the propverify library</a></p>
  </li>
  <li>
    <p><a href="https://github.com/project-oak/rust-verification-tools/blob/main/docs/using-annotations.md">Using the lower level <code>verification-annotations</code> API</a></p>
  </li>
  <li>
    <p><a href="https://github.com/project-oak/rust-verification-tools/blob/main/docs/using-klee.md">The many, many flags we use when compiling Rust code so that it can be verified with KLEE</a></p>
  </li>
</ul>

<p>The latter two are mostly for the benefit of verification tool developers.</p>

<h2 id="our-next-steps">Our next steps</h2>

<p><em>[You can either read this as a sketch of what we plan to do or
as an admission of what we have not yet done.
As <a href="https://www.youtube.com/watch?v=LOILZ_D3aRg">Fred and Ginger said</a>,
“Tomato. Tomato.”]</em></p>

<ul>
  <li>
    <p>The tools have a horrifically complicated set of dependencies.</p>

    <p>This is partly because formal verification is not quite popular
enough for enough Debian/Homebrew packages to exist and partly
because we need some very specific versions.
(It may also be possible to remove some dependencies!)</p>
  </li>
  <li>
    <p>We are in the process of adding <a href="https://github.com/GaloisInc/mir-verifier">Crux-MIR</a> support to the library
and tool.
Crux-MIR is a new part of Galois’
<a href="https://galois.com/project/software-analysis-workbench/">Software Analysis Workbench (SAW)</a>
that verifies the MIR code generated by the Rust compiler.</p>

    <p>This is taking us a bit longer than using KLEE
because the functionality of our <code>cargo-verify</code> script
overlaps with the functionality of Crux-MIR but they have slightly different
approaches – we’re still working on the best way to handle the resulting
conflicts.</p>
  </li>
  <li>
    <p>It can be useful to focus our attention on a single crate at a time.
We have some ideas for how to do that with fuzzing and verification
but we have not had a chance to try them yet.</p>
  </li>
  <li>
    <p>We have not looked seriously at how well this approach scales.</p>

    <p>The collections support in the <code>propverify</code> library has a few
tricks to avoid some obvious scaling issues, but we don’t know if
those tricks work well for all verification tools and they
probably only scratch the surface of what needs to be done.</p>
  </li>
</ul>

<h2 id="summary">Summary</h2>

<p>Testing and formal verification are usually portrayed as mortal enemies.
I think that misses a huge opportunity to use your existing familiarity and
comfort with testing to let you get value out of formal verification tools.</p>

<p>I encourage you to download <a href="https://github.com/project-oak/rust-verification-tools">our library and tool</a>,
try it out and give us feedback.
If you are working on a Rust verification tool, we would love it
if you tried to use our library with your tool.
And we would love it even more if you sent us a pull request.</p>

<p>It’s probably not a good idea to commit to using the library at this stage:
it is not very robust at the moment and I expect that it will change a lot
as we gain experience from porting the library to other types of
formal verification tools.</p>

<p>Enjoy!</p>

<hr>



<p>If you found this article interesting, you might also enjoy these related posts</p>

<ul>
  <li><a href="https://alastairreid.github.io/verification-competitions/">Verification competitions</a></li>
  <li><a href="https://alastairreid.github.io/rust-verification-tools/">Rust verification tools</a></li>
</ul>

<p>And these papers:</p>

<ul>
  <li>
    <p><a href="https://alastairreid.github.io/RelatedWork/papers/goodman:ndss:2018">goodman:ndss:2018</a> applied some very similar ideas to C++ testing
in 2018 using the <a href="https://github.com/google/googletest">GoogleTest</a> DSL as a starting point and providing
support for <a href="https://angr.io/">angr</a>, <a href="https://blog.trailofbits.com/2017/04/27/manticore-symbolic-execution-for-humans/">Manticore</a> and <a href="https://dynamorio.org/drmemory_docs/page_drfuzz.html">Dr. Fuzz</a>.</p>

    <p>They ran into and solved problems related to logging (causing a path
explosion), symbolic loop bounds and symbolic array indices (causing
a path explosion) and how to combine swarm testing with verification.</p>

    <p>Highly recommended!</p>
  </li>
</ul>

<hr>



  </div><p>
    Written on September  3, 2020.
    <br>
    The opinions expressed are my own views and not my employer's.
  </p></div>]]>
            </description>
            <link>https://alastairreid.github.io/why-not-both/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24372760</guid>
            <pubDate>Fri, 04 Sep 2020 07:58:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Preserving data integrity: A ZFS-inspired storage system]]>
            </title>
            <description>
<![CDATA[
Score 56 | Comments 32 (<a href="https://news.ycombinator.com/item?id=24372662">thread link</a>) | @d2wa
<br/>
September 4, 2020 | https://insanity.industries/post/preserving-data-integrity/ | <a href="https://web.archive.org/web/*/https://insanity.industries/post/preserving-data-integrity/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
		<p>
			
			
			<h2>A ZFS-inspired storage system</h2>
			
		</p>
		<h2>07. Mar 2020</h2>
		<ul>
	<li><a href="https://insanity.industries/tags/funwithdata">#funwithdata</a></li>
	
	<li><a href="https://insanity.industries/tags/linux">#Linux</a></li>
	</ul>

		<section>
			<p>When we are storing data, we typically assume that our storage system of choice returns that data later just as we put it in.
However what guarantees do we have that this is actually the case?
The case made here is the case of <a href="https://www.redhat.com/en/blog/what-bit-rot-and-how-can-i-detect-it-rhel">bitrot</a>, the silent degradation of the physical charges that physically make up today’s storage devices.</p>
<p>To counter this type of problem, one can employ data checksumming, as it is done by both btrfs and ZFS.
However, while in the long run btrfs might be the tool of choice for this, it is fairly complex and not yet too mature, whereas ZFS, the most prominent candidate for this type of features, is <a href="https://github.com/zfsonlinux/zfs/issues/8314">not</a> <a href="https://sfconservancy.org/blog/2016/feb/25/zfs-and-linux/">without</a> <a href="https://ubuntu.com/blog/zfs-licensing-and-linux">hassle</a> and it must be recompiled for every kernel update (although <a href="https://github.com/zfsonlinux/zfs/wiki/Custom-Packages">automation exists</a>).</p>
<p>In this blogpost, we’ll therefore take a look into a storage design that actually checks whether the returned data is actually valid and not silently corrupted inside our storage system and is completely designed with components available in Linux itself without the need to recompile and test your storage layer on every kernel upgrade.
We find that this storage design, while fulfilling the same purpose as ZFS, does not only yield comparable performance, but actually in some cases even able to significantly outperform it, as the benchmarks at the end indicate.</p>

<p>The system we will test in the following will be constructed from four components (including the filesystem):</p>
<ul>
<li><strong>dm-integrity</strong> provides blocklevel checksumming for all incoming data and will check said checksum for all data read through it (and return a read error if it doesn’t match).</li>
<li><strong>mdraid</strong> provides redundancy if a disk misbehaves, avoiding data loss. If the dm-integrity-layer detects invalid data from the disk, mdraid will see a read-error and can immediately correct this error by reading it from somewhere else in the redundancy pool.</li>
<li><strong>dm-crypt</strong> on top of the RAID, right below the filesystem will encrypt the data<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></li>
<li><strong>ext4</strong> as a filesystem to actually store files on it</li>
</ul>
<p>The order matters, dm-integrity must be placed between the harddisks and the mdraid, to ensure that data corruption errors can be corrected by mdraid after they have been detected by dm-integrity. Similarly, dm-crypt has nothing to do with redundancy, therefore it should be placed on top of of mdraid, to only having to be passed once, and not multiple times as it would be the case for placing it alongside dm-integrity.<sup id="fnref:2"><a href="#fn:2" role="doc-noteref">2</a></sup></p>
<p>This yields the following storage design:</p>
<figure>
    <h4>Storage system architecture</h4>
    <img src="https://insanity.industries/post/preserving-data-integrity/architecture.svg" alt="Architecture of the storeage system. Grey depicts physical hardware, green depicts device mapper technologies, yellow indicates mdraid and blue indicates filesystems. The encryption layer will later be ommitted when comparing performance to ZFS." height="500"> <figcaption><p>Architecture of the storeage system. Grey depicts physical hardware, green depicts device mapper technologies, yellow indicates mdraid and blue indicates filesystems. The encryption layer will later be ommitted when comparing performance to ZFS.</p>
        </figcaption>
</figure>

<p><a href="https://insanity.industries/post/preserving-data-integrity/mkstorage.sh.txt">This script will assemble the above depicted system and mount it to /mnt.</a></p>

<p>Now that we have an assembled system, we’d like to quantify the performance impact of the different layers.
The test setup for this is a <a href="https://kobol.io/helios4/">Kobol Helios4</a> (primarily because it’s the intended target platform for this storage system) with four disks<sup id="fnref:3"><a href="#fn:3" role="doc-noteref">3</a></sup> running Debian 10 “Buster”.
The Helios4 is powered by an energy efficient dual-core ARM SoC optimized for storage systems.
The setup was not built based on the entire disks, but on partitions of a size of 10GiB each, allowing multiple parallel designs and therefore easing the benchmarking procedure, as well as speeding up the experiments<sup id="fnref:4"><a href="#fn:4" role="doc-noteref">4</a></sup>.</p>
<h2 id="layer-analysis-of-performance-impact">Layer analysis of performance impact</h2>
<h3 id="throughput">Throughput</h3>
<p>To benchmark throughput, the following commands were used:</p>
<pre><code># write:
echo 3 &gt; /proc/sys/vm/drop_cache  # drop all caches
dd if=/dev/zero of=/path/to/testfile bs=1M count=8000 conv=fdatasync

# read
echo 3 &gt; /proc/sys/vm/drop_cache  # drop all caches
dd if=/path/to/testfile of=/dev/zero conv=fdatasync
</code></pre>
<p>This procedure was repeated ten times for proper statistics for the different cases for both read (<code>r</code>) and write (<code>w</code>):</p>
<ul>
<li>ext4 filesystem only (<code>f</code>, one disk only)</li>
<li>encryption, topped by ext4 (<code>cf</code>, one disk only)</li>
<li>mdraid (RAID6/double parity), topped by the former (<code>rcf</code>, 4 disks)</li>
<li>the final setup, including integrity (<code>ircf</code>, 4 disks)</li>
</ul>
<figure>
    
    <img src="https://insanity.industries/post/preserving-data-integrity/layers.svg" alt="Throughput for different arrangements of layers on Kobol Helios4 system for both read (top row) and write (bottom row). Each dot indicates one measurement. f: filesystem, cf: crypto+f, rcf:raid+cf, ircf: integrity+rcf. Testplatform"> <figcaption><p>Throughput for different arrangements of layers on Kobol Helios4 system for both read (top row) and write (bottom row). Each dot indicates one measurement. f: filesystem, cf: crypto+f, rcf:raid+cf, ircf: integrity+rcf. <a href="https://kobol.io/helios4/">Testplatform</a></p>
        </figcaption>
</figure>

<p>We see different interesting results in the data:
First of all, the encryption engine on the helios4 is not completely impact free, although the resulting performance is still more than sufficient for the designated uses for a Helios4.</p>
<p>Secondly, we see that adding integrity layer does have a noticable impact on writing, but a negligible impact on reading, indicating that especially for systems primarily intended to read data from adding the integrity layer is a matter of negligible cost.</p>
<p>For write-heavy systems, the performance impact is more considerable, but for certain workloads, such as the home-NAS-case the Helios is designed for, the performance can still be considered fully sufficient, especially as normally the system would cache those writes to a certain extend, which was explicitly disabled for benchmarking purposes (see <code>conv=fdatasync</code> in the benchmark procedure).</p>
<p>The reason for the degradation in write (but not in read) is likely due to the fact that the integrity layer and the mdraid layer are decoupled from one another, the raid-layer is not aware that for a double parity setup it effectively has to write the same information three times, and the integrity layer has to account for all three writes before the data is considered synced as required by <code>conv=fdatasync</code>.</p>
<h3 id="latency">Latency</h3>
<p>We have found that the throguhput of such a storage design is yielding useful throughput-rates.
The next question is about the latency of the system, which we will, for simplicity, only estimate for random 4K-reads.
Again, just as above, we will investigate the impact of the different layers of the system.
To do so, we read 100.000 sectors randomly from an 18GB sized testfile filling the storage mountpoint for each configuration after dropping all caches.</p>
<figure>
    <h4>Latency comparison of system layers</h4>
    <img src="https://insanity.industries/post/preserving-data-integrity/layerlatencies.svg" alt="Distribution of latencies in milliseconds for different arrangements of layers on Kobol Helios4 system. green denotes the median, black denotes the average of the latencies of the respective setup, access times below 1ms were considered cache hits and therefore excluded for the computation of mean and average. Testplatform"> <figcaption><p>Distribution of latencies in milliseconds for different arrangements of layers on Kobol Helios4 system. green denotes the median, black denotes the average of the latencies of the respective setup, access times below 1ms were considered cache hits and therefore excluded for the computation of mean and average. <a href="https://kobol.io/helios4/">Testplatform</a></p>
        </figcaption>
</figure>

<p>The figure above yields several interesting insights:
First of all, we do see cache hits close to zero milliseconds, furthermore, we see that the latency distribution is fairly evenly distributed over the available range and finally and most interestingly, we see that the impact of the several layers onto the latency is measurable, but rather irrellevant for typical practical purposes.</p>
<h2 id="performance-comparison-with-zfs">Performance comparison with ZFS</h2>
<p>So far we have tested the setup on its intended target platform, the Helios4.
To compare the resulting system against the elephant in the room, ZFS, we will use a different test platform, based on an Intel i7-2600K as CPU and 4x1TB disks, as the <code>zfs-dkms</code> was not reliably buildable on Debian Buster on ARM, and when it actually built, it explicitly stated that 32bit-processors (as the Helios’ CPU) are not supported by upstream, although technically the system would run.</p>
<p>To allow for a cleaner comparison, the testbed was accordinly changed to accomodate for ZFS’s preferences.
As ZFS did not adhere to <code>conv=fdatasync</code><sup id="fnref:5"><a href="#fn:5" role="doc-noteref">5</a></sup>, the main memory was restricted to 1GB, swap was turned off and the size of the testfile was chosen to be 18GB.
This way, any caching happening would be at least significantly reduced as there was little space next to the OS inside of the main memory for caching.</p>
<p>All tests were run on a Debian 10 “Buster” with Linux 4.19, ZFS was used in form of the package <code>zfs-dkms</code> in version 0.8.2 from backports.
The storage layer for both setups was layouted with double parity (RAID6/raidz2) and, as the <code>zfs-dkms</code> package in Debian was not able to do encryption, the mdraid-based setup was also setup without the encryption layer.</p>
<h3 id="throughput-1">Throughput</h3>
<p>The commands used for benchmarking throughput were conceptually the same as above:</p>
<pre><code># write:
echo 3 &gt; /proc/sys/vm/drop_cache  # drop all caches
dd if=/dev/zero of=/path/to/testfile bs=1M count=18000 conv=fdatasync

# read
echo 3 &gt; /proc/sys/vm/drop_cache  # drop all caches
dd if=/path/to/testfile of=/dev/zero conv=fdatasync
</code></pre>
<p>Although ZFS seemingly does not adhere to any cache dropping or syncing instructions, they were still performed and adhered to by the mdraid-based setup.</p>
<figure>
    <h4>Throughput comparison, zfs &amp; md</h4>
    <img src="https://insanity.industries/post/preserving-data-integrity/zfs-md-throughput.svg" alt="Throughput for both ZFS and the md-based setup for both read and write. Each dot indicates one measurement, the green line indicates the median of all measurements. Testplatform"> <figcaption><p>Throughput for both ZFS and the md-based setup for both read and write. Each dot indicates one measurement, the green line indicates the median of all measurements. <a href="https://insanity.industries/post/preserving-data-integrity/zfs-testbed.txt">Testplatform</a></p>
        </figcaption>
</figure>

<p>The results are very interesting: While the md-based setup performs less consistent in and of itself, it still consistently outperforms ZFS in read performance.
When it comes to write, though, ZFS performs noticably better.<sup id="fnref:6"><a href="#fn:6" role="doc-noteref">6</a></sup></p>
<p>To investigate the cause of this unexpected balance, we note that while ZFS combines raid, integrity and filesystem in one component, for the md-based setup these are separate components.
Of these components, not only the filesystem, but also the <code>dm-integrity</code> implements journalling to avoid inconsistencies in case of a power outage.
This leads to increased work until the transaction has been fully flushed to disk, which can be seen in the next figure, where the md-based system (without the encryption layer) is tested with both journalling enabled and disabled in the integrity layer:</p>
<figure>
    <h4>Throughput effect of integrity journaling</h4>
    <img src="https://insanity.industries/post/preserving-data-integrity/mdraid-journalling.svg" alt="Throughput for the md-based setup, with journalling enabled and disabled in the integrity-layer for both read and write. Each dot indicates one measurement, the green line indicates the median of all measurements. Testplatform"> <figcaption><p>Throughput for the md-based setup, with journalling enabled and disabled in the integrity-layer for both read and write. Each dot indicates one measurement, the green line indicates …</p></figcaption></figure></section></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://insanity.industries/post/preserving-data-integrity/">https://insanity.industries/post/preserving-data-integrity/</a></em></p>]]>
            </description>
            <link>https://insanity.industries/post/preserving-data-integrity/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24372662</guid>
            <pubDate>Fri, 04 Sep 2020 07:41:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[VS Code extensions I use to be more productive]]>
            </title>
            <description>
<![CDATA[
Score 21 | Comments 0 (<a href="https://news.ycombinator.com/item?id=24372192">thread link</a>) | @fazlerocks
<br/>
September 3, 2020 | https://dev.vamsirao.com/vs-code-extensions | <a href="https://web.archive.org/web/*/https://dev.vamsirao.com/vs-code-extensions">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section itemprop="articleBody mainEntityOfPage"><meta itemprop="thumbnailUrl image" content="https://cdn.hashnode.com/res/hashnode/image/upload/v1599156332456/7WwIJcqD0.jpeg?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=format&amp;q=60"><div><div itemprop="text"><p>We all use some VS Code extensions for our day to day development and are always on a hunt for more useful extensions. Here is a list of mine and how I use them:</p>
<p><a target="_blank" href="https://www.peacockcode.dev/">Peacock</a> </p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1593585542633/58FKxSF2u.png?auto=format&amp;q=60" alt=""></p>
<p>We have all been there when we have two editors open and coded in the wrong one 🤦  Peacock helps me differentiate between different workspaces by setting a different color theme to VS Code.</p>
<p><a target="_blank" href="https://marketplace.visualstudio.com/items?itemName=CoenraadS.bracket-pair-colorizer-2">Bracket Pair Colorizer</a></p>
<p><img src="https://github.com/CoenraadS/Bracket-Pair-Colorizer-2/raw/master/images/activeScopeBorder.png" alt=""></p>
<p>Easily distinguishable bracket scopes are always good for programming and this extension does it well.</p>
<p><a target="_blank" href="https://marketplace.visualstudio.com/items?itemName=alefragnani.Bookmarks">Bookmarks</a></p>
<p><img src="https://github.com/alefragnani/vscode-bookmarks/raw/master/images/printscreen-select-lines.gif" alt=""></p>
<p>Sometimes you are in the flow but navigating the code from one function to another becomes tedious, this lets you bookmark code at different lines and you can easily access it using shortcuts.</p>
<p><a target="_blank" href="https://marketplace.visualstudio.com/items?itemName=streetsidesoftware.code-spell-checker">Code Spell Checker</a></p>
<p><img src="https://raw.githubusercontent.com/streetsidesoftware/vscode-spell-checker/master/packages/client/images/suggestions.gif" alt=""></p>
<p>There have been times when I have embarrassingly spent more time than needed debugging an error only to find out it was a spelling mistake. This extension keeps that in check.</p>
<p><a target="_blank" href="https://marketplace.visualstudio.com/items?itemName=auiworks.amvim">AmVim</a></p>
<p> I am a huge fan of Vim, it legit increases your productivity without having to touch the trackpad. AmVim is one of the Vim mode extension for VSCode, I found this to be quite good and not bloaty compared to other Vim extensions.</p>
<p><a target="_blank" href="https://marketplace.visualstudio.com/items?itemName=Shan.code-settings-sync">Settings Sync</a></p>
<p>Settings Sync helps you keep the same environment of VSCode (extensions and preferences) across machines.</p>
<p><a target="_blank" href="https://marketplace.visualstudio.com/items?itemName=ChakrounAnas.turbo-console-log">Turbo Console log</a></p>
<p><img src="https://image.ibb.co/dysw7p/insert_log_message.gif" alt=""></p>
<p>This one is really useful while debugging, it helps create meaningful logs using shortcuts. All you have to do is highlight the variable and press the keybindings.</p>
<p><a target="_blank" href="https://marketplace.visualstudio.com/items?itemName=TabNine.tabnine-vscode">Tab Nine</a></p>
<p><img src="https://github.com/codota/tabnine-vscode/raw/master/assets/tabnine.gif" alt="sdfsdf"></p>
<p>I just recently came across this one in one of <a href="https://hashnode.com/@dailydevtips">Chris Bongers</a>'s article. Really like it! It is sometimes just scary how accurately it predicts auto-completion (It uses deep-learning model locally on your machine to come up with the predictions).</p>
<hr><p>
Other than these I use the common ones like <a target="_blank" href="https://marketplace.visualstudio.com/items?itemName=esbenp.prettier-vscode">Prettier</a>, <a target="_blank" href="https://marketplace.visualstudio.com/items?itemName=PKief.material-icon-theme">Material Icon theme</a>, <a target="_blank" href="https://marketplace.visualstudio.com/items?itemName=naumovs.color-highlight">Color Highlight</a>. <a target="_blank" href="https://marketplace.visualstudio.com/items?itemName=wart.ariake-dark">Ariake Dark</a> is my VS Code theme.</p>
<blockquote>
<p>Hope you found this useful and feel free to share the extensions you use in the comments below. You can also @ me on <a target="_blank" href="https://twitter.com/vamsirao7">twitter</a></p>
</blockquote>
</div></div></section></div>]]>
            </description>
            <link>https://dev.vamsirao.com/vs-code-extensions</link>
            <guid isPermaLink="false">hacker-news-small-sites-24372192</guid>
            <pubDate>Fri, 04 Sep 2020 06:11:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Costs of running a Python webapp for 55k monthly users]]>
            </title>
            <description>
<![CDATA[
Score 281 | Comments 243 (<a href="https://news.ycombinator.com/item?id=24372084">thread link</a>) | @caspii
<br/>
September 3, 2020 | https://keepthescore.co/blog/posts/costs-of-running-webapp/ | <a href="https://web.archive.org/web/*/https://keepthescore.co/blog/posts/costs-of-running-webapp/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
        <div>

          


<article>
  <header>
    <h2 dir="auto"><a href="https://keepthescore.co/blog/posts/costs-of-running-webapp/">Costs of running a Python webapp for 55k monthly users</a></h2>
    <p><time datetime="2020-09-04T00:00:00+02:00">Fri Sep 4, 2020</time> by Caspar</p>
  </header>
  

<p>How much does running a webapp in production actually cost? Maybe more than you think. Keepthescore.co is a Python flask application running on DigitalOcean and Firebase. It currently has around 55k unique visitors per month, per day it’s around 3.4k.</p>

<p><img src="https://keepthescore.co/blog/money-tree.jpg" alt="A money tree">
<em>Image by <a href="https://unsplash.com/@micheile">Micheile Henderson</a></em></p>

<p>Here’s a monthly breakdown with some background information on each cost:</p>

<h3 id="servers-and-database-on-digitalocean">Servers and database on DigitalOcean</h3>

<p><strong>Costs per month: $95</strong></p>

<p>The webapp runs on two identical DigitalOcean servers (4 vCPUs, 8GB RAM, 80GB disk). We use a <a href="https://en.wikipedia.org/wiki/Blue-green_deployment">blue-green deployment</a>, which is a great way of running and hosting a webapp (more on that in a future post) but it does mean that you need 2 identical production servers.</p>

<p>The database is a hosted Postgres instance also on DigitalOcean.</p>

<p>Note: the servers are oversized for the load we’re currently seeing. The reason for that is that we tried to solve a production issue by increasing the server specs. It didn’t solve the problem, and now we can’t down-size the servers without re-provisioning them 🤷‍♀️.</p>

<hr>

<h3 id="code-repo-on-github">Code repo on GitHub</h3>

<p><strong>Costs per month: $0</strong></p>

<p>This is free. Thanks GitHub!
</p><hr>

<h3 id="amazon-web-services">Amazon Web Services</h3>

<p><strong>Costs per month: $60</strong></p>

<p>We use a reporting tool called <a href="https://www.metabase.com/">Metabase</a> to generate insights and reports from the database. The tool itself is opensource and free, but hosting it is fairly expensive. Currently it runs on an EC2 instance. There are definitely some savings that could be realised here.
</p><hr>

<h3 id="google-cloud">Google Cloud</h3>

<p><strong>Costs per month: $1.32</strong></p>

<p>We use Firebase for the <a href="https://keepthescore.co/basketball-scoreboard/">realtime basketball scoreboard</a>. We also use the Google Sheets API for some custom scoreboards. Overall it must be said that the Google Cloud APIs are great value for money (so far).</p>

<p>We have plans to move away from DigitalOcean and onto Google Cloud infrastructure in the next year.
</p><hr>

<h3 id="dns-hosting">DNS hosting</h3>

<p><strong>Costs per month: $5</strong></p>

<p>Our domain is registered with <a href="https://dnsimple.com/">DNSimple.com</a>.
</p><hr>



<p><strong>Costs per month: $10</strong></p>

<p>Scoreboards and this blog have the option of allowing a discussion on the same page – this uses the <a href="https://disqus.com/">Disqus</a> service. Disqus has a free tier but they began showing ads so we were forced to move to the paid tier.</p>

<p>We are planning to move to <a href="https://commento.io/">Commento</a> in the future, which is cheaper, has no ads, and respects your privacy.
</p><hr>

<h2 id="is-it-worth-it-is-there-revenue">Is it worth it? Is there revenue?</h2>

<p>In total that’s around <strong>$171 USD per month</strong>. If you’re running a company with employees that would be peanuts, but in this case the cost is being borne by a single indie-developer out of his own pocket.</p>

<p>The bigger issue is that on the revenue side there’s a big fat zero. This is the reason why we are currently working on monetization. This will include banner ads and payments via Stripe. Expect to see these features land by next week! Anyway, more about that in a future post.</p>

<h2 id="you-are-spending-way-too-much-money">You are spending way too much money!</h2>

<p>This post generated an interesting discussion on <a href="https://news.ycombinator.com/item?id=24372084">Hacker News</a>. One recurring theme
was that our stack is large and expensive and that we could massively reduce costs.</p>

<p>This is true. But here are some things that are also true:</p>

<ul>
<li>“Reducing costs” is not our primary objective at the moment: time is more valuable than money right now.</li>
<li>Our primary objective right now is velocity of product development.</li>
<li>We are not devops experts and optimizing operations takes time.</li>
</ul>

<p>There will be a follow up post once we do optimize our stack.</p>

<p>Thanks for listening and so long, 👋</p>


  

  
  <hr>
  
  

</article> 



        </div> <!-- /.blog-main -->

        


      </div> <!-- /.row -->
    </div></div>]]>
            </description>
            <link>https://keepthescore.co/blog/posts/costs-of-running-webapp/</link>
            <guid isPermaLink="false">hacker-news-small-sites-24372084</guid>
            <pubDate>Fri, 04 Sep 2020 05:46:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Libvirt – The Unsung Hero of Cloud Computing (2013)]]>
            </title>
            <description>
<![CDATA[
Score 211 | Comments 106 (<a href="https://news.ycombinator.com/item?id=24370966">thread link</a>) | @vikrantrathore
<br/>
September 3, 2020 | https://vyomtech.com/2013/12/17/libvirt_the_unsung_hero_of_cloud_computing.html | <a href="https://web.archive.org/web/*/https://vyomtech.com/2013/12/17/libvirt_the_unsung_hero_of_cloud_computing.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><article><p><span>December 17, 2013</span>
        </p>
    <div id="libvirt-the-unsung-hero-of-cloud-computing">

<p><img alt="Libvirt The Unsung Hero of Cloud Computing Platforms" src="https://vyomtech.com/_images/libvirt_the_unsung_hero.png"></p><p>Initially my intention was to write an article on <strong>Round up of open source Cloud Management Platforms (CMP)</strong>, but while doing research found one piece of software library so fundamental, that it holds the key to very existence of Cloud Computing services and platforms as we know it today (that includes Amazon AWS, OpenStack and CloudStack). So I decided to postpone my idea and started to write an article on this <strong>Unsung hero of Cloud Computing</strong> called libvirt <a href="#f1" id="id1">[1]</a>, which I believe many people won’t have heard of. Obviously talking about software library tend to be technical, so this article will have a technical tone, but I will attempt to make it readable for everyone interested in Cloud Computing.</p>
<p>Libvirt is an open source API, daemon and management tool for managing platform virtualzation and these APIs are widely used in the orchestration layer of Cloud Management Platform. Libvirt makes it possible to control and manage millions of compute nodes, storage and network devices via common programmable interface. Its like being able to control and orchestrate fleet of millions of car irrespective of different manufacturer, model, or engine through a common interface from a single car (it can be more then one car for redundancy and high availability). What started as a management API for Xen, today has been extended to support major components of Cloud Computing platforms.</p>
<div id="libvirt-goals-architecture">
<h2>Libvirt Goals &amp; Architecture</h2>
<p>Libvirt defines following terms for its goals <a href="#f2" id="id2">[2]</a>:</p>
<p><img alt="Mapping of libvirt terms to virtualized or containerized physical machine" src="https://vyomtech.com/_images/libvirt_terms_mapping.png"></p><ul>
<li><strong>Node</strong> is a single physical machine.</li>
<li><strong>Hypervisor</strong> is a layer of softeare allowing to virtualize a node in a set of physical machines with possible different configurations that the node itself.</li>
<li><strong>Domain</strong> is an instance of an operating system (or subsystem in case of container virtualization like OpenVZ and lxc) running on a virtualized machine provided by the hypervisor.</li>
</ul>
<p>Based on above terms <strong>“The goal of libvirt is to provide a common and stable layer sufficient to securely manage domains on a node, possibly remote”</strong>. So libvirt should provide all APIs needed to do the management, such as: provision, create, modify, monitor, control, migrate and stop the domains - within the limits of the support of the hypervisor for those operations. This implies following sub-goals:</p>
<ul>
<li>All API can be carried remotely though secure APIs</li>
<li>While most API will be generic in term of hypervisor or Host OS, some API may be targeted to a single virtualization environment as long as the semantic for the operations from a domain management perspective is clear</li>
<li>the API should allow to do efficiently and cleanly all the operations needed to manage domains on a node, including resource provisioning and setup</li>
<li>the API will not try to provide high level virtualization policies or multi-nodes management features like load balancing, but the API should be sufficient so they can be implemented on top of libvirt</li>
<li>stability of the API is a big concern, libvirt should isolate applications from the frequent changes expected at the lower level of the virtualization framework</li>
<li>the node being managed may be on a different physical machine than the management program using libvirt, to this effect libvirt supports remote access, but should only do so by using secure protocols.</li>
<li>libvirt will provide APIs to enumerate, monitor and use the resources available on the managed node, including CPUs, memory, storage, networking, and NUMA partitions.</li>
</ul>
<p>So libvirt is intended to be a building block for higher level management tools and for applications focusing on virtualization of a single node (the only exception being domain migration between node capabilities which involves more than one node).</p>
<div id="libvirt-driver-based-architecture">
<h3>Libvirt Driver Based Architecture</h3>
<p><img alt="libvirt driver based architecture" src="https://vyomtech.com/_images/libvirt_architecture.png"></p><p>Libvirt to support wide variety of hypervisor implements a driver-based architecture. Based on car analogy, it means delegating the actual implementation of control of different cars to the drivers specifically designed for make, model and engine of the specific car. Libvirt currently supports:</p>
<p><strong>Hypervisor</strong></p>
<ul>
<li>LXC - Linux Containers</li>
<li>OpenVZ</li>
<li>QEMU</li>
<li>Test - Used for testing</li>
<li>UML - User Mode Linux</li>
<li>VirtualBox</li>
<li>VMware ESX</li>
<li>VMware Workstation/Player</li>
<li>Xen</li>
<li>Microsoft Hyper-V</li>
<li>IBM PowerVM (phyp)</li>
<li>Parallels</li>
<li>Remote - Accessing libvirt on remote node through libvirtd (libvirt daemon)</li>
</ul>
<p><strong>Storage</strong></p>
<ul>
<li>Directory backend</li>
<li>Local filesystem backend</li>
<li>Network filesystem backend</li>
<li>Logical Volume Manager (LVM) backend</li>
<li>Disk backend</li>
<li>iSCSI backend</li>
<li>SCSI backend</li>
<li>Multipath backend</li>
<li>RBD (RADOS Block Device) backend</li>
<li>Sheepdog backend</li>
</ul>
<p><strong>Virtual Networks</strong></p>
<ul>
<li>Bridging</li>
<li>NAT</li>
<li>VEPA (Virtual Ethernet Port Aggregator)</li>
<li>VN-LINK</li>
</ul>
</div>
<div id="libvirt-api-structure">
<h3>Libvirt API structure <a href="#f3" id="id3">[3]</a></h3>
<p><img alt="libvirt API structure" src="https://vyomtech.com/_images/libvirt_api_structure.png"></p><p>The figure above shows the five main objects exported by the API:</p>
<dl>
<dt><strong>virConnectPtr</strong></dt>
<dd>Represents the connection to a hypervisor. Use one of the virConnectOpen functions to obtain connection to the hypervisor which is then used as a parameter to other connection API’s.</dd>
<dt><strong>virDomainPtr</strong></dt>
<dd>Represents one domain either active or defined (i.e. existing as permanent config file and storage but not currently running on that node). The function virConnectListAllDomains lists all the domains for the hypervisor.</dd>
<dt><strong>virNetworkPtr</strong></dt>
<dd>Represents one network either active or defined (i.e. existing as permanent config file and storage but not currently activated). The function virConnectListAllNetworks lists all the virtualization networks for the hypervisor.</dd>
<dt><strong>virStorageVolPtr</strong></dt>
<dd>Represents one storage volume generally used as a block device available to one of the domains. The function virStorageVolLookupByPath finds the storage volume object based on its path on the node.</dd>
<dt><strong>virStoragePoolPtr</strong></dt>
<dd>Represents a storage pool, which is a logical area used to allocate and store storage volumes. The function virConnectListAllStoragePools lists all of the virtualization storage pools on the hypervisor. The function virStoragePoolLookupByVolume finds the storage pool containing a given storage volume.</dd>
</dl>
<p>These names follow C conventions, but developers of cloud computing platforms and applications do not need to use C directly, there are language bindings available for major languages. Currently libvirt API language bindings <a href="#f4" id="id4">[4]</a> are available for C#, Java, OCaml, Perl, PHP, Python, Ruby.</p>
</div>
<div id="domain-management-architecture">
<h3>Domain Management Architecture</h3>
<p>There are two distinct means for domain management using libvirt API.</p>
<p><strong>1. Single node domain management</strong></p>
<p><img alt="libvirt API structure" src="https://vyomtech.com/_images/libvirt_single_node_domain_management.png"></p><p>As illustrated in the figure above in this mode applications (cloud management platform i.e. CMP applications) and domains exist on the same node. In this scenario applications directly works through the libvirt api on the host operating system (os) to control and manage the local domains.</p>
<p><strong>2. Multi node domain management</strong></p>
<p><img alt="libvirt API structure" src="https://vyomtech.com/_images/libvirt_multi_node_domain_management.png"></p><p>As shown in the figure above, applications (CMP applications) using libvirt API and the domains to manage or control are on separate nodes. In this mode a special domain called <strong>libvirtd</strong> (libvirt daemon) needs to run on remote nodes. The management application nodes use the nodes underlying network communicattion to communicate with remote <em>libvirtd</em> through the local libvirt using custom protocol. Actually libvirt uses Remote <a href="#f6" id="id5">[6]</a> driver for communicating with remote node and remote API calls are handled synchronously. Remote driver for libvirt supports a range of transports like:</p>
<dl>
<dt><em>tls</em></dt>
<dd>TLS 1.0 (SSL 3.1) authenticated and encrypted TCP/IP socket, usually listening on a public port number. To use this you will need to generate client and server certificates. The standard port is 16514. This is the <strong>default</strong> transport, if no other is specified.</dd>
<dt><em>unix</em></dt>
<dd>nix domain socket. Since this is only accessible on the local machine, it is not encrypted, and uses Unix permissions or SELinux for authentication. The standard socket names are /var/run/libvirt/libvirt-sock and /var/run/libvirt/libvirt-sock-ro (the latter for read-only connections).</dd>
<dt><em>ssh</em></dt>
<dd>Transported over an ordinary ssh (secure shell) connection. Requires Netcat (nc) installed and libvirtd should be running on the remote machine. You should use some sort of ssh key management (eg. ssh-agent) otherwise programs which use this transport will stop to ask for a password.</dd>
<dt><em>ext</em></dt>
<dd>Any external program which can make a connection to the remote machine by means outside the scope of libvirt.</dd>
<dt><em>tcp</em></dt>
<dd>nencrypted TCP/IP socket. Not recommended for production use, this is normally disabled, but an administrator can enable it for testing or use over a trusted network. The standard port is 16509.</dd>
<dt><em>libssh2</em></dt>
<dd>Transport over the SSH protocol using libssh2 instead of the OpenSSH binary. This transport uses the libvirt authentication callback for all ssh authentication calls and therefore supports keyboard-interactive authentication even with graphical management applications. As with the classic ssh transport netcat is required on the remote side.</dd>
</dl>
</div>
</div>
<div id="libvirt-project">
<h2>Libvirt Project <a href="#f5" id="id6">[5]</a></h2>
<p>According to statistics on ohloh libvirt in a nutshell:</p>
<ul>
<li>15,188 commits made  by 331 contributors representing 481,506 lines of code</li>
<li>Mostly written in C</li>
</ul>
<table>
<colgroup>
<col width="50%">
<col width="50%">
</colgroup>
<thead>
<tr><th colspan="2">Libvirt project</th>
</tr>
</thead>
<tbody>
<tr><td>C</td>
<td>74%</td>
</tr>
<tr><td>C++</td>
<td>9%</td>
</tr>
<tr><td>XML</td>
<td>6%</td>
</tr>
<tr><td>14 Other</td>
<td>11%</td>
</tr>
</tbody>
</table>
<ul>
<li>Established, mature codebase maintained by a very large development team with increasing year on year commits.</li>
<li>Estimated 128 years of efforts (COCOMO model)</li>
</ul>
</div>
<div id="conclusion">
<h2>Conclusion</h2>
<p>Libvirt is one very important library on whose giant shoulders cloud computing services and platforms like Amazon AWS, Google Compute Engine, OpenStack, CloudStack, Eucalyptus and numberous others are standing. Also this API enables developers and companies to build new and innovative cloud computing services or platforms and build awesome applications or services on top of it. Libvirt started in 2005 and with growing popularity of cloud computing, this project will continue to grow. But in most of the conferences, talks and papers related to Cloud Computing I did not find much coverage of libvirt, so while researching Cloud Management platform thought of writing and article on it. Kudos to all the libvirt code contributors for building a beautiful abstraction layer and making life easier for cloud computing services and platform developers. In spite of not getting as much press and coverage as …</p></div></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://vyomtech.com/2013/12/17/libvirt_the_unsung_hero_of_cloud_computing.html">https://vyomtech.com/2013/12/17/libvirt_the_unsung_hero_of_cloud_computing.html</a></em></p>]]>
            </description>
            <link>https://vyomtech.com/2013/12/17/libvirt_the_unsung_hero_of_cloud_computing.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-24370966</guid>
            <pubDate>Fri, 04 Sep 2020 01:46:34 GMT</pubDate>
        </item>
    </channel>
</rss>
