<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 20]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 20. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Sun, 15 Nov 2020 00:54:12 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-20.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Sun, 15 Nov 2020 00:54:12 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Why TCP over TCP is a bad idea (2001)]]>
            </title>
            <description>
<![CDATA[
Score 48 | Comments 13 (<a href="https://news.ycombinator.com/item?id=25080693">thread link</a>) | @fanf2
<br/>
November 13, 2020 | http://sites.inka.de/~bigred/devel/tcp-tcp.html | <a href="https://web.archive.org/web/*/http://sites.inka.de/~bigred/devel/tcp-tcp.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
    
    <p>
      A frequently occurring idea for IP tunneling applications is to
      run a protocol like PPP, which encapsulates IP packets in a
      format suited for a stream transport (like a modem line), over a
      TCP-based connection. This would be an easy solution for
      encrypting tunnels by running <em>PPP over SSH</em>, for which
      several recommendations already exist (one in the Linux HOWTO
      base, one on my own website, and surely several others). It
      would also be an easy way to compress arbitrary IP traffic,
      while datagram based compression has hard to overcome efficiency
      limits.
    </p><p>
      Unfortunately, it doesn't work well. Long delays and frequent
      connection aborts are to be expected. Here is why.

    </p><h2>TCP's retransmission algorithm</h2>
    <p>
      TCP divides the data stream into <em>segments</em> which are
      sent as individual IP datagrams. The segments carry a
      <em>sequence number</em> which numbers the bytes in the stream,
      and an <em>acknowledge number</em> which tells the other side
      the last received sequence number. [RFC793]
    </p><p>
      Since IP datagrams may be lost, duplicated or reordered, the
      sequence numbers are used to reassemble the stream. The
      acknowledge number tells the sender, indirectly, if a segment
      was lost: when an acknowledge for a recently sent segment does
      not arrive in a certain amount of time, the sender assumes a
      lost packet and re-sends that segment.
    </p><p>
      Many other protocols using a similar approach, designed mostly
      for use over lines with relatively fixed bandwidth, have the
      "certain amount of time" fixed or configurable. In the Internet
      however, parameters like bandwidth, delay and loss rate are
      vastly different from one connection to another and even
      changing over time on a single connection. A fixed timeout in
      the seconds range would be inappropriate on a fast LAN and
      likewise inappropriate on a congested international link. In
      fact, it would increase the congestion and lead to an effect
      known as "meltdown".
    </p><p>
      For this reason, TCP uses adaptive timeouts for all
      timing-related parameters. They start at conservative estimates
      and change dynamically with every received segment. The actual
      algorithms used are described in [RFC2001]. The details are not
      important here but one critical property: <em>when a segment
      timeouts, the following timeout is increased</em>
      (exponentially, in fact, because that has been shown to avoid
      the meltdown effect).

    </p><h2>Stacking TCPs</h2>
    <p>
      The TCP timeout policy works fine in the Internet over a vast
      range of different connection characteristics. Because TCP tries
      very hard not to break connections, the timeout can increase up
      to the range of several minutes. This is just what is sensible
      for unattended bulk data transfer. (For interactive
      applications, such slow connections are of course undesirable
      and likely the user will terminate them.)
    </p><p>
      This optimization for reliability breaks when stacking one TCP
      connection on top of another, which was never anticipated by the
      TCP designers. But it happens when running PPP over SSH or
      another TCP-based protocol, because the PPP-encapsulated
      IP datagrams likely carry TCP-based payload, like this:
    </p><p>
      <img src="http://sites.inka.de/~bigred/devel/tcp-tcp.png" width="371" height="270" alt="(TCP over IP over PPP over SSH over TCP over IP)">
    </p><p>
      Note that the upper and the lower layer TCP have different
      timers. When an upper layer connection starts fast, its timers
      are fast too. Now it can happen that the lower connection has
      slower timers, perhaps as a leftover from a period with a
      slow or unreliable base connection.
    </p><p>
      Imagine what happens when, in this situation, the base
      connection starts losing packets. The lower layer TCP queues up
      a retransmission and increases its timeouts. Since the
      connection is blocked for this amount of time, the upper layer
      (i.e. payload) TCP won't get a timely ACK, and will also queue a
      retransmission. Because the timeout is still less than the lower
      layer timeout, <em>the upper layer will queue up more
      retransmissions faster than the lower layer can process
      them</em>. This makes the upper layer connection stall very
      quickly and every retransmission just adds to the problem - an
      internal meltdown effect.
    </p><p>
      TCPs reliability provisions backfire here. The upper layer
      retransmissions are completely unnecessary, since the carrier
      guarantees delivery - but the upper layer TCP can't know this,
      because TCP always assumes an unreliable carrier.

    </p><h2>Practical experience</h2>
    <p>
      The whole problem was the original incentive to start the <a href="http://sites.inka.de/~bigred/devel/cipe.html">CIPE</a>
      project, because I used a PPP over SSH solution for some time
      and it proved to be fairly unusable. At that time it had to run
      over an optical link which suffered frequent packet loss,
      sometimes 10-20% over an extended period of time. With plain
      TCP, this was just bearable (because the link was not
      congested), but with the stacked protocols, connections would
      get really slow and then break very frequently.

    </p><p>
      This is the detailed reason why CIPE uses a datagram carrier.
      (The choice for UDP, instead of another IP-level protocol like
      IPsec does, is for several reasons: this allows to distinguish
      tunnels by their port number, and it adds the ability to run
      over SOCKS.) The datagram carrier has exactly the same
      characteristics as plain IP, for which TCP was designed to run
      over.

    </p><hr>
    <address><a href="mailto:olaf@bigred.inka.de">Olaf Titz</a></address>
<!-- Created: Sun Oct 10 20:56:27 CEST 1999 -->
<!-- hhmts start -->
Last modified: Mon Apr 23 11:50:59 CEST 2001
<!-- hhmts end -->
  

</div>]]>
            </description>
            <link>http://sites.inka.de/~bigred/devel/tcp-tcp.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25080693</guid>
            <pubDate>Fri, 13 Nov 2020 09:43:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: A 1000 auto-generated hexagonal SVG logos]]>
            </title>
            <description>
<![CDATA[
Score 154 | Comments 62 (<a href="https://news.ycombinator.com/item?id=25071643">thread link</a>) | @graderjs
<br/>
November 12, 2020 | https://dosycorp.gitlab.io/dosylogo/?v923418754891239875624v1 | <a href="https://web.archive.org/web/*/https://dosycorp.gitlab.io/dosylogo/?v923418754891239875624v1">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://dosycorp.gitlab.io/dosylogo/?v923418754891239875624v1</link>
            <guid isPermaLink="false">hacker-news-small-sites-25071643</guid>
            <pubDate>Thu, 12 Nov 2020 16:44:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Migrating Large Heroku Postgres Instances to AWS Aurora Without Downtime]]>
            </title>
            <description>
<![CDATA[
Score 116 | Comments 37 (<a href="https://news.ycombinator.com/item?id=25071502">thread link</a>) | @sciguymcq
<br/>
November 12, 2020 | https://thecodinginterface.com/blog/heroku-postgres-migration-to-aurora/ | <a href="https://web.archive.org/web/*/https://thecodinginterface.com/blog/heroku-postgres-migration-to-aurora/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
        <div>
          
          

          
          
          <h3>Introduction</h3>
<p>In this article I discuss a general process I used recently to migrate a large multi-terabyte Heroku Postgres Database from the Heroku Platform to Amazon Aurora Postgres on a live Heroku based application architecture with near zero downtime and builtin failovers during the process. Not only did this migration save significant costs associated with running a large managed Postgres instance it also resulted in increased scalability and flexibility of parameter turing and other management abilities afforded by AWS RDS.</p>

<h4>Contents</h4>
<ul>
<li><a title="Heroku Architecture and Constraints" href="#heroku-architecture-and-constraints">Heroku Architecture and Constraints</a></li>
<li><a title="The RandoNumba Demo App" href="#rando-numba-demo-app">The RandoNumba Demo App</a></li>
<li><a title="Mimicing the Heroku Postgres Service" href="#mimicing-heroku-postgres">Mimicking the Heroku Postgres Service</a></li>
<li><a title="Restore and Promote EC2 Postgres Log Shipped Replica" href="#log-shipped-replica">Restore and Promote EC2 Postgres Log Shipped Replica</a></li>
<li><a title="Create EC2 Postgres Log Streamed Replica" href="#log-streamed-replica">Create EC2 Postgres Log Streamed Replica</a></li>
<li><a title="Switch App Dyno to Promoted EC2 Postgres" href="#switch-to-log-shipped-replica">Switch App Dyno to Promoted EC2 Postgres</a></li>
<li><a title="Initiate Postgres Logical Replication to Aurora Postgres" href="#initiate-logical-replication-to-aurora">Initiate Postgres Logical Replication to Aurora Postgres</a></li>
<li><a title="Switch App Dyno to Aurora Postgres" href="#switch-to-aurora-postgres">Switch App Dyno to Aurora Postgres</a></li>
</ul>
<h3><a id="heroku-architecture-and-constraints"></a>Heroku Architecture and Constraints</h3>
<p><img src="https://thecodinginterface-images.s3.amazonaws.com/blogposts/heroku-postgres-migration/Heroku-Postgres-Migration.jpg" alt="Heroku Migration Diagram" width="991" height="733"></p>
<p>The Heroku Postgres Migration to AWS Aurora Postgres Architecture and Process Flow diagram above shows, at a high level, the Heroku Postgres Data Layer Architecture for a typical Heroku Premium Level Service with High Availability plus a Read Replica for load balancing Read heavy apps. Then on the right is the migration target goal within the AWS platform boundary annotated with the sequence of steps used to migrate the data to the Aurora Postgres instance.</p>
<p>Before I get into discussing the constraints that I've experienced I would like to put forth an important disclaimer. The Heroku platform is a phenominally innovative service that has paved the way for developing countless extremely useful and profitable apps and services by lowering the barrier to entry for small development teams. Many, if not most, apps will not hit the constraints that I point out below alleviating the need for a migration of their data layer off the platform.</p>
<ul>
<li>Heroku can be costly, perhaps for good reasons, because they alleviate much of the sysops / devops investment and dev time costs of maintaining and scaling services like Postgres</li>
<li>Heroku Postgres locks down the majority of the Postgres parameters that are often necessary to tune for large enterprise level, high throughput, Postgres usage</li>
<li>Heroku Postgres monitoring falls well short of many of the other options, particularly AWS RDS, which becomes very important for large enterprise grade applications</li>
<li>Heroku Postgres does not allow Postgres superuser or Replication User roles so migration options become limited</li>
<li>Postgres pg_dump / pg_restore is not a viable option for large databases of a terabyte and up because of the amount of time it requires to run and thus inherently implies down time or data loss if used as an option for failover or migration which isn't possible for most applications</li>
</ul>

<h3><a id="rando-numba-demo-app"></a>The RandoNumba Demo App</h3>
<p>To faciltate this discussion I've provided a toy app that is deployable to Heroku and built using the Django web framework which simply generates random numbers and scrapes quotes from the web.</p>
<p>1) Clone dj-randonumba-heroku app from my GitHub repo</p>
<pre><code>git clone https://github.com/amcquistan/dj-randonumba-heroku.git
cd dj-randonumba-heroku
</code></pre>
<p>2) Create a Heroku App</p>
<pre><code>heroku create</code></pre>
<p>giving the following output but, note you're output will give a different app name and url</p>
<pre><code>Creating app... done, ⬢ intense-headland-79519
https://intense-headland-79519.herokuapp.com/ | https://git.heroku.com/intense-headland-79519.git</code></pre>
<p>Be sure to update `randonumba.settings.ALLOWED_HOSTS` to include the host that Heroku provides.</p>
<p>3) Push the App Code to Heroku</p>
<pre><code>git push heroku master</code></pre>
<p>4) Attach a free tier Heroku Postgres Add on for Demo Purposes</p>
<pre><code>heroku addons:create heroku-postgresql:hobby-dev -a intense-headland-79519</code></pre>
<p>Use psql to create the hstore Postgres extension this demo app uses</p>
<pre><code>heroku pg:psql -a intense-headland-79519
create extension hstore;
\q</code></pre>
<p>then run migrations</p>
<pre><code>heroku run python manage.py migrate -a intense-headland-79519</code></pre>
<p>5) Register and Play with the App</p>
<p>Open the app in your default browser with the following (replace -a intense-headland-79519 with your app name).</p>
<pre><code>heroku open -a intense-headland-79519</code></pre>
<p>Then register the app and generate some randomness</p>
<p><img src="https://thecodinginterface-images.s3.amazonaws.com/blogposts/heroku-postgres-migration/rando-numba-randomness.png" alt="" width="1838" height="948"></p>

<h3><a id="mimicing-heroku-postgres"></a>Mimicking the Heroku Postgres Service</h3>
<p>In the Real World for this process to work you need to request for the Heroku Data Support team to establish continuous WAL log shipping to an AWS S3 bucket along with a base physical backup using the <a title="WAL-E Python based library" href="https://github.com/wal-e/wal-e" target="_blank" rel="noopener">WAL-E Python based library</a>. Rather than bothering Heroku's Data Support team for this demo and to allow readers to fully reproduce the demo I will simply mimick this step with my own AWS EC2 instance running Postgres 11 and shipping a continuous archive to my own AWS S3 bucket.</p>
<h4>Infrastructure Specs and Services</h4>
<ul>
<li>Amazon Linux 2 AMI</li>
<li>Security Group Allowing port 5432 and SSH access</li>
<li>Separate EBS Volume for Installing the Postgres DB Cluster</li>
<li>S3 Bucket for Pushing Base Physical Backup and WAL</li>
<li>IAM User with Programmatic Access to S3</li>
</ul>
<h4>Procedure for Mimicking Heroku Postgres</h4>
<p>1) Update VPS (Virtual Private Server) and Install Dependencies</p>
<pre><code>sudo yum update -y
sudo amazon-linux-extras install epel -y
sudo amazon-linux-extras install postgresql11 -y
sudo yum install postgresql-server postgresql-contrib python3 lzop pv -y
curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
sudo python3 get-pip.py
sudo python3 -m pip install envdir wal-e[aws]</code></pre>
<p>2) Mount Volume and Create File System for Postgres Cluster</p>
<p>Use lsblk to identify the EBS volume to install Postgres Cluster on</p>
<pre><code>$ lsblk
NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
xvda    202:0    0   8G  0 disk
└─xvda1 202:1    0   8G  0 part /
xvdb    202:16   0  16G  0 disk</code></pre>
<p>Format volume as a XFS filesystem.</p>
<pre><code>sudo mkfs -t xfs /dev/xvdb</code></pre>
<p>Create a mount point for the volume and mount it to the new directory.</p>
<pre><code>sudo mkdir /database
sudo mount /dev/xvdb /database</code></pre>
<p>Find the device's block id and make the mount permanent in /etc/fstab</p>
<pre><code>sudo blkid # will give you the block id </code></pre>
<p>example entry in /etc/fstab</p>
<pre><code>UUID=19ee2212-7fa0-4c9a-bcbf-cd7019d50fd6     /database   xfs    defaults,nofail   0   2</code></pre>
<p>Test the mount (no errors is the successful outcome).</p>
<pre><code>sudo mount -a</code></pre>
<p>Update permissions of the /database directory for postgres.</p>
<pre><code>sudo chown -R postgres:postgres /database
sudo chmod -R 700 /database</code></pre>
<p>3) Create envdir Directory for AWS Creds used for WAL-E</p>
<pre><code>sudo mkdir -p /etc/wal-e/env
sudo chown -R ec2-user:ec2-user /etc/wal-e
echo "INSERT-VALUE-HERE" &gt; /etc/wal-e/env/AWS_ACCESS_KEY_ID
echo "REGION-HERE" &gt; /etc/wal-e/env/AWS_REGION
echo "INSERT-VALUE-HERE" &gt; /etc/wal-e/env/AWS_SECRET_ACCESS_KEY
echo "S3-BUCKET-FOLDER-URL-HERE" &gt; /etc/wal-e/env/WALE_S3_PREFIX
sudo chown -R postgres:postgres /etc/wal-e
sudo chmod -R 755 /etc/wal-e/env</code></pre>
<p>4) Initialize Postgres Cluster</p>
<p>Run as postgres user.</p>
<pre><code>pg_ctl init -D /database</code></pre>
<p>5) Modify Postgres Configs</p>
<p>First modify /database/postgresql.conf with the following.</p>
<pre><code>listen_addresses = '*' or your apps specific IP
wal_level = replica
archive_mode = on
archive_command = 'envdir /etc/wal-e/env wal-e wal-push %p'
archive_timeout = 60</code></pre>
<p>Update /database/pg_hba.conf for auth (substiture 0.0.0.0/0 with your APPs IP as necessary).</p>
<pre><code>host    pgdb            pguser          0.0.0.0/0               md5</code></pre>
<p>6) Start and Enable Postgres Service</p>
<p>First create a systemd service file for managing the postgresql service in /etc/systemd/system/postgresql.service</p>
<pre><code>.include /lib/systemd/system/postgresql.service

[Service]
Environment=PGDATA=/database
</code></pre>
<p>Reload systemd, start and enable postgresql service</p>
<pre><code>sudo systemctl daemon-reload
sudo systemctl start postgresql
sudo systemctl status postgresql
sudo systemctl enable postgresql</code></pre>
<p>7) Create App Postgres User and Database</p>
<pre><code>createuser -e -l --pwprompt pguser
createdb -e --owner=pguser pgdb</code></pre>
<p>Make hstore extension</p>
<pre><code>psql -d pgdb
create extension hstore;
\q</code></pre>
<p>8) Detach Heroku Postgres Addon and Switch Connection String to EC2 Instance</p>
<p>Note that this is ran locally not on the Amazon EC2 VPS</p>
<p>List addons to get name of Heroku Postgres</p>
<pre><code>heroku addons</code></pre>
<p>Detach the addon for Heroku Postgres</p>
<pre><code>heroku addons:detach name-of-addon -a name-of-heroku-app</code></pre>
<p>Replace the DATABASE_URL config variable to point to the newly spun up EC2 Postgres instance mimicking Heroku Postgres</p>
<pre><code>heroku config:set DATABASE_URL=postgres://pguser:develop3r@ec2-ip-address:5432/pgdb -a name-of-heroku-app
heroku ps:restart -a name-of-heroku-app</code></pre>
<p>Run migrations.</p>
<pre><code>heroku run python manage.py migrate -a name-of-heroku-app</code></pre>
<p>At this point you'll want to register a new user and generate some more test data to migrate. You could also use pg_dump / pg_restore to transfer any existing data from Heroku Postgres to this new EC2 Postgres instance being used to mimic Heroku Postgres</p>
<p><br>9) Push Base Backup to S3 Using WAL-E</p>
<p>Note that tests next commands are to be ran on the Amazon Linux VPS.</p>
<p>As a personal preference I like to wrap potentially long running commands in shell scripts so I can explicitly echo out when things begin and end then to protect against SSH connections from timing out during potentially long running processes I use nohup with backgrounding.</p>
<p>For this I create the following script</p>
<pre><code>mkdir /var/lib/pgsql/scripts &amp;&amp; cd /var/lib/pgsql/scripts
vi wal-e-push-backup.sh</code></pre>
<p>with contents</p>
<pre><code>#!/bin/bash

echo "starting wal-e backup-push"

envdir /etc/wal-e/env wal-e backup-push /database

echo "wal-e backup-push complete"
</code></pre>
<p>Run it.</p>
<pre><code>nohup ./wal-e-push-backup.sh &amp;</code></pre>
<p>After the push finishes I should be able to verify that the backup has been pushed with the following command.</p>
<pre><code>envdir /etc/wal-e/env wal-e backup-list</code></pre>
<p>At this point I have an EC2 Instance with Postgres installed mimicking Heroku Postgres and continuously shipping backups and WAL to S3.</p>
<h3><a id="log-shipped-replica"></a>Restore and Promote EC2 Postgres Log Shipped Replica</h3>
<p>In the real world this is where you will start, that is by creating your log shipped replica loaded from data in S3 in the form of a physical base backup and WAL files. The Heroku Data Support team will likely provide you with S3 creds for WAL-E in the form of Heroku config variables which you can …</p></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://thecodinginterface.com/blog/heroku-postgres-migration-to-aurora/">https://thecodinginterface.com/blog/heroku-postgres-migration-to-aurora/</a></em></p>]]>
            </description>
            <link>https://thecodinginterface.com/blog/heroku-postgres-migration-to-aurora/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25071502</guid>
            <pubDate>Thu, 12 Nov 2020 16:27:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Charles Proxy for Web Scraping]]>
            </title>
            <description>
<![CDATA[
Score 28 | Comments 5 (<a href="https://news.ycombinator.com/item?id=25070934">thread link</a>) | @daolf
<br/>
November 12, 2020 | https://www.scrapingbee.com/blog/charles-proxy/ | <a href="https://web.archive.org/web/*/https://www.scrapingbee.com/blog/charles-proxy/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p><img src="https://d33wubrfki0l68.cloudfront.net/0c341714fac1e681a51f25db8d80853d341830a9/c3aea/images/authors/kevin.png" alt="Kevin Sahin">
            
            <span>
                
                
                
                <span>
                    <small> ● </small>
                    
                    
                    <span>12 November, 2020</span>
                    
                    <small> ● </small>
                    <span> 10 min read </span>
                </span>
                <p> Kevin has been working in the web scraping industry for 10 years before co-founding <a href="https://www.scrapingbee.com/">ScrapingBee</a>. He is also the author of the Java Web Scraping Handbook.
                        
                        <a href="https://twitter.com/SahinKevin" target="_blank">
                        <svg style="height: 14px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
                            <path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z" fill="currentColor"></path>
                        </svg>
                        </a>
                        
                        
                    </p>
            </span>
        </p><div property="articleBody">
          





















<div>
  
  <p><img loading="lazy" srcset="
      
        , https://d33wubrfki0l68.cloudfront.net/27c691c6188c7fe4eb579cde7c5f824aa683c60e/4794a/blog/charles-proxy/charles_proxy_header_hue585f61b28a74a671118de43150c5d63_43495_825x0_resize_q75_catmullrom.jpg 825w
      
      
        , https://d33wubrfki0l68.cloudfront.net/3fb096be65ede7086722d0f0a7a0f128c07b6a22/74124/blog/charles-proxy/charles_proxy_header_hue585f61b28a74a671118de43150c5d63_43495_1200x0_resize_q75_catmullrom.jpg 1200w
      
      " src="https://d33wubrfki0l68.cloudfront.net/4163ae1ee1b2e68f8dcbed8300bf0932a70ec915/10de1/blog/charles-proxy/charles_proxy_header.jpg" width="1200" height="628" alt="Charles proxy for web scraping" data-srcset="
    
      , https://d33wubrfki0l68.cloudfront.net/27c691c6188c7fe4eb579cde7c5f824aa683c60e/4794a/blog/charles-proxy/charles_proxy_header_hue585f61b28a74a671118de43150c5d63_43495_825x0_resize_q75_catmullrom.jpg 825w
    
    
      , https://d33wubrfki0l68.cloudfront.net/3fb096be65ede7086722d0f0a7a0f128c07b6a22/74124/blog/charles-proxy/charles_proxy_header_hue585f61b28a74a671118de43150c5d63_43495_1200x0_resize_q75_catmullrom.jpg 1200w
    
    " data-src="https://d33wubrfki0l68.cloudfront.net/4163ae1ee1b2e68f8dcbed8300bf0932a70ec915/10de1/blog/charles-proxy/charles_proxy_header.jpg">
  
</p></div>




<p>Charles proxy is an HTTP debugging proxy that can inspect network calls and debug SSL traffic. With Charles, you are able to inspect requests/responses, headers and cookies. Today we will see how to set up Charles, and how we can use Charles proxy for web scraping. We will focus on extracting data from Javascript-heavy web pages and mobile applications. Charles sits between your applications and the internet:</p>






















<div>
  
  <p><img loading="lazy" srcset="
      
      
      " src="https://d33wubrfki0l68.cloudfront.net/96cee26a1c13dff5a76f17f5c760c70b4e267a67/71117/blog/charles-proxy/charles_drawing.png" width="759" height="419" alt="Charles proxy drawing" data-src="https://d33wubrfki0l68.cloudfront.net/96cee26a1c13dff5a76f17f5c760c70b4e267a67/71117/blog/charles-proxy/charles_drawing.png">
  
</p></div>




<p>Charles is like the Chrome dev tools on steroids. It has many incredible features:</p>
<ul>
<li>Bandwidth throttling to emulate slow internet connection</li>
<li>Request editing (the test the behavior of a back-end API for example)</li>
<li>Repeat requests</li>
<li>Full-text search on a list of request (very interesting for web-scraping)</li>
<li>Many more</li>
</ul>
<p>Charles is a very interesting tool when debugging Single Page application or mobile applications.</p>
<h2 id="installation-and-setup">Installation and set-up</h2>
<p>We are going to see how to install and configure Charles on macOS, but there is also a Windows and Linux version.</p>
<p>First visit <a href="https://www.charlesproxy.com/download/">https://www.charlesproxy.com/download/</a> and download Charles.</p>






















<div>
  
  <p><img loading="lazy" srcset="
      
      
      " src="https://d33wubrfki0l68.cloudfront.net/c942420600c60ee1d70fa6c522f77134fd378055/6c20f/blog/charles-proxy/charles_install.png" width="758" height="568" alt="Charles Installation Screen" data-src="https://d33wubrfki0l68.cloudfront.net/c942420600c60ee1d70fa6c522f77134fd378055/6c20f/blog/charles-proxy/charles_install.png">
  
</p></div>

<figcaption>
    <small> <em> Network tab of your browser developer console </em> </small>
</figcaption>


<p>After installing Charles, you need to install its root certificate. This will allow Charles to intercept and decrypt the SSL traffic.</p>






















<div>
  
  <p><img loading="lazy" srcset="
      
        , https://d33wubrfki0l68.cloudfront.net/b858dc5ff02dbfcf22c15af6f28c903834ca56a2/17b6c/blog/charles-proxy/root-certificate_hu6cc3d5836dbca159577b3f6dd3405add_276647_825x0_resize_catmullrom_2.png 825w
      
      
        , https://d33wubrfki0l68.cloudfront.net/0d05c8b0e8cf7c75e6f3103ab7f7d8c0e7b2801a/380e5/blog/charles-proxy/root-certificate_hu6cc3d5836dbca159577b3f6dd3405add_276647_1200x0_resize_catmullrom_2.png 1200w
      
      
        , https://d33wubrfki0l68.cloudfront.net/1b097620236e124bfea452b202612c875279950c/7af21/blog/charles-proxy/root-certificate_hu6cc3d5836dbca159577b3f6dd3405add_276647_1500x0_resize_catmullrom_2.png 1500w 
      " src="https://d33wubrfki0l68.cloudfront.net/7557901e88720e280d75eb07f7881f37df6a17cc/a65ba/blog/charles-proxy/root-certificate.png" width="1907" height="989" alt="Charles Root certificate" data-srcset="
    
      , https://d33wubrfki0l68.cloudfront.net/b858dc5ff02dbfcf22c15af6f28c903834ca56a2/17b6c/blog/charles-proxy/root-certificate_hu6cc3d5836dbca159577b3f6dd3405add_276647_825x0_resize_catmullrom_2.png 825w
    
    
      , https://d33wubrfki0l68.cloudfront.net/0d05c8b0e8cf7c75e6f3103ab7f7d8c0e7b2801a/380e5/blog/charles-proxy/root-certificate_hu6cc3d5836dbca159577b3f6dd3405add_276647_1200x0_resize_catmullrom_2.png 1200w
    
    
      , https://d33wubrfki0l68.cloudfront.net/1b097620236e124bfea452b202612c875279950c/7af21/blog/charles-proxy/root-certificate_hu6cc3d5836dbca159577b3f6dd3405add_276647_1500x0_resize_catmullrom_2.png 1500w 
    " data-src="https://d33wubrfki0l68.cloudfront.net/7557901e88720e280d75eb07f7881f37df6a17cc/a65ba/blog/charles-proxy/root-certificate.png">
  
</p></div>




<p>After clicking on Install Root Certificate, it will open your macOS Keychain Access, and you will have to open the Trust menu and click on “Always trust”.</p>
<p>This will ask you for your system password.</p>
<p>Now you will need to go to Proxy &gt; SSL Proxying Settings and add “*” or the domain you want to inspect SSL on.</p>






















<div>
  
  <p><img loading="lazy" srcset="
      
      
      " src="https://d33wubrfki0l68.cloudfront.net/9a166609eee9057a6b518e983faf3a7939961f14/da076/blog/charles-proxy/ssl-settings.png" width="704" height="577" alt="SSL Proxying settings" data-src="https://d33wubrfki0l68.cloudfront.net/9a166609eee9057a6b518e983faf3a7939961f14/da076/blog/charles-proxy/ssl-settings.png">
  
</p></div>




<p>Charles will now capture the HTTPS traffic on the domain you've selected.</p>
<h2 id="finding-hidden-apis-on-single-page-applications">Finding hidden APIs on Single Page Applications</h2>
<p>In a traditional <em><strong>server-side</strong></em> rendered website, the HTML code is built by the backend service, and the full page is returned to the HTTP client (generally your browser).
Over the past 10 years, more and more websites are rendered <em><strong>client-side</strong></em> using a Single Page Application framework like React.js, Vue.js or Angular.</p>
<p>Those framework are sending many requests to a back-end API, and it can be a great idea to consume those APIs directly instead of scraping the site and rendering the Javascript with a headless browser to extract the data you want. It will be much faster, you don't need expensive hardware (headless browser needs a lot of RAM and powerful CPUs.).</p>
<p>We are going to look at different Single Page Application to see how Charles proxy can help you discover and extract data from back-end APIs.</p>
<p>Let's start with <a href="https://www.producthunt.com/">ProductHunt</a></p>
<p>ProductHunt is a famous website to launch products online. It's very popular in the tech ecosystem. There are dozens of projects launched every day, so the front page only loads the products of the day. There is an infinite scroll to look at previous days products.</p>
<p>We are going to use Charles proxy to analyze the backend API call and reproduce it with some Python code.</p>
<p>Now open Charles proxy and go to the Producthunt home page, scroll several times to the bottom of the page.</p>
<p>By default, Charles will capture every HTTP request made by your system, not only your browser. So you will get a lot of “pollution”. You can filter that by entering the domain you're interested in:</p>






















<div>
  
  <p><img loading="lazy" srcset="
      
        , https://d33wubrfki0l68.cloudfront.net/47931b5b58ae2245ff98bc98032f3404bb28a56f/b85f4/blog/charles-proxy/charles_filter_huea464f70727612c3c89c18a2a64372ba_104579_825x0_resize_catmullrom_2.png 825w
      
      
        , https://d33wubrfki0l68.cloudfront.net/9ecae3d05e809709f0d039ce8c19ca8025b11387/4bb81/blog/charles-proxy/charles_filter_huea464f70727612c3c89c18a2a64372ba_104579_1200x0_resize_catmullrom_2.png 1200w
      
      
        , https://d33wubrfki0l68.cloudfront.net/71bf530cdcf033637dba926220fa528d526df002/2f072/blog/charles-proxy/charles_filter_huea464f70727612c3c89c18a2a64372ba_104579_1500x0_resize_catmullrom_2.png 1500w 
      " src="https://d33wubrfki0l68.cloudfront.net/272a70f0775e173e0cabfc5a72690fcdc5e139e9/9eb41/blog/charles-proxy/charles_filter.png" width="1905" height="460" alt="Filter by domain in Charles" data-srcset="
    
      , https://d33wubrfki0l68.cloudfront.net/47931b5b58ae2245ff98bc98032f3404bb28a56f/b85f4/blog/charles-proxy/charles_filter_huea464f70727612c3c89c18a2a64372ba_104579_825x0_resize_catmullrom_2.png 825w
    
    
      , https://d33wubrfki0l68.cloudfront.net/9ecae3d05e809709f0d039ce8c19ca8025b11387/4bb81/blog/charles-proxy/charles_filter_huea464f70727612c3c89c18a2a64372ba_104579_1200x0_resize_catmullrom_2.png 1200w
    
    
      , https://d33wubrfki0l68.cloudfront.net/71bf530cdcf033637dba926220fa528d526df002/2f072/blog/charles-proxy/charles_filter_huea464f70727612c3c89c18a2a64372ba_104579_1500x0_resize_catmullrom_2.png 1500w 
    " data-src="https://d33wubrfki0l68.cloudfront.net/272a70f0775e173e0cabfc5a72690fcdc5e139e9/9eb41/blog/charles-proxy/charles_filter.png">
  
</p></div>




<p>If you click on one of the POST requests to the <code>/frontend/graphql</code> endpoint, you can inspect both the request and the response.</p>






















<div>
  
  <p><img loading="lazy" srcset="
      
        , https://d33wubrfki0l68.cloudfront.net/4e8a478174ef6a8e4bd7c4e8fec851e3595e7fa6/f4a8e/blog/charles-proxy/product_hunt_graphql1_hu3869c22f83f3b8a2b9a4675617d88b62_194867_825x0_resize_catmullrom_2.png 825w
      
      
        , https://d33wubrfki0l68.cloudfront.net/b956a33798961031c094f73e32ffe24e9cc55e02/44032/blog/charles-proxy/product_hunt_graphql1_hu3869c22f83f3b8a2b9a4675617d88b62_194867_1200x0_resize_catmullrom_2.png 1200w
      
      
        , https://d33wubrfki0l68.cloudfront.net/4db37d8e9dd976d341f07356c9d716219122b94e/4436d/blog/charles-proxy/product_hunt_graphql1_hu3869c22f83f3b8a2b9a4675617d88b62_194867_1500x0_resize_catmullrom_2.png 1500w 
      " src="https://d33wubrfki0l68.cloudfront.net/939c0b04b7b3d8613354fb3d3f89f0d19d20325b/451c1/blog/charles-proxy/product_hunt_graphql1.png" width="1904" height="960" alt="Sequence of request in Charles proxy, from ProductHunt home page" data-srcset="
    
      , https://d33wubrfki0l68.cloudfront.net/4e8a478174ef6a8e4bd7c4e8fec851e3595e7fa6/f4a8e/blog/charles-proxy/product_hunt_graphql1_hu3869c22f83f3b8a2b9a4675617d88b62_194867_825x0_resize_catmullrom_2.png 825w
    
    
      , https://d33wubrfki0l68.cloudfront.net/b956a33798961031c094f73e32ffe24e9cc55e02/44032/blog/charles-proxy/product_hunt_graphql1_hu3869c22f83f3b8a2b9a4675617d88b62_194867_1200x0_resize_catmullrom_2.png 1200w
    
    
      , https://d33wubrfki0l68.cloudfront.net/4db37d8e9dd976d341f07356c9d716219122b94e/4436d/blog/charles-proxy/product_hunt_graphql1_hu3869c22f83f3b8a2b9a4675617d88b62_194867_1500x0_resize_catmullrom_2.png 1500w 
    " data-src="https://d33wubrfki0l68.cloudfront.net/939c0b04b7b3d8613354fb3d3f89f0d19d20325b/451c1/blog/charles-proxy/product_hunt_graphql1.png">
  
</p></div>




<p>There are many things going on with these requests, but if you compare the content being sent to the GraphQL API, it seems the only thing that changes is the <code>cursor</code> parameter.</p>
<p>It is base64 encoded, but luckily, Charles has a feature to quickly decode Base64 content. You just have to select it and right-click:</p>






















<div>
  
  <p><img loading="lazy" srcset="
      
      
      " src="https://d33wubrfki0l68.cloudfront.net/b6734761222929a603852e3310a4248ce09d7ece/f5dc1/blog/charles-proxy/base64decode.png" width="433" height="470" alt="Sequence of request in Charles proxy, from ProductHunt home page" data-src="https://d33wubrfki0l68.cloudfront.net/b6734761222929a603852e3310a4248ce09d7ece/f5dc1/blog/charles-proxy/base64decode.png">
  
</p></div>




<p>One of the killer features Charles offers is the ability to edit any request and replay. In our case it's really great because there are many headers/cookies values inside the request, so it would be a nightmare to try to reproduce the request with an HTTP client or inside your code.</p>
<p>In order to do that, you can right-click on the request and click on Compose.</p>






















<div>
  
  <p><img loading="lazy" srcset="
      
        , https://d33wubrfki0l68.cloudfront.net/fa48d399f93a2ffdf8ff79613acb85956789ce76/2beba/blog/charles-proxy/compose_charles_hubc4f61e8969f6a1e56de0390583567a6_43272_825x0_resize_catmullrom_2.png 825w
      
      
      " src="https://d33wubrfki0l68.cloudfront.net/f6e5d702cb0531ba1719c9cc807ad4dc73282717/580da/blog/charles-proxy/compose_charles.png" width="1094" height="521" alt="Compose request Charles proxy" data-srcset="
    
      , https://d33wubrfki0l68.cloudfront.net/fa48d399f93a2ffdf8ff79613acb85956789ce76/2beba/blog/charles-proxy/compose_charles_hubc4f61e8969f6a1e56de0390583567a6_43272_825x0_resize_catmullrom_2.png 825w
    
    
    " data-src="https://d33wubrfki0l68.cloudfront.net/f6e5d702cb0531ba1719c9cc807ad4dc73282717/580da/blog/charles-proxy/compose_charles.png">
  
</p></div>




<p>You can then play with the <code>cursor</code> value and replace it with a different page number encoded in base64. Then click on Execute.</p>
<p>You can also try deleting the different cookies (it will work). It's great news because if cookies were mandatory to use this endpoint, it would have been more complicated to reproduce the request with our Python code.</p>
<p>Now you can export the request to cURL:</p>






















<div>
  
  <p><img loading="lazy" srcset="
      
      
      " src="https://d33wubrfki0l68.cloudfront.net/d4650e428f9936b26cd8a6cff8e4fc9a50af9f72/76968/blog/charles-proxy/export_curl.png" width="665" height="496" alt="Compose request Charles proxy" data-src="https://d33wubrfki0l68.cloudfront.net/d4650e428f9936b26cd8a6cff8e4fc9a50af9f72/76968/blog/charles-proxy/export_curl.png">
  
</p></div>




<p>Then you can use a tool like this one to convert the cURL command to Python code (using the wonderful Requests package): <a href="https://curl.trillworks.com/"></a><a href="https://curl.trillworks.com/">https://curl.trillworks.com/</a></p>
<p>I just added the <code>verify=False</code> parameter to avoid SSL warnings with Requests.</p>
<div><pre><code data-lang="python"><span>import</span> requests


headers <span>=</span> {
    <span></span><span>'</span><span>Host</span><span>'</span>: <span></span><span>'</span><span>www.producthunt.com</span><span>'</span>,
    <span></span><span>'</span><span>accept</span><span>'</span>: <span></span><span>'</span><span>*/*</span><span>'</span>,
    <span></span><span>'</span><span>x-requested-with</span><span>'</span>: <span></span><span>'</span><span>XMLHttpRequest</span><span>'</span>,
    <span></span><span>'</span><span>user-agent</span><span>'</span>: <span></span><span>'</span><span>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36</span><span>'</span>,
    <span></span><span>'</span><span>content-type</span><span>'</span>: <span></span><span>'</span><span>application/json</span><span>'</span>,
    <span></span><span>'</span><span>origin</span><span>'</span>: <span></span><span>'</span><span>https://www.producthunt.com</span><span>'</span>,
    <span></span><span>'</span><span>sec-fetch-site</span><span>'</span>: <span></span><span>'</span><span>same-origin</span><span>'</span>,
    <span></span><span>'</span><span>sec-fetch-mode</span><span>'</span>: <span></span><span>'</span><span>cors</span><span>'</span>,
    <span></span><span>'</span><span>sec-fetch-dest</span><span>'</span>: <span></span><span>'</span><span>empty</span><span>'</span>,
    <span></span><span>'</span><span>referer</span><span>'</span>: <span></span><span>'</span><span>https://www.producthunt.com/</span><span>'</span>,
    <span></span><span>'</span><span>accept-language</span><span>'</span>: <span></span><span>'</span><span>en-US,en;q=0.9</span><span>'</span>,
}

data <span>=</span> <span></span><span>'</span><span>{</span><span>"</span><span>operationName</span><span>"</span><span>:</span><span>"</span><span>HomePage</span><span>"</span><span>,</span><span>"</span><span>variables</span><span>"</span><span>:{</span><span>"</span><span>cursor</span><span>"</span><span>:</span><span>"</span><span>OA==</span><span>"</span><span>,</span><span>"</span><span>featured</span><span>"</span><span>:true,</span><span>"</span><span>includePromotedPost</span><span>"</span><span>:false,</span><span>"</span><span>visibleOnHomepage</span><span>"</span><span>:true,</span><span>"</span><span>includeLayout</span><span>"</span><span>:false},</span><span>"</span><span>query</span><span>"</span><span>:</span><span>"</span><span>query HomePage($cursor: String, $postCursor: String, $featured: Boolean!, $includePromotedPost: Boolean!, $visibleOnHomepage: Boolean!) {</span><span>\\</span><span>n sections(first: 1, after: $cursor, featured: $featured) {</span><span>\\</span><span>n edges {</span><span>\\</span><span>n cursor</span><span>\\</span><span>n node {</span><span>\\</span><span>n id</span><span>\\</span><span>n date</span><span>\\</span><span>n cutoff_index</span><span>\\</span><span>n posts_count</span><span>\\</span><span>n cards(first: 1, after: $cursor) {</span><span>\\</span><span>n edges {</span><span>\\</span><span>n node {</span><span>\\</span><span>n ...FeedCards</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n posts(after: $postCursor, visible_on_homepage: $visibleOnHomepage) {</span><span>\\</span><span>n edges {</span><span>\\</span><span>n node {</span><span>\\</span><span>n ...PostItemList</span><span>\\</span><span>n featured_comment {</span><span>\\</span><span>n id</span><span>\\</span><span>n body: body_text</span><span>\\</span><span>n user {</span><span>\\</span><span>n id</span><span>\\</span><span>n ...UserImageLink</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n pageInfo {</span><span>\\</span><span>n endCursor</span><span>\\</span><span>n hasNextPage</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n pageInfo {</span><span>\\</span><span>n endCursor</span><span>\\</span><span>n hasNextPage</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n ad(kind: </span><span>\\</span><span>"</span><span>feed</span><span>\\</span><span>"</span><span>) @include(if: $includePromotedPost) {</span><span>\\</span><span>n ...AdFragment</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n promoted_email_campaign(promoted_type: HOMEPAGE) @include(if: $includePromotedPost) {</span><span>\\</span><span>n id</span><span>\\</span><span>n abTestName</span><span>\\</span><span>n abVariant {</span><span>\\</span><span>n id</span><span>\\</span><span>n ...PromotedEmailAbTestVariantFragment</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n ...PromotedEmailCampaignFragment</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n daily_newsletter {</span><span>\\</span><span>n id</span><span>\\</span><span>n subject</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n viewer {</span><span>\\</span><span>n id</span><span>\\</span><span>n email</span><span>\\</span><span>n has_newsletter_subscription</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n ph_homepage_og_image_url</span><span>\\</span><span>n}</span><span>\\</span><span>n</span><span>\\</span><span>nfragment FeedCards on Card {</span><span>\\</span><span>n ...NewPostsCard</span><span>\\</span><span>n ...BestProductsFromLastWeekCard</span><span>\\</span><span>n ...MakersDiscussionCardFragment</span><span>\\</span><span>n ...GoldenKittyCardFragment</span><span>\\</span><span>n __typename</span><span>\\</span><span>n}</span><span>\\</span><span>n</span><span>\\</span><span>nfragment NewPostsCard on NewPostsCard {</span><span>\\</span><span>n is_dismissed</span><span>\\</span><span>n kind</span><span>\\</span><span>n posts {</span><span>\\</span><span>n ...PostItemList</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n __typename</span><span>\\</span><span>n}</span><span>\\</span><span>n</span><span>\\</span><span>nfragment PostItemList on Post {</span><span>\\</span><span>n id</span><span>\\</span><span>n ...PostItem</span><span>\\</span><span>n __typename</span><span>\\</span><span>n}</span><span>\\</span><span>n</span><span>\\</span><span>nfragment PostItem on Post {</span><span>\\</span><span>n id</span><span>\\</span><span>n _id</span><span>\\</span><span>n comments_count</span><span>\\</span><span>n name</span><span>\\</span><span>n shortened_url</span><span>\\</span><span>n slug</span><span>\\</span><span>n tagline</span><span>\\</span><span>n updated_at</span><span>\\</span><span>n topics {</span><span>\\</span><span>n edges {</span><span>\\</span><span>n node {</span><span>\\</span><span>n id</span><span>\\</span><span>n name</span><span>\\</span><span>n slug</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n ...PostThumbnail</span><span>\\</span><span>n ...PostVoteButton</span><span>\\</span><span>n __typename</span><span>\\</span><span>n}</span><span>\\</span><span>n</span><span>\\</span><span>nfragment PostThumbnail on Post {</span><span>\\</span><span>n id</span><span>\\</span><span>n name</span><span>\\</span><span>n thumbnail {</span><span>\\</span><span>n id</span><span>\\</span><span>n media_type</span><span>\\</span><span>n ...MediaThumbnail</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n ...PostStatusIcons</span><span>\\</span><span>n __typename</span><span>\\</span><span>n}</span><span>\\</span><span>n</span><span>\\</span><span>nfragment MediaThumbnail on Media {</span><span>\\</span><span>n id</span><span>\\</span><span>n image_uuid</span><span>\\</span><span>n __typename</span><span>\\</span><span>n}</span><span>\\</span><span>n</span><span>\\</span><span>nfragment PostStatusIcons on Post {</span><span>\\</span><span>n name</span><span>\\</span><span>n product_state</span><span>\\</span><span>n __typename</span><span>\\</span><span>n}</span><span>\\</span><span>n</span><span>\\</span><span>nfragment PostVoteButton on Post {</span><span>\\</span><span>n _id</span><span>\\</span><span>n id</span><span>\\</span><span>n featured_at</span><span>\\</span><span>n updated_at</span><span>\\</span><span>n created_at</span><span>\\</span><span>n disabled_when_scheduled</span><span>\\</span><span>n has_voted</span><span>\\</span><span>n ... on Votable {</span><span>\\</span><span>n id</span><span>\\</span><span>n votes_count</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n __typename</span><span>\\</span><span>n}</span><span>\\</span><span>n</span><span>\\</span><span>nfragment BestProductsFromLastWeekCard on BestProductsFromLastWeekCard {</span><span>\\</span><span>n posts {</span><span>\\</span><span>n ...PostItemList</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n __typename</span><span>\\</span><span>n}</span><span>\\</span><span>n</span><span>\\</span><span>nfragment MakersDiscussionCardFragment on MakersDiscussionCard {</span><span>\\</span><span>n isDismissed</span><span>\\</span><span>n discussion {</span><span>\\</span><span>n _id</span><span>\\</span><span>n id</span><span>\\</span><span>n ...DiscussionThreadListItem</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n __typename</span><span>\\</span><span>n}</span><span>\\</span><span>n</span><span>\\</span><span>nfragment DiscussionThreadListItem on DiscussionThread {</span><span>\\</span><span>n _id</span><span>\\</span><span>n id</span><span>\\</span><span>n title</span><span>\\</span><span>n description</span><span>\\</span><span>n descriptionHtml</span><span>\\</span><span>n slug</span><span>\\</span><span>n commentsCount</span><span>\\</span><span>n can_comment: canComment</span><span>\\</span><span>n discussionPath</span><span>\\</span><span>n canEdit</span><span>\\</span><span>n votesCount</span><span>\\</span><span>n hasVoted</span><span>\\</span><span>n createdAt</span><span>\\</span><span>n poll {</span><span>\\</span><span>n ...PollFragment</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n user {</span><span>\\</span><span>n id</span><span>\\</span><span>n name</span><span>\\</span><span>n username</span><span>\\</span><span>n headline</span><span>\\</span><span>n avatar</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n __typename</span><span>\\</span><span>n}</span><span>\\</span><span>n</span><span>\\</span><span>nfragment PollFragment on Poll {</span><span>\\</span><span>n id</span><span>\\</span><span>n answersCount</span><span>\\</span><span>n hasAnswered</span><span>\\</span><span>n options {</span><span>\\</span><span>n id</span><span>\\</span><span>n text</span><span>\\</span><span>n imageUuid</span><span>\\</span><span>n answersCount</span><span>\\</span><span>n answersPercent</span><span>\\</span><span>n hasAnswered</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n __typename</span><span>\\</span><span>n}</span><span>\\</span><span>n</span><span>\\</span><span>nfragment GoldenKittyCardFragment on GoldenKittyCard {</span><span>\\</span><span>n is_dismissed</span><span>\\</span><span>n category_for_voting {</span><span>\\</span><span>n id</span><span>\\</span><span>n slug</span><span>\\</span><span>n __typename</span><span>\\</span><span>n }</span><span>\\</span><span>n …</span></code></pre></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.scrapingbee.com/blog/charles-proxy/">https://www.scrapingbee.com/blog/charles-proxy/</a></em></p>]]>
            </description>
            <link>https://www.scrapingbee.com/blog/charles-proxy/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25070934</guid>
            <pubDate>Thu, 12 Nov 2020 15:34:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Koyeb Serverless Engine: Docker Containers, Continuous Deployment of Functions]]>
            </title>
            <description>
<![CDATA[
Score 47 | Comments 10 (<a href="https://news.ycombinator.com/item?id=25070416">thread link</a>) | @edouardb
<br/>
November 12, 2020 | http://koyeb.com/blog/announcing-the-koyeb-serverless-engine-docker-containers-and-continuous-deployment-of-functions | <a href="https://web.archive.org/web/*/http://koyeb.com/blog/announcing-the-koyeb-serverless-engine-docker-containers-and-continuous-deployment-of-functions">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>In July, we announced the early access of the Koyeb platform to help developers and businesses run serverless data-processing apps in minutes. Since then, we have received a lot of feedback and have been at work improving the product.</p><p>Today, we are proud to announce the public availability of the <strong>Koyeb Serverless Engine</strong> with our latest features to <strong>deploy your own code</strong>. In addition to the ready-to-use integrations, you can now seamlessly deploy <strong><a href="#native-support-of-docker-containers">Docker Containers</a></strong> and <strong><a href="#continuous-deployment-of-python-and-nodejs-functions-using-git">Code Functions with built-in Continuous Deployment using Git</a>.</strong></p><p>This release brings the power of the Koyeb Serverless Engine to all developers and businesses with <strong><a href="#event-driven-processing">event-driven processing</a>, <a href="#serverless-autoscaling-and-high-availability">native autoscaling</a>,</strong> and <strong>a complete secret management engine.</strong> The Koyeb platform provides strong primitives for data-processing with our <strong><a href="#object-storage-api-and-data-processing">Universal S3-Compliant Object Storage API</a></strong> and <strong>ready-to-use integrations</strong>. We're also happy to share that <strong><a href="#new-open-source-catalog">our catalog is now open-source</a></strong>.</p><p>The Koyeb platform offers an efficient solution to deploy your serverless applications. It is the best platform to <strong>deploy short and long-running background processing tasks with no time limit for the execution of your jobs</strong>. Common use-cases are:</p><ul><li><em>Media processing</em>: transforming images, videos, audio recordings or PDFs directly at upload</li><li><em>Web scraping and headless browser requests</em>: fetching data from and interacting with websites which do not have any API</li><li><em>Interfacing with slow or asynchronous APIs</em>: calling slow APIs or APIs using callbacks</li><li><em>Asynchronous Computer Vision and Inference</em>: automatic content detection in photos and videos for indexing, metadata enrichment or advanced anlysis</li><li><em>Batch processing</em>: running heavy computations on batches of database records or media</li><li><em>Data science and report generation</em>: analysing data and generating pre-computed reports</li><li><em>Notification reception and processing from IoT devices</em>: reacting to events generated by devices and triggering actions</li><li><em>DevOps</em>: backup, monitoring, build and deployment jobs</li><li>And much more!</li></ul><p><strong>We're happy to provide 1000 hours of compute, 1TB of storage, and 5TB of bandwidth per month for free until the end of the year.</strong>  All the function executions are currently powered by <strong>1GB of RAM and 1 vCPU</strong>. <a target="_blank" rel="noopener" href="https://app.koyeb.com/auth/signup"><strong>Sign up now</strong></a> and start deploying serverless functions!</p><p>One of the recurring requests we got was the ability to <strong>deploy your own code on Koyeb</strong>. We get it: you need to be able to inject your business logic and pair it with our ready-to-use integrations to build your application faster and better.</p><p>When looking for an efficient way to let you manage your stack functions and modifications, we decided to go with the best-practice for code and infrastructure management: <strong>version everything</strong>. We're glad to share that this new release brings a <strong>native integration with git and GitHub</strong> to seamlessly integrate Koyeb with your development workflows.</p><p>Integrating Koyeb in your development environment is a two-step process:</p><ol start="1"><li>Add a <span><code><span>koyeb.yaml</span></code></span> file in your repository describing your stack configuration. Stacks can now be deployed with a simple YAML syntax which should look familiar.
For instance, to deploy a Python 3.8 function with the <span><code><span>handler</span></code></span> entrypoint in the <span><code><span>hello_world</span></code></span> package, your <span><code><span>koyeb.yaml</span></code></span> will look like:</li></ol><pre><code><span>1</span><span>functions:
</span><span>2</span>  - name: hello-world
<span>3</span>    runtime: python3.8
<span>4</span>    handler: hello_world.handler
</code></pre><p>Fork our <a target="_blank" rel="noopener" href="https://github.com/koyeb-community/hello-world-python">Hello World in Python on GitHub</a> to see a simple example in action. You can deploy Python and Node.js functions with the same syntax.</p><ol start="2"><li>Connect your GitHub repository to Koyeb.</li></ol><p>Now each time you <span><code><span>git push</span></code></span>, we will build and deploy your code!</p><p>For Python and Node.js functions, we take care of the complete build process using standard dependency management tools. If you want to read more about deploying code functions, check our <a target="_blank" rel="noopener" href="http://koyeb.com/docs/stacks/quickstart/deploy-python-function">Python function</a> and <a target="_blank" rel="noopener" href="http://koyeb.com/docs/stacks/quickstart/deploy-nodejs-function">Node.js function</a> documentation.</p><p>After looking into the serverless space, we found that serverless solutions were fragmented into two separate generations of products to solve the same problem: containers and code functions. Our research shows that a lot of developers and companies try to use code functions but end up migrating to a container service due to runtime limitations.</p><p>We want you to be able to process your data with the technology you know and love, so we decided to provide <strong>a unified solution to deploy your applications</strong>.</p><p><strong>Containers can be deployed with the same simple YAML syntax as functions.</strong>
For instance, to deploy the <span><code><span>koyeb/cowsay</span></code></span> container from the Docker Hub, you simply need three lines of configuration:</p><pre><code><span>1</span><span>functions:
</span><span>2</span>  - name: hello-koyeb
<span>3</span>    image: koyeb/cowsay
</code></pre><p>The Koyeb Stacks work in a unified way for containers and code functions. The deployment of containers also integrates with git and lets you benefit from native versioning.</p><p>The Koyeb Serverless Engine is completely event-driven, allowing seamless integration with various sources and native autoscaling. The platform not only provides strong integration with events coming from our Object Storage gateway, it also lets you invoke your functions using events respecting the <a target="_blank" rel="noopener" href="https://cloudevents.io/">CloudEvent specification</a>.</p><p>The event system is designed to be powerful with <strong>easy filtering of incoming events using the Common Expression Language</strong>. Here is a simple example, triggering a container dumping the incoming event with <span><code><span>jq</span></code></span> each time an event is received on the Koyeb Object Storage gateway:</p><pre><code><span>1</span><span>functions:
</span><span>2</span>  - name: display-koyeb-event
<span>3</span>    image: stedolan/jq
<span>4</span>    args: [".", "/koyeb/events/in/raw"]
<span>5</span>    events:
<span>6</span>      - cloudevent:
<span>7</span>          expression: event.source == "koyeb.com/gateway"
</code></pre><p>One of the most challenging parts of serverless technologies is troubleshooting. We decided to provide <strong>essential observability features and event tracing as part of the core platform</strong>. All stacks have an audit log with all the events received and which function they triggered. The event content is highly accessible, so you can easily understand your functions' executions and failures.</p><p>As events are the foundation of our connected world, we're exploring use-cases in the IoT space. If you want to talk about events or IoT, please <a target="_blank" rel="noopener" href="http://koyeb.com/contact">contact us</a>!</p><p><a target="_blank" rel="noopener" href="https://www.koyeb.com/docs/stacks/quickstart/bind-store-events">Read more about events in our documentation</a>.</p><p>As part of the Koyeb Platform, we provide an S3-Compliant Object Storage API to store your data. You can use a Koyeb Managed Store or connect your own account cloud service provider. We're happy to share that we already support major cloud service providers including <strong>GCP Storage and AWS S3</strong>.</p><p>We also have an impressive list of cloud service providers in preview: <strong>Azure Blob, Wasabi Storage, Backblaze B2, DigitalOcean Spaces, StackPath Object Storage, and Scaleway Object Storage</strong>.</p><p><strong>Our Serverless Compute Engine is designed to seamlessly integrate with our Object Storage API</strong>. You can easily interact with your Stores from your Koyeb Stack functions and access your data with no effort.</p><p>When you do so, each function execution will get <strong>short-lived credentials</strong> in the environment to access your data store and <strong>prevent credentials leakage</strong>.</p><p>Here is an example of a function using the stores with our secret management engine to fetch the content of an object. The object to fetch and bucket location are automatically provided in the incoming event:</p><pre><code><span>1</span><span>import boto3
</span><span>2</span>import os
<span>3</span>
<span>4</span>def handler(event, context):
<span>5</span>        obj_name = event["object"]["key"]
<span>6</span>        store_name = event["bucket"]["name"]
<span>7</span>        boto_session = boto3.Session(region_name=os.environ[f"KOYEB_STORE_{store_name}_REGION"])
<span>8</span>    store_client = boto_session.resource(
<span>9</span>        "s3",
<span>10</span>        aws_access_key_id=os.environ[f"KOYEB_STORE_{store_name}_ACCESS_KEY"],
<span>11</span>        aws_secret_access_key=os.environ[f"KOYEB_STORE_{store_name}_SECRET_KEY"],
<span>12</span>        endpoint_url=os.environ[f"KOYEB_STORE_{store_name}_ENDPOINT"],
<span>13</span>    )
<span>14</span>    obj = store_client.Object(obj_key).get()
<span>15</span>    content = obj["Body"].read()
<span>16</span>    # Add your own processing logic!
</code></pre><p>Our S3-Compliant object storage API can now also be used as a <strong>standalone solution to benefit from a unified API wherever your data is stored</strong>.</p><p>One of the core benefits of the Koyeb serverless engine is that <strong>autoscaling and high-availability are provided by design</strong>.</p><p>On the availability side, you don't need to worry about dealing with failures of the underlying infrastructure, we take care of <strong>automatically provisioning your functions on a new server in case of a failure</strong>.</p><p>On the scaling side, we <strong>automatically increase the number of containers according to the number of incoming events</strong>. Free accounts have a default scaling limit at 10 to prevent abuse, <a target="_blank" rel="noopener" href="https://app.koyeb.com/support">contact us</a> if you need to scale more!</p><p>Our function catalog has been completely refreshed with <strong>ready-to-use integrations which are now completely open-source</strong>: <a target="_blank" rel="noopener" href="http://github.com/koyeb-community">github.com/koyeb-community</a>.</p><p>It's easy to combine ready-to-use functions with your own code in Stacks. For instance, to to use the <a target="_blank" rel="noopener" href="http://koyeb.com/catalog/function/image-resize">image-resize function from the catalog</a>, simply add to your <span><code><span>koyeb.yaml</span></code></span>:</p><pre><code><span>1</span><span>functions:
</span><span>2</span>  - name: image-resize
<span>3</span>    use: image-resize@1.0.0
<span>4</span>    with:
<span>5</span>      STORE: your-store
<span>6</span>      IMAGE_RESIZE_WIDTH: 150
</code></pre><p>All catalog functions can be easily forked, modified to your needs and deployed thanks to the GitHub integration.</p><p>This post extensively covers all of the platform's new features. If you want to read about complete examples, head to our new <a target="_blank" rel="noopener" href="http://koyeb.com/tutorials">tutorials section</a> where we cover complete end-to-end use-cases:</p><ul><li><a target="_blank" rel="noopener" href="https://www.koyeb.com/tutorials/image-search-app-with-koyeb-aws-rekognition-and-algolia">How to build an application with automatic labeling and indexation of medias using Koyeb, AWS Rekognition and Algolia</a></li><li><a target="_blank" rel="noopener" href="https://www.koyeb.com/tutorials/watermark-images-uploaded-to-your-backblaze-b2-bucket-automatically">How to automatically watermark images uploaded to a Backblaze B2 bucket</a></li><li><a target="_blank" rel="noopener" href="https://www.koyeb.com/tutorials/process-digitalocean-spaces-bucket-images-to-generate-thumbnail">How to process DigitalOcean Spaces images to generate thumbnails</a></li></ul><p>Some of you already spotted some of the new features under development in our documentation: cron to schedule recurring jobs, HTTP event sources, and our CLI are all under construction and scheduled for a release in the coming weeks!</p><p><strong>We're happy to provide 1000 hours of compute, 1TB of storage, and 5TB of bandwidth per month for free until the end of the year! <a target="_blank" rel="noopener" href="https://app.koyeb.com/auth/signup">Sign up now</a> ;)</strong></p><p>As always, we're available through our <a target="_blank" rel="noopener" href="https://app.koyeb.com/support"><strong>support channel</strong></a>, <a target="_blank" rel="noopener" href="https://slack.koyeb.com/"><strong>Slack</strong></a> or through our integrated instant messaging system if you have a question or want to share feedback.</p><p>We'…</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://koyeb.com/blog/announcing-the-koyeb-serverless-engine-docker-containers-and-continuous-deployment-of-functions">http://koyeb.com/blog/announcing-the-koyeb-serverless-engine-docker-containers-and-continuous-deployment-of-functions</a></em></p>]]>
            </description>
            <link>http://koyeb.com/blog/announcing-the-koyeb-serverless-engine-docker-containers-and-continuous-deployment-of-functions</link>
            <guid isPermaLink="false">hacker-news-small-sites-25070416</guid>
            <pubDate>Thu, 12 Nov 2020 14:47:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Netlify for the frontend, Micro for the backend]]>
            </title>
            <description>
<![CDATA[
Score 137 | Comments 59 (<a href="https://news.ycombinator.com/item?id=25068782">thread link</a>) | @chuhnk
<br/>
November 12, 2020 | https://blog.m3o.com/2020/11/12/netlify-for-the-frontend-micro-for-the-backend.html | <a href="https://web.archive.org/web/*/https://blog.m3o.com/2020/11/12/netlify-for-the-frontend-micro-for-the-backend.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">

    <article>

        

<!--         <header class="post-header">
            <a id="blog-logo" href="">
                
                    <span class="blog-title">M3O</span>
                
            </a>
        </header> -->

        <!-- <span class="post-meta">
            <time datetime="2020-11-12">12 Nov 2020</time>
            
        </span> -->

        <!-- <h1 class="post-title">Netlify for the frontend, Micro for the backend</h1> -->

        <section>
            <p><br>
Today Netlify has emerged as the modern platform for rapidly building web applications without having to worry about anything beyond your code. We at <a href="https://m3o.com/">Micro</a>
are users of Netlify and have bought into this phenomenal experience. What’s more Netlify has demonstrated to us a breakdown in the classic web architecture 
stack which previously combined web and api concerns in a single place. As we moved through a tiered architecture frontend had not been given anything more 
than hints on how to consume dynamic content from the backend. Today we’re all calling this the <a href="https://jamstack.org/">Jamstack</a>.</p>

<h2 id="jamstack">Jamstack</h2>

<p><img src="https://d33wubrfki0l68.cloudfront.net/b7d16f7f3654fb8572360301e60d76df254a323e/385ec/img/svg/architecture.svg"></p>
<center><i><small>Credit jamstack.org</small></i></center>
<p><br>
The jamstack rethinks the frontend architecture by separating the concerns of static and dynamic content and pushing for the dynamic side to be consumed 
through APIs and services. Effectively, Netlify embracing this model has tried to build microservices for the frontend and moved towards a unification 
of consumption of services via APIs on the backend. It’s clear this is the architecture of the future for the web and the majority of cloud services 
will be built and consumed soley as APIs.</p>

<p>The one question we’ve really been seeing a lot though is “What’s Netlify for the backend?”. Many of those frontend users building Jamstack apps on 
Netlify are looking to where they can find and build these APIs. It seems even Netlify’s current answer has been, “go host something on heroku”. I think 
in 2020 this just doesn’t fly. If the frontend is being reimagined then the same has to happen on the backend to cater for that use.</p>

<h2 id="netlify-for-the-backend">Netlify for the Backend</h2>

<p><img src="https://blog.m3o.com/assets/images/netlify.png"></p>

<p><a href="https://m3o.com/"><strong>M3O</strong></a> is a platform for cloud services development. The fastest way to build APIs without managing the infrastructure. M3O makes use of 
<a href="https://micro.mu/"><strong>Micro</strong></a>, an open source platform for microservices development. What we get from Micro is a powerful framework for building, running 
and consuming APIs as microservices. What M3O brings to the table is Micro as a Service, a fully managed platform for building microservices. Write services 
in Go and gRPC on the backend, expose them dynamically via HTTP API to be consumed by the frontend. M3O looks to fill that gap in the market for frontend 
devs. M3O is Netlify for the backend.</p>

<h2 id="m3o-features">M3O Features</h2>

<p>As we mentioned, M3O is a fully managed <a href="https://micro.mu/">Micro</a> services platform. What does that mean? Micro provides the building blocks for 
writing, running and consuming microservices. From source to running and beyond. M3O takes that and hosts it so you can just get on with writing 
APIs without worrying about the underlying infrastructure.</p>

<p>Here’s a few of the key features and services:</p>

<ul>
  <li><strong>Microservices development</strong> using <a href="https://grpc.io/">gRPC</a> and protobuf code generation</li>
  <li><strong>Service runtime</strong> and process lifecycle management</li>
  <li><strong>Source to running</strong> builds without need for CI/CD</li>
  <li><strong>Authentication and authorization</strong> for access control and user management</li>
  <li><strong>Dynamic configuration</strong> and secrets management</li>
  <li><strong>PubSub messaging</strong> and event streaming</li>
  <li><strong>Service discovery</strong> and secure networking</li>
  <li><strong>Key-value storage</strong> and persistent CRUD</li>
  <li><strong>Automatic HTTP routing</strong> with path based resolution</li>
  <li><strong>Identity aware proxy</strong> for remote access and gRPC-web apps</li>
  <li><strong>Public API gateway</strong> and TLS support by default</li>
  <li><strong>Public and private repos</strong> support including  github, gitlab and bitbucket</li>
</ul>

<p>M3O is a feature complete platform for microservices development from generating service templates on your local machine through to writing and running 
it in the cloud all using the same Micro CLI experience. M3O exposes HTTPS urls for you dynamically by default. So every service automatically becomes 
an API as soon as you deploy it.</p>

<p>Where a new development model has emerged for the frontend, we think its dictating the “headless” paradigm shift for the backend and M3O wants to be there 
to host all of those APIs as Micro services.</p>

<h2 id="api-first">API First</h2>

<p><img src="https://dev-to-uploads.s3.amazonaws.com/i/0znow24kgpu2dp3zg60n.png"></p>

<p>We are seeing the emergence of APIs as the dominant form factor for cloud services, from AWS all the way through to Twilio and Stripe. What’s even more 
compelling is while this model has emerged in the past few years, we are only really just getting started. It’s our belief that in a decade from now 
some of the most important companies will be API first yet strangely there is no platform to caters to this form of development.</p>

<p>Twilio, Stripe and others have all had to build out the infrastructure for their API first approach. We think as many more companies go down this path 
the tools must emerge to empower them, not just at the compute layer but by providing the higher level abstractions required. That’s the goal of M3O.</p>

<p>But don’t just take our word for it. We’re going to walk you through a demonstration of the value proposition so you can see for yourself just how 
powerful Micro and M3O are.</p>

<h2 id="building-a-backend">Building a backend</h2>

<p>You’re going to be writing and deploying APIs in minutes rather than hours or days! No more dealing with infrastructure on the 
backend, just as Netlify empowered devs on the frontend, we’re doing the same for a new generation of developers on the backend.</p>

<p>Let’s walk you through it. We’ll deploy an existing Micro blog service with this demo frontend on Netlify: <a href="https://loving-goodall-44ee08.netlify.app/">https://loving-goodall-44ee08.netlify.app/</a>. But first let’s start with signup.</p>

<h3 id="signup-to-m3o">Signup to M3O</h3>

<p>First you start by signing up to M3O and registering for a free account in our Dev environment.</p>

<p>Start by installing micro</p>

<div><div><pre><code>curl <span>-fsSL</span> https://install.m3o.com/micro | /bin/bash
</code></pre></div></div>

<p>For those wary of curl into bash, you can view it first <a href="https://install.m3o.com/micro">https://install.m3o.com/micro</a>.</p>

<p>Signup is purely CLI based for now so just issue the following command and follow the steps</p>



<p>Once you’re done you should have an account on M3O and be automatically logged in.</p>

<h3 id="run-helloworld">Run Helloworld</h3>

<p>Validate that by running helloworld.</p>

<div><div><pre><code>micro run github.com/micro/services/helloworld
</code></pre></div></div>

<p>Check it’s running and try call it via the CLI.</p>

<div><div><pre><code><span># check the status</span>
micro status

<span># call helloworld</span>
micro call helloworld <span>--name</span><span>=</span><span>"Alice"</span>
</code></pre></div></div>

<p>OK now we get to the more interesting part. Call it via the HTTP API that’s dynamically generated for you.</p>

<div><div><pre><code><span># get your namespace</span>
<span>NAMESPACE</span><span>=</span><span>$(</span>micro user namespace<span>)</span>

<span># curl your unique url</span>
curl <span>"https://</span><span>$NAMESPACE</span><span>.m3o.com/helloworld?name=Alice"</span>
</code></pre></div></div>

<p>If alls good, we can move on to running something a bit more interesting.</p>

<h3 id="deploying-a-dynamic-blog-backend">Deploying a dynamic blog backend</h3>

<p>We’re going to deploy a headless CMS, or better known as a Blog API.</p>

<p>On the open source side, Micro maintains some reusable example services we can all play with and contribute back to. 
You can check them out at <a href="https://github.com/micro/services">github.com/micro/services</a>. Let’s bootstrap the blog 
using a couple of them.</p>

<p>Because Micro is all about microservices development, we’ve decomposed the blog API into posts, comments and tags. 
Right now we’ll focus on posts and tags. You can find the code in <a href="https://github.com/micro/services/tree/master/blog">https://github.com/micro/services/tree/master/blog</a>.</p>

<p>Deploying these is super simple.</p>

<div><div><pre><code><span># run the posts service</span>
micro run github.com/micro/services/blog/posts

<span># run the tags service</span>
micro run github.com/micro/services/blog/tags
</code></pre></div></div>

<p>Check they’re running using <code>micro status</code>. You should see the status progress through starting, building and running. 
If you want to see logs or anything related just do <code>micro logs posts</code> and the same for any other service by name.</p>

<h3 id="write-a-post-on-the-cli">Write a post on the CLI</h3>

<p>Once services are running they become immediately callable via the CLI as dynamic commands.</p>

<p>Save a quick post:</p>

<div><div><pre><code>micro posts save <span>--id</span><span>=</span>1 <span>--title</span><span>=</span><span>"My first post"</span> <span>--content</span><span>=</span><span>"This is pretty epic"</span>
</code></pre></div></div>

<p>List posts:</p>



<p>The same calls can be made over the API too, just have to know your namespace:</p>

<h3 id="call-it-via-the-http-api">Call it via the HTTP API</h3>

<p>Now here’s where it gets cool and more importantly what you’ll be calling from your frontend apps 
running on Netlify. First grab your namespace like earlier.</p>

<div><div><pre><code><span>$ </span>micro user namespace
random-example-namespace
</code></pre></div></div>

<p>Now just curl it like any other api</p>

<div><div><pre><code><span>$ </span>curl <span>-H</span> <span>"Micro-Namespace: random-example-namespace"</span> https://api.m3o.dev/posts/query
<span>{</span>
  <span>"posts"</span>: <span>[</span>
    <span>{</span>
      <span>"id"</span>: <span>"1"</span>,
      <span>"title"</span>: <span>"My first post"</span>,
      <span>"slug"</span>: <span>"my-first-post"</span>,
      <span>"content"</span>: <span>"This is pretty epic"</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>The generic <code>api.m3o.dev</code> url requires us to pass in our namespace so we query our own service but 
every namespace also gets its own unique URL so you don’t have to worry about this in your frontend 
code. Just compose namespace + m3o.dev as <code>random-example-namespace.m3o.dev</code>.</p>

<div><div><pre><code><span>$ </span>curl https://random-example-namespace.m3o.dev/posts/query
<span>{</span>
  <span>"posts"</span>: <span>[</span>
    <span>{</span>
      <span>"id"</span>: <span>"1"</span>,
      <span>"title"</span>: <span>"My first post"</span>,
      <span>"slug"</span>: <span>"my-first-post"</span>,
      <span>"content"</span>: <span>"This is pretty epic"</span>
    <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>That’s it! We now have the backend for our frontend running on M3O.</p>

<p>Let’s deploy the sample frontend to Netlify just for kicks.</p>

<h2 id="deploying-the-frontend-to-netlify">Deploying the frontend to Netlify</h2>

<p>The frontend is a simple angular app we’ve put together to validate the premise:</p>

<p><strong>Netlify for the frontend, Micro for the backend</strong></p>

<p>You can find the code in <a href="https://github.com/m3o/blog-frontend">github.com/m3o/blog-frontend</a> but 
we’ll walk you through the install now. The deploy settings for the site hosted under 
<a href="https://loving-goodall-44ee08.netlify.app/">loving-goodall-44ee08.netlify.app</a> are as follows:</p>

<h3 id="build-settings">Build settings</h3>

<center>
<img src="https://blog.m3o.com/assets/images/deploysettings.png">
</center>

<p><br>
You can copy the below settings for ease of use. Where you see ‘concert-celtic-uncover’ replace it with your namespace from <code>micro user namespace</code> on the CLI. 
We need this to know what backend API to call.</p>

<div><div><pre><code>Repository        github.com/m3o/blog-frontend
Base directory    Not set
Build command     sed -i 's/micro/concert-celtic-uncover/g' ./src/environments/environment.prod.ts &amp;&amp; ng build --prod &amp;&amp; cp ./src/assets/_redirects ./dist/blog-frontend
Publish directory dist/blog-frontend
</code></pre></div></div>

<p>As you can see, it’s the original <code>m3o/blog-frontend</code> being deployed in the example, but in your case <code>m3o</code> will be replaced with your fork. 
This is because Netlify asks for the permissions to the repo.</p>

<p>The build command is a bit involved, here is what it’s doing:</p>

<div><div><pre><code><span># Replace the micro namespace with your own</span>
<span>namespace</span><span>=</span><span>$(</span>micro user namespace<span>)</span>

<span>sed</span> <span>-i</span> <span>"s/micro/</span><span>$namespace</span><span>/g"</span> ./src/environments/environment.prod.ts

<span># It's an angular app, so we have to ng build</span>
ng build <span>--prod</span>

<span># Single page …</span></code></pre></div></div></section></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.m3o.com/2020/11/12/netlify-for-the-frontend-micro-for-the-backend.html">https://blog.m3o.com/2020/11/12/netlify-for-the-frontend-micro-for-the-backend.html</a></em></p>]]>
            </description>
            <link>https://blog.m3o.com/2020/11/12/netlify-for-the-frontend-micro-for-the-backend.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25068782</guid>
            <pubDate>Thu, 12 Nov 2020 11:44:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Angular 11]]>
            </title>
            <description>
<![CDATA[
Score 147 | Comments 190 (<a href="https://news.ycombinator.com/item?id=25068668">thread link</a>) | @amitport
<br/>
November 12, 2020 | https://blog.angular.io/version-11-of-angular-now-available-74721b7952f7 | <a href="https://web.archive.org/web/*/https://blog.angular.io/version-11-of-angular-now-available-74721b7952f7">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section><div><div><div><div><div><div><p><a href="https://medium.com/@markathompson?source=post_page-----74721b7952f7--------------------------------" rel="noopener"><img alt="Mark Techson" src="https://miro.medium.com/fit/c/96/96/1*2hqK0rVXWghtk_RLFx33oA.jpeg" width="48" height="48"></a></p></div></div></div></div><figure><div role="button" tabindex="0"><div><div><p><img alt="Photo of a Torch Ginger by Jules Kremer" src="https://miro.medium.com/max/2368/0*55kr1t601sp22pkE" width="1184" height="1091" srcset="https://miro.medium.com/max/552/0*55kr1t601sp22pkE 276w, https://miro.medium.com/max/1104/0*55kr1t601sp22pkE 552w, https://miro.medium.com/max/1280/0*55kr1t601sp22pkE 640w, https://miro.medium.com/max/1400/0*55kr1t601sp22pkE 700w" sizes="700px" data-old-src="https://miro.medium.com/max/60/0*55kr1t601sp22pkE?q=20"></p></div></div></div><figcaption>Photo of a Torch Ginger by Jules Kremer</figcaption></figure><p id="1feb"><strong>Welcome to the Angular version 11 release.</strong></p><p id="1877">Version 11.0.0 is here and we’ve got some great updates for Angular developers everywhere. This release has updates across the platform including the framework, the CLI and components. Let’s dive in!</p><h2 id="1182">Updates on Operation Byelog</h2><p id="593b">When we shared <a href="https://angular.io/guide/roadmap" rel="noopener">Angular’s Roadmap</a>, one of the items was Operation Byelog where we committed to putting a significant engineering effort towards triaging issues and PRs until we have a clear understanding of the broader community needs. We can now report that the original goal is complete! We’ve triaged all the issues in all three of the monorepos and will continue this as an ongoing effort as new issues get reported.</p><p id="bc06">This is our commitment: Going forward all new issues reported will be triaged within 2 weeks.</p><p id="2104">In the process, we resolved a few <a href="https://github.com/angular/angular/issues/12842" rel="noopener">popular</a> <a href="https://github.com/angular/angular/issues/18469" rel="noopener">issues</a> in the <a href="https://github.com/angular/angular/issues/13011" rel="noopener">router</a> and <a href="https://github.com/angular/angular/issues/14542" rel="noopener">forms</a>.</p><p id="a1cd">Also, we’ve closed the <a href="https://github.com/angular/angular/issues/11405" rel="noopener"><em>third most popular issue</em></a><em>!</em></p><p id="5773">Now, we’re planning the next steps to support the Angular community. We’ll continue triaging and fixing issues, and work towards improving our processes for accepting community contributions.</p><h2 id="4683">Automatic Inlining of Fonts</h2><p id="6a67">To make your apps even faster by speeding up their <a href="https://web.dev/first-contentful-paint/" rel="noopener">first contentful paint</a>, we’re introducing automatic font inlining. During compile time Angular CLI will download and inline fonts that are being used and linked in the application. We enable this by default in apps built with version 11. All you need to do to take advantage of this optimization is update your app!</p><h2 id="124e">Component Test Harnesses</h2><p id="78d2">In Angular v9 we introduced Component Test Harnesses. They provide a robust and legible API surface to help with testing Angular Material components. It gives developers a way to interact with Angular Material components using the supported API during testing.</p><p id="2932">Releasing with version 11, we have harnesses for all of the components! Now developers can create more robust test suites.</p><p id="3e07">We’ve also included performance improvements and new APIs. The <em>parallel</em> function makes working with asynchronous actions in your tests easier by allowing developers to run multiple asynchronous interactions with components in parallel. The <em>manualChangeDetection</em> function gives developers access to finer grained control of change detection by disabling automatic change detection in unit tests.</p><p id="752c">For more details and examples of these APIs and other new features, be sure to check out the <a href="http://material.angular.io/cdk/test-harnesses/overview" rel="noopener">documentation for Angular Material</a> Test Harnesses!</p><h2 id="173a">Improved Reporting and Logging</h2><p id="968c">We’ve made changes to the builder phase reporting to make it even more helpful during development. We are bringing in new CLI output updates to make logs and reports easier to read.</p><figure><div><div><p><img alt="Screenshot of angular CLI output nicely formatted into columns." src="https://miro.medium.com/max/1214/0*-dCa80651cnfbjpX" width="607" height="355" srcset="https://miro.medium.com/max/552/0*-dCa80651cnfbjpX 276w, https://miro.medium.com/max/1104/0*-dCa80651cnfbjpX 552w, https://miro.medium.com/max/1214/0*-dCa80651cnfbjpX 607w" sizes="607px" data-old-src="https://miro.medium.com/max/60/0*-dCa80651cnfbjpX?q=20"></p></div></div><figcaption>Improved CLI output formatting</figcaption></figure><h2 id="970e">Updated Language Service Preview</h2><p id="650a">The Angular Language Service provides helpful tools to make development with Angular productive and fun. The current version of the language service is based on View Engine and today we’re giving a sneak peek of the Ivy-based language service. The updated language service provides a more powerful and accurate experience for developers.</p><p id="15e6">Now, the language service will be able to correctly infer generic types in templates the same way the TypeScript compiler does. For example, in the screenshot below we’re able to infer that the iterable is of type string.</p><figure><div role="button" tabindex="0"><div><div><p><img alt="Screenshot of intellisense style insights in Angular templates." src="https://miro.medium.com/max/3000/0*L1Tg13gdu3PCqUNN" width="1500" height="902" srcset="https://miro.medium.com/max/552/0*L1Tg13gdu3PCqUNN 276w, https://miro.medium.com/max/1104/0*L1Tg13gdu3PCqUNN 552w, https://miro.medium.com/max/1280/0*L1Tg13gdu3PCqUNN 640w, https://miro.medium.com/max/1400/0*L1Tg13gdu3PCqUNN 700w" sizes="700px" data-old-src="https://miro.medium.com/max/60/0*L1Tg13gdu3PCqUNN?q=20"></p></div></div></div><figcaption>Angular Language Service inferring iterable types in templates</figcaption></figure><p id="e67f">This powerful new update is still in development but we wanted to share an update as we keep preparing it for a full release in an upcoming version.</p><h2 id="48a5">Updated Hot Module Replacement (HMR) Support</h2><p id="303f">Angular has offered support for HMR but enabling it required configuration and code changes making it less than ideal to quickly include in Angular projects. In version 11 we’ve updated the CLI to allow enabling HMR when starting an application with ng serve. To get started, run the following command:</p><pre><span id="3ac4">ng serve --hmr</span></pre><p id="f3bc">After the local server starts the console will display a message confirming that HMR is active:</p><p id="f521">NOTICE: Hot Module Replacement (HMR) is enabled for the dev server.</p><p id="5155">See <a href="https://webpack.js.org/guides/hot-module-replacement" rel="noopener">https://webpack.js.org/guides/hot-module-replacement</a> for information on working with HMR for webpack.</p><p id="3796">Now during development the latest changes to components, templates and styles will be instantly updated into the running application. All without requiring a full page refresh. Data typed into forms are preserved as well as scroll position providing a boost to developer productivity.</p><h2 id="6de3">Faster Builds</h2><p id="630c">We’re bringing a faster development and build cycle by making updates to some key areas.</p><ul><li id="4336">When installing dependencies, the ngcc update process is now 2–4x faster.</li><li id="b30d">Faster compilation with TypeScript v4.0.</li></ul><p id="ed18">Now, teams can opt-in to webpack v5. Currently, you could experiment with <a href="https://webpack.js.org/concepts/module-federation/" rel="noopener">module federation</a>. In the future, webpack v5 will clear the path for:</p><ul><li id="db89">Faster builds with persistent disk caching</li><li id="7ab4">Smaller bundles thanks to <a href="https://webpack.js.org/guides/tree-shaking/" rel="noopener">cjs tree-shaking</a></li></ul><p id="dae8">Support is experimental and under development so we don’t recommend opting in for production uses.</p><p id="4ca8">Want to try out webpack 5? To enable it in your project, add the following section to your package.json file:</p><pre><span id="386c">"resolutions": {<br>     "webpack": "5.4.0"<br>}</span></pre><p id="65a0">Currently, you’ll need to use <strong>yarn</strong> to test this as npm does not yet support the resolutions property.</p><h2 id="d0a9">Linting</h2><p id="e4b2">In previous versions of Angular, we’ve shipped a default implementation for linting (TSLint). Now, TSLint is deprecated by the project creators who recommend migration to ESLint. <a href="https://twitter.com/mrjameshenry" rel="noopener">James Henry</a> together with other folks from the open-source community developed a third-party solution and migration path via <a href="https://github.com/typescript-eslint/typescript-eslint" rel="noopener">typescript-eslint</a>, <a href="https://github.com/angular-eslint/angular-eslint" rel="noopener">angular-eslint</a> and <a href="https://github.com/typescript-eslint/tslint-to-eslint-config" rel="noopener">tslint-to-eslint-config</a>! We’ve been collaborating closely to ensure a smooth transition of Angular developers to the supported linting stack.</p><p id="6071">We’re deprecating the use of TSLint and Codelyzer in version 11. This means that in future versions the default implementation for linting Angular projects will not be available.</p><p id="806d">Head over to the <a href="https://github.com/angular-eslint/angular-eslint#migrating-from-codelyzer-and-tslint" rel="noopener">official project page</a> for a guide to incorporate angular-eslint in a project and migrate from TSLint.</p><h2 id="98a6">Housekeeping</h2><p id="0727">In this update we’re removing support for IE9/IE10 and IE mobile. IE11 is the only version of IE <a href="https://angular.io/guide/browser-support" rel="noopener">still supported</a> by Angular. We’ve also <a href="https://angular.io/guide/deprecations" rel="noopener">removed deprecated APIs</a> and added a few to the deprecation list. Be sure to check this out to make sure you are using the latest APIs and following our recommended best practices.</p><h2 id="a3c7">Roadmap</h2><p id="9aad">We’ve also updated the <a href="https://angular.io/guide/roadmap" rel="noopener">roadmap</a> to keep you posted on our current priorities. Some of the announcements in this post are updates on in-progress projects from the roadmap. This reflects our approach to incrementally rollout larger efforts and allows developers to provide early feedback that we can incorporate it into the final release.</p><p id="8c07">We collaborated with <a href="https://twitter.com/simpulton" rel="noopener">Lukas Ruebbelke</a> from the Angular community on updating the content of some of the projects to better reflect the value they provide to developers.</p><h2 id="6939">How to update to get version 11</h2><p id="0170">When you are ready to go run this command to update Angular and CLI:</p><pre><span id="5983">ng update @angular/cli @angular/core</span></pre><p id="ac5f">Head over to <a href="https://update.angular.io/" rel="noopener">update.angular.io</a> to find detailed information and guidance on updating. We always recommend upgrading one major release at a time to have the best update experience.</p><p id="732f">We hope you enjoy this feature update and be sure to let us know what you think here or on <a href="https://twitter.com/angular" rel="noopener">Twitter</a>!</p></div></div></section></div></div>]]>
            </description>
            <link>https://blog.angular.io/version-11-of-angular-now-available-74721b7952f7</link>
            <guid isPermaLink="false">hacker-news-small-sites-25068668</guid>
            <pubDate>Thu, 12 Nov 2020 11:19:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Become a Shopify Developer – Resources to Become a Shopify Dev]]>
            </title>
            <description>
<![CDATA[
Score 120 | Comments 16 (<a href="https://news.ycombinator.com/item?id=25067407">thread link</a>) | @iliashad
<br/>
November 11, 2020 | https://iliashaddad.com/blog/how-to-become-shopify-developer | <a href="https://web.archive.org/web/*/https://iliashaddad.com/blog/how-to-become-shopify-developer">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><main><div><article><div><p><span>
      <a href="https://iliashaddad.com/static/376fa9f372b3050b854b10d91b9c4224/af240/how-to-become-shopify-developer-0.jpg" target="_blank" rel="noopener">
    <span></span>
  <img alt="how to become shopify developer 0" title="how to become shopify developer 0" src="https://iliashaddad.com/static/376fa9f372b3050b854b10d91b9c4224/0a251/how-to-become-shopify-developer-0.jpg" srcset="https://iliashaddad.com/static/376fa9f372b3050b854b10d91b9c4224/bce2d/how-to-become-shopify-developer-0.jpg 250w,https://iliashaddad.com/static/376fa9f372b3050b854b10d91b9c4224/953fe/how-to-become-shopify-developer-0.jpg 500w,https://iliashaddad.com/static/376fa9f372b3050b854b10d91b9c4224/0a251/how-to-become-shopify-developer-0.jpg 1000w,https://iliashaddad.com/static/376fa9f372b3050b854b10d91b9c4224/e3932/how-to-become-shopify-developer-0.jpg 1500w,https://iliashaddad.com/static/376fa9f372b3050b854b10d91b9c4224/451a4/how-to-become-shopify-developer-0.jpg 2000w,https://iliashaddad.com/static/376fa9f372b3050b854b10d91b9c4224/af240/how-to-become-shopify-developer-0.jpg 2600w" sizes="(max-width: 1000px) 100vw, 1000px" loading="lazy">
  </a>
    </span></p><p>Shopify has been growing so fast in the last couple of years. “We made history in 2018: no other SaaS company has crossed the $1 billion-dollar revenue mark at a faster growth rate than Shopify has,” said <a href="https://investors.shopify.com/Investor-News-Details/2019/Shopify-Announces-Fourth-Quarter-and-Full-Year-2018-Financial-Results/default.aspx">Tobi Lütke</a></p><p>I’m a self-taught developer who worked as a freelancer and indie maker in 2019 and I learned Shopify app and theme development and I want to share with you resources that help me to become a Shopify developer</p><h3><strong>Prerequisite to learn Shopify Theme Development&nbsp;:</strong></h3><p>Shopify theme is like WordPress themes but Shopify use their markup language (Liquid ) instead of PHP</p><p>You need to</p><ul><li>have basic knowledge with HTML5, CSS3, and JavaScript</li><li>have basic knowledge with jQuey (Many Shopify libraries use jQuery ) — Optional</li><li>have basic knowledge with Command Line</li><li>Create a <a href="https://accounts.shopify.com/signup">Shopify Partner account</a> to upload and test your Shopify theme (Free with unlimited Shopify store for development)</li></ul><h3>Prerequisite to learn Shopify App Development&nbsp;:</h3><p>Shopify app is a web app and you can use any programming language like Ruby, Python, PHP or Node JS to build one and in this guide, I’ll focus on Node JS in this guide</p><p>You need to&nbsp;:</p><ul><li>have basic knowledge with HTML5, CSS3</li><li>a good understanding of JavaScript and React (Shopify Polaris built With React but you can use any framework or just vanilla JS)</li><li>good understanding of how to build a full-stack web application (Authentication, consume external API, Send Requests from the front end and deal with it in the back end )</li><li>have basic knowledge with GraphQL (Shopify API built with GraphQL)</li><li>Create a <a href="https://accounts.shopify.com/signup">Shopify Partner account</a> to create, test and publish your Shopify app (Free)</li></ul><h3>Ressources to learn Shopify Theme Development</h3><ul><li><a href="https://www.shopify.com/partners/blog/">Shopify Partner Blog</a>: Articles and guides about design inspiration, Shopify development tips</li><li><a href="https://www.shopify.com/partners/academy">Shopify Partner Academy</a>: Free courses from Shopify team to learn how to work with Shopify tools</li><li><a href="https://www.skillshare.com/classes/Shopify-Essentials-for-Web-Developers-From-Store-Setup-to-Custom-Themes/1070001866/projects">Shopify for web dev</a>elopers: (Free course) Set up your first Shopify store and create your first custom Shopify theme by a Shopify Expert</li><li><a href="https://www.skillshare.com/classes/Advanced-Shopify-Theme-Development/708093439?utm_campaign=video-embed-708093439&amp;utm_source=Video&amp;utm_medium=video-embed">Advanced Shopify</a> Theme: (Free course) Create advanced Shopify theme (Make Shopify theme a single web app using AJAX ) by a Shopify expert</li><li><a href="https://www.shopify.co.uk/partners/shopify-cheat-sheet">Liquid Cheatsheet</a>: Cheatsheet to get all Shopify liquid variables, filters, and helpers</li><li><a href="https://shopify.github.io/liquid-code-examples/">Liquid Code Examples:</a> Collection of code snippets to speed up your development process and understand how to write liquid code</li><li><a href="https://www.youtube.com/playlist?list=PLXQCP3o-w1Pvras8iuflJKO3tfkBT8c0c">Shopify YB Playlist:</a> Youtube course to learn Shopify theme development</li><li><a href="https://shopify.github.io/themekit/">Shopify Theme Kit</a>: a command-line tool to upload your Shopify theme to a Shopify store automatically when changes are made locally</li><li><a href="https://help.shopify.com/en/themes/development">Shopify Theme Docs</a>: Shopify docs to create your Shopify theme</li></ul><h3>Ressources to learn Shopify App Development</h3><ul><li><a href="https://www.shopify.com/partners/blog/">Shopify Partner Blog</a>: Articles and guides about design inspiration, Shopify development tips</li><li><a href="https://www.shopify.com/partners/academy">Shopify Partner Academy</a>: Free courses from Shopify team to learn how to work with Shopify tools</li><li><a href="https://developers.shopify.com/tutorials/build-a-shopify-app-with-node-and-react/set-up-your-app">Shopify React Node App</a>: Tutorial by Shopify team to create your first React and Node JS Shopify app using Polaris (Shopify React design system) and KOA Js to handle server-side rendering</li><li><a href="https://github.com/Shopify/shopify-app-cli">Shopify App CLI:</a> Create your Shopify app like (Create React App), serve your shopify app in Ngrok server (Free) and update your Ngrok server link automatically in your Shopify App dashboard</li></ul><h3>Conclusion</h3><p>Thanks for reading, I wish you the best on your new journey.</p></div></article></div></main></div></div>]]>
            </description>
            <link>https://iliashaddad.com/blog/how-to-become-shopify-developer</link>
            <guid isPermaLink="false">hacker-news-small-sites-25067407</guid>
            <pubDate>Thu, 12 Nov 2020 07:32:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Booting a macOS Apple Silicon Kernel in QEMU]]>
            </title>
            <description>
<![CDATA[
Score 254 | Comments 70 (<a href="https://news.ycombinator.com/item?id=25064593">thread link</a>) | @empyrical
<br/>
November 11, 2020 | https://worthdoingbadly.com/xnuqemu3/ | <a href="https://web.archive.org/web/*/https://worthdoingbadly.com/xnuqemu3/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>I booted the arm64e kernel of macOS 11.0.1 beta 1 kernel in QEMU up to launchd. It’s completely useless, but may be interesting if you’re wondering how an Apple Silicon Mac will boot.</p>

<h2 id="howto">Howto</h2>

<p>This is similar to my previous guide on running <a href="https://worthdoingbadly.com/xnuqemu2/">iOS kernel in QEMU</a>:</p>

<ul>
  <li>install macOS 11.0.1 beta 1 (20B5012D)</li>
  <li>run <a href="https://github.com/zhuowei/XNUQEMUScripts/blob/macos1101b1/macos11/build_arm64e_kcache.sh"><code>build_arm64e_kcache.sh</code></a> to create an Apple Silicon Boot Kext Collection</li>
  <li>build the modified QEMU:
    <div><div><pre><code>git clone https://github.com/zhuowei/qemu
cd qemu
git checkout a12z-macos
mkdir build
cd build
../configure --target-list=aarch64-softmmu
make
</code></pre></div>    </div>
  </li>
  <li>create a modified device tree by running <a href="https://github.com/zhuowei/XNUQEMUScripts/blob/macos1101b1/FourthTry/DTRewriter.java">DTRewriter</a> on <a href="https://updates.cdn-apple.com/2020SummerSeed/fullrestores/001-30235/6D8C0CA3-5952-4FD8-AEB3-4B4CADB626BC/iPad8,11,iPad8,12_14.0_18A5332f_Restore.ipsw">iPad Pro firmware</a>:
    <div><div><pre><code>python3 extractfilefromim4p.py Firmware/all_flash/DeviceTree.j421ap.im4p DeviceTree_iPad_Pro_iOS_14.0_b3.devicetree
java DTRewriter DeviceTree_iPad_Pro_iOS_14.0_b3.devicetree DeviceTree_iPad_Pro_iOS_14.0_b3_Modified.dtb
</code></pre></div>    </div>
  </li>
  <li>run QEMU:
    <div><div><pre><code>./aarch64-softmmu/qemu-system-aarch64 -M virt -cpu max \
  -kernel /path/to/bootcache-arm64e \
  -dtb /path/to/DeviceTree_iPad_Pro_iOS_14.0_b3_Modified.dtb  \
  -monitor stdio -m 6G -s -S -d unimp,mmu \
  -serial file:/dev/stdout -serial file:/dev/stdout -serial file:/dev/stdout \
  -append "-noprogress cs_enforcement_disable=1 amfi_get_out_of_my_way=1 nvram-log=1 debug=0x8 kextlog=0xffff io=0xfff serial=0x7 cpus=1 rd=md0 apcie=0xffffffff" \
  -initrd /path/to/ios14.0b3/ramdisk.dmg $@
</code></pre></div>    </div>
  </li>
  <li>run gdb with <a href="https://github.com/zhuowei/XNUQEMUScripts/blob/macos1101b1/macos11/bootit.gdbscript">this script</a>:
    <div><div><pre><code>~/Library/Android/sdk/ndk/21.0.6113669/prebuilt/darwin-x86_64/bin/gdb \
-D /any/emptydir \
-x bootit.gdbscript
</code></pre></div>    </div>
    
  </li>
</ul>

<p>And the macOS kernel will <a href="https://gist.github.com/zhuowei/5aa668e76f387374cd56848313aa2197">boot into launchd</a>.</p>

<h2 id="is-this-useful">Is this useful?</h2>

<p><img src="https://worthdoingbadly.com/assets/blog/xnuqemu3/wwdc2018_no.jpg" alt="&quot;No.&quot; - Craig Federighi, WWDC 2018"></p>

<p>No:</p>

<ul>
  <li>Absolutely nothing is supported: literally only the kernel and the serial port works, not even the userspace since there’s no disk driver</li>
  <li>Userspace is instead borrowed from iOS 14 b3</li>
  <li>This will never boot anything close to graphical macOS UI</li>
  <li>Most importantly, even if I ever managed to fully boot the macOS kernel, emulating macOS is useless anyways.</li>
</ul>

<p>There are only three reasons I can think of for emulating macOS: security research, software development without a real Apple Silicon machine, and Hackintoshing. This approach will help with none of these:</p>

<ul>
  <li>Emulating iOS is useful for security research when jailbreak is not available. Apple Silicon Macs already support kernel debugging.</li>
  <li>Not useful for software dev: QEMU’s CPU emulation doesn’t support Apple Silicon-specific features, such as Rosetta’s memory ordering or the APRR JIT.</li>
  <li>as for Hackintosh: macOS uses CPU instructions that aren’t available yet on non-Apple ARM CPUs, so you can’t have hardware accelerated virtualization, only very slow emulation. Besides, Hackintoshes are often built when Apple’s own hardware isn’t fast enough; in this case, Apple’s ARM processors are already some of the fastest in the industry.</li>
</ul>

<p>I researched this this not because it’ll be practical, but only to understand how an Apple Silicon Mac works. This will never be a <a href="https://www.youtube.com/watch?v=1AtE54HpXBM">Time Train</a>: only a <a href="https://youtu.be/UswpJh6Zvd8?t=119">science experiment</a>.</p>

<h2 id="what-i-did">What I did</h2>

<h2 id="create-kext-collection">Create kext collection</h2>

<p>On iOS, the kernel and its Kexts are packed together into a bootable file called the <strong>Kernel Cache</strong>.</p>

<p>macOS 11 uses an evolved version of this format, called the <strong>Boot Kext Collection</strong>.</p>

<p>Like the iOS kernelcache, it contains all Kexts required for booting, so the bootloader only needs to load it into memory and jump into it.</p>

<p>To create a boot kext collection, macOS 11 introduces the <a href="https://developer.apple.com/documentation/kernel/installing_a_custom_kernel_extension?language=objc"><code>kmutil</code></a> tool.</p>

<p>Here’s <a href="https://github.com/zhuowei/XNUQEMUScripts/blob/macos1101b1/macos11/build_arm64e_kcache.sh">my script</a> to get <code>kmutil</code> to generate an arm64e kext collection.</p>

<p>It manually excludes some kexts because they cause kmutil to error out. Most are because they depend on ACPI, which is not available on Apple Silicon. I made a <a href="https://github.com/zhuowei/XNUQEMUScripts/blob/macos1101b1/macos11/printexcludekexts.sh">script</a> to detect them.</p>

<p>Debugging <code>kmutil</code> failures on macOS 11 beta 3 was easy because it dumped out the entire NSError message. However, on macOS 11.0.1 beta, Apple decided to hide the full error message and only print an error code. I had to disable SIP and put a breakpoint on <code>swift_errorRetain</code> to get at the underlying error.</p>

<p>Once the <code>build_arm64e_kcache.sh</code> runs, a Boot Kext Collection is created at <code>~/kcache_out/bootcache-arm64e</code>, which can be booted in QEMU.</p>

<h2 id="disassembling-the-boot-kext-collection">Disassembling the boot kext collection</h2>

<p>For debugging, I also had to disassemble the newly created Boot Kext Collection in Ghidra.</p>

<p>Unfortunately, Ghidra isn’t updated for macOS 11 and will refuse to load the file, first giving an error about XML DOCTYPE, then - once that’s worked around - an <a href="https://github.com/NationalSecurityAgency/ghidra/issues/2192">IOException</a> from the invalid <code>ntools</code> value in the <code>LC_BUILD_VERSION</code> load command.</p>

<p>I created a <a href="https://github.com/zhuowei/XNUQEMUScripts/blob/macos1101b1/macos11/patch_boot_kext_collection_for_ghidra.py">script</a> to fix up the kext collection so that Ghidra can load it.</p>

<p>Note that this is still not perfect - Ghidra still doesn’t fixup pointers or read symbols.</p>

<p>To get method names, I also disassembled the raw kernel file (<code>/System/Library/Kernels/kernel.release.t8020</code>) for cross reference. Note that the raw kernel is based at a different address - you can either rebase it in Ghidra, or just be careful to convert addresses.</p>

<h2 id="disable-pac">Disable PAC</h2>

<p>I already had a <a href="https://worthdoingbadly.com/xnuqemu2/">modified QEMU to boot an iOS kernel</a> (which has inspired others, such as <a href="https://alephsecurity.com/">Aleph Security</a>, to build much better open-source iOS emulation platforms)</p>

<p>In early 2019 I updated my modified QEMU to work with PAC for the iPhone Xs/Xr.</p>

<p>QEMU by that time already supported PAC instructions; however, Apple <a href="https://googleprojectzero.blogspot.com/2019/02/examining-pointer-authentication-on.html">modified</a> the crypto algorithm when implementing PAC, so the kernel fails to boot.</p>

<p>I decided to instead <a href="https://github.com/zhuowei/qemu/commit/16613b67ad15a902791109077ebfb1091f1873aa">turn PAC instructions</a> into no-ops, since I don’t know how Apple’s algorithm worked. This also makes it easier to debug the kernel.</p>

<p>Since the macOS DTK used an A12z processor, the modified QEMU just worked.</p>

<h2 id="device-tree">Device tree</h2>

<p>Like iOS, macOS on Apple Silicon uses a <strong>device tree</strong> to describe hardware to the kernel, and to pass boot arguments.</p>

<p>macOS 11.0.1 beta’s installer doesn’t contain a device tree for the DTK: I suspect it would be in the .ipsw files, which are not publically available. Instead, I borrowed the iPad Pro’s device tree from iOS 14 beta 3.</p>

<p>Like the <a href="https://worthdoingbadly.com/xnuqemu2/">iOS in QEMU experiments</a>, I first disabled every piece of hardware in the device tree except the serial port.</p>

<p>Unlike iOS, macOS expects some more information in the device tree:</p>
<ul>
  <li>ram size (since Macs have upgradeable RAM)</li>
  <li>nvram, otherwise panics with a null pointer while reading nonce-seed. (I copied nvram from <a href="https://gist.github.com/bazad/1faef1a6fe396b820a43170b43e38be1">bazad’s dump of an iPhone device tree</a>.)</li>
  <li>AMCC (KTRR) register positions</li>
  <li>System Integrity Protection status</li>
</ul>

<p>I rewrote my <a href="https://github.com/zhuowei/XNUQEMUScripts/blob/macos1101b1/FourthTry/DTRewriter.java">device tree editor</a> to allow populating these extra params.</p>

<h2 id="up-to-launchd">Up to launchd</h2>

<p>I can’t actually boot a macOS root filesystem as I don’t have an emulated hard disk.</p>

<p>I don’t have a recovery ramdisk either: that would likely only be included in the DTK IPSW, which is not public.</p>

<p>Instead, I decided to boot with an iOS ramdisk to test the kernel, and disable signature checking using a GDB breakpoint.</p>

<p>I also couldn’t get the trustcache (list of executables trusted by the kernel) to load. I tried following <a href="https://alephsecurity.com/2019/06/25/xnu-qemu-arm64-2/">Aleph Security’s guide</a>, but macOS 11 is more strict than iOS 12 and needs it below the kernel; I couldn’t figure out the correct memory address.</p>

<h2 id="why-it-took-so-long">Why it took so long</h2>

<p>I actually had this blog post ready since <a href="https://gist.github.com/zhuowei/27816d39f234468cf2956479c0dea7ad">August 9th</a>, but I spent an extra 3 months trying to fix issues, since I really wanted to at least get to a shell!</p>

<p>Unfortunately:</p>
<ul>
  <li>debugging why drivers wasn’t loading was hard</li>
  <li>I couldn’t disable signature checking properly</li>
  <li>I wanted to wait until Apple released an A14 kernel instead of the DTK’s A12, so that we can look at how virtualization works, but they never did</li>
</ul>

<p>It’s now November 9th and Apple’s holding their press conference tomorrow: so it’s <a href="https://www.youtube.com/watch?v=9tAbhrDUrqM">now or never</a>.</p>

<h2 id="whats-left">What’s left</h2>

<p>I’m probably not going to be working further on this, but here’s what one can do to make this an actual useful research platform:</p>

<ul>
  <li>Figure out why half the drivers aren’t loading at all</li>
  <li>Write basic drivers/emulations:
    <ul>
      <li>probably emulate AIC in QEMU (based on Project Sandcastle’s Linux driver) since a custom interrupt controller Kext would be hard to write</li>
      <li>port Apple’s old <a href="https://opensource.apple.com/source/AppleMacRiscPCI/AppleMacRiscPCI-3.4/AppleMacRiscPCI.cpp.auto.html">PowerPC PCIE</a> drivers, since it’s too hard to emulate the Apple Silicon PCIE controller. This will allow us to connect a virtual hard drive.</li>
    </ul>
  </li>
  <li>Switch to the A14 kernel when Apple releases Apple Silicon Macs, so we can test virtualization</li>
</ul>

<h2 id="what-i-learned">What I learned</h2>

<ul>
  <li>How to modify QEMU to disable PAC</li>
  <li>How iBoot on Apple Silicon passes boot options in the device tree</li>
  <li>How to generate an Apple Silicon kernel cache without an Apple Silicon Mac</li>
  <li>How to fight <code>kmutil</code> for the real error message</li>
  <li>Never procrastinate on a blog post for three months</li>
</ul>

  </div>
</article>

      </div>
      
    </div></div>]]>
            </description>
            <link>https://worthdoingbadly.com/xnuqemu3/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25064593</guid>
            <pubDate>Thu, 12 Nov 2020 00:09:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Notion Timeline View]]>
            </title>
            <description>
<![CDATA[
Score 309 | Comments 183 (<a href="https://news.ycombinator.com/item?id=25061781">thread link</a>) | @AlphaWeaver
<br/>
November 11, 2020 | https://www.notion.so/guides/timeline-view-unlocks-high-output-planning-for-your-team | <a href="https://web.archive.org/web/*/https://www.notion.so/guides/timeline-view-unlocks-high-output-planning-for-your-team">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.notion.so/guides/timeline-view-unlocks-high-output-planning-for-your-team</link>
            <guid isPermaLink="false">hacker-news-small-sites-25061781</guid>
            <pubDate>Wed, 11 Nov 2020 19:15:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[AndroWish – run desktop Tcl/Tk programs almost unaltered on the Android Platform]]>
            </title>
            <description>
<![CDATA[
Score 67 | Comments 20 (<a href="https://news.ycombinator.com/item?id=25061292">thread link</a>) | @jakobdabo
<br/>
November 11, 2020 | https://www.androwish.org/index.html/home | <a href="https://web.archive.org/web/*/https://www.androwish.org/index.html/home">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<tbody><tr><td>
<p>Tcl (Tool Command Language) is a very powerful but easy to learn dynamic programming language, suitable for a very wide range of uses, including web and desktop applications, networking, administration, testing and many more. Open source and business-friendly, Tcl is a mature yet evolving language that is truly cross platform, easily deployed and highly extensible.

</p><p>Tk is a graphical user interface toolkit that takes developing desktop applications to a higher level than conventional approaches. Tk is the standard GUI not only for Tcl, but for many other dynamic languages, and can produce rich, native applications that run unchanged across Windows, Mac OS X, Linux and more. 

</p><p>AndroWish allows to run desktop Tcl and Tk programs almost unaltered on the Android Platform while it opens the door to script a rich feature set of a mobile platform. Its sibling <a href="https://www.androwish.org/index.html/wiki?name=undroidwish">undroidwish</a> uses the same code base and  offers a similar feature set on various desktop and embedded platforms.

</p></td><td>
<img width="160px" height="160px" src="data:image/png;base64,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">
</td></tr>
</tbody></div><p>The current AndroWish-debug.apk can be downloaded <a href="https://www.androwish.org/download/AndroWish-c48f047f5b-debug.apk">here</a> (about 36 MByte, requires "install from unknown sources" in Android settings). Prehistoric versions are <a href="http://www.ch-werner.de/sdltk/AndroWish">still available here</a>.


</p></div>]]>
            </description>
            <link>https://www.androwish.org/index.html/home</link>
            <guid isPermaLink="false">hacker-news-small-sites-25061292</guid>
            <pubDate>Wed, 11 Nov 2020 18:36:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Raspberry Pi Homelab with Kubernetes]]>
            </title>
            <description>
<![CDATA[
Score 109 | Comments 48 (<a href="https://news.ycombinator.com/item?id=25061097">thread link</a>) | @amitpm
<br/>
November 11, 2020 | https://amithm.ca/2020/10/kubernetes-raspberrypi-homelab/ | <a href="https://web.archive.org/web/*/https://amithm.ca/2020/10/kubernetes-raspberrypi-homelab/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody"><p>I’ve been running <a href="https://pi-hole.net/">Pi-Hole</a> on a Raspberry Pi 3b wired into my wifi router for most of last year and its been great. So when the new Raspberry Pi 4 came out, I picked one up. It sits on my desk, mostly for easy access to its USB ports, which allows me to hook it up to some of my esp32 devkits and push micropython code onto them. The pi4 has been a great general purpose development environment.</p><p>Recently, I’ve been wanting to write some trivial web endpoints for “internal” dashboards and such for the house. Plus, its a great excuse to learn Golang. In this day and age, clearly a dockerized golang dev environment is the way to go. Have I truly built something, if my dev environment isn’t dockerized?</p><p>So we’re agreed that dockerizing my dev environment is the way to go. Surely if my dev environment is dockerized, how much more should my app deployments use containers? Nothing less will do. But now I need a way to deploy and orchestrate said containers? I know! I should run a kubernetes cluster across my two Pi’s! Might as well run the Pi-hole on it as well, how hard can it be?</p><p>So that is what I spent the better part of last week figuring out.</p><figure><img src="https://imgs.xkcd.com/comics/automation.png" alt="Mandatory xkcd"><figcaption><center>Mandatory xkcd</center></figcaption></figure><p>This blog post walks through what I did, and how I did it, It’s purpose is two-fold -</p><ol><li>It is a map to allow me to retrace my steps if I need to</li><li>Perhaps it may prove of (dubious) use to you.</li></ol><p>So, both my Pi’s run Ubuntu server. I decided I should start from scratch, and flashed the latest ubuntu server image onto the SD cards for both Pi’s. Being a very optimistic person by nature, I expected to have Pi-hole back up and running on this new Kubernetes cluster within a day, and a day of unfiltered ads was a small price to pay for the experience. Alas, it was close to a week before I had Pi-Hole working on my network again, but yay! you get to learn from my experience!</p><p>I didn’t have much of an understanding of Kubernetes components going into this project - but hey, that’s what these projects are meant to give you, and boy, did it. So fret not if you don’t understand some of these terms, the kubernetes documentation pages are great!</p><p>None of this work is original. I cobbled together guides and walkthroughts from various sources to get to this frankenstein’s monster of a post that you see here. You can find links to the sources I used at the end of this page.</p><p>The first step to this journey involves making sure you have the required packages on all your machines. In my case, this was two machines - the Pi4 (called Terminus) and the Pi3b (called Trantor). You need <code>docker</code>, <code>kubelet</code>, <code>kubeadm</code> and <code>kubectl</code>. You want this installed on all your nodes. Terminus will be my master node, Trantor will be my worker. Asimov fans may protest that the Second Foundation was on Trantor after all, but let’s go with this for now. Setting static IPs on the master and workers on your cluster also helps, but I won’t cover that here.</p><p>Update apt repos and packages.</p><div><pre><code data-lang="bash">sudo apt-get update
sudo apt-get upgrade
</code></pre></div><p>Install Docker using the Convenience script. Yes, shame on you for blindly running a script you downloaded from the internet.</p><div><pre><code data-lang="bash">curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
</code></pre></div><p>Let’s make sure our non-root user can use Docker.</p><div><pre><code data-lang="bash">sudo usermod -aG docker $USER
</code></pre></div><p>Now there’s some additional setup that needs to be done in order to get Kubernetes to work on the Raspberry Pi - specifically enabling <code>cgroups</code>. You can do this by editing the file <code>/boot/firmware/cmdline.txt</code> and adding the following options at the end.</p><pre><code>cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1 swapaccount=1
</code></pre><p>You’ll need to reboot the Pi after this.</p><p>Add the K8s apt repo.</p><div><pre><code data-lang="bash">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

cat <span>&lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
</span><span>deb https://apt.kubernetes.io/ kubernetes-xenial main
</span><span>EOF</span>
</code></pre></div><p>You’ll notice we’re using <code>kubernetes-xenial</code> which was the latest release at the time of writing this. Update this to the latest release available if you need to.</p><p>Let’s install our main K8s helpers. We’ll also make sure they’re excluded freom any system upgrades. As the kubernetes <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">documentation</a> says, “<code>kubeadm</code> and <code>kubectl</code> require special attention to upgrade.”</p><div><pre><code data-lang="bash">sudo apt update <span>&amp;&amp;</span> sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
</code></pre></div><figure><img src="https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png" width="200" height="200"></figure><p>Create the cluster by running the following commands on the master node only. Pay special attention to the <code>--pod-network-cidr</code> parameter. You’ll need this CIDR range later on when setting up Flannel.</p><div><pre><code data-lang="bash"><span># Create the bootstrap token</span>
TOKEN<span>=</span><span>$(</span>sudo kubeadm token generate<span>)</span>
sudo kubeadm init --token<span>=</span><span>${</span>TOKEN<span>}</span> --pod-network-cidr<span>=</span>10.10.0.0/16
</code></pre></div><p>Congratulations. You are now the proud owner of a bare-metal kubernetes cluster (with one node). Admire the output, and consider running the commands they ask you to. For example, you’ll need a config file in <code>$HOME/.kube/config</code> if you want <code>kubectl</code> to work without too much hassle. Also make special note of the <code>kubeadm join</code> command as well, you’ll need to run that on your worker nodes.</p><p>These are the commands that the output from the previous step suggest you to run. Run this on the master node, in case that isn’t clear.</p><div><pre><code data-lang="bash">mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown <span>$(</span>id -u<span>)</span>:<span>$(</span>id -g<span>)</span> $HOME/.kube/config
</code></pre></div><p>Go run the <code>kubeadm join</code> commands on all the worker nodes you’d like to dedicate to this cluster. I’ll wait.</p><p>Going through this guide, you’ll quickly become familiar with the command <code>kubectl apply</code>. This command “applies a configuration to a resource” in kubernetes parlance and is typically provided a YAML “manifest” file as parameter.</p><p>So now we have a cluster, but technically Kubernetes doesn’t know how to handle networking between any pods that are scheduled on this cluster - atleast, that’s what I’ve understood. This is why you need an addon like Flannel to handle this for you. You can find a full list of Networking and Network Policy Addons <a href="https://kubernetes.io/docs/concepts/cluster-administration/addons/">here</a>. But in case it isn’t clear yet, we’ll use Flannel.</p><figure><img src="https://raw.githubusercontent.com/coreos/flannel/master/logos/flannel-horizontal-color.png"></figure><p>If you’ve specified a <code>pod-network-cidr</code> parameter when creating your cluster, you’ll need to edit the Flannel manifest with this CIDR before you apply it to the cluster.</p><p>Let’s download the default flannel manifest</p><div><pre><code data-lang="bash">curl https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml --output kube-flannel-updated.yml
</code></pre></div><p>Open it the file up in your favourite editor, and find the key <code>net-conf.json</code>. Update the CIDR given there with the right CIDR for your cluster. Once done, apply the manifest like so.</p><div><pre><code data-lang="bash">kubectl apply -f ./kube-flannel-updated.yml
</code></pre></div><p>To check if this worked, run the following command to get all pods running on your cluster.</p><p>You should see <code>core-dns</code> and <code>kube-flannel</code> pods running like so. I have two pods for each because I have two nodes in my cluster.</p><div><pre><code data-lang="bash">NAMESPACE              NAME                                          READY   STATUS    RESTARTS   AGE
kube-system            coredns-f9fd979d6-h9m47                       1/1     Running   <span>1</span>          3d2h
kube-system            coredns-f9fd979d6-m5jrd                       1/1     Running   <span>1</span>          3d2h
kube-system            kube-flannel-ds-2ngxd                         1/1     Running   <span>1</span>          3d2h
kube-system            kube-flannel-ds-kqflv                         1/1     Running   <span>1</span>          3d2h
</code></pre></div><p>Namespaces are used to isolate pods and services running on the same cluster. My data engineer brain thinks of the cluster as a database and namespaces as schemas, but I could be mistaken and maybe should be thinking of the cluster as a single database install, and the namespaces as individual databases. Or maybe, this is entirely the wrong abstraction to bring in. Scratch all of this, let’s move on.</p><p>We now have a cluster, that knows how to handle pod networking. Let’s run something on it! How about the <a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/">Kubernetes dashboard</a>, so that you have something pretty to show your non-technically inclined significant other as the output of your hard work?</p><figure><img src="https://raw.githubusercontent.com/kubernetes/dashboard/master/docs/images/dashboard-ui.png" alt="Behold! The fruits of your labour!"><figcaption><center>Behold! The fruits of your labour!</center></figcaption></figure><p>We’ll create a namespace to hold everything related to the Kubernetes Dashboard. I’m calling the namespace - <code>kubernetes-dashboard</code>. Very imaginative, no?</p><div><pre><code data-lang="bash">kubectl create namespace kubernetes-dashboard
</code></pre></div><p>We’ll now download the manifest file for Kubernetes dashboard, because we need to make some changes.</p><div><pre><code data-lang="bash">curl https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.4/aio/deploy/recommended.yaml --output kubernetes-dashboard.yaml
</code></pre></div><p>I spent a few days trying to figure out why the manifest did not work out of the box, it kept failing when trying to pull the docker image. I worked around this by doing two things -</p><ol><li>Ran <code>docker pull kubernetesui/dashboard:v2.0.0</code> to cache a local copy of the docker image.</li><li>Commented out the <code>imagePullPolicy: Always</code> in the manifest file under the <code>kubernetes-dashboard</code> deployment block.</li></ol><p>For the more K8s experienced among you, you may be wondering why I did not try using the Helm chart - I did. Kubernetes-dashboard needs to run two services - <code>dashboard-metrics-scraper</code> and <code>kubernetes-dashboard</code>. The Helm chart only seemed to bring up <code>kubernetes-dashboard</code>. I’m sure I must be doing something wrong, but at this point my patience was wearing thin and I just wanted to get on with it.</p><p>Ok, so now we have an edited manifest, let’s apply it.</p><div><pre><code data-lang="bash">kubectl apply -f kubernetes-dashboard.yaml
</code></pre></div><p>It takes a little bit of time for the dashboard to come up. You can amuse yourself by looking at the pods as they spin up as follows -</p><div><pre><code data-lang="bash">watch kubectl get pods -n kubernetes-dashboard
</code></pre></div><p>You can get details on a specific pod by running -</p><div><pre><code data-lang="bash">kubectl describe pod &lt;pod_name&gt; -n kubernetes-dashboard
</code></pre></div><p>You can also tail logs on a specific pod by running -</p><div><pre><code data-lang="bash">kubectl -n kubernetes-dashboard logs &lt;pod_name&gt; -f
</code></pre></div><p>Once you see the dashbaord services up and running, let’s figure out how we actually get access to the dashboard UI.</p><p>We’ll assume that you haven’t configured kubectl on your local machine and are instead, running all these commands from your (headless) raspberry pi.</p><p>Run <code>kubectl proxy</code> first. This exposes the cluster API server over HTTP to the …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://amithm.ca/2020/10/kubernetes-raspberrypi-homelab/">https://amithm.ca/2020/10/kubernetes-raspberrypi-homelab/</a></em></p>]]>
            </description>
            <link>https://amithm.ca/2020/10/kubernetes-raspberrypi-homelab/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25061097</guid>
            <pubDate>Wed, 11 Nov 2020 18:21:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Overview of 3D meshing methods using open source tools]]>
            </title>
            <description>
<![CDATA[
Score 29 | Comments 4 (<a href="https://news.ycombinator.com/item?id=25060942">thread link</a>) | @alibabaSX
<br/>
November 11, 2020 | https://www.sesamx.io/blog/3d_mesh_with_free_tools/ | <a href="https://web.archive.org/web/*/https://www.sesamx.io/blog/3d_mesh_with_free_tools/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">
  <div>
    <div>
      <article role="main">
        

<p>The Internet is full of beautiful 3D mesh examples, but it is difficult to get clues
about how they were constructed. In fact, <strong>creating a good 3D mesh can be very
painful without the right tools or training</strong>. Furthermore, most of commercial
pre-processing software have been improving and propose powerful 3D meshing algorithm,
capable of building hybrid or full hexahedron mesh. <strong>The aim of
this article is to give an overview of the constraints involved when building
a 3D mesh for structural finite element, as well as exposing various meshing
methods relying on free and open source tools</strong>.</p>

<h2 id="introduction">Introduction</h2>

<p>Before we start, let’s give some background information about 3D mesh
construction. Usually a 3D mesh can be composed of 4 types of elements:</p>

<ul>
<li><p>tetrahedron (4 corners),</p></li>

<li><p>wedge (6 corners),</p></li>

<li><p>hexahedron (8 corners),</p></li>

<li><p>and rarely pyramids (5 corners).</p></li>
</ul>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/3d_mesh_element_types.png" alt="3D mesh element types"></p>

<p><strong>The goal is to get a mesh with the lowest number of
degrees of freedom (“dofs”), while maintaining a good representative capability</strong>.
We may be
tempted to race for tetrahedrons, which have only 4 nodes (for a linear
element). But reality is more involved: <strong>certain types of elements behave better
than others</strong>. Without entering to much into the details, we can provide some hints
about this:</p>

<ul>
<li><p><strong>4-node tetrahedron (linear element) must be avoided as mush as possible</strong>. It
behaves poorly and a lot of them are needed to get meaningful results. If you
have no other choice, try to convert them to 10-node tetrahedron which is much
better.</p></li>

<li><p>if you want to stay with linear elements, <strong>you must aim towards hexahedron</strong>. Most
finite element software (you can guess that SesamX is part of them) propose an
improved version of the hexahedron element. That makes it a better choice than
the other elements.</p></li>

<li><p><strong>6-node wedge (linear element)</strong> is better than 4-node tetrahedron but worse than
8-node hexahedron. <strong>It is ok to use them, but go for hexahedron wherever possible</strong>.</p></li>

<li><p>I have not tested the pyramid enough to give relevant advice. Nevertheless,
from the fact that this element is seldom used in 3D mesh, this article will not
shed light on it.</p></li>
</ul>

<p>It is easy to build a full tetrahedron mesh using an automatic mesher
(and it is widespread among various software). On the contrary, <strong>full hexahedron or
hybrid automatic meshers are more involved and harder to find</strong> (you can find them
among commercial solutions but almost not among free ones).</p>

<p><strong>However, using only free and open source tools, we are still able to build
quality 3D meshes</strong>.</p>

<p>The remaining of <strong>this article exhibits 4 methods to build 3D meshes using Salome or
Gmsh</strong>. The goal is not to enter into every detail about the options used, but to
give an overview of how 3D meshes can be built. Whatever the tool we use, changing
the element order is usually a trivial task (either linear or quadratic).
Therefore I will not detail it here. Instead, <strong>I will focus on controlling the
element shapes while meshing</strong>.</p>

<p>For each method, I provide a step by step guide with illustrative screenshots. And, when
appropriate, I also provide the final result file that you can edit and modify
on your own.</p>

<h2 id="geometry-used">Geometry used</h2>

<p>I will use the following con rod to showcase how to build each mesh.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/con_rod_geometry.PNG" alt="Con rod geometry"></p>

<p>One important feature to mention here, is that <strong>this con rod geometry is made of
a compound of 3 solids</strong> (this reason will make sense when talking about the hybrid
mesh generation method) corresponding to each color on the image above.</p>

<p>You can find the corresponding step file
<a href="https://www.sesamx.io/blog/files/008_3d_mesh_with_free_tools/con_rod_to_mesh.step">here</a>.</p>

<h2 id="full-tetrahedron-automatic-meshing-method">Full tetrahedron automatic meshing method</h2>

<p>As mentioned before, <strong>it is pretty straightforward to get a full tetrahedron<br>
mesh</strong>. To build this mesh, we use <strong>Salome</strong>.</p>

<p>First, we go to the geometry module and import the step file. The Salome tree
should look like this:</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/salome_import_tree.PNG" alt="Salome import tree"></p>

<p>Then we have to explode the compound geometry into its 3 sub-solids and create a
partition from these solids. <strong>This step is necessary to ensure that Salome will merge
coincident nodes from each solid faces</strong>. The result tree is then:</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/salome_partition_tree.PNG" alt="Salome partition"></p>

<p>Next we go to the mesh module, and create a new mesh on the partition. Under
algorithm we can select “NETGEN 1D-2D-3D” and under hypothesis “NETGEN 3D
Simple Parameters”. Finally we have to input the edge size that our elements should
have.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/tetrahedron_mesh_parameters.png" alt="Salome tetrahedron mesh parameters"></p>

<p>Eventually we have to right click on the mesh and hit “Compute”. The mesh should
look like this:</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/tetrahedron_mesh.PNG" alt="Salome tetrahedron mesh"></p>

<p>As you can see, the mesh is made of tetrahedron but also triangles and edges
elements. To get rid of the 2D and 1D elements, the first step is to
click on the mesh and select “Create Group”. A panel appear and we can create one
group containing all the 2D elements (as shown on the picture below) and
similarly for the 1D elements.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/salome_create_group.png" alt="Salome create group"></p>

<p>Next to delete these elements, we have to right
click on each group and select “Delete Group with Content”. <strong>And we get
the following full tetrahedron mesh</strong>.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/final_tetrahedron_mesh.PNG" alt="Salome final tetrahedron mesh"></p>

<p>Finally, <strong>to check that the mesh does not contain any duplicated nodes</strong> we have to
select the mesh and use “Controls / Node Controls / Double Nodes”.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/full_tet_double_nodes.PNG" alt="Tetrahedron duplicated nodes check"></p>

<p>If you want to manipulate this mesh, you can find the corresponding Salome database
<a href="https://www.sesamx.io/blog/files/008_3d_mesh_with_free_tools/full_tet_con_rod.hdf">here</a>.</p>

<h2 id="full-hexahedron-automatic-meshing-method">Full hexahedron automatic meshing method</h2>

<p>Next come the full hexahedron mesh. To build this mesh, we use <strong>Gmsh</strong>.</p>

<p>First, <strong>it is necessary to create a volume physical group containing the 3
solids of the model. It ensure afterwards that the mesh export will, in fact,
export only the 3D elements.</strong></p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/gmsh_volume_physical_group.PNG" alt="Gmsh volume physical group"></p>

<p>Next we go to “Tools / Options” then “Mesh / General” to select the meshing
parameters. We can
choose whatever makes it for the 2D algorithm, 3D algorithm and 2D recombination
algorithm. These parameters influence how the mesh is built, feel free to change
them to notice the difference in the mesh. As a first guess, we can
stay with “Delaunay” and “Blossom”. However, <strong>make sure to select “All Hexas” as the
“Subdivision algorithm” so that the volumes will be filled with hexahedron only.</strong></p>

<p>Finally under “Min/Max element size” we can fix the element size.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/gmsh_meshing_parameters.PNG" alt="Gmsh meshing parameters"></p>

<p>Eventually, we have to go back to the Gmsh tree and click “3D” under “Mesh” to
build the mesh.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/gmsh_mesh_creation.png" alt="Gmsh mesh creation"></p>

<p>We can check the mesh content under “Tools / Statistics”. As you can see, the mesh
is made of 1D, 2D and 3D-hexahedron elements. <strong>Because we have created a physical
group for the 3 volumes, only the 3D mesh will be exported.</strong></p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/final_hexaedron_mesh.PNG" alt="Gmsh mesh creation"></p>

<p>If you have troubles visualizing the 3D elements, you can adjust the visibility
parameters in the “Mesh” options window under the “Visibility” tab.</p>

<p>Unfortunately, there is a trap here. <strong>The mesh obtained has duplicated nodes</strong>
at the interfaces between the 3 solids. To visualize them, we can export the mesh
as a .med file, import it in Salome, and use the “double nodes” tool mentioned
previously.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/full_hexahedron_duplicated_nodes.PNG" alt="Hexahedron duplicated nodes"></p>

<p>Finally, to solve this issue, we have to use the “Merge Nodes” tool under
“Modification / Transformation”.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/full_hexahedron_remove_duplicated.PNG" alt="Hexahedron remove duplicated nodes"></p>

<h2 id="hybrid-meshing-method">Hybrid meshing method</h2>

<p>Next <strong>I am showcasing how to build an efficient hybrid mesh with Salome.</strong> This
method is my favorite because <strong>it leads to a well structured mesh, which can
capture more efficiently the details of the geometry</strong> (if you have a close look
to the automatic tetrahedron and hexadreon meshes, you can see that the fillet
is not always “well captured” for instance). The drawback of this method is that it does not
lead to a full hexahedron mesh but an hybrid mesh made with a majority of
hexahedrons, and a minority of wedges used to fill gaps.</p>

<p><strong>The 3D mesh will be built first by meshing 2D surfaces and then by extruding
them.</strong> To make this process workable, the geometry has been split into 3 solids
beforehand. Each of these solids can then be meshed as an extrusion of the surface
meshes.</p>

<table>
<thead>
<tr>
<th><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/hybride_mesh_solid_1.PNG" alt="Solid 1"></th>
<th><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/hybride_mesh_solid_2.PNG" alt="Solid 2"></th>
<th><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/hybride_mesh_solid_3.PNG" alt="Solid 3"></th>
</tr>
</thead>

<tbody>
<tr>
<td><center><em>Solid 1</em></center></td>
<td><center><em>Solid 2</em></center></td>
<td><center><em>Solid 3</em></center></td>
</tr>
</tbody>
</table>

<p>As explained for the full-tetrahedron mesh, <strong>we first need to explode the
compound geometry and build a partition.</strong></p>

<p>Then, in order to build the 2D meshes on the surfaces and the 3D extrusion meshes,
we need to extract (using explode) the relevant geometries from this partition:</p>

<ul>
<li>the 3 solids geometries,</li>
<li>the top face of solid 1 (red face on solid 1 image), that will drive the
3D mesh on solid 1,</li>
<li>the “fillet face” of solid 2 and 3 (red face on solid 3 image) that will drive
the 3D mesh on solid 2 and solid 3.</li>
</ul>

<p>The Salome tree should look like this:</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/salome_partition_tree_hybrid.PNG" alt="Salome tree for hybrid mesh"></p>

<p>Next, we go to the mesh module. The meshing process is the following:</p>

<ul>
<li><p>Create a mesh object and assign default 3D mesh parameters to the whole
partition,</p></li>

<li><p>Create 2 sub-meshes for the 2 surfaces to mesh,</p></li>

<li><p>Create 3 sub-meshes for the 3 solids to mesh.</p></li>
</ul>

<p><strong>The default 3D meshing parameters will not be used while computing the mesh,
because the sub-meshes definition will cover the whole partition.</strong> Nevertheless, Salome
still requires these default parameters.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/tetrahedron_mesh_parameters.png" alt="Salome import tree"></p>

<p>To create the faces sub-meshes, we have to right click on the mesh and
select “Create Sub-mesh”. We then need to select one of the faces and choose the
“NETGEN 1D-2D”
algorithm with “NETGEN 2D Simple Parameters”. Then we can input the element size
and <strong>make sure to check “Quad-dominated” (to avoid at most triangles)</strong>.</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/face_submesh_parameters.png" alt="Salome face submesh parameters"></p>

<p>To compute the sub-mesh, we have to right click on it and select “Compute
Sub-mesh”. And we repeat these operations for the second face.</p>

<p><strong>Creating the 3D sub-meshes is similar.</strong> Once we have selected the solid to mesh,
we choose “Extrusion 3D” as the meshing algorithm and no hypothesis needs to be associated.
However, we have to provide the 1D algorithm and hypothesis to define
how the mesh extrusion should behave.</p>

<p>We select “Wire Discretisation” as the 1D algorithm, and the previous local length
used for the 2D sub-meshes as the hypothesis.</p>

<table>
<thead>
<tr>
<th><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/hybrid_3d_submesh_parameters.png" alt="Hybrid 3D submesh parameters"></th>
<th><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/hybrid_1d_submesh_parameters.png" alt="Hybrid 1D submesh parameters"></th>
</tr>
</thead>

<tbody>
<tr>
<td><center><em>3D submesh parameters</em></center></td>
<td><center><em>1D submesh parameters</em></center></td>
</tr>
</tbody>
</table>

<p>We repeat this for the 2 other solids. The mesh tree should look like this:</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/hybrid_mesh_tree.PNG" alt="Salome hybrid mesh tree"></p>

<p><strong>Before computing the mesh, we need to tell Salome in which order the sub-meshes
should be computed.</strong> To avoid meshing conflict while extruding, it is best in
our case, to fully mesh solid 1 before meshing the driving surface of solid 2
and solid 3. We have to right click on the mesh and select “Change sub-mesh Priority”.
The meshing order should be the following:</p>

<p><img src="https://www.sesamx.io/blog/img/008_3d_mesh_with_free_tools/hybrid_mesh_order.PNG" alt="Salome hybrid mesh tree"></p>

<p>After …</p></article></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.sesamx.io/blog/3d_mesh_with_free_tools/">https://www.sesamx.io/blog/3d_mesh_with_free_tools/</a></em></p>]]>
            </description>
            <link>https://www.sesamx.io/blog/3d_mesh_with_free_tools/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25060942</guid>
            <pubDate>Wed, 11 Nov 2020 18:07:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Doomsday prepping – Disaster planning for less crazy folk (2016)]]>
            </title>
            <description>
<![CDATA[
Score 170 | Comments 115 (<a href="https://news.ycombinator.com/item?id=25060418">thread link</a>) | @VBprogrammer
<br/>
November 11, 2020 | https://lcamtuf.coredump.cx/prep/ | <a href="https://web.archive.org/web/*/https://lcamtuf.coredump.cx/prep/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">

 

<a name="1"></a>
<h2>1. Introduction <span>[<a href="#1">link</a>]</span></h2>

<p>
The prepper culture begs to be taken with a grain of salt. In a sense, it has
all the makings of a doomsday cult: a tribe of unkempt misfits who hoard gold
bullion, study herbalism, and preach about the imminent collapse of our society.
</p>

<p>
Today, we see such worries as absurd. It's not that life-altering disasters are
rare: every year, we hear about millions of people displaced by wildfires, earthquakes,
hurricanes, or floods. Heck, not a decade goes by without at least one first-class
democracy lapsing into armed conflict or fiscal disarray. But having grown up in a period
of unprecedented prosperity and calm, we take our way of life for granted - and find
it difficult to believe that an episode of bad weather or a currency crisis could
upend our lives for good.
</p>

<p>
I suspect that we dismiss such hazards not only because they seem surreal, but also because
worrying about them makes us feel helpless and lost. What's more, we follow the same instincts
to tune out far more pedestrian and avoidable risks; for example, 
most of us don't plan ahead for losing a job, for dealing with a week-long water outage, or
for surviving the night if our home goes up in smoke.
</p>

<p>
For many, the singular strategy for dealing with such dangers is to pray for the
government to bail us out. But no matter if our elected officials prefer to school us with
passages from
<a href="https://smile.amazon.com/dp/0156334607">Milton
Friedman</a> or from
<a href="https://smile.amazon.com/dp/0486477487" title="I'm sorry... I'm really sorry!">Thomas
Piketty</a>, the hard truth is that no state can provide a robust safety net for all
of life's likely contingencies; in most places, government-run social programs are severely deficient in funding, in
efficiency, and in scope. Large-scale disasters pit us against even worse odds; from New Orleans in 2005 to
Fukushima in 2011, there are countless stories of people left behind due to political dysfunction, poorly
allocated resources, or lost paperwork.
</p>

<p>
And so, the purpose of this guide is to combat the mindset of learned helplessness by
promoting simple, level-headed, personal preparedness techniques that are easy to
implement, don't cost much, and will probably help you cope with whatever life throws your way.
</p>

<p>
Oh, one thing: in contrast to most other docs of its kind, this page an
unadulterated labor of love; there are no affiliate links, paid product placements, or ads anywhere in the guide.
</p>

<a name="2"></a>
<h2>2. Mapping out the unknown <span>[<a href="#2">link</a>]</span></h2>

<p>
Effective preparedness can be simple, but it has to be rooted in an honest and
systematic review of the risks you are likely to face. Plenty of excited newcomers begin
by shopping for ballistic vests and night vision goggles; they would be better
served by grabbing a fire extinguisher, some bottled water, and then putting the rest of
their money in a rainy-day fund.
</p>

<p>
To maintain sanity while trying to enumerate risks, I found that it's best to focus on
broad outcomes instead of trying to track down every single way for things to go south.
Say, it should not matter if you are laid off because of a downsizing, because
your new boss hates you, or because they finally catch you stealing paperclips. The
outcome is the same: you are out of a job and urgently need a way to pay your bills.
</p>

<p>
Another insidious distraction is the desire to immediately figure out how to respond to all the scenarios
we end up dreaming of. Let's save that for later; by prematurely focusing on the second half of the
problem, we may end up glossing over some of the less tractable scenarios - or make
haphazard assumptions that will cloud our judgment in other ways.
</p>

<p>
I also found that to come up with a rational threat model, we need to think of "risk" as a product of
both the probability and the consequences of a given event. By that metric, stubbed toes
and zombie outbreaks are equally uninteresting; one of them has nearly zero significance,
the other, nearly zero odds.
</p>

<p>
What else? Ah, right: the final piece of advice I have is to keep things uncomplicated. There are
popular doomsday predictions that deal with cutting-edge particle physics, god-like computer
hackers, vast government conspiracies, or extraterrestrial messages hidden in pop songs. I suppose
we can't <i>really</i> rule that stuff out, but historical data suggests that there's a lot more
merit in worrying about falling off a ladder or getting hit by a car.
</p>

<p>
All right! With these caveats in mind, let's go over some canonical scenarios that are worth thinking about.
</p>

<a name="2.1"></a>
<h3>2.1. Problem space #1: Small-scale events <span>[<a href="#2.1">link</a>]</span></h3>

<p>
It's always fun to speculate about solar flares and supervolcanoes; it's far more mind-numbing to
seriously evaluate the consequences of backed up sewage or burst water mains. But in reality,
such unglamorous, small-scale incidents are far more likely to disrupt and reshape our
lives.
</p>

<p>
Broadly speaking, disastrous outcomes of such humdrum contingencies can be divided into
several groups:
</p>

<ul>

<li>
<p>
<b>Insolvency.</b>
  If a person over the age of 40 tells you that they have never lost a job, they are
  pretty lucky (or lying). Yet, the risk is seldom taken seriously; many middle-class,
  single-income families would be in deep trouble if it ever took them more than 2-3 months
  to find a new, equally well-paying gig.
</p>

</li><li>
<p>
<b>Disrupted access to water, food, energy, or transportation.</b>
  Substantial and prolonged outages happen everywhere; many of us will experience
  at least one at some point in our lives. A week without electricity may be just
  inconvenient and scary, especially in a high-rise or
  in a seedy neighborhood; but even a single hot day without potable water is really
  bad news.
</p>

</li><li>
<p>
<b>Loss of shelter.</b>
  Every year, there are over 350,000 house fires in the United States. Such accidents
  usually aren't deadly - but if you are unlucky, they can leave you stranded in the middle
  of the night in your PJs, with no documents or credit cards in hand.
</p>

</li><li>
<p>
<b>Unintentional injury.</b>
  Largely preventable and predictable incidents - such as falls, vehicle collisions, and poisonings -
  account for some 40 million ER visits annually. And lest you say people are simply too
  quick to rush to the hospital, said incidents also result in about 100,000 US
  deaths every year.
</p>

</li><li>
<p>
<b>Intentionally inflicted harm.</b>
  Violent crime is essentially <i>normal</i> almost everywhere in the world.
  In the US in the 90s, your lifetime likelihood of victimization was
  estimated to be around 80%; the odds of suffering criminal injury hovered at 40%. More recent
  research is hard to come by - but rest assured, life-threatening encounters remain a very real risk.
</p>

</li><li>
<p>
<b>Debilitating illness or death.</b>
  It's going to get you; maybe next week, maybe in 50 years. We can't really predict the day,
  but we can understand and meaningfully manage the impact it will have on those who depend on us -
  say, our stay-at-home partners or young kids.
</p>

</li></ul>

<p>
All in all, the risks discussed in this section have three defining characteristics: they are relatively
likely to happen; are strikingly easy to mitigate (we'll get into that soon); and tend to be so
unglamorous that they seldom make the cut in any "serious" guide to emergency preparedness.
</p>

<a name="2.2"></a>
<h3>2.2. Problem space #2: Mass calamities <span>[<a href="#2.2">link</a>]</span></h3>

<p>
If an errant backhoe took out the utilities for your block, you would probably head to the
grocery store to pick up bottled water (and use their restrooms, too). But if a
once-in-a-century storm damaged major roads and left half the city without running water, your
options wouldn't be as clear-cut.
</p>


<p>
That's why we have to look at larger-scale emergencies through somewhat different lens, taking into
account their likely magnitude, duration, and the nature of the forces at play. Some of the
plausible scenarios to think about include:
</p>

<ul>

<li>
<p>
<b>Natural disasters.</b>
  Common examples include floods, hurricanes, earthquakes, wildfires, and heatwaves. In some
  regions, such events are very rare; in others, they are almost guaranteed every decade or two.
</p>

</li><li>
<p>
<b>Industrial accidents.</b>
  Many people live in the proximity of heavy industries - say, refineries, freight railroads, or power plants.
  Depending on the type of industrial facilities nearby, you may want to evaluate the potential
  consequences of upwind and upstream explosions or chemical spills.
</p>

</li><li>
<p>
<b>Social unrest.</b>
  Riots are a distinct risk in many urban and suburban areas around the world. When angry mobs
  take it to the streets, widespread arson and violent crime are not unheard of, sometimes going
  on for days or weeks.
</p>

</li><li>
<p>
<b>Economic crises.</b>
  All highly developed countries go through cyclic recessions and periods of high unemployment;
  the US had about ten big ones in the past 100 years alone. Sometimes, such events
  are accompanied by bank runs and collapses of financial institutions; other times,
  they involve hyperinflation, product rationing, and currency controls.
</p>

</li><li>
<p>
<b>Pandemic.</b>
  It's been a while since the highly developed world experienced a devastating outbreak, but it
  may be premature to flat out dismiss the risk. In 1918, an unusual strain of flu managed to kill 75
  million people. Few years later, a mysterious sleeping sickness - probably also of viral origin -
  swept the globe, crippling millions, some for life. We aren't necessarily better prepared
  for similar events today.
</p>

</li><li>
<p>
<b>Terrorism or conventional war.</b>
  We think we would see it coming - but history shows that such events tend to catch nations
  off guard. These phenomena are noteworthy not only because of their immediate death toll,
  which can be relatively low - but because of the far-reaching and long-term socioeconomic
  disruption they can cause.
</p>

</li></ul>

<p>
Most of us will probably not get tangled up in a large-scale disaster of any sort, but it
is only wise to hedge your bets. There are countless examples to demonstrate that such events
happen often and can strike close to home - say:
</p>

<ul>

<li>
<p>
The EU debt crisis, from 2009 onward. A series of events that led to staggering unemployment rates
in Greece, deposit confiscations in Cyprus, and uncertain prospects for the entire
eurozone.
</p></li></ul></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://lcamtuf.coredump.cx/prep/">https://lcamtuf.coredump.cx/prep/</a></em></p>]]>
            </description>
            <link>https://lcamtuf.coredump.cx/prep/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25060418</guid>
            <pubDate>Wed, 11 Nov 2020 17:26:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Xilinx-Samsung SmartSSD Computational Storage Drive Launched]]>
            </title>
            <description>
<![CDATA[
Score 115 | Comments 81 (<a href="https://news.ycombinator.com/item?id=25059946">thread link</a>) | @blopeur
<br/>
November 11, 2020 | https://www.servethehome.com/xilinx-samsung-smartssd-computational-storage-drive-launched/ | <a href="https://web.archive.org/web/*/https://www.servethehome.com/xilinx-samsung-smartssd-computational-storage-drive-launched/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <!-- image --><div><figure><a href="https://www.servethehome.com/wp-content/uploads/2020/11/smartssd-pr-1120x560-1.jpg" data-caption="Smartssd Pr 1120x560"><img width="696" height="461" src="https://www.servethehome.com/wp-content/uploads/2020/11/smartssd-pr-1120x560-1-696x461.jpg" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/smartssd-pr-1120x560-1-696x461.jpg 696w, https://www.servethehome.com/wp-content/uploads/2020/11/smartssd-pr-1120x560-1-400x265.jpg 400w, https://www.servethehome.com/wp-content/uploads/2020/11/smartssd-pr-1120x560-1-634x420.jpg 634w, https://www.servethehome.com/wp-content/uploads/2020/11/smartssd-pr-1120x560-1.jpg 800w" sizes="(max-width: 696px) 100vw, 696px" alt="Smartssd Pr 1120x560" title="Smartssd Pr 1120x560"></a><figcaption>Smartssd Pr 1120x560</figcaption></figure></div>
            <!-- content --><p>Computational storage is a small but growing segment of the market. To address this, the Samsung SmartSSD is being launched with a Xilinx Kintex FPGA inside to bring computational storage capabilities in a standard form factor. In this article, we are going to discuss how Xilinx and Samsung are delivering a computational storage platform.<span id="more-48283"></span></p>
<h2>Xilinx-Samsung SmartSSD Background</h2>
<p>First, why computational storage. One of the big drivers is that moving data, at high speeds, across systems can use a lot of power and consumes bandwidth. With computational storage, data can be processed without bringing it back to the main CPU.</p>
<figure id="attachment_48289" aria-describedby="caption-attachment-48289"><a href="https://www.servethehome.com/?attachment_id=48289" rel="attachment wp-att-48289"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Demand.png" alt="Xilinx SmartSSD Computational Storage Demand" width="1511" height="825" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Demand.png 1511w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Demand-400x218.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Demand-800x437.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Demand-696x380.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Demand-1068x583.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Demand-769x420.png 769w" sizes="(max-width: 1511px) 100vw, 1511px"></a><figcaption id="caption-attachment-48289">Xilinx SmartSSD Computational Storage Demand</figcaption></figure>
<p>Part of the other driver here is that Xilinx sees computational storage as becoming mainstream, projected to be 5% of the market in only a few years. For its part, Xilinx is covering a number of different types of accelerators aside form the Samsung SmartSSD including those from Pliops, ScaleFlux, and BittWare.</p>
<figure id="attachment_48288" aria-describedby="caption-attachment-48288"><a href="https://www.servethehome.com/?attachment_id=48288" rel="attachment wp-att-48288"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Becoming-Mainstream.png" alt="Xilinx SmartSSD Computational Storage Becoming Mainstream" width="1481" height="781" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Becoming-Mainstream.png 1481w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Becoming-Mainstream-400x211.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Becoming-Mainstream-800x422.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Becoming-Mainstream-696x367.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Becoming-Mainstream-1068x563.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Computational-Storage-Becoming-Mainstream-796x420.png 796w" sizes="(max-width: 1481px) 100vw, 1481px"></a><figcaption id="caption-attachment-48288">Xilinx SmartSSD Computational Storage Becoming Mainstream</figcaption></figure>
<p>The basic Samsung SmartSSD has two main sets of components. One is basically a 4TB Samsung V-NAND SSD. This includes a NAND controller, and we are told DRAM for the controller to use as well. The second part of the solution is a Xilinx Kintex FPGA with its own 4GB of memory.</p>
<figure id="attachment_48285" aria-describedby="caption-attachment-48285"><a href="https://www.servethehome.com/?attachment_id=48285" rel="attachment wp-att-48285"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Components.png" alt="Samsung Xilinx SmartSSD Internal Components" width="1263" height="783" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Components.png 1263w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Components-400x248.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Components-800x496.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Components-696x431.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Components-1068x662.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Components-677x420.png 677w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Components-356x220.png 356w" sizes="(max-width: 1263px) 100vw, 1263px"></a><figcaption id="caption-attachment-48285">Samsung Xilinx SmartSSD Internal Components</figcaption></figure>
<p>The basic flow is that commands can be issued to either the SSD or the FPGA portion of the drive and processing can occur at the FPGA instead of going back to the host system.</p>
<figure id="attachment_48286" aria-describedby="caption-attachment-48286"><a href="https://www.servethehome.com/?attachment_id=48286" rel="attachment wp-att-48286"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Operation.png" alt="Samsung Xilinx SmartSSD Internal Operation" width="1456" height="836" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Operation.png 1456w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Operation-400x230.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Operation-800x459.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Operation-696x400.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Operation-1068x613.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/Samsung-Xilinx-SmartSSD-Internal-Operation-731x420.png 731w" sizes="(max-width: 1456px) 100vw, 1456px"></a><figcaption id="caption-attachment-48286">Samsung Xilinx SmartSSD Internal Operation</figcaption></figure>
<p>We are going to show an example later but a common question will be how are these programmed. One can use a standard storage stack or the OpenCL stack for computational storage aspects.</p>
<figure id="attachment_48291" aria-describedby="caption-attachment-48291"><a href="https://www.servethehome.com/?attachment_id=48291" rel="attachment wp-att-48291"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Runtime-Stack.png" alt="Xilinx SmartSSD IP Runtime Stack" width="1470" height="723" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Runtime-Stack.png 1470w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Runtime-Stack-400x197.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Runtime-Stack-800x393.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Runtime-Stack-696x342.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Runtime-Stack-1068x525.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Runtime-Stack-854x420.png 854w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Runtime-Stack-324x160.png 324w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Runtime-Stack-533x261.png 533w" sizes="(max-width: 1470px) 100vw, 1470px"></a><figcaption id="caption-attachment-48291">Xilinx SmartSSD IP Runtime Stack</figcaption></figure>
<p>As one would expect with a FPGA, there is a tie in with partner IP solutions as well as those that Xilinx and Samsung will have.</p>
<figure id="attachment_48290" aria-describedby="caption-attachment-48290"><a href="https://www.servethehome.com/?attachment_id=48290" rel="attachment wp-att-48290"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Development.png" alt="Xilinx SmartSSD IP Development" width="1487" height="727" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Development.png 1487w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Development-400x196.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Development-800x391.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Development-696x340.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Development-1068x522.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Development-859x420.png 859w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Development-533x261.png 533w" sizes="(max-width: 1487px) 100vw, 1487px"></a><figcaption id="caption-attachment-48290">Xilinx SmartSSD IP Development</figcaption></figure>
<p>The Xilinx Storage Services (XSS) are offloads available for the platform. These include compression and crypto offloads.</p>
<figure id="attachment_48292" aria-describedby="caption-attachment-48292"><a href="https://www.servethehome.com/?attachment_id=48292" rel="attachment wp-att-48292"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Xilinx-Storage-Services.png" alt="Xilinx SmartSSD IP Xilinx Storage Services" width="1531" height="786" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Xilinx-Storage-Services.png 1531w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Xilinx-Storage-Services-400x205.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Xilinx-Storage-Services-800x411.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Xilinx-Storage-Services-696x357.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Xilinx-Storage-Services-1068x548.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-IP-Xilinx-Storage-Services-818x420.png 818w" sizes="(max-width: 1531px) 100vw, 1531px"></a><figcaption id="caption-attachment-48292">Xilinx SmartSSD IP Xilinx Storage Services</figcaption></figure>
<p>Taking the compression in VDO as an example, the following slides have the basic flow:</p>
<figure id="attachment_48294" aria-describedby="caption-attachment-48294"><a href="https://www.servethehome.com/?attachment_id=48294" rel="attachment wp-att-48294"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-1.png" alt="Xilinx SmartSSD VDO 1" width="1379" height="780" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-1.png 1379w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-1-400x226.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-1-800x453.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-1-696x394.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-1-1068x604.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-1-743x420.png 743w" sizes="(max-width: 1379px) 100vw, 1379px"></a><figcaption id="caption-attachment-48294">Xilinx SmartSSD VDO 1</figcaption></figure>
<p>For reads, the FPGA is used to decompress data at the SmartSSD. By putting the compression on the SSD, Xilinx says it can get better compression ratios.</p>
<figure id="attachment_48295" aria-describedby="caption-attachment-48295"><a href="https://www.servethehome.com/?attachment_id=48295" rel="attachment wp-att-48295"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-2.png" alt="Xilinx SmartSSD VDO 2" width="1447" height="784" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-2.png 1447w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-2-400x217.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-2-800x433.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-2-696x377.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-2-1068x580.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-VDO-2-775x420.png 775w" sizes="(max-width: 1447px) 100vw, 1447px"></a><figcaption id="caption-attachment-48295">Xilinx SmartSSD VDO 2</figcaption></figure>
<p>In terms of examples, we wanted to highlight one from Lewis Rhodes Labs where they are doing NPUSearch using computational storage. Effectively here the SmartSSDs are being used to scale out the number of accelerators with the number of SSDs. An application can send requests to the storage, data can be evaluated at the drives, and only results passed back to the main system.</p>
<figure id="attachment_48293" aria-describedby="caption-attachment-48293"><a href="https://www.servethehome.com/?attachment_id=48293" rel="attachment wp-att-48293"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Lewis-Rhodes-Labs-Search.png" alt="Xilinx SmartSSD Lewis Rhodes Labs Search" width="1538" height="837" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Lewis-Rhodes-Labs-Search.png 1538w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Lewis-Rhodes-Labs-Search-400x218.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Lewis-Rhodes-Labs-Search-800x435.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Lewis-Rhodes-Labs-Search-1536x836.png 1536w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Lewis-Rhodes-Labs-Search-696x379.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Lewis-Rhodes-Labs-Search-1068x580.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/Xilinx-SmartSSD-Lewis-Rhodes-Labs-Search-772x420.png 772w" sizes="(max-width: 1538px) 100vw, 1538px"></a><figcaption id="caption-attachment-48293">Xilinx SmartSSD Lewis Rhodes Labs Search</figcaption></figure>
<p>Since many of our readers will have noticed this, we asked about the PCIe Gen3 and we were told that there is a roadmap to the future.</p>
<h2>Final Words</h2>
<p>For STH readers, an immediate question is going to be why computational storage? Part of this model is that accelerators are tied to storage. For accelerator companies, this is great. Many of our readers though are going to ask about why not use DPUs instead. If you missed it&nbsp;<a href="https://www.servethehome.com/what-is-a-dpu-a-data-processing-unit-quick-primer/">What is a DPU A Data Processing Unit Quick Primer</a> is a good resource there. We asked since if the only goal is offload, and the SmartSSD is in many ways two devices that are co-packaged, then it could make sense to offload to a bigger chip. We were told that it is less expensive to use a smaller accelerator on each drive than to scale to a larger accelerator. This is one area that we know there is a lot of momentum behind each model in the data center. It will be interesting to see which ultimately wins.</p>
        </div></div>]]>
            </description>
            <link>https://www.servethehome.com/xilinx-samsung-smartssd-computational-storage-drive-launched/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25059946</guid>
            <pubDate>Wed, 11 Nov 2020 16:46:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Are Package Registries Holding Open-Source Hostage?]]>
            </title>
            <description>
<![CDATA[
Score 91 | Comments 75 (<a href="https://news.ycombinator.com/item?id=25059755">thread link</a>) | @aviaviavi
<br/>
November 11, 2020 | https://about.scarf.sh/post/package-registries-and-open-source | <a href="https://web.archive.org/web/*/https://about.scarf.sh/post/package-registries-and-open-source">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><p>A few days ago, I received an email from Docker about a change I already knew was coming:<em>‍</em></p><blockquote><em>Docker will begin enforcing rate limits on container pulls for Anonymous and Free users.</em><br></blockquote><p>To many, this came as no surprise. For years, <a href="http://hub.docker.com/">Docker Hub</a> has offered free hosting of container images, which typically range in size from a few megabytes to many gigabytes. Docker workflows as a result use a <em>lot</em> of bandwidth, and that bandwidth costs money.<br></p><p>Should we OSS (open-source software)&nbsp;developers have to think about the business and financial models of the platforms we host our software on? In a perfect world, we wouldn't have to—but in the real world we very much do. The incentives between OSS maintainers and the registries they use are often misaligned.&nbsp;<br></p><p>Docker Hub, <a href="http://npmjs.com/">npm</a>, and other comparable registries are incentivized to create lock-in, even if it makes the product experience worse for their users and customers. This is especially true of the for-profit companies behind the registries, but we see similar issues from many of the not-for-profit registries. </p><p>Maintainers, on the other hand, are incentivized to choose the best product for their needs at the lowest cost, which depends on being able to switch providers when a better service comes on the market.&nbsp;<br></p><p>This situation is fundamentally at odds with the today's package management ecosystem, where immutability is paramount in order to achieve stability. We avoid breaking things at all costs, on principle, since OSS packages are the nuts and bolts of the software ecosystem, the internet, and thus society itself.<br></p><h6><strong>Mechanics of registry lock-in</strong><br></h6><p>The registry where you host your packages and containers might be free today, but if that changes later, as is the case now with Docker, you and your users might be stuck paying whatever price the vendor chooses to set. Your users might even agree to access your package without the rate limit, but<strong> </strong><em>you</em> will not be seeing any of that revenue. Access to your open-source software was effectively just sold for a profit, and you, the author of that software, were cut out of the transaction.<br></p><p>While you could in theory just host your software somewhere else, can you really do that without breaking things for your current users? If you maintain and distribute a popular Docker image, switching the package registry is likely difficult.<br></p><p>Currently, any image on Docker Hub is installable as:<br></p><div><pre><code>$ docker run org-name/image-name</code></pre></div><p><em>‍</em>If you decide later you actually want to move your container hosting somewhere else - let's say <a href="https://cloud.google.com/container-registry" target="_blank">Google Container Registry</a> for this example - the Docker client is reasonable and lets you pull down images by their URL:<br></p><div><pre><code>$ docker run gcr.io/org-name/image-name</code></pre></div><p><em>‍</em>The problem here is that once you've changed the URL to your images, all of your existing users will stop getting updates! Even worse, this can break builds or pipelines for your users whenever they hit the new rate limits, which are not under your control. </p><p>At the point where your container has a sizable user-base all going through one of the existing container registries, your lock-in is substantial. Moving platforms will be painful. The crux of the problem here is that <em>you</em> don't own the distribution channel. The registry is the&nbsp; first place the web traffic goes, and everything that happens after that is at <em>the registry vendor’s</em><strong> </strong>discretion and to their advantage, not yours.<br></p><h6><strong>Effects of registry lock-in</strong><br></h6><p>Some might respond: <em>"This still seems like more of a theoretical problem than a practical one."</em><br></p><p>There are several practical downstream effects of the misaligned incentive structures to open-source package hosting. One major effect of registry lock-in is that maintainers cannot access their usage data. The data that registries naturally collect from package downloads can be quite useful to maintainers in a myriad of ways, yet registries typically don't share anything beyond a download count. </p><p>Registries know where the downloads are coming from, the devices, the package versions, which other packages are installed alongside, and a whole lot more. Little to none that information is shared with maintainers. Thus, maintainers are effectively locked out from observing the usage traffic.&nbsp;<br></p><p>Why is this the case? It's not because developers don't ask for it (<a href="https://github.com/npm/npm/issues/279">https://github.com/npm/npm/issues/279</a>). It's because the registries have no incentive to do so. It would cost the registries money to build and maintain the features to provide this data. Some registries even claim that exposing this data publicly would incentivize maintainers to game the system. Meanwhile, the extreme levels of inertia in software distribution keep maintainers locked in. </p><p>The registries' demonstrated distrust of maintainers seems counterproductive in a space where there's opportunity to work together cooperatively. If registry incentives were aligned accordingly, a registry like npm, for instance, would be in a great position to empower maintainers to leverage their own distribution data to deliver the best software possible.<br></p><p>What makes npm’s particular scenario even worse is that they've made it so difficult to use a registry that is<em> not</em> npm. There's no way to pull a single package from an alternate registry without switching to that registry. Which makes it quite impossible to actually publish a widely used JavaScript package without putting it on npm.&nbsp;<br></p><p>Contrast this scenario to Docker: Docker Hub creates different tradeoffs that both help and harm OSS maintainers. They've loosened their grip on OSS maintainers by making it user-friendly to pull containers down from alternative registries besides Docker Hub. However, even if you switch away from Docker Hub, you're still jumping from one company to another. This is because, at the end of the day, the registries—not the maintainers—own the distribution URL. The power imbalance continues.<br></p><p>My argument is not intended to dismiss the efforts of the registries as a whole. Package registries serve an essential role in software distribution, and have collectively serviced billions upon billions of package downloads. They’ve made it easy for anyone in the world to interact with open-source, and as a result have helped push open-source forward. Astonishingly, they have, for most part, remained free to use! But as software continues to eat the world and the distribution of that software becomes more important, conflicting interests in this space become increasingly problematic.<br></p><h6><strong>Looking forward</strong><br></h6><p>How do we solve this? Ultimately, package registries need to align their incentives with those of maintainers. Registries should build products maintainers <em>want</em> to use rather than products they <em>have</em> to use.&nbsp;The entire OSS&nbsp;community can benefit. <br></p><p>Part of this means registries must be more intentional about giving maintainers back control over the distribution of their own software, even when it means the maintainers could take their packages elsewhere. As a community, we should be empowering maintainers to do their best work rather than constraining them to work within a specific&nbsp; platform or framework.&nbsp;<br></p><p>For the health of the open source ecosystem, it's critical to ensure that maintainers are not locked out from accessing the data about their own software distribution. Maintainers must be able to make data-informed decisions and treat distribution data as something that rightfully belongs to them, instead of just the registry providers.<br></p><p>Unfortunately, the current software distribution model works to cut maintainers out, making all downstream actions more difficult and strictly less informed. When we as a community decide to better align ourselves with open-source maintainers and build platforms to empower them, the result will be better software for the entire ecosystem.</p><p>Scarf is working on new tooling to address these problems, so stay tuned! Follow <a href="https://twitter.com/scarf_oss" target="_blank">@scarf_oss</a> on Twitter or subscribe below for periodic updates.<br></p></div></div></div></div>]]>
            </description>
            <link>https://about.scarf.sh/post/package-registries-and-open-source</link>
            <guid isPermaLink="false">hacker-news-small-sites-25059755</guid>
            <pubDate>Wed, 11 Nov 2020 16:32:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Productivity vs. Privacy]]>
            </title>
            <description>
<![CDATA[
Score 49 | Comments 14 (<a href="https://news.ycombinator.com/item?id=25059205">thread link</a>) | @jessems
<br/>
November 11, 2020 | https://jessems.com/productivity-vs-privacy | <a href="https://web.archive.org/web/*/https://jessems.com/productivity-vs-privacy">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><div><p>In recent years there's been a steady growth in privacy focused companies. Some examples that have reached large-scale adoption are <a href="https://protonmail.com/">ProtonMail</a>, <a href="https://signal.com/">Signal</a>, and <a href="https://duckduckgo.com/">DuckDuckGo</a>. These are companies that have put privacy front and center to their value proposition and can be considered <em>privacy-preserving products</em>. I've come to beleive a goal of preserving user privacy is often inherently in tension with the goal of advancing user productivity.</p><p>What these services have in common is that they promise their users a higher degree of privacy relative to their competitors. Instead of the usual encryption in transit (protection from eavesdroppers) and encryption at rest (protection against unauthorized users), services like Signal and ProtonMail enable their users to hide data from anyone except the intended recipient, which — crucially — includes the service providers themselves.</p><p>This category of encryption is known as end-to-end encryption (e2e) and has found adopters in anyone from principled libertarians to journalists and human rights activitists whose lives may depend on their conversations remaining unwiretapped.</p><h2>Early privacy-preserving software was too difficult to use</h2><p>The canonical implementation of e2e for email is known as Pretty Good Privacy (PGP) and its reference implementation is GPG. GPG never reached mass adoption and there seems to be a myriad of reasons for that. The most salient reason, however, seems to be that to this day, it continues to be difficult to use. As the founder of Signal, Moxie Marlinspike <a href="https://moxie.org/2015/02/24/gpg-and-me.html">explains</a>, the spirit behind GPG was the following:</p><blockquote><p>Instead of developing opinionated software with a simple interface, GPG was written to be as powerful and flexible as possible.</p></blockquote><p>Powerful, flexible software written by nerds, unfortunately also tends to be prohibitively complex for normal users. Combined with the fact that <a href="https://signal.org/blog/the-ecosystem-is-moving/">decentralized technology seems unable to quickly adapt to change</a>, the result has been a clunky solution that has, quite frankly, stayed clunky. With no feasible privacy-preserving alternative <!-- -->[1]<!-- -->, non-privacy preserving email providers became the norm.</p><h2>Surveillance capitalist companies will not encrypt your data, because they rely on being able to read it</h2><p>One such email provider, Gmail by Google, gained millions of users by offering a free plan. Their initial monetization strategy was scanning your emails and serving you personalized ads. Although they've stopped personalizing the ads, they're still scanning your email's contents to serve you a better experience across their services. Similarly, Facebook tracks what you do to shape your experience and keep you glued (they would say 'engaged') to their platform.</p><p>What unites platforms like Google and Facebook, is described by Professor Shoshana Zuboff as “<a href="https://en.wikipedia.org/wiki/Surveillance_capitalism">surveillance capitalism</a>”. The business model of surveillance capitalist companies is to harvest personal data about you to build a model that predicts your behavior. These prediction models are packaged and sold as advertisement opportunities to companies eager to buy your attention. You might be the user, but you're not the customer — the advertisers are.</p><p>It should come as no surprise then, that none of these platforms has shipped with end-to-end encryption by default. Doing so would go against the incentives that undergird their very business model. Their ability to predict your behavior, and sell ads based on those predictions, hinges on their ability to harvest your data.</p><h2>Data is also collected to improve the service</h2><p>A company like Google has other business models of course. Google Workspace, aimed at businesses, is a collection of collaboration and productivity tools. This ranges from Google Docs, to chat, to video conferencing, and more. By offering this as a paid service, Google exposes itself to a different incentive, one where the customer and the user are now one and the same.</p><p>Even if you're both the user and the customer, your data is still being harvested. This data might not feed into personalized ads (because that’s no longer the primary business model) but rather into improving your experience. But as a business user, when does your experience improve? And as a service provider, how do you know what improves the experience?</p><h2>Improvements are productivity gains</h2><p>There's an inclination to think of improvements as things that help you do the thing you want to do quicker, better and/or with less frustration. We can go one step further and borrow some of the thinking used in economics and treat productivity simply as the ratio between outputs (salaries and corporate profits) and inputs (hours worked). Productivity increases if inputs can be decreased (for equal outputs) or outputs can be increased (for equal inputs). What's more, we would expect this quantity to improve along with advances in technology.</p><p>How does technology lead to increases in productivity? One obvious way is by making us more efficient. If some new technology saves us time doing a certain task (decreased input), all other things being equal, we’ll end up seeing those gains reflected in our outputs.</p><h2>Productivity gains are discovered, not planned</h2><p>What exactly are the things that increase efficiency? Here's where it gets tricky. In the realm of knowledge work, we don't always know where the gains will come from — that is, before they are made. We are still discovering new ways in which we can be more productive and especially so in the domain of collaborative productivity. An illustrative example of how productivity gains are discovered comes from Kevin A. Kwok's description of Figma's road to success.</p><p>In "Why Figma Wins", <a href="https://kwokchain.com/2020/06/19/why-figma-wins/">Kwok details</a> how the product team discovered a way to enable more efficient collaboration in the design process. That this potential existed wasn't at all  obvious to even those within the scene. While Sketch had broken new ground with their vector based design tool geared towards product designers, Figma took it to another level by taking many of the same (dare I say revolutionary) UX patterns and offering them in a web-native, multiplayer web application.</p><blockquote><p>The core insight of Figma is that design is larger than just designers. Design is all of the conversations between designers and PMs about what to build. It is the mocks and prototypes and the feedback on them. It is the handoff of specs and assets to engineers and how easy it is for them to implement them.</p></blockquote><p>As Kevin explains, Figma brought together the disparate disciplines that are involved in a design process into a synced browser window for everybody. This helped democratize design and remove a lot of friction that had existed before.</p><p>Not only did Figma push the frontier of productivity into new territory, it wasn’t obvious beforehand what that territory would look like. The lesson is that productivity improvements are won through a process of <em>discovery</em>. Kevin explains:</p><blockquote><p>As disciplines evolve, they figure out the social norms needed to operate better, build tools that can be shared across the industry, and invent abstractions that allow offloading more and more of the workload. They learn how to collaborate better, not just with each other but with all the other functions as well.</p></blockquote><p>Although there's some inherent uncertainty about what the productivity gains will look like (and where to look for them), there's no uncertainty about whether they will be made at all. If one thing can be counted on, it's the tech industry's relentless march towards higher productivity. The big tech platforms know this and don't shy away from investing heavily in innovation (discovery) in that direction.</p><h2>Productivity gains are unlocked by harvesting data</h2><p>Although there is some inherent tension between preserving privacy vs. allowing for a multiplayer mode like Figma, we can find even stronger tensions when it comes to harvesting data in favor of productivity gains.</p><p>A search feature relies on indexing your data. A recommendation feature relies on mining your browsing history. An autocomplete feature relies on what you (or other users) typed before.</p><p>All these potential features which are made possible through harvesting your user data are not available to privacy-preserving products. The user data isn't readable to them — and that's the whole point.</p><p>This creates a trade-off from the user's perspective. Whatever your particular motivation might be, as soon as you opt for a privacy-preserving service you're opting for a service that is not able to read your data, and by extension, not able to harvest it. Because the harvesting of data is what is driving many of the improvements in productivity, in choosing to preserve user privacy, these services are forgoing their ability to provide additional gains in productivity.</p><p>Historically, as we saw with the origins of GPG, there has always been additional friction involved in replicating a workflow in a privacy-preserving manner. Although using e2e services such as Signal and ProtonMail has become nearly frictionless, they lack many features their non-privacy preserving counterparts offer.</p><h2>The productivity gap between privacy-preserving and non-preserving services</h2><p>If you compare the productivity gains between privacy-preserving and non-preserving products from the perspective of the user, it's hard not to arrive at the conclusion that there’s a gap between the two — and it appears to be growing.</p><p>There is perhaps no better example of a feature which hinges on the ability to read user data than search. Although ProtonMail is reminiscent of Gmail in many ways, one area where it falls short is the absence of any ability to  search the contents of your emails. Search only works if the provider of such functionality can scan and index your content. It works even better if the provider is able to harvest search queries and use those to build predictive models (e.g. autocomplete and smart suggestions). These are features which make Gmail users more productive but aren't available to ProtonMail users <!-- -->[3]<!-- -->.</p><p>The absence of search might not be a dealbreaker for a …</p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://jessems.com/productivity-vs-privacy">https://jessems.com/productivity-vs-privacy</a></em></p>]]>
            </description>
            <link>https://jessems.com/productivity-vs-privacy</link>
            <guid isPermaLink="false">hacker-news-small-sites-25059205</guid>
            <pubDate>Wed, 11 Nov 2020 15:37:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Systematically removing code]]>
            </title>
            <description>
<![CDATA[
Score 32 | Comments 45 (<a href="https://news.ycombinator.com/item?id=25058632">thread link</a>) | @jerodsanto
<br/>
November 11, 2020 | https://thepugautomatic.com/2020/11/systematically-removing-code/ | <a href="https://web.archive.org/web/*/https://thepugautomatic.com/2020/11/systematically-removing-code/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section><hgroup><p><span>Written November 8, 2020. <span>Tagged <a href="https://thepugautomatic.com/tag/methodology">Methodology</a>.</span></span></p></hgroup><div><p>It's easy to miss things when removing code, leaving behind unused methods, templates, CSS classes or translation keys. (Especially in a dynamic language like Ruby, without a compiler to help you spot dead code.)</p><p>I avoid this by removing code systematically, line by line, depth-first.</p><p>This is one of those things that seems obvious when you do it, but in my experience, many people do it haphazardly.</p><p>Say we wanted to remove the "item box" from this page:</p><p>page.html.erb</p><pre><code><span><span><span>&lt;</span>p</span><span>&gt;</span></span>Welcome to my page!<span><span><span>&lt;/</span>p</span><span>&gt;</span></span><p><span><span>&lt;%=</span> render<span>(</span><span>"item_box"</span><span>,</span> item<span>:</span> item<span>)</span> <span>%&gt;</span></span></p></code></pre><p>_item_box.html.erb</p><pre><code><span><span><span>&lt;</span>div</span> <span>class</span><span><span>=</span><span>"</span>box box--fancy<span>"</span></span><span>&gt;</span></span><br>  <span><span><span>&lt;</span>h2</span><span>&gt;</span></span><span><span>&lt;%=</span> item<span>.</span>title <span>%&gt;</span></span><span><span><span>&lt;/</span>h2</span><span>&gt;</span></span><br>  <span><span>&lt;%=</span> format_description<span>(</span>item<span>.</span>description<span>)</span> <span>%&gt;</span></span><br>  <span><span><span>&lt;</span>p</span><span>&gt;</span></span><span><span>&lt;%=</span> <span>I18n</span><span>.</span>translate<span>(</span><span>"my.translation.key"</span><span>)</span> <span>%&gt;</span></span><br><span><span><span>&lt;/</span>div</span><span>&gt;</span></span></code></pre><p>So our end goal is to remove the <code>&lt;%= render("item_box", item: item) %&gt;</code> line.</p><p>First, we search the project to check that <code>_item_box.html.erb</code> isn't used somewhere else, or referenced in docs that we'll need to update. It isn't, so we're OK to remove it – but before we do that, we must go through it line by line.</p><p>The first line is <code>&lt;div class="box box--fancy"&gt;</code>. So we search the project for these two CSS classes, checking if they're in use somewhere else. If not, we remove them from the CSS files.</p><p>We go deeper if required – perhaps the CSS for <code>.box--fancy</code> uses a CSS variable. Then we check if that variable is in use elsewhere. <a href="https://thepugautomatic.com/2014/03/stacked-vim-searches-down-cold/">Stacked searches in Vim</a> are helpful here.</p><p>Once we've checked a line in the file, we delete that line. This helps us keep track of what we've already checked.</p><p>So after we've checked and removed that line, we're left with</p><p>_item_box.html.erb</p><pre><code>  <span><span><span>&lt;</span>h2</span><span>&gt;</span></span><span><span>&lt;%=</span> item<span>.</span>title <span>%&gt;</span></span><span><span><span>&lt;/</span>h2</span><span>&gt;</span></span><br>  <span><span>&lt;%=</span> format_description<span>(</span>item<span>.</span>description<span>)</span> <span>%&gt;</span></span><br>  <span><span><span>&lt;</span>p</span><span>&gt;</span></span><span><span>&lt;%=</span> <span>I18n</span><span>.</span>translate<span>(</span><span>"my.translation.key"</span><span>)</span> <span>%&gt;</span></span><br><span><span><span>&lt;/</span>div</span><span>&gt;</span></span></code></pre><p>And we continue this way, line by line. Is the <code>item.title</code> used elsewhere? If not, we should probably remove it, too. What about <code>format_description</code>, <code>item.description</code>, the <code>my.translation.key</code> translation key?</p><p>Again, we go deeper if required, not removing the <code>format_description</code> method until we've gone through <em>it</em> line by line.</p><p>When we've looked at every line in <code>_item_box.html.erb</code> and deleted them as we went, the file will be empty, and we can start popping the stack.</p><p>We remove the empty <code>_item_box.html.erb</code> file.</p><p>And we can finally remove the <code>&lt;%= render("item_box", item: item) %&gt;</code> line, fairly confident that we didn't leave dead code behind.</p><p>This probably sounds more tedious than it is. It tends to be quick work, and you can take shortcuts – removing a swathe of lines that don't reference anything else, or that only call methods that you know are used elsewhere.</p></div></section></div><div><p>Content and design © <a href="https://henrik.nyh.se/">Henrik Nyh</a> (<a href="https://twitter.com/henrik">@henrik</a>). Code is under a <a href="http://en.wikipedia.org/wiki/MIT_License">MIT License</a> unless otherwise stated.</p><p>Pug art by <a href="https://johannaost.com/">Johanna Öst</a>; other graphics are under a <a href="http://creativecommons.org/licenses/by/3.0/">CC BY License</a>.</p><p>Powered by <a href="https://11ty.dev/">Eleventy</a>.</p></div></div>]]>
            </description>
            <link>https://thepugautomatic.com/2020/11/systematically-removing-code/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25058632</guid>
            <pubDate>Wed, 11 Nov 2020 14:26:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Introduction to Linear Algebra for Applied Machine Learning with Python]]>
            </title>
            <description>
<![CDATA[
Score 412 | Comments 52 (<a href="https://news.ycombinator.com/item?id=25058619">thread link</a>) | @Anon84
<br/>
November 11, 2020 | https://pabloinsente.github.io/intro-linear-algebra | <a href="https://web.archive.org/web/*/https://pabloinsente.github.io/intro-linear-algebra">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <section id="main_content">
        
<!-- https://docs.mathjax.org/en/latest/configuration.html#local-config-files -->




<article>
  <h2>Introduction to Linear Algebra for Applied Machine Learning with Python</h2>
  <time datetime="2020-05-26T00:00:00+00:00">26 May 2020</time>
  

  

<p>Linear algebra is to machine learning as flour to bakery: <strong>every machine learning model is based in linear algebra, as every cake is based in flour</strong>. It is not the only ingredient, of course. Machine learning models need vector calculus, probability, and optimization, as cakes need sugar, eggs, and butter. Applied machine learning, like bakery, is essentially about combining these mathematical ingredients in clever ways to create useful (tasty?) models.</p>

<p>This document contains <strong>introductory level linear algebra notes for applied machine learning</strong>. It is meant as a reference rather than a comprehensive review. If you ever get confused by matrix multiplication, don’t remember what was the $L_2$ norm, or the conditions for linear independence, this can serve as a quick reference. It also a good introduction for people that don’t need a deep understanding of linear algebra, but still want to learn about the fundamentals to read about machine learning or to use pre-packaged machine learning solutions. Further, it is a good source for people that learned linear algebra a while ago and need a refresher.</p>

<p>These notes are based in a series of (mostly) freely available textbooks, video lectures, and classes I’ve read, watched and taken in the past. If you want to obtain a deeper understanding or to find exercises for each topic, you may want to consult those sources directly.</p>

<p><strong>Free resources</strong>:</p>

<ul>
  <li><strong>Mathematics for Machine Learning</strong> by Deisenroth, Faisal, and Ong. 1st Ed. <a href="https://mml-book.github.io/">Book link</a>.</li>
  <li><strong>Introduction to Applied Linear Algebra</strong> by Boyd and Vandenberghe. 1sr Ed. <a href="http://vmls-book.stanford.edu/">Book link</a></li>
  <li><strong>Linear Algebra Ch. in Deep Learning</strong> by Goodfellow, Bengio, and Courville. 1st Ed. <a href="https://www.deeplearningbook.org/contents/linear_algebra.html">Chapter link</a>.</li>
  <li><strong>Linear Algebra Ch. in Dive into Deep Learning</strong> by Zhang, Lipton, Li, And Smola. <a href="https://d2l.ai/chapter_preliminaries/linear-algebra.html">Chapter link</a>.</li>
  <li><strong>Prof. Pavel Grinfeld’s Linear Algebra Lectures</strong> at Lemma. <a href="https://www.lem.ma/books/AIApowDnjlDDQrp-uOZVow/landing">Videos link</a>.</li>
  <li><strong>Prof. Gilbert Strang’s Linear Algebra Lectures</strong> at MIT. <a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/">Videos link</a>.</li>
  <li><strong>Salman Khan’s Linear Algebra Lectures</strong> at Khan Academy. <a href="https://www.khanacademy.org/math/linear-algebra">Videos link</a>.</li>
  <li><strong>3blue1brown’s Linear Algebra Series</strong> at YouTube. <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Videos link</a>.</li>
</ul>

<p><strong>Not-free resources</strong>:</p>

<ul>
  <li><strong>Introduction to Linear Algebra</strong> by Gilbert Strang. 5th Ed. <a href="https://www.amazon.com/Introduction-Linear-Algebra-Gilbert-Strang/dp/0980232775">Book link</a>.</li>
  <li><strong>No Bullshit Guide to Linear Algebra</strong> by Ivan Savov. 2nd Ed. <a href="https://www.amazon.com/No-bullshit-guide-linear-algebra/dp/0992001021">Book Link</a>.</li>
</ul>

<p>I’ve consulted all these resources at one point or another. Pavel Grinfeld’s lectures are my absolute favorites. Salman Khan’s lectures are really good for absolute beginners (they are long though). The famous 3blue1brown series in linear algebra is delightful to watch and to get a solid high-level view of linear algebra.</p>

<p>If you have to pic one book, I’d pic <strong>Boyd’s and Vandenberghe’s Intro to applied linear algebra</strong>, as it is the most beginner friendly book on linear algebra I’ve encounter. Every aspect of the notation is clearly explained and pretty much all the key content for applied machine learning is covered. The Linear Algebra Chapter in Goodfellow et al is a nice and concise introduction, but it may require some previous exposure to linear algebra concepts. Deisenroth et all book is probably the best and most comprehensive source for linear algebra for machine learning I’ve found, although it assumes that you are good at reading math (and at math more generally). Savov’s book it’s also great for beginners but requires time to digest. Professor Strang lectures are great too but I won’t recommend it for absolute beginners.</p>

<p>I’ll do my best to keep notation consistent. Nevertheless, learning to adjust to changing or inconsistent notation is a useful skill, since most authors will use their own preferred notation, and everyone seems to think that its/his/her own notation is better.</p>

<p>To make everything more dynamic and practical, I’ll introduce bits of Python code to exemplify each mathematical operation (when possible) with <code>NumPy</code>, which is the facto standard package for scientific computing in Python.</p>

<p>Finally, keep in mind this is created by a non-mathematician for (mostly) non-mathematicians. I wrote this as if I were talking to myself or a dear friend, which explains why my writing is sometimes conversational and informal.</p>

<p>If you find any mistake in notes feel free to reach me out at pcaceres@wisc.edu and to https://pablocaceres.org/ so I can correct the issue.</p>



<p><strong>Note:</strong> <em>underlined sections</em> are the newest sections and/or corrected ones.</p>

<p><strong><a href="#preliminary-concepts">Preliminary concepts</a></strong>:</p>
<ul>
  <li><a href="#sets">Sets</a></li>
  <li><a href="#belonging-and-inclusion">Belonging and inclusion</a></li>
  <li><a href="#set-specification">Set specification</a></li>
  <li><a href="#ordered-pairs">Ordered pairs</a></li>
  <li><a href="#relations">Relations</a></li>
  <li><a href="#functions">Functions</a></li>
</ul>

<p><strong><a href="#vectors">Vectors</a></strong>:</p>
<ul>
  <li><a href="#types-of-vectors">Types of vectors</a>
    <ul>
      <li><a href="#geometric-vectors">Geometric vectors</a></li>
      <li><a href="#polynomials">Polynomials</a></li>
      <li><a href="#elements-of-r">Elements of R</a></li>
    </ul>
  </li>
  <li><a href="#zero-vector-unit-vector-and-sparse-vector">Zero vector, unit vector, and sparse vector</a></li>
  <li><a href="#vector-dimensions-and-coordinate-system">Vector dimensions and coordinate system</a></li>
  <li><a href="#basic-vector-operations">Basic vector operations</a>
    <ul>
      <li><a href="#vector-vector-addition">Vector-vector addition</a></li>
      <li><a href="#vector-scalar-multiplication">Vector-scalar multiplication</a></li>
      <li><a href="#linear-combinations-of-vectors">Linear combinations of vectors</a></li>
      <li><a href="#vector-vector-multiplication-dot-product">Vector-vector multiplication: dot product</a></li>
    </ul>
  </li>
  <li><a href="#vector-space-span-and-subspace">Vector space, span, and subspace</a>
    <ul>
      <li><a href="#vector-space">Vector space</a></li>
      <li><a href="#vector-span">Vector span</a></li>
      <li><a href="#vector-subspaces">Vector subspaces</a></li>
    </ul>
  </li>
  <li><a href="#linear-dependence-and-independence">Linear dependence and independence</a></li>
  <li><a href="#vector-null-space">Vector null space</a></li>
  <li><a href="#vector-norms">Vector norms</a>
    <ul>
      <li><a href="#euclidean-norm">Euclidean norm: $L_2$</a></li>
      <li><a href="#manhattan-norm">Manhattan norm: $L_1$</a></li>
      <li><a href="#max-norm">Max norm: $L_\infty$</a></li>
    </ul>
  </li>
  <li><a href="#vector-inner-product-length-and-distance">Vector inner product, length, and distance</a></li>
  <li><a href="#vector-angles-and-orthogonality">Vector angles and orthogonality</a></li>
  <li><a href="#systems-of-linear-equations">Systems of linear equations</a></li>
</ul>

<p><strong><a href="#matrices">Matrices</a></strong>:</p>

<ul>
  <li><a href="#basic-matrix-operations">Basic matrix operations</a>
    <ul>
      <li><a href="#matrix-matrix-addition">Matrix-matrix addition</a></li>
      <li><a href="#matrix-scalar-multiplication">Matrix-scalar multiplication</a></li>
      <li><a href="#matrix-vector-multiplication-dot-product">Matrix-vector multiplication: dot product</a></li>
      <li><a href="#matrix-matrix-multiplication">Matrix-matrix multiplication</a></li>
      <li><a href="#matrix-identity">Matrix identity</a></li>
      <li><a href="#matrix-inverse">Matrix inverse</a></li>
      <li><a href="#matrix-transpose">Matrix transpose</a></li>
      <li><a href="#hadamard-product">Hadamard product</a></li>
    </ul>
  </li>
  <li><a href="#special-matrices">Special matrices</a>
    <ul>
      <li><a href="#rectangular-matrix">Rectangular matrix</a></li>
      <li><a href="#square-matrix">Square matrix</a></li>
      <li><a href="#diagonal-matrix">Diagonal matrix</a></li>
      <li><a href="#upper-triangular-matrix">Upper triangular matrix</a></li>
      <li><a href="#lower-triangular-matrix">Lower triangular matrix</a></li>
      <li><a href="#symmetric-matrix">Symmetric matrix</a></li>
      <li><a href="#identity-matrix">Identity matrix</a></li>
      <li><a href="#scalar-matrix">Scalar matrix</a></li>
      <li><a href="#null-or-zero-matrix">Null or zero matrix</a></li>
      <li><a href="#echelon-matrix">Echelon matrix</a></li>
      <li><a href="#antidiagonal-matrix">Antidiagonal matrix</a></li>
      <li><a href="#design-matrix">Design matrix</a></li>
    </ul>
  </li>
  <li><a href="#matrices-as-systems-of-linear-equations">Matrices as systems of linear equations</a></li>
  <li><a href="#the-four-fundamental-matrix-subsapces">The four fundamental matrix subsapces</a>
    <ul>
      <li><a href="#the-column-space">The column space</a></li>
      <li><a href="#the-row-space">The row space</a></li>
      <li><a href="#the-null-space">The null space</a></li>
      <li><a href="#the-null-space-of-the-transpose">The null space of the transpose</a></li>
    </ul>
  </li>
  <li><a href="#solving-systems-of-linear-equations-with-matrices">Solving systems of linear equations with matrices</a>
    <ul>
      <li><a href="#gaussian-elimination">Gaussian Elimination</a></li>
      <li><a href="#gauss-jordan-elimination">Gauss-Jordan Elimination</a></li>
    </ul>
  </li>
  <li><a href="#matrix-basis-and-rank">Matrix basis and rank</a></li>
  <li><a href="#matrix-norm">Matrix norm</a></li>
</ul>

<p><strong><a href="#linear-and-affine-mappings">Linear and affine mappings</a></strong>:</p>

<ul>
  <li><a href="#linear-mappings">Linear mappings</a></li>
  <li><a href="#examples-of-linear-mappings">Examples of linear mappings</a>
    <ul>
      <li><a href="#negation-matrix">Negation matrix</a></li>
      <li><a href="#reversal-matrix">Reversal matrix</a></li>
    </ul>
  </li>
  <li><a href="#examples-of-nonlinear-mappings">Examples of nonlinear mappings</a>
    <ul>
      <li><a href="#norms">Norms</a></li>
      <li><a href="#translation">Translation</a></li>
    </ul>
  </li>
  <li><a href="#affine-mappings">Affine mappings</a>
    <ul>
      <li><a href="#affine-combination-of-vectors">Affine combination of vectors</a></li>
      <li><a href="#affine-span">Affine span</a></li>
      <li><a href="#affine-space-and-subspace">Affine space and subspace</a></li>
      <li><a href="#affine-mappings-using-the-augmented-matrix">Affine mappings using the augmented matrix</a></li>
    </ul>
  </li>
  <li><a href="#special-linear-mappings">Special linear mappings</a>
    <ul>
      <li><a href="#scaling">Scaling</a></li>
      <li><a href="#reflection">Reflection</a></li>
      <li><a href="#shear">Shear</a></li>
      <li><a href="#rotation">Rotation</a></li>
    </ul>
  </li>
  <li><a href="#projections">Projections</a>
    <ul>
      <li><a href="#projections-onto-lines">Projections onto lines</a></li>
      <li><a href="#projections-onto-general-subspaces">Projections onto general subspaces</a></li>
      <li><a href="#projections-as-approximate-solutions-to-systems-of-linear-equations">Projections as approximate solutions to systems of linear equations</a></li>
    </ul>
  </li>
</ul>

<p><strong><a href="#matrix-decompositions">Matrix decompositions</a></strong>:</p>
<ul>
  <li><a href="#lu-decomposition">LU decomposition</a>
    <ul>
      <li><a href="#elementary-matrices">Elementary matrices</a></li>
      <li><a href="#the-inverse-of-elementary-matrices">The inverse of elementary matrices</a></li>
      <li><a href="#lu-decomposition-as-gaussian-elimination">LU decomposition as Gaussian Elimination</a></li>
      <li><a href="#lu-decomposition-with-pivoting">LU decomposition with pivoting</a></li>
    </ul>
  </li>
  <li><a href="#qr-decomposition">QR decomposition</a>
    <ul>
      <li><a href="#orthonormal-basis">Orthonormal basis</a></li>
      <li><a href="#orthonormal-basis-transpose">Orthonormal basis transpose</a></li>
      <li><a href="#gram-schmidt-orthogonalization">Gram-Schmidt Orthogonalization </a></li>
      <li><a href="#qr-decomposition-as-gram-schmidt-orthogonalization">QR decomposition as Gram-Schmidt Orthogonalization</a></li>
    </ul>
  </li>
  <li><a href="#determinant">Determinant</a>
    <ul>
      <li><a href="#determinant-as-measures-of-volume">Determinant as measures of volume</a></li>
      <li><a href="#the-2-x-2-determinant">The 2X2 determinant</a></li>
      <li><a href="#the-n-x-n-determinant">The NXN determinant</a></li>
      <li><a href="#determinants-as-scaling-factors">Determinants as scaling factors</a></li>
      <li><a href="#the-importance-of-determinants">The importance of determinants</a></li>
    </ul>
  </li>
  <li><a href="#eigenthings">Eigenthings</a>
    <ul>
      <li><a href="#change-of-basis">Change of basis</a></li>
      <li><a href="#eigenvectors-eigenvalues-and-eigenspaces">Eigenvectors, Eigenvalues, and Eigenspaces</a></li>
      <li><a href="#trace-and-determinant-with-eigenvalues">Trace and determinant with eigenvalues</a></li>
      <li><a href="#eigendecomposition">Eigendecomposition</a></li>
      <li><a href="#eigenbasis-are-a-good-basis">Eigenbasis are a good basis</a></li>
      <li><a href="#geometric-interpretation-of-eigendecomposition">Geometric interpretation of Eigendecomposition</a></li>
      <li><a href="#the-problem-with-eigendecomposition">The problem with Eigendecomposition</a></li>
    </ul>
  </li>
  <li><a href="#singular-value-decomposition">Singular Value Decomposition</a>:
    <ul>
      <li><a href="#singular-value-decomposition-theorem">Singular Value Decomposition Theorem</a></li>
      <li><a href="#singular-value-decomposition-computation">Singular Value Decomposition computation</a></li>
      <li><a href="#geometric-interpretation-of-the-singular-value-decomposition">Geometric interpretation of the Singular Value Decomposition</a></li>
      <li><a href="#singular-value-decomposition-vs-eigendecomposition">Singular Value Decomposition vs Eigendecomposition</a></li>
    </ul>
  </li>
  <li><a href="#matrix-approximation">Matrix Approximation</a>:
    <ul>
      <li><a href="#best-rank-k-approximation-with-svd">Best rank-k approximation with SVD</a></li>
      <li><a href="#best-low-rank-approximation-as-a-minimization-problem">Best low-rank approximation as a minimization problem</a></li>
    </ul>
  </li>
</ul>

<p><strong><a href="#epilogue">Epilogue</a></strong></p>



<p>While writing about linear mappings, I realized the importance of having a basic understanding of a few concepts before approaching the study of linear algebra. If you are like me, you may not have formal mathematical training beyond high school. If so, I encourage you to read this section and spent some time wrapping your head around these concepts before going over the linear algebra content (otherwise, you might prefer to skip this part). I believe that reviewing these concepts is of great help to understand the <em>notation</em>, which in my experience is one of the main barriers to understand mathematics for nonmathematicians: we are <em>non</em>native speakers, so we are continuously building up our vocabulary. I’ll keep this section very short, as is not the focus of this mini-course.</p>

<p>For this section, my notes are based on readings of:</p>

<ul>
  <li><strong>Geometric transformations (Vol. 1)</strong> (1966) by Modenov &amp; Parkhomenko</li>
  <li><strong>Naive Set Theory</strong> (1960) by P.R. Halmos</li>
  <li><strong>Abstract Algebra: Theory and Applications</strong> (2016) by Judson &amp; Beeer. <a href="http://abstract.pugetsound.edu/download/aata-20160809.pdf">Book link</a></li>
</ul>

<h2 id="sets">Sets</h2>

<p>Sets are one of the most fundamental concepts in mathematics. They are so fundamental that they are not defined in terms of anything else. On the contrary, other branches of mathematics are defined in terms of sets, including linear algebra. Put simply, <strong>sets are well-defined collections of objects</strong>. Such objects are called <strong>elements or members</strong> of the set. The crew of a ship, a caravan of camels, and the LA Lakers roster, are all examples of sets. The captain of the ship, the first camel in the caravan, and LeBron James are all examples of “members” or “elements” of their corresponding sets. We denote a set with an upper case italic letter as $\textit{A}$. In the context of linear algebra, we say that a line is a set of points, and the set of all lines in the plane is a set of sets. Similarly, we can say that <em>vectors</em> are sets of points, and <em>matrices</em> sets of vectors.</p>

<h2 id="belonging-and-inclusion">Belonging and inclusion</h2>

<p>We build sets using the notion of <strong>belonging</strong>. We denote that $a$ <em>belongs</em> (or is an <em>element</em> or <em>member</em> of) to $\textit{A}$ with the Greek letter epsilon as:</p>



<p>Another important idea is <strong>inclusion</strong>, which allow us to build <em>subsets</em>. Consider sets $\textit{A}$ and $\textit{B}$. When every element of $\textit{A}$ is an element of $\textit{B}$, we say that $\textit{A}$ is a <em>subset</em> of $\textit{B}$, or that $\textit{B}$ <em>includes</em> $\textit{A}$. The notation is:</p>



<p>or</p>



<p>Belonging and inclusion are derived from <strong>axion of extension</strong>: <em>two sets are equal if and only if they have the same elements</em>. This axiom may sound trivially obvious but is necessary to make belonging and …</p></article></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://pabloinsente.github.io/intro-linear-algebra">https://pabloinsente.github.io/intro-linear-algebra</a></em></p>]]>
            </description>
            <link>https://pabloinsente.github.io/intro-linear-algebra</link>
            <guid isPermaLink="false">hacker-news-small-sites-25058619</guid>
            <pubDate>Wed, 11 Nov 2020 14:24:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What mother never told you about VM service (1983) [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 39 | Comments 10 (<a href="https://news.ycombinator.com/item?id=25058543">thread link</a>) | @fanf2
<br/>
November 11, 2020 | http://www.leeandmelindavarian.com/Melinda/tutorial.pdf | <a href="https://web.archive.org/web/*/http://www.leeandmelindavarian.com/Melinda/tutorial.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://www.leeandmelindavarian.com/Melinda/tutorial.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-25058543</guid>
            <pubDate>Wed, 11 Nov 2020 14:15:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[$200k in sales from a $6k advertisement]]>
            </title>
            <description>
<![CDATA[
Score 383 | Comments 104 (<a href="https://news.ycombinator.com/item?id=25058363">thread link</a>) | @mildlyclassic
<br/>
November 11, 2020 | https://www.wifidabba.com/blog/200000-dollars-in-sales-from-one-daringfireball-ad | <a href="https://web.archive.org/web/*/https://www.wifidabba.com/blog/200000-dollars-in-sales-from-one-daringfireball-ad">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
      <div>
        <div>
          <div>


          
            <table>
                <thead>
                  <tr>
                    <th>
                      Metric
                    </th>
                    <th>
                      Count
                    </th>
                  </tr>
                </thead>
                <tbody>
                <tr data-href="#!">
                    <td>
                      Time period
                    </td>
                    <td>
                      Aug 17,2020 - Aug 24,2020
                    </td>
                  </tr>
                  <tr data-href="#!">
                    <td>
                      Visitors
                    </td>
                    <td>
                      7,200
                    </td>
                  </tr>
                  <tr data-href="#!">
                    <td>
                      Emails
                    </td>
                    <td>
                      45
                    </td>
                    
                  </tr>
                  <tr data-href="#!">
                    <td>
                      Video calls
                    </td>
                    <td>
                      30
                    </td>
                    
                  </tr>
                  <tr data-href="#!">
                    <td>
                      <strong>Units sold</strong>
                    </td>
                    <td>
                      <b>10</b>
                    </td>
                  </tr>   
                  <tr data-href="#!">
                    <td>
                      <strong>Unit price</strong>
                    </td>
                    <td>
                      <b>$20,000</b>
                    </td>
                  </tr>
                  <tr data-href="#!">
                    <td>
                      <strong>Total revenue from DF</strong>
                    </td>
                    <td>
                      <b>$200,000</b>
                    </td>
                  </tr>
                  
                </tbody>
              </table>
              
              
          <ul>
              <li>
                Build cheap broadband distribution technology.
              </li>
              <li>
                Prove the tech works by connecting 1M people in one city.
              </li>
              <li>
                Deploy across 1,000 cities in India
              </li>
            </ul>

            

            <p>
                Our goal at <a href="https://www.wifidabba.com/">Wifi Dabba</a> is to lower the cost of broadband access in India. We use lasers instead of underground fiber as our core network and commodity components to dramatically lower the cost of deploying a broadband network.
            
                We've been running a beta network in Bengaluru, India for the last 9 months serving thousands of live customers. We're now ready to deploy a city wide network and provide cheap internet access to a million people.
            </p>

            <iframe width="100%" height="450" src="https://www.youtube.com/embed/LwVWJXBNQg8?autoplay=1" srcdoc="<style>*{padding:0;margin:0;overflow:hidden}html,body{height:100%}img,span{position:absolute;width:100%;top:0;bottom:0;margin:auto}span{height:1.5em;text-align:center;font:48px/1.5 sans-serif;color:white;text-shadow:0 0 0.5em black}</style><a href=https://www.youtube.com/embed/LwVWJXBNQg8?autoplay=1><img width='100%' style='min-height:250px;' src='https://img.youtube.com/vi/LwVWJXBNQg8/hqdefault.jpg' alt='Wifi Dabba overview'><span>▶</span></a>" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" title="Wifi Dabba overview"></iframe>

            
            <p>
              A core tenet of the Wifi Dabba network is distributed ownership. We believe that ownership of the internet should be in the hands of as many people as possible. If the cost of broadband tech drops, then more people can help pay for the cost of the network.
              And if you're one of the people paying for the distribution, we believe you should get revenue in return.
            </p>

            <p>
              We've divided the city of Bengaluru into 100 regions called PoPs. Anyone can buy a region and get a share in the revenue from those subscribers. 
            </p>

            <p>
              The Wifi Dabba franchise model:
            </p>

            <ul>
              <li>
                <strong>$20,000</strong> to purchase a 4sqkm. PoP.
              </li>
              <li>
                <strong>Minimum guaranteed revenue</strong> Paid quarterly with a 6 year rev share agreement.
              </li>
              <li>
                <strong>Fully managed service</strong> Be an absentee landlord
              </li>
            </ul>

            

            <p>
              <span>We've sold 40 as of the time of writing this.</span>
              <br>
              
              Wifi Dabba is insanely lucky for the amount of public support we have as a company. We regularly get phone calls, emails and even people dropping by our office just to tell us they like our service. Over the last 3 years we've received dozens of emails from people requesting franchises or other types of partnerships.
              We're incredibly humbled and thankful for this support on a daily basis.
            </p>

            
            <p>
              We believe there is a large group of people that care about the future of the internet and would be willing to put their money where their mouths are. As long as the price and the level of risk involved is reasonable.
              Our gut told us that this group would most likely be people that have seen success in the technology business as engineers, operators and entreprenuers.
            </p>

            
            <p>
              We've had our heads down over the last three years building and testing our network stack. Publicity or notariety has never been high on our list. 
              We've begun ramping up our social media efforts but it was clear that to kickstart our outreach, we had to do a little bit of advertising.
            </p>

            
            
              <p><img src="https://www.wifidabba.com/images/df-venn.png" height="300" width="330" alt="...">
                <br>
              </p>
            <p>
              <a href="https://www.daringfireball.net/">Daringfireball.net</a> is a great blog authored by <a href="https://en.wikipedia.org/wiki/John_Gruber">John Gruber</a> who is also the creator of <a href="https://daringfireball.net/projects/markdown/">Markdown</a>.
              DF was a natural choice for us as we've been readers of the blog for a long while and we knew from experience that DF readers would fit our target market rather well. Given the high quality of John's writing and insights into the industry, we felt that there would be a large pool of senior tech veterans that would be interested in Wifi Dabba among DF's audience.
            </p>

            <p>
                The sponsorship cost us $6,500 and ran for the week starting Aug 17, 2020 and we got:
                </p><ul>
                <li>
                  A display ad in the sidebar on every page of the site, all week long.
                </li>
                <li>
                  A post from the sponsor in the RSS feed at the start of the week. Us, the sponsor, got to address Daring Fireball’s most dedicated readers directly.
                </li>
                <li>
                  At the end of the week, John also posts an item thanking and linking to the feed sponsor.
                </li>
              </ul>
            

            <p>
                Stats about DF readership
                </p><ul>
                <li>
                Typical weekday web page views: 80,000–100,000.
                </li>
                <li>
                Estimated monthly web page views: 2.5 million.
                </li>
                <li>
                Estimated Daring Fireball RSS feed subscribers: Over 200,000.
                </li>
                <li>
                Twitter followers on the @daringfireball  account: Over 92,000.
                </li>
              </ul>
            
            

            
            <p>
                We created two variants of our message. Designed in bold colours to stand out against DF's dark theme. These creatives rotated randomly.
                We decided to focus on the technology because of the nature of the audience and hoped that the website did a good job of explaining the product.  
                </p><p><img src="https://www.wifidabba.com/images/df-ads.png" width="100%" alt="Buy internet POP">
                </p>
            

            
            <div>
                <p><strong>Click Ad -&gt; Browse site -&gt; Setup a call</strong></p><p>

                We expected visitors to click on the ad in DF and land on our homepage. Once on our site, we hoped that visitors would check out our videos as well as browse through a few pages.
                If they liked what they saw, we had a prominent buy button on the front page which led to a page to setup a video call.
            </p></div>

            <p>
                It's worth noting here that we knew going in that a large percentage of DF's audience would be using Ad-blockers. Nothing wrong with that, we use ad-blockers ourselves.
            </p>

            <p>
              Furthermore, we made a deliberate choice to add a high friction call to action and contact process. In order to purchase a PoP, a visitor would be directed to a calendar managed by calendly that would help them setup a call with someone from our team at a convenient time.
            </p>
            <p><a href="https://wifidabba.com/buy">
              <img width="100%" src="https://www.wifidabba.com/images/df-buy.png" alt="Setup Call">
            </a></p><p>
              The reason for this is that we knew DF would deliver a few hundred visitors a day to our site. We're a small team and our core focus is deploying the network, not necessarily sales and our goal is to sell the PoPs to people that are really excited a lot about our idea and show a high level of interest.
              The $20,000 price point of our product + the high friction of the contact process + users that are OK with ads = A high signal to noise ratio from DF visitors. 
              We'd love to hear any feedback on what you think about this.
            </p>

            
              

              <p>
                Our thesis turned out to be pretty spot on. Senior engineers from Google, Apple and a host of other technology companies purchased the PoPs.
                The actual sales process turned out to be fairly quick and straight forward. Most of the people that purchased the PoPs did so within a period of 48 hours of having the call.
              </p>

              



          </div>
        </div>
    </div>
    </section></div>]]>
            </description>
            <link>https://www.wifidabba.com/blog/200000-dollars-in-sales-from-one-daringfireball-ad</link>
            <guid isPermaLink="false">hacker-news-small-sites-25058363</guid>
            <pubDate>Wed, 11 Nov 2020 13:51:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Visualization of connections between politicians and orgs awarded gov contracts]]>
            </title>
            <description>
<![CDATA[
Score 35 | Comments 6 (<a href="https://news.ycombinator.com/item?id=25057931">thread link</a>) | @roxanneonhacker
<br/>
November 11, 2020 | https://sophieehill.shinyapps.io/my-little-crony/ | <a href="https://web.archive.org/web/*/https://sophieehill.shinyapps.io/my-little-crony/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <p>
          A visualization of the connections between
          <strong>Tory politicians</strong>
          and
          <strong>companies being awarded government contracts during the pandemic,</strong>
          based on reporting by
          <a href="https://www.opendemocracy.net/en/dark-money-investigations/">openDemocracy,</a>
          <a href="https://bylinetimes.com/">Byline Times,</a>
          and more.
        </p>
    </div></div>]]>
            </description>
            <link>https://sophieehill.shinyapps.io/my-little-crony/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25057931</guid>
            <pubDate>Wed, 11 Nov 2020 12:57:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Advanced System-on-Chip Design Lecture Notes]]>
            </title>
            <description>
<![CDATA[
Score 215 | Comments 16 (<a href="https://news.ycombinator.com/item?id=25057889">thread link</a>) | @allending
<br/>
November 11, 2020 | https://iis-people.ee.ethz.ch/~gmichi/asocd/lecturenotes/ | <a href="https://web.archive.org/web/*/https://iis-people.ee.ethz.ch/~gmichi/asocd/lecturenotes/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://iis-people.ee.ethz.ch/~gmichi/asocd/lecturenotes/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25057889</guid>
            <pubDate>Wed, 11 Nov 2020 12:51:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Write Unit Tests for Logging]]>
            </title>
            <description>
<![CDATA[
Score 55 | Comments 37 (<a href="https://news.ycombinator.com/item?id=25057372">thread link</a>) | @JanVanRyswyck
<br/>
November 11, 2020 | https://principal-it.eu/2020/11/unit-tests-for-logging/ | <a href="https://web.archive.org/web/*/https://principal-it.eu/2020/11/unit-tests-for-logging/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				<div>
					<h2>
						How To Write Unit Tests For Logging
					</h2>
					<p><span>
						November 11, 2020
					</span>
				</p></div>

				
<p>Once in a while I get asked the question whether one should write <a href="https://principal-it.eu/2019/10/taxonomy-of-tests/">solitary tests</a> for 
logging functionality. My answer to this question is the typical consultant answer: “It depends”. In essence, logging 
is an infrastructure concern. The end result is log data that is being written to a resource which is external to 
an application. Usually the generated data ends up in a file, a database or it might even end up in a cloud service.</p>

<p>Because logging crosses the process boundary of an application, it is more useful to write 
<a href="https://principal-it.eu/2019/10/taxonomy-of-tests/">sociable tests</a> to verify this particular functionality. It doesn’t make sense to 
use solitary tests in this particular case.</p>

<p>That being said, there are situations where business requirements explicitly state that logging should be a part of the 
interface of an application. In this situation, the intent of logging should be expressed explicitly by the code which 
in turn should also be exercised by solitary tests. The excellent book 
<a href="https://bit.ly/tdd-goos2" target="blank" rel="noopener noreferrer nofollow">Growing Object Oriented Software 
Guided By Tests</a>, written by Steve Freeman and Nat Pryce, mentions that there are generally two separate types of 
logging:</p>

<ul>
  <li>Support logging</li>
  <li>Diagnostic logging</li>
</ul>

<p>A support log contains messages that are intended for those that perform operational activities. These messages are used 
to determine whether the system behaves correctly or not. The log level for these messages is usually of type <em>error</em> 
or <em>info</em>.</p>

<p>A diagnostic log on the other hand holds messages that are targeted towards software developers. These messages provide 
valuable insights into the details of a running system. The log level for these messages is usually of type <em>debug</em> or 
<em>trace</em>.</p>

<p>Given these two types of logging, the basic idea is that code which expresses the intent of support logging should be 
exercised by solitary tests. Code statements that initiate diagnostic logging are usually not covered by tests.</p>

<p>Let’s have a look at an example that demonstrates both support and diagnostic logging in action.</p>

<pre><code>public class ExpenseSheetController : Controller
{
    private readonly ICommandHandler&lt;CreateExpenseSheet&gt; _commandHandler;
    private readonly ISupportNotifier _supportNotifier;

    public ExpenseSheetController(ICommandHandler&lt;CreateExpenseSheet&gt; commandHandler,
                                  ISupportNotifier supportNotifier)
    {
        _commandHandler = commandHandler;
        _supportNotifier = supportNotifier;
    }
    
    [HttpPost]
    [ServiceFilter(typeof(PerformanceTracing))]
    public IActionResult Create(CreateExpenseSheetFormModel formModel)
    {
        try
        {
            var command = new CreateExpenseSheet(Guid.NewGuid(), formModel.EmployeeId);
            _commandHandler.Handle(command);
        }
        catch(Exception ex)
        {
            _supportNotifier.ErrorDuringExpenseSheetCreation(ex, formModel.EmployeeId);
            return BadRequest();
        }
        
        _supportNotifier.ExpenseSheetCreated(formModel.EmployeeId);
        return Ok();
    }
}
</code></pre>

<p>Here we have the implementation of a controller that can receive a request for creating a new expense sheet. Notice that 
the constructor of this controller class expects an instance of the <em>ISupportNotifier</em> interface. This dependency is 
being used by the implementation of the <em>Create</em> method for logging an error when an exception occurs. It is also used 
for logging when an expense sheet has been successfully created.</p>

<p>This is how the implementation of the <em>SupportNotifier</em> looks like.</p>

<pre><code>public class SupportNotifier : ISupportNotifier
{
    private readonly ILogger&lt;SupportNotifier&gt; _logger;

    public SupportNotifier(ILogger&lt;SupportNotifier&gt; logger)
    {
        _logger = logger;
    }
    
    public void ExpenseSheetCreated(Guid employeeId)
    {
        _logger.LogInformation("Expense sheet created for employee with ID '{employeeId}'.");
    }

    public void ErrorDuringExpenseSheetCreation(Exception ex, Guid employeeId)
    {
        _logger.LogError(ex, $"Unable to create a new expense sheet for employee with ID '{employeeId}'");
    }
}
</code></pre>

<p>This code demonstrates that support logging uses log levels <em>error</em> or <em>info</em> depending on the context. Verifying the
code of the <em>SupportNotifier</em> class itself can be done by using sociable tests. It’s not a good idea to write
solitary tests for the <em>SupportNotifier</em> class. This would imply that a test double should be used as an instance of 
<em>ILogger</em>. As we already touched on in a <a href="https://principal-it.eu/2020/05/test-double-heuristics/">previous blog post</a>, it’s much better to 
avoid using test doubles for types that you don’t own. In this particular case it would even be quite hard to do as 
the <em>Logxx</em> methods of <em>ILogger</em> are actually extension methods and not regular methods.</p>

<p>Let’s have a look at the tests for the <em>ExpenseSheetController</em>.</p>

<pre><code>[Specification]
public class When_handling_a_request_for_creating_a_new_expense_sheet
{
    [Establish]
    public void Context()
    {
        var commandHandler = Substitute.For&lt;ICommandHandler&lt;CreateExpenseSheet&gt;&gt;();
        _supportNotifier = Substitute.For&lt;ISupportNotifier&gt;();

        _sut = new ExpenseSheetController(commandHandler, _supportNotifier);
    }

    [Because]
    public void Of()
    {
        var formModel = new CreateExpenseSheetFormModel 
        { 
            EmployeeId = new Guid("94EDE8F3-9675-4DD7-A18F-E37B1F323699") 
        };

        _sut.Create(formModel);
    }
    
    [Observation]
    public void Then_it_should_notify_support()
    {
        _supportNotifier.Received()
            .ExpenseSheetCreated(new Guid("94EDE8F3-9675-4DD7-A18F-E37B1F323699"));
    }

    private ExpenseSheetController _sut;
    private ISupportNotifier _supportNotifier;
}

[Specification]
public class When_an_error_occurs_while_handling_a_request_for_creating_a_new_expense_sheet
{
    [Establish]
    public void Context()
    {
        _supportNotifier = Substitute.For&lt;ISupportNotifier&gt;();
        _exception = new InvalidOperationException("Meltdown");
        
        var commandHandler = Substitute.For&lt;ICommandHandler&lt;CreateExpenseSheet&gt;&gt;();
        commandHandler.WhenForAnyArgs(ch =&gt; ch.Handle(null))
            .Throw(_exception);
        
        _sut = new ExpenseSheetController(commandHandler, _supportNotifier);
    }
    
    [Because]
    public void Of()
    {
        var formModel = new CreateExpenseSheetFormModel 
        { 
            EmployeeId = new Guid("D1067157-5C73-4140-9D29-0FE5C1C4C2FB") 
        };

        _sut.Create(formModel);
    }
    
    [Observation]
    public void Then_it_should_notify_support_that_a_new_expense_sheet_has_been_created()
    {
        _supportNotifier.Received()
            .ErrorDuringExpenseSheetCreation(_exception, 
                new Guid("D1067157-5C73-4140-9D29-0FE5C1C4C2FB"));
    }
    
    private ExpenseSheetController _sut;
    private ISupportNotifier _supportNotifier;
    private Exception _exception;
}
</code></pre>

<p>These tests verify whether support logging occurs when an expense sheet has been created or when an exception gets 
raised. This way we express the intent of the operational requirements.</p>

<p>Notice that controller method has been decorated with a <em>ServiceFilter</em> attribute.</p>

<pre><code>[HttpPost]
[ServiceFilter(typeof(PerformanceTracing))]
public IActionResult Create(CreateExpenseSheetFormModel formModel)
{
    ...
}
</code></pre>

<p>By applying this attribute, the <em>PerformanceTracing</em> action filter is being registered to surround the execution of the 
controller method. Let’s have a look at the implementation of this action filter.</p>

<pre><code>public class PerformanceTracing : ActionFilterAttribute
{
    private readonly ILogger&lt;PerformanceTracing&gt; _logger;
    private readonly Stopwatch _stopWatch;

    public PerformanceTracing(ILogger&lt;PerformanceTracing&gt; logger)
    {
        _logger = logger;
        _stopWatch = new Stopwatch();
    }

    public override void OnActionExecuting(ActionExecutingContext context)
    {
        _stopWatch.Start();
    }

    public override void OnActionExecuted(ActionExecutedContext context)
    {
        _stopWatch.Stop();

        var controllerName = context.Controller.GetType().Name;
        var controllerActionName = context.ActionDescriptor.DisplayName;
        
        _logger.LogTrace($"Action '{controllerActionName}' of controller {controllerName} executed in " + 
            $"{_stopWatch.ElapsedMilliseconds} ms.");
    }
}
</code></pre>

<p>This implementation is a nice example of diagnostic logging. The action filter measures the execution time of a 
controller method and logs the result. Notice that we’re injecting the <em>ILogger</em> interface directly into the constructor.
By registering the <em>PerformanceTracing</em> action filter using the <em>ServiceFilter</em> attribute, we ensure that an instance 
of <em>ILogger</em> gets resolved and properly injected. We didn’t provide any tests for this implementation.</p>

<p>I think it’s useful to consider support logging and diagnostic logging as two separate concepts, even though they quite 
often use the same mechanisms under the hood.</p>


				<p>
						<em>
							If you and your team want to learn more about how to <u>write maintainable unit tests</u>
							and <u>get the most out of TDD practices</u>, make sure to have look at our
							<a href="https://principal-it.eu/training.html">trainings and workshops</a> or checkout
							the <a href="https://principal-it.eu/books.html">books section</a>. Feel free to reach
							out at <span>info@principal-it.be</span>.
						</em>
					</p>

				

				

				
			</div></div>]]>
            </description>
            <link>https://principal-it.eu/2020/11/unit-tests-for-logging/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25057372</guid>
            <pubDate>Wed, 11 Nov 2020 11:06:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Option Hacking the Tektronix TDS 420A]]>
            </title>
            <description>
<![CDATA[
Score 57 | Comments 21 (<a href="https://news.ycombinator.com/item?id=25057162">thread link</a>) | @segfaultbuserr
<br/>
November 11, 2020 | https://tomverbeure.github.io/2020/07/11/Option-Hacking-the-Tektronix-TDS-420A.html | <a href="https://web.archive.org/web/*/https://tomverbeure.github.io/2020/07/11/Option-Hacking-the-Tektronix-TDS-420A.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p><em>Previous installments in this series: <a href="https://tomverbeure.github.io/2020/06/27/In-the-Lab-Tektronix-TDS420A.html">In the Lab - Tektronix TDS 420A Oscilloscope</a>, 
 <a href="https://tomverbeure.github.io/2020/06/27/Tektronix-TDS420A-Remote-Control-over-GPIB.html">Tektronix TDS 420A Remote Control over GPIB</a>, 
 <a href="https://tomverbeure.github.io/2020/07/02/Extracting-the-Tektronix-TDS420A-Firmware.html">Extracting the Tektronix TDS 420A Firmware</a>, 
 <a href="https://tomverbeure.github.io/2020/07/03/TDS420A-Serial-Debug-Console-Symbol-Table-Ghidra.html">A Tektronix TDS 420A, a Serial Debug Console, a Symbol Table, and Ghidra</a></em></p>

<ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#how-a-tds400-oscilloscope-manages-hardware-features" id="markdown-toc-how-a-tds400-oscilloscope-manages-hardware-features">How a TDS400 Oscilloscope Manages Hardware Features</a></li>
  <li><a href="#the-key-to-enabling-option-05---video-triggering" id="markdown-toc-the-key-to-enabling-option-05---video-triggering">The Key to Enabling Option 05 - Video Triggering</a></li>
  <li><a href="#the-key-to-enabling-option-2f---advanced-dsp-math" id="markdown-toc-the-key-to-enabling-option-2f---advanced-dsp-math">The Key to Enabling Option 2F - Advanced DSP Math</a></li>
  <li><a href="#options-05-and-2f-enabled" id="markdown-toc-options-05-and-2f-enabled">Options 05 and 2F Enabled!</a></li>
  <li><a href="#option-1m---120k-sample-points---a-different-story" id="markdown-toc-option-1m---120k-sample-points---a-different-story">Option 1M - 120K Sample Points - A Different Story</a></li>
  <li><a href="#in-search-of-the-missing-memory" id="markdown-toc-in-search-of-the-missing-memory">In Search of the Missing Memory</a></li>
  <li><a href="#success-at-last" id="markdown-toc-success-at-last">Success at Last!</a></li>
  <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
</ul>



<p>I wrote <a href="https://tomverbeure.github.io/2020/06/27/In-the-Lab-Tektronix-TDS420A.html#the-tektronix-tds-420a-in-brief">earlier</a>
about the optional features of TDS 400 series of oscilloscopes:</p>

<ul>
  <li>Option 05: Video Trigger</li>
  <li>Option 13: RS-232/Centronics Hardcopy Interface</li>
  <li>Option 1F: File System/Floppy</li>
  <li>Option 2F: Advanced DSP Math</li>
  <li>Option 1M: 120k waveform sample points</li>
</ul>

<p>Most scopes, including mine, come with options 13 and 1F, but the remaining ones are less common.</p>

<p>The video triggering and advanced DSP math options are pure firmware functions, but even 
the 120k sample points option seemed like something that could be enabled with a software hack, since
the signal acquisition board has the 512KB of RAM available to store the data.</p>

<p>Here, I’ll describe how the TDS 400 series manages option enablement, and
how you can hack the scope into getting them to work.</p>



<p>Using Ghidra and the debug console, I figured out how the scope manages hardware
features: it has a function called <code>hwAccountantQuery</code> that has a single
parameter which I’ll call the ‘feature ID’.</p>

<p><code>hwAccountantQuery</code> will return an integer value for that feature ID. These values
can be boolean in nature (“Is a certain feature present or not”) or can be the
amount of DSP memory etc.</p>

<p>Here’s a very non-exhaustive list of codes that I’ve been able to identify:</p>

<div><div><pre><code>0x20d: number of scope channels
0x20f: size of acquisition RAM
0x216: ProbeD2MemSize
0x248: CPU clock period
0x255: InstrumentNameStringPtr
0x271: hwProbeSpecialDiagModeActive
0x2a0: hwProbeSpecialDiagLoopCount
0x2a1: hwProbeSpecialDaigSeqId
0x2b8: 30000 points -&gt; value when 1M option is not possible
0x2bf: TDS420A
0x2d2: RS232 Debug uart present
0x317: MathPak      -&gt; this is the advanced DSP math function
0x461: Floppy drive present
0x537: flashRomDateStringPtr
0x54c: TDS410A
0x560: TDS430A
0x700: hwProbeTvTrigPresent
</code></pre></div></div>

<p><code>hwAccountantQuery</code> calls <code>hwAccountantGetValue</code>. The first part of that function looks liks this:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/hwAccountantGetValue.png" alt="hwAccountantGetValue"></p>

<p>It’s a large <code>if-then-else</code> or <code>case</code> statement that calls a dedicated function for a particular
feature ID.</p>



<p>Did you see <code>_hwProbeTvTrigPresent()</code>? That’s the function that checks
if the video triggering feature should be enabled:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/hwProbeTvTrigPresent.png" alt="hwProbeTvTrigPresent"></p>

<p>And there we have it! To enable “Option 05 - Video Triggering”, all you need to do
is store a non-zero value in non-volatile RAM location 7!</p>

<p><em>This is not a shocking new discovery: plenty of online sources already mentioned this,
but it’s great to confirm it from first principles, by going to the source.</em></p>



<p>Internally, the Advanced Math DSP is called “MathPak”. Just like for video triggering, 
the <code>hwAccountGetValue</code> function issues a call to <code>hwProbeMathPakPresent()</code>:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/hwProbeMathPakPresent.png" alt="hwProbeMathPakPresent"></p>

<p>Option 2F simply relies on a non-zero value in NVRAM location 9!</p>



<p>It’s now just a matter of issuing the following 2 commands on the debug console:</p>

<div><div><pre><code>libManagerWordAtPut 0x50007, 1
libManagerWordAtPut 0x50009, 1
</code></pre></div></div>

<p>My scope booted up with this image:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/options_05_and_2f_enabled.jpg" alt="Options 05 and 2F enabled"></p>

<p>Success! I’m now the proud owner of a scope that supports an entirely obsolete video triggering
mode, and a FFT math option!</p>

<p>Video Triggering Menu:
<img src="https://tomverbeure.github.io/assets/tds420a/video_triggering_features.jpg" alt="Video triggering features"></p>

<p>Live FFT of a 1kHz square wave:
<img src="https://tomverbeure.github.io/assets/tds420a/fft.jpg" alt="FFT"></p>



<p>Unfortunately, the <code>case</code> statement is only a small part of the <code>hwAccountGetValue</code> function: most
feature checking functions are performed by looping through an array of structs that
have the feature ID and a function pointer to the checking function. It’s a bit harder to figure 
out in Ghidra, but we already know that the function names to enable options start with <code>hwProbe</code>.</p>

<p>With Ghidra, we can filter on this, and that gives the <code>hwProbe1MOption</code> and the 
<code>hwProbe1MPresent</code> functions.</p>

<p><code>hwProbe1MPresent</code> looks very familiar:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/hwProbe1MPresent.png" alt="hwProbe1MPresent"></p>

<p>Just like for the 05 and 2F options, we need to set a specific byte in the
NVRAM:</p>

<div><div><pre><code>libManagerWordAtPut 0x50006, 1
</code></pre></div></div>

<p><code>hwProbe1MOption</code> is a different story:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/hwProbe1MOption.png" alt="hwProbe1MOption"></p>

<p>When you run <code>hwProbe1MOption</code> on the command line, the function returns a 0.</p>

<p>Feature IDs 0x216 and 0x20f are also part of the array of structs. They call the functions
<code>hwProbeD2MemSize</code> and <code>hwProbeAcqMemSize</code> respectively.</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/hwProbeTable.png" alt="hwProbe table"></p>

<p><code>hwProbeD2MemSize</code> and <code>hwProbeAcqMemSize</code> both run a test to check the amount of RAM that 
is populated on the board.</p>

<p>When you run these query commands on the debug console, you get:</p>

<div><div><pre><code>hwAccountantQuery(0x216)    
262143
hwAccountantQuery(0x20f)    
131071
</code></pre></div></div>

<p>It’s now clear why option 1M doesn’t get enabled after changing the NVRAM value: 
feature ID 0x20f is fine (131071/0x1ffff is larger than 0x1fffe), but feature ID 0x216 is not 
(262143/0x3ffff is smaller than 0xffffe).</p>

<p>Whatever it is used for, the amount of “D2” memory in the scope is too small.</p>



<p>This finally gave me the crucial hint to start looking at other PCBs inside the scope and
try to find if there’s a place with empty footprints for RAM chips.</p>

<p>I call this the DSP PCB. Luckily, it’s a board that’s easy to remove from the chassis, without 
fragile flex cables or connectors.</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/dsp_pcb.jpg" alt="DSP PCB"></p>

<p>Look at those 6 beautiful, unused footprints!</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/ram_footprints_closeup.jpg" alt="RAM footprints closeup"></p>

<p>The RAM chips are M5M51008 with a 100ns speed rating, made by Mitsubishi LSI.</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/memory_datasheet.jpg" alt="Memory Datasheet"></p>

<p>Surprisingly, Digikey still carries these parts: they’re now made by Rochester
Electronics, and only available in 70ns or 55ns version, but faster is better,
so that shouldn’t be a problem.</p>

<p>They’re cheap too at just $2.56 a piece.</p>

<p>The only issue is a minimum order quantity of 100 parts. $256 for a feature
on a 25 years old $190 oscilloscope is a bit too much! Luckily, the parts
are available at various Chinese chip brokers: I was able to buy them at 
<a href="https://utsource.net/">UTSource</a> for just $1.81 a piece. Even when buying 10 
of them (for redundancy), shipping was the biggest part of the cost:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/memory_order.png" alt="Memory Order"></p>

<p><em>Once ordered, UTSource let me know that these parts were refurbished…</em></p>

<p>A few days later, the parts arrived at my front door, ready to be populated
on the DSP board:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/dsp_board_before_surgery.jpg" alt="DSP Board Before Surgery"></p>

<p>Note how I did not disconnect the battery that’s wired to the board: it’s used to
permanently provide power to those 4 RAMs chips on the left that are encased into 
some transparant polymer gu. Removing the battery will result in lost calibration
data (or so they say.)</p>

<p>I used a regular soldering iron instead of a hot air gun to attach the 6 RAMs:
there was enough solder on the pads and I’m most comfortable doing it that way.
Afterwards I Ohm’ed out most of the pins, and I’m glad I did because
there were some open connections.</p>

<p>The end result isn’t perfect, but it’s good enough:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/rams_populated.jpg" alt="RAMs Populated"></p>



<p>With the RAM populated, it’s time to power on the scope and check the result
of the enhancement surgery!</p>

<p>The scope bootup screen looks good:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/option_1m_enabled.jpg" alt="Option 1M enabled"></p>

<p>And this formerly grayed out 12000 points menu option is now available:</p>

<p><img src="https://tomverbeure.github.io/assets/tds420a/120k_points.jpg" alt="120K Points"></p>

<p>Victory at Last!</p>



<p>The TDS 420A is an old oscilloscope, and even with those 3 new options enabled, it’s
far inferior to my Siglent 2304X or even my HP 54825A (Windows 95!) loaner.</p>

<p>120K sample points is obviously better than 30K, but it still pales in comparison
to the 140M sample points of the Siglent.</p>

<p>So what then was the point of this whole exercise?</p>

<p>I got a close up view of oscilloscope internals, I learned Ghidra from scratch and
applied it on a real, non-trival project, I added RAM to a 25 year old oscilloscope 
and it worked, I spent tons of late night hours decoding firmware, and 
I had an unreasonable amount of fun doing so.</p>

<p>I even started to appreciate the Tektronix user interface a little bit!</p>

<p>It was time well spent.</p>

<p>For now, the scope will remain on my bench while I start adding Tektronix support 
in glscopeclient. That was the whole point of acquiring the scope to being with!</p>

<p>And if it turns out that it’s really too limited for my use, I can always
sell it back on eBay, this time with 3 additional features enabled.</p>



<ul>
  <li>
    <p><a href="https://www.eevblog.com/forum/testgear/hacking-my-tds460a-to-have-options-1m2f/">Hacking my TDS460A to have options 1M/2F?</a></p>
  </li>
  <li>
    <p><a href="https://forum.tek.com/viewtopic.php?t=140268">TDS420 Options Possible?</a></p>
  </li>
  <li>
    <p><a href="http://videohifi17.rssing.com/chan-62314146/all_p49.html">Upgrade Tektronix: FFT analyzer</a></p>

    <p>Story about upgrading the CPU board from 8MB to 16MB on a TDS420 (not the 420A?) and then FFT in the
  NVRAM.</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=iJt2O5zaLRE">Enabling FFT option in Tektronix TDS 540A oscilloscope</a></p>

    <p>Not very useful for 420A owners: enables FFT by copying NVRAM EEPROM.</p>
  </li>
  <li>
    <p><a href="https://www.eevblog.com/forum/testgear/tds420-with-lost-options/msg2032465/?PHPSESSID=021nnvu02ca549sh5le7s9r8i5#msg2032465">TDS420 with lost options</a></p>

    <p>Specific comment about how to enable options on the 420A over GPIB. I wasn’t able to get this to 
  work for some reason.</p>
  </li>
  <li>
    <p><a href="http://www.ko4bb.com/getsimple/index.php?id=enable-tds754d-options">Enable TDS754D Options using GPIB</a></p>

    <p>Another one about using GPIB.</p>
  </li>
</ul>


  </div>
</article>

      </div>
    </div></div>]]>
            </description>
            <link>https://tomverbeure.github.io/2020/07/11/Option-Hacking-the-Tektronix-TDS-420A.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25057162</guid>
            <pubDate>Wed, 11 Nov 2020 10:22:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Bdshemu: Bitdefender shellcode emulator]]>
            </title>
            <description>
<![CDATA[
Score 65 | Comments 23 (<a href="https://news.ycombinator.com/item?id=25057062">thread link</a>) | @mdontu
<br/>
November 11, 2020 | https://hvmi.github.io/blog/2020/11/11/bdshemu.html | <a href="https://web.archive.org/web/*/https://hvmi.github.io/blog/2020/11/11/bdshemu.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <h2 id="introduction">Introduction</h2>

<p>Detecting exploits is one of the major strengths of Hypervisor Memory Introspection (HVMI). The ability to monitor guest physical memory pages against different kinds of accesses, such as write or execute, allows HVMI to impose restrictions on critical memory regions: for example, stack or heap pages can be marked as being non-executable at the EPT level, so when an exploit manages to gain arbitrary code execution, the introspection logic would step in and block the execution of the shellcode.</p>

<p>In theory, intercepting execution attempts from memory regions such as the stack or the heap should be enough to prevent most of the exploits. Real life is often more complicated, and there are many cases where legit software uses techniques that may resemble on attack - Just In Time compilation (JIT) in browsers is one good example. In addition, an attacker may store its payload in other memory regions, outside the stack or the heap, so a method of discerning good code from bad code is useful.</p>

<p>We will talk in this blog post about the Bitdefender Shellcode Emulator, or <a href="https://github.com/bitdefender/bddisasm">bdshemu</a> for short. bdshemu is a library capable of emulating basic x86  instructions (in all modes - 16, 32 and 64 bit), while observing shellcode-like behavior. Legitimate code, such as JIT code, will look different compared to a traditional shellcode, so this is what bdshemu is trying to determine: whether the emulated code behaves like a shellcode or not.</p>

<h2 id="bdshemu-overview">bdshemu Overview</h2>

<p>bdshemu is a library written in C, and is part of the bddisasm project (and of course, it makes use of bddisasm for instruction decoding). The bdshemu library is built to emulate x86 code only, so it has no support for API calls. In fact, the emulation environment is highly restricted and stripped down, and there are only two memory regions available:</p>

<ul>
  <li>The page(s) containing the emulated code;</li>
  <li>The stack;</li>
</ul>

<p>Both of these memory regions are virtualized, meaning that they are in fact copies of the actual memory being emulated, so modifications made to them don’t affect the actual system state. Any access made by the emulated code outside of these two areas (which we will call the shellcode and the stack, respectively) will trigger immediate emulation termination. For example, an API call will automatically cause a branch outside the shellcode region, thus terminating emulation. However, in bdshemu, all we care about is instruction-level behavior of the code, which is enough to tell us whether the code is malicious or not.</p>

<p>While bdshemu provides the main infrastructure for detecting shellcodes inside a guest operating-system, it is worth noting that this is not the only way HVMI determines that execution of a certain page is malicious - two other important indicators are used:</p>

<ul>
  <li>The executed page is located on the stack - this is common with stack-based vulnerabilities;</li>
  <li>The stack is pivoted - when a page is first executed and the <code>RSP</code> register points outside the normal stack allocated for the thread;</li>
</ul>

<p>These two indicators are enough on their own to trigger an exploit detection. If these are not triggered, bdshemu is used to take a good look at the executed code, and decide if it should be blocked or not.</p>

<h2 id="bdshemu-architecture">bdshemu Architecture</h2>

<p>bdshemu is created as a standalone C library, and it only depends on bddisasm. Working with bdshemu is fairly simple, as just like bddisasm, it is a single-API library:</p>
<div><div><pre><code><span>SHEMU_STATUS</span>
<span>ShemuEmulate</span><span>(</span>
    <span>SHEMU_CONTEXT</span> <span>*</span><span>Context</span>
    <span>);</span>
</code></pre></div></div>

<p>The emulator expects a single <code>SHEMU_CONTEXT</code> argument, containing all the needed information in order to emulate the suspicious code. This context is split in two sections - input parameters and output parameters. The input parameters must be supplied by the caller, and they contain information such as the code to be emulated, or initial register values. The output parameters contain information such as what shellcode indicators bdshemu detected. All these fields are well documented in the source-code.</p>

<p>Initially, the context is filled in with the following main information (please note that emulation outcome may change depending on the value of the provided registers and stack):</p>

<ul>
  <li>Input registers, such as segments, general purpose registers, MMX and SSE registers; they can be left 0, if they are not known, or if they are irrelevant;</li>
  <li>Input code, which is the actual code to be emulated;</li>
  <li>Input stack, which can contain actual stack contents, or can be left 0;</li>
  <li>Environment info, such as mode (32 or 64 bit), or ring (0, 1, 2 or 3);</li>
  <li>Control parameters, such as minimum stack-string length, minimum NOP sled length or the maximum number of instructions that should be emulated;</li>
</ul>

<p>The main output parameter is the <code>Flags</code> field, which contains a list of shellcode indicators detected during the emulation. Generally, a non-zero value of this field strongly suggests that the emulate code is, in fact, a shellcode.</p>

<p>bdshemu is built as a plain, quick and simple x86 instruction emulator: since it only works with the shellcode itself and a small virtual stack, it doesn’t have to emulate any architectural specifics - interrupts or exceptions, descriptor tables, page-tables, etc. In addition, since we only deal with the shellcode and stack memory, bdshemu does not do memory access checks, since it doesn’t even allow accesses to other addresses. The only state apart from the registers that can be accessed is the shellcode itself and the stack, and both are copies of the actual memory contents - the system state is never modified during the emulation, only the provided <code>SHEMU_CONTEXT</code> is. This makes bdshemu extremely fast, simple, and lets us focus on its main purpose: detecting shellcodes.</p>

<p>As far as instruction support goes, bdshemu supports all the basic x86 instructions, such as branches, arithmetic, logic, shift, bit manipulation, multiplication/divison, stack access and data transfer instructions. In addition, it also has support for other instructions, such as some basic MMX or AVX instructions - <code>PUNPCKLBW</code> or <code>VPBROADCAST</code> are two good examples.</p>

<h2 id="bdshemu-detection-techniques">bdshemu Detection Techniques</h2>

<p>In order to determine whether an emulated piece of code behaves like a shellcode, there are several indicators bdshemu uses.</p>

<h3 id="nop-sled">NOP Sled</h3>

<p>This is the classic presentation of shellcodes; since the exact entry point of the shellcode when gaining code execution may be unknown, attackers usually prepend a long sequence of <code>NOP</code> instructions, encoding <code>0x90</code>. The parameters for the NOP-sled length can be controlled when calling the emulator, via the <code>NopThreshold</code> context field. The default value is <code>SHEMU_DEFAULT_NOP_THRESHOLD</code>, which is <code>75</code>, meaning that minimum 75% of all the emulated instruction must be <code>NOP</code>.</p>

<h3 id="rip-load">RIP Load</h3>

<p>Shellcodes are designed to work correctly no matter what address they’re loaded at. This means that the shellcode has to determine, dynamically, during runtime, the address it was loaded at, so absolute addressing can be replaced with some form of relative addressing. This is typically achieved by retrieving the value of the instruction pointer using well-known techniques:</p>

<ul>
  <li><code>CALL $+5/POP ebp</code> - executing these two instructions will result in the value of the instruction pointer being stored in the <code>ebp</code> register; data can then be accessed inside the shellcode using offsets relative to the <code>ebp</code> value;</li>
  <li><code>FNOP/FNSTENV [esp-0xc]/POP edi</code> - the first instruction is any FPU instruction (not necessarily <code>FNOP</code>), and the second instruction, <code>FNSTENV</code> saves the FPU environment on the stack; the third instruction will retrieve the <code>FPU Instruction Pointer</code> from <code>esp-0xc</code>, which is part of the FPU environment, and contains the address of the last FPU executed - in our case, <code>FNOP</code>; from there on, addressing relative to the <code>edi</code> can be used to access shellcode data;</li>
</ul>

<p>Internally, bdshemu keeps track of all the instances of the instruction pointer being saved on the stack. Later loading that instruction pointer from the stack in any way will result in triggering this detection. Due to the way bdshemu keeps track of the saved instruction pointers, it doesn’t matter when, where or how the shellcode attempts to load the RIP in a register and use it, bdshemu will always trigger a detection.</p>

<p>In 64 bit, RIP-relative addressing can be used directly, since the instruction encoding allows it. However, surprisingly, a large number of shellcodes still use a classic method of retrieving the instruction pointer (generally the <code>CALL/POP</code> technique), which is somehow weird, but it probably indicated that 32 bit shellcodes were ported to 64 bit with minimal modifications.</p>

<h3 id="write-self">Write Self</h3>

<p>Most often, shellcodes come in encoded or encrypted forms, in order to avoid certain bad characters (for example, <code>0x00</code> in a shellcode that should resemble a string may break the exploit) or to avoid detection by security technologies (for example, AV scanners). This means that during runtime, the shellcode must decode itself (usually in-place), by modifying its own contents, and then executing the plain-text code. Typical methods of decoding involve <code>XOR</code> or <code>ADD</code> based decryption algorithms.</p>

<p>Certainly, bdshemu follows this kind of behavior, and keeps track internally of each modified byte inside the shellcode. Whenever the suspected shellcode writes any portion of itself, and then it executes it, the self-write detection will be triggered.</p>

<h3 id="tib-access">TIB Access</h3>

<p>Once a shellcode has gained code execution, it needs to locate several functions inside various modules, in order to carry its actual payload (for example, downloading a file, or creating a process). On Windows, the most common way of doing this is by parsing the user-mode loader structures, in order to locate the addresses where the required modules were loaded, and then locate the needed functions inside these modules. The sequence of structures the shellcode will access is:</p>

<ol>
  <li>The Thread Environment Block (<code>TEB</code>), which is located at <code>fs:[0]</code> (32 bit thread) or <code>gs:[0]</code> (64 bit thread);</li>
  <li>The Process Environment Block (<code>PEB</code>), which is located at <code>TEB+0x30</code> (32 bit) or <code>TEB+0x60</code> (64 bit)</li></ol></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://hvmi.github.io/blog/2020/11/11/bdshemu.html">https://hvmi.github.io/blog/2020/11/11/bdshemu.html</a></em></p>]]>
            </description>
            <link>https://hvmi.github.io/blog/2020/11/11/bdshemu.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25057062</guid>
            <pubDate>Wed, 11 Nov 2020 10:01:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Cell Signaling Technologies – Detailed 3D model of human cells]]>
            </title>
            <description>
<![CDATA[
Score 163 | Comments 50 (<a href="https://news.ycombinator.com/item?id=25055908">thread link</a>) | @ozten
<br/>
November 10, 2020 | http://www.digizyme.com/cst_landscapes.html | <a href="https://web.archive.org/web/*/http://www.digizyme.com/cst_landscapes.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="u6389-bw">
     <div id="u6389"><!-- column -->
      <div id="u6389_align_to_page">
       <!-- m_editable region-id="editable-static-tag-U6264-BP_infinity" template="cst_landscapes.html" data-type="html" data-ice-options="disableImageResize,link,txtStyleTarget" -->
       <p><span id="u6264">Cell Signaling Technologies</span></p>
       <!-- /m_editable -->
       <!-- m_editable region-id="editable-static-tag-U6399-BP_infinity" template="cst_landscapes.html" data-type="html" data-ice-options="disableImageResize,link,txtStyleTarget" -->
       <p>Molecular Landscapes</p>
       <!-- /m_editable -->
       
       
       
      </div>
     </div>
    </div></div>]]>
            </description>
            <link>http://www.digizyme.com/cst_landscapes.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25055908</guid>
            <pubDate>Wed, 11 Nov 2020 06:02:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Eddie's Ink Chip Hack (2002)]]>
            </title>
            <description>
<![CDATA[
Score 78 | Comments 22 (<a href="https://news.ycombinator.com/item?id=25054177">thread link</a>) | @userbinator
<br/>
November 10, 2020 | http://www.eddiem.com/photo/CIS/inkchip/chip.html | <a href="https://web.archive.org/web/*/http://www.eddiem.com/photo/CIS/inkchip/chip.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<tr>
<td>
<p lang="en-GB"><a href="http://www.eddiem.com/photo/CIS/cis.htm">My CIS page.</a></p></td>
<th>
<p lang="en-GB"><img alt="" src="http://www.eddiem.com/photo/CIS/inkchip/inkchip.JPG" name="Graphic2" width="401" height="400"></p></th>
<td>
<p lang="en-GB"><a href="http://www.eddiem.com/photo/printer/chipreset/resetchip.html">Part
				2 build your own reseter</a></p></td></tr>
<tr>
<td colspan="3">
<p lang="en-GB"><strong>What is a Intellidge ink chip.</strong><span size="5"><br>
</span>Epson fit
				small circuit boards to most of their ink cartridges. These
				record the amount of ink that is estimated to be in the
				cartridge. I read that the official epson line is that it is for
				the customers benefit and not an anti-refill device. Whether you
				believe this or not they are a bloody nuisance to anyone wanting
				to refill the cartridges or use bulk ink. It also stops people
				using old cartridges full of solvent for cleaning the heads.
				Another problem was early printer models didn't check if the
				cartridge had been changed while power was on. This was good if
				you wanted to trick the printer into copying a “full”
				chip to and empty one, however the reverse was also true and you
				could easily copy and “empty” one into your full
				one.<br>
They are just a small memory device holds 32 bytes of
				data, they do not measure real ink level and nor does the
				printer. The printer reads the chips on startup, estimates
				(sometimes badly) how much ink should have been used and writes
				this back at shutdown. They hold other data as well.<br>
So epson
				go to the trouble of fitting chips to cartridges and building all
				the extra sockets, wiring, electronics and software into the
				printer so you can use the computer to see the predicted level
				and it can stop you printing if it think you've used enough ink.
				High-end Canon's on the other hand make the inks tank clear so
				you can see and have optical sensor to detect emptiness. This
				make a lot more sense – unless you are making an
				anti-refill device that is. Canon almost got my business this
				time but nobody I could find has run pigment in them – too
				risky.<br>
To get around the chip problems someone usually end up
				producing read-only chip which always read full (for use with
				CIS) and chip reseters for those who want to refill. These are
				not available for 2100p at the time of writing as far as I can
				tell.<br>
Before ordering my 2100p I did my homework and it seemed
				fairly likely a chip reseter would become available at some point
				and read-only chips as well. I was also cocky enough to think I
				could crack it myself and I have. It didn't go quite as expected
				though.<br>
<strong>What do I want to do?</strong><br>
I want the easiest way
				to fool the printer into believing it has full cartridges present
				so I can build my CIS.<br>
<strong>What did I expect?</strong><br>
A logical
				interface for Intellidge is i2c (i squared c) or TWI (two wire
				interface). Then the chip could just be some standard i2c eeprom.
				The Intellidge have too many pads for this but I was hopeful.
				After that would could SPI or microwire – again this could
				use off the shelf parts. If the chips were micro-controllers then
				plain asynchronous serial would be my choice.<br>
<strong>I had a
				look.</strong><br>
To do this I use a AVR mega323 micro, I declined
				offers of logic analyzers being a homebrew type of guy. The 323
				has 2K of internal ram which is enough for some minimalist data
				logging. It was about $50AUS in parts ($30US) to make. I wired a
				cartridge to bring the signals out and took a quick look with a
				voltmeter.<br>
<strong>Nothing!</strong><br>
There was nothing there. I
				expected some power but no, the chips are only powered briefly
				when the are accessed. I used leds to get a rough idea what was
				what and hooked up the micro via resistors to give some degree of
				protection to the printer if I screwed up. The code in the micro
				was written is assembler and captured data sent via rs232 to my
				PC where I wrote a delphi program to display and process the
				data.</p></td></tr>
<tr>
<td colspan="3">
</td></tr>
<tr>
<td colspan="3">
<p lang="en-GB"><a href="http://www.eddiem.com/photo/CIS/inkchip/traces.html"><img alt="" src="http://www.eddiem.com/photo/CIS/inkchip/fastendsml.gif" name="Graphic5" width="615" height="213"></a><br>
<a href="http://www.eddiem.com/photo/CIS/inkchip/traces.html">Click
				here for more traces.</a></p>
<p lang="en-GB">This is the sort of thing I got. No
				protocol I ever seen. Obviously synchronous with bi-directional
				data, very short format. I was confused a little by how short it
				was - because I expect much better precision for the ink
				level.<br>
The traces seem to be.<br>
Top – some sort of sync
				line, this always goes low before the start of transmission.<br>
Next
				– power this goes low (off) between chip reads at printer
				startup but stays high during the shutdown – when data is
				written to the chip.<br>
Next – the clock, data is read of
				the rising edge and changed on the falling.<br>
Bottom –
				bi-directional data, the first 4 bits are always from printer to
				the chip, the rest depend on whether it is a read or write. LSB
				first (left).</p></td></tr>
<tr>
<td colspan="3">
<p lang="en-GB">Convert to binary and some patterns emerge.<br>
It
				was not real obvious how the chips were addressed or which bits
				encoded ink levels. Some more data when some ink had been used
				made it easier.</p>
<p lang="en-GB">Below is one chip being read at startup, there
				are 7 accesses one for each chip. Only 3 block have data –
				the other chips must be hooked to different data lines.</p>
<div lang="en-GB"><p><img alt="" src="http://www.eddiem.com/photo/CIS/inkchip/strtbinary.gif" name="Graphic6" width="818" height="45"></p></div></td></tr>
<tr>
<td colspan="3">
<p lang="en-GB">Below is the complete shutdown stream. Again we
				can only see 3 chips from here.</p>
<p lang="en-GB"><img alt="" src="http://www.eddiem.com/photo/CIS/inkchip/endbinary.gif" name="Graphic7" width="297" height="199"><br clear="left">
After
				printing a few bits near the beginning of the bit stream did
				change. It looks to me like the first 3 bits are the chip address
				the next is a write bit then the ink level, I get the feeling
				there aren't many bits used to encode it (later looks like 6).<br>
So
				– the top one shows 252 bits of data being read out of the
				chip.<br>
The first part of the shutdown shows just the ink level
				being read out, this is to check the same chip is there.<br>
The
				second part is the ink-level and some other stuff (printer serial
				number maybe) being written into the chip. Seeing I didn't use
				any ink the bit-stream is identical to the read except for bit 3
				– presumably the write bit.</p></td></tr>
<tr>
<td colspan="3">
<p lang="en-GB"><span size="5"><br>
Tuesday
				24 Sept 2002. I fooled the printer.</span></p></td></tr>
<tr>
<td colspan="3">
<p lang="en-GB"><img alt="" src="http://www.eddiem.com/photo/CIS/inkchip/spoofed.gif" name="Graphic8" width="381" height="363"><span size="3">The
				interesting thing about this screen grab is the black cartridge
				is really only two thirds full. I spoofed the printer by pulling
				the serial data line low during the time the ink level bits are
				being clocked out of the ink chip.<br>
This is means 6 bits
				starting at the 5'th bit in the stream.</span></p>
<p lang="en-GB"><span size="3">The first 3 bits appear to be the
				chip address, I guess the next is a read/write select. I used a
				AVR mega323 to detect the start of the serial transmission look
				for the address of chip1+read (black apparently) then pull data
				low for 6 clock edges. </span></p>
<p lang="en-GB"><span size="3">I'm sure I can reset 3 of the chips
				by tapping into chip1 signal. Reseting the rest will mean tapping
				into at least one more. </span></p>
<p lang="en-GB"><span size="3">The current set up is for
				experimentation only – it is not “the real thing”.</span></p>
<p lang="en-GB"><span size="3">Shorting the data to ground may be a
				bit drastic but it is only for a very brief time. I hoped the
				data line would be open collector but this doesn't seem to be the
				case.</span></p></td></tr></div></div>]]>
            </description>
            <link>http://www.eddiem.com/photo/CIS/inkchip/chip.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25054177</guid>
            <pubDate>Wed, 11 Nov 2020 00:41:43 GMT</pubDate>
        </item>
    </channel>
</rss>
