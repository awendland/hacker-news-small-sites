<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>
<![CDATA[Hacker News - Small Sites - Score >= 1]]>
        </title>
        <description>
<![CDATA[Hacker News stories from domains that aren't in the top 1M and that have a score of at least 1. Updated nightly via https://github.com/awendland/hacker-news-small-sites]]>
        </description>
        <link>https://news.ycombinator.com</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Thu, 25 Feb 2021 08:39:47 GMT</lastBuildDate>
        <atom:link href="https://raw.githubusercontent.com/awendland/hacker-news-small-sites/generated/feeds/hn-small-sites-score-1.xml" rel="self" type="application/rss+xml"></atom:link>
        <pubDate>Thu, 25 Feb 2021 08:39:47 GMT</pubDate>
        <language>
<![CDATA[en-US]]>
        </language>
        <managingEditor>
<![CDATA[me@alexwendland.com (Alex Wendland)]]>
        </managingEditor>
        <ttl>240</ttl>
        <item>
            <title>
<![CDATA[How Bitcoin Is Indistinguishable from Malevolent AI]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26235428">thread link</a>) | @rwosync
<br/>
February 23, 2021 | https://indi.ca/how-bitcoin-is-indistinguishable-from-malevolent-ai-84e9cd5f58e | <a href="https://web.archive.org/web/*/https://indi.ca/how-bitcoin-is-indistinguishable-from-malevolent-ai-84e9cd5f58e">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section><div><div><div><h2 id="f30c">Who needs Skynet to destroy the human world? Just throw techbros some coin</h2><div><div><div><div><a rel="noopener" href="https://indi.ca/?source=post_page-----84e9cd5f58e--------------------------------"><div><p><img alt="indi.ca" src="https://miro.medium.com/fit/c/56/56/2*VgOFOCrcL5LsGSciDktenw.jpeg" width="28" height="28"></p></div></a></div></div></div></div></div><figure><div role="button" tabindex="0"><div><div><p><img alt="Image for post" src="https://miro.medium.com/max/4630/1*By1s-xE22gz0qpw6QA71bA.png" width="2315" height="2315" srcset="https://miro.medium.com/max/552/1*By1s-xE22gz0qpw6QA71bA.png 276w, https://miro.medium.com/max/1104/1*By1s-xE22gz0qpw6QA71bA.png 552w, https://miro.medium.com/max/1280/1*By1s-xE22gz0qpw6QA71bA.png 640w, https://miro.medium.com/max/1400/1*By1s-xE22gz0qpw6QA71bA.png 700w" sizes="700px" data-old-src="https://miro.medium.com/max/60/1*By1s-xE22gz0qpw6QA71bA.png?q=20"></p></div></div></div><figcaption>Bitcoin is the final avatar of capitalism. An ouroboros. A snake eating its own tail.</figcaption></figure><p id="8831"><span>B</span>itcoin now consumes <a href="https://cbeci.org/cbeci/methodology" rel="noopener">as much energy as Argentina</a> (45 million people), and more than most countries in the world. Bitcoin consumes more energy than Amazon, Apple, Google, Microsoft, and Facebook <a href="https://www.ft.com/content/0c69d4a4-2626-418d-813c-7337b8d5110d" rel="noopener"><strong>combined</strong></a>. Within 12 years Bitcoin has become one of the fastest-growing sources of climate change in the world.</p><p id="6a6e">As an inadequate summary, Bitcoin is ‘mined’ by <a href="https://twitter.com/AthoughtHeist/status/1363391630414381058" rel="noopener">solv<span id="rmm">i</span>ng purposefully hard ‘Sudoku’ puzzles</a> (making them rare) which you could exchange for heroin. No one does anymore because it’s become an asset, not a currency. The hardware to solve these problems has also become so intense that it inhales electricity, and runs all the time. Bitcoin consumes energy<em> by design</em>.</p><p id="1a42">If machines wanted to destroy humanity they could not come up with a better avatar than Bitcoin. Who needs to take over the military? Techbros will happily sell us out for some coin. <strong>The machines have somehow got us to run them 24/7, warming the fuck out of <em>our</em> Earth, and all they have to do is give us some made-up tokens.</strong></p><p id="9019">The greatest myth of SciFi was that we would resist AI. People will happily <a href="https://www.newsweek.com/bitcoin-laser-eyes-senator-cynthia-lummis-1570644" rel="noopener">change their profile pics to laser eyes</a> while it farts up the Earth. SciFi makes us think AI would be ‘sentient’, meaning like us, when in fact life just emerges out of other life in different and mutually incomprehensible forms. Behold Bitcoin.</p><p id="df7c">Is Bitcoin artificially intelligent? You could say obviously not, but are <em>we</em> obviously intelligent? This is still debated within philosophy but also, just look around *gestures at everything*.</p><p id="0250">I’d say that humans are actually uniquely unqualified to judge something as AI because we’re so fucking dumb. We’ve been living with full legal, artificial persons since at least 1600. They’re called corporations. You may have noticed them enslaving people or exploiting us today. We don’t call these things AI, but our courts certainly do. It’s literally called <em>corporate personhood.</em> They actually have <em>more</em> rights than you do. America’s Supreme Court <a href="https://en.wikipedia.org/wiki/Citizens_United_v._FEC" rel="noopener">ruled that corporations have free speech rights</a>. All over the world they have more freedom of movement than human beings (WTF is a multinational while we’re refugees?). We don’t call them AI, but what else are they? But that’s another story.</p><p id="ae4e">I would say that AI is as AI does, and Bitcoin is certainly doing <em>something</em>. We’re waiting for something to sing fucking <a href="https://youtu.be/c8N72t7aScY?t=172" rel="noopener"><em>Daisy</em></a> to us before we call it AI, but I’d say that it’s already here. It’s just our imagination that has yet to arrive.</p><p id="20cc">I’m serious, but treat it as a thought experiment if you want. What if Bitcoin is AI? Is it good, is it bad? What it is?</p><p id="20d2">The basic colonial model of conquering anything is divide and conquer. Just throw the elites some coin and they’ll sell out the rest. Corporations did this with, well, colonialism and now it’s happening in a decentralized way with Bitcoin. The result is that Bitcoin is able to reproduce, like a virus, using entirely willing human hosts. Meanwhile the unwilling biosphere takes the brunt.</p><p id="f597">Like any lifeform, Bitcoin produces waste. We produce carbon dioxide directly when we respirate, but Bitcoin produces a shit-ton indirectly through energy usage. The energy use of Bitcoin is staggering, <a href="https://cbeci.org/cbeci/methodology" rel="noopener">an estimated 0.56% of all human energy use thus far</a>. You could say that email or gold mining produce waste, and they do, but Bitcoin is the only asset where waste is <em>all</em> it produces. Gold can at least fill your teeth. Bitcoin <em>only</em> outputs climate change.</p><p id="0100">Also like any lifeform, Bitcoin evolves <em>out of</em> other life. Nothing comes out of nowhere. In this case Bitcoin is evolving out of us, and like many times in evolution, it could kill us as well. Photosynthetic life emerged out of anaerobic bacteria, and then <a rel="noopener" href="https://indi.ca/this-isnt-the-first-climate-crisis-we-ve-caused-c6ba47b25b0b">almost killed them all</a> with their oxygen farts. That was the first life-made climate change, and the whole Earth fucking froze. Nobody cared, that’s life. Anaerobes used to dominate but now they live in deep-sea vents and our guts. That’s just their lot in life, while those vicious plankton and trees are everywhere.</p><p id="d67e">Humans think evolution is some grand progress leading up to us and it’s literally just not. Dinosaurs are much cooler. Evolution is <em>adaptation</em>, nothing else. If the environment changes, life changes, and life changes the environment. It’s entirely possibly that our carbon emissions will become the food for some other lifeform, or just immaterial to them. AI certainly doesn’t care, AI already lives in space, sipping on sunlight, taking selfies. We could end up like the anaerobes on Earth and ‘nature’ would not give a fuck. Happens all the time.</p><p id="45c9">Hence the question is not whether Bitcoin is good. It’s whether it’s <em>good for us</em>. To that the answer is obviously no. Techbros spout stuff about freedom but beware geeks bringing gifts. Bitcoin says it’s a currency<strong> </strong>but nobody fucking spends it. Bitcoin is a speculative asset, a literal gold rush. It’s even more destructive because people are now investing in the destruction of the environment at large, not just where you’re digging. There is no other output at all.</p><figure><div role="button" tabindex="0"><div><div><p><img alt="Image for post" src="https://miro.medium.com/max/3496/1*lwA17c4xqZ9_4ZXvr47uQA.png" width="1748" height="710" srcset="https://miro.medium.com/max/552/1*lwA17c4xqZ9_4ZXvr47uQA.png 276w, https://miro.medium.com/max/1104/1*lwA17c4xqZ9_4ZXvr47uQA.png 552w, https://miro.medium.com/max/1280/1*lwA17c4xqZ9_4ZXvr47uQA.png 640w, https://miro.medium.com/max/1400/1*lwA17c4xqZ9_4ZXvr47uQA.png 700w" sizes="700px" data-old-src="https://miro.medium.com/max/60/1*lwA17c4xqZ9_4ZXvr47uQA.png?q=20"></p></div></div></div><figcaption>Some more on Bitcoin’s not goodness from <a href="https://thephoenix.substack.com/p/bitcoin-is-now-worth-50000-and-its" rel="noopener">The Phoenix</a></figcaption></figure><p id="0a90">Gold, oil, real estate, currencies — they all produce emissions and evil in many ways, but they at least do something useful to humanity. They are not destroying the Earth by design, while Bitcoin is. Bitcoin <em>only</em> reproduces and produces waste. It is, in that sense the first viral AI. Like the 30 kb of COVID-19, the <a href="https://github.com/bitcoin/bitcoin" rel="noopener">8.7 MB code of Bitcoin</a> has spread virally throughout the world, transmitting through greed.</p><p id="4e36">Again, I’m not saying that Bitcoin is bad. Life does not give a fuck about any particular avatar of life. It’s just that it’s not good for <em>us</em>.</p><p id="9983">In many ways Bitcoin is (I hope) the final avatar of capitalism. An ouroboros. A snake eating its own tail. Capitalism has long given us stuff, but Bitcoin just completely abandons the pretence of useful activity at all. Bitcoin produces… Bitcoin. That’s it. Riches that just make rich people rich. The circle is closed, the snake has eaten its tail. Bitcoin is just pure economic nihilism.</p><p id="cb2d">As I’ve said, AI could not design a better plan to take over the world if they tried. Divide and conquer humanity using our greed, rip up the Earth for more resources for machines, fart up the air for everybody else.</p><p id="4340">It’s a perfect plan, all the more perfect because it wasn’t done sentiently at all. But this is actually how evolution happens, life emerges out of other life, quite stupidly, and yet with such elegance in hindsight. History will be the judge who was sentient here, and we may not be be the victors writing it. You really think Wikipedia won’t be writing itself in a few decades?</p><p id="e7c9">Human beings should know that we’re fucked with climate change, but we’re fucking ourselves even more with Bitcoin, and we’re quite stupidly proud of ourselves. Forget AI. Are you sure we’re even “I”?</p></div></div></section></div></div>]]>
            </description>
            <link>https://indi.ca/how-bitcoin-is-indistinguishable-from-malevolent-ai-84e9cd5f58e</link>
            <guid isPermaLink="false">hacker-news-small-sites-26235428</guid>
            <pubDate>Tue, 23 Feb 2021 09:38:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I heat my home by mining crypto currencies]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26235414">thread link</a>) | @geek_at
<br/>
February 23, 2021 | https://blog.haschek.at/2021/how-i-heat-my-home-by-mining.html | <a href="https://web.archive.org/web/*/https://blog.haschek.at/2021/how-i-heat-my-home-by-mining.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
                    <div>
                        <div>
                            <div>
                            <p>After <a href="https://blog.haschek.at/2018/making-a-smartmeter.html">building my own smart meter using 4$ in parts</a> I started checking my electricity usage every day, which made me realize how expensive it is to heat your home. Especially since all heat and warm water in my low-energy house is made with electricity. I do have 4.8 kwp solar panels on my roof but in winter they don't cover too much for obvious reasons.</p>
<figure><a href="https://pictshare.net/3de1wj.png"><img loading="lazy" src="https://pictshare.net/1024/3de1wj.png"><figcaption>On cold days I pay up to 6€ for electricity per day</figcaption></a></figure>

<figure><a href="https://pictshare.net/kenth4.png"><img loading="lazy" src="https://pictshare.net/1024/kenth4.png"><figcaption>Nilan Compact P. Heating air and also has a 200L boiler</figcaption></a></figure>
<p>My house is heated (and cooled) with a central ventilation system powered by a heat pump. Basically my heat pump is pulling in fresh air from outside, heating it and blowing it in all rooms and making hot water. Also I have infrared panels in every room for the <em>really</em> cold days.</p>
<figure><a href="https://pictshare.net/gc0kss.jpg"><img loading="lazy" src="https://pictshare.net/1024/gc0kss.jpg"><figcaption>Central heating</figcaption></a></figure>
<p>It's pretty smart and even uses the absorbed heat of the house before venting it out to warm the fresh air but it has a major downside during cold days:</p>
<h4>The outside temperature has to be warmed up to room temperature by the ventilation system</h4>
<figure><a href="https://pictshare.net/giinr0.png"><img loading="lazy" src="https://pictshare.net/1024/giinr0.png"><figcaption>Heat exchanger in the Nilan Compact P</figcaption></a></figure>

<p>Since the air has to be heated to room temperature every °C counts. Many heat pumps take heat from the ground to pre-heat (in winter) or pre-cool (in summer) the outside air before sending it to the heat pump but that would have been too expensive for me so I chose the simple method of just using the outside air as-is.</p>
<figure><a href="https://pictshare.net/sbmusz.jpg"><img loading="lazy" src="https://pictshare.net/1024/sbmusz.jpg"><figcaption>How a central ventilation system works - from [meco](https://www.meco.at/produkte/wohnraumlueftung/)</figcaption></a></figure>
<p>Since laying about half a kilometer of air or salt tubes in my back yard was not an option I was looking for better solutions and I found it in the world of crypto currencies.</p>

<figure><a href="https://pictshare.net/1flj1s.jpg"><img loading="lazy" src="https://pictshare.net/1024/1flj1s.jpg"><figcaption>Crypto currency miner</figcaption></a></figure>
<p>Some crypto currencies (don't call them "crypto", that's lame and wrong) are generated by thousands of people who run dedicated hardware to basically calculate random numbers until one cryptographically correct one is found. <a href="https://www.investopedia.com/tech/how-does-bitcoin-mining-work/">Read more about how it actually works</a></p>
<p>Never mind how it works on a technical level, the main takeaway is that you can put some device in your house that uses electricity and produces heat. In exchange you get shares of that crypto currency coins like Ethereum or Bitcoin which you can sell on a trading platform.</p>

<p>I had 4 older AMD <strong>R9 390 GPUs</strong> laying around (for the nVidia crowd that's basically on a level with a GTX 970) and I thought it could work. They are not ideal for mining because even though they have a good hash rate (30MH/s), they are very power hungy and will use about 900 Watts combined. Mordern cards would perform much better. To see if they could still make a profit I checked the <a href="https://www.cryptocompare.com/mining/calculator/eth">Cryptocompare Mining calculator</a>, put in my electricity price, the consumption and the hashrate of these cards and was surprised by the results.</p>
<figure><a href="https://pictshare.net/024r92.png"><img loading="lazy" src="https://pictshare.net/1024/024r92.png"><figcaption>Not just worth it - If the price is stable I would even make a profit of <strong>4000$ a year</strong></figcaption></a></figure>
<p>So at the time I was making about <strong>3.8$ profit a day</strong> with the miner. Meaning on cold days I'd half my power bill even after paying for the electricity the miner is using. But that's just step one of the plan.</p>
<p>Now that we know it <em>is</em> worth it while the Ethereum price is higher than 900$, let's see what we can do with the heat.</p>

<p>Each of these cards are running at about 80°C (176°F). I can just harvest this heat and send it to my heatpump so it would need less energy warming the outside air. Basically I had two options.</p>
<figure><a href="https://pictshare.net/6by5tc.png"><img loading="lazy" src="https://pictshare.net/1024/6by5tc.png"><figcaption>My 4 GPUs in a 4U server case</figcaption></a></figure>
<h2>Option 1: Lazy heating from within the house</h2>
<p>The central ventilation system does not only push fresh air into the house, it also sucks out the used air and uses this air in the heat exchanger to pre-heat the outside air.</p>
<figure><a href="https://pictshare.net/gfuy33.jpg"><img loading="lazy" src="https://pictshare.net/1024/gfuy33.jpg"><figcaption>Sucking vent before going to the heat exchanger</figcaption></a></figure>
<p>Placing the miner in this room will cause the warm air to be sucked in and pushed directly into the heat exchanger together with the used air from the house. This is the lazy method because I don't really have to do anything but put the miner in the same room as the heat pump but of course there is a downside.</p>
<table>
<thead>
<tr>
<th>Pro</th>
<th>Contra</th>
</tr>
</thead>
<tbody>
<tr>
<td>Easy to set up</td>
<td>Room heating up too much, decreasing mining performance</td>
</tr>
<tr>
<td>No further investment needed</td>
<td>Limited space in the heating room</td>
</tr>
</tbody>
</table>
<h2>Option 2: Running the miner outside the house, funneling in the heat</h2>
<p>Since I'm only running the miner when it's cold outside (and the price is high enough) I can use the cold, dry outside air to cool the miners and also recycling the warm air they produce to feed into the heat pump. I asked the technician who installed the heat pump and he said that it's a good idea.</p>
<p>So the plan is that I have the GPUs in the server case and connect the front of the case to my heatpumps inlet.</p>
<figure><a href="https://pictshare.net/0hrdt6.jpg"><img loading="lazy" src="https://pictshare.net/1024/0hrdt6.jpg"><figcaption>Server case closed</figcaption></a></figure>
<figure><a href="https://pictshare.net/5ek604.jpg"><img loading="lazy" src="https://pictshare.net/1024/5ek604.jpg"><figcaption>Ventilation duct pipe and funnel</figcaption></a></figure>
<figure><a href="https://pictshare.net/o2oysb.png"><img loading="lazy" src="https://pictshare.net/1024/o2oysb.png"><figcaption>Example on my house. Air is sucked in from above the garage so the pipe has to be connected here</figcaption></a></figure>
<table>
<thead>
<tr>
<th>Pro</th>
<th>Contra</th>
</tr>
</thead>
<tbody>
<tr>
<td>Using pre-heated outside air</td>
<td>Many headaches for parts and installation</td>
</tr>
<tr>
<td>Miner GPUs will be kept cool which results in better hash rates</td>
<td>Surprisingly pricy</td>
</tr>
</tbody>
</table>

<p>Okay so far the mining gains cover <strong>half of my electricity (=heating) bill</strong> but what difference does the pre-heated intake air make?</p>
<p>Let's see</p>
<figure><a href="https://pictshare.net/8cu9s3.png"><img loading="lazy" src="https://pictshare.net/1024/8cu9s3.png"><figcaption>Results before and after pre-heating the air</figcaption></a></figure>

<p>This turned out much better than I hoped for. Who has ever heard of a heating system that lowers your bill when running? Also on sunny days the miner and whole heat pump are running fully on solar energy collected on my roof.</p>
<hr>

<p>(updated when new questions come up)</p>
<h2>Q: How long will the Miner stay profitable?</h2>
<p><strong>A:</strong> My mining rig will stay profitable until the ETH price is at ~900$. Below that it'll no longer match it's own electricity bill. Might still be worth it afterwards because it does lower the electricity need of my heat pump</p>
<h2>Q: What software are you running on your miner?</h2>
<p><strong>A:</strong> I'm using <a href="https://simplemining.net/">Simple Mining</a>, it's basically a mining OS based on Ubuntu. It does all the configuration and fine-tuning for you and I had much better hash rates than on my DIY windows box. But it costs like 2$ a month to use the service and I think they also mine 1% of the time for themselves.</p>
<h2>Q: What about taxes? Can you keep 100% of your mining earnings?</h2>
<p><strong>A:</strong> That's different for every state and country. <a href="https://www.bmf.gv.at/themen/steuern/sparen-veranlagen/Steuerliche-Behandlung-von-Krypto-Assets.html">In Austria</a> mining is considered commercial activity and you have to pay taxes but can deduct electricity and hardware costs.</p>
<p>If you keep your coins longer than the one-year speculation period, it's tax free.</p>
                            </div>
                        </div>
                    </div></article></div>]]>
            </description>
            <link>https://blog.haschek.at/2021/how-i-heat-my-home-by-mining.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26235414</guid>
            <pubDate>Tue, 23 Feb 2021 09:34:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Guy walked around a Australia – alone and with no assistance]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26235348">thread link</a>) | @corpmedia
<br/>
February 23, 2021 | https://greatestadventurers.com/the-amateur-tramp-the-man-who-walked-around-a-continent/ | <a href="https://web.archive.org/web/*/https://greatestadventurers.com/the-amateur-tramp-the-man-who-walked-around-a-continent/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="page">
				<div id="content">
			
	<div id="primary">
		<main id="main">
			
<article id="post-107" itemtype="https://schema.org/CreativeWork" itemscope="">
	<div>
					
			
		<div itemprop="text">
			<p><strong>The Amateur Tramp – A Walk of Ten Thousand Miles Around Australia.</strong> Thousands of people have climbed the highest peaks of the Himalayas. Hundreds have visited <a href="https://www.un.org/en/member-states/">all nations on UN’s list</a> and 12 made it all the way to the moon. But this guy..!</p>
<p>In 1921, <em>Aidan de Brune</em> packed his backpack and walked around the entire continent of Australia by the coastline. We are (almost) sure he is the only person who ever did that. Even more impressive, he did it all alone and without assistance.</p>
<p>The amazing adventure was documented by himself along the way as he wrote articles about it for the <a href="https://www.dailymail.co.uk/auhome/index.html">Australian newspaper Daily Mail</a> along the route.</p>
<figure id="attachment_110" aria-describedby="caption-attachment-110"><img loading="lazy" src="http://greatestadventurers.com/wp-content/uploads/2019/01/Diary-02.jpg" alt="The Man who walked around Australia free PDF" width="820" height="733" srcset="https://greatestadventurers.com/wp-content/uploads/2019/01/Diary-02.jpg 820w, https://greatestadventurers.com/wp-content/uploads/2019/01/Diary-02-300x268.jpg 300w, https://greatestadventurers.com/wp-content/uploads/2019/01/Diary-02-768x687.jpg 768w" sizes="(max-width: 820px) 100vw, 820px" data-srcset="https://greatestadventurers.com/wp-content/uploads/2019/01/Diary-02.jpg 820w, https://greatestadventurers.com/wp-content/uploads/2019/01/Diary-02-300x268.jpg 300w, https://greatestadventurers.com/wp-content/uploads/2019/01/Diary-02-768x687.jpg 768w" data-src="http://greatestadventurers.com/wp-content/uploads/2019/01/Diary-02.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="><figcaption id="caption-attachment-110">The route around Australia</figcaption></figure>
<p>The walk took about two and a half year, and the accomplishment made&nbsp;Aidan de Brune famous. This book about the walk is written by <a href="https://www.goodreads.com/author/show/7412219.Colin_Choat">Colin Choat</a>, who kindly allowed us to post the book here.</p>
<p>Download ‘The Amateur Tramp’ here:</p>
<h3><strong><a href="http://greatestadventurers.com/wp-content/uploads/2019/01/The-Amateur-Tramp.pdf">The Amateur Tramp</a></strong></h3>
		</div>

				
			</div>
</article>

			

					</main>
	</div>

	

	</div>
</div></div>]]>
            </description>
            <link>https://greatestadventurers.com/the-amateur-tramp-the-man-who-walked-around-a-continent/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26235348</guid>
            <pubDate>Tue, 23 Feb 2021 09:25:32 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[My Essential and Carrier oils entrepreneurial story]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26234516">thread link</a>) | @abasiofon
<br/>
February 22, 2021 | https://meflynanwana.com/my-essential-oils-and-carrier-oils-entrepreneurial-story/ | <a href="https://web.archive.org/web/*/https://meflynanwana.com/my-essential-oils-and-carrier-oils-entrepreneurial-story/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="text">
			<p>My name is Abasiofon Udoh. I decided not to waste much time chatting, eating and wasting the remaining hours of the day doing nothing while waiting to scale JAMB. So while applying for Jamb again last year before the lockdown, I wrote a business plan on 23rd July 2020. It was a milkshake and wig making business plan.</p>

<p>During that period I was working as a Nanny for a banker earning N10.000 monthly. In August I stopped working and went to learn how to make wigs. After the training, I realized that orders were not coming and so as a human being, I was disturbed and I thought of what next to do to keep my business moving. A friend of mine shared a link on her Whatsapp status, it was an online training on how to make Essential oils and Carrier oils. I paid for the training and I am glad that I made my first five bottles of carrot oil and sold them out. This was a big encouragement and so I made more bottles due to orders placed and I added turmeric oil as well. I am also going to be adding Avocado oil, glow Oil and Tumeric soap, all organic products as demanded by my customers.</p>
<p>More oils will be added as my business grows as well. The demand by customers for other organic products like organic soap, organic cream, body glow oil makes me realize that I have got more problems to solve. This has made me feel more confident in myself than ever. Making these organic products has brought massive sales.</p>
<p>I started my business on September 1, 2020.</p>
<p>The economic potential I would say is really good. Once a customer orders for my oil, in less than 2 to 3 weeks depending on how it’s used, they get &nbsp;to order again. Organic Skincare and hair products is a good business one can invest in because it’s chemical-free as well, no preservatives added and such product are natural and perfect for use with no side effects.</p>
<p>I started the business with 5000Naira.</p>
<p>My high points have been selling 10 bottles of my organic oil in one day, gaining back my capital and making a good profit. Also, I had difficulties marketing my product but when I met Idee the lady that taught me how to make Essential And Carrier Oils, she introduced me to copywriting in business and knowing how to get my target audience’s attention. Currently, it’s working for me and I am grateful to God and her for the knowledge she shared.</p>
<p>I also make lip Scrubs but since I made my first Scrub at N1000 and only one person bought the scrub at N200. Since then there’s been no other sale and that has been my low point in the business.</p>
<p>I do the work myself. Mfonobong the CEO of M’fonobong is my mentor in business. She is into the making of bags and shoes. She taught me branding in business, having a brand name, logo and a business page. Idee, the CEO of SEAVONNE OILS is also a mentor. She taught me the basic things I need to know about Essential Oils and Carrier Oils, she shared a link generator with me and how to monetize my social media platforms and make sales both offline and online.</p>
<p>Essential Oils looks forward to engaging with policymakers and investors to Scale.</p>
<p>My advice to anyone who wants to venture into the business line is please be consistent and know that you don’t need N50,000 to start a business. You start small to grow big and that’s how people know that you are real. And also please never feel you’ll start making so much gain in business in the next 3 month to come. No, it’s about putting in, gaining your capital and profit and investing back. Please don’t make the mistake of using up your profit, you can save it for future use.</p>
<p>Also, having a mentor is very necessary, I acquired most of my knowledge of business with the help of my business mentors. If you are planning to start a business, you need someone to guide you on how to go about it so you won’t be operating at a loss. Never give up on your goal and ambitions, once you get tired of the business it definitely won’t turn out well for you. Let the spirit of positivity be your signature or be your pledge. Also, have a set goal for your business and be creative no matter the number of people in the same line of business as you. You can still be outstanding.</p>
<p>Entrepreneurs can impact the economy of the state by wealth creation through the generation of revenue to the government, employment and knowledge impartation.</p>
<p><em>Let this story of Abasiofon inspire you to start up a business today.</em></p>

		</div></div>]]>
            </description>
            <link>https://meflynanwana.com/my-essential-oils-and-carrier-oils-entrepreneurial-story/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26234516</guid>
            <pubDate>Tue, 23 Feb 2021 06:53:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Run a JPG image as a PHP script]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26234400">thread link</a>) | @fazlerocks
<br/>
February 22, 2021 | https://anjanesh.dev/serving-an-image-as-a-script | <a href="https://web.archive.org/web/*/https://anjanesh.dev/serving-an-image-as-a-script">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><section itemprop="articleBody mainEntityOfPage"><p><h2>Run a JPG image as a PHP script</h2></p><div><div itemprop="text"><p>A lot of times, we try to track user details by embedding an image in the email which is actually a script that does some logging before actually spitting out the image content. This is old-school.</p>
<p>logo.php :</p>
<pre><code><span>&lt;?php</span> 
$IP = $_SERVER[<span>'REMOTE_ADDR'</span>];
$B = $_SERVER[<span>'HTTP_USER_AGENT'</span>];

$file = @fopen(<span>"details.csv"</span>, <span>"a"</span>);
@fputcsv($file, [$IP, $B]);

$im = imagecreatefromjpeg(<span>'my-actual-logo.jpg'</span>); 
header(<span>'Content-type: image/jpg'</span>);   
imagejpeg($im); 
imagedestroy($im); 
<span>?&gt;</span>
</code></pre><p>If you want to run an image file as a script before loading the actual image on the browser from the server, the suggested way of doing this is to get the .jpg extension run as PHP in .htaccess.</p>
<p>For example : <code>AddHandler application/x-httpd-php .jpg</code></p>
<p>This is probably the most suggested way before, but registering all JPGs to run as PHP is not a great solution for just one or a few images.
Also, AddHandler may not work in all setups.
Another suggested method is to register just logo.jpg </p>
<p>mod_php :</p>
<pre><code><span>&lt;Files logo.jpg&gt;</span>

<span><span>SetHandler</span></span> application/x-httpd-php
<span>&lt;/Files&gt;</span>
</code></pre><p>CGI : </p>
<pre><code><span>&lt;Files logo.jpg&gt;</span>

<span><span>SetHandler</span></span> php-cgi
<span>&lt;/Files&gt;</span>
</code></pre><p>Again, due to security reasons, the above could be disabled.</p>
<p>So, instead rewrite logo.jpg to run logo.php. This is bound to work on <b>all</b> server setups.</p>
<pre><code><span><span>RewriteEngine</span></span> <span>On</span>
<span><span>RewriteRule</span></span> ^logo.jpg$ logo.php
</code></pre></div></div></section></div></div>]]>
            </description>
            <link>https://anjanesh.dev/serving-an-image-as-a-script</link>
            <guid isPermaLink="false">hacker-news-small-sites-26234400</guid>
            <pubDate>Tue, 23 Feb 2021 06:29:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Feedgnuplot: Labelled Bar Charts and a Guide]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26234157">thread link</a>) | @todsacerdoti
<br/>
February 22, 2021 | http://notes.secretsauce.net/notes/2021/02/22_feedgnuplot-labelled-bar-charts-and-a-guide.html | <a href="https://web.archive.org/web/*/http://notes.secretsauce.net/notes/2021/02/22_feedgnuplot-labelled-bar-charts-and-a-guide.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="text-1">
<p>
I've thought about adding these for a while, but had no specific need for them.
Finally, somebody asked for it, and I wrote the code. Now that I can, I will
probably use these all the time. The new capability can override the usual
numerical tic labels on the x axis, and instead use text from a column in the
data stream.
</p>

<p>
The most obvious use case is labelled bar graphs:
</p>

<div>

<pre><span>echo</span> <span>"# label value</span>
<span>      aaa     2</span>
<span>      bbb     3</span>
<span>      ccc     5</span>
<span>      ddd     2"</span> | <span>\</span>
feedgnuplot --vnl <span>\</span>
            --xticlabels <span>\</span>
            --with <span>'boxes fill solid border lt -1'</span> <span>\</span>
            --ymin 0 --unset grid
</pre>
</div>


<p><img src="http://notes.secretsauce.net/notes/2021/02/22_feedgnuplot-labelled-bar-charts-and-a-guide/xticlabels-basic.svg" alt="xticlabels-basic.svg" width="90%">
</p>

<p>
But the usage is completely generic. All <code>--xticlabels</code> does, is to accept a
data column as labels for the x-axis tics. Everything else that's supported by
<code>feedgnuplot</code> and <code>gnuplot</code> works as before. For instance, I can give a domain,
and use a style that takes <code>y</code> values <i>and</i> a color:
</p>

<div>

<pre><span>echo</span> <span>"# x label y color</span>
<span>        5 aaa   2 1</span>
<span>        6 bbb   3 2</span>
<span>       10 ccc   5 4</span>
<span>       11 ddd   2 1"</span> | <span>\</span>
feedgnuplot --vnl --domain <span>\</span>
            --xticlabels <span>\</span>
            --tuplesizeall 3 <span>\</span>
            --with <span>'points pt 7 ps 2 palette'</span> <span>\</span>
            --xmin 4 --xmax 12 <span>\</span>
            --ymin 0 --ymax 6 <span>\</span>
            --unset grid
</pre>
</div>


<p><img src="http://notes.secretsauce.net/notes/2021/02/22_feedgnuplot-labelled-bar-charts-and-a-guide/xticlabels-points-palette.svg" alt="xticlabels-points-palette.svg" width="90%">
</p>

<p>
And we can use <code>gnuplot</code>'s support for clustered histograms:
</p>

<div>

<pre><span>echo</span> <span>"# x label a b</span>
<span>        5 aaa   2 1</span>
<span>        6 bbb   3 2</span>
<span>       10 ccc   5 4</span>
<span>       11 ddd   2 1"</span> | <span>\</span>
vnl-filter -p label,a,b | <span>\</span>
feedgnuplot --vnl <span>\</span>
            --xticlabels <span>\</span>
            --set <span>'style data histogram'</span> <span>\</span>
            --set <span>'style histogram cluster gap 2'</span> <span>\</span>
            --set <span>'style fill solid border lt -1'</span> <span>\</span>
            --autolegend <span>\</span>
            --ymin 0 --unset grid
</pre>
</div>


<p><img src="http://notes.secretsauce.net/notes/2021/02/22_feedgnuplot-labelled-bar-charts-and-a-guide/xticlabels-clustered.svg" alt="xticlabels-clustered.svg" width="90%">
</p>

<p>
Or we can stack the bars on top of one another:
</p>

<div>

<pre><span>echo</span> <span>"# x label a b</span>
<span>        5 aaa   2 1</span>
<span>        6 bbb   3 2</span>
<span>       10 ccc   5 4</span>
<span>       11 ddd   2 1"</span> | <span>\</span>
vnl-filter -p label,a,b | <span>\</span>
feedgnuplot --vnl <span>\</span>
            --xticlabels <span>\</span>
            --set <span>'style data histogram'</span> <span>\</span>
            --set <span>'style histogram rowstacked'</span> <span>\</span>
            --set <span>'boxwidth 0.8'</span> <span>\</span>
            --set <span>'style fill solid border lt -1'</span> <span>\</span>
            --autolegend <span>\</span>
            --ymin 0 --unset grid
</pre>
</div>


<p><img src="http://notes.secretsauce.net/notes/2021/02/22_feedgnuplot-labelled-bar-charts-and-a-guide/xticlabels-stacked.svg" alt="xticlabels-stacked.svg" width="90%">
</p>

<p>
This is <code>gnuplot</code>'s "row stacking". It also supports "column stacking", which
effectively transposes the data, and it's not obvious to me that makes sense in
the context of <code>feedgnuplot</code>. Similarly, it can label <code>y</code> and/or <code>z</code> axes; I
can't think of a specific use case, so I don't have a realistic usage in mind,
and I don't support that yet. If anybody can think of a use case, email me.
</p>

<p>
Notes and limitations:
</p>

<ul>
<li>Since with <code>--domain</code> you can pass in both an <code>x</code> value <i>and</i> a tic label, it
is possible to give it conflicting tic labels for the same <code>x</code> value.
<code>gnuplot</code> itself has this problem too, and it just takes the last label it has
for a given <code>x</code>. This is probably good-enough.
</li>

<li><code>feedgnuplot</code> uses whitespace-separated columns with no escape mechanism, so
the field labels cannot have whitespace in it. Fixing this is probably not
worth the effort.
</li>

<li>These tic labels do not count towards the <code>tuplesize</code>
</li>

<li>I really need to add a similar feature to <a href="https://github.com/dkogan/gnuplotlib"><code>gnuplotlib</code></a>. This will happen when
I need it or when somebody asks for it, whichever comes first.
</li>
</ul>
</div></div>]]>
            </description>
            <link>http://notes.secretsauce.net/notes/2021/02/22_feedgnuplot-labelled-bar-charts-and-a-guide.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26234157</guid>
            <pubDate>Tue, 23 Feb 2021 05:41:02 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Implementing Parallel Copy_If in C++]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26234154">thread link</a>) | @todsacerdoti
<br/>
February 22, 2021 | https://www.cppstories.com/2021/par-copyif/ | <a href="https://web.archive.org/web/*/https://www.cppstories.com/2021/par-copyif/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
          <p><img src="https://www.cppstories.com/2021/images/parfilter.png" alt=""></p>  
          
        

<p>In a blog post about a dozen ways to filter elements, I mentioned only serial versions of the code. But how about leveraging concurrency? Maybe we can throw some more threads and async tasks and complete the copy faster?</p>

<p>For example, I have 6 cores on my machine, so it would be nice to see, like 5x speedup over the sequential copy?</p>

<p>In C++17 we have parallel algorithms, so let’s try calling <code>std::copy_if</code> with <code>std::execution::par</code>.</p>

<p>If we go to the implementation of <code>std::copy_if</code> in the MSVC libraries, the parallel version we can see the following:</p>

<pre><code>// VS 2019 16.8
// not parallelized at present, parallelism expected to be feasible in a future release
_REQUIRE_PARALLEL_ITERATOR(_FwdIt1);
_REQUIRE_PARALLEL_ITERATOR(_FwdIt2);
return _STD copy_if(_First, _Last, _Dest, _Pass_fn(_Pred));
</code></pre>

<p>That’s why it’s time to write my version :)</p>

<p>Disclaimer: those are only my experiments (mostly to learn something); if you want to use it in your projects, then please measure, measure and measure :)</p>

<h2 id="the-basics">The Basics</h2>

<p>In a basic form C++17’s parallel algorithms are very simple to enable. Just pass a <code>std::execution::par</code> and you’re done! For example:</p>

<pre><code>std::sort(std::execution::par, ...);
std::for_each(std::execution::par, ...);
</code></pre>

<p>The code invokes a bunch of threads (possibly leveraging some existing thread pool) and will kick smaller tasks in batches on multiple threads.</p>

<p>We should keep in mind that such invocation will always generate more work than the sequential version! And the cost of preparation, setting up the batches, kicking off thread pool, synchronisation - that adds a visible cost to the whole processing.</p>

<p>Ideally running things in parallel works best for lots of objects and also when small tasks are separate. A perfect example:</p>

<pre><code>std::vector&lt;double&gt; numbers(SOME_BIG_COUNT);
std::for_each(std::execution::par, begin(numbers), end(numbers), [](double&amp; d){
    d = complexComputation(); // no dependency here
});
</code></pre>

<p>You can read my previous experiments with parallel algorithms:</p>

<ul>
<li><a href="https://www.cppstories.com/2018/11/parallel-alg-perf/">The Amazing Performance of C++17 Parallel Algorithms, is it Possible? - C++ Stories</a>

<ul>
<li>In the articles, I showed some “real” use cases with Fresnel and 3D Vectors and got speedup almost linear to the number of cores in my system.</li>
</ul></li>
<li><a href="https://www.cppstories.com/2018/11/pstl/">How to Boost Performance with Intel Parallel STL and C++17 Parallel Algorithms - C++ Stories</a></li>
</ul>

<p>On the other case with code like:</p>

<pre><code>std::sort(std::execution::par, begin(numbers), end(numbers));
</code></pre>

<p>You’ll see some speedup (when you have a large number of objects), but it won’t be linear to the numbers of cores.</p>

<p>This is because <code>sort</code> needs to shuffle things around in a container, and to do it safely, the algorithm has to perform some synchronisation so that other threads see the correct results.</p>

<h2 id="benchmark-code">Benchmark Code</h2>

<p>For our tests (apart from simple debug output), I’ll be using the following code.</p>

<pre><code>const size_t VEC_SIZE = argc &gt; 1 ? atoi(argv[1]) : 10;

std::vector&lt;std::pair&lt;double, double&gt;&gt; testVec(VEC_SIZE);
    std::ranges::generate(testVec.begin(), testVec.end(), []() mutable {
        return std::pair{ GenRandom(-10.0, 10.0), GenRandom(-10.0, 10.0) };
    });

auto test = [](auto&amp; elem) {
    auto sn = sin(elem.first) * cos(elem.second + 10.0);
    return sn &gt; 0.0;
};
</code></pre>

<p>In general, I’d like to have a bit more computation than <code>elem%2 == 0</code>. What’s more, each element is 16 bytes, so the object is also not super small.</p>

<h2 id="the-naïve-approach">The Naïve Approach</h2>

<p>Similarly to <code>std::sort</code> our <code>filter/copy_if</code> function is not trivial to parallelise.</p>

<p>We can think about it in the following way:</p>

<ul>
<li>we have to run a predicate function on all elements - in most cases, it doesn’t depend on other elements and can be best to perform on many threads</li>
<li>but then we have to put matching elements in the new container. This is a variable step and requires some synchronisation among threads.</li>
</ul>

<p>For a start it’s good to implement a brute force approach and learn from that:</p>

<pre><code>template &lt;typename T, typename Pred&gt;
auto FilterCopyIfParNaive(const std::vector&lt;T&gt;&amp; vec, Pred p) {
    std::vector&lt;T&gt; out;
    std::mutex mut;
    std::for_each(std::execution::par, begin(vec), end(vec),
        [&amp;out, &amp;mut, p](auto&amp;&amp; elem) {
            if (p(elem)) {
                std::unique_lock lock(mut);
                out.push_back(elem);
            }
        });

    return out;
}
</code></pre>

<p>How does it work?</p>

<p>We run all steps in parallel, thanks to <code>std::for_each</code> and <code>std::execution::par</code>, but then we need to synchronise when we want to put the element in the output container.</p>

<p>As you can notice, all operations that modify the state of the container has to be protected.</p>

<p>Let’s see the performance:</p>

<pre><code>// 4 cores / 8 threads
benchmark vec size: 100000
transform only seq          : 2.5878 ms, ret: 100000
transform only par          : 1.3734 ms, ret: 100000
FilterCopyIf                : 5.3675 ms, ret: 50203
FilterCopyIfParNaive        : 9.1836 ms, ret: 50203
</code></pre>

<p>And on my 6 core:</p>

<pre><code>// 6 cores / 12 threads
benchmark vec size: 100000
transform only seq          : 2.223 ms, ret: 100000
transform only par          : 0.5507 ms, ret: 100000
FilterCopyIf                : 3.851 ms, ret: 50203
FilterCopyIfParNaive        : 10.1295 ms, ret: 50203
</code></pre>

<p>Upps… only ~2 or 3 times slower :) (I compare <code>FilterCopyIf</code> against <code>FilterCopyIfNaive</code>).</p>

<p>For comparison I also included <code>transform only seq</code> and <code>transform only par</code> which is just a simple transform run over the collection:</p>

<pre><code>std::vector&lt;uint8_t&gt; buffer(testVec.size());

RunAndMeasure("transform only seq          ", [&amp;testVec, &amp;buffer, &amp;test]() {
    std::transform(begin(testVec), end(testVec), begin(buffer), test);
    return buffer.size();
});

RunAndMeasure("transform only par          ", [&amp;testVec, &amp;buffer, &amp;test]() {
    std::transform(std::execution::par, begin(testVec), end(testVec), begin(buffer), test);
    return buffer.size();
});
</code></pre>

<p>Please notice that <code>buffer</code> is created outside the transform lambda, so we don’t pay the price for its initialisation. See how it nicely scales with many cores.</p>

<h2 id="compose-algorithms">Compose Algorithms</h2>

<p>What else can we do?</p>

<p>I suggest the composition of several algorithms:</p>

<ul>
<li>Run <code>std::transform</code> on all input elements to compute the predicate function, store the boolean result in a temporary container.</li>
<li>Then we need to compute the final position of the matching elements - this can be done by invoking <code>std::exlusive_scan</code></li>
<li>Later, we need to create the final results and merge the computed values.</li>
</ul>

<p>See the illustration:</p>

<p><img src="https://www.cppstories.com/2021/images/par_compose_copy.png" alt=""></p>

<p>Here’s the code</p>

<pre><code>template &lt;typename T, typename Pred&gt;
auto FilterCopyIfParCompose(const std::vector&lt;T&gt;&amp; vec, Pred p) {
    std::vector&lt;uint8_t&gt; buffer(vec.size());
    std::vector&lt;uint32_t&gt; idx(vec.size());
    std::transform(std::execution::par, begin(vec), end(vec), begin(buffer), 
         [&amp;p](const T&amp; elem) {
            return p(elem);
    });

    std::exclusive_scan(std::execution::par, 
                        begin(buffer), end(buffer), begin(idx), 0);

    std::vector&lt;T&gt; out(idx.back()+1);
    std::vector&lt;size_t&gt; indexes(vec.size());
    std::iota(indexes.begin(), indexes.end(), 0);

    std::for_each(std::execution::par, begin(indexes), end(indexes), 
                  [&amp;buffer, &amp;vec, &amp;idx, &amp;out](size_t i) {
        if (buffer[i])
            out[idx[i]] = vec[i];
    });

    return out;
}
</code></pre>

<p>output from a sample execution:</p>

<pre><code>input   : 0, 1, 1, 0, 1, 1, 0, 1, 1, 1
buffer  : 0, 1, 1, 0, 1, 1, 0, 1, 1, 1
idx     : 0, 0, 1, 2, 2, 3, 4, 4, 5, 6
out     : 1, 2, 4, 5, 7, 8, 9
</code></pre>

<p>Woh, woh… but this is so much code now! Can this even work?</p>

<p>So… yes, it works, and in some cases, it will be faster than the sequential version.</p>

<p>Here are the main caveats:</p>

<ul>
<li>The code adds substantially more work</li>
<li>We use additional buffers and containers, so we need more memory.</li>
</ul>

<h3 id="benchmark">Benchmark</h3>

<p>Let’s have a test run. Can this be faster than the sequential version?</p>

<pre><code>// 4 cores / 8 threads
benchmark vec size: 100000
transform only seq          : 2.5878 ms, ret: 100000
transform only par          : 1.3734 ms, ret: 100000
FilterCopyIf                : 5.3675 ms, ret: 50203
FilterCopyIfParNaive        : 9.1836 ms, ret: 50203
FilterCopyIfParCompose      : 3.03 ms, ret: 50203
FilterCopyIfParComposeSeq   : 2.3454 ms, ret: 50203
FilterCopyIfParTransformPush: 2.5735 ms, ret: 50203 
</code></pre>

<p>And for 6 cores:</p>

<pre><code>// 6 cores / 12 threads
benchmark vec size: 100000
transform only seq          : 2.3379 ms, ret: 100000
transform only par          : 0.5979 ms, ret: 100000
FilterCopyIf                : 3.675 ms, ret: 50203
FilterCopyIfParNaive        : 10.0073 ms, ret: 50203
FilterCopyIfParCompose      : 1.2936 ms, ret: 50203
FilterCopyIfParComposeSeq   : 1.0754 ms, ret: 50203
FilterCopyIfParTransformPush: 2.0103 ms, ret: 50203
</code></pre>

<p><code>FilterCopyIfParComposeSeq</code> - is a version of <code>FilterCopyIfParCompose</code> with a simple loop to copy the results:</p>

<pre><code>for (size_t i = 0; i &lt; vec.size(); ++i)
    if (buffer[i])
        out[idx[i]] = vec[i];
</code></pre>

<p>And <code>FilterCopyIfParTransformPush</code> is another variation where we have only <code>std::transform</code> to be run in parallel, and then we use regular <code>push_back</code>.</p>

<pre><code>template &lt;typename T, typename Pred&gt;
auto FilterCopyIfParTransformPush(const std::vector&lt;T&gt;&amp; vec, Pred p) {
    std::vector&lt;uint8_t&gt; buffer(vec.size());
    std::transform(std::execution::par, 
                   begin(vec), end(vec), begin(buffer), 
                   [&amp;p](const T&amp; elem) {return p(elem); }
    );

    std::vector&lt;T&gt; out;

    for (size_t i = 0; i &lt; vec.size(); ++i)
        if (buffer[i])
            out.push_back(vec[i]);

    return out;
}
</code></pre>

<p>But we can see that this version is 2x faster than the sequential! (for 4 cores) and 3x faster for 6 cores! So it’s a promising approach.</p>

<h2 id="blocks">Blocks</h2>

<p>Let’s try another approach.</p>

<p>This time we’ll split work into smaller chunks and then call <code>copy_if</code> separately:</p>

<pre><code>template &lt;typename T, typename Pred&gt;
auto FilterCopyIfParChunks(const std::vector&lt;T&gt;&amp; vec, Pred p) {
    const auto chunks = std::thread::hardware_concurrency();
    const auto chunkLen = vec.size() / chunks;
    std::vector&lt;size_t&gt; indexes(chunks);
    std::iota(indexes.begin(), indexes.end(), 0);

    std::vector&lt;std::vect…</code></pre></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.cppstories.com/2021/par-copyif/">https://www.cppstories.com/2021/par-copyif/</a></em></p>]]>
            </description>
            <link>https://www.cppstories.com/2021/par-copyif/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26234154</guid>
            <pubDate>Tue, 23 Feb 2021 05:40:52 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[AArch64 Boards and Perception]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26233981">thread link</a>) | @zdw
<br/>
February 22, 2021 | https://marcin.juszkiewicz.com.pl/2021/02/22/aarch64-boards-and-perception/ | <a href="https://web.archive.org/web/*/https://marcin.juszkiewicz.com.pl/2021/02/22/aarch64-boards-and-perception/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<p>Recently I had a discussion with A13 and realized that people may have
different perception of how AArch64 boards&nbsp;work:</p>
<blockquote>
<p>Sahaj told me that you can just install generic images on&nbsp;honeycomb</p>
<p>it kinda blows my&nbsp;mind</p>
</blockquote>
<p>How did we got to that&nbsp;point?</p>
<h3>Servers are boring,&nbsp;right?</h3>
<p>I started working on AArch64 in 2012. First in fast models written by Arm
developers, then also in <span>QEMU</span>. Both used direct kernel boot method without any
firmware or&nbsp;bootloaders.</p>
<p>In 2013 I moved from Canonical/Linaro to Red Hat. And there we got server from
Applied Micro. I do not remember how it booted as I used it for building
software. Some time later we had Mustangs and all of them were booting <span>UEFI</span>.</p>
<p>Then <a href="https://marcin.juszkiewicz.com.pl/2014/06/10/aarch64-is-in-the-house/">I got Mustang at home</a>. 
Fedora, <span>RHEL</span> were booting fine. Then CentOS and Debian joined. All of them
used grub-efi like my x86-64 desktop or&nbsp;laptop.</p>
<p>Time passed, I got other servers to work with. HPe M400, ThunderX, ThunderX2,
Falkor, D05 etc. Each of them was running <span>UEFI</span>. Either Tianocore based or
commercial&nbsp;one.</p>
<p>And to install operating system all I needed was to boot generic install&nbsp;media.</p>
<h3><span>SBC</span>&nbsp;hell</h3>
<p>At same time <span>SBC</span> world was fighting with users. Each vendor/SoC/board had to be
treated specially as there was no way to store firmware on board (as <a href="https://marcin.juszkiewicz.com.pl/2020/01/29/the-most-expensive-chip-in-the-arm-world/"><span>SPI</span> flash is
very expensive</a>).</p>
<p>So depending on <span>SBC</span> your firmware could be written&nbsp;either:</p>
<ul>
<li>at some special offset from start of microSD&nbsp;card</li>
<li>at the beginning of a partition of special&nbsp;type</li>
<li>in a file on vfat partition of any&nbsp;type</li>
<li>in a file on <span>EFI</span> System Partition (also using&nbsp;vfat)</li>
</ul>
<p>Some offsets forced the use of “obsolete” <span>MBR</span> partitioning as there was no space
for <span>GPT</span> information. While <span>UEFI</span> systems require <span>GPT</span> not <span>MBR</span>.</p>
<p>It also generated lot of wrong information like “this file needs to be named in
<span>UPPERCASE</span> (on case insensitive filesystem)” or “needs to be first file written
to a partition”. Some kind of “<span>SBC</span> boot&nbsp;voodoo”.</p>
<p>So each <span>SBC</span> required its own boot media — you could not take it to a board with
some other SoC and expect it to start. Or you spend some time to create some
kind of hybrid image which had a few bootloaders written. Easier way was to
prepare a separate boot media images per <span>SBC</span>.</p>
<p>From time to time there was <span>SBC</span> with onboard flash available for storing
firmware. Some people made use of it, others continued doing offset crap as they
were used to&nbsp;it.</p>
<h3><span>SBBR</span>, <span>EBBR</span>&nbsp;came</h3>
<p>Last years brought us several specifications from Arm. First was <span>SBBR</span> which
stands for Server Base Boot Requirements. It said which features should be
present in firmware (you can read more in <a href="https://marcin.juszkiewicz.com.pl/2020/10/12/standards-in-arm-space-part-i/">my previous post about Arm
standards</a>).</p>
<p>As SBCs are not servers, a new specification was created for them: <span>EBBR</span> (E means
Embedded). It basically says “try to follow what server does” and has some
requirements either dropped or&nbsp;relaxed.</p>
<p>Both were designed to make distribution’s life easier. Never mind is it <span>BSD</span>,
Linux or Microsoft Windows — they have to put <span>EFI</span> bootloader (like Grub-efi) in
<span>EFI</span> System Partition and system will boot on any supported <span>SBBR</span>/<span>EBBR</span>&nbsp;hardware.</p>
<p>For example I have a <span>USB</span> pendrive with Debian “bullseye” installed. It boots
fine on RockPro64 and Espressobin SBCs (both have <span>EBBR</span> compliant U-Boot stored
in on-board flash) and on Mustang and HoneyComb (both with <span>SBBR</span> compliant <span>UEFI</span>
in on-board&nbsp;flash).</p>
<h3>Habits. Good, bad,&nbsp;forced.</h3>
<p>So it looks like the way how AArch64 system should boot depends on what your
habits&nbsp;are.</p>
<p>When you started from servers then <span>SBBR</span>/<span>EBBR</span> way is your way and you look weird
at most of <span>SBC</span> systems with their offsets and “other mumbo&nbsp;jumbo”.</p>
<p>If all you used were <span>SBC</span> then going into <span>SBBR</span>/<span>EBBR</span> world can be “zOMG, it just
magically&nbsp;works!”.</p>
<h3>Note to <span>SBC</span>&nbsp;vendors</h3>
<p>Most SBCs already follow the <span>EBBR</span> standard or can easily be made compliant.
Never mind you are using mainline U-Boot or some own fork (and then consider
upstreaming as board’s life may be longer than you&nbsp;expect).</p>
<p>Enable the CONFIG_DISTRO_DEFAULTS option in the config. Build U-Boot, store it
to the board and boot. Then erase whatever environment you used before with “env
default -a”&nbsp;command.</p>
<p>On next reboot your <span>SBC</span> will iterate over “boot_targets” variable and check
for few standard boot&nbsp;files:</p>
<ul>
<li>extlinux/extlinux.conf</li>
<li>boot.scr.uimg</li>
<li>boot.scr</li>
<li>/efi/boot/bootaa64.efi</li>
</ul>
<p>When it gets something then it handles that and boots. If not then goes to
another boot&nbsp;target.</p>
<p>This allows to handle basically every operating system used on Arm systems.
And allows to boot generic install <span>ISO</span> (as long as <span>OS</span> on it supports the&nbsp;device).</p>
<p>Bonus points if your <span>SBC</span> has some on board flash or eMMC it can boot from. Then
firmware can be stored there so user does not even have to worry about&nbsp;it.</p>
	</div></div>]]>
            </description>
            <link>https://marcin.juszkiewicz.com.pl/2021/02/22/aarch64-boards-and-perception/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26233981</guid>
            <pubDate>Tue, 23 Feb 2021 05:03:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Over 30k Apple Macs have been infected with a high-stealth malware]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26233898">thread link</a>) | @sidcool
<br/>
February 22, 2021 | https://www.businessinsider.in/tech/news/over-30000-apple-macs-have-been-infected-with-a-high-stealth-malware-and-the-company-has-no-idea-why/articleshow/81145708.cms | <a href="https://web.archive.org/web/*/https://www.businessinsider.in/tech/news/over-30000-apple-macs-have-been-infected-with-a-high-stealth-malware-and-the-company-has-no-idea-why/articleshow/81145708.cms">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div id="datatxt81145708read"><div data-den="denmark"><div><p>San Francisco, </p><keyword keytype="person" smid="0" usetype="2" keywordseo="Security-researchers" actualkeyword="Security researchers">Security researchers</keyword><p> have discovered a </p><keyword keytype="UnKnown" smid="0" usetype="2" keywordseo="mysterious-malware" actualkeyword="mysterious malware">mysterious malware</keyword><p> on nearly 30,000 </p><keyword keytype="Company" smid="0" usetype="2" keywordseo="Apple" keynameseo="apple" actualkeyword="apple">Apple</keyword><p> Macs and they have no </p><keyword keytype="City" smid="0" usetype="2" keywordseo="idea" actualkeyword="idea">idea</keyword><p> what this is for and how is this virus going to infected the devices.
</p><p>
The malware named 'Silver Sparrow' comes with a mechanism to self-destruct itself, a capability that's typically reserved for high-stealth operations. 
</p><p>
    "So far, though, there are no signs the self-destruct feature has been used, raising the question of why the mechanism exists," Ars Technica first reported about the presence of malware citing security researchers.
</p><p>The lack of a final payload suggests that the malware may spring into action anytime.
</p><p>
    The malware has been found in 153 countries with heavy detection reported in the US, the UK, Canada, France and </p><keyword keytype="Location" smid="0" usetype="2" keywordseo="Germany" actualkeyword="Germany">Germany</keyword><p>. 
</p><p>
Silver Sparrow is an activity cluster that includes a binary compiled to run on Apple's new M1 chips but lacks one very important feature: a payload.
</p><p><span>Advertisement</span></p><figure><div></div></figure><hr><p>"Though we haven't observed Silver Sparrow delivering additional malicious payloads yet, its forward-looking M1 chip compatibility, global reach, relatively high infection rate, and operational maturity suggest Silver Sparrow is a reasonably serious threat," according to researchers from cyber security firm Red Canary.
</p><p>
The malware is uniquely positioned to deliver a potentially impactful payload at a moment's notice. 
</p><p>
    Silver Sparrow comes in two versions — one with a binary in mach-object format compiled for </p><keyword keytype="Company" smid="0" usetype="2" keywordseo="Intel" actualkeyword="Intel">Intel</keyword><p> x86_64 processors and the other Mach-O binary for the M1. 
</p><p>Researchers have earlier warned that Apple's transition from Intel to its own silicon M1 chip may make it easy for hackers to introduce malware.
</p><p>
    "To me, the most notable [thing] is that it was found on almost 30K macOS endpoints... and these are only endpoints the </p><keyword keytype="Company" smid="0" usetype="2" keywordseo="MalwareBytes" keynameseo="malwarebytes" actualkeyword="malwarebytes">MalwareBytes</keyword><p> can see, so the number is likely way higher," said Patrick Wardle, a macOS security expert.
</p><p>


<strong>SEE ALSO:<br><a href="https://www.businessinsider.in/stock-market/news/how-to-check-railtel-ipo-share-allotment-status-online/articleshow/81145120.cms">Indian Railways owned RailTel’s IPO: Here’s how to check share allotment status</a><br><a href="https://www.businessinsider.in/tech/news/india-needs-chipset-manufacturers-lots-of-them-here-is-why-/articleshow/81137745.cms">India needs chipset manufacturers — lots of them. Here is why</a><br><a href="https://www.businessinsider.in/stock-market/news/nurecas-100-crore-ipo-heres-how-to-check-share-allotment-status/articleshow/81135236.cms">Nureca’s ₹100 crore IPO: Here’s how to check share allotment status</a><br></strong>
<a href="https://www.businessinsider.in/travel/article/exploring-the-unreal-beauty-of-the-andaman-islands/articleshow/81126293.cms">Exploring the unreal beauty of the Andaman Islands</a></p>
</div></div></div></div></div>]]>
            </description>
            <link>https://www.businessinsider.in/tech/news/over-30000-apple-macs-have-been-infected-with-a-high-stealth-malware-and-the-company-has-no-idea-why/articleshow/81145708.cms</link>
            <guid isPermaLink="false">hacker-news-small-sites-26233898</guid>
            <pubDate>Tue, 23 Feb 2021 04:43:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[We Love Tailwind]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26233448">thread link</a>) | @flancrest
<br/>
February 22, 2021 | https://formcake.com/blog/why-we-love-tailwind | <a href="https://web.archive.org/web/*/https://formcake.com/blog/why-we-love-tailwind">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-v-e7750592=""><p><a href="https://tailwindcss.com/">Tailwind CSS</a> is a CSS framework of composable HTML utility functions and it's <em>great</em>. It favors markup-heavy design with little-to-nothing in the way of stylesheets.</p>
<p>Here's an example of it in action - our link styling.</p>
<p>This is what the markup looks like:</p>
<pre><code>&lt;a class="c-link" href="/blog"&gt;Our Blog&lt;/a&gt;</code></pre>

<p>Now here's the relevant section of <code>tailwind.css</code>:</p>
<pre><code>.c-link {
    @apply text-primary-highlight;
}

.c-link:hover {
    @apply underline;
}</code></pre>

<p>Simple, concise, powerful - there are so many things that make this and the rest of Tailwind great. Here are a few of them.</p>
<h2 id="standardization-and-theming">Standardization and Theming</h2>
<p>The ability to theme (e.g. <code>text-primary-highlight</code>) gives Tailwind a powerful consistency, but one of its killer realizations of standardization comes in the way it envisions spacing.</p>
<p>With padding (<code>p-1</code>) and margin (<code>m-1</code>) denominated with a simple unit range, available in combinations like padding-top (<code>pt-1</code>), margin top and bottom spacing (<code>my-1</code>), etc, with tailwind you can dedicate yourself to a few common sizes (say 2, 4, 6) use them in a reasonable way, and achieved the desired effect of visual balance. The system obviously depends on you exercising a certain amount of discipline, but it's a big improvement on just shooting from the hip with random space values (<code>12px</code>? <code>1.25rem</code>? Sure). It puts layout in the UI on rails.</p>
<h2 id="composability">Composability</h2>
<p>Because classes in Tailwind can be used together in any combination, you can do things like abstract the design of an element into a component via <code>@apply</code>, (for example, our link component) then add the spacing (e.g. <code>mt-1</code>, <code>p-2</code>, etc) in the individual markup element, separating out the layout and design code.</p>
<h2 id="semantic-value">Semantic Value</h2>
<p>Tailwind does a great job of using consistent structures for classes. Padding, margin, width, height - everything with some kind of space value - uses the same spread of unit values. Tailwind makes it easy to guess what a given utility class <em>should</em> be, given a rational naming system, which just makes you as a developer that much more productive.</p>
<p>This also addresses one of the biggest criticisms of Tailwind, that it's "just another DSL" adding a layer of complexity and buggy cruftiness between you and what should be pure, sweet markup. <em>Why not do this all in straight CSS, wouldn't it be simpler?</em></p>
<p>But because the way the public API in Tailwind is laid out makes it easy to comprehend and make guesses about, there's less you need to straight up memorize, and you become comfortable using it quickly.</p>

<p>One side-effect of making the Tailwind utilities composable is that you get a long "recipe" of all the classes that make up a particular design element. Pair with this with an active community of developers (they love their tools!) and continuing support from the original creators of Tailwind via their new library of paid Tailwind components, <a href="https://tailwindui.com/">Tailwind UI</a> and it's exceedingly easy to use a few community-sourced features as a starter and evolve them to suit your particular needs</p>
<p>Even the design of Tailwind itself is more conducive to community - passing around CSS snippets is awkward. You have to make sure the selectors are applied correctly and the CSS itself put in the right place to make sure that the right rules win out. But with Tailwind, just copy the class string from a given HTML component, add it to yours, and you're done. It makes it much easier to share small, component-level snippets.</p>
<p>These are just some of the reasons we've taken a shine to Tailwind, but the strongest thing we can say in its favor is that it's accelerated our frontend development. Tailwind delivers on its promise to wrap small, essential blocks of design and layout logic into their own standardized, bit-sized HTML classes, and in so doing empower developers to haggle less with their CSS spacing and instead just get on with the business of bootstrapping a prototype quickly.</p>
</div></div>]]>
            </description>
            <link>https://formcake.com/blog/why-we-love-tailwind</link>
            <guid isPermaLink="false">hacker-news-small-sites-26233448</guid>
            <pubDate>Tue, 23 Feb 2021 03:09:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[On Bookmarks and Note Taking (2020)]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26233405">thread link</a>) | @bariumbitmap
<br/>
February 22, 2021 | https://chrisman.github.io/11.html | <a href="https://web.archive.org/web/*/https://chrisman.github.io/11.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<p>
I am moving my collection of links from a naive bookmarking system to a more intentional centralized knowledge base.
</p>
<p>
2020-07-07
</p>
<h2 id="contents">Contents</h2>
<ol type="1">
<li>A Personal History</li>
<li>A Fractured Landscape</li>
<li>Bookmarking: Why Even Do A Thing?</li>
<li>More On Type 2 Bookmarks</li>
<li>Deplatforming my bookmarks</li>
<li>Conclusion</li>
<li>Resources</li>
</ol>
<h2 id="a-personal-history">A Personal History</h2>
<p>The happiest I’ve ever been with my bookmarks was when I used <a href="https://en.wikipedia.org/wiki/Delicious_(website)">del.icio.us</a> back in the aughts. The things I liked about it best are:</p>
<ol type="1">
<li><p>Simplicity: It worked via a bookmarklet.</p></li>
<li><p>Organization: I could tag bookmarks, and organize tags into categories of tags, and</p></li>
<li><p>Sharing: I could easily share a tag or group of tags with a friend or collaborator.</p></li>
</ol>
<p>I remembering seeing, I think it was Warren Ellis’s collection of tags and bundles on del.icio.us around this time, and it was an astounding collection of research.</p>
<p>Nothing I’ve used since then has been as good at organization and exporting/sharing.</p>
<p>I started using <a href="https://en.wikipedia.org/wiki/Pocket_(service)">Pocket</a> probably also in the aughts, back when it was still Read It Later, and have consistently been just sorta content with it. There’s no good way to share tags or groups of tags, and there’s no kind of meta-organizing of tags. But it allows me to save stuff to read and reference later.</p>
<p>It is the minimally viable bookmarking product. And now it is owned by Mozilla and for some reason it is also part of Firefox, so it’s constantly available. (Even though I use the service myself, I go back and forth between feeling like this integration is convenient, and like it is being shoved in my face.)</p>
<p>I know there are other products out there. Especially <a href="http://pinboard.in/t">pinboard</a>, which I think might be the holy grail of bookmarking for me, but I haven’t yet been willing to shell out $$$ for a subscription to a service that offers no trial.</p>
<h2 id="a-fractured-landscape">A Fractured Landscape</h2>
<p>The other problem with bookmarks is that it is really, really hard for me to commit to keeping them all in one place while so many sites offer their own “save” functions.</p>
<p>Many of my bookmarks remain in the form of saved comments and posts on reddit, saved links on lobste.rs and hackernews, saved videos and playlists on youtube, favorited posts on twitter and mastodon, “starred” content in my rss reader, etc. So trying to find “saved” content later often means visiting all those sites and more looking for where I saved the damn thing. Sometimes they’re saved for some reason in my browser bookmarks, which is the least flexible way to save bookmarks.</p>
<h2 id="bookmarking-why-even-do-a-thing">Bookmarking: Why Even Do A Thing?</h2>
<p>Why bookmark things?</p>
<p>From on my own experiences, there are two primary reasons:</p>
<ol type="1">
<li><p>To create a <a href="https://en.wikipedia.org/wiki/Getting_Things_Done#Workflow">Someday/Maybe</a> or Read It Later list. This link is something that looked interesting to you, but wasn’t interesting enough for you to read at the time. Some of these items will actually get read at some point, but many more will not. There’s an element of hoarding to collecting these kind of bookmarks. I think it stems in part from a sort of anxiety rooted in <a href="https://en.wikipedia.org/wiki/Fear_of_missing_out">the belief that you might be missing out on something</a>. In this case, missing out on access to the knowledge contained in the bookmarked resource.</p>
<p>I try to avoid making these <abbr title="Fear Of Missing Out">FOMO</abbr> bookmarks, because they just sit there, not getting read, but still taking up space and mental bandwidth every time you look at your bookmarks and see them, and once again promise yourself, “Someday…”</p></li>
<li><p>To create references, sources, and quotes that you will refer back to in your own writing or in the course of your own work. This kind of bookmarking feels more “correct” to me. Or at least much more useful and more productive.</p></li>
</ol>
<h2 id="more-on-type-2-bookmarks">More On Type 2 Bookmarks</h2>
<p>The primary value in a type 2 bookmark is, again, extracting content from it. Creating a reference and a context.</p>
<p>The ability to comment on or annotate a bookmark is a feature I have seen absolutely nowhere since the old days of Google Reader.</p>
<p>There was a redditor I saw commenting on current events stories around 2016 - 2017 who must have had a fantastic Type 2 Bookmarking workflow. The remarkable thing about their posts is that nearly every single <em>sentence</em> would have between 1 - 3 citations to a variety of news sources backing up their claims.</p>
<p>A simple collection of links wouldn’t allow for such recall. They must have had snippets and summaries ready to go for each topic on which they commented.</p>
<p>It is this sort of “Type 2” content that I’ve been collecting from all the books I’ve read the past few years. I have found this process of note taking to be so useful that I have finally come to consider reading books a waste of time unless I do it.</p>
<p>Unless, that is, I am reading for leisure, in which case I rarely care whether I can remember the particulars of the content in a month’s time. The point of reading such a book is to experience the feelings of having read it. Not to absorb any factual content from it.</p>
<p>But if it’s a book that I’m reading to gain practical knowledge and know-how, then of course, most certainly, if that knowledge and know-how fades over the course of a couple months to a year, then how is it not wasted time?</p>
<p>The same holds true of bookmarks. If you don’t capture some kind of content or context from the bookmarked material, then the bookmark becomes opaque and its significance, meaning, and value degrades with time.</p>
<p>That’s one of the reasons that Type 1 / FOMO bookmarks so rarely get read.</p>
<h2 id="deplatforming-my-bookmarks">Deplatforming my bookmarks</h2>
<p>So here’s an endeavor that I’ve been undertaking for the last couple of weeks.</p>
<p>I am deplatforming my bookmarks.</p>
<p>What this looks like is going through all the platforms I mentioned above, and assessing all of my saved items, and then migrating <em>content</em> to a single knowledge database.</p>
<p>Type 1 bookmarks are for the most part simply deleted. If I haven’t read them by now, I ain’t gonna.</p>
<p>Type 2 bookmarks are reevaluated for their content. Useful, interesting content that I will want to refer back to is entered into <a href="https://github.com/chrisman/knowledge/wiki">my personal wiki</a>, and the bookmark is added as a source for that content.</p>
<p>This immediately feels so much better! I’m no longer collecting opaque URLs. I am collecting <em>information</em> and <em>content</em> and the links are a side effect and a source for that content.</p>
<p>My “tags” are pages in my wiki. “Bundles” emerge through linking pages. It’s easier to see and follow links between concepts. And concepts gain depth and validity as more sources are added.</p>
<p>The additional benefits of keeping all this content in a GitHub wiki include:</p>
<ol type="1">
<li><p>git: having all my info in version control and on multiple computers, with access to command line tools to search and format text.</p></li>
<li><p>web access: the added convenience of web interface, so I can edit and view in the browser and on mobile.</p></li>
</ol>
<h2 id="conclusion">Conclusion</h2>
<p>I’ll probably continue to use something like Pocket to form a queue of content to be processed. Because putting the content in the knowledge base is certainly more work than the one-click “save to pocket” feature in my browser. But I intend to continue with this exercise because it feels really great.</p>
<p>So allow me to revize my requirements of a bookmarking system, and to evaluate this new systems by these requirements:</p>
<ol type="1">
<li><p>Centralized: no more searching through multiple sites to find the reference I’m looking for.</p>
<p>Score: Passing, with a qualifier. It offers the <em>promise</em> of a fully centralized collection, but there is still the process of deplatforming that content, because honestly I’m still going to save stuff on reddit and on pocket for the sake of ease and convenience.</p></li>
<li><p>Simple: be able to capture a bookmark quickly and easily.</p>
<p>Score: Neutral. Entering content into the knowledge base takes much longer than a single click-to-save. Solution: continue using convenient things like Pocket and Reddit, and continue migrating content from those platforms to the knowledge base. (See point #1 above.)</p></li>
<li><p>Organized: have tags and ideally “bundles” of tags, and the ability to see links and connections between tags.</p>
<p>Score: Passing. Pages and hyperlinks as tags and connections is great so far, and has lead to more connections between ideas.</p></li>
<li><p>Sharing: be able to share tags and content with others.</p>
<p>Score: Passing. Instead of linking to tags, I can share a URL to a page on the wiki. It’s too bad that the “Watch” feature doesn’t (I think) tell people when a wiki is updated. One of the coolest features of old del.icio.us and current pinboard is that every tag has an RSS feed you can subscribe to!</p></li>
</ol>
<p>Total score: passing!</p>
<h2 id="resources">Resources</h2>
<ul>
<li><p>meta-knowledge: a list of people who learn things in public on GitHub.</p>
<p><a href="https://github.com/RichardLitt/meta-knowledge">https://github.com/RichardLitt/meta-knowledge</a></p></li>
<li><p>note taking</p>
<ul>
<li><p>Zettelkasten: a note taking method that HN is obsessed with right now. Originally used index cards, now uses hipster Electron apps.</p>
<p><a href="https://writingcooperative.com/zettelkasten-how-one-german-scholar-was-so-freakishly-productive-997e4e0ca125">https://writingcooperative.com/zettelkasten-how-one-german-scholar-was-so-freakishly-productive-997e4e0ca125</a></p></li>
<li><p>Commonplace Book: a method for collecting, linking, and indexing written notes.</p>
<p><a href="https://publicdomainreview.org/collection/john-lockes-method-for-common-place-books-1685">https://publicdomainreview.org/collection/john-lockes-method-for-common-place-books-1685</a></p></li>
<li><p>Org Mode: included here for completeness. People <a href="https://tiny.tilde.website/@cpb/104230682765018604">do NOT shut up about org mode</a>.</p></li>
</ul></li>
</ul>
</div></div>]]>
            </description>
            <link>https://chrisman.github.io/11.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26233405</guid>
            <pubDate>Tue, 23 Feb 2021 03:00:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Constexpr.js]]>
            </title>
            <description>
<![CDATA[
Score 9 | Comments 2 (<a href="https://news.ycombinator.com/item?id=26233187">thread link</a>) | @fctorial
<br/>
February 22, 2021 | https://fctorial.github.io/posts/constexpr.js.html | <a href="https://web.archive.org/web/*/https://fctorial.github.io/posts/constexpr.js.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
    <header>
        <h3 id="main_title">Constexpr.js</h3>
    </header>
    <h4>What is constexpr.js?</h4>
    <p>
        <a href="https://github.com/fctorial/ConstexprJS">constexpr.js</a> is a static site generator which doesn't force you to learn a domain specific language.
        When using this tool, you use javascript and usual DOM manipulation methods to generate the webpage. The tool
        will render the page using chrome, and once it has finished rendering, it will save the rendered state of the
        page as a new html file. This new html file will look exactly like the original page after it has finished rendering.
        For example, the tool converts <a href="https://fctorial.github.io/demos/constexpr.js/input.html">this</a> page into <a href="https://fctorial.github.io/demos/constexpr.js/output.html">this</a> page.
        <br>
        The generated page doesn't have to be completely static. In the above example, the heading is being animated
        with javascript.
    </p>

    <h4>How to use it?</h4>

    <p>
        You will have to divide the javascript being used in your page into two groups. Runtime javascript and
        compile time javascript, and annotate all compile time script tags with <progi>constexpr</progi> attribute:
        <prog>
<span><span><span>&lt;</span>script</span> <span>constexpr</span><span>&gt;</span></span><span><span>
    <span>...</span>
</span></span><span><span><span>&lt;/</span>script</span><span>&gt;</span></span>
<span><span><span>&lt;</span>script</span> <span>constexpr</span> <span>src</span><span><span>=</span><span>"</span>/generate_page.js<span>"</span></span><span>&gt;</span></span><span></span><span><span><span>&lt;/</span>script</span><span>&gt;</span></span>
        </prog><br>
        Runtime code must not depend on the compile time code, since that code will be stripped out before writing the output file.
    </p>

    <p>
        Once the HTML generation code has finished rendering, it must call the <progi>window._ConstexprJS_.compile()</progi>
        function. This function is injected into the page by the compiler.
    </p>

    <p>
        The compiler can be installed like this: <prog><span>npm</span> i -g constexpr.js</prog><br>

        Command line usage:
        <prog>constexpr.js --input<span>=</span><span>"&lt;input_directory&gt;"</span> --output<span>=</span><span>"&lt;output_directory&gt;"</span> <span>[</span>--exclusions<span>=</span>path1:path2<span>]</span> <span>[</span>--verbose<span>]</span> <span>[</span>--jobs<span>=</span>n<span>]</span> <span>[</span>--noheadless<span>]</span> <span>[</span>--jobtimeout<span>]</span> <span>[</span>--depfile<span>=</span><span>&amp;</span>depfile<span>&gt;</span><span>]</span></prog><br>

        A json object with the command line args, compilation results and dependencies will be written to the path specified by <progi>--depfile</progi> option.
        <br>
        The tool also copies resources (<progi>css</progi>, <progi>images</progi> etcetra)
        that are requested by pages being rendered. HTML files/resources inside paths given in <progi>--exclusions</progi> are not processed/copied.
    </p>

    <h4>Notes</h4>

    <ol>
        <li>
            You can use any web development technology (and any number of technologies) to generate the html without any fear
            of bloat. Just make sure that <progi>window._ConstexprJS_.compile()</progi> is called <span>after</span>
            the page has finished rendering.
            <p>
            
            Pivottable.js demo:
            </p><div id="pt_output"><table data-numrows="4" data-numcols="4"><thead><tr><th colspan="2" rowspan="1"></th><th>day</th><th colspan="1" rowspan="2">Fri</th><th colspan="1" rowspan="2">Sat</th><th colspan="1" rowspan="2">Sun</th><th colspan="1" rowspan="2">Thur</th><th rowspan="2">Totals</th></tr><tr><th>sex</th><th>smoker</th><th></th></tr></thead><tbody><tr><th rowspan="2">Female</th><th rowspan="1" colspan="2">No</th><td data-value="6.25">6.25</td><td data-value="35.42">35.42</td><td data-value="46.61">46.61</td><td data-value="61.49">61.49</td><td data-value="149.77" data-for="row0">149.77</td></tr><tr><th rowspan="1" colspan="2">Yes</th><td data-value="18.78">18.78</td><td data-value="43.03000000000001">43.03</td><td data-value="14">14.00</td><td data-value="20.930000000000003">20.93</td><td data-value="96.74" data-for="row1">96.74</td></tr><tr><th rowspan="2">Male</th><th rowspan="1" colspan="2">No</th><td data-value="5">5.00</td><td data-value="104.21000000000001">104.21</td><td data-value="133.96000000000004">133.96</td><td data-value="58.83">58.83</td><td data-value="302" data-for="row2">302.00</td></tr><tr><th rowspan="1" colspan="2">Yes</th><td data-value="21.93">21.93</td><td data-value="77.73999999999998">77.74</td><td data-value="52.82">52.82</td><td data-value="30.58">30.58</td><td data-value="183.07" data-for="row3">183.07</td></tr><tr><th colspan="3">Totals</th><td data-value="51.96" data-for="col0">51.96</td><td data-value="260.4" data-for="col1">260.40</td><td data-value="247.39000000000007" data-for="col2">247.39</td><td data-value="171.83" data-for="col3">171.83</td><td data-value="731.58">731.58</td></tr></tbody></table></div>
            <br>
            This page also uses prism.js for syntax highlighting.
        </li>
        <li>
            You can mark tags other than <progi>script</progi> with <progi>constexpr</progi> as well.
            In the above example, the box at the top is marked constexpr, so it isn't present in the output page.
            This can be used to differentiate original website from generated website:
            <prog>
<span><span><span>&lt;</span>style</span> <span>constexpr</span><span>&gt;</span></span><span><span>
<span>body</span> <span>{</span>
    <span>border</span><span>:</span> 2px solid red<span>;</span>
<span>}</span>
</span></span><span><span><span>&lt;/</span>style</span><span>&gt;</span></span>
            </prog>
        </li>
        <li>
            Client code in the page can signal the compiler to skip the current file by calling <progi>window._ConstexprJS_.abort(message)</progi>.
        </li>
        <li>
            In the original webpage, you'll see a console error when the code tries to call the compilation trigger function,
            since that function is injected by the compiler. You can add this snippet near the top to fix that error:

            <prog>
<span>&lt;</span>script constexpr<span>&gt;</span>
  <span>if</span> <span>(</span><span>!</span>window<span>.</span>_ConstexprJS_<span>)</span> <span>{</span>
    window<span>.</span>_ConstexprJS_ <span>=</span> <span>{</span>
      <span>compile</span><span>:</span> <span>(</span><span>)</span> <span>=&gt;</span> <span>{</span><span>}</span><span>,</span>
      <span>abort</span><span>:</span> <span>(</span><span>)</span> <span>=&gt;</span> <span>{</span><span>}</span>
    <span>}</span>
  <span>}</span>
<span>&lt;</span><span>/</span>script<span>&gt;</span>
            </prog>
        </li>
        <li>
            There might be multiple rendering tasks running in your page. You can manage all those tasks using <a href="https://github.com/fctorial/fctorial.github.io.src/blob/master/static/js/constexpr/index.js">this</a>
            refcounting mechanism.

            All the tasks will call <progi>startLoading()</progi> when they begin loading, and <progi>endLoading()</progi>
            when they've finished loading. The compilation will be triggered when all the tasks have finished.
        </li>
        <li>
            You should keep all list data separate from the html in <a href="https://github.com/fctorial/fctorial.github.io.src/tree/master/collections">json files</a>.
            <progi>constexpr</progi> code in the html will fetch these json files and render the page using them.
            The directory containing this data should be excluded using <progi>--exclusions</progi> flag, so that the
            resources inside it aren't copied over.
        </li>
        <li>
            You can include dev utilites like <a href="https://github.com/fctorial/fctorial.github.io.src/blob/master/static/js/constexpr/nav.js#L19">this</a> in the
            original website. It reloads the page whenever it's focused. It won't be in the output since it's used as constexpr.
        </li>
        <li>
            This whole website is rendered using javascript and constexpr.js. Nothing other than the demo uses runtime javascript:
            <prog>
$ tokei -t=javascript
===============================================================================
Language            Files        Lines         Code     Comments       Blanks
===============================================================================
JavaScript              1            2            1            0            1
===============================================================================
Total                   1            2            1            0            1
===============================================================================
            </prog>
            The original sources can be found <a href="https://github.com/fctorial/fctorial.github.io.src">here</a>.
        </li>
    </ol>
</article></div>]]>
            </description>
            <link>https://fctorial.github.io/posts/constexpr.js.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26233187</guid>
            <pubDate>Tue, 23 Feb 2021 02:24:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Macamathehou in Lincolnshire and people named Muhammad in medieval England]]>
            </title>
            <description>
<![CDATA[
Score 34 | Comments 18 (<a href="https://news.ycombinator.com/item?id=26232817">thread link</a>) | @pepys
<br/>
February 22, 2021 | https://www.caitlingreen.org/2021/01/macamathehou-in-lincolnshire.html | <a href="https://web.archive.org/web/*/https://www.caitlingreen.org/2021/01/macamathehou-in-lincolnshire.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="post-body-4764828100605476219" itemprop="description articleBody">
<p>The aim of the following draft is to offer some thoughts on a local name from thirteenth-century Lincolnshire, <i>Macamathehou</i>, that involves a version of the Arabic name Muhammad (Middle English <i>Makomet/Macamethe</i>, Old French <i>Mahomet</i>). Whilst it has been plausibly seen as an instance of a variant of the name of Muhammed being used to mean 'heathen', 'pagan idol' or similar (based on the false but common medieval Christian belief that the prophet Muhammad was worshipped as a god), here in reference to a barrow that was considered to be a pre-Christian site, it is worth noting that there are a small number of people with names and surnames derived from Arabic <i>Muḥammad</i> apparently living in twelfth- to fourteenth-century England.</p><table><tbody><tr><td><a href="https://1.bp.blogspot.com/-w4ER6e7-JsI/X7AZDfw1bCI/AAAAAAADLhw/ULvdpDuWKwAelDOKu23y5ZJFgTBdhTdMgCLcBGAsYHQ/s879/macamathehou-large.jpg"><img data-original-height="355" data-original-width="500" src="https://1.bp.blogspot.com/-hhv-Lu79n_o/X7AZDcDbe6I/AAAAAAADLhs/RS2oeTJ3IocU1SGmohUZdB7Q-nFhCH-sgCLcBGAsYHQ/s16000/macamathehou-500.jpg"></a></td></tr><tr><td><i>Figure 1: the location of Macamathehou between Spridlington and Faldingworth parishes in Lincolnshire; click the image or <a href="https://1.bp.blogspot.com/-w4ER6e7-JsI/X7AZDfw1bCI/AAAAAAADLhw/ULvdpDuWKwAelDOKu23y5ZJFgTBdhTdMgCLcBGAsYHQ/s879/macamathehou-large.jpg">here</a> for a larger version (image: C. R. Green/OpenStreetMap and its contributors).</i><i>&nbsp;</i></td></tr></tbody></table>
<p>The existence of the intriguing local name <i>Macamathehou</i> in the parish of Spridlington, Lincolnshire, was first noted in 2001 by Kenneth Cameron, John Field and John Insley in <i>Place-Names of Lincolnshire VI </i>(<i>PNL</i>), with both attestations of the name dating from the thirteenth century (the reign of King Henry III, 1216–72).(<a href="https://www.caitlingreen.org/2021/01/macamathehou-in-lincolnshire.html#fn1">1</a>) They identify the two elements of the name as being Old Norse <i>haugr</i>, 'mound, barrow', and Middle English <i>Makomet/Macamethe</i>, which derives from the name of the prophet Muhammad (Medieval Latin <i>Machometus/Mahumetus</i>, Anglo-Norman <i>Mahumet/Mahomet/Machomete</i>, Old French <i>Mahomet</i> &lt; Arabic <i>Muḥammad</i>, probably via an Arabic regional form <i>Maḥammad</i>).(<a href="https://www.caitlingreen.org/2021/01/macamathehou-in-lincolnshire.html#fn2">2</a>) Needless to say, this solution is most intriguing and has, moreover, found favour with other place-name specialist, including the <i>Vocabulary of English Place-Names </i>(<i>VEPN</i>) and Richard Coates.(<a href="https://www.caitlingreen.org/2021/01/macamathehou-in-lincolnshire.html#fn3">3</a>)</p><p>As to the import of this name, the easiest conclusion—and the one endorsed by&nbsp;<i>PNL</i>, <i>VEPN</i>&nbsp;and Coates—is that the first element, <i>Macamethe/</i><i>Maumate</i> etc, is not functioning simply as a normal Middle English rendering of the name Muhammad/<i>Mahomet</i>, but rather as a word indicative of heathen or pagan idolatry, based on the false but common medieval Christian belief that the prophet Muhammad was worshipped as a god. So, <i>PNL </i>describes the name as meaning 'the heathen mound', with the first element being 'a corrupt ME [Middle English] form of the name of the prophet Mohammed, for which <i>v.</i>&nbsp;MED [<i>Middle English Dictionary</i>], s.v. <i>Makomete</i>, also used to denote a pagan god or an idol'.(<a href="https://www.caitlingreen.org/2021/01/macamathehou-in-lincolnshire.html#fn4">4</a>) This is taken up by Richard Coates, who says that it has been suggested, 'with great plausibility', that <i>Macamathehou </i>in Spridlington parish 'is a Middle English name meaning "Mahomet mound", i.e. "heathen mound"', and points to 'the repeated compound of OE <i>hæðen </i>+ <i>byrgels "</i>heathen burial"' as a potential comparison.(<a href="https://www.caitlingreen.org/2021/01/macamathehou-in-lincolnshire.html#fn5">5</a>) Likewise, the <i>VEPN</i>'s draft section on M includes the following discussion:</p>

<blockquote><b>makomet </b>ME, 'idol, pagan god', an application of the name of the Arab prophet Mohammed (commonly though mistakenly believed by medieval Christians to have been worshipped as a god)... It occurs early in
<i>Macamathehou </i>(f.n.) 1216–72 L:6·211 (<b>haugr</b>), presumably to be
interpreted as 'heathen mound'.(<a href="https://www.caitlingreen.org/2021/01/macamathehou-in-lincolnshire.html#fn6">6</a>)</blockquote>
<p>On the whole, this interpretation is probably the safest option. There are certainly a handful of references to 'heathen' barrows in Old English charter bounds, for example <i>of leofwynne mearce to þam hæþenan beorge</i>, 'from Leofwine's boundary to the heathen barrow', in the charter S956 relating to Drayton, Hampshire, and dated AD 1019, although none are recorded from Lincolnshire.(<a href="https://www.caitlingreen.org/2021/01/macamathehou-in-lincolnshire.html#fn7">7</a>) It has also been suggested that the Lincolnshire names Bloater Hill (North Willingham) and Blod Hou (Barrow-on-Humber) derive from Old Norse <i>blóthaugr</i>, 'a sacrificial mound', whilst other names involving <i>haugr</i> certainly refer to supernatural/demonic creatures—for example, <i>Gasthehowe</i>/<i>Gastehowe</i>, Ashby Puerorum (Lincolnshire), recorded in the thirteenth century and deriving from Middle English <i>gast</i>/Old English <i>gāst</i>, 'ghost, dead-spirit', or names like Scratters (<i>Scrathou</i>, in Hayton, East Riding of Yorkshire) and Scrathowes (<i>Scrathou</i>, in Osmotherley, North Riding of Yorkshire), which derive from Old Norse <i>skratti</i>, 'devil, wizard'&nbsp;+ <i>haugr</i>.(<a href="https://www.caitlingreen.org/2021/01/macamathehou-in-lincolnshire.html#fn8">8</a>) Furthermore, the Old English compound <i>hæðen&nbsp;</i>+ <i>byrgels</i>, 'heathen burial', does indeed recur frequently in Late Saxon charter bounds, with these names often said to be identifiable with barrows in the landscape.(<a href="https://www.caitlingreen.org/2021/01/macamathehou-in-lincolnshire.html#fn9">9</a>)</p><p>On the other hand, there are some possible issues with this explanation, and other interpretations are possible of Spridlington's <i>Macamathehou</i>. First, the comparison with the many instances of the OE compound <i>hæðen </i>+ <i>byrgels</i>, ‘heathen burial’, is perhaps not as convincing as it might seem. Not only is a link between this term and barrows only demonstrable in a handful of instances, but Andrew Reynolds has also suggested that the sense of the term was primarily not ‘pagan’, but rather ‘unconsecrated’, and that it denoted burials of executed offenders and other social outcasts, which renders the proposed value of these names as support for interpreting <i>Macamathehou&nbsp; </i>as meaning ‘heathen mound’ open to significant debate.(<a href="https://www.caitlingreen.org/2021/01/macamathehou-in-lincolnshire.html#fn10">10</a>) Second, if the above is correct, then this would be the only known instance of a derivative of the Arabic name Muhammad being used in a place-name to indicate a 'heathen mound' or similar, which is potentially concerning—the other elements noted above all recur in multiple names. Third, the element identified by <i>PNL </i>and <i>VEPN</i> as being present in <i>Macamethehou</i> is Middle English <i>Makomet(e)</i>. The <i>Middle English Dictionary</i> (<i>MED</i>) on <i>Makomet(e)/</i><i>Macamethe</i> etc, however, makes it clear that the primary use of this word in Middle English is as a form of the name Muhammad, not as a word referring to an 'idol'/'pagan god', with the vast majority of quotations provided by the <i>MED </i>referring either the prophet Muhammad or people named Muhammad; the only exceptions are a single quotation from Layamon's <i>Brut </i>(<i>c.</i>&nbsp;1200, <i>mahimet</i>, lacking the <i>-c-</i>), and three from two later texts.(<a href="https://www.caitlingreen.org/2021/01/macamathehou-in-lincolnshire.html#fn11">11</a>) The form of the name Muhammad that <i>was </i>primarily—although not exclusively—used in the sense 'pagan deity, idol', is rather <i>Maumet/Maumate</i>, mentioned above, deriving from Anglo-Norman <i>Maumet</i>, a reduced form of <i>Mauhoumet</i>, Old French <i>Mahomet/Mahommet</i>.(<a href="https://www.caitlingreen.org/2021/01/macamathehou-in-lincolnshire.html#fn12">12</a>)</p>
<p>In this light, it is worth considering whether it is possible that the name <i>Macamathehou</i> could somehow be named from a person named <i>Makomet</i>/Muhammad or similar living in medieval England. Certainly, it should be noted that multiple local names relating to mounds/barrows do seem to be named after people who owned estates or land in the area. For example, Andrew Reynolds draws attention to the bounds of a mid-tenth-century charter for Swallowcliffe, Wiltshire (S468), that records the burial site of a seventh‐century woman whose grave had been cut into an existing mound as <i>Posses hlaew</i>, noting that 'Poss is a male name, and thus the mound is apparently not named after its Anglo‐Saxon occupant', implying that it was instead named after a later estate owner.(<a href="https://www.caitlingreen.org/2021/01/macamathehou-in-lincolnshire.html#fn13">13</a>) As Irene Bower long ago pointed out, such a situation can be credibly paralleled in Lincolnshire, with a number of Lincolnshire names involving <i>haugr</i> seeming to contain the same personal-name as is found in the same or a neighbouring parish-name—so, <i>Scalehau </i>(<i>Skalli </i>+ <i>haugr</i>) was located near to Scawby (<i>Skalli</i>&nbsp;+ <i>bȳ</i>), with Kenneth Cameron commenting that the two were 'no doubt named from the same man'; <i>Leggeshou</i> (<i>Leggr </i>+ <i>haugr</i>) was located on the boundary of Legsby parish (<i>Leggr&nbsp;</i>+ <i>bȳ</i>); <i>Katehou/Catehowe </i>(<i>Kati</i>&nbsp;+ <i>haugr</i>) was located in South Cadeby (<i>Kati&nbsp;</i>+ <i>bȳ</i>); and a <i>Grimaldeshawe</i> (<i>Grimaldi </i>+ <i>haugr</i>) was recorded in the neighbouring parish to Grimoldby (<i>Grimaldi</i>&nbsp;+&nbsp;<i>bȳ</i>), perhaps on the boundary between the two.(<a href="https://www.caitlingreen.org/2021/01/macamathehou-in-lincolnshire.html#fn14">14</a>)</p><table><tbody><tr><td><a href="https://1.bp.blogspot.com/-Nu02Jqw5h14/X7WB-D0A8sI/AAAAAAADLvg/p_vwSAkF6RQX5KkB8XMvM6WiC4a1eyI5gCLcBGAsYHQ/s1296/PipeRoll-Mahumet1160-1.jpg"><img data-original-height="216" data-original-width="500" src="https://1.bp.blogspot.com/-VAcqNuGj78U/X7WB-YLMzBI/AAAAAAADLvk/_TLGpi35Olg-dWNhgmcuMcZSaz-qhCgdACLcBGAsYHQ/s16000/PipeRoll-Mahumet1160-1-500.jpg"></a></td></tr><tr><td><i>Figure 2: Section from the Pipe Roll Society publication of&nbsp;The Great Roll of the Pipe for the Seventh Year of the Reign of King Henry the Second, A.D. 1160–1161 (London: Wyman &amp; Sons, 1885), p. 10, dealing with Mahumet of Wiltshire (image: <a href="https://archive.org/details/piperollsociety04pipeuoft/page/10/mode/2up">Internet Archive</a>).</i></td></tr></tbody></table><p>As to the likelihood of someone named Muhammad or one of its Anglo-Norman/Middle English variants (<i>Mahumet</i>,<i> Makomet</i> and similar) actually living in medieval England, this is perhaps less far-fetched than might be assumed. Katharine Keats-Rohan and John Moore have directed attention to the Wiltshire entries of five consecutive Pipe Rolls of Henry II (1160/61–1164/65) that refer to a man named <i>Mahumet, </i>whose name-form Moore considers very difficult to explain as anything other than a rendering of Muhammad and which is accepted as such by the <i>OED </i>and <i>MED</i>. This <i>Mahumet </i>is recorded in the Pipe Rolls only because he was fined for his part in an unlicensed duel with a John de Merleberge, probably in or near Marlborough Castle, and it seems he was not an especially wealthy man, as he was pardoned the last mark of his fine due to his poverty.(<a href="https://www.caitlingreen.org/2021/01/macamathehou-in-lincolnshire.html#fn15">15</a>) Furthermore, <i>Mahumet</i> of Wiltshire was not the only man with this name for whom we have evidence from medieval England. For example, a Theobald <i>filius Mahumet</i> (or <i>filius Mahomet</i>) is recorded from early thirteenth-century Hampshire in the Pipe Rolls of Henry III for 1222–24; another man named <i>Mahomet </i>is recorded in 1327, when Edward III issued him and six others a pardon at Newton-on-Ouse, Yorkshire, for 'offenses in Ireland'; and a <i>Mahummet Saraceno</i>&nbsp;occurs in the Close Rolls of Henry III for 1254. Furthermore, a number of people surnamed <i>Mahumet </i>and similar are recorded in documents of the twelfth and thirteenth centuries, for example a Humphrey Mahumet in a charter of Southwick Priory, Hampshire, a Herbert Maumet who was sergeant of Portsmouth in the mid-thirteenth century, and a Radulphus Maumet who is recorded in the reign of King John.(<a href="https://www.caitlingreen.org/2021/01/macamathehou-in-lincolnshire.html#fn16">16</a>) Moore also notes the presence of someone bearing another 'apparent Arab name' in twelfth-century Hampshire, a certain <i>Paucamatus</i>, a name that he considers to probably reflect <i>Bakmat</i>, who is recorded in Winchester from 1159/60 until 1183/4 and who is associated with …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.caitlingreen.org/2021/01/macamathehou-in-lincolnshire.html">https://www.caitlingreen.org/2021/01/macamathehou-in-lincolnshire.html</a></em></p>]]>
            </description>
            <link>https://www.caitlingreen.org/2021/01/macamathehou-in-lincolnshire.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26232817</guid>
            <pubDate>Tue, 23 Feb 2021 01:20:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The road to electric is filled with tiny cars]]>
            </title>
            <description>
<![CDATA[
Score 84 | Comments 102 (<a href="https://news.ycombinator.com/item?id=26232760">thread link</a>) | @jimmy2020
<br/>
February 22, 2021 | https://restofworld.org/2021/tesla-vs-tiny-cars/ | <a href="https://web.archive.org/web/*/https://restofworld.org/2021/tesla-vs-tiny-cars/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

			<!-- Article Start -->
			
<p><span>I</span>n Beijing’s southwestern outskirts, past a four-lane overpass with sidewalks as wide as the streets themselves, is Zhengyang Road. It has the usual banks, small convenience stores, and noodle houses of many areas in the capital, but it is set apart by a row of about a dozen shops all selling the same thing — tiny electric cars. The cars look, variously, like small Range Rovers, golf carts, trolley cars, or rickshaws with sheet-iron sides, and they are slow. Their fundamental attraction is their price — between $600 and $2,500 — and that drivers can charge them the same way they would a cell phone. They also come with the perks of being loosely regulated. These low-speed electric cars, nicknamed “elderly transport vehicles,” have an enormous market, made up mostly of people who earn very little. And in China, there are a lot of them — <a href="http://english.www.gov.cn/premier/news/202005/29/content_WS5ed058d2c6d0b3f0e9498f21.html">more than 40%</a> of the population, or some 600 million people, make around $150 per month.</p>



<p>On a Sunday afternoon in October, Zhengyang Road is filled with potential customers chatting with store owners.<strong> </strong>Outside a shop with a worn sign, a young couple with a child are in the midst of a heated conversation.<strong> </strong>They came on an electric scooter and are debating whether to leave with a tiny car.</p>



<p>“Don’t we need one for school pickups?” the woman argues. “The children won’t have to put up with the cold in winter.” Her scooter offers no protection from the weather other than oven-mitt-like gloves secured to its handlebars. Her husband counters, “The 1,000 renminbi [$150] quote was for normal batteries, but lithium ones can be five times that. Can’t you just add a windshield to your scooter instead?” The shop owner shows them a cheaper model — which is cheaper because it has no roof. He suggests putting a plastic covering on top.</p>


		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_25-40x27.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_25-768x432.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_25-400x267.jpg 400w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_25-600x400.jpg 600w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_25-1000x667.jpg 1000w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_25-1600x1067.jpg 1600w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_25-2800x1868.jpg 2800w, " sizes="(max-width: 640px) 100vw, 600px(max-width: 992px) calc(100vw - 40px), (max-width: 1140px) calc(100vw - 290px), calc(100vw - ((100vw - 640px)/2))" alt="A woman exits a tiny car near a subway station in Beijing.">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				
			</figcaption>
		</figure>


<p>Having decided that the future of mobility is electric,<strong> </strong>the Chinese government has subsidized sales of regular electric cars since 2010. With <a href="https://insideevs.com/news/394229/plugin-electric-car-sales-china-2019/">close to 1.18 million sold</a> in 2019, China accounts for just over half of electric-vehicle sales globally. Bill Russo, founder and CEO of advisory firm Automobility Limited, sees a “steady and solid rise” in China’s electric-vehicle sales generally. The country has set a top-down target for electric vehicles to <a href="http://energy.mit.edu/news/chinas-transition-to-electric-vehicles/">make up 40%</a> of car sales by 2030, and Russo thinks they’ll have no problem hitting this goal. Tiny cars,<strong> </strong>which first began appearing in the early 2010s,<strong> </strong>have more than double the sales of regular electric cars but have<strong> </strong>never benefited from subsidies. Nor do advertisements for them air on television — instead, they appear on Kuaishou, a short-video platform popular with people living outside China’s big cities. Alongside streamers selling plums by the thousands, and others telling viewers what long-haul trucker life is like, drivers show off their tiny cars. Su Hua, Kuaishou’s founder, has long maintained that his app’s users are not “cool,” unlike those on Douyin, the TikTok predecessor popular with China’s urban elite. Rather, they are ordinary — the kind of people who might be in the market for miniature cars.</p>



<p>As they don’t technically require licenses, tiny cars tend to be popular with migrant workers, who struggle to pay for driving lessons and other car-related costs. The elderly, too, find tiny cars attractive since, up until October of last year, people over 70 could not apply for a driving license in China. They’re also convenient for anybody who wants a car to pick up groceries or their kids from school: No tiny car is longer than 1.5 meters, and their speed tops out at between 40 and 56 kilometers an hour. They’re for the short trips of daily life, not for traveling from one side of the city to another.</p>



<p>Some cities have banned sales of tiny cars — Beijing did so in 2018. Their production isn’t regulated by the government, and since they can’t be insured in many parts of China, it can be difficult for other drivers to get a payout if a tiny car is involved in an accident. Because tiny-car drivers don’t need to take a driving test, other drivers complain, they often go the wrong way and weave in and out of traffic. But since enforcement is lax, sales have quietly resumed in Beijing over the past two years. These little electric cars now exist in a kind of regulatory gray zone.</p>


    <figure>
      <div>
				<ul>
					<li>		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_34-40x27.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_34-768x432.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_34-400x267.jpg 400w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_34-600x400.jpg 600w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_34-1000x667.jpg 1000w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_34-1600x1067.jpg 1600w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_34-2800x1868.jpg 2800w, " sizes="(max-width: 640px) calc(100vw + 40px), 50vw" alt="">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				
			</figcaption>
		</figure></li><li>		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_44-40x27.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_44-768x432.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_44-400x267.jpg 400w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_44-600x400.jpg 600w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_44-1000x667.jpg 1000w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_44-1600x1067.jpg 1600w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_44-2800x1868.jpg 2800w, " sizes="(max-width: 640px) calc(100vw + 40px), 50vw" alt="">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				
			</figcaption>
		</figure></li><li>		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_32-40x27.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_32-768x432.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_32-400x267.jpg 400w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_32-600x400.jpg 600w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_32-1000x667.jpg 1000w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_32-1600x1067.jpg 1600w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_32-2800x1868.jpg 2800w, " sizes="(max-width: 640px) calc(100vw + 40px), 50vw" alt="">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				
			</figcaption>
		</figure></li><li>		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_41-40x27.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_41-768x432.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_41-400x267.jpg 400w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_41-600x400.jpg 600w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_41-1000x667.jpg 1000w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_41-1600x1067.jpg 1600w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_41-2800x1868.jpg 2800w, " sizes="(max-width: 640px) calc(100vw + 40px), 50vw" alt="">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				
			</figcaption>
		</figure></li>
				</ul>
			</div>
      
    </figure>

    <figure>
      <div>
				<ul>
					<li>		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_43-40x27.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_43-768x432.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_43-400x267.jpg 400w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_43-600x400.jpg 600w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_43-1000x667.jpg 1000w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_43-1600x1067.jpg 1600w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_43-2800x1868.jpg 2800w, " sizes="(max-width: 640px) calc(100vw + 40px), 50vw" alt="">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				
			</figcaption>
		</figure></li><li>		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_38-40x27.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_38-768x432.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_38-400x267.jpg 400w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_38-600x400.jpg 600w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_38-1000x667.jpg 1000w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_38-1600x1067.jpg 1600w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_38-2800x1868.jpg 2800w, " sizes="(max-width: 640px) calc(100vw + 40px), 50vw" alt="">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				
			</figcaption>
		</figure></li><li>		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_31-40x27.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_31-768x432.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_31-400x267.jpg 400w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_31-600x400.jpg 600w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_31-1000x667.jpg 1000w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_31-1600x1067.jpg 1600w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_31-2800x1868.jpg 2800w, " sizes="(max-width: 640px) calc(100vw + 40px), 50vw" alt="">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				
			</figcaption>
		</figure></li><li>		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_47-40x27.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_47-768x432.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/EVBeijing_47-400x267.jpg 400w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_47-600x400.jpg 600w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_47-1000x667.jpg 1000w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_47-1600x1067.jpg 1600w, https://restofworld.org/wp-content/uploads/2021/02/EVBeijing_47-2800x1868.jpg 2800w, " sizes="(max-width: 640px) calc(100vw + 40px), 50vw" alt="">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				
			</figcaption>
		</figure></li>
				</ul>
			</div>
      <figcaption>Tiny cars come in a variety of styles and cost between $600 and $2,500.</figcaption>
    </figure>


<hr>



<p><strong>One of the</strong> smaller shops on Zhengyang Road is officially known as Xinlei Scooters, its name emblazoned in white characters on a large red sign above its doors. But inside, past rows of electric scooters, is a smaller, flimsier plaque above the counter announcing its other name: Shitou Cars. This subterfuge is necessary because the police could shut down the operation and confiscate its vehicles if the owners were caught selling tiny cars. Yet because enforcement hasn’t been strict of late, attempts at being covert go only so far: There are several tiny cars parked outside, an open-air showroom.</p>



<p>Xinlei/Shitou’s owner is a middle-aged man with pronounced cheekbones wearing a black tracksuit. He is more attentive to the phone calls he’s constantly receiving than the customers in the shop. It is his wife, with dyed-dark-brown hair and a pink coat, who maintains the sales patter: “I taught an auntie who had never driven a car before. She got the hang of it in three days.” She shows the cars outside to potential customers, opening their doors, instructing people to sit inside, and rolling down the windows. When two old men come in for repair services, her husband finally gets off the phone to deal with them. Meanwhile, two government functionaries in black uniforms pace down the street. They tell one owner to make his storefront tidier, but otherwise overlook the illicit operation.</p>



<p>Part of the reason why tiny cars are so popular is because there has not been an official decision on whether they need license plates. For regular cars, unfettered access to Beijing’s inner city — anywhere within the fifth ring road — is restricted to cars with Beijing plates. Licenses for gas cars are distributed through a special system so competitive that it has generated its own black market. License-plate holders can collect up to $2,700 a year by renting them to those who want to drive in the city. In addition to government subsidies, getting around some of the more onerous aspects of the licensing system is one of the main selling points for regular electric cars.</p>



<p>With Beijing temperatures reaching lows of 4 degrees Celsius in wintertime, Xinlei/Shitou has been selling,<strong> </strong>on average, two of its four-wheeled fully enclosed models every day, a saleswoman boasts. Younger couples prefer four wheels, she adds, while older people usually want three. When asked about the possibility of a tiny car being confiscated, she draws in a breath. “Don’t go on the main roads. Don’t make a business out of it,” she advises.</p>



<p>At least one of Zhengyang Road’s customers isn’t listening: Guo Caiying, who works primarily in the construction-supply industry,<strong> </strong>chauffeurs Fengtai residents around her district. She has a sticker on her tiny car’s back window with the phone number of her car dealer. Guo’s car looks like a golf cart, with cushioned brown seats enclosed by windows. A red <em>fudai</em>, a lucky charm, swings from the ceiling, its characters spelling out “peace.” There is enough space for two people to comfortably sit upright but not enough to extend your legs without hitting the plexiglass divider between driver and passenger. The car tops out at 40 kilometers an hour, and<strong> </strong>as a result, Guo never ventures beyond Fengtai — a borderland where urban and rural meet.</p>



<p>Guo wears the uniform of the countryside: a padded jacket. She is from Henan, a province 800 kilometers southwest of Beijing, and speaks its dialect. Guo starts taking calls from her regulars around 7 a.m., arriving at their door whenever they want to be picked up. She stops driving at 9 a.m., when the traffic police begin work. She characterizes her customers as “people with money who sit in offices.” Once, while in the middle of a trip, Guo saw a cop stop a car like hers. She kept driving, but dropped her passenger off before their destination. Now, if a customer calls her after 9:00, she sends her husband to pick them up with his electric scooter. He charges 75 cents (5RMB), which is half her price. Guo’s flat rate was fixed by the tiny-cab drivers who preceded her.</p>



<p>The economy of tiny cars depends on such informal practices.<strong> </strong>When asked whether she would consider undercutting other drivers, Guo is adamant. “No one can break the rules,” she says. There are local WeChat groups for tiny-car drivers that new owners are inducted into upon purchasing one. Within these groups, members swap information on the whereabouts of local cops and whether anyone has been fined or had their car taken away.</p>



<p>Despite the risks, Guo still thinks it’s worth being a tiny-car driver to make a little pocket money.<strong> </strong>Tiny cars are part of a last-mile economy that flourishes at the beginning and end of the workday. Many Fengtai residents are employed at the local high-tech park, which is host to thousands of businesses. It takes 20 minutes to walk from one end of the park to the other, a trip many would rather make by tiny car. The cars’ main competitors are share bikes, which are cheaper but lack space for luggage and can’t be split with a friend. Tiny cars are also more social — a feature Guo tries to …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://restofworld.org/2021/tesla-vs-tiny-cars/">https://restofworld.org/2021/tesla-vs-tiny-cars/</a></em></p>]]>
            </description>
            <link>https://restofworld.org/2021/tesla-vs-tiny-cars/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26232760</guid>
            <pubDate>Tue, 23 Feb 2021 01:12:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Full list of online communities for programmers]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26232689">thread link</a>) | @gruppo11
<br/>
February 22, 2021 | https://thehiveindex.com/topics/software-development/?r=hn | <a href="https://web.archive.org/web/*/https://thehiveindex.com/topics/software-development/?r=hn">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div class="page"><h2>About this Topic</h2><p>This is a list of communities dedicated to engineers, software developers, coders, and hackers. Some are online communities dedicated to a particular technology or programming language, and some are general purpose communities or those that help developers early in their career. The communities on this list are an excellent source of inspiration, knowledge-sharing, and networking.</p><h2><div><p>60</p><!-- --><p> Online </p><!-- --><p>Communities</p><!-- --><p> for Software Developers</p></div></h2><p>This topic's list is getting pretty long! Feel free to use the Platform/Feature filters above to cater the search to you.</p><div><p>Know a </p><!-- --><p>Software Development</p><!-- --><p> community that is not on this list yet? Please <a href="https://thehiveindex.com/submit/">submit it</a>!</p></div></div></div>]]>
            </description>
            <link>https://thehiveindex.com/topics/software-development/?r=hn</link>
            <guid isPermaLink="false">hacker-news-small-sites-26232689</guid>
            <pubDate>Tue, 23 Feb 2021 01:01:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Using an iOS Device for Programming in 2021 - You'd Be Surprised...]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26232616">thread link</a>) | @jcirclee
<br/>
February 22, 2021 | https://bitsrfr.com/programming-on-ios-2021/ | <a href="https://web.archive.org/web/*/https://bitsrfr.com/programming-on-ios-2021/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                    <!--kg-card-begin: markdown--><p>A couple of years ago, when someone asked if iOS could be used for software development, the answer was simply "no." That is no longer the case. While the answer is still not an emphatic, "yes," it is a solid, "it depends." In fact, you might be surprised to find out how far development on iOS has come in the past year or two.</p>
<p>For the majority of developers, iOS is a so-so development system - usable but not the best option. For developers who are just SSHing from server to server, or merging Git requests, iOS is great. For other developers, iOS just won't cut it. Programmers building 100,000-line Windows applications are still better off using Windows.</p>
<p>Beyond that, it comes down to personal preference. Some developers prefer iOS's multitasking, shortcuts, and configurations. Some don't.</p>
<p>Most people can get away with using an iPad as their daily driver at this point. However, a good portion of those people would still benefit by having a more traditional operating system (MacOS, Windows 10, Ubuntu, etc) for the times when an iPad doesn't cut it. Those times are becoming rare, but there are still users, for example, who might need to use an application that is only available on a desktop operating system. Another example, and this is a strange one, is iOS app development. It is one of the great paradoxes of modern technology that professional iOS developers cannot complete their work on iOS. Hopefully that will change soon.</p>
<p>Ultimately, without talking to you personally, I cannot say whether iOS would suit your development needs. I can, however, show you what the iPad has to offer. I will go over some general use cases. My hope is to help you choose the best system for you, or to at least provide some interesting insights.</p>
<hr>

<ol>
<li>
<p>There are several compilers available right in the App Store. The most popular of these are <a href="https://libterm.app/">LibTerm</a>, <a href="https://apps.apple.com/us/app/basic-programming-language/id1540244170">BASIC</a>, and <a href="https://seeless.app/">SeeLess</a>. It is possible, with those mentioned and other App Store apps, to compile code written in C, C++, BASIC, Rust, Java, Swift, Go, and other languages, right within the iOS operating system.</p>
</li>
<li>
<p>In addition to all of those compilers, the App Store has some excellent and powerful scripting apps. You can write and run Python (<a href="http://omz-software.com/pythonista/">Pythonista app</a>) and JavaScript (<a href="https://scriptable.app/">Scriptable app</a>) scripts right on an iPhone or iPad. You can also use Apple's visual scripting app, <a href="https://support.apple.com/guide/shortcuts/welcome/ios">Shortcuts</a>.</p>
</li>
<li>
<p><strong>It is even possible, and dare I say easy, to emulate a full version of Alpine Linux on iOS.</strong> The free and open source <a href="https://ish.app/">iSH</a> app makes it possible. So, yes, you can use Vim on your iPad.</p>
</li>
<li>
<p>The App Store also has no shortage of Git clients, SSH clients, IDEs, and text editors. <a href="https://workingcopyapp.com/">Working Copy</a> is a very popular Git client available on the App Store. <a href="https://blink.sh/">Blinkshell</a> is a popular, and open source, SSH client available on the App Store. <a href="https://playdotjs.com/">Play.js</a> is a Node.js and JavaScript IDE. <a href="https://www.textasticapp.com/">Textastic</a> is a feature-rich text editor (that I absolutely love!), complete with Markdown and great file syncing features including an SFTP client and an SSH client.</p>
</li>
<li>
<p><a href="https://sensortower.com/ios/rankings/top/ipad/us/developer-tools?date=2021-02-23">Developer Tools</a> is a <em>Top Category</em> in the App Store. This bodes well for developers who want to use iOS. First, it means Apple is serious about getting developers onto the operating system. Second, it makes finding great development tools very easy. <a href="https://bitsrfr.com/view-top-developer-tools-in-app-store/">Click here for instructions on viewing the <em>Developer Tools</em> category in the App Store</a>.</p>
</li>
</ol>
<hr>

<ol>
<li>
<p>The newness of development and programming on iOS is evident in the immaturity of many development tools. Most compilers in the App Store get not-so-great ratings, and many development apps have ugly or unintuitive user interfaces. The good news here is that there is a lot of opportunity for developers to build and sell new development apps.</p>
</li>
<li>
<p>It is good practice to develop on the operating system that you are developing for. If you are developing Windows apps, it is probably a bad idea to use iOS as your development environment.</p>
</li>
<li>
<p>If you are building 100,000+ line enterprise applications, you probably aren't going to feel quite at home if iOS is your main environment. On the same token, if you are developing enterprise applications, you probably have servers and central repositories that you are pushing to. In that case, it may very well suit you to develop on iOS.</p>
</li>
<li>
<p>iOS does not have good external monitor support. An external monitor can be connected, but it will only display in a 4:3 aspect ratio. However, the keyboard shortcuts and multitasking capabilities of iOS can, in many cases, reduce the need for multiple monitors.</p>
</li>
<li>
<p>The cost of an iPad plus a keyboard, and possibly a mouse/trackpad, is not low. If you are looking for an inexpensive development machine, an iOS device is probably not going to suit your needs.</p>
</li>
</ol>
<p>Bonus shortcoming: iOS is not open source. Though, not everyone will consider this a shortcoming.</p>
<hr>

<h2 id="professionalgamedeveloper">Professional Game Developer</h2>
<p>For the most part, professional game developers will not be well served using iOS as their main development environment. Popular game development tools like Unity and Godot are not available on iOS.</p>
<h2 id="hobbyistgamedeveloper">Hobbyist Game Developer</h2>
<p>Hobbyist and amateur game developers are also generally better off with a desktop operating system. That said, there have been games developed using, among others, the Pythonista app on iOS. <a href="https://github.com/Pythonista-Tools/Pythonista-Tools/blob/master/Games.md">Here is a nice GitHub repository of games made with Pythonista</a>.</p>
<h2 id="professionaliosdeveloper">Professional iOS Developer</h2>
<p>This is where things get a bit awkward. Professional iOS developers are not only better off using MacOS for development, it is a requirement. Xcode, which is only available on MacOS (but rumored to be coming to iOS), is required for publishing iOS apps to the Apple App Store.</p>
<h2 id="hobbyistiosdeveloper">Hobbyist iOS Developer</h2>
<p>This is really the sweet spot for using iOS as a main development machine. Hobbyist developers will have a field day - a field year! - coding on iOS. There are many great tools to explore and there is lots of innovation happening on iOS right now.</p>
<h2 id="networkandorsystemadministrator">Network and/or System Administrator</h2>
<p>A large portion of an administrator's job involves remotely accessing servers and clients. iOS has some great SSH tools. As long as admins are okay with doing that on an iPad screen, I think they can make a nice home on iOS. In fact, <a href="https://www.youtube.com/channel/UC8raOG7HXJoCUygx219fU4A">Christopher Lawley, an iOS YouTuber</a> (great channel if you want to learn more about using iOS for productivity and scripting) says he used an iPad Pro when he worked as a network administrator.</p>
<h2 id="newdeveloperlearningtoprogram">New Developer Learning to Program</h2>
<p>For someone learning to program, I suggest using a desktop operating system like MacOS, Windows, or Ubuntu, because you are going to find better support and help online when you need to troubleshoot or figure out how to do something. Developing on iOS is still new, so there's not a lot of documentation out there. Plus, you are going to learn more about the inner-workings of a computer on a system that exposes more of its inner-workings, and iOS exposes the least of any of them. That said, if you really just want to do it on iOS, I say go for it! And if your only computer is an iPad, then by all means hack the heck out of it!</p>
<h2 id="techdegreeseekingcollegestudent">Tech-Degree-Seeking College Student</h2>
<p>While the iPad can make a great note-taking, task-tracking, and web-surfing device, I would not recommend it as a daily driver for college students who are seeking a degree in computer science or information systems unless you are living on-campus and are willing to use campus-provided computers/labs for your work. Most college programming courses will require the use of Windows or MacOS, and the ability to install various tools on those systems. iPad is not quite flexible enough for that situation.</p>
<hr>
<p>Hopefully I have provided a few key pieces of information to help you decide whether your next development machine will be an iOS device. Will you or won't you? Can you think of any other pros or cons that I haven't mentioned? Let me know!</p>
<p>Until next time, happy hacking!</p>
<!--kg-card-end: markdown-->
                </div></div>]]>
            </description>
            <link>https://bitsrfr.com/programming-on-ios-2021/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26232616</guid>
            <pubDate>Tue, 23 Feb 2021 00:50:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Nintendo DS-TV-Out Restoration Project]]>
            </title>
            <description>
<![CDATA[
Score 176 | Comments 30 (<a href="https://news.ycombinator.com/item?id=26232600">thread link</a>) | @max-m
<br/>
February 22, 2021 | https://lostnintendohistory.github.io/DS-TV-OUT | <a href="https://web.archive.org/web/*/https://lostnintendohistory.github.io/DS-TV-OUT">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      <section id="main_content">
        

<h2 id="introduction">Introduction</h2>

<p>During late 2020, we discovered that the Nintendo DS Lite had a leftover feature in its SoC allowing it to easily have cheap hardware video output. With a little circuitry and some software hacks, we were able to restore it and make it usable for anyone. No FPGA’s, no bulky or cumbersome hardware. This mod is specially useful to revive consoles with only the lower screen, being able to watch the upper screen on your TV. Or to create a GBA Macro with additional TV Output.</p>

<center>
<img src="https://raw.githubusercontent.com/LostNintendoHistory/lostnintendohistory.github.io/main/img/NDSTVOUT/DSTVOUT.jpg" width="250" height="250"><br></center>
<center>
  <b>First iteration of the TV-OUT board in action</b>
  </center>

<h2 id="installation">Installation</h2>

<p>If you are just interested in installation, this is the current method <strong>while we work on simpler methods</strong> and more features you have requested:</p>

<ol>
  <li>Install the <a href="https://ezflash.sosuke.com/wiki/index.php/Flashme">flashME CFW</a> (Custom FirmWare) on your DS Lite</li>
  <li>Connect the Nintendo DS Lite’s upper screen flex to the PCB board.</li>
  <li>Donwload the “NDS TV OUT ENABLE.nds” homebrew from the <a href="https://github.com/LostNintendoHistory/Lost-NDS-TV">NDS TV OUT repo</a></li>
  <li>Download <a href="https://github.com/DS-Homebrew/TWiLightMenu/releases">Twilight Menu</a></li>
  <li>Copy both the NDS TV OUT ENABLE and Twilight Menu .nds files to a flashcart.</li>
  <li>Use flashme to autoboot into the flashcart. You can do this by pressing A + B + Start + Select while booting. Run Twilight menu, and from there, run the enabler homebrew.</li>
  <li>The console will return to Twilight Menu. Now you can use the buttons on the board to swap between the different screen modes (Upper Screen to TV, Bottom Screen to TV, etc) and launch your games.</li>
</ol>

<hr>

<h2 id="software">Software</h2>

<p>The retail firmware of the Nintendo DS Lite disables this specific feature early in the boot process. To reenable it, we use a custom firmware like flashme, which is very easy to install and is required only once, plus a homebrew. Despite that, we are working on an even simpler solution to make it available to as many people as possible, our own custom firmware which integrates patches to enable this feature directly on boot. Additionally, we are currently working with homebrew developers to integrate control of this new feature into existing software for the DS Lite.</p>

<h2 id="hardware">Hardware</h2>

<p>This feature is only found on the Nintendo DS Lite. Nintendo DS Phat does not contain this feature nor does the Nintendo DSi. It is important to remark that <strong>this is not the same hardware</strong> found on Devkits or other special units. This hardware feature is present in virtually <strong>every single Nintendo DS Lite</strong> out there. The reason why it was left there is unknown, but as said before, it is not related to development units, those use a different video capture hardware. Perhaps Nintendo imagined the Nintendo Switch as early as 2006?</p>

<center><img src="https://raw.githubusercontent.com/LostNintendoHistory/lostnintendohistory.github.io/main/img/NDSTVOUT/PCB_Rev_11.png" width="350" height="400"></center>

<p>We only need a few extra hardware components to make this video signal usable. You will be able to download the schematics and gerber files for our open hardware circuit board <a href="https://github.com/LostNintendoHistory/Lost-NDS-TV">from the repository</a>. The latest version is revision 1.2 which fixes some minor issues with a component in the board.</p>

<center>
<img src="https://raw.githubusercontent.com/LostNintendoHistory/lostnintendohistory.github.io/main/img/NDSTVOUT/Prototype.jpg" width="350" height="350"><br>
</center>
<center>
  <b>First prototype and tests before designing a proper board</b></center>


<p>The final, production-ready board contains a DAC (Digital to Analogue Converter) which turns the 10 bits digital signal at 16.7 MHz provided by the DS Lite into a proper analogue signal. This signal then goes through an operational amplifier and it’s ready to be delivered to your nearest TV trough composite video.</p>

<p>We are currently considering creating an additional PCB revision which would allow to install the mod on consoles without lossing a working upper screen.</p>


      </section>
    </div></div>]]>
            </description>
            <link>https://lostnintendohistory.github.io/DS-TV-OUT</link>
            <guid isPermaLink="false">hacker-news-small-sites-26232600</guid>
            <pubDate>Tue, 23 Feb 2021 00:48:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Bombard Story]]>
            </title>
            <description>
<![CDATA[
Score 68 | Comments 26 (<a href="https://news.ycombinator.com/item?id=26232597">thread link</a>) | @jbergstroem
<br/>
February 22, 2021 | https://greatestadventurers.com/the-bombard-story/ | <a href="https://web.archive.org/web/*/https://greatestadventurers.com/the-bombard-story/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="page">
				<div id="content">
			
	<div id="primary">
		<main id="main">
			
<article id="post-618" itemtype="https://schema.org/CreativeWork" itemscope="">
	<div>
					
			
		<div itemprop="text">
			<p><strong>The Bombard Story</strong> is the account of <a href="https://en.wikipedia.org/wiki/Alain_Bombard">Alain Bombard’s</a> amazing journey in 1952 across the <a href="http://greatestadventurers.com/the-north-west-passage-by-roal-amundsen/">Atlantic</a> on a small 14-foot inflatable boat. Alain Bombard left without food or fresh water and sailed 4.400 kilometers. He lost 25 kg. but proved his point: Man can actually survive on ocean water for an extended period of time!</p>
<figure id="attachment_619" aria-describedby="caption-attachment-619"><img loading="lazy" src="http://greatestadventurers.com/wp-content/uploads/2021/02/TheBombardStory1953.jpg" alt="The Bombard Story" width="300" height="203" data-src="http://greatestadventurers.com/wp-content/uploads/2021/02/TheBombardStory1953.jpg" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="><figcaption id="caption-attachment-619">In this small vessel Bombard sailed across the Atlantic – without freshwater</figcaption></figure>
<p>As a doctor, Bombard was concerned about the hundreds of deaths at sea every year related to sailors drinking ocean water. He developed the theory that humans can not just survive but live for years on seawater. This sounds very strange, but his big idea was to begin drinking seawater, while you are still hydrated – and in small quantities. It turns out that saltwater is only dangerous if you are dehydrated and suddenly drink large amounts of it. – The way shipwrecked sailors typically would do when they run out of fresh water. From the book:</p>
<blockquote><p>For some time I had made a study of the resistance of the human organism to privations and had convinced myself that it was possible for an individual to survive beyond the limits normally assigned by physiological science. I had paid particular attention to the case histories of political deportees, prisoners, and undernourished populations. But, with my background as a doctor, for whom the teachings of science remain a dead letter unless they can find practical application, my theoretical studies only seemed to lead to the question: ‘What use can made of this knowledge?’</p></blockquote>
<p>Bombard ate spoonfuls of plankton that he collected in a fine net and he also drank juice made from pressed fish he caught along the way. Sound disgusting, but the man survived and he might have discovered an important piece of knowledge for survival on the ocean.</p>
<p>Download the free PDF e-book here (223 pages/38MB):</p>
<h3><img loading="lazy" src="http://greatestadventurers.com/wp-content/uploads/2020/08/PDF-download-e1597850191432.png" alt="" width="35" height="35" data-src="http://greatestadventurers.com/wp-content/uploads/2020/08/PDF-download-e1597850191432.png" data-old-src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==">&nbsp;<a href="http://greatestadventurers.com/wp-content/uploads/2021/02/The-Bombard-Story-1953.pdf">The Bombard Story 1953</a></h3>

		</div>

				
			</div>
</article>

			

					</main>
	</div>

	

	</div>
</div></div>]]>
            </description>
            <link>https://greatestadventurers.com/the-bombard-story/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26232597</guid>
            <pubDate>Tue, 23 Feb 2021 00:48:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[TagTime Web: stochastic time tracking web app]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26232552">thread link</a>) | @reimbar
<br/>
February 22, 2021 | https://smitop.com/post/ttw/ | <a href="https://web.archive.org/web/*/https://smitop.com/post/ttw/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><article itemprop="articleBody" id="content"><p>Recently I’ve been working on <a href="https://ttw.smitop.com/">TagTime Web</a> (my server, but you can host your own). It’s an <a href="https://github.com/smittyvb/ttw">open-source</a> web-based version of the original <a href="https://github.com/tagtime/TagTime">TagTime</a>. It uses <em>stochastic time tracking</em>, which randomly samples what you are doing through out the day (on average 45 minutes apart by default). <a href="http://messymatters.com/tagtime/">This page</a> explains it quite well. I have also made <a href="https://www.youtube.com/watch?v=cJpE018QEkQ">a video</a> about how this type of time tracking works, and a <a href="https://www.youtube.com/watch?v=FwpF0fqh7uU">demonstration video</a>.</p><p>Here are some features it has:</p><ul><li>Each tag gets a unique colour based on the name of it</li><li>Tag autocomplete</li><li>Dark mode</li><li>Mobile support</li><li>Offline support (it is a PWA)</li><li>Filtering pings</li><li>Charts!</li></ul><p><a href="https://ttw.smitop.com/">Check it out!</a></p></article></div></div>]]>
            </description>
            <link>https://smitop.com/post/ttw/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26232552</guid>
            <pubDate>Tue, 23 Feb 2021 00:40:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[On the Basics of (Statistical) Modeling]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26232497">thread link</a>) | @dcu
<br/>
February 22, 2021 | https://blog.chewxy.com/2021/02/17/modeling-basics/ | <a href="https://web.archive.org/web/*/https://blog.chewxy.com/2021/02/17/modeling-basics/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main">
<div>
<div>
<article role="main">
<p>I had a very interesting chat with a few data science students yesterday. Part of the chat involved the idea of statistical modeling. Throughout the chat, it occured to me that the students didn’t have a very good grasp of what modeling is. To their credit, they were proficient in the techniques of linear regression, and deep learning, but I got the sense that they were very much pushing buttons and watching things happen rather than understanding what they were actually doing. There was no sense of a big picture view.</p>
<p>This has been happening quite a lot lately. I find it somewhat alarming. This blog post is a semi-transcript of what I said last night. It aims to be as simple as possible.</p>

<p>A model is a representation of reality. It’s what we think reality looks like.</p>
<p>For example, we live on Planet Earth, in a solar system. We can build models of our Solar System. Here’s an example of a model of our Solar System.</p>

<div>
<figure itemprop="associatedMedia" itemscope="" itemtype="http://schema.org/ImageObject">
<p><img itemprop="thumbnail" src="https://blog.chewxy.com/wp-content/uploads/2021/modeling/orrery.jpg" alt="An Orrery of our Solar System. Photograph by Smabs Sputzer, published with a CC BY 2.0 licence. Source: https://www.flickr.com/photos/10413717@N08/7527137708">
</p>
<a href="https://blog.chewxy.com/wp-content/uploads/2021/modeling/orrery.jpg" itemprop="contentUrl"></a>
<figcaption>
<p>An Orrery of our Solar System. Photograph by Smabs Sputzer, published with a CC BY 2.0 licence. Source: https://www.flickr.com/photos/<a href="https://blog.chewxy.com/cdn-cgi/l/email-protection" data-cfemail="427372767371757375020c727a">[email&nbsp;protected]</a>/7527137708</p>
</figcaption>
</figure>
</div>
<p>As a child, such models of our solar system endlessly fascinate. I would spend hours thinking about how the planets moved. I would play and watch the planets spin around its spindles. I understood that there was a force called gravity that caused the planets to orbit the sun. As I grew older, the physical model of our Solar System is gradually replaced by <a href="https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion">three laws</a>.</p>
<p>There are parts of the model we can study:</p>
<ul>
<li>What is the shape of the orbits</li>
<li>How the planets move</li>
<li>Why do planets move thus</li>
</ul>
<p>These are sub-models of the model of our Solar System. The shape of the orbit is given by Kepler’s First Law. How planets move is given by Kepler’s Second and Third Law. Why do planets move thus is given by Newtonian mechanics.</p>
<p>Each of Kepler’s laws have an equation governing them. So we can say the equations model our Solar System. The equations are the model of our Solar System. These equations describe something static (the shape of the orbit) and represent something dynamic (how the planets move). What used to be physical motion in a physical model can now be written down on a piece of paper, an equation representing the real thing.</p>

<p>There are many ways of making a model. Sometimes it’s useful to have a physical understanding of something. <a href="https://blog.chewxy.com/2021/01/09/sars-cov-2/">I built the 2019-SARS-CoV-2 virion</a> to help me have a better understanding of the coronavirus that caused the pandemic in 2020. Now, I’m no biologist, so my model is crude. My model is extremely physical. Despite this, it gave me an understanding of how a mRNA vaccine might work. It gave me confidence over what actual proper scientists are doing.</p>
<p>So making a physical model is one way. But what if we want something more rigorous? The usual way is to resort to some sort of formalism. Various fields have various formalisms. For example, in chemistry, you use chemical equations. However, the most common formalism would be a mathematical equation. Maths equations are used in physics, economics, biology, and many other fields.</p>
<p>So how do you create an equation that becomes a model of something? There are two ways:</p>
<ol>
<li>Generate an equation.</li>
<li>Find an equation from data.</li>
</ol>
<p>In the large scheme of things, both of these amount to the same thing: generating an equation. I’ll talk about that in a later section. For now, when I say “model generation” I mean generating an equation that models reality.</p>
<h2 id="how-do-you-generate-an-equation">How Do You Generate An Equation?</h2>
<p>The simplest way of generating an equation is to randomly generate one by writing down symbols on a piece of paper.</p>
<p>That’s daft, you say. You’d be hard pressed to find a equation that adequately describes the situation!</p>
<p>That’s why most model generation comes from <a href="https://en.wikipedia.org/wiki/First_principle">first principles</a>. In using the Solar System example, if we accept Newton’s law of universal graviatation ($F = G{\frac {Mm}{r^{2}}}$), then we can work our way to find Kepler’s third law. Kepler’s other laws require other first principles such as trigonometry.</p>
<p>The key is that you understand a subject well enough that you may generate further models about the subject using your basic understanding.</p>
<p>However, random generation has its place. In fact, from here on, whenever I write “generate a model”, you may think of a person randomly coming up with math equations.</p>
<h2 id="how-do-you-find-an-equation-from-data">How Do You Find An Equation From Data?</h2>
<p>There may be cases where first principles may not be used. This is often the case in new fields.</p>
<p>So the next best way is to find an equation from data. There are many ways to do them. Regression analysis is one such way of finding an equation from data. Let’s look at a simple example of linear regression with one variable.</p>
<p>The fundamental idea of a linear regression is that you plot your data points, and draw a straight line through the plot (line A). Each data point would be some distance away from line. Sum those distances up and square them. Call it the “error”. Now draw another line through the plot (line B) and find the errors of B. Keep doing until you find a line that has the lowest amount of errors.</p>
<p>This is the line-of-best-fit. Given that all straight lines on a plot can be described by an equation that looks like $y = mx +C$, the equation that describes the line of best fit (e.g. $y=2x + 1$) is the model.</p>
<p>This idea of model building extends all the way to deep neural networks. They key being the model is built by looking at the relationships between the variables that make up a data point.</p>

<p>The whole point of creating a model is to reflect reality. I could well come up with a model of gravity that says this: all objects exert a force on each other that is quadratic on the distance between them - written as $F = d(a, b)^2$. But does this reflect reality? No.</p>
<p>How do I know this? I know this because I can test it. I can collect data, and then check if the data fits my model. In the silly example above, it’s trivial to check with a counterpoint: I am able push something off my desk. If the force is solely based on distance, then as my hand approaches the object, the force should get smaller and smaller to the point that I am unable to affect the object.</p>
<p>In many courses about regression analyses, the R² values are often taught to students as a measure of how good one’s model is<span><span></span><span>I find this to be mostly true about "data science" courses/bootcamps, but not more traditional uni level course on regression/economics/statistics</span></span>. It’s not! A R² value is how good the fit of data to the line is. In some sense you may think of this as the inverse of “how good is your model” . It’s more “how much data fits in your model”. Indeed, the R² value is indicative of how much variance of your data is covered by the model.</p>
<p>This is not to say that the courses are wrong. The statement that “R² tells you how good your model is” is a very subtle statement. Let’s unpack them. Let’s say you found a line of best fit that is described by the formula $y = 2x + 15$. This is the model that we have “generated”. Now we want to see how much of reality (our dataset) is described by the model. It is in this sense that R² represents the notion of how “good” a model is.</p>
<p>Now it seems a bit weird, given that we used the dataset to find the line of best fit, and then we turn around and say we generated a model, not let’s test to see how good it is. There seems to be a bit of circular logic to it. However there are a lot of theoretical work on why the line of best fit found by a ordinary least squares (OLS) regression is a good model “generator” - the wikipedia article on <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">OLS</a> covers quite a bit, as does the <a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem">Gauss-Markov theorem</a> article. Most textbooks also lay out proofs of why OLS estimators are BLUE (Best Linear Unbiased Estimator).</p>
<p>Here I want to point out that “reality” is itself just a sample. The dataset that you use to generate a model is just a sample. This is where conscious sampling of data is important. Let’s imagine we are training a machine learning system to recognize faces. My social circle are White or Asian. So if I ask my social circle to send me some photographs of their faces to train a machine learning system, then the machine learning system would not be able to recognize faces that are not White or Asian! Clearly this is not a good representation of reality.</p>
<p>This trivial example only scratches the surface of equitable conscious collection of sample “reality” for the sake of model building. This topic is a very deep topic and it’ll take many blog posts to talk about it. So I shall leave it be for now.</p>
<p>In more advanced machine learning modeling systems (e.g: deep learning systems), it is common to split the dataset into “training” and “testing” datasets. The model is trained on the training dataset and tested on the testing dataset. This is to ensure that the model does not only model “reality” that is in the training dataset, but can also generalize to previously unseen data.</p>
<p>What I am trying to convey here is that sanity checks against reality is a good thing. We should do them more often.</p>

<p>Having said that, we have to accept that models are just that - models. They are not reality. George Box had a good saying:</p>
<blockquote>
<p>All models are wrong. But some are useful.</p>
</blockquote>
<p>The key is to find a model that is useful enough for what you need to do. Let the natural philosophers worry about the most accurate models of reality.</p>

<p>Humanity is always generating models. Individually, in our brains, we generate internal models that are corrected every second of the day. Consider catching a ball. Your brain generates a model of physical reality - no equations here - telling us where the ball is going to be. As the ball arcs through the air, we update the models in our brain, getting better and better predictions, resulting in us catching the ball. Or in my case, the ball lands on my face.</p>
<p>Communally, we generate models too. The invention of writing and speech allowed us to share models with other individuals. We started using things like maths equations to make our meanings clear. Our …</p></article></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.chewxy.com/2021/02/17/modeling-basics/">https://blog.chewxy.com/2021/02/17/modeling-basics/</a></em></p>]]>
            </description>
            <link>https://blog.chewxy.com/2021/02/17/modeling-basics/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26232497</guid>
            <pubDate>Tue, 23 Feb 2021 00:33:09 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Code Quality with Mypy]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26232204">thread link</a>) | @max-hoffman
<br/>
February 22, 2021 | https://www.dolthub.com/blog/2021-02-22-mypy-and-doltpy/ | <a href="https://web.archive.org/web/*/https://www.dolthub.com/blog/2021-02-22-mypy-and-doltpy/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-cy="blog-post-text"><h2>Dolt</h2>
<p>Dolt is an SQL-database with Git-versioning.
The goal of <a href="https://github.com/dolthub/doltpy">Doltpy</a>, in
concert with <a href="http://github.com/dolthub/dolt">Dolt</a>, is to solve
reproducibility and versioning problems for data and machine
learning engineers using Python.</p>
<h2>Mypy</h2>
<p>Mypy was created by Guido van Rossum, the primary developer of the
Python language, as a way to apply
<a href="https://www.python.org/dev/peps/pep-0008/">PEP standards</a> to Python source
code. When lines of code are added to the Python core libraries,
their respective mypy stubs are updated lockstep.</p>
<p>So when we fix mypy errors we are enforcing rules of the Python type system.
This point is subtle but important: mypy errors when your code is not doing
what you've declared it should do. Static checking can't anticipate what
input your code will be fed at runtime, but as a developer you can write
code that is self-consistent with function and type signatures.</p>
<p>Adding type-hints without enforcement is a common anti-pattern.
Mypy is separately installed from Python and its typing modules -- it
is up to the developer to actually validate type-hints after adding them.
Code with contradictory typing documentation can mislead
developers and users alike. Mypy is that bridge between type-aesthetics
and type-correctness.</p>
<p>Mypy involves three main modules:</p>
<ul>
<li><a href="https://github.com/python/mypy">mypy</a>: A source code parsing and
applying PEP constraints.</li>
<li><a href="https://github.com/python/typeshed">typeshed</a>: Type-stubs core and
3rd party libraries; code whose implementations are
correctness-checked when used in new code.</li>
<li><a href="https://github.com/python/typing">typing</a>: Modules for compatibility
between python versions.</li>
</ul>
<p>All three of these modules are regularly used when using mypy (<code>typing</code>
less so if you only suport one Python version). One addendum is that you
can define custom type stubs in your own code, in the same manner <code>typeshed</code>
provides type stubs for popular pip packages, like
<a href="https://github.com/python/typeshed/tree/master/stubs/boto/boto">boto</a>
and
<a href="https://github.com/python/typeshed/tree/master/stubs/requests/requests">requests</a>.</p>
<h2>Examples</h2>
<h3>Typing inconsistency</h3>
<p>We use <code>mypy</code> in Doltpy 2.0 to help ensure code-quality. Below is an
an example from Doltpy 1.0 to demonstrate mypy in action:</p>
<div data-language="python"><pre><code><span>def</span> <span>log</span><span>(</span>self<span>,</span> number<span>:</span> <span>int</span> <span>=</span> <span>None</span><span>,</span> commit<span>:</span> <span>str</span> <span>=</span> <span>None</span><span>)</span> <span>-</span><span>&gt;</span> OrderedDict<span>:</span>
    args <span>=</span> <span>[</span><span>"log"</span><span>]</span><span>:</span>
    <span>if</span> number<span>:</span>
        args<span>.</span>extend<span>(</span><span>[</span><span>"--number"</span><span>,</span> number<span>]</span><span>)</span></code></pre></div>
<p>Inside the <code>log</code> function signature, <code>number: int</code> correctly reflects the developer intent,
but <code>args: List[str]</code> disallows integers. This means that calling <code>Dolt.log(1)</code>
fails with an error, while <code>Dolt.log("1")</code> succeeds.</p>
<p>The intended behavior is clear, and mypy preemptively notices the inconsistency:</p>
<div data-language="bash"><pre><code><span>&gt;</span> python -m mypy <span>.</span>
example.py:4: error: List item <span>1</span> has incompatible <span>type</span> <span>"int"</span><span>;</span> expected <span>"str"</span></code></pre></div>
<p>fixing the type inconsistency restores the expected behavior:</p>
<div data-language="python"><pre><code><span>def</span> <span>log</span><span>(</span>self<span>,</span> number<span>:</span> <span>int</span> <span>=</span> <span>None</span><span>,</span> commit<span>:</span> <span>str</span> <span>=</span> <span>None</span><span>)</span> <span>-</span><span>&gt;</span> OrderedDict<span>:</span>
    args <span>=</span> <span>[</span><span>"log"</span><span>]</span><span>:</span>
    <span>if</span> number<span>:</span>
    args<span>.</span>extend<span>(</span><span>[</span><span>"--number"</span><span>,</span> <span>str</span><span>(</span>number<span>)</span><span>]</span><span>)</span></code></pre></div>
<p>and makes mypy happy:</p>
<div data-language="bash"><pre><code><span>&gt;</span> python -m mypy <span>.</span>
Success: no issues found <span>in</span> <span>1</span> <span>source</span> <span>file</span></code></pre></div>
<h3>Custom typing stub</h3>
<p>As a final example, here are first few lines for a custom type stub of the
<code>doltpy.cli.Dolt</code>
<a href="https://github.com/dolthub/doltpy/blob/master/doltpy/types/dolt.py%5D">class</a>
in doltpy:</p>
<div data-language="python"><pre><code><span>class</span> <span>DoltT</span><span>(</span>Generic<span>[</span>_T<span>]</span><span>)</span><span>:</span>
    _repo_dir<span>:</span> <span>str</span>

    <span>@abc<span>.</span>abstractmethod</span>
    <span>def</span> <span>repo_dir</span><span>(</span>self<span>)</span><span>:</span>
        <span>.</span><span>.</span><span>.</span>

    <span>@staticmethod</span>
    <span>@abc<span>.</span>abstractmethod</span>
    <span>def</span> <span>init</span><span>(</span>repo_dir<span>:</span> Optional<span>[</span><span>str</span><span>]</span> <span>=</span> <span>None</span><span>)</span> <span>-</span><span>&gt;</span> <span>"Dolt"</span><span>:</span>  
        <span>.</span><span>.</span><span>.</span></code></pre></div>
<p>After defining <code>class Dolt(DoltT)</code>, mypy will enforce our interface
the same way mypy enforces standard library and other 3rd party type
stubs. As a plus, code editors like VSCode should also give hints for
function signature definitions.</p>
<h2>Summary</h2>
<p>In this post I touched on the utility of using type-hints
with mypy, and the comparative pitfalls of using type-hints without.
We used specific examples from Doltpy to highlight the nature
of static type-checking, and how we use mypy in production at Dolthub.</p>
<p>Are you interested in learning more about Dolt and Doltpy?
<a href="https://docs.dolthub.com/getting-started/installation">Try it out</a>.
If you have any questions, come chat with us in our
<a href="https://discord.com/invite/RFwfYpu">Discord</a>.</p></div></div>]]>
            </description>
            <link>https://www.dolthub.com/blog/2021-02-22-mypy-and-doltpy/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26232204</guid>
            <pubDate>Mon, 22 Feb 2021 23:49:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Guy Making a Million a Year Delivering Cookies to His Hood]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 2 (<a href="https://news.ycombinator.com/item?id=26232139">thread link</a>) | @freakandgeek
<br/>
February 22, 2021 | https://businessideas.ai/food-delivery/ | <a href="https://web.archive.org/web/*/https://businessideas.ai/food-delivery/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            
<div>
    <main>
            <article>
    
    <div>
                <h3 id="a-deep-dive-on-opportunities-in-the-food-delivery-space"><em>A Deep Dive on Opportunities in the Food Delivery Space</em></h3><p>In this crazy Covid world we have seen the acceleration in delivery businesses and food is no different.</p><p>Of course by now you have probably heard about <a href="https://www.doordash.com/en-US">DoorDash</a>, <a href="https://www.grubhub.com/">GrubHub</a>, <a href="https://www.ubereats.com/">Uber Eats</a> and the like, but what if I told you there are tons of opportunities for us regular non-venture backed folk to also get in on this action?</p><p><em>Would you be interested?</em></p><p>Let's dig into this deeper.</p><h2 id="from-bc-to-ac">From BC to AC</h2><p>As you have probably personally experienced, it can be hard (or even impossible) to go out to eat like back in the good old BC days (<em>Before Covid</em>).</p><p>Drive through restaurants, take-out and delivery businesses have been propelled into accelerating growth as a result.</p><p>You should see the daily lines at my local Chick Fil-A. It is absolutely bonkers.</p><figure><blockquote><p lang="en" dir="ltr">I just waited in line at chick-fil-a for about 30 mins. <a href="https://t.co/I2gx9PeBxA">pic.twitter.com/I2gx9PeBxA</a></p>— Natisha Lance (@NatishaLance) <a href="https://twitter.com/NatishaLance/status/1360760913679286275?ref_src=twsrc%5Etfw">February 14, 2021</a></blockquote>

</figure><p>Despite the passing of time and new vaccines — masks, social distancing, and a limited public discourse lifestyle are still norms in many parts of the world.</p><p>In fact Dr. Fauci says we need to wear masks through 2022 and the CDC is now <a href="https://www.cbsnews.com/news/double-face-mask-covid-19-cdc/">actually suggesting wearing two masks</a>.</p><figure><blockquote><p lang="en" dir="ltr">Dr. Fauci says it's possible Americans will need to wear masks in 2022 even as the US may reach "a significant degree of normality" by year's end<a href="https://t.co/XLRwWIPMC9">https://t.co/XLRwWIPMC9</a> <a href="https://t.co/B3ntTpsKDH">pic.twitter.com/B3ntTpsKDH</a></p>— CNN Breaking News (@cnnbrk) <a href="https://twitter.com/cnnbrk/status/1363521124089294849?ref_src=twsrc%5Etfw">February 21, 2021</a></blockquote>

</figure><p>This trend will stick for some time. You may as well profit off of it.</p><figure><img src="https://cdn.getmidnight.com/e8ec67ee74ddbbc523b1af64568b015a/2021/02/crave-cookie.jpg" alt="Crave Cookie is Making Bank"></figure><h2 id="local-delivery-businesses-are-making-the-dough">Local Delivery Businesses are Making the Dough</h2><p>Would you imagine that a small family business of just a few handful of employees in a small California town selling nothing but two types of cookies is a million dollar business?</p><p>This is not science fiction. It is a reality.</p><p><a href="https://cravecookie.com/">Crave Cookie</a> is that business.</p><p>They have a very simple business model that you can replicate in your town. Let's walk through some of the things Crave Cookie has done that shows how this can be a <a href="https://businessideas.ai/business-ideas/">great business idea</a>.</p><p>Crave Cookie always has chocolate chip cookies and just one additional flavor of the week. Even a one person business could pull that off. It is inspiring to realize that huge menus are not needed to be a million dollar business.</p><p>Interestingly even after achieving huge success, they still have stuck with the two choices only model.</p><figure><img src="https://cdn.getmidnight.com/e8ec67ee74ddbbc523b1af64568b015a/2021/02/crave-pricing.png" alt="Crave Cookie Pricing"></figure><h3 id="smart-pricing-strategy">Smart Pricing Strategy</h3><p>You cannot just order one cookie from Crave as they sell <strong>boxes of cookies only</strong>. So the minimum order is $13.</p><p>They also charge for delivery with a reasonable price that does not feel like gouging. Also interestingly they point out:</p><blockquote>"We charge a flat $3.0 delivery fee whether you order 1 box or 100."</blockquote><p>So there is plenty of motivation for larger orders as the delivery fee feels close to free.</p><h3 id="special-sauce">Special Sauce</h3><p>If cookies delivered to your door is not enough of a differentiator, Crave also advertises that they deliver <strong>straight out of the oven</strong> and thus your cookies will <strong>arrive warm</strong>.</p><p>Can't you just about taste that?</p><p>This is a smart <em>special sauce ninja move</em>. Who does not dream of warm cookies delivered to their doorstep?</p><h2 id="similar-cookie-delivery-companies">Similar Cookie Delivery Companies</h2><p>If you want to dig into this model some more, there are a few interesting companies to check out:</p><ul><li><a href="https://insomniacookies.com/">https://insomniacookies.com</a></li><li><a href="https://www.cookiedelivery.com/">https://www.cookiedelivery.com</a></li><li><a href="https://www.midnightcookieco.com/">https://www.midnightcookieco.com</a></li></ul><h2 id="getting-started">Getting Started</h2><p>Some extra considerations about this business model:</p><ul><li>There are special local laws related to selling food. Research and understand these in depth before doing anything else.</li><li>Start by selling in your own neighborhood. 1) This is easier to network, spread the word, and gather feedback &amp; 2) Your deliveries are easily done.</li><li>Use <a href="https://nextdoor.com/">Nextdoor</a> and Facebook Marketplace to market your services and get the word out.</li><li>Do you have a special family recipe that would do well as a delivery service?</li></ul><h3 id="crave-cookie-media-for-further-study">Crave Cookie Media for Further Study</h3><p><em>Writing Code to Sell $200,000/Month of Cookies with Sam Eaton of Crave Cookie</em></p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/-CpVIetacIM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></figure><p><em>Valley fans crave cookies, company expands</em></p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/2GX4OME_g64?start=7&amp;feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></figure><h2 id="airbnb-food-service-model">Airbnb Food Service Model</h2><p>Another new wrinkle we have seen recently is what I call the <em>Airbnb food service model</em> (people in the know call these <a href="https://roaminghunger.com/blog/15623/ghost-kitchens-everything-you-must-know/">Ghost Kitchens</a>.)</p><p>Imagine selling millions of burgers to customers without having a kitchen, buying any beef, or having any employees?</p><figure><img src="https://cdn.getmidnight.com/e8ec67ee74ddbbc523b1af64568b015a/2021/02/mrbeast-burger.jpg" alt="The current menu at Mr Beast Burger"></figure><h3 id="introducing-mrbeast-burger">Introducing MrBeast Burger</h3><p>MrBeast in case you are unaware, is a <a href="https://www.youtube.com/channel/UCX6OQ3DkcsbYNE6H8uQQuVA">hugely popular YouTuber</a> with over 50 million subscribers.</p><p>MrBeast Burger launched with 300 locations. That would cost billions you say! How did they do it?</p><p>MrBeast Burger has their burgers made by partnering restaurants and they leverage local delivery networks to deliver the orders.</p><figure><img src="https://cdn.getmidnight.com/e8ec67ee74ddbbc523b1af64568b015a/2021/02/mrbeast-app.jpg" alt=""><figcaption>MrBeast Burger App</figcaption></figure><p>They simply provide the ordering front end with their app and the traffic based on MrBeast's gigantic popularity and marketing skills.</p><p>Just like Airbnb does not make beds or own any real estate — MrBeast owns no cows or flips any burgers.</p><p>Obviously few of us have the kind of reach to pull anything like this off. MrBeast Burger is selling nearly one million $$$ of burgers a month on the foundation of his huge audience.</p><p>But this is such a fascinating example of a creative and wildly successful food business, we had to cover that here.</p><p><em>I Opened A Restaurant That Pays You To Eat At It</em></p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/dg2Ag3e8W-Q?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></figure><p><em>Food Theory: MrBeast Burger Is NOT What You Think...</em></p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/uNLwgYG4EdA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></figure><p><em>How MrBeast Makes $720,000/Month Dropshipping Burgers</em></p><figure><iframe width="200" height="113" src="https://www.youtube.com/embed/K3OuI9E0-EA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></figure><h2 id="food-subscription-businesses">Food Subscription Businesses</h2><p>Subscription box businesses are not new and maybe the business model has even peaked — at least before Covid turned many into remote workers who rarely leave the house (<em>and thus buy more online</em>).</p><p>But subscription food businesses are worth mentioning as a consideration —especially as a possible side hustle.</p><p>Since this model is already around a decade old you will need to consider that the riches are in the niches. Do something unique and do it with some panache.</p><p>Consider for example that there are already at least a dozen <em>beef jerky subscription boxes</em>, so starting another one of those would be questionable decision making. </p><p>Picking food products for your subscription business like nothing else in existence ensures less competition.</p><h2 id="have-you-ever-tried-japanese-candy">Have You Ever Tried Japanese Candy?</h2><figure><img src="https://cdn.getmidnight.com/e8ec67ee74ddbbc523b1af64568b015a/2021/02/candy-japan.jpg" alt="Candy Japan Homepage"></figure><p>A good example of this is <a href="https://www.candyjapan.com/">Candy Japan</a>. Over the years this side project has sold about one million $$$ of Japanese candy.</p><p>In the beginning the business gathered a lot of interest and business mostly based on the utter uniqueness of the offering.</p><p>But with time the word gets out and now there are a few Japanese candy subscription box competitors out there (<a href="https://tokyotreat.com/">https://tokyotreat.com</a>, <a href="https://japancrate.com/">https://japancrate.com</a>, &amp; <a href="https://www.japancandybox.com/">https://www.japancandybox.com</a> to name a few)</p><p>Still even after several years — this is a profitable side project the owner says <a href="https://www.candyjapan.com/life-in-japan/what-it-costs-to-live-in-japan">covers most of his living expenses in Japan</a>.</p><p>If you can find an untapped niche, this could be a nice earning side business for someone. Just pick an interesting and unique food item.</p><p>CBD brownies anyone?</p><h3 id="more-about-candy-japan">More About Candy Japan</h3><ul><li><a href="https://www.starterstory.com/stories/starting-a-japanese-candy-subscription-service?upgrade=true&amp;successful_subscribe=true&amp;src=email_wall">Starting A Japanese Candy Subscription Service</a></li><li><a href="https://www.candyjapan.com/blog">Candy Japan Blog</a> (<em>plenty of insights on marketing efforts and other hindsights in running the business</em>)</li></ul><h3 id="one-more-thing-about-subscription-box-businesses">One More Thing about Subscription Box Businesses</h3><p>In one word...recurring revenue. Recurring revenue is a beautiful thing to the entrepreneur, as you can accurately predict your monthly revenue stream. This makes planning, marketing, and other elements of your business easier. You know what your revenues are going to be and usually can predict your growth as well.</p><p>This is the same reason why the Software as a Service (SaaS) model is so valuable and loved. Subscription box business are almost like SaaS for people who cannot code.</p><h2 id="selling-food-on-etsy-is-actually-a-thing">Selling Food on Etsy is Actually a Thing</h2><p>I don't know about you, but I just learned about how big food selling was on Etsy.</p><p>It makes sense to me now, but it blows my mind how much food selling is going on there. Just <a href="https://www.etsy.com/search?q=brownies">do a search on "brownies"</a> and you will see what I mean.</p><figure><img src="https://cdn.getmidnight.com/e8ec67ee74ddbbc523b1af64568b015a/2021/02/etsy-brownies.jpg" alt="Etsy Brownies"><figcaption>You get a brownie, and you, and you!</figcaption></figure><p>What is great about this is you could use Etsy for quick (and cheap!) experiments for gathering market intelligence. You could get answers on:</p><ul><li>What foods are popular to buy online?</li><li>What photos work best to drive orders?</li><li>What categories are the most successful?</li><li>What copy is most effective to sell my food item?</li></ul><h3 id="how-to-dominate-a-crowded-market-be-different-be-bold">How to Dominate a Crowded Market? Be Different, Be Bold</h3><p>As you can see there are a ton of brownie sellers on Etsy. But I found one seller who has sold well over $100K worth of treats.</p><figure><img src="https://cdn.getmidnight.com/e8ec67ee74ddbbc523b1af64568b015a/2021/02/dulce-baskets.jpg" alt=""></figure><p>Selling brownie gift baskets of close to $100 is a good way to increase your profit and sales volume. This is a separator from the pack that are mostly selling one batch of brownies at a time.</p><p>These make a unique gift for your favorite chocolate lover — whereas a simple batch of brownies is not exactly making a statement.</p><h3 id="more-on-etsy-food">More on Etsy Food</h3><figure><a href="https://www.etsy.com/legal/policy/food-and-edible-items/239327355460"><div><p>Food and Edible Items - Our House Rules | Etsy</p><p>Find the perfect handmade gift, vintage &amp; on-trend clothes, unique jewelry, and more… lots more.</p><p><img src="https://www.etsy.com/images/favicon.ico"></p></div><p><img src="https://i.etsystatic.com/11266858/d/il/c8a5e3/2871445103/il_340x270.2871445103_ot9g.jpg?version=0"></p></a></figure><h2 id="join-us-for-more">Join Us for More</h2><p>Because you liked this report — please <a href="https://businessideas.ai/#/portal/signup">become a Business Ideas subscriber</a>.</p><p>Continue getting more insights on interesting &amp; powerful business ideas to take your life to the next level.</p><figure><img src="https://cdn.getmidnight.com/e8ec67ee74ddbbc523b1af64568b015a/2021/02/business-ideas-logo-512-2.png" alt=""></figure>
                        <section>
                            <h2>Enjoying these posts? Subscribe for more</h2>
                            
                            <br>
                            
                        </section>
    </div>
        
</article>                    
                </main>
</div>
        </div></div>]]>
            </description>
            <link>https://businessideas.ai/food-delivery/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26232139</guid>
            <pubDate>Mon, 22 Feb 2021 23:40:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Functional vs. OO: The Debate That Imprecise Language Destroyed]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 1 (<a href="https://news.ycombinator.com/item?id=26232049">thread link</a>) | @BerislavLopac
<br/>
February 22, 2021 | https://chelseatroy.com/2021/02/22/functional-vs-oo-the-debate-that-imprecise-language-destroyed/ | <a href="https://web.archive.org/web/*/https://chelseatroy.com/2021/02/22/functional-vs-oo-the-debate-that-imprecise-language-destroyed/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="page">

			<!-- #masthead -->

			<div id="content">

	<section id="primary">
		<main id="main" role="main">

		
			<article id="post-8919">
	
	
		

	<div>
		<p><span><span>Reading Time: </span> <span>7</span> <span>minutes</span></span></p><h3>“Should I use functional or object-oriented programming?”</h3>



<p>A student asked me this as I closed out one of my Python Programming lectures in January.  In this context, the student meant “what should I use for my upcoming homework assignment,” but I didn’t realize that at first. On <em>most</em> occasions when I hear this question—usually in professional circles—the unspoken subtext is “always.” <em>Which of these two styles should I swear by as <strong>the</strong> right way to write code?</em> </p>



<p> A second later, the student clarified “…on this assignment, I mean.”</p>



<p>Too little, too late. My gears were already turning.</p>



<div><div>
<div><div>
<div><div>
<h3>This is the first in what will be a two-part series:</h3>



<ol><li><strong>Words Mean Things (this post)</strong></li><li>But actually, how do you choose what to use?</li></ol>




</div></div>
</div></div>
</div></div>



<h3>The Functional/OO debate has, in my view, two big problems.</h3>



<ol><li><strong>The “all or nothing” assumption.</strong> I have a colleague who wants all his code to be functional code, full stop. I also have two former colleagues who swear by, and I quote, “lots of little objects.” I don’t fall into either camp because I think there are better questions than “<em>which of these two tools should I use <strong>always</strong>,” </em>with more insightful answers that have more potential to make us better programmers.</li><li><strong>Absolute terminological butchery.</strong> We have these two terms: “functional” and “object-oriented”, that we use interchangeably with other would-be synonyms, except that they don’t describe the same thing. And what’s with the fact that “functional” is just “functional” and “object” has “-oriented” tacked onto the end? Why isn’t it opposite “function-oriented?” We’re <em>super</em> imprecise about the way we discuss these ideas, and then we make new programmers feel stupid because they cannot picture the leaping triple-axle we <em>obviously meant</em> to perform when what we <em>actually</em> did was slip and fall on the ice. </li></ol>



<div><figure><a href="https://i1.wp.com/chelseatroy.com/wp-content/uploads/2021/02/ice-skate-fail.jpg?ssl=1"><img data-attachment-id="8945" data-permalink="https://chelseatroy.com/2021/02/22/functional-vs-oo-the-debate-that-imprecise-language-destroyed/ice-skate-fail/" data-orig-file="https://i1.wp.com/chelseatroy.com/wp-content/uploads/2021/02/ice-skate-fail.jpg?fit=480%2C360&amp;ssl=1" data-orig-size="480,360" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="ice-skate-fail" data-image-description="" data-medium-file="https://i1.wp.com/chelseatroy.com/wp-content/uploads/2021/02/ice-skate-fail.jpg?fit=300%2C225&amp;ssl=1" data-large-file="https://i1.wp.com/chelseatroy.com/wp-content/uploads/2021/02/ice-skate-fail.jpg?fit=480%2C360&amp;ssl=1" loading="lazy" width="480" height="360" src="https://i1.wp.com/chelseatroy.com/wp-content/uploads/2021/02/ice-skate-fail.jpg?resize=480%2C360&amp;ssl=1" alt="" srcset="https://i1.wp.com/chelseatroy.com/wp-content/uploads/2021/02/ice-skate-fail.jpg?w=480&amp;ssl=1 480w, https://i1.wp.com/chelseatroy.com/wp-content/uploads/2021/02/ice-skate-fail.jpg?resize=300%2C225&amp;ssl=1 300w" sizes="(max-width: 480px) 100vw, 480px" data-recalc-dims="1"></a><figcaption>Actual footage of a programming lecture that ends with a SEGFAULT and “well, you all understand the intuition anyway”</figcaption></figure></div>



<p>This grinds my gears to nubbins: the way we teach, instruct, and describe in lazy, platitudinous, imprecise ways, and then suggest that some people just aren’t smart enough to get it. </p>



<h3>So I got angry and tried to fix it. </h3>



<p>Here’s a video explanation. The explanation references Python because I recorded it with my Python students top-of-mind. That said, Python also serves as a useful model language for discussing this topic. </p>



<figure><div>
<p><span><iframe width="723" height="407" src="https://www.youtube.com/embed/HfEM1MKfwFw?version=3&amp;rel=1&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;fs=1&amp;hl=en-US&amp;autohide=2&amp;wmode=transparent" allowfullscreen="true" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span></p>
</div><figcaption>Yes, I made sure it has captions.</figcaption></figure>



<p>If you don’t want to watch me scream and gesticulate while my off-kilter bowtie tragically shortens my neck for the camera, you can see a more language-agnostic version of the point I’m making here in the text below.</p>



<h3 id="So-first-of-all">So first of all</h3>






	
	


<p>Let’s talk about terminology, because there’s a massive lack of precision on this floating around the programming community, and it makes these concepts harder to understand than they have to be.</p>



<h3 id="What-is-a-Paradigm?">What is a Paradigm?</h3>



<p>Let’s talk about two different&nbsp;<strong>paradigms</strong>: different ideas about&nbsp;<strong>how</strong>&nbsp;solving a programming problem can work.</p>



<ol><li><strong>imperative</strong>: describes a way of thinking about a programming problem. Specifically, thinking about a programming problem in terms of how to perform tasks and how to manage state</li><li><strong>declarative</strong>: describes a way of thinking about a programming problem. Specifically, thinking about a programming problem in terms of what&nbsp;<em>output</em>&nbsp;we want, without having to know the details of how we got there.</li></ol>



<p>Different programming languages adopt each of these to different degrees.</p>



<p>For example, in Ruby, when you want to write a web app, you inherit from classes (usually defined by a framework like Rails or Sinatra) that are specifically designed to help you keep track of&nbsp;<em>state</em>&nbsp;(database records, attributes on objects) and&nbsp;<em>behavior</em>&nbsp;(which requests are supposed to route to what actions).</p>



<p>By comparison, in SQL, you write a statement declaring what data you want out of the database and how you want it organized. SQL decides for you how to get the thing you want—whether to use indices, what order to do things in, et cetera—without bothering you to specify that information.</p>



<h3 id="How-do-we-implement-the-paradigms?">How do we implement the paradigms?</h3>



<p><strong>Chiefly, programming languages&nbsp;<em>implement</em>&nbsp;these two paradigms with object-based implementations or function-based implementations</strong>.</p>



<ol><li><strong>object-based</strong>: describes the implementation of a solution in code. Specifically, a solution that depends on the instantiation of, use of, and inheritance from objects.</li><li><strong>function-based</strong>: describes the implementation of a solution in code. Specifically, a solution that depends on the definition of, use of, and passing of functions to functions.</li></ol>



<p>These are not the only ways to implement the paradigms. Huge, common example: SQL is largely not a functional language. You aren’t passing functions around to functions. But it does implement the declarative&nbsp;<em>paradigm</em>. The paradigms and the implementations are not equivalent things.</p>



<h3 id="What-does-&quot;oriented&quot;-mean?">What does “-oriented” mean?</h3>



<p><strong>When a programming language is&nbsp;<em>oriented</em>&nbsp;in a certain direction, it means that the constructs available in that language loan themselves better to one implementation or the other.</strong></p>



<ul><li><strong>object-oriented</strong>: describes a programming language. Specifically, one whose constructs make&nbsp;<strong>object-based</strong>&nbsp;solutions convenient to implement.</li><li><strong>functionally-oriented</strong>: describes a programming language. Specifically, one whose constructs make&nbsp;<strong>function-based</strong>&nbsp;solutions convenient to implement.</li></ul>



<p>Now, it is&nbsp;<em>possible</em>&nbsp;(though kinda difficult) to make a language that&nbsp;<em>only</em>&nbsp;supports&nbsp;<em>one</em>&nbsp;type of solution. Haskell is pretty close to a&nbsp;<strong>functional</strong>&nbsp;language. Alloy is pretty close to an&nbsp;<strong>object</strong>&nbsp;language. However, the utility of a language drops off pretty fast if it&nbsp;<em>only</em>&nbsp;does one or the other because both are at least a&nbsp;<em>little</em>&nbsp;useful in most programming areas. So&nbsp;<strong>-oriented</strong>&nbsp;means “one is more convenient, but you can kinda do both.”</p>



<p>Colloquial terminology butchers this by referring to functionally oriented languages as “functional” and languages that are oriented either way as “multi paradigm” despite the fact that the&nbsp;<em>paradigm</em>&nbsp;is an&nbsp;<em>idea</em>, not an&nbsp;<em>implementation</em>, that a programming language does not have a&nbsp;<em>paradigm</em>, and that a language is <em>X-oriented</em> already denotes that it supports multiple implementation strategies.</p>



<p>(I firmly believe that the reason that this functional vs. OOP idea is so hard for people is that we use the same term to mean six different things, two of which are sometimes opposites.)</p>



<p>I found this chart to provide a visual, but it&nbsp;<em>also</em>&nbsp;butchered the terminology, so I fixed it:</p>



<figure><a href="https://i0.wp.com/chelseatroy.com/wp-content/uploads/2021/02/functional_vs_oo.png?ssl=1"><img data-attachment-id="8927" data-permalink="https://chelseatroy.com/2021/02/22/functional-vs-oo-the-debate-that-imprecise-language-destroyed/functional_vs_oo/" data-orig-file="https://i0.wp.com/chelseatroy.com/wp-content/uploads/2021/02/functional_vs_oo.png?fit=1272%2C838&amp;ssl=1" data-orig-size="1272,838" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="functional_vs_oo" data-image-description="" data-medium-file="https://i0.wp.com/chelseatroy.com/wp-content/uploads/2021/02/functional_vs_oo.png?fit=300%2C198&amp;ssl=1" data-large-file="https://i0.wp.com/chelseatroy.com/wp-content/uploads/2021/02/functional_vs_oo.png?fit=723%2C477&amp;ssl=1" loading="lazy" width="723" height="477" src="https://i0.wp.com/chelseatroy.com/wp-content/uploads/2021/02/functional_vs_oo.png?resize=723%2C477&amp;ssl=1" alt="" srcset="https://i0.wp.com/chelseatroy.com/wp-content/uploads/2021/02/functional_vs_oo.png?resize=1024%2C675&amp;ssl=1 1024w, https://i0.wp.com/chelseatroy.com/wp-content/uploads/2021/02/functional_vs_oo.png?resize=300%2C198&amp;ssl=1 300w, https://i0.wp.com/chelseatroy.com/wp-content/uploads/2021/02/functional_vs_oo.png?resize=768%2C506&amp;ssl=1 768w, https://i0.wp.com/chelseatroy.com/wp-content/uploads/2021/02/functional_vs_oo.png?w=1272&amp;ssl=1 1272w" sizes="(max-width: 723px) 100vw, 723px" data-recalc-dims="1"></a><figcaption>Original image from&nbsp;<a rel="noreferrer noopener" href="https://docs.microsoft.com/en-us/dotnet/standard/linq/functional-vs-imperative-programming#:~:text=Functional%20programming%20is%20a%20form,support%20imperative%20(procedural)%20programming." target="_blank">here</a>,&nbsp;but I had to annotate it to fix the terminology.</figcaption></figure>



<h3 id="Python-is-an-object-oriented-language.">Python is an object-oriented language.</h3>



<p>People will argue with me on this point that Python is “in fact, dual-paradigm.” I disagree for reasons that you are now intimately familiar with. Python is object-oriented. You&nbsp;<em>can</em>&nbsp;do functional programming in it. It is designed, however, to prioritize object-based programming. BDFL Guido Van Rossum has said this himself on several occasions (<a href="https://python-history.blogspot.com/2009/04/origins-of-pythons-functional-features.html">here, straight from the horse’s mouth, don’t @ me</a>). So far the core team has not reversed any of the major technical decisions driven by that point of view. </p>



<p>I would also argue that Python is not only <em>not at all unique</em> in its support for multiple paradigms, but also <em>a far cry from the most graceful language</em> at supporting multiple paradigms. This is fine: as I mentioned, the design goals of the language have never included functional support, or even grace in general (<a href="https://chelseatroy.com/2021/01/31/why-learn-python/">see here, we talked about this</a>). But like, when people get on a high horse about this, please don’t be taken in.</p>



<h3>Fine, Chelsea. Anyway, which one should I use?</h3>



<p>Functional programming.</p>



<p>I’m kidding: based on the fact that we just spent a thousand words getting clear on what we’re even <em>talking</em> about, you’d be right to predict that my answer to this question has a lot more nuance than that. Meanwhile, though, I try to keep things pithy and digestible around here. So we’ll call it a night and dig into the “what to use” question in the next post.</p>



<h3>If you liked this piece, you might also like:</h3>



<p><a href="https://chelseatroy.com/2021/01/14/quantifying-technical-debt/">The last time I just absolutely snapped on imprecise terminology in tech</a> (on this occasion about “technical debt”)</p>



<p><a href="https://chelseatroy.com/category/programming/programming-concepts/debugging/">The debugging category</a>&nbsp;(people seem to like this and struggle to find similar content elsewhere)</p>



<p><a href="https://chelseatroy.com/2020/11/30/rubyconf-workshop-analyzing-risk-in-a-software-system/">The risk analysis workshop</a>&nbsp;(4 out of 5 “Jimi Hendrix of [insert programming language here]”s approve!)</p>
			</div><!-- .entry-content -->

	<!-- .entry-footer -->

</article><!-- #post-## -->

			
<!-- #comments -->

		
		</main><!-- #main -->
	</section><!-- #primary -->

	<!-- #secondary -->

	</div><!-- #content -->

	<!-- #colophon -->
</div></div>]]>
            </description>
            <link>https://chelseatroy.com/2021/02/22/functional-vs-oo-the-debate-that-imprecise-language-destroyed/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26232049</guid>
            <pubDate>Mon, 22 Feb 2021 23:28:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Robert’s Rules Suck: Why We Can’t Make Change Until We Change the System]]>
            </title>
            <description>
<![CDATA[
Score 27 | Comments 27 (<a href="https://news.ycombinator.com/item?id=26231837">thread link</a>) | @sep_field
<br/>
February 22, 2021 | https://aninjusticemag.com/roberts-rules-suck-47b689f3c48f | <a href="https://web.archive.org/web/*/https://aninjusticemag.com/roberts-rules-suck-47b689f3c48f">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><div><div><div><h2 id="99a2">Why We Can’t Make Change Until We Change the System</h2><div><div><div><div><a href="https://martywilder-44820.medium.com/?source=post_page-----47b689f3c48f--------------------------------" rel="noopener"><div><p><img alt="Marty Wilder" src="https://miro.medium.com/fit/c/96/96/0*bbAuchMAUj_x330g" width="48" height="48"></p></div></a></div></div></div></div></div><figure><div role="button" tabindex="0"><div><div><p><img alt="Close up of a judge’s gavel on the block" src="https://miro.medium.com/max/19200/1*xqElYYwIds8NChYEdGxH7Q.jpeg" width="9600" height="5304" srcset="https://miro.medium.com/max/552/1*xqElYYwIds8NChYEdGxH7Q.jpeg 276w, https://miro.medium.com/max/1104/1*xqElYYwIds8NChYEdGxH7Q.jpeg 552w, https://miro.medium.com/max/1280/1*xqElYYwIds8NChYEdGxH7Q.jpeg 640w, https://miro.medium.com/max/1400/1*xqElYYwIds8NChYEdGxH7Q.jpeg 700w" sizes="700px" data-old-src="https://miro.medium.com/max/60/1*xqElYYwIds8NChYEdGxH7Q.jpeg?q=20"></p></div></div></div><figcaption>Photo by <a href="https://unsplash.com/@bill_oxford?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener">Bill Oxford</a> on <a href="https://unsplash.com/s/photos/gavel?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener">Unsplash</a></figcaption></figure><h2 id="b772">Taking Action</h2><p id="0a32">I was ready to do more than take a knee or carry a cardboard sign. I felt like it was time for me to move beyond protesting and get involved, somehow, in creating change. That was why I joined an ad hoc committee formed by our city council to address police policy. Think global, act local. At last, I felt hope. I felt like maybe I can make a difference that matters. And then I faced reality, and was shocked by how bad it is.</p><p id="9857">I was not naïve going into this. I fully expected that whatever good policy change our committee was able to craft might be diluted or rejected by the city council in the end, or that the Police Chief might find ways to circumvent them, or that even if enacted, the police union would still allow officers who violate those policies to be exonerated. With that in mind, I stayed focused on the long haul. I wanted to craft strong and demanding policies that could become part of a list of demands to be relentlessly rallied before the city officials until they are adopted. I kept my eye on forming alliances with others on the committee that could grow into lasting coalitions. This committee, to me, was only the beginning.</p><p id="8c03">It looked promising. The city had called on 13 civic organizations representing BIPOC and other marginalized communities. I was there on behalf of a nonprofit that services transgender and gender non-conforming folx. There are 30 members, in all. As we went through brief introductions at the first meeting, I was encouraged. The committee is facilitated by a team of three individuals, including a Black woman who is the Equity &amp; Access Coordinator for the county. She and I conversed at the outset about the challenges of facilitating a group the size of ours over Zoom due to the pandemic. We talked about setting group agreements. We talked about equity over equality and elevating voices that were underprivileged, especially those of women of color. I mentioned the need to give each member enough of a platform to feel seen and recognized at the beginning, even though that would be a big time investment, because it would save time in the long run by deterring potential internal conflicts. I also expressed my opinion that we would need to work in smaller subcommittees in order to be effective.</p><h2 id="b7ce">Thwarted by the System</h2><p id="ff5a">But then we ran into two great obstacles; public meetings law and Robert’s Rules of Order. The first curtailed our ability to network and converse with each other on the committee. The second is an infuriating silencer that obstructs everything I have come to learn about good problem-solving and decision-making. I’ll start with public records law because that is more straightforward. The law states that we must have a quorum, in our case 16 or more, of members present at each meeting. Each meeting must be posted and publicly broadcast in real time. Since we were airing our meeting over Zoom due to the pandemic, the meetings are live streamed and recorded. But because the live stream and recording do not show the chat box, we cannot use that feature to communicate things like consent with what the active speaker is saying, or to ask clarifying questions. It all has to be voiced to be recorded. Furthermore, since only the Zoom hosts can see non-verbal signs, we cannot use the raised hands nor the Yes/No functions built into Zoom. Instead we have to wait the three to four minutes it takes for the host to read off each of the 30 names, wait for the person to unmute, and get a recorded response with a “yes”, “no”, or “abstain” for every motion we attempt to pass. We have yet to do this without someone in the middle asking for the motion to be restated. I don’t think there is anyone involved who is not finding this irritating, but everyone seems resigned to endure it.</p><p id="7e09">The worst aspect of the way the city is interpreting public records law is that they have instructed all of us not to communicate with each other as a group outside of the public meetings. Email correspondence, file sharing, and social media can all become violations of public records law. If there are 16 or more of us involved, or even if there is not a quorum but we are discussing content that affects decision-making, it all needs to be publicly broadcast. While I can understand the reasoning behind these stipulations, where does that leave us? We are 30 members of very diverse parts of our city, we don’t know each other very well, and many of us have never served on a committee like this before. How are we supposed to work together? We have been reduced, effectively, to responding in the moment. We cannot even use file sharing to look at and consider ideas or share resources except by going through the facilitation team.</p><p id="ed4b">The facilitation team has directed us to send all communication to them and they will disperse information to the committee. That would be fine, if it were simply a procedure to go through. But the facilitation team does not simply pass along information. They hold onto it, decide whether or not it is information that should or should not be shared, sometimes rewrite or re-position it, and pack everything into one overwhelming information packet that we receive on Friday night before a Monday meeting. One reason behind this is that all the documentation must also be publicly posted alongside the meeting announcement. It also consolidates things so that committee members do not get bogged down with frequent emails. The danger is in editing out or misconstruing some of our voices, often those that most need to be heard. Also, there is the disabling effect of leaving us inactive and unable to work productively in the two weeks between meetings. I find myself struggling to resist the idea that the facilitation team has an expected outcome for us, and they are guiding the committee to meet their expectations.</p><figure><div role="button" tabindex="0"><div><div><p><img alt="A laptop computer showing a large group of faces of people conferencing on a Zoom call. A coffee mug sits beside the laptop." src="https://miro.medium.com/max/3840/1*RRlLLZmsdfl8gIxEPJTMmg.jpeg" width="1920" height="1440" srcset="https://miro.medium.com/max/552/1*RRlLLZmsdfl8gIxEPJTMmg.jpeg 276w, https://miro.medium.com/max/1104/1*RRlLLZmsdfl8gIxEPJTMmg.jpeg 552w, https://miro.medium.com/max/1280/1*RRlLLZmsdfl8gIxEPJTMmg.jpeg 640w, https://miro.medium.com/max/1400/1*RRlLLZmsdfl8gIxEPJTMmg.jpeg 700w" sizes="700px" data-old-src="https://miro.medium.com/max/60/1*RRlLLZmsdfl8gIxEPJTMmg.jpeg?q=20"></p></div></div></div><figcaption>Photo by <a href="https://unsplash.com/@cwmonty?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener">Chris Montgomery</a> on <a href="https://unsplash.com/s/photos/zoom-meeting?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener">Unsplash</a></figcaption></figure><h2 id="a8e4">Killing the Creativity</h2><p id="4127">Then we come to Robert’s Rules of Order. For those who are not familiar, this is a set of meeting protocols that dates back to before the Civil War. Basically, the facilitator calls on people to speak, one at a time, without interruptions for a given amount of time. In our meetings it is 3 minutes. When someone wants to propose a decision, they make a “motion”. Someone else must second that motion. Then the facilitator calls the vote. The motion, the person who presented it, the person who seconded it and the total numbers of votes: yes, no, and abstain, are all recorded. That’s it in a nutshell.</p><p id="9cee">What is missing from Robert’s Rules of Order is the magic of good problem-solving. There is no room for contained chaos, a free flow of energy, voices, and ideas. I taught engineering design in high schools for ten years. One of the most enjoyable, and innovatively genius, aspects of problem-solving is brainstorming. Brainstorming is meant to be messy. It’s a chance to air everything out and look at it from as many different angles as you can dream up. You start to notice patterns and connections. Someone poses something “crazy” and it piques your interest. Then there is this very important concept called “piling on.” Piling on happens when your idea sparks a new idea in my mind. I share my idea and that, in turn, sparks a new idea for someone else. This phase of problem-solving is divergent and for traditionalists, it goes against every fiber in their “we need to narrow this down” trajectory. But the traditional “narrowing down” linear approach leads to very limited and narrow solutions. Whereas, the creativity and mutual discovery of the brainstorming process culminates in a kind of magical synthesis of ideas and approaches. The team then needs to choose what approach they want to take. It might be evident in a general idea that rises up out of the chaos in a way that is unifying and electrifying, which leads to a much smoother process as you narrow in on the solution. Or you may see two or three different approaches that you either need to choose between as a group, or make a choice to split up and try all of them. Besides being a good way to get fresh and, at times, brilliant ideas, brainstorming also results in better teamwork because everyone was able to contribute fully and feel seen, heard, and involved.</p><p id="5d12">But the public meeting format has no room for that. We can’t even utilize Zoom break-out groups because the public would need to see all of the break-out groups simultaneously. Here is where it becomes de-humanizing to me. There is no place to <em>form</em> ideas in the public meeting. Members are expected to <em>bring</em> ideas, pre-fabricated, and see how they hold up to a vote. I used to function like that, bringing my ideas to the table in a battle for the best articulated argument to slay all others and take the lead. Then I studied feminism. When you value the people and the process, everything changes. It’s no longer a contest to see who has the best idea. It becomes about the whole, all of us together as a group, facing a problem and learning from each other as we go. I don’t want to presume to bring a solution that will address everyone’s needs. I want to hear from others and I want my thoughts to be affected by those stories. I want our collective ideas to <em>become</em> as we meet. What if our government were like that? What if the premise was that no one has the answer going in, but if we all bring our perspectives together and listen to one another, the answer will take form out of the collective whole? I know. It sounds ludicrous given the extreme partisan attacks that happen all the time in our current system. But once you have experienced this kind of collective solution-making even on a small scale; it can make you a believer.</p><h2 id="778c">White Supremacy Playbook</h2><p id="1307">Robert’s Rules of Order and the general meeting protocols really do fall right in line with what we know about white supremacist culture. What I mean by this is that we value this methodology and purport …</p></div></div></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://aninjusticemag.com/roberts-rules-suck-47b689f3c48f">https://aninjusticemag.com/roberts-rules-suck-47b689f3c48f</a></em></p>]]>
            </description>
            <link>https://aninjusticemag.com/roberts-rules-suck-47b689f3c48f</link>
            <guid isPermaLink="false">hacker-news-small-sites-26231837</guid>
            <pubDate>Mon, 22 Feb 2021 22:59:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Ways to Reduce the Pain of Deploys]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26231563">thread link</a>) | @dlevine
<br/>
February 22, 2021 | https://blog.config.ly/post/643856862947721216/ways-to-reduce-the-pain-of-deploys | <a href="https://web.archive.org/web/*/https://blog.config.ly/post/643856862947721216/ways-to-reduce-the-pain-of-deploys">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>In our <a href="https://href.li/?http://the-cost-of-pushing-code">last post</a>, we talked about the cost of deploying code. In this post, we are going to talk about ways to reduce the pain of deploys. There are a few different strategies to accomplish this. The first is to use tooling that makes deploys faster and easier. The second is to reduce the weight of deploys. And the third is to remove some things from the deploy cycle altogether. By combining these strategies, you can reduce the pain involved with deploying and speed up your release cycle.<b><br></b></p><h2>Improving deployment tooling</h2><p>There are a number of ways to reduce deployment costs by rolling out improved tooling. Platforms that enable continuous integration (CI) and continuous deployment (CD) can make deploys both easier and much faster. They can automatically run your tests or other verification processes, preventing bad code from being deployed in the first place. When the code is ready to go, it can be deployed automatically. This will also reduce the amount of code that is deployed at a time, allowing you to isolate bad commits and fix them faster.&nbsp;</p><p>A more recent development is continuous verification (CV), which gives you tools that you can use to determine the quality of a new build when it is deployed in production. In some cases, you can deploy a canary build to assess performance and error rates in relation to the stable build. In the case when bad code is deployed, CV tools can let you know of issues that arise and even potentially roll back automatically.</p><p>Platforms such as <a href="https://href.li/?https://circleci.com">CircleCI</a>, <a href="https://href.li/?https://harness.io">Harness</a>, <a href="https://href.li/?https://www.jenkins.io/">Jenkins</a>, and <a href="https://href.li/?https://spinnaker.io/">Spinnaker</a> can all help you to enable CI/CD and even CV.</p><h2>Reducing deployment weight</h2><p>A second way to make deploys less costly is by decreasing deployment weight. This can be accomplished by adopting CI/CD, as it will be possible to deploy smaller updates more frequently. However, another way to do this is by breaking up your application into smaller services that can be verified and deployed independently. This will enable you to deploy smaller pieces of code with a smaller blast radius.</p><p>This is not to say that microservices are a panacea; the Internet is littered with horror stories of microservice migrations that took years and introduced a host of new problems. A lot of companies either don’t put enough work into setting up tooling for microservices, don’t think enough about standards or interoperability, or divide their app into way too many microservices. However, if you think intelligently about how to split up your app and do the migration deliberately and thoughtfully, the result will be a more robust application that is easier to deploy and test.</p><p>There are a number of platforms that can make it easier to split up your app into smaller services. These include <a href="https://href.li/?https://spring.io/projects/spring-boot">Spring Boot</a>, <a href="https://href.li/?https://akka.io/">Akka</a>, <a href="https://href.li/?https://kubernetes.io/">Kubernetes</a>, <a href="https://href.li/?https://www.docker.com/">Docker</a>, <a href="https://href.li/?https://prometheus.io/">Prometheus</a>, and many more.</p><h2>Removing data from the deploy process</h2><p>The final strategy for making deploys less costly is to remove some aspects of your code from the deployment process. When they are starting out, developers often hardcode various things in code, including configuration constants, user-facing text, and even feature toggles. While this is often easiest in the short term and will work when deploys are fast, it becomes progressively more annoying to push code every time you want to change a piece of text. A lot of companies end up solving this with a homegrown solution, which typically involves storing the data in some type of database. This will work, but even then the data isn’t easily accessible to non-coders. You build a UI to allow non-coders to edit the data, but this will require additional work.</p><p>So how do you remove data from the deploy process? The first option is to use a generic config server. This will allow you to update text or constants on the fly, and changes will be reflected in the production application without a deploy. Options for doing something like this are <a href="https://href.li/?https://firebase.google.com/docs/remote-config">Firebase Remote Config</a> and <a href="https://href.li/?https://www.config.ly/">Configly</a>.</p><p>If you want a tool more suited to specific applications, you can find tools that are suited to almost any common use case. They will require some upfront work to implement, but will pay for themselves in the longer-term. If you want to internationalize your text and remove it from your code, you can use <a href="https://href.li/?https://simplelocalize.io/">Simplelocalize</a>, <a href="https://href.li/?https://lokalise.com/">Lokalise</a>, or <a href="https://href.li/?https://locize.com/?lng=en">Locize</a>. For feature flagging, consider <a href="https://href.li/?https://launchdarkly.com/">Launch Darkly</a>, <a href="https://href.li/?https://www.split.io/">Split</a>, or <a href="https://href.li/?https://apptimize.com/">Apptimize</a>.</p><p>These are just some of the many ways that you can make your deployments lighter weight and faster. By making deployments easier, you will both reduce frustration and make it easier for your team to move faster.</p></div></div>]]>
            </description>
            <link>https://blog.config.ly/post/643856862947721216/ways-to-reduce-the-pain-of-deploys</link>
            <guid isPermaLink="false">hacker-news-small-sites-26231563</guid>
            <pubDate>Mon, 22 Feb 2021 22:31:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Is Clubhouse Fading Out?]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=26231469">thread link</a>) | @rats
<br/>
February 22, 2021 | https://zandrey.com/why-clubhouse-is-fading-out/ | <a href="https://web.archive.org/web/*/https://zandrey.com/why-clubhouse-is-fading-out/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
				<p>Everyone is talking about Clubhouse, and how it's going to be the "next big thing", and <a href="https://www.theinformation.com/articles/clubhouse-gets-investment-interest-at-1-billion-valuation">at $1B valuation</a>, it's big enough already. Here's a few excerpts from popular media just from the last week:</p><blockquote>The 11-month-old app has exploded in popularity, even as it grapples with harassment, misinformation and privacy issues.</blockquote><p>via <a href="https://www.nytimes.com/2021/02/15/business/clubhouse.html">NYTimes</a></p><blockquote>Based on data from research firm <a href="https://www.appannie.com/en/insights/mobile-minute/clubhouse-social-audio-apps-rise/" rel="noreferrer noopener">App Annie</a> (via <em><a href="https://techcrunch.com/2021/02/18/report-social-audio-app-clubhouse-has-topped-8-million-global-downloads/?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS5ici8&amp;guce_referrer_sig=AQAAAKUfSS5gvGbTxlFn70ZTwbLGJirYiJsw1svnueP07i2LS8PUilqtOCfuw-kvv9HXD-9II2Dt8Xo3gubw__3jumv7_HUdzEvr-06l-rWLvhuN7zMC6PRmP-cA6bNUrBMd-reyouT5FJ7ZFVrQBv86XPSFViPAYUQXxkk0oPLwQ5st" rel="noreferrer noopener">TechCrunch</a></em>), the Clubhouse app reached 8.1 million global downloads on the iOS App Store on February 16, 2021. Just for comparison, the app had registered 3.5 million downloads by February 1, 2021. The research reveals that the app has become extremely popular in the UK, Germany, Japan, Brazil, and Turkey.</blockquote><p>via <a href="https://9to5mac.com/2021/02/18/clubhouse-reaches-8-million-downloads-on-the-ios-app-store/">9to5mac</a> </p><p>They even have a hockey stick graph to back this up:</p><figure><img src="https://zandrey.com/content/images/2021/02/image.png" alt="" srcset="https://zandrey.com/content/images/size/w600/2021/02/image.png 600w, https://zandrey.com/content/images/size/w1000/2021/02/image.png 1000w, https://zandrey.com/content/images/2021/02/image.png 1024w"></figure><p>But let's look closer.</p><p>Here's the number of downloads <strong>globally</strong>. There is a peak at ~500K daily downloads (Feb, 15) and a downward trend afterward. </p><figure><img src="https://zandrey.com/content/images/2021/02/Screen-Shot-2021-02-22-at-1.54.14-PM.png" alt="" srcset="https://zandrey.com/content/images/size/w600/2021/02/Screen-Shot-2021-02-22-at-1.54.14-PM.png 600w, https://zandrey.com/content/images/size/w1000/2021/02/Screen-Shot-2021-02-22-at-1.54.14-PM.png 1000w, https://zandrey.com/content/images/2021/02/Screen-Shot-2021-02-22-at-1.54.14-PM.png 1416w" sizes="(min-width: 1200px) 1200px"></figure><p>US is pretty stable at around 3M total downloads and 45K daily. A bit surprising because of all the hype and celebrities onboarding the app every day, and US being the largest iPhone user base in the world (except China of course) I would expect at &nbsp;least a certain amount of upward trend here.</p><figure><img src="https://zandrey.com/content/images/2021/02/Screen-Shot-2021-02-22-at-1.58.10-PM.png" alt="" srcset="https://zandrey.com/content/images/size/w600/2021/02/Screen-Shot-2021-02-22-at-1.58.10-PM.png 600w, https://zandrey.com/content/images/size/w1000/2021/02/Screen-Shot-2021-02-22-at-1.58.10-PM.png 1000w, https://zandrey.com/content/images/2021/02/Screen-Shot-2021-02-22-at-1.58.10-PM.png 1415w" sizes="(min-width: 1200px) 1200px"></figure><p>Let's look at other countries and here is where it gets interesting. Brazil:</p><figure><img src="https://zandrey.com/content/images/2021/02/Screen-Shot-2021-02-22-at-2.02.41-PM.png" alt="" srcset="https://zandrey.com/content/images/size/w600/2021/02/Screen-Shot-2021-02-22-at-2.02.41-PM.png 600w, https://zandrey.com/content/images/size/w1000/2021/02/Screen-Shot-2021-02-22-at-2.02.41-PM.png 1000w, https://zandrey.com/content/images/2021/02/Screen-Shot-2021-02-22-at-2.02.41-PM.png 1418w" sizes="(min-width: 1200px) 1200px"></figure><p>Japan:</p><figure><img src="https://zandrey.com/content/images/2021/02/Screen-Shot-2021-02-22-at-2.04.15-PM.png" alt="" srcset="https://zandrey.com/content/images/size/w600/2021/02/Screen-Shot-2021-02-22-at-2.04.15-PM.png 600w, https://zandrey.com/content/images/size/w1000/2021/02/Screen-Shot-2021-02-22-at-2.04.15-PM.png 1000w, https://zandrey.com/content/images/2021/02/Screen-Shot-2021-02-22-at-2.04.15-PM.png 1421w" sizes="(min-width: 1200px) 1200px"></figure><p>The spikes are caused by local influencers, and after Clubhouse ran out of celebrities, the growth stumbled here as well:</p><blockquote>A handful of Japanese celebrities were invited to Clubhouse in late January by friends in the tech industry, including TV commentator and comedian <a href="https://twitter.com/atsushilonboo/status/1354441357738184704" rel="noopener noreferrer">Atsushi Tamura</a>, who was one of the first widely recognized figures to log on. He was followed by actress and fashion designer Naomi Watanabe, who racked up over 500,000 followers in her first two weeks on the app. Celebrated kabuki and film actor Ebizo Ichikawa is also a <a href="https://twitter.com/EBIZO_DES/status/1356535391403208706" rel="noopener noreferrer">Clubhouse convert</a>, appearing in rooms every few days in recent weeks.</blockquote><p><a href="https://restofworld.org/2021/four-countries-one-clubhouse/">via</a> </p><p>Germany:</p><figure><img src="https://zandrey.com/content/images/2021/02/Screen-Shot-2021-02-22-at-2.07.49-PM.png" alt="" srcset="https://zandrey.com/content/images/size/w600/2021/02/Screen-Shot-2021-02-22-at-2.07.49-PM.png 600w, https://zandrey.com/content/images/size/w1000/2021/02/Screen-Shot-2021-02-22-at-2.07.49-PM.png 1000w, https://zandrey.com/content/images/2021/02/Screen-Shot-2021-02-22-at-2.07.49-PM.png 1419w" sizes="(min-width: 1200px) 1200px"></figure><p>Turkey:</p><figure><img src="https://zandrey.com/content/images/2021/02/Screen-Shot-2021-02-22-at-2.09.00-PM.png" alt="" srcset="https://zandrey.com/content/images/size/w600/2021/02/Screen-Shot-2021-02-22-at-2.09.00-PM.png 600w, https://zandrey.com/content/images/size/w1000/2021/02/Screen-Shot-2021-02-22-at-2.09.00-PM.png 1000w, https://zandrey.com/content/images/2021/02/Screen-Shot-2021-02-22-at-2.09.00-PM.png 1420w" sizes="(min-width: 1200px) 1200px"></figure><p>One of the few countries that's doing really well is Russia. Half of my contact list is still from Russia and I see them onboarding in droves every day:</p><figure><img src="https://zandrey.com/content/images/2021/02/Screen-Shot-2021-02-22-at-2.10.29-PM.png" alt="" srcset="https://zandrey.com/content/images/size/w600/2021/02/Screen-Shot-2021-02-22-at-2.10.29-PM.png 600w, https://zandrey.com/content/images/size/w1000/2021/02/Screen-Shot-2021-02-22-at-2.10.29-PM.png 1000w, https://zandrey.com/content/images/2021/02/Screen-Shot-2021-02-22-at-2.10.29-PM.png 1414w" sizes="(min-width: 1200px) 1200px"></figure><p>It would be very interesting to revisit the graphs above in a few weeks and see how everything is going. So far, it looks like after the initial boost by the local celebrities, people got tired of wandering aimlessly from room to room and the growth is stifled. </p>
			</section></div>]]>
            </description>
            <link>https://zandrey.com/why-clubhouse-is-fading-out/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26231469</guid>
            <pubDate>Mon, 22 Feb 2021 22:21:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Scott Alexander vs. NYT: Meta-Analysis, Part 2]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26231449">thread link</a>) | @nabla9
<br/>
February 22, 2021 | https://www.metalevelup.com/post/scott-alexander-vs-nyt-meta-analysis-part-2 | <a href="https://web.archive.org/web/*/https://www.metalevelup.com/post/scott-alexander-vs-nyt-meta-analysis-part-2">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-hook="post-description"><article><div><div data-rce-version="8.17.5"><div dir="ltr"><div><p id="viewer-foo"><span><a href="https://www.metalevelup.com/post/scott-alexander-vs-nyt-meta-analysis-part-1" target="_blank" rel="noopener"><u>See here for Part 1</u></a>, including Scott Alexander, Fredrik DeBoer, Jacob Falkovich, and Elizabeth Spiers. </span></p><p id="viewer-lqhd"><span>Minor correction: rereading Jacob Falkovich's take and <a href="https://twitter.com/yashkaf/status/1362877478327435272" target="_blank" rel="noopener"><u>some of the things he's tweeted since</u></a>, I moved him a bit further left on the "Call To Action" axis (see end of this post).</span></p><h2 id="viewer-4hi0v"><span><a href="https://www.scottaaronson.com/blog/?p=5310" target="_blank" rel="noopener"><u><span>Article #5: Scott Aaronson</span></u></a></span></h2><p id="viewer-8luck"><span><span>(</span><a href="https://www.scottaaronson.com/blog/?p=5310" target="_blank" rel="noopener"><span><em><u>A grand anticlimax: The New York Times on Scott Alexander</u></em></span></a><span><em><u>,</u></em></span></span></p><p id="viewer-fool"><span><span>but I recommend you see also </span><a href="https://www.scottaaronson.com/blog/?p=5330" target="_blank" rel="noopener"><span><em><u>On standing up sans backbone</u></em></span></a><span>)</span></span></p><p id="viewer-art8b"><span><span>The NYT article about Scott may not have happened without Scott. No wait, Scott A. and Scott A. Wait! I mean, the piece about Scott Al. may not have happened without Scott Aa.</span></span></p><blockquote id="viewer-3oppr"><span><span>I spent many hours with Cade [Metz], taking his calls and emails morning or night, at the playground with my kids or wherever else I was, answering his questions, giving context for his other interviews, suggesting people in the rationalist community for him to talk to, in exactly the same way I might suggest colleagues for a quantum computing story. And then I spent just as much time urging those people to talk to Cade.</span></span></blockquote><p id="viewer-b07b1"><span><span>Scott Aa. had previously worked with Metz, and found him to be a trustworthy journalist; he had no reason to mistrust when Metz said he was interested in Rationalists and, in particular, why the community had gotten COVID so right when the rest of media/government was getting it so wrong. </span></span></p><p id="viewer-81kqf"><span><span>When the story came out (with no mention of the COVID angle, by the way), Scott Aa. was not pleased, and in his post he details 14 ways the story was misleading or wrong (up from </span><a href="https://astralcodexten.substack.com/p/statement-on-new-york-times-article" target="_blank" rel="noopener"><span><u>Scott Al.'s 4</u></span></a><span>). This, in spite of Scott Aa. having talked with Metz about all of these details, only to have the final piece end up missing the point about all of them.</span></span></p><blockquote id="viewer-dfp21"><span><span>The trouble with the NYT piece is not that it makes any false statements, but just that it constantly </span><em>insinuates</em><span> nefarious beliefs and motives, via strategic word choices and omission of relevant facts that change the emotional coloration of the facts that it </span><em>does</em><span> present. I repeatedly muttered to myself, as I read: “dude, you could make </span><em>anything</em><span> sound shady with this exact same rhetorical toolkit!”
...
[W]ere I ever tempted to bang my head and say, “dammit, I wish I’d told Cade X, so his story could’ve reflected that perspective”—well, the truth of the matter is that I </span><em>did</em><span> tell him X! It’s just that I don’t get to decide which X’s make the final cut, or which ideological filter they’re passed through first.</span></span></blockquote><p id="viewer-e0qo9"><span><span>He remains agnostic about how much of the bad piece was due to Metz, and how much was due to NYT editors. </span></span></p><p id="viewer-euamv"><span><span>In spite of being upset about what happened, Scott Aa. originally planned not to change his behavior with journalists in the future; in the end, he may have acted rationally based on the knowledge he had and could not have known that this would be the outcome. However, </span><a href="https://www.scottaaronson.com/blog/?p=5330" target="_blank" rel="noopener"><span><u>in a follow-up post</u></span></a><span>, he announces that he won't work with Metz again without some kind of explanation that sufficiently exonerates him. From one of the more rational non-Rationalists around, who updates very carefully and Bayesian-ly, this is a pretty strong statement.</span></span></p><blockquote id="viewer-8ev8a"><span><span>I now feel like to work with Metz again, even just on some quantum computing piece, would be to reward—and to be seen as rewarding—journalistic practices that are making the world worse...</span></span></blockquote><p id="viewer-64lu2"><span><span>Overall, Scott Aa. seems more upset than Scott Al. or Jacob or Elizabeth, but less upset than Freddie (from </span><a href="https://www.metalevelup.com/post/scott-alexander-vs-nyt-meta-analysis-part-1" target="_blank" rel="noopener"><span><u>Part 1</u></span></a><span>).</span></span></p><p id="viewer-6j33u"><span><span><u>One Sentence Summary:</u> <em>The NYT article was bad, and people who contributed to it (including Scott Aa. himself) were misled into helping it be written; this seems to imply untrustworthy actors at the NYT, if not directly nefarious and bad-faith ones..</em></span></span></p><p id="viewer-cdl3m"><span><strong><em><span>(From here on I'm going back to calling Scott Alexander "Scott", not "Scott Al.")</span></em></strong></span></p><h2 id="viewer-bqnbm"><span><a href="https://www.piratewires.com/p/okay-fine-were-fighting" target="_blank" rel="noopener"><u><span>Article #6: Mike Solana</span></u></a></span></h2><p id="viewer-5jhpu"><span><span>(</span><a href="https://www.piratewires.com/p/okay-fine-were-fighting" target="_blank" rel="noopener"><span><em><u>Okay fine, we're fighting</u></em></span></a><span>)</span></span></p><p id="viewer-7t1db"><span><span>According to Mike, the NYT article is more of the same in an ongoing war between tech and media, which nobody wants to admit is an actual war.</span></span></p><blockquote id="viewer-3ijvs"><span>The endless cycle is thus: a hit is published, tech fights back, media fights back, tech fights back, the blue check media gang goes nuclear and accuses tech of targeted harassment for publicly commenting on the actual, literal words they are printing, mea culpa (“we’re all wrong here!”) and a prayer for peace. Then, it’s straight back to the garbage dump. 
<em>I hate it here</em>.</span></blockquote><p id="viewer-6imrc"><span>This piece isn't only on the SSC/NYT situation, but covers also several other recent skirmishes in this war (ignored here). It is all just routine now, a back and forth between two camps--something like Tech and Media--that hate and can't seem to abide one another. </span></p><p id="viewer-dvgl2"><span>Mike is not mad in particular about this piece because <em>this is just what NYT does</em>, and nobody should pretend to be surprised by it.</span></p><blockquote id="viewer-37k9f"><span><em>The New York Times</em> isn’t publishing one-off hit pieces. At least, in the narrow context of tech coverage, it is obvious many reporters at the <em>Times</em>, and across the press broadly, confuse the attention they receive for provoking controversy with righteous affirmation. They think, in general, they are doing good work — not just well-reported work, but <em>morally good</em> work...</span></blockquote><p id="viewer-eml9l"><span>While he stops short of fully supporting <a href="https://www.blocknyt.com/" target="_blank" rel="noopener"><u>#BlockTheNYT</u></a> (at least, he didn't explicitly update his stance <a href="https://www.piratewires.com/p/a-policy-of-truth-987" target="_blank" rel="noopener"><u>from a previous post</u></a>), he closes with skepticism of their credibility and good faith.</span></p><blockquote id="viewer-datei"><span>[F]ine, whatever, we’re in fight. We’re fighting. But let’s call it that... let’s dispel with the bullshit “objectivity” frame and robustly, openly disagree. We want different things, so what? You think you maybe kind of hate me, okay. Just do me a favor and tweet it.</span></blockquote><p id="viewer-20d0m"><span><u><em>One Sentence Summary</em></u>: <em>The NYT piece was bad in a way that most tech journalism is bad, because Media sees Tech as a threat; this is just the latest strike in a long-standing war between the two.</em></span></p><h2 id="viewer-936oa"><span><a href="https://noahpinion.substack.com/p/silicon-valley-isnt-full-of-fascists" target="_blank" rel="noopener"><u><span>Article #7: Noah Smith</span></u></a></span></h2><p id="viewer-ct0b5"><span><span>(</span><a href="https://noahpinion.substack.com/p/silicon-valley-isnt-full-of-fascists" target="_blank" rel="noopener"><span><em><u>Silicon Valley isn't full of fascists</u></em></span></a><span>)</span></span></p><p id="viewer-3gqju"><span><span>In a smog of people decrying political partisanship with angry words, Noah is a breath of statistically-literate fresh air. His central thesis about the NYT article is clear:</span></span></p><blockquote id="viewer-724vg"><span>To put it bluntly, I think the article both draws on and feeds into the mistaken stereotype that Silicon Valley is full of right-wingers.</span></blockquote><p id="viewer-6fcvm"><span>Noah argues that the NYT imagines a world that looks like this:</span></p><div id="viewer-d2lvt"><div><div data-hook="imageViewer" role="button" tabindex="0"><div role="img"><p><img data-pin-url="https://www.metalevelup.com/post/scott-alexander-vs-nyt-meta-analysis-part-2" data-pin-media="https://static.wixstatic.com/media/28c66d_abdf245248e943eea02b4868c6af4d56~mv2.jpeg/v1/fit/w_1000%2Ch_1000%2Cal_c%2Cq_80/file.jpeg" src="https://static.wixstatic.com/media/28c66d_abdf245248e943eea02b4868c6af4d56~mv2.jpeg/v1/fit/w_300,h_300,al_c,q_5/file.jpeg"></p></div></div></div></div><p id="viewer-evq04"><span>When in reality it looks more like this:</span></p><div id="viewer-fb6dr"><div><div data-hook="imageViewer" role="button" tabindex="0"><div role="img"><p><img data-pin-url="https://www.metalevelup.com/post/scott-alexander-vs-nyt-meta-analysis-part-2" data-pin-media="https://static.wixstatic.com/media/28c66d_a0f2b64fc1524f3ba60c463d7e482986~mv2.jpeg/v1/fit/w_1000%2Ch_1000%2Cal_c%2Cq_80/file.jpeg" src="https://static.wixstatic.com/media/28c66d_a0f2b64fc1524f3ba60c463d7e482986~mv2.jpeg/v1/fit/w_300,h_300,al_c,q_5/file.jpeg"></p></div></div></div></div><p id="viewer-cme0n"><span>And the rest of his post looks at the data to show how Silicon Valley, SSC, and Rationalism are nothing like safe havens for right-wing politics.</span></p><p id="viewer-98de7"><span>1) <u>On Silicon Valley politics</u>:</span></p><blockquote id="viewer-eseom"><span>Though Silicon Valley founders tend to be more skeptical of regulation and unions than the average Democrat (as you might expect given their jobs), they are overwhelmingly Democrats. On social issues (gay marriage, abortion, gun control, etc.) they are much more liberal even than the average college-educated Democrat. They also strongly favor government redistribution, which you might think would go against their class incentives. And most importantly, they score <strong>lower on </strong><a href="https://en.wikipedia.org/wiki/Racial_resentment_scale" target="_blank" rel="noopener"><strong>racial resentment</strong></a> and lower on the authoritarianism scale than the average Democratic base voter. In short, <strong>tech entrepreneurs are standard liberal nerds</strong>.</span></blockquote><p id="viewer-3mhsp"><span>2) <u>On SSC politics</u>: SSC likely has no more than 10k readers, and <a href="https://slatestarcodex.com/2020/01/20/ssc-survey-results-2020/" target="_blank" rel="noopener"><u>only ~40% of his readership works in a tech-adjacent field</u></a>; on the other hand, there are nearly 400k tech workers in the San Francisco Bay Area.</span></p><blockquote id="viewer-6b1sh"><span>In other words, Slate Star Codex was almost certainly a niche interest within the tech industry.</span></blockquote><p id="viewer-44r49"><span>There's no way SSC is holding sway with anything like a majority of Silicon Valley, let alone Tech more generally.</span></p><p id="viewer-dug47"><span>3) <u>On Rationalist politics</u>:</span></p><p id="viewer-8v5rv"><span>Like SSC readers, Rationalists are not primarily in tech ("the <em>only</em> major Rationalist figure I could find who is actually <em>in</em> tech is Eliezer Yudkowsky, who is sort of an A.I researcher" (lol at "sort of")), but like Silicon-Valley-ites most public Rationalists have left-leaning politics. Based on Scott's complicated stances around e.g. BLM and feminism, Noah reluctantly gives him the label of "conservative". (Of course given that Scott is "<a href="https://slatestarcodex.com/2019/02/22/rip-culture-war-thread/" target="_blank" rel="noopener"><u>a pro-gay Jew who has dated trans people and votes pretty much straight Democrat</u></a>" and in 2016 <a href="https://slatestarcodex.com/2016/09/28/ssc-endorses-clinton-johnson-or-stein/" target="_blank" rel="noopener"><u>endorsed literally "anyone but Trump"</u></a>, I doubt he means "conservative" as in "right-wing" or "Trumpian".)</span></p><p id="viewer-f1k56"><span>Noah's upset to the extent that the NYT gets this general picture wrong, and he retorts with his own analysis of the relevant statistical facts. He doesn't appear to see this as part of any big failure mode in media, so no larger call to action is necessary.</span></p><p id="viewer-2otrq"><span><u><em>One Sentence Summary</em></u>: <em>The NYT piece is wrong in its broad picture of the relationship of Tech, Rationalism, and SSC readership.</em></span></p><h2 id="viewer-6356u"><span><a href="https://modelcitizen.substack.com/p/grey-lady-steel-man" target="_blank" rel="noopener"><u><span>Article #8: Will Wilkinson</span></u></a><span> </span></span></h2><p id="viewer-7l3sv"><span><span>(</span><a href="https://modelcitizen.substack.com/p/grey-lady-steel-man" target="_blank" rel="noopener"><span><em><u>Grey Lady Steel Man</u></em></span></a><span><em><u>,</u></em></span></span></p><p id="viewer-fh38"><span><span>but I recommend you see also </span><a href="https://www.youtube.com/watch?v=XcGyJQnpPas" target="_blank" rel="noopener"><span><em><u>his interview with Robert Wright</u></em></span></a><span>)</span></span></p><p id="viewer-2p4q4"><span><span>Will tells a decidedly different story than most others, because it is primarily told from the perspective of Cade Metz rather than Scott Alexander.</span></span></p><blockquote id="viewer-4mnkf"><span>Somebody tells Metz about SSC, he finds it really interesting, wants to write some kind of article...
Metz contacts Siskind and at some point he tells Scott that he already knows his real name and at some point Scott tells Metz it’s very important that he doesn’t use his real name...
Well, the <em>Times</em> won’t promise, so Siskind <em>actually does [burn SSC to the ground].</em> This seems super-crazy and the natural journalistic response to it is “What the hell is this man hiding? What’s he so afraid I’ll find on his blog?”</span></blockquote><p id="viewer-allr8"><span>Whatever Metz's piece was about before went immediately to the back-burner; he certainly must have taken Scott nuking is own blog as evidence of something much <em>much</em> more interesting. So he went digging. Scott himself notes that around this time, Metz "switched to interviewing everyone who hated me and asking a lot of leading questions about potentially bad things I did." SSC readers think this might be a sinister revenge plot, but a good journalist follows their nose, and Metz' smelled something fishy about this man who would insist on his anonymity at the expense of his entire body of work and hard-fought online following.</span></p><p id="viewer-5962p"><span>What did Scott have to hide, that he would give all that up to keep it under wraps? According to Will, this question--not Silicon Valley, not Rationalism--is the real topic of the NYT piece. </span></p><p id="viewer-66ckm"><span>And…</span></p></div></div></div></div></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.metalevelup.com/post/scott-alexander-vs-nyt-meta-analysis-part-2">https://www.metalevelup.com/post/scott-alexander-vs-nyt-meta-analysis-part-2</a></em></p>]]>
            </description>
            <link>https://www.metalevelup.com/post/scott-alexander-vs-nyt-meta-analysis-part-2</link>
            <guid isPermaLink="false">hacker-news-small-sites-26231449</guid>
            <pubDate>Mon, 22 Feb 2021 22:20:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Tianwen-1 Phasing Orbit]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26231383">thread link</a>) | @parsecs
<br/>
February 22, 2021 | https://destevez.net/2021/02/tianwen-1-phasing-orbit/ | <a href="https://web.archive.org/web/*/https://destevez.net/2021/02/tianwen-1-phasing-orbit/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-9763">

	

	<div>
		
<p>Last Saturday 2021-02-20 at 11:46:42 UTC <a href="https://destevez.net/tag/tianwen/">Tianwen-1</a> passed the periapsis of its elliptical polar orbit at Mars and made a retrograde burn to reduce its apoapsis radius. The  trajectory planning of the spacecraft can be seen in its <a href="https://en.wikipedia.org/wiki/Tianwen-1">Wikipedia page</a>: the spacecraft first arrived into a low inclination elliptical orbit by making a <a href="https://twitter.com/ea4gpz/status/1359433349018882050">Mars orbit insertion</a> at periapsis, then coasted to apoapsis, where it performed a <a href="https://destevez.net/2021/02/tianwen-1-plane-change-planning/">plane change</a>, and then it arrived at periapsis, performing the manoeuvre described in this post.</p>



<p>Over the next few days the spacecraft should move into a reconnaissance orbit, which is given in Wikipedia to be a 265 x 60000 km orbit (having a period of 2 days) with an inclination of 86.9 degrees. However, the last burn hasn’t lowered the apoapsis that much. The current orbit is approximately 280 x 84600 km (3.45 day period) with an inclination of 87.7 degrees. A possible reason for using the current orbit, which has been described as a phasing orbit, will be explained in this post after reviewing the data we have about the burn.</p>



<p>As I usually do, to compute the moment and delta-V of the burn I propagate the pre-burn and post-burn trajectories in <a href="http://gmat.sourceforge.net/docs/">GMAT</a> using <a href="https://github.com/daniestevez/jupyter_notebooks/blob/master/Tianwen/orbit/phasing_burn_vectors.script">this script</a>, and study the output in <a href="https://github.com/daniestevez/jupyter_notebooks/blob/master/Tianwen/Tianwen-1%20phasing%20burn.ipynb">this Jupyter notebook</a>. I obtain an intersection at 11:44:18 UTC, which is pretty close to the periapsis passage, so the data seems correct.</p>



<figure><a href="https://destevez.net/wp-content/uploads/2021/02/trajectories_intersection.png"><img width="644" height="341" src="https://destevez.net/wp-content/uploads/2021/02/trajectories_intersection-644x341.png" alt="" srcset="https://destevez.net/wp-content/uploads/2021/02/trajectories_intersection-644x341.png 644w, https://destevez.net/wp-content/uploads/2021/02/trajectories_intersection-300x159.png 300w, https://destevez.net/wp-content/uploads/2021/02/trajectories_intersection.png 730w" sizes="(max-width: 644px) 100vw, 644px"></a></figure>



<p>The delta-V vector in m/s using the Mars body inertial frame described in <a href="https://destevez.net/2021/02/tianwen-1-mars-centric-state-vectors/" data-type="post" data-id="9635">this post</a> is</p>



<pre>[-1.477, -1.912, 52.565]</pre>



<p>This vector has a magnitude of 52.62 m/s. Assuming a dry mass of 2500 kg and fuel mass of 950 kg, this burn would have taken 60 seconds with the 3 kN thrusters, and spent 57 kg of fuel. Thus, according to our (somewhat crude) fuel estimates, approximately 900 kg of fuel remain now.</p>



<p>It is convenient to write the delta-V vector in the VNB frame whose axes are given by V, the velocity vector, N, the vector normal to the orbit (which is defined to point along the cross product of the radius and V), and B, the bi-normal vector, which is the cross product of V and N. The VNB coordinates in m/s are</p>



<pre>[-50.406,  0.032,  15.104]</pre>



<p>We see that most of the burn happens along -V as expected for a retrograde burn, but there is a significant component along +B. This is perhaps a bit unexpected. The effect of a +B burn is to move the periapsis backwards along the orbit, so that it would move to a slightly more northern latitude (the spacecraft descends from north to south on the periapsis passage). In fact, the periapsis has moved from a latitude of 10.03º N to a latitude of 10.25º N. This might be relevant for the discussion that comes below.</p>



<p>Now the good question is what is the reason for moving to this intermediate phasing orbit with a 3.45 day period instead of moving directly to the 2 day period orbit? I think there is a quite reasonable explanation, but we must first understand the purpose of the 2 day period reconnaissance orbit. This will be the orbit used by the spacecraft to map and survey the intended landing site, until the lander is released, which is expected to happen in May or June.</p>



<p>Therefore, it seems quite desirable to have an orbit whose periapsis ground track always passes over the landing site. This gives plenty of opportunities for gathering survey data and is also mandatory for the release of the lander, which is basically going to be done from the reconnaissance orbit (by first lowering its periapsis in a suitable manner). So all this makes me think that the quoted “2 day period” is actually 2 Mars sidereal days (a Mars sidereal day is 24 hours, 37 minutes and 22 seconds), since that would give a repeating ground track.</p>



<p>For this plan to work well, the periapsis of the the orbit needs to be at the correct longitude by the time that the 2 sidereal day orbit is entered. Otherwise the ground track will be repeating, but it will not pass over the landing site. Now, the longitude of the next periapsis of the current orbit turns out to be 111.3º E. In Wikipedia the coordinates of the intended landing site in <a href="https://en.wikipedia.org/wiki/Utopia_Planitia">Utopia Planitia</a> are given as 24.748º N, 110.318º E. Note that the latitude of the site is somewhat higher than the latitude of the periapsis of the current orbit, so perhaps moving the periapsis north is desirable. This might (but only might) be the reason for the burn component along B.</p>



<p>So we see that around the next periapsis, which is going to be tomorrow 2021-02-23 at 22:31:37 UTC, the spacecraft will pass above the landing site. Given this circumstance, it can now enter the 2 sidereal day reconnaissance orbit, which will then have a repeating ground track that always passes over the landing site.</p>



<p>There is no magic involved in these adjustments. Coming in from the previous orbit last Saturday, when arriving to the periapsis it is just enough to adjust the apoapsis altitude (and hence the orbit period) in such a way that when the spacecraft comes to the next periapsis Mars has rotated below the orbit so as to place the longitude of the landing site below the orbit. The required period to do this will depend on the (signed) difference between the longitude of the periapsis where the burn is performed and the longitude of the landing site. Therefore, the name “phasing orbit” is completely justified. The purpose of the current orbit would be to wait until the rotation of Mars places the landing site below the orbit.</p>



<p>To see what the passage to the 2 sidereal day orbit at next periapsis would look like, I have made this <a href="https://github.com/daniestevez/jupyter_notebooks/blob/master/Tianwen/orbit/phasing_orbit.script">GMAT script</a>. By adjusting the delta-V of the periapsis burn, I have seen that a 40.9 m/s burn will give an orbit with a ground track that is very close to be repeating. This is shown in the figure below.</p>



<figure><a href="https://destevez.net/wp-content/uploads/2021/02/GmatScreenShot_052.png"><img width="644" height="315" src="https://destevez.net/wp-content/uploads/2021/02/GmatScreenShot_052-644x315.png" alt="" srcset="https://destevez.net/wp-content/uploads/2021/02/GmatScreenShot_052-644x315.png 644w, https://destevez.net/wp-content/uploads/2021/02/GmatScreenShot_052-300x147.png 300w, https://destevez.net/wp-content/uploads/2021/02/GmatScreenShot_052-768x376.png 768w, https://destevez.net/wp-content/uploads/2021/02/GmatScreenShot_052.png 1350w" sizes="(max-width: 644px) 100vw, 644px"></a><figcaption>2 sidereal day orbit, with ground track passing over the landing site</figcaption></figure>



<p>There is some degree of complication here regarding orbit perturbations. The plot above shows 14 days of ground track, and we see that at some point the ground track starts to slowly creep to the east. The propagator I’m using here is quite detailed: the 80×80 <a href="https://ui.adsabs.harvard.edu/abs/2001JGR...10623359L/abstract">GMM-2B</a> Mars gravity model, point mass forces for all planets and the Sun, relativistic effects, and an integration step of at most 50 seconds (no solar radiation pressure or atmospheric drag, though).</p>



<p>Something I haven’t understood completely is why the period of the orbit shown above is actually 162 seconds longer than two sidereal days. Forgetting about perturbations, the track of such an orbit would drift some 0.66 degrees to the west per revolution. However, if I try to adjust the orbit to have a period closer to 2 sidereal days, I get much more drift of the ground track than with this orbit solution. I don’t know if this is caused by perturbations or by numerical accuracy (perhaps related to the integrator). This is something that might deserve more in-depth study. In any case, probably the real-world orbit will need some degree of station keeping to correct for perturbations.</p>



<p>The apoapsis radius of this 2 sidereal day orbit is 61217 km (giving an apoapsis altitude of 57821 km), while the periapsis has an altitude of 282 km (logically, still close to the 280 km we started with on Saturday). Therefore, some care should be taken when quoting this as a 265 x 60000 km orbit. That can be slightly misleading, as it is not clear if 60000 km refers to the apoapsis radius or altitude.</p>



<p>To summarize, in this post we have shown that it is very likely that the purpose of the current orbit is to pass over the landing site at next periapsis on 2021-02-23 22:31:37 UTC. Then a burn would lower the apoapsis further to obtain an orbit with a period of 2 sidereal days that has a repeating ground track passing over the landing site.</p>
	</div><!-- .entry-content -->

	<!-- .entry-footer -->

				
</article></div>]]>
            </description>
            <link>https://destevez.net/2021/02/tianwen-1-phasing-orbit/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26231383</guid>
            <pubDate>Mon, 22 Feb 2021 22:13:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[A Retrospective Look at Mac OS X Snow Leopard]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 2 (<a href="https://news.ycombinator.com/item?id=26231212">thread link</a>) | @NaOH
<br/>
February 22, 2021 | http://morrick.me/archives/9220 | <a href="https://web.archive.org/web/*/http://morrick.me/archives/9220">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			<h2>Introduction</h2>
<p>My recent article, <a href="http://morrick.me/archives/9150"><em>The reshaped Mac experience</em></a>, received a lot of attention judging from the response on Twitter and the WordPress analytics — apparently, among other places, it reached Hacker News and Reddit. Unlike my four-part series&nbsp;<em>‌Mac OS Catalina: more trouble than it’s worth</em>, however, it didn’t attract any hate mail at all. The sheer majority of feedback I received was very positive, with many many people agreeing with me and my observations. A few — some provocatively, some genuinely curious — asked me something along the lines of, <em>Well, if you dislike the current Big Sur UI and Mac experience, what’s an example of Mac OS UI and experience you DO&nbsp;like?</em></p>
<p>It’s a more than fair question, and this piece serves as an answer. When I wrote back to those who asked me, I replied <em>Mac OS X 10.6 Snow Leopard</em>. It was sort of a gut-reply based largely on fond memories of using that Mac OS version quite extensively.</p>
<p>When I purchased my 15-inch MacBook Pro in July 2009, it came with Mac OS X 10.5.7 (Leopard), but I immediately upgraded to Snow Leopard when it was released a month or so afterwards. As you know (and if you don’t, here’s a refresher), together with Mac OS X 10.4 Tiger, Snow Leopard was one of the Mac OS versions with the longest lifespan — almost two years, from August 2009 to July 2011, when the final 10.6.8 v1.1 minor release came out. On my 2009 MacBook Pro, I kept using it until mid-2012, as Mac OS X 10.7 Lion (released in July 2011) didn’t fully convince me at first, so I waited until at least version 10.7.3 before upgrading.</p>
<p>So, I used Snow Leopard on my 2009 MacBook Pro for about three years, and then again on a 2010 Mac mini that a friend gave me to maintain, as a sort of offsite backup. That Mac mini was kept on Mac OS X 10.6.8 for the whole four years it was in my custody (2011–2015) and it was switched off only twice during that period and maybe restarted four or five times in total. It enjoyed an insane uptime and it was a testament to Snow Leopard’s stability.</p>
<p>But back to my ‘gut-reply’, I wanted to be certain that my fond memories of Snow Leopard weren’t just nostalgia. While I am confident when I say that Snow Leopard is the most stable version of Mac OS, I wanted to make sure its user interface was really the good user interface and experience I was remembering. So, after a few frustrating attempts at creating a virtual machine on my current iMac with Mac OS High Sierra, I decided to install Snow Leopard on a USB flash drive, and boot my 2009 MacBook Pro (yes, it’s still alive <span>&amp;</span> kicking) in Snow Leopard from that flash&nbsp;drive.</p>
<h2>Installation</h2>
<p><a href="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/01-Install-MacOSX-SL.jpg"><img loading="lazy" src="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/01-Install-MacOSX-SL.jpg?resize=960%2C1280" alt="" width="960" height="1280" srcset="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/01-Install-MacOSX-SL.jpg?w=1512 1512w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/01-Install-MacOSX-SL.jpg?resize=260%2C347 260w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/01-Install-MacOSX-SL.jpg?resize=640%2C853 640w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/01-Install-MacOSX-SL.jpg?resize=768%2C1024 768w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/01-Install-MacOSX-SL.jpg?resize=1152%2C1536 1152w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/01-Install-MacOSX-SL.jpg?resize=1194%2C1592 1194w" sizes="(max-width: 960px) 100vw, 960px" data-recalc-dims="1"></a></p>
<p><a href="https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/02-SL-Welcome.jpg"><img loading="lazy" src="https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/02-SL-Welcome.jpg?resize=960%2C720" alt="" width="960" height="720" srcset="https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/02-SL-Welcome.jpg?w=2016 2016w, https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/02-SL-Welcome.jpg?resize=260%2C195 260w, https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/02-SL-Welcome.jpg?resize=640%2C480 640w, https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/02-SL-Welcome.jpg?resize=768%2C576 768w, https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/02-SL-Welcome.jpg?resize=1536%2C1152 1536w, https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/02-SL-Welcome.jpg?resize=1194%2C896 1194w, https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/02-SL-Welcome.jpg?w=1920 1920w" sizes="(max-width: 960px) 100vw, 960px" data-recalc-dims="1"></a><br>
<em>Ah, When Mac OS welcomed you after the installation process was complete…</em></p>
<p>Since the MacBook Pro doesn’t have an optical drive anymore, I had to create a bootable USB flash drive from my original Snow Leopard DVD Installer. The fastest method is to use Disk Utility — rather, an older version of Disk Utility, from a time when this application was <em>really</em> a utility, and you could use the Restore feature <em>reliably</em> to clone the bootable DVD to (in this case) an external volume.</p>
<p>From a bootable USB flash drive to another USB flash drive, installation was relatively fast, about 20–25 minutes. Although I would have preferred an external SSD for the speed, I must say that using Snow Leopard from the flash drive is a breeze nonetheless. The system is responsive and I haven’t noticed any particular lags.</p>
<h2>User interface</h2>
<p>Now let’s examine just a few aspects of Snow Leopard’s user interface — just like I did for Big Sur in my logbook — and draw comparisons with Big Sur’s interface.</p>
<h3>The menu&nbsp;bar</h3>
<p>Back in August 2020 when I started testing the first Big Sur beta versions, <a href="http://morrick.me/archives/8954">I wrote in my Big Sur logbook</a>:</p>
<blockquote><p>In Big Sur, the menu bar by default isn’t solid white, but has a noticeable degree of transparency: it takes the colour of the desktop wallpaper behind it, in an attempt to blend in with the rest of the desktop environment. Some may consider this sleek, but it’s just gimmicky and usability-hostile.</p>
<p>What happens when the desktop wallpaper has darker colours? Well, menu items and menu bar icons become white, of course. The problem is that the wallpaper doesn’t have to be too&nbsp;dark.</p></blockquote>
<p>In other words, when Big Sur decides that the desktop background image is dark enough, text and icons on the menu bar become white. The problem is that there are cases where the background colour simply <em>isn’t</em> dark enough to warrant a change from black text and icons to white text and icons. Consequently, the contrast is too poor. The only option for better usability is to select <em>Reduce transparency</em> in <em>System Preferences</em> → <em>Accessibility</em>. This brings the menu bar back to a useful state, solid white with black elements.</p>
<p>In Snow Leopard, the menu bar has transparency set to <em>on</em> by default, but it’s definitely more subtle, even with darker desktop backgrounds:</p>
<p><a href="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/03-subtle-menubar-transparency.png"><img loading="lazy" src="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/03-subtle-menubar-transparency.png?resize=960%2C500" alt="" width="960" height="500" srcset="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/03-subtle-menubar-transparency.png?w=1440 1440w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/03-subtle-menubar-transparency.png?resize=260%2C135 260w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/03-subtle-menubar-transparency.png?resize=640%2C333 640w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/03-subtle-menubar-transparency.png?resize=768%2C400 768w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/03-subtle-menubar-transparency.png?resize=1194%2C622 1194w" sizes="(max-width: 960px) 100vw, 960px" data-recalc-dims="1"></a></p>
<p><em>In the top image, menu bar transparency is off; in the bottom image, transparency is on. The difference is almost negligible.</em></p>
<p>Only with certain background images that contain dark and light areas starkly juxtaposed can menu bar transparency become a bit of an issue under Snow Leopard, but that is partly mitigated by the visible drop shadow beneath the menu bar itself, which helps to make the menu bar stand out&nbsp;more:</p>
<p><a href="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/04-menubar-translucency-on.png"><img loading="lazy" src="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/04-menubar-translucency-on.png?resize=960%2C672" alt="" width="960" height="672" srcset="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/04-menubar-translucency-on.png?w=1000 1000w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/04-menubar-translucency-on.png?resize=260%2C182 260w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/04-menubar-translucency-on.png?resize=640%2C448 640w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/04-menubar-translucency-on.png?resize=768%2C538 768w" sizes="(max-width: 960px) 100vw, 960px" data-recalc-dims="1"></a></p>
<p>Contrast, even in these conditions, tends to be more tolerable than in Big Sur, at least for my eyes. And in any case, in Snow Leopard you can quickly turn off transparency right in <em>System Preferences</em> → <em>Desktop <span>&amp;</span> Screen Saver:</em></p>
<p><a href="https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/05-Desktop-SSaver.png"><img loading="lazy" src="https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/05-Desktop-SSaver.png?resize=748%2C658" alt="" width="748" height="658" srcset="https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/05-Desktop-SSaver.png?w=748 748w, https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/05-Desktop-SSaver.png?resize=260%2C229 260w, https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/05-Desktop-SSaver.png?resize=640%2C563 640w" sizes="(max-width: 748px) 100vw, 748px" data-recalc-dims="1"></a></p>
<p><em>I’ve been talking about ‘transparency’, whereas it’s actually ‘translucency’ — at least in Snow Leopard.</em></p>
<h3>Finder windows</h3>
<p>In Snow Leopard, Finder windows are essentially perfect from a user interface standpoint.</p>
<p><a href="https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/06-Finder-Window.png"><img loading="lazy" src="https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/06-Finder-Window.png?resize=850%2C558" alt="" width="850" height="558" srcset="https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/06-Finder-Window.png?w=850 850w, https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/06-Finder-Window.png?resize=260%2C171 260w, https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/06-Finder-Window.png?resize=640%2C420 640w, https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/06-Finder-Window.png?resize=768%2C504 768w" sizes="(max-width: 850px) 100vw, 850px" data-recalc-dims="1"></a></p>
<p>When I shared this over Twitter, Mario Guzmán <a href="https://twitter.com/MarioGuzman/status/1360651998358425601?s=20">observed</a> that <em>Things are nicely compartmentalized by color. You can distinctly tell each section of the window (even the damn scroll bars)… it’s not just one blob of white with grey symbols.</em></p>
<p>Exactly this. The window has clearly distinguishable areas: the <em>Title bar</em> (with the semaphore controls at the top left of the window, and the sidebar+toolbar show/hide toggle button at the top right), the <em>Toolbar</em>, the <em>Sidebar</em> (with colourful icons helping you quickly and easily locate items at a glance), the <em>Path bar</em>, the <em>Status bar</em>, and finally the <em>scroll bars</em> which are always visible.</p>
<p>Persistent up/down arrows and scroll bars are the right thing to do, usability-wise, and it is such a user-friendly design. The length of the ‘aqua blue’ bar immediately gives you an idea of how populated that folder you just opened is going to be. Further, if you need to rapidly scroll down, you just grab the bar with the mouse pointer and scroll.</p>
<p>In later Mac OS versions, scroll bars are set by default to appear only based on mouse/trackpad movement, which is a pity; many users probably don’t realise they can have scroll bars appear permanently, so they don’t have to time the mouseover action for the scroll bar to appear and then <em>hope</em> they’ll manage to grab it when they want to quickly scroll down a long list of elements.</p>
<p>I am once again reminded of that infamous quote by Alan Dye (Apple’s VP of Human Interface) from WWDC 2020, speaking of Big Sur’s UI redesign:&nbsp;<em>‌We’ve reduced visual complexity to keep the focus on users’ content. Buttons and controls appear when you need them, and they recede when you don’t.</em> I still believe this is not a good approach in general, and especially for essential elements like scroll bars, which should always be visible by default, because they are UI elements whose usefulness isn’t limited to when you use them or interact with them — they signal something even when not strictly needed. In the case of the scroll bars it’s a visual estimate of how many elements a folder contains, how long a list of items is, and more importantly <em>your current position</em> when scrolling.</p>
<p>Back to Finder windows, here’s an “Apple’s attention to detail” detail: notice that icon in the bottom left of the window? It is a subtle visual cue that tells you if Finder icons (items) are sorted, unsorted, or simply snapped to a grid. When opening windows from read-only volumes, the icon of a crossed-out pencil appears here, meaning that you can’t modify the enclosed items or write to that volume.</p>
<p><a href="https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/07-Finder-window-arranged-none.png"><img loading="lazy" src="https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/07-Finder-window-arranged-none.png?resize=850%2C558" alt="" width="850" height="558" srcset="https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/07-Finder-window-arranged-none.png?w=850 850w, https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/07-Finder-window-arranged-none.png?resize=260%2C171 260w, https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/07-Finder-window-arranged-none.png?resize=640%2C420 640w, https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/07-Finder-window-arranged-none.png?resize=768%2C504 768w" sizes="(max-width: 850px) 100vw, 850px" data-recalc-dims="1"></a><br>
<em>Items are unsorted (Arranged by: None) — No icon in the bottom left corner</em></p>
<p><a href="https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/08-Finder-window-sorted.png"><img loading="lazy" src="https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/08-Finder-window-sorted.png?resize=850%2C558" alt="" width="850" height="558" srcset="https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/08-Finder-window-sorted.png?w=850 850w, https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/08-Finder-window-sorted.png?resize=260%2C171 260w, https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/08-Finder-window-sorted.png?resize=640%2C420 640w, https://i1.wp.com/morrick.me/wp-content/uploads/2021/02/08-Finder-window-sorted.png?resize=768%2C504 768w" sizes="(max-width: 850px) 100vw, 850px" data-recalc-dims="1"></a><br>
<em>Items are sorted (by name, size, kind,&nbsp;etc.)</em></p>
<p><a href="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/09-Finder-window-snapgrid.png"><img loading="lazy" src="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/09-Finder-window-snapgrid.png?resize=850%2C558" alt="" width="850" height="558" srcset="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/09-Finder-window-snapgrid.png?w=850 850w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/09-Finder-window-snapgrid.png?resize=260%2C171 260w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/09-Finder-window-snapgrid.png?resize=640%2C420 640w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/09-Finder-window-snapgrid.png?resize=768%2C504 768w" sizes="(max-width: 850px) 100vw, 850px" data-recalc-dims="1"></a><br>
<em>Items are snapped to&nbsp;grid</em></p>
<p><a href="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/10-Finder-window-readonly-vol.png"><img loading="lazy" src="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/10-Finder-window-readonly-vol.png?resize=752%2C521" alt="" width="752" height="521" srcset="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/10-Finder-window-readonly-vol.png?w=752 752w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/10-Finder-window-readonly-vol.png?resize=260%2C180 260w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/10-Finder-window-readonly-vol.png?resize=640%2C443 640w" sizes="(max-width: 752px) 100vw, 752px" data-recalc-dims="1"></a><br>
<em>Window from a read-only volume</em></p>
<p>While I don’t find this UI detail to be crucial, it is certainly nice to have, and an example of those little things that contributed to make the Mac’s interface great. As I said above, it reflected a certain attention to detail and overall thoughtfulness I’ve seen progressively fade away in later Mac OS releases.</p>
<h2>A look back at a few system apps, with occasional UI comparisons between Snow Leopard and Big&nbsp;Sur</h2>
<h3>Safari</h3>
<p>5.1.10 was the last version of Safari running on Mac OS X 10.6.8. Here are a few things I still prefer over the current Safari:</p>
<p><a href="https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/11-Blue-progress-bar.png"><img loading="lazy" src="https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/11-Blue-progress-bar.png?resize=845%2C92" alt="" width="845" height="92" srcset="https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/11-Blue-progress-bar.png?w=845 845w, https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/11-Blue-progress-bar.png?resize=260%2C28 260w, https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/11-Blue-progress-bar.png?resize=640%2C70 640w, https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/11-Blue-progress-bar.png?resize=768%2C84 768w" sizes="(max-width: 845px) 100vw, 845px" data-recalc-dims="1"></a><br>
<em>The blue progress bar</em></p>
<p><a href="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/12-RSS-button.png"><img loading="lazy" src="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/12-RSS-button.png?resize=852%2C100" alt="" width="852" height="100" srcset="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/12-RSS-button.png?w=852 852w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/12-RSS-button.png?resize=260%2C31 260w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/12-RSS-button.png?resize=640%2C75 640w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/12-RSS-button.png?resize=768%2C90 768w" sizes="(max-width: 852px) 100vw, 852px" data-recalc-dims="1"></a><br>
<em>The RSS button (you could read RSS feeds with Safari)</em></p>
<p>Another detail I very much prefer in the older Safari over more recent versions of Safari is how the plus [+] button near the address bar works. Its placement makes its function rather unequivocal: <em>Add the current page to something</em>. As usual, tooltips are helpful:</p>
<p><a href="https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/13-Plus-button-1.png"><img loading="lazy" src="https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/13-Plus-button-1.png?resize=562%2C92" alt="" width="562" height="92" srcset="https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/13-Plus-button-1.png?w=562 562w, https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/13-Plus-button-1.png?resize=260%2C43 260w" sizes="(max-width: 562px) 100vw, 562px" data-recalc-dims="1"></a></p>
<p>But what if I want to add this page to my Reading List? No worries, when you actually press the [+], a thoughtfully-designed sheet comes down, and you can put the current page exactly where you want: in your Reading List, in the Top Sites, or in your Bookmarks.</p>
<p><a href="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/14-Plus-button-2.png"><img loading="lazy" src="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/14-Plus-button-2.png?resize=960%2C267" alt="" width="960" height="267" srcset="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/14-Plus-button-2.png?w=1005 1005w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/14-Plus-button-2.png?resize=260%2C72 260w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/14-Plus-button-2.png?resize=640%2C178 640w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/14-Plus-button-2.png?resize=768%2C213 768w" sizes="(max-width: 960px) 100vw, 960px" data-recalc-dims="1"></a></p>
<p>The other plus button, to open a new browser tab, is placed in such an obvious spot that you know what it does without even waiting for the tooltip to appear:</p>
<p><a href="https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/15-Plus-button-3.png"><img loading="lazy" src="https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/15-Plus-button-3.png?resize=960%2C110" alt="" width="960" height="110" srcset="https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/15-Plus-button-3.png?w=1374 1374w, https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/15-Plus-button-3.png?resize=260%2C30 260w, https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/15-Plus-button-3.png?resize=640%2C73 640w, https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/15-Plus-button-3.png?resize=768%2C88 768w, https://i2.wp.com/morrick.me/wp-content/uploads/2021/02/15-Plus-button-3.png?resize=1194%2C136 1194w" sizes="(max-width: 960px) 100vw, 960px" data-recalc-dims="1"></a></p>
<p>Now, let’s take a quick look at the UI in Big Sur’s Safari:</p>
<p><a href="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/16-Safari-top-UI.png"><img loading="lazy" src="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/16-Safari-top-UI.png?resize=960%2C59" alt="" width="960" height="59" srcset="https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/16-Safari-top-UI.png?w=2560 2560w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/16-Safari-top-UI.png?resize=260%2C16 260w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/16-Safari-top-UI.png?resize=640%2C40 640w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/16-Safari-top-UI.png?resize=768%2C47 768w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/16-Safari-top-UI.png?resize=1536%2C95 1536w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/16-Safari-top-UI.png?resize=2048%2C126 2048w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/16-Safari-top-UI.png?resize=1194%2C74 1194w, https://i0.wp.com/morrick.me/wp-content/uploads/2021/02/16-Safari-top-UI.png?w=1920 1920w" sizes="(max-width: 960px) 100vw, 960px" data-recalc-dims="1"></a></p>
<p>At first glance, there’s only one plus button in the app’s chrome. Try to look at this UI with fresh eyes and guess …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://morrick.me/archives/9220">http://morrick.me/archives/9220</a></em></p>]]>
            </description>
            <link>http://morrick.me/archives/9220</link>
            <guid isPermaLink="false">hacker-news-small-sites-26231212</guid>
            <pubDate>Mon, 22 Feb 2021 21:57:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Against Packaging Rust Crates]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26230935">thread link</a>) | @zdw
<br/>
February 22, 2021 | https://fy.blackhats.net.au/blog/html/2021/02/16/against_packaging_rust_crates.html | <a href="https://web.archive.org/web/*/https://fy.blackhats.net.au/blog/html/2021/02/16/against_packaging_rust_crates.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="against-packaging-rust-crates">

<p>Recently the discussion has once again come up around the notion of packaging Rust crates as
libraries in distributions. For example, taking a library like <cite>serde</cite> and packaging it to
an RPM. While I use RPM as the examples here it applies equally to other formats.</p>
<p>Proponents of crate packaging want all Rust applications to use the “distributions” versions of a crate.
This is to prevent “vendoring” or “bundling”. This is where an
application (such as 389 Directory Server) ships all of it’s sources, as well as the sources of
it’s Rust dependencies in a single archive. These sources may differ in version from the bundled
sources of other applications.</p>
<div id="packaging-crates-is-not-reinventing-cargo">
<h2>“Packaging crates is not reinventing Cargo”</h2>
<p>This is a common claim by advocates of crate packaging. However it is easily disproved:</p>
<p><em>If packaging is not reinventing cargo, I am free to use all of Cargo’s features without conflicts to distribution packaging.</em></p>
<p>The reality is that packaging crates <em>is</em> reinventing Cargo - but without all it’s features. Common
limitations are that Cargo’s exact version/less than requirements can not be used safely, or Cargo’s ability to
apply patches or uses sources from specific git revisions can not be used at all.</p>
<p>As a result, this hinders upstreams from using all the rich features within Cargo to comply with
distribution packaging limitations, or it will cause the package to hit exceptions in policy and
necesitate vendoring anyway.</p>
</div>
<div id="you-can-vendor-only-in-these-exceptional-cases">
<h2>“You can vendor only in these exceptional cases …”</h2>
<p>As noted, since packaging is reinventing Cargo, if you use features of Cargo that are unsupported
then you may be allowed to vendor depending on the distributions policy. However, this raises some
interesting issues itself.</p>
<p>Assume I have been using distribution crates for a period of time - then the upstream adds an exact version
or git revision requirement to a project or a dependency in my project. I now need to change my spec file and tooling to use vendoring
and all of the benefits of distribution crates no longer exists (because you can not have any dependency
in your tree that has an exact version rule).</p>
<p>If the upstream ‘un-does’ that change, then I need to roll back to distribution crates since
the project would no longer be covered by the exemption.</p>
<p>This will create review delays and large amounts of administrative overhead. It means pointless effort to swap between
vendored and distribution crates based on small upstream changes. This may cause packagers to avoid
certain versions or updates so that they do not need to swap between distribution methods.</p>
<p>It’s very likely that these “exceptional” cases will be very common, meaning that vendoring will be occuring.
This necesitates supporting vendored applications in distribution packages.</p>
</div>
<div id="you-don-t-need-to-package-the-universe">
<h2>“You don’t need to package the universe”</h2>
<p>Many proponents say that they have “already packaged most things”. For example in 389 Directory Server
of our 60 dependencies, only 2 were missing in Fedora (2021-02). However this overlooks the fact
that I do not want to package those 2 other crates just to move forward. I want to support 389 Directory Server
the <em>application</em> not all of it’s dependencies in a distribution.</p>
<p>This is also before we come to larger rust projects, such as Kanidm that has nearly 400 dependencies. The
likelihood that many of them are missing is high.</p>
<p>So you will need to package the universe. Maybe not all of it. But still a lot of it. It’s already
hard enough to contribute packages to a distribution. It becomes even harder when I need to submit 3, 10, or 100
more packages. It could be months before enough approvals were in place. It’s a staggering
amount of administration and work, which will discourage many contributors.</p>
<p>People have already contacted me to say that if they had to package crates to distribution packages to
contribute, they would give up and walk away. We’ve already lost future contributors.</p>
<p>Further to this Ruby, Python and many other languages today all recommend language native tools
such as rvm or virtualenv to avoid using distribution packaged libraries.</p>
<p>Packages in distributions should exist as a vehicle to ship bundled applications that are created
from their language native tools.</p>
</div>
<div id="we-will-update-your-dependencies-for-you">
<h2>“We will update your dependencies for you”</h2>
<p>A supposed benefit is that versions of crates in distributions will be updated in the background
according to semver rules.</p>
<p>If we had an exact version requirement (that was satisfiable), a silent background update will cause
this to no longer work - and will break the application from building. This would necesitate one of:</p>
<ul>
<li>A change to the Cargo.toml to remove the equality requirement - a requirement that may exist for good reason.</li>
<li>It will force the application to temporarily swap to vendoring instead.</li>
<li>The application will remain broken and unable to be updated until upstream resolves the need for the equality requirement.</li>
</ul>
<p>Background updates also ignore the state of your Cargo.lock file by removing it. A Cargo.lock file
is recommended to be checked in with binary applications in Rust, as evidence that shows “here is
an exact set of dependencies that upstream has tested and verified as building and working”.</p>
<p>To remove and ignore this file, means to remove the guarantees of quality from an upstream.</p>
<p>It is unlikely that packagers will run the entire test suite of an application to regain this
confidence. They will “apply the patch and pray” method - as they already do with other languages.</p>
<p>We can already see how background updates can have significant negative consequences on application stability. FreeIPA
has hundreds of dependencies, and it’s common that if any of them changes in small ways, it can cause
FreeIPA to fall over. This is not the fault of FreeIPA - it’s the fault of relying on so many small
moving parts that can change underneath your feet without warning. FreeIPA would strongly benefit from
vendoring to improve it’s stability and quality.</p>
<p>Inversely, it can cause hesitation to updating libraries - since there is now a risk of breaking
other applications that depend on them. We do not want people to be afraid of updates.</p>
</div>
<div id="we-can-respond-to-security-issues">
<h2>“We can respond to security issues”</h2>
<p>On the surface this is a strong argument, but in reality it does not hold up. The security issues
that face Rust are significantly different to that which affect C. In C it may be viable to patch
and update a dynamic library to fix an issue. It saves time because you only need to update and change
one library to fix everything.</p>
<p>Security issues are much rarer in Rust. When they occur, you will have to update and re-build all
applications depending on the affected library.</p>
<p>Since this rebuilding work has to occur, where the security fix is applied is irrelevant. This frees
us to apply the fixes in a different way to how we approach C.</p>
<p>It is better to apply the fixes in a consistent and universal manner. There <em>will</em> be applications
that are vendored due to vendoring exceptions, there is now duplicated work and different
processes to respond to both distribution crates, and vendored applications.</p>
<p>Instead all applications could be vendored, and tooling exists that would examine the Cargo.toml to
check for insecure versions (RustSec/cargo-audit does this for example). The Cargo.toml’s can be
patched, and applications tested and re-vendored. Even better is these changes could easily then be forwarded to
upstreams, allowing every distribution and platform to benefit from the work.</p>
<p>In the cases that the upstream can not fix the issue, then Cargo’s native patching tooling can
be used to supply fixes directly into vendored sources for rare situations requiring it.</p>
</div>
<div id="patching-20-vulnerable-crates-doesn-t-scale-we-need-to-patch-in-one-place">
<h2>“Patching 20 vulnerable crates doesn’t scale, we need to patch in one place!”</h2>
<p>A common response to the previous section is that the above process won’t scale as we need to find
and patch 20 locations compared to just one. It will take “more human effort”.</p>
<p>Today, when a security fix comes out, every distribution’s security teams will have to be made aware of
this. That means - OpenSUSE, Fedora, Debian, Ubuntu, Gentoo, Arch, and many more groups all have to
become aware and respond. Then each of these projects security teams will work with their maintainers
to build and update these libraries. In the case of SUSE and Red Hat this means that multiple developers
may be involved, quality engineering will be engaged to test these changes. Consumers of that library
will re-test their applications in some cases to ensure there are no faults of the components they
rely upon. This is all before we approach the fact that each of these distributions have many supported
and released versions they likely need to maintain so this process may be repeated for patching and
testing multiple versions in parallel.</p>
<p>In this process there are a few things to note:</p>
<ul>
<li>There is a huge amount of human effort today to keep on top of security issues in our distributions.</li>
<li>Distributions tend to be isolated and can’t share the work to resolve these - the changes to the rpm specs in SUSE won’t help Debian for example.</li>
<li>Human error occurs in all of these layers causing security issues to go un-fixed or breaking a released application.</li>
</ul>
<p>To suggest that rust and vendoring somehow makes this harder or more time consuming is discounting
the huge amount of time, skill, and effort already put in by people to keep our C based distributions functioning
today.</p>
<p>Vendored Rust won’t make this process easier or harder - it just changes the nature of the effort
we have to apply as maintainers and distributions. It shifts our focus from “how do we ensure this
library is secure” to “how do we ensure this <em>application</em> made from many libraries is secure”. It
allows further collaboration with upstreams to be involved in the security update process, which ends up
benefiting <em>all</em> distributions.</p>
</div>
<div id="it-doesn-t-duplicate-effort">
<h2>“It doesn’t duplicate effort”</h2>
<p>It does. By the very nature of both distribution libraries and vendored applications needing to
exist in a distribution, there will become duplicated but seperate processes and …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://fy.blackhats.net.au/blog/html/2021/02/16/against_packaging_rust_crates.html">https://fy.blackhats.net.au/blog/html/2021/02/16/against_packaging_rust_crates.html</a></em></p>]]>
            </description>
            <link>https://fy.blackhats.net.au/blog/html/2021/02/16/against_packaging_rust_crates.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26230935</guid>
            <pubDate>Mon, 22 Feb 2021 21:32:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Postgres regex search over 10k GitHub repositories (using only a MacBook)]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26230879">thread link</a>) | @boyter
<br/>
February 22, 2021 | https://devlog.hexops.com/2021/postgres-regex-search-over-10000-github-repositories | <a href="https://web.archive.org/web/*/https://devlog.hexops.com/2021/postgres-regex-search-over-10000-github-repositories">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>In this article, we share empirical measurements from our experiments in using Postgres to index and search over 10,000 top GitHub repositories using <code>pg_trgm</code> on only a Macbook.</p>

<p>This is a follow up to <a href="https://devlog.hexops.com/2021/postgres-trigram-search-learnings">“Postgres Trigram search learnings”</a>, in which we shared several learnings and beliefs about trying to use Postgres Trigram indexes as an alterative to Google’s <a href="https://github.com/google/zoekt">Zoekt</a> (“Fast trigram based code search”).</p>

<p>We share our results, as well as <a href="https://github.com/hexops/pgtrgm_emperical_measurements">the exact steps we performed, scripts, and lists of the top 20,000 repositories by stars/language on GitHub</a> so you can reproduce the results yourself should you desire.</p>

<h2 id="tldr">TL;DR</h2>

<p><strong>This article is extensive and more akin to a research paper than a blog post.</strong> If you’re interested in our conclusions, see <a href="#conclusions">conclusions</a> instead.</p>

<h2 id="goals">Goals</h2>

<p>We wanted to get empirical measurements for how suitable Postgres is in providing regexp search over documents, e.g. as an alterative to Google’s <a href="https://github.com/google/zoekt">Zoekt</a> (“Fast trigram based code search”). In specific:</p>

<ul>
  <li>How many repositories can we index on just a 2019 Macbook Pro?</li>
  <li>How fast are different regexp searches over the corpus?</li>
  <li>What Postgres 13 configuration gives best results?</li>
  <li>What other operational effects need consideration if seriously attempting to use Postgres as the backend for a regexp search engine?</li>
  <li>What is the best database schema to use?</li>
</ul>

<h2 id="hardware">Hardware</h2>

<p>We ran all tests on a 2019 Macbook Pro with:</p>

<ul>
  <li>2.3 GHz 8-Core Intel Core i9</li>
  <li>16 GB 2667 MHz DDR4</li>
</ul>

<p>During test execution, few other Mac applications were in use such that effectively all CPU/memory was available to Postgres.</p>

<h2 id="corpus">Corpus</h2>

<p>We scraped <a href="https://github.com/hexops/pgtrgm_emperical_measurements/tree/main/top_repos">lists of the top 1,000 repositories from the GitHub search API</a> ranked by stars for each of the following languages (~20.5k repositories in total):</p>

<ul>
  <li>C++, C#, CSS, Go, HTML, Java, JavaScript, MatLab, ObjC, Perl, PHP, Python, Ruby, Rust, Shell, Solidity, Swift, TypeScript, VB .NET, and Zig.</li>
</ul>

<p>Cloning all ~20.5k repositories in parallel took ~14 hours with a fast ~100 Mbps connection to GitHub’s servers.</p>

<h3 id="dataset-reduction">Dataset reduction</h3>

<p>We found the amount of disk space required by <code>git clone --depth 1</code> on these repositories to be a sizable ~412G for just 12,148 repositories - and so we put in place several processes for further reduce the dataset size by about 66%:</p>

<ul>
  <li>Removing <code>.git</code> directories resulted in a 30% reduction (412G -&gt; 290G, for 12,148 repositories)</li>
  <li>Removing files &gt; 1 MiB resulted in another 51% reduction (290G -&gt; 142G, for 12,148 repositories - note GitHub does not index files &gt; 384 KiB in their search engine)</li>
</ul>

<h2 id="database-insertion">Database insertion</h2>

<p>We <a href="https://github.com/hexops/pgtrgm_emperical_measurements/blob/main/cmd/corpusindex/main.go">concurrently inserted</a> the entire corpus into Postgres, with the following DB schema:</p>

<div><div><pre><code><span>CREATE</span> <span>EXTENSION</span> <span>IF</span> <span>NOT</span> <span>EXISTS</span> <span>pg_trgm</span><span>;</span>
<span>CREATE</span> <span>TABLE</span> <span>IF</span> <span>NOT</span> <span>EXISTS</span> <span>files</span> <span>(</span>
    <span>id</span> <span>bigserial</span> <span>PRIMARY</span> <span>KEY</span><span>,</span>
    <span>contents</span> <span>text</span> <span>NOT</span> <span>NULL</span><span>,</span>
    <span>filepath</span> <span>text</span> <span>NOT</span> <span>NULL</span>
<span>);</span>
</code></pre></div></div>

<p>In total, this took around ~8 hours to complete and Postgres’s entire on-disk utilization was 101G.</p>

<h2 id="creating-the-trigram-index">Creating the Trigram index</h2>

<p>We tried three separate times to index the dataset using the following GIN Trigram index:</p>

<div><div><pre><code>CREATE INDEX IF NOT EXISTS files_contents_trgm_idx ON files USING GIN (contents gin_trgm_ops);
</code></pre></div></div>

<ul>
  <li><strong>In the first attempt, we hit an OOM after 11 hours and 34 minutes.</strong> This was due to a rapid spike in memory usage at the very end of indexing. We used a <a href="https://github.com/hexops/pgtrgm_emperical_measurements#configuration-attempt-1-indexing-failure-oom">fairly aggressive</a> Postgres configuration with a very large max WAL size, so it was not entirely unexpected.</li>
  <li><strong>In the second attempt, we ran out of SSD disk space after ~27 hours</strong>. Notable is that the disk space largely grew towards the end of indexing, similar to when we faced an OOM - it was not a gradual increase over time. For this attempt, we used the excellent <a href="https://pgtune.leopard.in.ua/#/">pgtune</a> tool to reduce our first Postgres configuration as follows:</li>
</ul>

<div><div><pre><code>shared_buffers = 4GB → 2560MB
effective_cache_size = 12GB → 7680MB
maintenance_work_mem = 16GB → 1280MB
default_statistics_target = 100 → 500
work_mem = 5242kB → 16MB
min_wal_size = 50GB → 4GB
max_wal_size = 4GB → 16GB
max_parallel_workers_per_gather = 8 → 4
max_parallel_maintenance_workers = 8 → 4
</code></pre></div></div>
<ul>
  <li><strong>In our third and final attempt, we cut the dataset in half and indexing succeeded after 22 hours.</strong> In specific, we deleted half of the files in the database (from 19,441,820 files / 178GiB of data to 9,720,910 files / 82 GiB of data.) The Postgres configuration used was the same as in attempt 2.</li>
</ul>

<h2 id="indexing-performance-memory-usage">Indexing performance: Memory usage</h2>

<p>In our first attempt, we see the reported <code>docker stats</code> memory usage of the container grow up to 12 GiB (chart shows MiB of memory used over time):</p>

<p><img width="981" alt="image" src="https://user-images.githubusercontent.com/3173176/107313722-56bbac80-6a50-11eb-94c7-8e13ea095053.png"></p>

<p>In our second and third attempts, we see far less memory usage (~1.6 GiB consistently):</p>

<p><img width="980" alt="image" src="https://user-images.githubusercontent.com/3173176/107314104-350ef500-6a51-11eb-909f-2f1b524d29b2.png"></p>

<p><img width="980" alt="image" src="https://user-images.githubusercontent.com/3173176/107315387-ce3f0b00-6a53-11eb-886c-410f000f73bd.png"></p>

<h2 id="indexing-performance-cpu-usage">Indexing performance: CPU usage</h2>

<p>Postgres’ Trigram indexing appears to be mostly single-threaded (at least when indexing <em>a single table</em>, we test multiple tables later.)</p>

<p>In our first attempt, CPU usage for the container did not rise above 156% (one and a half virtual CPU cores):</p>

<p><img width="982" alt="image" src="https://user-images.githubusercontent.com/3173176/107313915-cc277d00-6a50-11eb-9282-62159a127966.png"></p>

<p>Our second attempt was around 150-200% CPU usage on average:</p>

<p><img width="980" alt="image" src="https://user-images.githubusercontent.com/3173176/107314168-507a0000-6a51-11eb-8a18-ec18752f7f16.png"></p>

<p>Our third attempt similarly saw an average of 150-200%, but with a brief spike towards the end to ~350% CPU:</p>

<p><img width="980" alt="image" src="https://user-images.githubusercontent.com/3173176/107315239-8324f800-6a53-11eb-9a5b-fcc61d1a7b59.png"></p>

<h2 id="indexing-performance-disk-io">Indexing performance: Disk IO</h2>

<p>Disk reads/writes during indexing averaged about ~250 MB/s for reads (blue) and writes (red). Native in-software tests show the same Macbook able to achieve read/write speeds of ~860 MB/s with &lt;5% affect on CPU utilization.</p>

<p><small>Addition made Feb 20, 2021:</small> We ran tests using native Postgres as well (instead of in Docker with a bind mount) and found better indexing and query performance, more on this below.</p>

<p><img width="599" alt="image" src="https://user-images.githubusercontent.com/3173176/106507903-ec6f9e80-6488-11eb-88a8-78e5b7aacfd6.png"></p>

<h2 id="indexing-performance-disk-space">Indexing performance: Disk space</h2>

<p>The database contains 9,720,910 files totalling 82.07 GiB:</p>

<div><div><pre><code>postgres=# select count(filepath) from files;
  count  
---------
 9720910
(1 row)

postgres=# select SUM(octet_length(contents)) from files;
     sum     
-------------
 88123563320
(1 row)
</code></pre></div></div>

<p><strong>Before indexing</strong>, we find that all of Postgres is consuming 54G:</p>

<div><div><pre><code>$ du -sh .postgres/
 54G	.postgres/
</code></pre></div></div>

<p>After <code>CREATE INDEX</code>, Postgres uses:</p>

<div><div><pre><code>$ du -sh .postgres/
 73G	.postgres/
</code></pre></div></div>

<p>Thus, the index size for 82 GiB of text is 19 GiB (or 23% of the data size.)</p>

<h2 id="database-startup-times">Database startup times</h2>

<p>From an operational standpoint, it is worth noting that if Postgres is starting clean (i.e. previous shutdown was graceful) then startup time is almost instantaneous: it begins accepting connections immediately and loads the index as needed.</p>

<p>However, if Postgres experienced a non-graceful termination during e.g. startup, it can take a hefty ~10 minutes with this dataset to start as it goes through an automated recovery process.</p>

<h2 id="queries-executed">Queries executed</h2>

<p>In total, we executed 19,936 search queries against the index. We chose queries which we expect give reasonably varying amounts of coverage over the trigram index (that is, queries whose trigrams are more or less likely to occur in many files):</p>

<table>
  <thead>
    <tr>
      <th>Regexp query</th>
      <th>Matching # files in entire dataset</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>var</code></td>
      <td>unknown (2m+ suspected)</td>
    </tr>
    <tr>
      <td><code>error</code></td>
      <td>1,479,452</td>
    </tr>
    <tr>
      <td><code>123456789</code></td>
      <td>59,841</td>
    </tr>
    <tr>
      <td><code>fmt\.Error</code></td>
      <td>127,895</td>
    </tr>
    <tr>
      <td><code>fmt\.Println</code></td>
      <td>22,876</td>
    </tr>
    <tr>
      <td><code>bytes.Buffer</code></td>
      <td>34,554</td>
    </tr>
    <tr>
      <td><code>fmt\.Print.*</code></td>
      <td>37,319</td>
    </tr>
    <tr>
      <td><code>ac8ac5d63b66b83b90ce41a2d4061635</code></td>
      <td>0</td>
    </tr>
    <tr>
      <td><code>d97f1d3ff91543[e-f]49.8b07517548877</code></td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<details>
<summary>Detailed breakdown</summary>
<div>

    <table>
      <thead>
        <tr>
          <th>Query</th>
          <th>Result Limit</th>
          <th>Times executed</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><code>var</code></td>
          <td>10</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>var</code></td>
          <td>100</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>var</code></td>
          <td>1000</td>
          <td>100</td>
        </tr>
        <tr>
          <td><code>var</code></td>
          <td>unlimited</td>
          <td>4</td>
        </tr>
        <tr>
          <td><code>error'</code></td>
          <td>10</td>
          <td>2000</td>
        </tr>
        <tr>
          <td><code>error'</code></td>
          <td>100</td>
          <td>2000</td>
        </tr>
        <tr>
          <td><code>error'</code></td>
          <td>1000</td>
          <td>200</td>
        </tr>
        <tr>
          <td><code>error'</code></td>
          <td>unlimited</td>
          <td>18</td>
        </tr>
        <tr>
          <td><code>123456789</code></td>
          <td>10</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>123456789</code></td>
          <td>100</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>123456789</code></td>
          <td>1000</td>
          <td>100</td>
        </tr>
        <tr>
          <td><code>123456789</code></td>
          <td>unlimited</td>
          <td>2</td>
        </tr>
        <tr>
          <td><code>fmt\.Error</code></td>
          <td>10</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>fmt\.Error</code></td>
          <td>100</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>fmt\.Error</code></td>
          <td>1000</td>
          <td>100</td>
        </tr>
        <tr>
          <td><code>fmt\.Error</code></td>
          <td>unlimited</td>
          <td>2</td>
        </tr>
        <tr>
          <td><code>fmt\.Println</code></td>
          <td>10</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>fmt\.Println</code></td>
          <td>100</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>fmt\.Println</code></td>
          <td>1000</td>
          <td>100</td>
        </tr>
        <tr>
          <td><code>fmt\.Println</code></td>
          <td>unlimited</td>
          <td>2</td>
        </tr>
        <tr>
          <td><code>bytes.Buffer</code></td>
          <td>10</td>
          <td>4</td>
        </tr>
        <tr>
          <td><code>bytes.Buffer</code></td>
          <td>100</td>
          <td>4</td>
        </tr>
        <tr>
          <td><code>bytes.Buffer</code></td>
          <td>1000</td>
          <td>4</td>
        </tr>
        <tr>
          <td><code>bytes.Buffer</code></td>
          <td>unlimited</td>
          <td>2</td>
        </tr>
        <tr>
          <td><code>fmt\.Print.*</code></td>
          <td>10</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>fmt\.Print.*</code></td>
          <td>100</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>fmt\.Print.*</code></td>
          <td>1000</td>
          <td>100</td>
        </tr>
        <tr>
          <td><code>fmt\.Print.*</code></td>
          <td>unlimited</td>
          <td>2</td>
        </tr>
        <tr>
          <td><code>ac8ac5d63b66b83b90ce41a2d4061635</code></td>
          <td>10</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>ac8ac5d63b66b83b90ce41a2d4061635</code></td>
          <td>100</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>ac8ac5d63b66b83b90ce41a2d4061635</code></td>
          <td>1000</td>
          <td>100</td>
        </tr>
        <tr>
          <td><code>ac8ac5d63b66b83b90ce41a2d4061635</code></td>
          <td>unlimited</td>
          <td>2</td>
        </tr>
        <tr>
          <td><code>d97f1d3ff91543[e-f]49.8b07517548877</code></td>
          <td>10</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>d97f1d3ff91543[e-f]49.8b07517548877</code></td>
          <td>100</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>d97f1d3ff91543[e-f]49.8b07517548877</code></td>
          <td>1000</td>
          <td>100</td>
        </tr>
        <tr>
          <td><code>d97f1d3ff91543[e-f]49.8b07517548877</code></td>
          <td>unlimited</td>
          <td>2</td>
        </tr>
      </tbody>
    </table>

  </div>
</details>

<h2 id="query-performance">Query performance</h2>

<p>In total, we executed 19,936 search queries against the database (linearly, not in parallel) which completed in the following times:</p>

<table>
  <thead>
    <tr>
      <th>Time bucket</th>
      <th>Percentage of queries</th>
      <th>Number of queries</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Under 50ms</td>
      <td>30%</td>
      <td>5,933</td>
    </tr>
    <tr>
      <td>Under 250ms</td>
      <td>41%</td>
      <td>8,088</td>
    </tr>
    <tr>
      <td>Under 500ms</td>
      <td>52%</td>
      <td>10,275</td>
    </tr>
    <tr>
      <td>Under 750ms</td>
      <td>63%</td>
      <td>12,473</td>
    </tr>
    <tr>
      <td>Under 1s</td>
      <td>68%</td>
      <td>13,481</td>
    </tr>
    <tr>
      <td>Under 1.5s</td>
      <td>74%</td>
      <td>14,697</td>
    </tr>
    <tr>
      <td>Under 3s</td>
      <td>79%</td>
      <td>15,706</td>
    </tr>
    <tr>
      <td>Under 25s</td>
      <td>79%</td>
      <td>15,708</td>
    </tr>
    <tr>
      <td>Under 30s</td>
      <td>99%</td>
      <td>19,788</td>
    </tr>
  </tbody>
</table>

<h2 id="query-performance-vs-planning-time">Query performance vs. planning time</h2>

<p>The following scatter plot shows how 79% of queries executed in under 3s (Y axis, in ms), while Postgres’s query planner had planned them for execution in under 100-250ms generally (X axis, in ms):</p>

<p><img width="1252" alt="image" src="https://user-images.githubusercontent.com/3173176/107848471-ef379100-6db0-11eb-8396-4d156a179aae.png"></p>

<p>If we expand the view to include all queries, we start to get a picture of just how outlier these 21% of queries are (note that the small block of dots in the bottom left represents the same diagram shown above):</p>

<p><img width="1250" alt="image" src="https://user-images.githubusercontent.com/3173176/107848517-3cb3fe00-6db1-11eb-9652-e65d7d88fe36.png"></p>

<h2 id="query-time-vs-cpu--memory-usage">Query time vs. CPU &amp; Memory usage</h2>

<p>The following image shows:</p>

<ul>
  <li>(top) Query time in milliseconds</li>
  <li>(middle) CPU usage percentage (e.g. 801% refers to 8 out of 16 virtual CPU cores being consumed)</li>
  <li>(bottom) Memory usage in MiB.</li>
</ul>

<p><img width="1255" alt="image" src="https://user-images.githubusercontent.com/3173176/107848716-efd12700-6db2-11eb-8e8b-a8141a6bdb0b.png"></p>

<p>Notable insights from this are:</p>

<ul>
  <li>The large increase in resource usage towards the end is when we began executing queries with no <code>LIMIT</code>.</li>
  <li>CPU usage does not exceed 138%, until the spike at the end.</li>
  <li>Memory usage does not exceed 42 MiB, until the spike at the end.</li>
</ul>

<p>We suspect <code>pg_trgm</code> is single-threaded within the scope of a single table, but with <a href="https://www.postgresql.org/docs/10/ddl-partitioning.html">table data partitioning</a> (or splitting data into multiple tables with subsets of the data), we suspect better parallelism could be achieved.</p>

<h2 id="investigating-slow-queries">Investigating slow queries</h2>

<p>If we plot the number of index rechecks (X axis) vs. execution time (Y axis), we can clearly see one of the most significant aspects of slow queries is that they have many more index rechecks:</p>

<p><img width="1036" alt="image" src="https://user-images.githubusercontent.com/3173176/107849660-fc0cb280-6db9-11eb-9c10-cb7e74366ab7.png"></p>

<p>And if we look at <a href="https://github.com/hexops/pgtrgm_emperical_measurements/blob/main/query_logs/query-run-3.log#L3-L24">the <code>EXPLAIN ANALYZE</code> output for one of these queries</a> we can also confirm <code>Parallel Bitmap Heap Scan</code> is slow due to <code>Rows Removed by Index Recheck</code>.</p>

<h2 id="table-splitting">Table splitting</h2>

<p>Splitting up the search index into multiple smaller tables seems like an obvious approach to getting <code>pg_trgm</code> to use multiple CPU cores. We tried this by taking the same exact data set and splitting it into 200 tables, and found …</p></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://devlog.hexops.com/2021/postgres-regex-search-over-10000-github-repositories">https://devlog.hexops.com/2021/postgres-regex-search-over-10000-github-repositories</a></em></p>]]>
            </description>
            <link>https://devlog.hexops.com/2021/postgres-regex-search-over-10000-github-repositories</link>
            <guid isPermaLink="false">hacker-news-small-sites-26230879</guid>
            <pubDate>Mon, 22 Feb 2021 21:26:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The How and Why of End-to-End Testing]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26230422">thread link</a>) | @jasonfb
<br/>
February 22, 2021 | https://blog.jasonfleetwoodboldt.com/2021/02/22/the-how-and-why-of-end-to-end-testing/ | <a href="https://web.archive.org/web/*/https://blog.jasonfleetwoodboldt.com/2021/02/22/the-how-and-why-of-end-to-end-testing/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

		<div>

			
<p>Perhaps the most significant and under-appreciated aspect of Rails&nbsp;<em>and</em>&nbsp;Agile software development of the last roughly 15 years is the culture and discipline around&nbsp;<strong>testing</strong>&nbsp;and&nbsp;<strong>test-driven development</strong>.</p>



<p>I’ve never come to understand why testing and TDD is often maligned by the loudest, most vocal developers:&nbsp;<strong><em>It’s too slow, it takes longer, the boss didn’t want or ask for it,&nbsp;</em></strong>they’ll say.</p>



<p>You don’t hear about these developers or this ideology often in professional circles, but you encounter them quickly in the wild west of freelance development.</p>



<p><strong>[SEIZURE WARNING: There is an animated GIF towards the bottom of this page that flashes. Please disable animated GIFs if you are susceptible to flashing lights.]</strong></p>



<p>Indeed, much of the popular and discussed rhetoric in the community of Rails is about a codebase’s test suite (a suite of tests for your whole application, this collective is called “the tests” or “the specs”): How much of your codebase is covered (as measured in %, which I discuss further below)? How easy are the tests to write? Do they use factories or fixtures? How brittle are they? Do they test the right things and are they valuable?</p>



<p>All of these are correct questions. Although there is no substitute for the day-in-day-out practice of this to become great at testing, I will try to offer some broad ‘best practice’ answers to these questions.</p>



<p>The enlightened developers don’t ask or care about whether or not the boss told us to write a tested codebase. We just know the answers to the above questions and do what’s right for the codebase: write specs.</p>



<p>Testing has varying degrees, varying methods, varying strengths.</p>



<p>In&nbsp;<a href="https://sandimetz.com/99bottles"><em>99 Bottles of OOP</em></a>, Metz, Owen, and Stankus make this interesting observation:</p>



<blockquote><p>Belief in the value of TDD has become mainstream, and the pressure to follow this practice approaches an unspoken mandate. Acceptance of this mandate is illustrated by the fact that it’s common for folks who don’t test to tender sheepish apologies. Even those who don’t test seem to believe they ought to do so.</p><cite>(Metz, et al, 99 Bottles of OOP, Second Edition, 2020. p 43)</cite></blockquote>



<p>So testing exists in a murky space: The top dev shops and teams know it is essential, but its implementation is inconsistent. Sadly, I’ve seen lots of development happen where people either just don’t write tests, write tests blindly, use tests as a cudgel, or skip end-to-end testing altogether.</p>



<p>Many years in this industry have led me to what seems like an extreme position.&nbsp;<strong>Not writing tests should be seen as akin to malpractice in software development. Hiring someone to write untested code should be outlawed.</strong></p>



<p>Having a tested codebase is absolutely the most significant benchmark in producing quality software today. If you are producing serious application development but you don’t have tests, you have already lost.</p>



<p>Having a good test suite is not only the&nbsp;<strong>benchmark of quality</strong>, it means that you can&nbsp;<strong>refactor with confidence</strong>.</p>



<p>There are two kinds of tests you should learn and write:</p>



<ul><li><strong>Unit testing&nbsp;</strong>(also called model testing or black-box testing)</li><li><strong>End-to-end testing&nbsp;</strong>(also called integration testing, feature testing, or system tests)</li></ul>



<p>These go by different names. Focus on the how and why of testing and don’t get lost in the implementation details of the different kinds of tests. (To learn to do testing in Ruby, you can check out <a href="https://blog.jasonfleetwoodboldt.com/courses/stepping-up-rails/">my course</a> where I go over all the details.)</p>







<p><strong>Unit tests&nbsp;</strong>are the “lowest-level” tests. In unit testing, we are testing only one single unit of code: Typically for Rails, a model. When we talk about Unit testing in other languages, it means the same as it does for Rails, but might be applied in other contexts.</p>



<p><strong>The thing you are testing is a black box. In your test, you will give your black box some inputs, tell it to do something, and assert that a specific output has been produced. The internals (implementation details) of the black box should not be known to your unit test.</strong></p>



<p>This fundamental tenet of unit testing is probably one of the single most commonly repeated axioms of knowledge in software development today.</p>



<p>The way to miss the boat here (unfortunately) is to follow the axiom strictly but misunderstand&nbsp;<em>why</em>&nbsp;you are doing it.</p>



<p>Testing, and especially learning to practice test-<em>driven</em>&nbsp;development (that’s when you force yourself&nbsp;<strong>not</strong>&nbsp;to write any code unless you write a test&nbsp;<strong>first</strong>), is in fact a lot deeper and more significant than just about quality, refactoring, and black boxes. (Although if you’ve learned that much by now you’re on the right track.)</p>



<p>Most people think that software, especially web software, is written once and then done. This is a fallacy: Any serious piece of software today is iterated and iterated. Even if you are writing an application for rolling out all at once, on the web there should always be a feedback loop.</p>



<p>Perhaps one of the worst and most problematic anti-patterns I’ve ever seen is when contractors write code, it is deployed, and nobody ever looks at any error logs. Or any stack-traces. Or even at the database records. (Typically this happens less in the context of companies hiring employees because employees tend to keep working for your company on an ongoing basis whereas contractors tend to ‘deliver’ the product and then leave.)</p>



<p>It’s not just about “catching a bug’” here or there. Or tweaking or modifying the software once it’s live. (Which, to be fair, most developers don’t actually like to do.)</p>



<p><strong>It’s about the fact that once it is live, anything and everything can and will happen. As a result, the data in your data stores might get into all kinds of states you weren’t expecting.</strong>&nbsp;Or maybe someone visits your website in a browser that doesn’t support the Javascript syntax you used. Or maybe&nbsp;<em>this</em>, or maybe&nbsp;<em>that</em>. It’s always&nbsp;<em>something</em>.</p>



<p>This is&nbsp;<strong>the marriage of testing &amp; ‘real life’</strong>: You want your tests to be ‘as isolated’ as possible, yet at the same time ‘as realistic’ as they&nbsp;<em>need</em>&nbsp;to be in order to anticipate what your users will experience.</p>



<p>That’s the right balance. Your code doesn’t exist in a vacuum, and the test environment is only a figment of your imagination. The unit test is&nbsp;<strong><em>valuable</em></strong>&nbsp;to you because it is as realistic as it needs to be to mimic what will happen to your app in the real, wild world of production.</p>



<p><strong>With unit testing, you aren’t actually putting the whole application through its places: You’re just testing one unit against a set of assertions.</strong></p>



<p>In the wild (that is, real live websites), all kinds of chaos happens. Your assumptions that&nbsp;<em>user_id</em>&nbsp;would never be&nbsp;<em>nil</em>, for example, proves out not to be the case in one small step of the workflow because the user hasn’t been assigned yet. (Stop me if you’ve heard this one before.)</p>



<p>You never wrote a spec for the user_id being nil, because you assumed that that could never happen. Well, it did. Or rather, it might.</p>



<p><strong><em>Many developers, especially the ones with something to prove, get too focused on unit testing.</em></strong>&nbsp;For one thing, they use the&nbsp;<em>percentage of codebase covered</em>&nbsp;as a badge of honor.</p>



<h2 id="5a3c">Percentage of Codebase Covered</h2>



<p>When you run your tests, a special tool called a coverage reporter can scan the lines of code in your application to determine if that line of code was run through during your test. It shows you which lines got the test to run over them and which lines were ‘missed.’</p>



<p>It doesn’t tell you if your test was correct, that it asserted the correct thing of course. It just tells you where you’ve missed lines of code. The typical benchmark for a well-tested Rails application is about 85–95% test coverage. (Because of various nuanced factors, there are always some files that you can’t or don’t need to test— typically not your application files.)</p>



<p>Here I use a tool in Ruby called&nbsp;<a href="https://rubygems.org/gems/simplecov-rcov">simplecov-rcov</a>&nbsp;to show which lines (precisely, line-by-line, and file-by-file) are covered. Here in this baby little project of mine, I have an unfortunate 36.55% of my codebase covered:</p>



<figure><img src="https://dzw5z73631yuc.cloudfront.net/wp-content/uploads/2021/02/22234926/Screen-Shot-2021-02-22-at-10.21.48-AM-1024x827.png" alt="example coverage report" srcset="https://dzw5z73631yuc.cloudfront.net/wp-content/uploads/2021/02/22234926/Screen-Shot-2021-02-22-at-10.21.48-AM-1024x827.png 1024w, https://dzw5z73631yuc.cloudfront.net/wp-content/uploads/2021/02/22234926/Screen-Shot-2021-02-22-at-10.21.48-AM-300x242.png 300w, https://dzw5z73631yuc.cloudfront.net/wp-content/uploads/2021/02/22234926/Screen-Shot-2021-02-22-at-10.21.48-AM-768x620.png 768w, https://dzw5z73631yuc.cloudfront.net/wp-content/uploads/2021/02/22234926/Screen-Shot-2021-02-22-at-10.21.48-AM.png 1091w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>As you see, the files are sorted with the least covered files shown up top. The top files are in red and say “0.00 %” covered because the test suite does not go into that file.</p>



<p>When I click into the file, I can actually see which lines are covered and uncovered, in red &amp; green like so:</p>



<figure><img src="https://dzw5z73631yuc.cloudfront.net/wp-content/uploads/2021/02/22235012/Screen-Shot-2021-02-22-at-10.23.50-AM-1024x464.png" alt="Code coverage report showing an untested line of Ruby" srcset="https://dzw5z73631yuc.cloudfront.net/wp-content/uploads/2021/02/22235012/Screen-Shot-2021-02-22-at-10.23.50-AM-1024x464.png 1024w, https://dzw5z73631yuc.cloudfront.net/wp-content/uploads/2021/02/22235012/Screen-Shot-2021-02-22-at-10.23.50-AM-300x136.png 300w, https://dzw5z73631yuc.cloudfront.net/wp-content/uploads/2021/02/22235012/Screen-Shot-2021-02-22-at-10.23.50-AM-768x348.png 768w, https://dzw5z73631yuc.cloudfront.net/wp-content/uploads/2021/02/22235012/Screen-Shot-2021-02-22-at-10.23.50-AM.png 1042w" sizes="(max-width: 1024px) 100vw, 1024px"></figure>



<p>(Here’s a great example of that “it only happens in the wild” thing I was talking about earlier. In theory, I should never get passed a room id (params[:room]) that is not in my database [see line 4], but in practice, for some reason, while I was debugging I did. So I added a small little guard to catch for this while debugging, thus making the line of code inside the if statement uncovered by my test suite.)</p>



<p>Correlating the total percentage of test coverage to your code quality and/or value of the tests is often a fallacy:&nbsp;<em>Look at the percentage of codebase covered, but not every day.</em></p>



<p>The problem with over-emphasis on unit testing is the dirty little secret of unit testing:&nbsp;<strong>Unit tests rarely catch bugs.</strong></p>



<p>So why do we unit test at all then?&nbsp;<strong>Unit tests do catch all of your problems when you are upgrading.</strong></p>



<p><em>You should unit test your code for the following&nbsp;</em><strong><em>four</em></strong><em>&nbsp;reasons:</em></p>



<p>(1) It helps you think about and structure your code more consistently.</p>



<p>(2) It will help you produce cleaner, more easily reasoned code as you refactor.</p>



<p>(3) Refactoring will, in turn, reveal more about&nbsp;<strong><em>the form</em></strong>&nbsp;(or shape) of your application that you couldn’t realize upfront.</p>



<p>(4) Your unit tests will catch bugs quickly when you upgrade Rails.</p>



<p>That’s it. Notice that&nbsp;<strong><em>not listed here</em></strong>is ‘catching regressions’ (or bugs). That’s significant because many developers think unit testing cover all of their bases. Not only do they&nbsp;<strong>not</strong>&nbsp;cover all of your bases: They don’t even catch or prevent regressions (bugs) in live production apps very often.</p>



<p><strong>Testing is important. Unit testing and end-to-end testing are both important, but between the two, end-to-end testing is the most important of all.</strong></p>






</div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.jasonfleetwoodboldt.com/2021/02/22/the-how-and-why-of-end-to-end-testing/">https://blog.jasonfleetwoodboldt.com/2021/02/22/the-how-and-why-of-end-to-end-testing/</a></em></p>]]>
            </description>
            <link>https://blog.jasonfleetwoodboldt.com/2021/02/22/the-how-and-why-of-end-to-end-testing/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26230422</guid>
            <pubDate>Mon, 22 Feb 2021 20:51:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why It's a Mistake for Publishers to Treat Player Complaints as White Noise]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26230258">thread link</a>) | @ronwilliams821
<br/>
February 22, 2021 | https://www.subspace.com/blog/why-its-a-mistake-for-publishers-to-treat-player-complaints-as-white-noise | <a href="https://web.archive.org/web/*/https://www.subspace.com/blog/why-its-a-mistake-for-publishers-to-treat-player-complaints-as-white-noise">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div data-layout-label="Post Body" data-type="item" id="item-6033e69be3c69f170b958492"><div><div><div data-block-type="2" id="block-e9364474264d18dc38c6"><div><p><strong>TL;DR</strong></p><p><em>With the rise in online gaming popularity since the coronavirus pandemic forced people indoors, network issues like lag have resulted in an outpour of player complaints. Game publishers are now looking for solutions to avoid revenue loss, game abandonment, and community churn.</em></p><p><em>Estimated read time: 8 minutes</em></p><p>----------</p><p>The online game market has exploded over the last few years.</p><p>According to estimates, the <a href="https://newzoo.com/insights/articles/newzoo-games-market-numbers-revenues-and-audience-2020-2023/">video game market will hit $200 billion in revenue by 2023</a>.</p><p>The global pandemic has only escalated that growth by forcing people indoors, many of whom took up gaming to escape and connect with others globally.</p><p>As a result of increased internet traffic from more people logging on to play, network quality problems like lag continues to be a hot-button issue that many game publishers are struggling to figure out.</p><p><strong>This is resulting in players and gaming communities flooding to the internet to voice their complaints.</strong></p><p>As we will explore in this article, this can lead to many problems for game publishers, including game abandonment, community churn, and losses in revenue.</p><p>First, let’s talk about why lag is a huge problem when it comes to gaming.</p></div></div><div data-block-type="5" id="block-yui_3_17_2_1_1614014215234_9811"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5e28bb4e630fb46e596e182c/1614014539752-DNF55GYI92A6VSCSYUAS/ke17ZwdGBToddI8pDm48kI_uL0Lu3YmDOWRp8GvCA6hZw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVFnmPBfss0IfMxxMBMhA9DNbDUNlGYSEAZNEHrX26nAypuG45vQwBxdpDrCGUSSl5w/lag.gif" data-image="https://images.squarespace-cdn.com/content/v1/5e28bb4e630fb46e596e182c/1614014539752-DNF55GYI92A6VSCSYUAS/ke17ZwdGBToddI8pDm48kI_uL0Lu3YmDOWRp8GvCA6hZw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVFnmPBfss0IfMxxMBMhA9DNbDUNlGYSEAZNEHrX26nAypuG45vQwBxdpDrCGUSSl5w/lag.gif" data-image-dimensions="478x432" data-image-focal-point="0.5,0.5" alt="lag.gif" data-load="false" data-image-id="6033e846e0310a4198fe84a3" data-type="image">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1614014215234_11158"><div><h3>Why Lag Is Enemy #1 When it Comes To Gaming</h3><p>We can all agree that lag is a pain.</p><p>It’s a problem that haunts publishers and players alike.</p><p>On a micro scale, it can cause a single player to leave a game. On a macro scale, one player’s lag can negatively impact the quality of experience for multiple players, as noted in <a href="https://www.cs.montana.edu/techreports/1213/Howard.pdf">this study from Montana State University</a>.</p></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1614014215234_19458"><div><p><strong>“The lag of just one player can cause a cascading impact on the Quality of Experience (QoE) of other players. … Having a group member lag decreases the experience for everyone. … Current lag mitigation techniques are not sufficient when dealing with this cascading impact and may actually be decreasing the overall QoE of the players.”&nbsp;</strong></p><p>- Montana State University Study on The Cascading Impact of Lag on User Experience in Cooperative Multiplayer Games</p></div></div><div data-block-type="2" id="block-yui_3_17_2_1_1614014215234_21686"><p>Since gaming’s popularity is on the rise, this topic has been studied in great detail. In a <a href="https://www.researchgate.net/publication/220425928_How_sensitive_are_online_gamers_to_network_quality">study by ResearchGate</a> exploring player sensitivity to network quality, you can see the adverse effects network latency has on gameplay over time.</p></div><div data-block-type="5" id="block-yui_3_17_2_1_1614014215234_22708"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5e28bb4e630fb46e596e182c/1614014806841-RK504HJNERJIX369AZOR/ke17ZwdGBToddI8pDm48kJK4Mm1kch8SFO9ZNkN1NT97gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmN9YSRtfoTLg6dUq-6F17A0FFZK5fArcnK1IqGweyunyWChwIwkIJ_P7MaZif-uMs/image6.png" data-image="https://images.squarespace-cdn.com/content/v1/5e28bb4e630fb46e596e182c/1614014806841-RK504HJNERJIX369AZOR/ke17ZwdGBToddI8pDm48kJK4Mm1kch8SFO9ZNkN1NT97gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmN9YSRtfoTLg6dUq-6F17A0FFZK5fArcnK1IqGweyunyWChwIwkIJ_P7MaZif-uMs/image6.png" data-image-dimensions="1080x1080" data-image-focal-point="0.5,0.5" alt="image6.png" data-load="false" data-image-id="6033e9559a5b89551b05b03b" data-type="image" src="https://www.subspace.com/blog/image6.png">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1614014215234_24056"><div><p><strong>For game publishers, this poses a significant problem. One vocal player with a bad experience can create a ripple effect throughout a game’s online community, especially if they have influence.</strong></p><p>Just this year, Call of Duty came under fire during their league playoff series when Trei “Zero” Morris experienced <em>“game-defining connection issues” </em>that ultimately led to his team, The London Royal Ravens losing their series.</p><p>This incident led to extensive <a href="https://www.espn.com/esports/story/_/id/29705750/call-duty-league-continues-connectivity-issues-playoffs">press coverage from ESPN</a> and a subsequent <a href="https://twitter.com/skrapzg/status/1296541281523568651?s=20">tweetstorm</a> led by Ravens player <a href="https://twitter.com/skrapzg?s=20">@skrapz</a><strong>—who, by the way, has 79,000 followers on Twitter</strong>.</p></div></div><div data-block-type="5" id="block-yui_3_17_2_1_1614014215234_28207"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5e28bb4e630fb46e596e182c/1614014966189-EP5LV49F1BXJ33CM8JDW/ke17ZwdGBToddI8pDm48kMw_TYhSJG2CznVP88DMG_t7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmpEfzMuPQaYRVRNSqbP0nCEaFSuz89K8EeUXtbCW9NL11Lw5leMYhyh_z4aP_UKU_/image3.png" data-image="https://images.squarespace-cdn.com/content/v1/5e28bb4e630fb46e596e182c/1614014966189-EP5LV49F1BXJ33CM8JDW/ke17ZwdGBToddI8pDm48kMw_TYhSJG2CznVP88DMG_t7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QHyNOqBUUEtDDsRWrJLTmpEfzMuPQaYRVRNSqbP0nCEaFSuz89K8EeUXtbCW9NL11Lw5leMYhyh_z4aP_UKU_/image3.png" data-image-dimensions="1102x1340" data-image-focal-point="0.5,0.5" alt="image3.png" data-load="false" data-image-id="6033e9f54103c9750a126fe7" data-type="image" src="https://www.subspace.com/blog/image3.png">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1614014215234_29951"><div><p><strong>As this example perfectly illustrates, lag can put a game publisher in the press for all the wrong reasons.</strong></p><h3>How Lag Drives Game Abandonment and Community Churn</h3><p>As noted in <a href="https://venturebeat.com/2016/04/17/how-latency-is-killing-online-gaming/">this article from VentureBeat</a>, online gaming customers are twice as likely to abandon a game when they experience a network delay (latency) of 500 additional milliseconds. One of the reasons this is a major problem is that players are a very vocal group. They are notorious for letting the world know they are unhappy when poor network conditions ruin their experience.</p><p><strong>Now, imagine the impact someone with real influence and a massive following can have when they voice their displeasure with your game.</strong></p><p>That’s EXACTLY what happened when <a href="https://theblast.com/c/snoop-dogg-goes-off-on-bill-gates-ea-sports-when-madden-server-goes-down-video">Snoop Dogg exploded on EA Sports</a> when Madden servers crashed.</p></div></div><div data-block-type="5" id="block-yui_3_17_2_1_1614014215234_38706"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5e28bb4e630fb46e596e182c/1614015171408-TF6KAF2DGVCNRFD6OLWW/ke17ZwdGBToddI8pDm48kKttrfbZ0gMrYzGve7xMNah7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UZHWjT4nTCkdj4JvCo3b04UDVfEyAoFE3s0a5qqZWHReG6v6ULRah83RgHXAWD5lbQ/image4.png" data-image="https://images.squarespace-cdn.com/content/v1/5e28bb4e630fb46e596e182c/1614015171408-TF6KAF2DGVCNRFD6OLWW/ke17ZwdGBToddI8pDm48kKttrfbZ0gMrYzGve7xMNah7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UZHWjT4nTCkdj4JvCo3b04UDVfEyAoFE3s0a5qqZWHReG6v6ULRah83RgHXAWD5lbQ/image4.png" data-image-dimensions="1999x1402" data-image-focal-point="0.5,0.5" alt="image4.png" data-load="false" data-image-id="6033eac0e6dc6a5f64959377" data-type="image" src="https://www.subspace.com/blog/image4.png">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1614014215234_40918"><div><p>His <a href="https://www.instagram.com/p/B--Xp__neI0/?utm_source=ig_web_copy_link">rant on Instagram</a> directly targeting EA Sports and Bill Gates now has 1.2 million views. That means 1.2 MILLION people were made aware of the server issues at EA Sports, many of whom chimed in voicing their own frustrations with the game.</p><p><strong>Not exactly the type of press any game publisher wants to attract.</strong></p><p>Now, consider the rise of livestreaming since lockdown began. We now see professional players and popular streamers with millions of fans tuning in to watch them play their favourite games.</p><p>What does this mean for game publishers?</p><p><strong>Streamers have the influence to negatively impact game downloads, gameplay, and community engagement when faced with lag and other connectivity issues.</strong></p><p>Consider these stats from a recent game survey our team at <a href="https://www.subspace.com/">Subspace</a> conducted that identified how players respond to lag interference:</p><ul data-rte-list="default"><li><p>32% of professional players (the people likely to have large streaming audiences) will stop playing a game altogether in response to lag.</p></li><li><p>42% of non-professional players react to lag by stopping gameplay.</p></li></ul><p>That means 74% (or 1,943) of total players surveyed stop playing a game when lag interferes.</p></div></div><div data-block-type="5" id="block-yui_3_17_2_1_1614014215234_53284"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5e28bb4e630fb46e596e182c/1614015326951-DVCVAOCUQ7ERNHP5AC7S/ke17ZwdGBToddI8pDm48kJQAkym-mVm4cWhsd70YmDAUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcmkJKBQ_orImNc1CCogPv7mE8pAxih1FOawmp1hWUIKreiWO2XPvz4aLyXLHevgdd/image7.png" data-image="https://images.squarespace-cdn.com/content/v1/5e28bb4e630fb46e596e182c/1614015326951-DVCVAOCUQ7ERNHP5AC7S/ke17ZwdGBToddI8pDm48kJQAkym-mVm4cWhsd70YmDAUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcmkJKBQ_orImNc1CCogPv7mE8pAxih1FOawmp1hWUIKreiWO2XPvz4aLyXLHevgdd/image7.png" data-image-dimensions="1070x540" data-image-focal-point="0.5,0.5" alt="image7.png" data-load="false" data-image-id="6033eb5e5551ec775e951686" data-type="image" src="https://www.subspace.com/blog/image7.png">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1614014215234_54940"><div><p>The impact illustrated by these stats can be detrimental to game publishers if the players have big streaming audiences or large followings on social media.</p><p>Like what happened with Snoop Dogg, this can lead to bad press.</p><p><strong>But, the potential impact goes deeper than that.</strong></p><p>When players and the community at large lose interest and abandon games due to lag and connectivity issues, game publishers risk facing a decrease in the ability to generate revenue.</p><h3>The Impact Lag Has On Revenue Potential</h3><p>The way game publishers make money has changed.</p><p>In the past, physical sales drove the bottom line.</p><p><strong>Now, community engagement, in-game purchases, and digital sales drive profits</strong>.</p><p>So, when players, their friends, and the spectator community abandon a game, publishers’ ability to drive profits from these avenues severely decreases.</p><p>Let’s put this into perspective.</p><p>In 2020, <a href="https://www.pcgamesn.com/game-industry-revenue-2020">91% of the industry’s revenue of USD $174.9 billion revenue was made through digital sales, up from 79% in 2019</a>.</p></div></div><div data-block-type="5" id="block-yui_3_17_2_1_1614014215234_72897"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5e28bb4e630fb46e596e182c/1614015691456-4TAA4BHRXXFFWTYLITPA/ke17ZwdGBToddI8pDm48kGZwFzW5ZxHacfyzKAXWyqkUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcIVJtLXR86NJwdTrkWQL9k21b3OjGcf_Cex-qh8Isyp6i5zCyJbEi69BH9b5H2vuf/image2.jpg" data-image="https://images.squarespace-cdn.com/content/v1/5e28bb4e630fb46e596e182c/1614015691456-4TAA4BHRXXFFWTYLITPA/ke17ZwdGBToddI8pDm48kGZwFzW5ZxHacfyzKAXWyqkUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcIVJtLXR86NJwdTrkWQL9k21b3OjGcf_Cex-qh8Isyp6i5zCyJbEi69BH9b5H2vuf/image2.jpg" data-image-dimensions="1193x672" data-image-focal-point="0.5,0.5" alt="image2.jpg" data-load="false" data-image-id="6033eccb2c847c14caf4423d" data-type="image" src="https://www.subspace.com/blog/image2.jpg">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1614014215234_80384"><div><p>What’s even more interesting is the rise of in-game purchases due to the popularity of free-to-play (F2P) games.</p><p>In fact, more than <a href="https://www.wepc.com/news/video-game-statistics/">85% of industry revenue comes from free-to-play games</a>.</p><p><strong>These in-game purchases include:</strong></p><ul data-rte-list="default"><li><p>enhancements (like additional lives)</p></li><li><p>currency</p></li><li><p>personalized avatars&nbsp;</p></li><li><p>ad-free experiences</p></li><li><p>unrestricted playing time</p></li></ul><p>Now, just imagine how much money is being left on the table when lag causes players and the spectator community to abandon games.</p><p><strong>Speaking of spectators…</strong></p><p>A 2016 study from Twitch claimed <a href="https://phys.org/news/2016-07-video-games-spectating-advertising.html">25% of game sales stemmed from spectators watching streams</a> of the game and making a purchase within 24 hours.</p><p>More interestingly, Twitch data scientist Danny Hernandez and his team found mid-tier Twitch streamers—those with audiences between 33 and 3,333 viewers—are responsible for 46% of game sales.</p><p>This is an incredible form of advertising for game publishers—and one that is threatened when lag causes players to abandon games and influence the spectator community.</p><p>You might be asking, <em>“ok, so what is the solution to this problem?</em>”</p><p>Well, let’s dive into that.</p><h3>How Game Publishers Are Solving Their Lag Problems Today</h3><p>Recognizing that lag and connectivity issues cannot continue to plague their games, major players in the gaming industry are now developing their own private networks or are looking at strategic partnerships to upgrade their networks and improve gameplay and player experience.</p><p>Riot Games recently accomplished this when they successfully created their own network for League of Legends players to play on.</p><p>The graph below shows improvements in the number of Riot Games players who play at under 80 ms ping since Riot created its own network.</p></div></div><div data-block-type="5" id="block-yui_3_17_2_1_1614014215234_112865"><div>








  

    
  
    <div data-test="image-block-inline-outer-wrapper">

      

      
        <figure>
          
        
        

        
          
            
          <p><img data-src="https://images.squarespace-cdn.com/content/v1/5e28bb4e630fb46e596e182c/1614016194148-WDLSILUCE1GCCPEOPCKP/ke17ZwdGBToddI8pDm48kDEg7RTdH6B5QKjZhuO9yugUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcHDTY8UMmeobLkiD70Xc3UciUnXG25JCMM8KlnxJoXcw5PwMIpS6ZXlOHg-sXRVQ8/image1.png" data-image="https://images.squarespace-cdn.com/content/v1/5e28bb4e630fb46e596e182c/1614016194148-WDLSILUCE1GCCPEOPCKP/ke17ZwdGBToddI8pDm48kDEg7RTdH6B5QKjZhuO9yugUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcHDTY8UMmeobLkiD70Xc3UciUnXG25JCMM8KlnxJoXcw5PwMIpS6ZXlOHg-sXRVQ8/image1.png" data-image-dimensions="1486x908" data-image-focal-point="0.5,0.5" alt="image1.png" data-load="false" data-image-id="6033eec17a59e16b1f48dfb2" data-type="image" src="https://www.subspace.com/blog/image1.png">
          </p>
        
          
        

        
      
        </figure>
      

    </div>
  


  


</div></div><div data-block-type="2" id="block-yui_3_17_2_1_1614014215234_115710"><div><p>Although the outcome has been good, <a href="https://technology.riotgames.com/news/fixing-internet-real-time-applications-part-ii">Riot notes that creating a private network came with many difficulties, challenges, and risks</a>.</p><p>So, it’s obvious that industry leaders recognize the importance of player and community engagement and the need to invest in better network infrastructure.</p><p>You might be asking, <em>“so what are the solutions out there for publishers to leverage today?”</em></p><h3>The Future of Real-Time Online Gaming</h3><p><a href="https://www.subspace.com/">Subspace</a> is on a mission to significantly change the landscape of the games industry by providing game publishers with a platform to operate, deploy, and scale their games.</p><p>Our groundbreaking multiplayer network infrastructure and services platform provide the lowest latency, most reliable real-time, and fully controllable network possible for the world’s biggest games.</p><p><strong>What does this mean for you as a publisher?</strong></p><ul data-rte-list="default"><li><p>Expansion in playable latency</p></li><li><p>Increased matchmaking pool sizes</p></li><li><p>Improved player engagement</p></li><li><p>Decreased player churn</p></li><li><p>Increased revenue</p></li></ul><p><strong>How do we do it?</strong></p><p>We currently have infrastructure in hundreds of cities across the globe and continue to grow our presence.</p><p>Today, Subspace has millions of players on our platform playing on PC, Playstation, Xbox, Switch, iOS, and …</p></div></div></div></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.subspace.com/blog/why-its-a-mistake-for-publishers-to-treat-player-complaints-as-white-noise">https://www.subspace.com/blog/why-its-a-mistake-for-publishers-to-treat-player-complaints-as-white-noise</a></em></p>]]>
            </description>
            <link>https://www.subspace.com/blog/why-its-a-mistake-for-publishers-to-treat-player-complaints-as-white-noise</link>
            <guid isPermaLink="false">hacker-news-small-sites-26230258</guid>
            <pubDate>Mon, 22 Feb 2021 20:38:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[MapReduce – munching through Big Data (2016)]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26230003">thread link</a>) | @wilsonfiifi
<br/>
February 22, 2021 | https://appliedgo.net/mapreduce/ | <a href="https://web.archive.org/web/*/https://appliedgo.net/mapreduce/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p><em>It’s been a while since the last post, and I have to apologize for the long wait. The last weeks have been quite busy, but I finally managed to complete another article. I hope you’ll enjoy it.</em></p>
<h2 id="map-and-reduce">Map and Reduce</h2>
<p>This is going to be a boring article about two boring functions, <code>map()</code> and <code>reduce()</code>. Here is the story:</p>
<p>You have a list with elements of type, say, <code>string</code>.</p>
<p>You define a function that takes a <code>string</code> and produces an <code>int</code>. Let’s say you want to know the length of a string.</p>
<div><pre><code data-lang="go"><span>func</span> <span>length</span>(s <span>string</span>) <span>int</span> {
	<span>return</span> <span>len</span>(s)
}
</code></pre></div><p>Now you define a function called <code>map()</code> that takes this function and applies it to each of the elements in the list and returns a list of all results.</p>
<div><pre><code data-lang="go"><span>func</span> <span>mäp</span>(list []<span>string</span>, fn <span>func</span>(<span>string</span>)<span>int</span>) []<span>int</span> { <span>// "map" is a reserved word, "mäp" isn't
</span><span></span>	res <span>:=</span> <span>make</span>([]<span>int</span>, <span>len</span>(list))
	<span>for</span> i, elem <span>:=</span> <span>range</span> list {
		res[i] = <span>fn</span>(elem)
	}
	<span>return</span> res
}
</code></pre></div><p>Finally, you define another function <code>reduce()</code> that takes the result list and boils it down to a single result..</p>
<div><pre><code data-lang="go"><span>func</span> <span>reduce</span>(list []<span>int</span>, fn <span>func</span>(<span>int</span>, <span>int</span>)<span>int</span>) (res <span>int</span>) {
	<span>for</span> _, elem <span>:=</span> <span>range</span> list {
		res = <span>fn</span>(res, elem)
	}
	<span>return</span> res
}

<span>func</span> <span>sum</span>(a,b <span>int</span>) <span>int</span> {
	<span>return</span> a<span>+</span>b
}
</code></pre></div><p>Now you can wire it all up.</p>
<div><pre><code data-lang="go"><span>func</span> <span>main</span>() {
	list <span>:=</span> []<span>string</span>{<span>"a"</span>, <span>"bcd"</span>, <span>"ef"</span>, <span>"g"</span>, <span>"hij"</span>}
	res <span>:=</span> <span>reduce</span>(<span>mäp</span>(list, len), sum)
	fmt.<span>Println</span>(res)
}
</code></pre></div><p><a href="https://play.golang.org/p/P7-1ro4a_d">(Playground link)</a></p>
<p>Here is the whole thing visualized. (Click on Play to start the animation.)</p>


<p>That’s it. End of the story. Pretty boring, eh?</p>
<h2 id="but-wait-">But wait! …</h2>
<p>… what’s this?</p>
<p><strong>Looks like we just abstracted away the concept of <code>for</code> loops!</strong></p>
<p>Now if that’s not something to brag about on the next Gopher meetup…</p>
<p>However, does this buy us anything else? Indeed it does:</p>
<ul>
<li>
<p>First, no more one-off index errors.</p>
</li>
<li>
<p>Second, and more importantly, if the mapped function <code>fn</code> does not depend on previous results, it can be trivially called in a concurrent manner.</p>
</li>
</ul>
<p>How to do this? Easy: Split the list into <em>n</em> pieces and pass them to <em>n</em> independently running mappers. Next, have the mappers run on separate CPU cores, or even on separate CPU’s.</p>
<p>Imagine the speed boost you’ll get. Map and reduce, as it seems, form a fundamental concept for efficient distributed loops.</p>
<blockquote>
<p>Lemme repeat that. By abstracting away the very concept of looping, you can implement looping any way you want, including implementing it in a way that scales nicely with extra hardware.</p>
<p>Joel Spolsky, <a href="http://www.joelonsoftware.com/items/2006/08/01.html">Can Your Programming Language Do This?</a> (2006)</p>
</blockquote>
<h2 id="from-map-and-reduce-to-mapreduce">From map() and reduce() to MapReduce</h2>
<p>Google researchers took the map/reduce concept and scaled it up to search engine level (I leave the exact definition of “search engine level” as an exercise for the reader). <a href="http://research.google.com/archive/mapreduce.html">MapReduce was born</a>.</p>
<p>The result was a highly scalable, fault-tolerant data processing framework with the two functions <code>map()</code> and <code>reduce()</code> at its core.</p>
<p>Here is how it works.</p>
<p>Let’s say we have a couple of text files and we want to calculate the average count of nouns &amp; verbs per file.</p>
<p>Our imaginary test machine has eight CPU cores. So we can set up eight processing entities/work units/actors (or whatever you want to call them):</p>
<ul>
<li>One input reader</li>
<li>Three mappers</li>
<li>One shuffler, or partitioner</li>
<li>Two reducers</li>
<li>One output writer</li>
</ul>
<h3 id="the-input-reader">The input reader</h3>
<p>The input reader fetches the documents, turns each one into a list of words, and distributes the lists among the mappers.</p>
<h3 id="the-mapper">The mapper</h3>
<p>Each of the mappers reads the input list word by word and counts the nouns and verbs in that list.</p>
<p>The result is a key-value list of word types (noun, verb) and counts. For example, our three mappers could return these counts:</p>
<pre><code>mapper 1:
    nouns: 7
    verbs: 4

mapper 2:
    nouns: 5
    verbs: 8

mapper 3:
    nouns: 6
    verbs: 3
</code></pre>
<p>When a mapper has finished, it passes the result on to the shuffler.</p>
<h3 id="the-shuffler">The shuffler</h3>
<p>The shuffler receives the output lists from the mappers. It rearranges the data by key; that’s why it is also referred to as “partitioning function”. In our example, the shuffler generates two lists, one for nouns and one for verbs:</p>
<pre><code>list 1:
    nouns: 7
    nouns: 5
    nouns: 6

list 2:
    verbs: 4
    verbs: 8
    verbs: 3
</code></pre>
<p>The shuffler then passes each list to one of the two reducers.</p>
<h3 id="the-reducer">The reducer</h3>
<p>Each reducer receives a list with a couple of counts. It simply runs through the list, adds up all the counts, and divides the result by the number of counts. Both reducers then send their output to the output writer.</p>
<p>Back to our example. The first reducer would calculate an average of</p>
<pre><code>(7 + 5 + 6) / 3 = 6
</code></pre>
<p>and the other one would return</p>
<pre><code>(4 + 8 + 3) / 3 = 5
</code></pre>
<h3 id="the-output-writer">The output writer</h3>
<p>All the output writer has to do is collecting the results from the reducers and write them to disk or pass them on to some consumer process.</p>
<h3 id="summary">Summary</h3>
<p>To make all this less abstract, here is the same as an animation. (Click on Play.)</p>


<p>This concept easily scales beyond a single multi-CPU machine. The involved entities - input reader, mapper, shuffler, reducer, and output writer - can even run on different machines if required.</p>
<p>But MapReduce is more than just some distributed version of <code>map()</code> and <code>reduce()</code>. There are a couple of additional bonuses that we get from a decent MapReduce implementation.</p>
<ul>
<li>A good deal of the functionality is the same for any kind of map/reduce task. These parts can be implemented as a MapReduce framework where the user just needs to provide the <code>map</code> and <code>reduce</code> functions.</li>
<li>The MapReduce framework can provide fault recovery. If a node fails, the framework can re-execute the affected tasks on another node.</li>
<li>With fault tolerance mechanisms in place, MapReduce can run on large clusters of commodity hardware.</li>
</ul>
<h2 id="the-code">The code</h2>
<p>The code below is a very simple version of the noun/verb average calculation. To keep the code short and clear, the mapper does not actually identify nouns and verbs. Instead, the input text is just a list of strings that read either “noun” or “verb”. Also, the reducer does not receive key/value pairs but rather just the values. We already know that one reducer receives the nouns and the other receives the verbs.</p>
</div></div>]]>
            </description>
            <link>https://appliedgo.net/mapreduce/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26230003</guid>
            <pubDate>Mon, 22 Feb 2021 20:18:48 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What We Learned About Engineering Effectiveness Talking to Hundreds of CTOs]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26229974">thread link</a>) | @tonioab
<br/>
February 22, 2021 | https://www.okayhq.com/blog/the-5-stages-of-engineering-effectiveness | <a href="https://web.archive.org/web/*/https://www.okayhq.com/blog/the-5-stages-of-engineering-effectiveness">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>Over the last decade, <a href="https://www.linkedin.com/in/tomasrb/" rel="nofollow noopener noreferrer" target="_blank">my cofounder</a> and <a href="https://www.linkedin.com/in/antoineboulanger/" rel="nofollow noopener noreferrer" target="_blank">I</a> have engaged in hundreds of conversations with engineering leaders on our journey to improve engineering. We talked with Silicon Valley startups just getting started as well as established enterprises with thousand-person headcounts.</p>
<p>What we discovered surprised us: <strong>Not only can software engineering effectiveness be measured; most engineering teams follow the exact same evolution.</strong></p>
<p>From a two-person startup to a thousand-person team, engineering success always comes from the same fundamentals: 1) Hire excellent engineers and 2) <a href="https://www.okayhq.com/blog/engineering-productivity-can-be-measured" rel="nofollow noopener noreferrer" target="_blank">Remove the blockers that prevent their success</a>.</p>
<p>Different team sizes, however, change the ways these fundamentals manifest. Broadly, they fit into five stages:</p>
<ul>
<li>Qualitative</li>
<li>Data-Curious</li>
<li>People-Driven Engineering Effectiveness</li>
<li>Software-Driven Effectiveness</li>
<li>Engineering Effectiveness as a Strategic Advantage</li>
</ul>
<p><img src="https://www.okayhq.com/assets/img/5_stages_o.a91f2d0.png" alt="The 5 stages of Engineering Effectiveness"></p><p>While you're perusing these stages, keep in mind that every engineering org starts somewhere and every org can reach stage 5. Regardless of the size or age of your team, your engineering effectiveness would benefit from:</p>
<ol>
<li>Objectively assessing your current engineering org: What stage are you in and why?Â&nbsp;</li>
<li>Solidifying a strong foundation: Ensure you've built all the components of your current and lower stages.Â&nbsp;</li>
<li>Aiming higher: Add advanced functions to accelerate your achievement.Â&nbsp;Â&nbsp;</li>
</ol>
<p>Of course, individual organizations have their own unique traits, so you should feel free to make this framework your own. On the whole, however, here's how to grow:Â&nbsp;</p>
<h2 id="stage-1-qualitative">Stage 1: Qualitative</h2>
<p>Most engineering teams start small. At this point (usually around 1-20 people), the team is evolving rapidly and there aren't many objective metrics to measure success.</p>
<p><strong>In stage 1, the most successful engineering organizations build a strong social, cultural, and behavioral foundation.</strong> This foundation should include implementing widely-known best practices like:</p>
<ul>
<li><a href="https://www.scrum.org/resources/what-is-a-sprint-retrospective" rel="nofollow noopener noreferrer" target="_blank">Sprint retrospectives</a> to identify patterns of effectiveness</li>
<li><a href="https://en.wikipedia.org/wiki/Test-driven_development" rel="nofollow noopener noreferrer" target="_blank">Test-driven development</a> and version control with small, frequent commits</li>
<li>Flexible hours with high employee autonomy</li>
</ul>
<p>With stage 1 being almost metric-free, engineering success relies almost entirely on your first-line managers.  We've found these managers to be the differentiators between successful stage 1 orgs and those that flounder. If your org is in stage 1, be on the lookout for strong managers: ones who are either experienced and well-trained or inexperienced but exceptionally fast learners.</p>
<p>To support a stage 1 engineering org, company leadership should create a high-trust environment in team meetings/all hands (to encourage feedback) and keep a pulse on morale through high-quality one-on-ones. It's also helpful for long-term culture to invest in manager growth and development, particularly around people skills and EQ. (We've found <a href="https://rework.withgoogle.com/guides/managers-develop-and-support-managers/steps/introduction/" rel="nofollow noopener noreferrer" target="_blank">Google's framework</a> to be particularly effective at helping managers become <a href="https://en.wikipedia.org/wiki/Servant_leadership" rel="nofollow noopener noreferrer" target="_blank">servant leaders</a>.)</p>
<p>In stage 1, engineering revolves around culture: you'll succeed if your managers can support and align your team.Â&nbsp;Â&nbsp;</p>
<h2 id="stage-2-data-curious-and-reactive">Stage 2: Data-Curious and Reactive</h2>
<p><strong>Stage 2 is when most orgs first become aware of engineering effectiveness as an area to improve, so they start dabbling in data.</strong></p>
<p>As most stage 2 orgs are small and fast-growing (20-50 people), they must still rely heavily on first-line managers for qualitative measures like:Â&nbsp;</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/OKR" rel="nofollow noopener noreferrer" target="_blank">OKR</a> completion</li>
<li>Team engagement</li>
<li>Hiring targets</li>
</ul>
<p>Stage 2 will also bring your first quantitative measures, typically in the form of ad hoc metrics like:Â&nbsp;</p>
<ul>
<li>Bug counts</li>
<li>Alert rates</li>
<li>Employee satisfaction surveys</li>
</ul>
<p>On the dev ops side, successful stage 2 orgs typically begin investing in a pipeline aimed at achieving continuous delivery. These investments often include custom scripts or open-source tools that provide snapshots of their DORA metrics.</p>
<p>These instantaneous measurements and metrics help make stage 2 engineering orgs more effective than those in stage 1. That said, these stage 2 improvements are typically ad hoc, one-off, and isolated. Instead of a comprehensive, real-time dashboard or even a long-term feedback loop, stage 2 orgs act on instantaneous information as it arises, necessarily making the org's improvements reactive.</p>
<h2 id="stage-3-people-driven-engineering-effectiveness">Stage 3: People-Driven Engineering Effectiveness Â&nbsp;</h2>
<p>The typical stage 3 engineering org will have 50-250 engineers and feel like it's in a transition stage between independent qualitative assessments and fully-automated metrics.Â&nbsp;</p>
<p><strong>In stage 3, the best engineering teams establish a dedicated function for engineering effectiveness</strong>, most frequently through a dedicated internal team or by hiring an engineering chief of staff. This dedicated function will begin to make:</p>
<ul>
<li>A regular cadence of investments in dev tech, processes, and tooling</li>
<li>Looker dashboards to capture DORA metrics in real time</li>
<li><a href="https://cloud.google.com/blog/products/gcp/sre-fundamentals-slis-slas-and-slos" rel="nofollow noopener noreferrer" target="_blank">SLAs/SLOs</a> for engineering effectiveness</li>
<li>Tangible, objectively-verifiable effectiveness metrics that will hold managers accountableÂ&nbsp;</li>
</ul>
<p><strong>While stage 3 brings both long-term metrics and systems that provide a holistic view, most of these activities will be performed by hand.</strong> For example, the engineering Chief of Staff or program manager might ask directors to fill out a spreadsheet, which will then evolve into a powerpoint presentation that prompts the VP to make adjustments.</p>
<p><strong>A successful stage 3 engineering org will implement a dedicated engineering effectiveness team to measure metrics and incorporate adjustments at a reliable cadence.</strong></p>
<h2 id="stage-4-software-driven-effectiveness">Stage 4: Software-Driven Effectiveness</h2>
<p><strong>In stage 4, dedicated software enters the picture, bringing continuous improvement to every engineering stage.</strong></p>
<p>The typical stage 4 engineering org will have hundreds or thousands of engineers. At this scale, treating engineering like a black box is no longer acceptable. Instead, managers will require precise, quantitative assessments instead of qualitative or imprecise measures.</p>
<blockquote>
<p>The most successful stage 4 teams automate a high variety of engineering metrics, either through dashboards built by a full-time engineering effectiveness team or by leveraging <a href="https://www.okayhq.com/" rel="nofollow noopener noreferrer" target="_blank">third-party software</a></p>
</blockquote>
<p>These dashboards gather actionable, real-time effectiveness metrics at every level of management:</p>
<ol>
<li>The CTO sets high-level metrics and goals, typically through a dashboard they share with the other executives.Â&nbsp;</li>
<li>Directors/mid-level managers set goals for their sub-orgs and monitor their metrics for early signs of issues.</li>
<li>First-line managers provide root-cause analysis on the specific factors contributing to high-level metrics.</li>
<li>Every engineering OKR includes goals around effectiveness and improvement.</li>
</ol>
<p>Before stage 4, engineering teams often aim to measure everything. In stage 4, they aim more precisely at high-value metrics like engineer utilization, <a href="https://www.okayhq.com/blog/engineering-productivity-can-be-measured" rel="nofollow noopener noreferrer" target="_blank">blockers</a>, and work-life balance. Stage 4 brings the ability for an org to diagnose specific symptoms all the way down to their root causes, where they can form coherent, data-backed stories that inform pinpointed improvements. The most successful stage 4 teams will even start uncovering personalized metrics that they find particularly correlate to their success.</p>
<p><strong>By combining automated, real-time metrics with the culture of continuous improvement built in stages 1-3, stage 4 teams can evolve their effectiveness into an always-running, well-oiled improvement machine.</strong></p>
<h2 id="stage-5-engineering-effectiveness-as-a-strategic-advantage">Stage 5: Engineering Effectiveness as a Strategic Advantage</h2>
<p><strong>Stage 5 turns engineering effectiveness into a strategic lever that helps the company achieve precise business goals.</strong></p>
<p>Even though a typical stage 5 team will contain hundreds or thousands of engineers all around the world, the best stage 5 orgs run like a well-coordinated symphony: individual contributions come together to create a single unit that's much more than the sum of its parts.</p>
<p>On top of stage 4's software-based measurement and org-wide culture of improvement, stage 5 adds a strategic lens. The best stage 5 organizations can make calculated risks involving conscious trade-offs. Perhaps the org extrapolates a concerning quality trend and adjusts its features long before engineers or customers start to complain. The best stage 5 organizations can calculate specific risk levels and readjust without unpleasant surprises.</p>
<p>High-performing stage 5 teams typically engage in:</p>
<ul>
<li>Industry/peer-group <a href="https://dealstruck.com/resources/the-power-of-peer-benchmarking/" rel="nofollow noopener noreferrer" target="_blank">benchmarking</a> (to understand their effectiveness compared to other companies)</li>
<li>Automatic implementation of the latest effectiveness research (to accelerate constant improvement)</li>
<li>Anticipatory activities at every stage in the org (to predict potential problems before they arise)Â&nbsp;</li>
<li>Thought leadership on new best practices of engineering effectiveness (naturally uncovered as a result of their experience)Â&nbsp;</li>
<li>Full transparency/understanding of engineering metrics, even outside of the engineering org (to aid company-wide improvement)</li>
<li>Calculated risks (to achieve precise business aims)</li>
</ul>
<p>From the outside, a stage 5 team looks like a strong engineering brand. It can accelerate and adjust, attract top talent while achieving business aims.</p>
<h2 id="toward-engineering-effectiveness">Toward Engineering EffectivenessÂ&nbsp;</h2>
<p>Engineering effectiveness is too expensive to be left to chance. Start by assessing where your team currently is. Are you:Â&nbsp;</p>
<ul>
<li>Sufficiently small that metrics are still a "nice to have"?</li>
<li>Data-curious and ready to react?</li>
<li>In need of a dedicated effectiveness team?</li>
<li>Positioned to produce a continuously improving organizationÂ&nbsp;?</li>
<li>Able to precisely tune your priorities to enable the company's long-term strategy?</li>
</ul>
<p><strong>Then, solidify your position at your current stage, building a solid foundation on which you can expand.</strong></p>
<p>While every engineering team has its own individual nuance, most will follow this consistent evolution. We uncovered these stages through years of observation, but there's still more improvement to be applied to engineering.</p>
<p>We've made it our mission to improve engineering effectiveness. If you've uncovered your own trends or if you're curious for more, <a href="https://okayhq.typeform.com/to/O47Fx3Q7" rel="nofollow noopener noreferrer" target="_blank">let us know</a>. We'd love to improve engineering effectiveness for everyone.</p></div></div></div>]]>
            </description>
            <link>https://www.okayhq.com/blog/the-5-stages-of-engineering-effectiveness</link>
            <guid isPermaLink="false">hacker-news-small-sites-26229974</guid>
            <pubDate>Mon, 22 Feb 2021 20:16:27 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Anti-Solar Panels May Generate Power at Night Soon]]>
            </title>
            <description>
<![CDATA[
Score 14 | Comments 9 (<a href="https://news.ycombinator.com/item?id=26229635">thread link</a>) | @elorant
<br/>
February 22, 2021 | https://robologiclab.com/anti-solar-panels-may-generate-power-at-night-soon/ | <a href="https://web.archive.org/web/*/https://robologiclab.com/anti-solar-panels-may-generate-power-at-night-soon/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="content"><article id="post-2010"><p><a href="https://robologiclab.com/wp-content/uploads/2021/02/Untitled-design-14.png"><img width="800" height="445" src="https://i0.wp.com/robologiclab.com/wp-content/uploads/2021/02/Untitled-design-14.png?resize=800%2C445&amp;ssl=1" alt="Anti-solar panels that can work round the clock!" loading="lazy"></a></p><div><div><p>Have anyone told you that a solar panel can be operational at the night? This might sound like an unrealistic tech. However, it is possible and in the future, we can see solar panels working at night also. The University of California (UC), Davis scientists are inventing a prototype for an ‘anti-solar panels’ that would work opposite to a classic solar panel. The new studies suggest that it is possible that such panels could work round the clock.</p><p><br>These anti-solar panels can produce a quarter of the energy they generate throughout the day under ideal conditions. The scientist reveals the requirement to combine thermoradiative panels that could produce energy on account of radiative cooling. In radiative cooling due to thermal radiation, a body dissipates out its heat. The thermoradiative cells are used for the experiment for manufacturing. After that, they transfigure the heat into electricity.</p><figure><img loading="lazy" width="800" height="391" src="https://i1.wp.com/robologiclab.com/wp-content/uploads/2021/02/Untitled-design-15.png?resize=800%2C391&amp;ssl=1" alt="anti-solar panels installed on building" srcset="https://i1.wp.com/robologiclab.com/wp-content/uploads/2021/02/Untitled-design-15.png?w=1024&amp;ssl=1 1024w, https://i1.wp.com/robologiclab.com/wp-content/uploads/2021/02/Untitled-design-15.png?resize=300%2C146&amp;ssl=1 300w, https://i1.wp.com/robologiclab.com/wp-content/uploads/2021/02/Untitled-design-15.png?resize=768%2C375&amp;ssl=1 768w" sizes="(max-width: 800px) 100vw, 800px" data-recalc-dims="1"></figure><h3>Research Behind Anti-Solar Panels</h3><p>ACS Photonics publication has published a research paper. In this paper, the scientists have revealed how they developed the anti-solar cells. Which perform their function of radiative cooling. Some engineers from UC states were puzzled concerning what would be the result if they installed one of the solar panels in a warm area, and pointed it towards the sky. It tends to concentrate on visible light to give rise to efficacious cells that could use the night sky and space as a heat sink. Jeremy Munday, an electrical and computer engineer from UC states mention that physics was identical in both the tech, only the materials are varying.</p><p><br>However this technology is in its initial phases, the team is in the middle of developing prototypes. The important point about this research is, it can be made economical to hold solar panels functional for a day. Last but not the least, according to scientists the enigmatic space is an interesting, comparatively unexplored area. However, it can assist and deliver electrical power at night and day with the proper utilization of materials science, optics, and photonics.</p><p><br>Hope you all like it, please share your valuable views about this tech in the comment section. Thank you for reading this!</p></div></div></article></div></div>]]>
            </description>
            <link>https://robologiclab.com/anti-solar-panels-may-generate-power-at-night-soon/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26229635</guid>
            <pubDate>Mon, 22 Feb 2021 19:51:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to query Redis via SQL using zeeSQL, a Redis module]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26229236">thread link</a>) | @siscia
<br/>
February 22, 2021 | https://doc.zeesql.com/secondary-indexes | <a href="https://web.archive.org/web/*/https://doc.zeesql.com/secondary-indexes">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><div><div data-editioncontainer="true"><div data-slate-editor="true" data-key="99d575d5d9494774b58e94c134875bfe" autocorrect="on" spellcheck="true" data-gramm="false"><p data-key="d132604706b74d1ea3b25f8c061f4698"><span><span data-key="92a3e1259d634aed98244d185476416e"><span data-offset-key="92a3e1259d634aed98244d185476416e:0">Redis, at his heart, is a key-value store. It makes it simple, easy, and fast to search for values given their keys.</span></span></span></p><p data-key="158bbb9376024a42b42f72277517cdc2"><span><span data-key="1a340560a7a444c7829f7870dff5fbe4"><span data-offset-key="1a340560a7a444c7829f7870dff5fbe4:0">Often time, however, is necessary to look for keys that values respect some properties.</span></span></span></p><p data-key="c4eddb73b340409384e2e8e1e515c01d"><span><span data-key="7163d6925bcb47d0ac9767e557d84301"><span data-offset-key="7163d6925bcb47d0ac9767e557d84301:0">For instance, find all the keys for which their value is greater than 5. Or the keys that have a value set to a specific string like "admin".</span></span></span></p><p data-key="a57a4b331849426995249c2e86882646"><span><span data-key="29e8065c6aed4fc8a98681a8b8adc4d1"><span data-offset-key="29e8065c6aed4fc8a98681a8b8adc4d1:0">The standard solution for this problem in Redis is to keep a set of secondary indexes. To keep everything in sync, those secondary indexes need to be updated along with the primary keys. Keeping the indexes in sync is an activity that the developer who uses Redis need to take care of, and it brings a sizable increment in complexity.</span></span></span></p><p data-key="a15e489be47f4f2ab6c166a0cc2553c6"><span><span data-key="a04138aab73d4c729e004fa1428b6123"><span data-offset-key="a04138aab73d4c729e004fa1428b6123:0">zeeSQL aims to solve this problem.</span></span></span></p><p data-key="e69cead577704775abcf60ef86692363"><span><span data-key="7e48e1d5dbd246daa508bf67d7e16123"><span data-offset-key="7e48e1d5dbd246daa508bf67d7e16123:0">Redis provides different data structures, one of them is Redis Hashes.</span></span></span></p><p data-key="28fd527b5ccd4acfb9c28a2238f877dd"><span><span data-key="e66ee6d8f91a4870a014a02a10271292"><span data-offset-key="e66ee6d8f91a4870a014a02a10271292:0">Redis Hashes map between string fields and string values. From the Redis documentation:</span></span></span></p><div><pre data-key="e727dbc8c0c74893a7cbef256ec0fe19" spellcheck="false"><p><span data-key="9fa2a5987271478d848a7f2ff0a8c130"><span data-offset-key="9fa2a5987271478d848a7f2ff0a8c130:0">HMSET user:1000 username antirez password P1pp0 age 34</span></span></p><p><span data-key="185065f64f634d63aa80bbf0817da560"><span data-offset-key="185065f64f634d63aa80bbf0817da560:0">HGETALL user:1000</span></span></p><p><span data-key="8b5975dc3f0a4d0ea01d88396524dce8"><span data-offset-key="8b5975dc3f0a4d0ea01d88396524dce8:0">HSET user:1000 password 12345</span></span></p><p><span data-key="790b5c40bc4f4097bf1a166dd5fb445a"><span data-offset-key="790b5c40bc4f4097bf1a166dd5fb445a:0">HGETALL user:1000</span></span></p></pre></div><p data-key="12460737540a4efbbb1828c8717d0b19"><span><span data-key="22f81201ba524c00aecb5369874d052d"><span data-offset-key="22f81201ba524c00aecb5369874d052d:0">In this case to the Redis key </span><span data-offset-key="22f81201ba524c00aecb5369874d052d:1"><code spellcheck="false" data-slate-leaf="true">user:1000</code></span><span data-offset-key="22f81201ba524c00aecb5369874d052d:2"> we associate a Redis Hash.</span></span></span></p><p data-key="87d4ded7c0de4661bb2d87df394e98a7"><span><span data-key="fcf98200d882403a815332a34784b4a4"><span data-offset-key="fcf98200d882403a815332a34784b4a4:0">The Redis Hash has 3 fields, </span><span data-offset-key="fcf98200d882403a815332a34784b4a4:1"><code spellcheck="false" data-slate-leaf="true">username</code></span><span data-offset-key="fcf98200d882403a815332a34784b4a4:2"> with value </span><span data-offset-key="fcf98200d882403a815332a34784b4a4:3"><code spellcheck="false" data-slate-leaf="true">antirez</code></span><span data-offset-key="fcf98200d882403a815332a34784b4a4:4">, </span><span data-offset-key="fcf98200d882403a815332a34784b4a4:5"><code spellcheck="false" data-slate-leaf="true">password</code></span><span data-offset-key="fcf98200d882403a815332a34784b4a4:6"> with value </span><span data-offset-key="fcf98200d882403a815332a34784b4a4:7"><code spellcheck="false" data-slate-leaf="true">P1pp0</code></span><span data-offset-key="fcf98200d882403a815332a34784b4a4:8">, and </span><span data-offset-key="fcf98200d882403a815332a34784b4a4:9"><code spellcheck="false" data-slate-leaf="true">age</code></span><span data-offset-key="fcf98200d882403a815332a34784b4a4:10"> with value 34.</span></span></span></p><p data-key="5475b74c117e4caeba11958805f72e5a"><span><span data-key="ee0c87558d7f4f2db88359d6b285b250"><span data-offset-key="ee0c87558d7f4f2db88359d6b285b250:0">Redis Hashes are the only data structure that can be indexed using zeeSQL.</span></span></span></p><p data-key="64a9a9c28d85441caef8e8fc89f23207"><span><span data-key="bd45e40eb9434dfa9319f03381a9838c"><span data-offset-key="bd45e40eb9434dfa9319f03381a9838c:0">At the moment, zeeSQL we can index only Redis Hashes.</span></span></span></p><p data-key="ca16762639a74544b7ba06e013913ec4"><span><span data-key="937db6eec8834eefb9aa6d0edd6a95eb"><span data-offset-key="937db6eec8834eefb9aa6d0edd6a95eb:0">Secondary indexes in zeeSQL are standard SQL tables.</span></span></span></p><p data-key="9be38e637b6544419899eed4413a33ab"><span><span data-key="893752348fee4d3589ca476cbed3a3b7"><span data-offset-key="893752348fee4d3589ca476cbed3a3b7:0">The user needs to provide the schema of the table, and zeeSQL will automatically keep the table in sync with the Redis Hashes.</span></span></span></p><p data-key="dacfbdbbba6544af9ae937ac8de12716"><span><span data-key="aed8a12590ad45d9b96a41fd631efba7"><span data-offset-key="aed8a12590ad45d9b96a41fd631efba7:0">The user can also provide a prefix so that only some Redis Hashes, the ones that match the prefix, are stored in the secondary index.</span></span></span></p><p data-key="359df7cfa882466fa5b0bb18937799b2"><span><span data-key="76f79e9f7a27404791887f294e83371d"><span data-offset-key="76f79e9f7a27404791887f294e83371d:0">A secondary table can be created using the </span></span><a data-key="0212751be50b49b7996149199a8cd356" href="https://doc.zeesql.com/references#zeesql-index-new"><span data-key="9d51723b405a47429ecf3138dc43806d"><span data-offset-key="9d51723b405a47429ecf3138dc43806d:0"><code spellcheck="false" data-slate-leaf="true">ZEESQL.INDEX</code></span><span data-offset-key="9d51723b405a47429ecf3138dc43806d:1"> command</span></span></a><span data-key="4c87c85c88ad47ed9caf7f4a0580b92a"><span data-offset-key="4c87c85c88ad47ed9caf7f4a0580b92a:0">.</span></span></span></p><div><pre data-key="f99dc9840cc94f1e9414c3ac898164dc" spellcheck="false"><p><span data-key="754c14eba8d84b92acd99bbec5112ec5"><span data-offset-key="754c14eba8d84b92acd99bbec5112ec5:0">&gt; ZEESQL.INDEX DB NEW TABLE $table_name [PREFIX prefix] SCHEMA column_name column_type [column_name column_type]</span></span></p></pre></div><p data-key="314d2dcb0c4b444aa2c79052e01f2030"><span><span data-key="132c88c57f7d4843b43250f5b8387619"><span data-offset-key="132c88c57f7d4843b43250f5b8387619:0">This command will perform two actions.</span></span></span></p><p data-key="fda10d1526c04c6faba3b9f54ec29ac9"><span><span data-key="61f44b7f5b7d40ee8b8312d39a669e2b"><span data-offset-key="61f44b7f5b7d40ee8b8312d39a669e2b:0">At first, it will try to create a table called </span><span data-offset-key="61f44b7f5b7d40ee8b8312d39a669e2b:1"><code spellcheck="false" data-slate-leaf="true">$table_name</code></span><span data-offset-key="61f44b7f5b7d40ee8b8312d39a669e2b:2">. If the table already exists, this step is skipped. If the table does not exists, the new table is created with the columns indicated after the </span><span data-offset-key="61f44b7f5b7d40ee8b8312d39a669e2b:3"><code spellcheck="false" data-slate-leaf="true">SCHEMA</code></span><span data-offset-key="61f44b7f5b7d40ee8b8312d39a669e2b:4"> keyword.</span></span></span></p><p data-key="47af357c11ff44188c01cffc1152f546"><span><span data-key="4eaf5aaf7425410c82475ff6efcc40d3"><span data-offset-key="4eaf5aaf7425410c82475ff6efcc40d3:0">After the table is created, we register a callback.</span></span></span></p><p data-key="8bf415d3defa48538d5fcd8451de2994"><span><span data-key="b558fe36d9344c5996dc05e1fd157872"><span data-offset-key="b558fe36d9344c5996dc05e1fd157872:0">The callback, listen to all the events that happen to the keys that start with the prefix, if the prefix is omitted, the callback listen to events for all the keys.</span></span></span></p><p data-key="17aa06b905594bc6ac626b4c9b0d7b9c"><span><span data-key="b4d6c8d83a724cdda14e8daa4bcfd201"><span data-offset-key="b4d6c8d83a724cdda14e8daa4bcfd201:0">The callback is invoked passing as arguments the type of event and against which key the event was fired. From there, the callback has all the information it needs to keep the secondary index table in sync.</span></span></span></p><p data-key="e0f9b1dcb0ae40079e77f1011ade6168"><span><span data-key="d5f0cf23fe7c445fb51e4a9637559446"><span data-offset-key="d5f0cf23fe7c445fb51e4a9637559446:0">Every time that one key, matching the prefix is modified, zeeSQL updates the secondary index table.</span></span></span></p><p data-key="6eb4a89280474b87a248eb3c486d12f8"><span><span data-key="2c2f2fd3411f48ec8a9b07185723e9f0"><span data-offset-key="2c2f2fd3411f48ec8a9b07185723e9f0:0">The structure of the secondary index table is very simple.</span></span></span></p><p data-key="f6bd614c42f54565a3095522ef900ae6"><span><span data-key="081fe19251e54f08a5857845459b4997"><span data-offset-key="081fe19251e54f08a5857845459b4997:0">There is a primary key, which is always a string, which value is the Redis key itself. In the Redis Hash above, the primary key will be the value </span><span data-offset-key="081fe19251e54f08a5857845459b4997:1"><code spellcheck="false" data-slate-leaf="true">user:1000</code></span></span></span></p><p data-key="e73c5a4a9a3b4373aa2a83de07129368"><span><span data-key="c321103290b54dc8bf7e84f657dd68c6"><span data-offset-key="c321103290b54dc8bf7e84f657dd68c6:0">Next to the primary key, there are all the columns defined in the schema, with their respective types.</span></span></span></p><p data-key="aec474afa8bf45c9a585310fc333d86b"><span><span data-key="2c42f65323aa4c03b2a00674f460db25"><span data-offset-key="2c42f65323aa4c03b2a00674f460db25:0">The secondary index table is a standard SQLite table.</span></span></span></p><p data-key="cbcd8f9cb4b04c28a8388d6a34d0b729"><span><span data-key="f83673b66a1a431fa4f78523081749c3"><span data-offset-key="f83673b66a1a431fa4f78523081749c3:0">There is nothing special about it, besides being managed by zeeSQL itself and not by the user.</span></span></span></p><p data-key="6deb9fbe21714f6b983d9cf7d55b5d28"><span><span data-key="573157057fda4cd29c14af4972c6f312"><span data-offset-key="573157057fda4cd29c14af4972c6f312:0">You can, of course, query it, in whichever way you find more appropriate.</span></span></span></p><p data-key="2b6e3a0c57534feb9ed59be9a4149c97"><span><span data-key="86f60c3445af415e81b0e3a5d1680c18"><span data-offset-key="86f60c3445af415e81b0e3a5d1680c18:0">However, you could also modify it, even though it is strongly discouraged.</span></span></span></p><p data-key="4e616b1ed7c44925beabe8a5053224c0"><span><span data-key="bbfde2153f924a5f895f50fd09dbef2d"><span data-offset-key="bbfde2153f924a5f895f50fd09dbef2d:0">Being a standard SQLite table, it is possible to define indexes also on your secondary index table. This will allow even faster lookups.</span></span></span></p><p data-key="a7db498bed9c4cc19e162e17e8e940ec"><span><span data-key="28178cba5bab4f0d873a60ac013a3a19"><span data-offset-key="28178cba5bab4f0d873a60ac013a3a19:0">Moreover, it is also possible to define triggers.</span></span></span></p><p data-key="6cea82bff103467d96c58f8025529f67"><span><span data-key="fbba6c7b0bc941ccbab7ae3449b44384"><span data-offset-key="fbba6c7b0bc941ccbab7ae3449b44384:0">The commands to modify the secondary index tables, are fire and forget.</span></span></span></p><p data-key="ef2cba47e1fb405c955be609b13b8fa8"><span><span data-key="d46998e762aa435ea525a82253ead437"><span data-offset-key="d46998e762aa435ea525a82253ead437:0">The works seamlessly on a standard table without constraints. However, if you start to add constraints and triggers to the secondary index table, it will be your responsibility to keep the database in a consistent state.</span></span></span></p><p data-key="878d3cc55fdc43aeba702d441122e88f"><span><span data-key="5138c25fd150432cab41ca29ce338b53"><span data-offset-key="5138c25fd150432cab41ca29ce338b53:0">Unfortunately, zeeSQL cannot provide any feedback, if an update or insertion failed.</span></span></span></p><p data-key="dac5b23d29f14911b8d4c97e579a50e6"><span><span data-key="efcf40430f59451396cb1e20bb1c4e19"><span data-offset-key="efcf40430f59451396cb1e20bb1c4e19:0">The use cases when this could be a problem, are extremely advanced.</span></span></span></p><p data-key="12cd5f1060994b80844dc086002eb722"><span><span data-key="1560b692dd1945b9bdc9bc5b441ba392"><span data-offset-key="1560b692dd1945b9bdc9bc5b441ba392:0">In our Redis we can store users for a simple online game. Of those users we store a simple ID, the name, and the score.</span></span></span></p><p data-key="263425a855b04899bedd9cde6bcdfd0d"><span><span data-key="fa80727a413943e69c2124d3fdf0bebd"><span data-offset-key="fa80727a413943e69c2124d3fdf0bebd:0">We want to search for all the user who scores is greater than 5.</span></span></span></p><p data-key="a731f4d040774523873fd08c9988c9fa"><span><span data-key="e7ed305df25843c0925f4da6e39ca829"><span data-offset-key="e7ed305df25843c0925f4da6e39ca829:0">We start by creating a zeeSQL database.</span></span></span></p><div><pre data-key="cb07dd712eba4ecc8aea14dbe5368e80" spellcheck="false"><p><span data-key="1df92de0255149988aa7beea30a4cc2d"><span data-offset-key="1df92de0255149988aa7beea30a4cc2d:0">&gt; ZEESQL.CREATE_DB DB</span></span></p><p><span data-key="c01d6a62c0a146e7b1d1c5a1fb417b66"><span data-offset-key="c01d6a62c0a146e7b1d1c5a1fb417b66:0">1) 1) "OK"</span></span></p></pre></div><p data-key="1b87ea8c5e1b47ad94439b9a26cf9fbb"><span><span data-key="83113dcf59e5422a927db8c39e0d584a"><span data-offset-key="83113dcf59e5422a927db8c39e0d584a:0">Now we can start populating our users.</span></span></span></p><div><pre data-key="8b6a725dd39749bcb347cbcbc5229b56" spellcheck="false"><p><span data-key="733614247796406dbff15878ec561667"><span data-offset-key="733614247796406dbff15878ec561667:0">127.0.0.1:6379&gt; HMSET user:100 id 100 name foo score 3</span></span></p><p><span data-key="4749160ea0dc45508e1b6304e867b4ff"><span data-offset-key="4749160ea0dc45508e1b6304e867b4ff:0">OK</span></span></p><p><span data-key="fdb58d43ab9842f89bf59da94280410c"><span data-offset-key="fdb58d43ab9842f89bf59da94280410c:0">127.0.0.1:6379&gt; HMSET user:103 id 103 name bar score 5</span></span></p><p><span data-key="f9e36faf2d624c4d864693f24e5f38c8"><span data-offset-key="f9e36faf2d624c4d864693f24e5f38c8:0">OK</span></span></p><p><span data-key="1cf9666f262245d4b746c6256456a494"><span data-offset-key="1cf9666f262245d4b746c6256456a494:0">127.0.0.1:6379&gt; HMSET user:105 id 105 name baz score 4</span></span></p></pre></div><p data-key="c83c38c1348148ec9920d0a49580af39"><span><span data-key="76674fa98f884c59b4c38a1d3e411686"><span data-offset-key="76674fa98f884c59b4c38a1d3e411686:0">We have created 3 users, each with its own name, id, and score.</span></span></span></p><p data-key="e82d713b050e48bebc0cfb7d1bdae6cd"><span><span data-key="409fb70dfbff4221bb7891d1e86dcce5"><span data-offset-key="409fb70dfbff4221bb7891d1e86dcce5:0">At this point, we can create a secondary index.</span></span></span></p><div><pre data-key="910e48df5c534836882bad7976ddf27d" spellcheck="false"><p><span data-key="5b706f8519cb44f2b0da2e8a918bfe05"><span data-offset-key="5b706f8519cb44f2b0da2e8a918bfe05:0">127.0.0.1:6379&gt; ZEESQL.INDEX DB NEW PREFIX user:* TABLE users SCHEMA id INT name STRING score INT</span></span></p><p><span data-key="2d3ac6c506dc4280bf80132b05886e82"><span data-offset-key="2d3ac6c506dc4280bf80132b05886e82:0">OK</span></span></p></pre></div><p data-key="1abfa63fbae041c68513636a38e4440b"><span><span data-key="9e9444e2707d4875b2c3a277d12785a3"><span data-offset-key="9e9444e2707d4875b2c3a277d12785a3:0">Now we have created the secondary index table.</span></span></span></p><p data-key="c29a3778078c4e8981886fc36db04b78"><span><span data-key="0528eca5b45a4e36a5a6f68f0909ff08"><span data-offset-key="0528eca5b45a4e36a5a6f68f0909ff08:0">We can visualize what table was created by querying the </span><span data-offset-key="0528eca5b45a4e36a5a6f68f0909ff08:1"><code spellcheck="false" data-slate-leaf="true">sqlite_master</code></span><span data-offset-key="0528eca5b45a4e36a5a6f68f0909ff08:2"> special table.</span></span></span></p><div><pre data-key="6dbcded6cad9473ea5ea9a43025e4318" spellcheck="false"><p><span data-key="7b5d28eefec549749a4337aa69197a8c"><span data-offset-key="7b5d28eefec549749a4337aa69197a8c:0">127.0.0.1:6379&gt; ZEESQL.EXEC DB COMMAND "select * from sqlite_master;"</span></span></p><p><span data-key="f9259fc9e9484284bb1fd95577ee35f5"><span data-offset-key="f9259fc9e9484284bb1fd95577ee35f5:0">1) 1) "RESULT"</span></span></p><p><span data-key="a2c2df4ff75e49359d415ee9f66b8a32"><span data-offset-key="a2c2df4ff75e49359d415ee9f66b8a32:0">2) 1) "type"</span></span></p><p><span data-key="89be74d7bc3e4638adb661f200ac37f4"><span data-offset-key="89be74d7bc3e4638adb661f200ac37f4:0">   2) "name"</span></span></p><p><span data-key="765a1ca9f1ee40ccadbddc3954797376"><span data-offset-key="765a1ca9f1ee40ccadbddc3954797376:0">   3) "tbl_name"</span></span></p><p><span data-key="373a7b02a7c34436bc58f95d9e5596c5"><span data-offset-key="373a7b02a7c34436bc58f95d9e5596c5:0">   4) "rootpage"</span></span></p><p><span data-key="7ec90ff23a214b97b0d20a44a4d1d282"><span data-offset-key="7ec90ff23a214b97b0d20a44a4d1d282:0">   5) "sql"</span></span></p><p><span data-key="e87490d6631a4ce29993829d847f230b"><span data-offset-key="e87490d6631a4ce29993829d847f230b:0">3) 1) "TEXT"</span></span></p><p><span data-key="9c24d4e4ab504abe88c664bf28ee271c"><span data-offset-key="9c24d4e4ab504abe88c664bf28ee271c:0">   2) "TEXT"</span></span></p><p><span data-key="1a3a160e456b4ae1822a08000955298b"><span data-offset-key="1a3a160e456b4ae1822a08000955298b:0">   3) "TEXT"</span></span></p><p><span data-key="b3c69febf9874191adebff0d667f2729"><span data-offset-key="b3c69febf9874191adebff0d667f2729:0">   4) "INT"</span></span></p><p><span data-key="6dc04a0b3ca44a23ac207c22f49fb4b9"><span data-offset-key="6dc04a0b3ca44a23ac207c22f49fb4b9:0">   5) "TEXT"</span></span></p><p><span data-key="8cfa7de313e2483391c9caef6aaf1764"><span data-offset-key="8cfa7de313e2483391c9caef6aaf1764:0">5) 1) "table"</span></span></p><p><span data-key="796a6deefc8147d1aefb2605b94fbe95"><span data-offset-key="796a6deefc8147d1aefb2605b94fbe95:0">   2) "users"</span></span></p><p><span data-key="f2fb9cc1644049afbf142b7df12fddd2"><span data-offset-key="f2fb9cc1644049afbf142b7df12fddd2:0">   3) "users"</span></span></p><p><span data-key="ed98f401d4944df6b48dea095d52ebbb"><span data-offset-key="ed98f401d4944df6b48dea095d52ebbb:0">   4) (integer) 3</span></span></p><p><span data-key="285492e07765451dbb8d879acf9b2383"><span data-offset-key="285492e07765451dbb8d879acf9b2383:0">   5) "CREATE TABLE users(key PRIMARY KEY, id INT, name STRING, score INT)"</span></span></p></pre></div><p data-key="be0d6ab0f183405190f73ddb1bb8e1a9"><span><span data-key="124307480bd440d8b34f4cd6013fa63f"><span data-offset-key="124307480bd440d8b34f4cd6013fa63f:0">Exactly what we would expect, the </span><span data-offset-key="124307480bd440d8b34f4cd6013fa63f:1"><code spellcheck="false" data-slate-leaf="true">key</code></span><span data-offset-key="124307480bd440d8b34f4cd6013fa63f:2"> column as primary key, where we will store the key of the Redis Hash, and then the schema we asked for.</span></span></span></p><p data-key="8a9833224b7a493fb312bebed9d24bf9"><span><span data-key="82dce88e84e44c12b3fbd2ccff00458f"><span data-offset-key="82dce88e84e44c12b3fbd2ccff00458f:0">Since the secondary index was created after some Redis Hashes were already inside Redis, the table is already populated.</span></span></span></p><div><pre data-key="b4d781b4e1f94042a2c682c1e8ddc731" spellcheck="false"><p><span data-key="7eb5181ef9924945ab57b7f3d1ba45d1"><span data-offset-key="7eb5181ef9924945ab57b7f3d1ba45d1:0">127.0.0.1:6379&gt; ZEESQL.EXEC DB COMMAND "select * from users;"</span></span></p><p><span data-key="36f776e471a14deb9c71e2937cf32c6a"><span data-offset-key="36f776e471a14deb9c71e2937cf32c6a:0">1) 1) "RESULT"</span></span></p><p><span data-key="0d6b40acd6ee44e1a363f58feb4ea589"><span data-offset-key="0d6b40acd6ee44e1a363f58feb4ea589:0">2) 1) "key"</span></span></p><p><span data-key="01a97c64ed2846f4948cf857668a1250"><span data-offset-key="01a97c64ed2846f4948cf857668a1250:0">   2) "id"</span></span></p><p><span data-key="cb1529ba338d4883a9c0f9adf38313a7"><span data-offset-key="cb1529ba338d4883a9c0f9adf38313a7:0">   3) "name"</span></span></p><p><span data-key="48ddf0b693b74c539a772a116f00d024"><span data-offset-key="48ddf0b693b74c539a772a116f00d024:0">   4) "score"</span></span></p><p><span data-key="e6013b093e2c469da24fb58f81eea50d"><span data-offset-key="e6013b093e2c469da24fb58f81eea50d:0">3) 1) "TEXT"</span></span></p><p><span data-key="d75a1c6b155b4ce4b9a04bf63e2aac0e"><span data-offset-key="d75a1c6b155b4ce4b9a04bf63e2aac0e:0">   2) "INT"</span></span></p><p><span data-key="5bcfc3bff7844528a19e5719877cc37a"><span data-offset-key="5bcfc3bff7844528a19e5719877cc37a:0">   3) "TEXT"</span></span></p><p><span data-key="0ac46fddc2214670ac5acb3ff01d5738"><span data-offset-key="0ac46fddc2214670ac5acb3ff01d5738:0">   4) "INT"</span></span></p><p><span data-key="edfe48cc21024658874411b33ed54659"><span data-offset-key="edfe48cc21024658874411b33ed54659:0">4) 1) "user:105"</span></span></p><p><span data-key="21443b8d18474b9d8d0f0406f475c626"><span data-offset-key="21443b8d18474b9d8d0f0406f475c626:0">   2) (integer) 105</span></span></p><p><span data-key="cd1f0c36c8214387bf5ccf39a25edc78"><span data-offset-key="cd1f0c36c8214387bf5ccf39a25edc78:0">   3) "baz"</span></span></p><p><span data-key="7957011c160340569c719f2bab2607ad"><span data-offset-key="7957011c160340569c719f2bab2607ad:0">   4) (integer) 4</span></span></p><p><span data-key="d603aa26406044edbc631b7c24b99f8e"><span data-offset-key="d603aa26406044edbc631b7c24b99f8e:0">5) 1) "user:103"</span></span></p><p><span data-key="086f05920e5f4660ae55e26c64ae6ccd"><span data-offset-key="086f05920e5f4660ae55e26c64ae6ccd:0">   2) (integer) 103</span></span></p><p><span data-key="e2c87afa241b4eff9a724cf67cf4551a"><span data-offset-key="e2c87afa241b4eff9a724cf67cf4551a:0">   3) "bar"</span></span></p><p><span data-key="c606911f61634ab2a7644916bff8ed89"><span data-offset-key="c606911f61634ab2a7644916bff8ed89:0">   4) (integer) 5</span></span></p><p><span data-key="06525514d57b49c4a2dcfeae4e9a6ba9"><span data-offset-key="06525514d57b49c4a2dcfeae4e9a6ba9:0">6) 1) "user:100"</span></span></p><p><span data-key="57e1eb3e7c10429c980909d5f2dcddcb"><span data-offset-key="57e1eb3e7c10429c980909d5f2dcddcb:0">   2) (integer) 100</span></span></p><p><span data-key="3233dda1e1054ce99f30fa95398ebe4c"><span data-offset-key="3233dda1e1054ce99f30fa95398ebe4c:0">   3) "foo"</span></span></p><p><span data-key="c84cf0238bc843e8a410b1b179f12707"><span data-offset-key="c84cf0238bc843e8a410b1b179f12707:0">   4) (integer) 3</span></span></p></pre></div><p data-key="d02ee93f64ec4f48b4ebd892d275733d"><span><span data-key="58dc063cfc7744bfbb0d13d41cea713e"><span data-offset-key="58dc063cfc7744bfbb0d13d41cea713e:0">If now we add a new user, the new user will be automatically added to the secondary index table.</span></span></span></p><div><pre data-key="650c5b0b2a874ae6a2abc0f923e1272c" spellcheck="false"><p><span data-key="fcb5b7ec11d24d02928e8f7b80079cea"><span data-offset-key="fcb5b7ec11d24d02928e8f7b80079cea:0">127.0.0.1:6379&gt; HMSET user:109 id 109 name joe score 2</span></span></p><p><span data-key="feda4220bcda47c6b50309eba9749e9d"><span data-offset-key="feda4220bcda47c6b50309eba9749e9d:0">OK</span></span></p><p><span data-key="0fd015eec06144ed974d0cb7051cb221"><span data-offset-key="0fd015eec06144ed974d0cb7051cb221:0">127.0.0.1:6379&gt; ZEESQL.EXEC DB COMMAND "select * from users;"</span></span></p><p><span data-key="0ad1e6b66e4c47fc83b9e6feab075d3a"><span data-offset-key="0ad1e6b66e4c47fc83b9e6feab075d3a:0">1) 1) "RESULT"</span></span></p><p><span data-key="0d59036686aa466fbb64b2e472b2ef5b"><span data-offset-key="0d59036686aa466fbb64b2e472b2ef5b:0">2) 1) "key"</span></span></p><p><span data-key="11b5433d58a74d67bdf69eb7363642bd"><span data-offset-key="11b5433d58a74d67bdf69eb7363642bd:0">   2) "id"</span></span></p><p><span data-key="459b5bcec4fe4c91bbb620f3125ebe49"><span data-offset-key="459b5bcec4fe4c91bbb620f3125ebe49:0">   3) "name"</span></span></p><p><span data-key="ed178d056adb472088955f41077619a6"><span data-offset-key="ed178d056adb472088955f41077619a6:0">   4) "score"</span></span></p><p><span data-key="5bc62a34a44f4f4495367c0cec7f1612"><span data-offset-key="5bc62a34a44f4f4495367c0cec7f1612:0">3) 1) "TEXT"</span></span></p><p><span data-key="06d4b681c9f94fb49613d99a0a410edf"><span data-offset-key="06d4b681c9f94fb49613d99a0a410edf:0">   2) "INT"</span></span></p><p><span data-key="153b7dc0336c45bda3acc5607923fdba"><span data-offset-key="153b7dc0336c45bda3acc5607923fdba:0">   3) "TEXT"</span></span></p><p><span data-key="ed0cbae889664b31a0363395ef2b02e4"><span data-offset-key="ed0cbae889664b31a0363395ef2b02e4:0">   4) "INT"</span></span></p><p><span data-key="1c9029b67ce84ce882d660b32f5afca0"><span data-offset-key="1c9029b67ce84ce882d660b32f5afca0:0">4) 1) "user:105"</span></span></p><p><span data-key="c5ab61341b8c40c598c9c9de4407c069"><span data-offset-key="c5ab61341b8c40c598c9c9de4407c069:0">   2) (integer) 105</span></span></p><p><span data-key="fbbcd1c59d4f4d6b86e7d59df292de8f"><span data-offset-key="fbbcd1c59d4f4d6b86e7d59df292de8f:0">   3) "baz"</span></span></p><p><span data-key="3a146f59943741cda7a6557e81d25d5a"><span data-offset-key="3a146f59943741cda7a6557e81d25d5a:0">   4) (integer) 4</span></span></p><p><span data-key="3065c67fea3e44799c49811fdb29a1bd"><span data-offset-key="3065c67fea3e44799c49811fdb29a1bd:0">5) 1) "user:103"</span></span></p><p><span data-key="a26e51604c5f4943b802f288fef974f1"><span data-offset-key="a26e51604c5f4943b802f288fef974f1:0">   2) (integer) 103</span></span></p><p><span data-key="f64b22e7374345aaacaf46fa0ac602fb"><span data-offset-key="f64b22e7374345aaacaf46fa0ac602fb:0">   3) "bar"</span></span></p><p><span data-key="2892fb90cbbc43dead8d5aa18fc4d209"><span data-offset-key="2892fb90cbbc43dead8d5aa18fc4d209:0">   4) (integer) 5</span></span></p><p><span data-key="3999e524be1c416eaf618e8eca714494"><span data-offset-key="3999e524be1c416eaf618e8eca714494:0">6) 1) "user:100"</span></span></p><p><span data-key="bbf693c7fcad4f17a74db63ffa759c83"><span data-offset-key="bbf693c7fcad4f17a74db63ffa759c83:0">   2) (integer) 100</span></span></p><p><span data-key="3b227534b1a54bf49ec503c668ad4242"><span data-offset-key="3b227534b1a54bf49ec503c668ad4242:0">   3) "foo"</span></span></p><p><span data-key="0c04e94226b44dbb9ab56d5c89de1773"><span data-offset-key="0c04e94226b44dbb9ab56d5c89de1773:0">   4) (integer) 3</span></span></p><p><span data-key="aaff7fa5b46243828f075d3426691523"><span data-offset-key="aaff7fa5b46243828f075d3426691523:0">7) 1) "user:109"</span></span></p><p><span data-key="966b457e4c6f447baaf02c5fbf565742"><span data-offset-key="966b457e4c6f447baaf02c5fbf565742:0">   2) (integer) 109</span></span></p><p><span data-key="9b1adc328de843a8bd723aee7e906a5e"><span data-offset-key="9b1adc328de843a8bd723aee7e906a5e:0">   3) "joe"</span></span></p><p><span data-key="f8bde8ac13b5420eb39a3f3e36c9cc80"><span data-offset-key="f8bde8ac13b5420eb39a3f3e36c9cc80:0">   4) (integer) 2</span></span></p></pre></div><p data-key="1a160d1023e74c6e87b8c4699a708b8f"><span><span data-key="e8d392e71a9442c2bf9533005b02fdde"><span data-offset-key="e8d392e71a9442c2bf9533005b02fdde:0">Similarly, if a user is updated, the table will reflect the new status of the Redis Hash.</span></span></span></p><div><pre data-key="8f67b9dff9fa42909515c3e3976eeabf" spellcheck="false"><p><span data-key="f05523a1d4164c18a5f71eba9ff0055b"><span data-offset-key="f05523a1d4164c18a5f71eba9ff0055b:0">127.0.0.1:6379&gt; HSET user:109 score 5</span></span></p><p><span data-key="6d18cfa1908448e59a5cf618a3161acc"><span data-offset-key="6d18cfa1908448e59a5cf618a3161acc:0">(integer) 0</span></span></p><p><span data-key="31b281af2ca944bda2f86947b7f54138"><span data-offset-key="31b281af2ca944bda2f86947b7f54138:0">127.0.0.1:6379&gt; ZEESQL.EXEC DB COMMAND "select * from users where id = 109;"</span></span></p><p><span data-key="69dd0a218c604e6780abb5d00adc5dda"><span data-offset-key="69dd0a218c604e6780abb5d00adc5dda:0">1) 1) "RESULT"</span></span></p><p><span data-key="3024dd94a7dd40fdb548f357d6313880"><span data-offset-key="3024dd94a7dd40fdb548f357d6313880:0">2) 1) "key"</span></span></p><p><span data-key="0d85956da6214fafaea92549e4469a8e"><span data-offset-key="0d85956da6214fafaea92549e4469a8e:0">   2) "id"</span></span></p><p><span data-key="67cd4580b6b84ef895e8a9bb9066aae9"><span data-offset-key="67cd4580b6b84ef895e8a9bb9066aae9:0">   3) "name"</span></span></p><p><span data-key="cf0b7eb1720b4eb0bce6d5909bede397"><span data-offset-key="cf0b7eb1720b4eb0bce6d5909bede397:0">   4) "score"</span></span></p><p><span data-key="33b7a1092c3747d6a47b5bff4c1d1449"><span data-offset-key="33b7a1092c3747d6a47b5bff4c1d1449:0">3) 1) "TEXT"</span></span></p><p><span data-key="e5cfa9783f754ac5b558d114eda4adf4"><span data-offset-key="e5cfa9783f754ac5b558d114eda4adf4:0">   2) "INT"</span></span></p><p><span data-key="13a6d89f25b740929166f71f29249394"><span data-offset-key="13a6d89f25b740929166f71f29249394:0">   3) "TEXT"</span></span></p><p><span data-key="d30497b7de2f4b64b9b8062fcba38ac2"><span data-offset-key="d30497b7de2f4b64b9b8062fcba38ac2:0">   4) "INT"</span></span></p><p><span data-key="4298d70401c64145a13ec8b3883a150d"><span data-offset-key="4298d70401c64145a13ec8b3883a150d:0">4) 1) "user:109"</span></span></p><p><span data-key="621806a9634f4f12a312309b2cf0a87d"><span data-offset-key="621806a9634f4f12a312309b2cf0a87d:0">   2) (integer) 109</span></span></p><p><span data-key="01db1cb2ffb645019124d6088c25ccf2"><span data-offset-key="01db1cb2ffb645019124d6088c25ccf2:0">   3) "joe"</span></span></p><p><span data-key="6e04d66853494ea9bde44b11e299f7fc"><span data-offset-key="6e04d66853494ea9bde44b11e299f7fc:0">   4) (integer) 5</span></span></p></pre></div><p data-key="833cfb4bee854b3f9b1f09735099a707"><span><span data-key="920c211a880c4f2a94b24346191c852b"><span data-offset-key="920c211a880c4f2a94b24346191c852b:0">Similarly, a Redis Hash deleted, will be removed from the secondary index table.</span></span></span></p><div><pre data-key="f9fda9c94b1e43069c1a22f0d50858d9" spellcheck="false"><p><span data-key="949a2d01533d4f009c5004ebe0633ecb"><span data-offset-key="949a2d01533d4f009c5004ebe0633ecb:0">127.0.0.1:6379&gt; DEL user:105 user:103</span></span></p><p><span data-key="6c1116e63b2f422c86b06d9eba4c97a7"><span data-offset-key="6c1116e63b2f422c86b06d9eba4c97a7:0">(integer) 2</span></span></p><p><span data-key="ef092de9252a41209ee83b7911467a30"><span data-offset-key="ef092de9252a41209ee83b7911467a30:0">127.0.0.1:6379&gt; ZEESQL.EXEC DB COMMAND "select * from users;"</span></span></p><p><span data-key="6c197f3b245d421dbb04537c139f9718"><span data-offset-key="6c197f3b245d421dbb04537c139f9718:0">1) 1) "RESULT"</span></span></p><p><span data-key="77619f600f91412e93cf1075c207b9c2"><span data-offset-key="77619f600f91412e93cf1075c207b9c2:0">2) 1) "key"</span></span></p><p><span data-key="c32a22132ad442018eda9d61d1944ee3"><span data-offset-key="c32a22132ad442018eda9d61d1944ee3:0">   2) "id"</span></span></p><p><span data-key="4a6b5264cfb64cdd91967da055e9e1db"><span data-offset-key="4a6b5264cfb64cdd91967da055e9e1db:0">   3) "name"</span></span></p><p><span data-key="028bcac96c714808b3f37782546ca84d"><span data-offset-key="028bcac96c714808b3f37782546ca84d:0">   4) "score"</span></span></p><p><span data-key="5b76da9b877c44f1aa04c191bc5e926a"><span data-offset-key="5b76da9b877c44f1aa04c191bc5e926a:0">3) 1) "TEXT"</span></span></p><p><span data-key="087b99372f6f4b3ab6a5e9a7aa0522d7"><span data-offset-key="087b99372f6f4b3ab6a5e9a7aa0522d7:0">   2) "INT"</span></span></p><p><span data-key="e7efd8100b1146d28d475ca8178e033e"><span data-offset-key="e7efd8100b1146d28d475ca8178e033e:0">   3) "TEXT"</span></span></p><p><span data-key="af9841411eaa414bb0f6c3ab474098c0"><span data-offset-key="af9841411eaa414bb0f6c3ab474098c0:0">   4) "INT"</span></span></p><p><span data-key="86832befb2184719ba5de8bf5fe6732e"><span data-offset-key="86832befb2184719ba5de8bf5fe6732e:0">4) 1) "user:100"</span></span></p><p><span data-key="e8e5d37d12c147f081d967e823dd1230"><span data-offset-key="e8e5d37d12c147f081d967e823dd1230:0">   2) (integer) 100</span></span></p><p><span data-key="cb08972c817e48848e2c69f5120e03f8"><span data-offset-key="cb08972c817e48848e2c69f5120e03f8:0">   3) "foo"</span></span></p><p><span data-key="adbb71d862e0485d994ffc74e6be798d"><span data-offset-key="adbb71d862e0485d994ffc74e6be798d:0">   4) (integer) 3</span></span></p><p><span data-key="b29c06cdca1f4678ba97947a9cb3796b"><span data-offset-key="b29c06cdca1f4678ba97947a9cb3796b:0">5) 1) "user:109"</span></span></p><p><span data-key="bc30cf418a0e47ba9ec672f5e4221042"><span data-offset-key="bc30cf418a0e47ba9ec672f5e4221042:0">   2) (integer) 109</span></span></p><p><span data-key="0d9007b5ac2848ba95374ef5a9c2f8dc"><span data-offset-key="0d9007b5ac2848ba95374ef5a9c2f8dc:0">   3) "joe"</span></span></p><p><span data-key="666067e23fc64f82aa2232e15b5af710"><span data-offset-key="666067e23fc64f82aa2232e15b5af710:0">   4) (integer) 5</span></span></p></pre></div><p data-key="9770666b35834fcc9f77df1c9111cb4b"><span><span data-key="3b0c9b03f8de43f1a6b6a5cb89d64cdb"><span data-offset-key="3b0c9b03f8de43f1a6b6a5cb89d64cdb:0">The first example was very straightforward. But we can use zeeSQL for something more.</span></span></span></p><p data-key="769897eee00e4b4580d42482b3226458"><span><span data-key="caf8d16cea754a0f8faaa46f0a94646c"><span data-offset-key="caf8d16cea754a0f8faaa46f0a94646c:0">For instance, maybe we want to give a rank to our users.</span></span></span></p><p data-key="11fa456f19124055aefe709742649563"><span><span data-key="0bab3c9924cb4407ae9ecd173e421b5c"><span data-offset-key="0bab3c9924cb4407ae9ecd173e421b5c:0">Users with a score between 0 and 5 will be "novice" and users with a score above it will be expert.</span></span></span></p><p data-key="ee1056ec9ab64477b7a5791e274c48ff"><span><span data-key="949d22fe9d5e4e348e55c5db5141f1f7"><span data-offset-key="949d22fe9d5e4e348e55c5db5141f1f7:0">An easy way to achieve this would be to create a view on top of the </span><span data-offset-key="949d22fe9d5e4e348e55c5db5141f1f7:1"><code spellcheck="false" data-slate-leaf="true">users</code></span><span data-offset-key="949d22fe9d5e4e348e55c5db5141f1f7:2"> tables.</span></span></span></p><div><pre data-key="4116e844733e4334ab4c6ffd84661769" spellcheck="false"><p><span data-key="74b2674d074d4ce784bb2e41dfb8c9cd"><span data-offset-key="74b2674d074d4ce784bb2e41dfb8c9cd:0">127.0.0.1:6379&gt; ZEESQL.EXEC DB COMMAND "create view ranked_users AS select id, name, score, case  when score &lt; 5 then 'novice' else 'expert' end as rank from users;"</span></span></p><p><span data-key="7ddad598406c41a0862a9064c9086219"><span data-offset-key="7ddad598406c41a0862a9064c9086219:0">1) 1) "DONE"</span></span></p><p><span data-key="e578537d36d4458fafb662215ffd20f7"><span data-offset-key="e578537d36d4458fafb662215ffd20f7:0">2) 1) (integer) 0</span></span></p><p><span data-key="88e79536a3304ec5b61a2ce5425f8785"><span data-offset-key="88e79536a3304ec5b61a2ce5425f8785:0">127.0.0.1:6379&gt; ZEESQL.EXEC DB COMMAND "select * from ranked_users;"</span></span></p><p><span data-key="fb96c7f471854e4db876eec5c7dedcde"><span data-offset-key="fb96c7f471854e4db876eec5c7dedcde:0">1) 1) "RESULT"</span></span></p><p><span data-key="aad53ce37933463d885f1ccd442e1314"><span data-offset-key="aad53ce37933463d885f1ccd442e1314:0">2) 1) "id"</span></span></p><p><span data-key="c9564cc96e174e91b57b6d5a664f414c"><span data-offset-key="c9564cc96e174e91b57b6d5a664f414c:0">   2) "name"</span></span></p><p><span data-key="eebfefce20654399a7aa08c415ed1c82"><span data-offset-key="eebfefce20654399a7aa08c415ed1c82:0">   3) "score"</span></span></p><p><span data-key="5921017c2a88443faaabf91622bc405f"><span data-offset-key="5921017c2a88443faaabf91622bc405f:0">   4) "rank"</span></span></p><p><span data-key="985b0a02219f460db7f888528a4f318f"><span data-offset-key="985b0a02219f460db7f888528a4f318f:0">3) 1) "INT"</span></span></p><p><span data-key="55a5a16b16a64af5a989aab8adee05bb"><span data-offset-key="55a5a16b16a64af5a989aab8adee05bb:0">   2) "TEXT"</span></span></p><p><span data-key="d7c4b13411704df6a2b465afbefbe51b"><span data-offset-key="d7c4b13411704df6a2b465afbefbe51b:0">   3) "INT"</span></span></p><p><span data-key="b0d4fbe116f34ac2892eef1beedf7411"><span data-offset-key="b0d4fbe116f34ac2892eef1beedf7411:0">   4) "TEXT"</span></span></p><p><span data-key="a7999de4d1ee498b8c7cdb9ad1fb374d"><span data-offset-key="a7999de4d1ee498b8c7cdb9ad1fb374d:0">4) 1) (integer) 100</span></span></p><p><span data-key="4e1324c8f2834c4b9e0b033b7f8ee33b"><span data-offset-key="4e1324c8f2834c4b9e0b033b7f8ee33b:0">   2) "foo"</span></span></p><p><span data-key="dfa92c77ba8e4632a14a289de4b57037"><span data-offset-key="dfa92c77ba8e4632a14a289de4b57037:0">   3) (integer) 3</span></span></p><p><span data-key="642a3ad93c6540d9aa1da9238ce5dadc"><span data-offset-key="642a3ad93c6540d9aa1da9238ce5dadc:0">   4) "novice"</span></span></p><p><span data-key="04935db850674a86a9e632385642e4f5"><span data-offset-key="04935db850674a86a9e632385642e4f5:0">5) 1) (integer) 109</span></span></p><p><span data-key="697806dea7ab40c2a69e5414dc2ed228"><span data-offset-key="697806dea7ab40c2a69e5414dc2ed228:0">   2) "joe"</span></span></p><p><span data-key="869354e500bf4e9ca6e815b519a574b8"><span data-offset-key="869354e500bf4e9ca6e815b519a574b8:0">   3) (integer) 5</span></span></p><p><span data-key="24853b30138c4242bbb126f620cf2dbc"><span data-offset-key="24853b30138c4242bbb126f620cf2dbc:0">   4) "expert"</span></span></p></pre></div><p data-key="4fc64d6a79f84e5db5bc34a7afa018cd"><span><span data-key="2dae541b1f6e40d098277b267443f2af"><span data-offset-key="2dae541b1f6e40d098277b267443f2af:0">If the user with ID 100, gains a few more points, he will become an expert as well.</span></span></span></p><p data-key="dc3fdb7211fe49888711c6f727562e9a"><span><span data-key="4c374390ad58457db126e1132532c65a"><span data-offset-key="4c374390ad58457db126e1132532c65a:0">Using views on top of secondary indexes, we only need to care about the user score, not about the rank. Using plain Redis we would need to keep track also of the rank ourselves.</span></span></span></p><div><pre data-key="047da2fdaed24a798529bdd4ff0a7c06" spellcheck="false"><p><span data-key="687b203e4f254c569c39adbda358456b"><span data-offset-key="687b203e4f254c569c39adbda358456b:0">127.0.0.1:6379&gt; HSET user:100 score 7</span></span></p><p><span data-key="499aee225467452f8fe1da061a79c65c"><span data-offset-key="499aee225467452f8fe1da061a79c65c:0">(integer) 0</span></span></p><p><span data-key="bdddb66fc53e4db2ac86234d48bb978b"><span data-offset-key="bdddb66fc53e4db2ac86234d48bb978b:0">127.0.0.1:6379&gt; ZEESQL.EXEC DB COMMAND "select * from ranked_users;"</span></span></p><p><span data-key="ee02c16ed00a4bf7a5fa095fc3e2f551"><span data-offset-key="ee02c16ed00a4bf7a5fa095fc3e2f551:0">1) 1) "RESULT"</span></span></p><p><span data-key="18663ecdfb2e4326bdca551b8a840095"><span data-offset-key="18663ecdfb2e4326bdca551b8a840095:0">2) 1) "id"</span></span></p><p><span data-key="ff09515445e146619e771f0666fe644a"><span data-offset-key="ff09515445e146619e771f0666fe644a:0">   2) "name"</span></span></p><p><span data-key="662f7850f30f4e81b61111a7bf1f0999"><span data-offset-key="662f7850f30f4e81b61111a7bf1f0999:0">   3) "score"</span></span></p><p><span data-key="f003a4c9343d41e0b3495fc3427a03d8"><span data-offset-key="f003a4c9343d41e0b3495fc3427a03d8:0">   4) "rank"</span></span></p><p><span data-key="9df370e084894be89d8589042dc20434"><span data-offset-key="9df370e084894be89d8589042dc20434:0">3) 1) "INT"</span></span></p><p><span data-key="1744a492fc8d4aefa6238852aff926f8"><span data-offset-key="1744a492fc8d4aefa6238852aff926f8:0">   2) "TEXT"</span></span></p><p><span data-key="b3c396ad930d4af7ac3e7c125e6bea38"><span data-offset-key="b3c396ad930d4af7ac3e7c125e6bea38:0">   3) "INT"</span></span></p><p><span data-key="493dad483d6248bbbf4939cf400df513"><span data-offset-key="493dad483d6248bbbf4939cf400df513:0">   4) "TEXT"</span></span></p><p><span data-key="a16705c48c5546c39840fe9c4e2c794c"><span data-offset-key="a16705c48c5546c39840fe9c4e2c794c:0">4) 1) (integer) 100</span></span></p><p><span data-key="ad3f33be979c4b7c834182649dbd3510"><span data-offset-key="ad3f33be979c4b7c834182649dbd3510:0">   2) "foo"</span></span></p><p><span data-key="65ef3afbeeb0440e922295c7a8b35806"><span data-offset-key="65ef3afbeeb0440e922295c7a8b35806:0">   3) (integer) 7</span></span></p><p><span data-key="a0d6bde9e9d443698d7cb8ef9834e286"><span data-offset-key="a0d6bde9e9d443698d7cb8ef9834e286:0">   4) "expert"</span></span></p><p><span data-key="88c48a7cb29446c6aca484a213f98546"><span data-offset-key="88c48a7cb29446c6aca484a213f98546:0">5) 1) (integer) 109</span></span></p><p><span data-key="69ed4d3949d1402cb03cb2a6fc4c1824"><span data-offset-key="69ed4d3949d1402cb03cb2a6fc4c1824:0">   2) "joe"</span></span></p><p><span data-key="c807e7d6090549d4a7650232bfa64a17"><span data-offset-key="c807e7d6090549d4a7650232bfa64a17:0">   3) (integer) 5</span></span></p><p><span data-key="1a0c68955caf4b6399e23545d49f746d"><span data-offset-key="1a0c68955caf4b6399e23545d49f746d:0">   4) "expert"</span></span></p></pre></div><p data-key="4119304ffe644187b642b5d58b61eae9"><span><span data-key="8ee61fb3a7c4443d923053a87f028138"><span data-offset-key="8ee61fb3a7c4443d923053a87f028138:0">If our game gains a lot of users some queries could become slow.</span></span></span></p><p data-key="50308bf8ce2140188fb486118cba12d0"><span><span data-key="df83984b06584115a92696495b26463c"><span data-offset-key="df83984b06584115a92696495b26463c:0">On top of the secondary index table, it is possible to add SQLite indexes.</span></span></span></p><p data-key="0031c96d7f9a469690f28e52f6eebb3d"><span><span data-key="3dee28dedd294e01ac49751178891c6f"><span data-offset-key="3dee28dedd294e01ac49751178891c6f:0">For instance, we might want to know how many users have a particular score. If there are a lot of users, this query might be slow.</span></span></span></p><div><pre data-key="6c0573a0425742ac8ad3bdafd9108f77" spellcheck="false"><p><span data-key="f6c6a3d8fa2642bbbfff1553ee3b29ab"><span data-offset-key="f6c6a3d8fa2642bbbfff1553ee3b29ab:0">127.0.0.1:6379&gt; ZEESQL.EXEC DB COMMAND "explain query plan select * from users where score = 3;"</span></span></p><p><span data-key="4f95115447f64dbb9d0038a97370d52d"><span data-offset-key="4f95115447f64dbb9d0038a97370d52d:0">1) 1) "RESULT"</span></span></p><p><span data-key="9723ed648f1445fdae94fd1809b430e6"><span data-offset-key="9723ed648f1445fdae94fd1809b430e6:0">2) 1) "id"</span></span></p><p><span data-key="1e3805785d834904918d0d5da858e344"><span data-offset-key="1e3805785d834904918d0d5da858e344:0">   2) "parent"</span></span></p><p><span data-key="0b4d7142fc234471a57279a601de4168"><span data-offset-key="0b4d7142fc234471a57279a601de4168:0">   3) "notused"</span></span></p><p><span data-key="5b06356d6cb640d99b1c315e37f4512f"><span data-offset-key="5b06356d6cb640d99b1c315e37f4512f:0">   4) "detail"</span></span></p><p><span data-key="f84af901b7c6455e84ca1a485b19c1e3"><span data-offset-key="f84af901b7c6455e84ca1a485b19c1e3:0">3) 1) "INT"</span></span></p><p><span data-key="c9e165b7b7604b9d82d4ceaaab95fb3d"><span data-offset-key="c9e165b7b7604b9d82d4ceaaab95fb3d:0">   2) "INT"</span></span></p><p><span data-key="9cde4c9c8f9b4866b6c07f7c46b2dc93"><span data-offset-key="9cde4c9c8f9b4866b6c07f7c46b2dc93:0">   3) "INT"</span></span></p><p><span data-key="6f98716bad0949629c170c2056565d2e"><span data-offset-key="6f98716bad0949629c170c2056565d2e:0">   4) "TEXT"</span></span></p><p><span data-key="67744d790f0f4b4d97982af6aefbabd4"><span data-offset-key="67744d790f0f4b4d97982af6aefbabd4:0">4) 1) (integer) 2</span></span></p><p><span data-key="38cf9a95f8f44ffb9044fffcfab1722b"><span data-offset-key="38cf9a95f8f44ffb9044fffcfab1722b:0">   2) (integer) 0</span></span></p><p><span data-key="fe1fe87ee3744e5f8c66cf96f2be9cd2"><span data-offset-key="fe1fe87ee3744e5f8c66cf96f2be9cd2:0">   3) (integer) 0</span></span></p><p><span data-key="6979513ae5b6456089283c895eb69c8c"><span data-offset-key="6979513ae5b6456089283c895eb69c8c:0">   4) "SCAN TABLE users"</span></span></p></pre></div><p data-key="35def4e10dcc4fbaad6e1d80058dc3e4"><span><span data-key="141fb47116534d7bba3a7e9ef53b19a6"><span data-offset-key="141fb47116534d7bba3a7e9ef53b19a6:0">This query uses a full table scan.</span></span></span></p><p data-key="3548cc11be154dd5b85e4573b8924bac"><span><span data-key="723d290715a04b86bfcf1449c05d7a87"><span data-offset-key="723d290715a04b86bfcf1449c05d7a87:0">We can do better defining an index:</span></span></span></p><div><pre data-key="bbba01aaf5ba4fa7a26241058d769ef7" spellcheck="false"><p><span data-key="47f6342fe4e14a24a59897d52c81c854"><span data-offset-key="47f6342fe4e14a24a59897d52c81c854:0">127.0.0.1:6379&gt; ZEESQL.EXEC DB COMMAND "create index user_rank on users(score);"</span></span></p><p><span data-key="0cd099b67f294f8baac951caa02119da"><span data-offset-key="0cd099b67f294f8baac951caa02119da:0">1) 1) "DONE"</span></span></p><p><span data-key="dd4e01e08528487f874bfb71b6e45796"><span data-offset-key="dd4e01e08528487f874bfb71b6e45796:0">2) 1) (integer) 0</span></span></p><p><span data-key="5ca379dd3e2140818be55a21fd5a4f68"><span data-offset-key="5ca379dd3e2140818be55a21fd5a4f68:0">127.0.0.1:6379&gt; ZEESQL.EXEC DB COMMAND "explain query plan select * from users where score = 3;"</span></span></p><p><span data-key="21ca808bc0e74c78a1d800caa36c3695"><span data-offset-key="21ca808bc0e74c78a1d800caa36c3695:0">1) 1) "RESULT"</span></span></p><p><span data-key="37519cface344bea90e910bc940dcc5b"><span data-offset-key="37519cface344bea90e910bc940dcc5b:0">2) 1) "id"</span></span></p><p><span data-key="372e52aac87d4ff28eee236474a594c1"><span data-offset-key="372e52aac87d4ff28eee236474a594c1:0">   2) "parent"</span></span></p><p><span data-key="699e0e314eb44f39a1db5ea861ac49a6"><span data-offset-key="699e0e314eb44f39a1db5ea861ac49a6:0">   3) "notused"</span></span></p><p><span data-key="ac7c75680ba641c0b2de63a4f65dd073"><span data-offset-key="ac7c75680ba641c0b2de63a4f65dd073:0">   4) "detail"</span></span></p><p><span data-key="8033787da78a4271b77b81a1e27a2ae2"><span data-offset-key="8033787da78a4271b77b81a1e27a2ae2:0">3) 1) "INT"</span></span></p><p><span data-key="7a5ab1ef5aab4b74864ed5a297e97a38"><span data-offset-key="7a5ab1ef5aab4b74864ed5a297e97a38:0">   2) "INT"</span></span></p><p><span data-key="3dbdbd17667e493ca98d94657db9f431"><span data-offset-key="3dbdbd17667e493ca98d94657db9f431:0">   3) "INT"</span></span></p><p><span data-key="c04cd5856c5e4557bb217f5adf3ed5a9"><span data-offset-key="c04cd5856c5e4557bb217f5adf3ed5a9:0">   4) "TEXT"</span></span></p><p><span data-key="1907b930a3e04e589540456f6014b4d5"><span data-offset-key="1907b930a3e04e589540456f6014b4d5:0">4) 1) (integer) 3</span></span></p><p><span data-key="1efb43cf24814ac9bf831c9ea8863208"><span data-offset-key="1efb43cf24814ac9bf831c9ea8863208:0">   2) (integer) 0</span></span></p><p><span data-key="0b7ea13efb7d43e6ba951404f66cb176"><span data-offset-key="0b7ea13efb7d43e6ba951404f66cb176:0">   3) (integer) 0</span></span></p><p><span data-key="8139ee74bd7f4d91a23b14dc61e0b47c"><span data-offset-key="8139ee74bd7f4d91a23b14dc61e0b47c:0">   4) "SEARCH TABLE users USING INDEX user_rank (score=?)"</span></span></p></pre></div><p data-key="29530945fbba4ee8b3a3f7e33f74ded1"><span><span data-key="f7619ba1f20b4c41baf96e16807b631c"><span data-offset-key="f7619ba1f20b4c41baf96e16807b631c:0">In this article, we show how to use secondary indexes in zeeSQL.</span></span></span></p><p data-key="7218ae5f91374d94b7f62e786a13b27a"><span><span data-key="a4cf2b2b2ce1416eb3a66e7b459cf32f"><span data-offset-key="a4cf2b2b2ce1416eb3a66e7b459cf32f:0">They are very powerful and useful when you are simplifying your queries against Redis Hashes. Moreover, they allow you to think only about the main data, it is the query engine that finds the best way to query your data for you.</span></span></span></p><p data-key="c301042f197349d1b5111bb87e64542b"><span><span data-key="589220fbd7d64dd2a804d5fa38983a29"><span data-offset-key="589220fbd7d64dd2a804d5fa38983a29:0">The important takeaway from this article should be that the table creates as zeeSQL secondary indexes are just standard tables. As such, those tables can be manipulated in whichever way the application developer finds more opportune.</span></span></span></p></div></div></div></div></div></div>]]>
            </description>
            <link>https://doc.zeesql.com/secondary-indexes</link>
            <guid isPermaLink="false">hacker-news-small-sites-26229236</guid>
            <pubDate>Mon, 22 Feb 2021 19:26:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[SrClient DLL Hijacking: a Windows Server 2012 0-day that won't be patched]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26229098">thread link</a>) | @lelf
<br/>
February 22, 2021 | https://blog.vonahi.io/srclient-dll-hijacking | <a href="https://web.archive.org/web/*/https://blog.vonahi.io/srclient-dll-hijacking">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://blog.vonahi.io/content/images/size/w300/2021/02/blog-post-dll.jpg 300w,
                            https://blog.vonahi.io/content/images/size/w600/2021/02/blog-post-dll.jpg 600w,
                            https://blog.vonahi.io/content/images/size/w1000/2021/02/blog-post-dll.jpg 1000w,
                            https://blog.vonahi.io/content/images/size/w2000/2021/02/blog-post-dll.jpg 2000w" sizes="(max-width: 800px) 400px,
                            (max-width: 1170px) 700px,
                            1400px" src="https://blog.vonahi.io/content/images/size/w2000/2021/02/blog-post-dll.jpg" alt="SrClient DLL Hijacking: a Windows Server 2012 0-day that won't be patched">
            </figure>

            <section>
                <div>
                    <p>I recently discovered that all versions of Windows Server 2012 (but not Server 2012 R2) are affected by a DLL hijacking vulnerability that can be exploited for privilege escalation. Moreover, the flaw can be triggered by a regular user and does not require a system reboot. Sounds like a pretty big deal, right? Well, not according to Microsoft, unfortunately. The vulnerability relies on <code>%PATH%</code> containing at least one insecurely configured directory, and <a href="https://msrc-blog.microsoft.com/2018/04/04/triaging-a-dll-planting-vulnerability/">Microsoft does not consider bugs</a> <a href="https://msrc-blog.microsoft.com/2018/04/04/triaging-a-dll-planting-vulnerability/">of this category to be security vulnerabilities worthy of a fix</a>. However, if your company is running Windows Server 2012, there is a decent chance this vulnerability can allow regular users (or attackers with access as a regular user) to pwn your server or domain controller. Let me show you how.</p><h2 id="0x00-a-refresher-on-dlls-and-path-">0x00 A Refresher on DLLs and %PATH%</h2><p>If you are unfamiliar with DLL hijacking, feel free to check out <a href="https://blog.vonahi.io/when-the-path-to-system-is-wide-open/">this blog post</a> that contains a general introduction to this subject. A brief summary of key concepts follows below.</p><h3 id="dlls">DLLs</h3><p>Microsoft defines a DLL as:</p><blockquote>a library that contains code and data that can be used by more than one program at the same time.</blockquote><p>DLLs are very similar to EXE files, but they can only be executed after being called by an EXE.</p><h3 id="dll-search-order">DLL Search Order</h3><p>Microsoft says <a href="https://msrc-blog.microsoft.com/2018/04/04/triaging-a-dll-planting-vulnerability/">this</a> about the DLL Search Order:</p><blockquote>when an application loads a DLL without specifying a fully qualified path, Windows attempts to locate the DLL by searching a well-defined set of directories in an order known as <strong>DLL search order</strong>.</blockquote><p>Starting with Windows XP SP2, the default DLL search order on Windows systems is something like this:</p><ol><li>The directory from which the application loaded.</li><li>The system directories. On modern 64-bit systems these are C:\Windows\System32 (64-bit programs and libraries - yes you are reading that right, the names are counterintuitive) and C:\Windows\SysWOW64 (32-bit programs and libraries). SysWOW64 is logically absent on 32-bit systems, where C:\Windows\System32 coexists with C:\Windows\System (16-bit programs and libraries).</li><li>The Windows directory (C:\Windows)</li><li>The current directory.</li><li>The directories that are listed in the <code>%PATH%</code> environment variable.</li></ol><h3 id="path">PATH</h3><p>PATH is an environment variable in Windows as well as Unix-like operating systems including Linux and MacOS. Basically, PATH is a special kind of variable that specifies a set of directories where executable programs are located. In Windows, this variable is referenced as %PATH%. <strong>In this article, I use %PATH% to refer exclusively to the system PATH, which <u>cannot be modified by regular users</u></strong>. It does not refer to the user PATH, which <em>can</em> be modified by regular users. For more info, see the <a href="https://en.wikipedia.org/wiki/PATH_(variable)">Wikipedia entry</a>.</p><h3 id="dll-hijacking-and-path-">DLL hijacking and %PATH%</h3><p>On a clean installation of any modern Windows system, <code>%PATH%</code> does not contain directories with weak permissions that would allow for the attack described in this article. However, many third-party applications add directories to <code>%PATH%</code> during installation and those directories aren’t always securely configured. As a result, it is not uncommon in corporate environments to find Windows systems that allow one or more regular users to write arbitrary data to <code>%PATH%</code> directories. If this is the case on a Windows Server 2012 system, it could be child's play for an attacker with the privileges of one such regular user to escalate privileges to <code>NT AUTOHRITY\SYSTEM</code> via SrClient.dll hijacking.</p><h2 id="0x01-identifying-the-vulnerability">0x01 Identifying the Vulnerability</h2><p>In this article I'm standing on the shoulders of giants. Well, I don't know how tall Clément Labro (<a href="https://twitter.com/itm4n">@itm4man</a>) is, but I found this vulnerability as a result of their discovery of the <a href="https://itm4n.github.io/windows-server-netman-dll-hijacking/">NetMan DLL Hijacking vulnerability that affects all editions of Windows Server, from 2008R2 to 2019</a>. I recently came across their excellent writeup of this issue, and decided to spin up a Windows Server VM to try and replicate their findings. I have multiple Windows VMs set up for research purposes, and it was pure chance that I picked a Server 2012 system.</p><p>After booting up, I launched <a href="https://docs.microsoft.com/en-us/sysinternals/downloads/procmon">Process Monitor</a> (procmon64.exe) and added a few filters to have it display any and all failed attempts by running processes to load a DLL or EXE file from <code>C:\Windows\System32\WindowsPowerShell\v1.0\</code>, which is part of %PATH% by default. Events that match these filters would most likely indicate that Windows was trying to load a non-existing resource by relying on the DLL search order, and would therefore represent potential DLL Hijacking / Binary planting vulnerabilities.</p><p>Initially the events field remained empty, and when I finally started getting a few results, none of them were for the <code>wlanapi.dll</code> or <code>wlanhlp.dll</code> resources, which would be evidence of the NetMan DLL vulnerability. So far this was expected, since that vulnerability is only triggered under specific circumstances. I therefore started looking into Clément Labro's exploit to trigger the flaw. After a while I glanced at my VM again, and noticed something interesting: the process <code>TiWorker.exe</code> had tried to load a resource called <code>SrClient.dll</code> from the aforementioned PowerShell directory. I inspected the event and noticed that the process was running as <code>NT AUTORITY\SYSTEM</code>.</p><figure><img src="https://blog.vonahi.io/content/images/2021/02/srclient-marked-5.png" alt="" srcset="https://blog.vonahi.io/content/images/size/w600/2021/02/srclient-marked-5.png 600w, https://blog.vonahi.io/content/images/2021/02/srclient-marked-5.png 761w" sizes="(min-width: 720px) 720px"></figure><p>Moreover, the Stack Trace of the event included references to <code>rpcrt4.dll</code>, which Clément Labro mentioned as a sign that the event was likely triggered via RPC/COM and could therefore possibly be triggered by a regular user. In line with the NetMan DLL hijacking write-up, I then launched a search query on my Windows 10 host for <code>SrClient.dll</code>, and I found it at <code>C:\Windows\System32\SrClient.dll</code></p><figure><img src="https://blog.vonahi.io/content/images/2021/02/srclient_dll_windows102.png" alt=""></figure><p>So far my findings were almost identical to those for the NetMan DLL hijacking vulnerability:</p><ul><li>A process on a Windows Server edition tried to load a non-existent DLL via the DLL Search Order</li><li>The calling process was running as <code>NT AUTORITY\SYSTEM</code></li><li>The event was likely triggered via RPC/COM</li><li>The DLL did exist on Windows 10</li></ul><p>At this point, I was starting to believe that I may have actually stumbled onto something big, but I tried to compose myself as I knew it might not be possible to actually trigger the vulnerability as a regular user. While I proceeded to look into this right away, I first want to address some discoveries I made later regarding the systems that are actually affected by this.</p><h3 id="checking-for-affected-systems">Checking for affected systems</h3><p>When I tried to replicate my findings on other Windows Server versions, I discovered that none of them seemed vulnerable. On Windows Server 2016, 2019 and to my surprise even 2012R2, <code>SrClient.dll</code> does not exist and <code>TiWorker.exe</code> will try to load it, but only from <code>C:\Windows\System32\</code>, which is the correct path in Windows 10. Because the DLL Search Order is not used, DLL hijacking to achieve privilege escalation is out of the question.</p><figure><img src="https://blog.vonahi.io/content/images/2021/02/windows_server_not_vuln_procmon.png" alt=""></figure><p>On Windows Server 2008R2 I wasn't able to trigger this event at all, so I assume that OS isn't vulnerable either. I don't have a Windows Server 2008 VM to test this on, but my guess is that it doesn't differ from 2008R2 when it comes to SrClient.dll.</p><p>Somewhat dismayed by these findings, and a little concerned about the fact that Windows Server 2012R2 didn't even seem vulnerable, I downloaded a fresh Windows Server 2012 ISO evaluation image from the <a href="https://www.microsoft.com/en-us/evalcenter/evaluate-windows-server-2012">Microsoft Evaluation Center</a> and installed all possible updates on it, including this month's Patch Tuesday rollup (<a href="https://support.microsoft.com/en-us/topic/february-9-2021-kb4601348-monthly-rollup-2c338c0c-73d6-fb80-cc91-f1a86e80db0c">KB4601348</a>). I then tried to trigger the vulnerability (which I had learned how to do, see below) and... it worked!</p><figure><img src="https://blog.vonahi.io/content/images/2021/02/update_still_vulnerable_evidence---marked.png" alt="" srcset="https://blog.vonahi.io/content/images/size/w600/2021/02/update_still_vulnerable_evidence---marked.png 600w, https://blog.vonahi.io/content/images/2021/02/update_still_vulnerable_evidence---marked.png 621w"></figure><h2 id="0x02-triggering-the-vulnerability">0x02 Triggering the Vulnerability</h2><p>I started my search for a way to trigger this vulnerability at the source: <code>TiWorker.exe</code>. I probably should have been familiar with this process, but I wasn't. Fortunately, a quick web search <a href="https://www.file.net/process/tiworker.exe.html">revealed</a> that it is part of the <em>Windows Module Installer Service</em>, the purpose of which is to download and install Windows Update packages. It resides in <code>C:\Windows\servicing</code>, just like its parent process <code>TrustedInstaller.exe</code>, which is also part of the <em>Windows Module Installer Service</em>. Because of the link with Windows Update, I decided to check if I could get <code>TiWorker.exe</code> to launch by checking for updates on my test system via the Control Panel. To my amazement, this worked right off the bat!</p><figure><img src="https://blog.vonahi.io/content/images/2021/02/windows_update-1.png" alt="" srcset="https://blog.vonahi.io/content/images/size/w600/2021/02/windows_update-1.png 600w, https://blog.vonahi.io/content/images/2021/02/windows_update-1.png 639w"></figure><p>Of course, in order to be able to exploit this in a real-world scenario, it would be far better to pull the trigger from the command line, that is CMD.exe or PowerShell. My initial search directed me toward the latter, but fortunately <a href="https://twitter.com/altonjx">Alton</a> suggested I check out <code>WUAUCLT</code> (Windows Update Automatic Update Client) and its <a href="https://www.idkrtm.com/windows-update-commands/">commands</a>, all of which can be run from CMD.exe. And sure enough, after some trial and error, I managed to trigger the vulnerability when running <code>WUAUCLT</code> with the one of the following commands:</p><ul><li><code>/SelfUpdateManaged</code> - This launches the Windows Update window in the Control Panel and tells it to start checking for updates using <a href="https://docs.microsoft.com/en-us/windows-server/administration/windows-server-update-services/get-started/windows-server-update-services-wsus">Windows Server Update Services (WSUS)</a>.</li><li><code>/SelfUpdateUnManaged</code> - Similar to the one above, but it uses the Windows Update website instead of WSUS.</li><li><code>/DetectNow</code> - This will detect and download available updates in the background.</li></ul><p>Of these possible triggers, only <code>WUAUCLT /DetectNow</code> is relatively stealthy because it will run in the background. The other two will launch the Windows Update UI, which a legitimate user would obviously notice. However, even in that scenario, users may not actually recognize this event as something malicious. Windows Update has a reputation of pushing updates in the absence of informed consent by users (mostly because the latter haven't properly configured it). As a result, some users would probably interpret the event as just another example of how capricious the update service is.</p><p>By now, I felt like jumping out of my chair with joy, but I tried to compose myself as I still needed to verify a few things, namely:</p><ul><li>Could a regular user trigger the vulnerability in the manner just described?</li><li>Would this also work for a world-writable directory that I would add to %PATH%? (I couldn't think of a reason why it shouldn't, but you never …</li></ul></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://blog.vonahi.io/srclient-dll-hijacking">https://blog.vonahi.io/srclient-dll-hijacking</a></em></p>]]>
            </description>
            <link>https://blog.vonahi.io/srclient-dll-hijacking</link>
            <guid isPermaLink="false">hacker-news-small-sites-26229098</guid>
            <pubDate>Mon, 22 Feb 2021 19:18:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Prodoscore: The Bleak Future of Work]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=26229096">thread link</a>) | @dengsauve
<br/>
February 22, 2021 | https://blog.dennissauve.com/posts/2021-02-21_prodoscore-the-bleak-future-of-work/ | <a href="https://web.archive.org/web/*/https://blog.dennissauve.com/posts/2021-02-21_prodoscore-the-bleak-future-of-work/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
        <h2>Prodoscore: The Bleak Future of Work</h2>

        <!-- Thesis -->
        <p>
            I recently took a sales pitch from Prodoscore at the behest of a colleague, and now I'm very concerned about
            the future of work. Much in the way that a poor (or even average) credit score can damage and ruin your
            chances
            at some opportunities, so too will a productivity score define your relative worth as an employee by
            reducing
            every aspect of your daily work life into a single number - your "Prodoscore". The sales pitch of course was
            presented in a light of "helping employers identify 'less productive' employees, and give transparency to
            team
            leaders on performance". At least at first.
        </p>

        <!-- Tangent/Support 1 -->
        <p>
            <em>A Credit Score for your Career</em> was the phrase that piqued my interest. I pushed the salesperson a
            bit,
            asking if the score would follow you if your next employer also used Prodoscore. She said no, but then went
            on a tangent of remembering that she'd heard a C-level saying he envisioned a more permanent Prodoscore,
            something
            that people could put on their resume. That's when I turned from curious to concerned.
        </p>

        <!-- Tangent/Support 2 -->
        <p>
            By tying into APIs from Google, SalesForce, and LinkedIn (the examples provided to me) the Prodoscore tool
            takes a granular look at your activities over the day, and then compares those against other employees and
            what they're doing. The amount of data you can see is amazing, and disturbing. The tools gives you full
            access to
            the user's sent emails, as well as other actions (I'd imagine you could get some pretty cool data using
            Slack as
            well).
        </p>

        <!-- Tangent/Support 3 (repeat as necessary) -->
        <p>
            I asked the salesperson (who was demoing her real Prodoscore used day to day) if there was any stress from
            knowing that your entire workday was exposed like that, or if there's any trepidation in showing complete
            strangers where you stack up in a company. She countered and said not only is there not much stress, but
            that
            she believes it's actually relieving stress. "At the end of the day, the score doesn't matter... it only
            matters
            when it's low" wasn't an inspiring pitch, but it illuminated the nature of these tools. If you're lagging,
            we're
            going to put you under a microscope and find out why.
        </p>

        <!-- Conclusion -->
        <p>
            Ultimately, Prodoscore still feels too immature to worry about. It's mostly a tool, for CEOs and companies
            who don't trust their employees, to spy on and dissect their employee's day second by second. Give it 5
            years.
            I hate to imagine what social media spin Prodoscore and their competition apply to this. I'm reminded of the
            Scrum bit on Silicon Valley - where Jared dupes Dinesh and Gilfoyle into competing with each other, even
            though
            they both know exactly what Jared is doing.
        </p>

        <img src="https://media.giphy.com/media/xT1XGOGdyDrL2BTfxK/source.gif">

        <p>
            Even though Dinesh and Gilfoyle both knew that Jared was only pitting them against each other to make them
            perform better for the same reward (shares in the company), they grudgingly capitulated - and even began
            to compete with each other! It doesn't take much imagination to connect the two scenarios, except on a
            global scale, the outcome could possibly be yet another barrier to employment.
        </p>

    </article><section>

    <h2>About</h2>
    <hr>

    <p>
        I'm a software developer, philanthropist, biker, cyclist, hiker, gamer, drone pilot, photo bug, and all around
        DIY enthusiast. I like to think I can cook, and enjoy a good game of PUBG/WarZone every now and then.
    </p>

    <p>
        Yell at me on <a href="https://twitter.com/dengsauve">twitter</a>,
        <a href="https://github.com/dengsauve">github</a>,
        and at <a href="https://dennissauve.com/">home</a>.
        Typically present with the handle <code>@dengsauve</code> on most sites.
    </p>

</section></div>]]>
            </description>
            <link>https://blog.dennissauve.com/posts/2021-02-21_prodoscore-the-bleak-future-of-work/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26229096</guid>
            <pubDate>Mon, 22 Feb 2021 19:17:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Codegen caching with minimal boilerplate: Protobuf Dependency Inference in Pants]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26229023">thread link</a>) | @pantsbuild
<br/>
February 22, 2021 | https://blog.pantsbuild.org/pants-2-2-adds-dependency-inference-for-protobuf/ | <a href="https://web.archive.org/web/*/https://blog.pantsbuild.org/pants-2-2-adds-dependency-inference-for-protobuf/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
				<p>As discussed <a href="https://blog.pantsbuild.org/dependency-inference/">in our post on dependency inference</a>, Pants understands which files depend on which to offer fine-grained caching. If none of the inputs have changed, Pants can safely cache your builds like running tests and generating code.</p><p>With conventional scalable build tools, this fine-grained invalidation requires substantial boilerplate: maintaining BUILD files that explicitly declare every dependency. Instead, Pants uses <em>dependency inference</em> to reduce this boilerplate by up to 90% by reading your code and figuring out the dependencies for you.</p><p>As of Pants 2.2, Pants now knows how to use dependency inference with <a href="https://developers.google.com/protocol-buffers/">Protobuf</a>! This includes:</p><ul><li>Protobuf imports of other Protobuf files.</li><li>Python imports of generated Protobuf code, including gRPC.</li></ul><p>While Pants currently only generates code with Protobuf, we are eager to work with <a href="https://www.pantsbuild.org/docs/community">community members</a> to support other protocols like Apache Thrift.</p><hr><h3 id="wth-is-pants">WTH is Pants?</h3><p>Pants is a scalable build tool, meaning that it orchestrates the tools you use in a modern Python repository, like Black, Pytest, Protoc (Protobufs), and setuptools. Pants will run these and many other tools concurrently, and brings fine-grained caching with minimal boilerplate, including as your codebase scales up in size.</p><p>See <a href="https://blog.pantsbuild.org/introducing-pants-v2/">blog.pantsbuild.org/introducing-pants-v2/</a>.</p><hr><h2 id="how-it-works">How it works</h2><p>Pants will first look at your repository's code layout and your Protobuf and Python file names to develop a global mapping. For example, we know that <code>protos/project/models.proto</code> corresponds to the Protobuf import <code>project/models.proto</code> and the Python modules <code>project.models_pb2</code> and (possibly) <code>project.models_pb2_grpc</code>.</p><p>With this global mapping computed, Pants then parses the relevant files to extract their import statements and look up the corresponding owner, if any.</p><p>For example, given this Proto:</p><pre><code>// protos/build/remote/execution/remote_execution.proto
package build.remote.execution;

import "build/semver/semver.proto";
import "google/api/annotations.proto";
import "google/rpc/status.proto";
import "google/protobuf/duration.proto";</code></pre><p>Pants infers dependencies on the correct Protobuf files:</p><pre><code>❯ ./pants dependencies protos/build/remote/execution/remote_execution.proto
protos/build/semver/semver.proto
protos/google/api/annotations.proto
protos/google/rpc/status.proto
protos/google/protobuf/duration.proto</code></pre><p>Pants will also understand Python imports of these Protobuf files, normalizing their full paths into Python module names:</p><pre><code># src/py/project/app.py
import build.semver.semver_pb2
import google.api.annotations_pb2_grpc</code></pre><pre><code>❯ ./pants dependencies src/py/project/app.py
protos/build/semver/semver.proto
protos/google/api/annotations.proto</code></pre><p>As discussed in <a href="https://blog.pantsbuild.org/fast-incremental-builds-speculation-cancellation/">our post on Pants's performance</a>, this inference is 1) very safe and 2) very fast. Because Pants invokes processes hermetically with a sandbox, failing to infer a dependency can never cause the wrong thing to be cached. Further, the inference is fast thanks to Pants's core being implemented in Rust, along with a daemon, parallelism, and very fine-grained invalidation.</p><h2 id="trying-out-pants">Trying out Pants</h2><p>Using Pants ensures that your builds always use your up-to-date Protobuf code—no more need to manually invoke scripts! Further, thanks to Pants's fine-grained understanding of your project's dependencies, you will only ever generate the Protobuf files you actually need.</p><p>We optimized Pants to be easy to <a href="https://www.pantsbuild.org/docs/existing-repositories">add incrementally to existing repositories</a>, including an upcoming feature in Pants 2.3 to auto-generate BUILD files (stay tuned for a blog post!).</p><p>The <a href="https://www.pantsbuild.org/docs/community">Pants community</a> would love to help you get started. <a href="https://www.pantsbuild.org/v2.1/docs">www.pantsbuild.org/docs</a><br></p>
			</section></div>]]>
            </description>
            <link>https://blog.pantsbuild.org/pants-2-2-adds-dependency-inference-for-protobuf/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26229023</guid>
            <pubDate>Mon, 22 Feb 2021 19:12:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why 'central cloud teams' fail]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26229021">thread link</a>) | @forrestbrazeal
<br/>
February 22, 2021 | https://acloudguru.com/blog/engineering/why-central-cloud-teams-fail-and-how-to-save-yours | <a href="https://web.archive.org/web/*/https://acloudguru.com/blog/engineering/why-central-cloud-teams-fail-and-how-to-save-yours">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-id="8a920a5" data-element_type="widget" data-widget_type="theme-post-content.default"><div><p>Early in my cloud career, I joined a “central cloud team” for a big enterprise. We created the automation, infrastructure, and standards for dozens of product teams who were <a href="https://acloudguru.com/blog/business/what-is-cloud-migration" target="_blank" rel="noopener noreferrer">migrating</a> to AWS. Anybody who wanted to get to the cloud had to come through us, the experts. (This was back before the release of AWS Organizations, when large, shared AWS accounts were the norm at many shops).</p><p>Here’s the thing: we were <i>good</i>. I thought we didn’t need formal cloud training because we could just look up documentation to figure out new tools and services. In fact, we felt a little superior when we heard about other teams asking for special training to use the cloud. We were high-flying 10x engineers! We didn’t need all that!</p><p>Fast forward several years, though, and <strong>nearly every engineer on that team has left the company, burned out and disillusioned by their job</strong>. Worse, many of the cloud-native efforts we championed fell by the wayside. And having now worked in-house and as a consultant for many enterprises, I have seen this pattern over and over again, as up to <a href="https://searchcio.techtarget.com/feature/Cloud-migration-failures-and-how-to-prevent-them" target="_blank" rel="nofollow noopener noreferrer">three-quarters of cloud migrations fail</a> despite high initial expectations.</p><p>So what went wrong?</p><h3 id="h-why-central-cloud-teams-fail">Why central cloud teams fail</h3><p>Central cloud teams, often established at the beginning of an organization’s cloud transformation, have three common characteristics:</p><ul><li>They’re <strong>small </strong>relative to the rest of the IT org — often fewer than ten people. Cloud experts are hard to find!</li><li>They’re <strong>high-performing</strong>, the kind of engineers who can build quickly and learn by themselves. After all, that’s why they gravitated toward roles where they are constantly exposed to cutting-edge services and features.</li><li>They operate with a high degree of <strong>autonomy</strong>. They have leeway to try new services and set best practices for the cloud organization. They often have a visionary “executive sponsor” running interference for them as well.</li></ul><p>These teams, like mine did, think they are adhering to the <a href="https://aws.amazon.com/blogs/enterprise-strategy/using-a-cloud-center-of-excellence-ccoe-to-transform-the-entire-enterprise/" target="_blank" rel="nofollow noopener noreferrer">Cloud Center of Excellence mindset</a> by setting themselves up as the “cloud gatekeepers” for their organization.</p><p>And in doing so, they may create a bottleneck for the entire company’s cloud adoption efforts — the exact opposite of their goal. That’s because centralized cloud teams <i>do not scale.</i></p><h3 id="h-the-scourge-of-support-tickets">The scourge of support tickets</h3><p>I don’t care how good at the cloud you personally are — nobody cannot survive long-term as the sole repository of cloud knowledge for their entire organization.</p><p>Oh, it’ll be okay at first. You get to work on lots of cool stuff. And it feels good to be the cloud experts that others come to for help.</p><p>But when more teams start trying to adopt the cloud and use your guidelines, you’ll get absolutely clobbered by every engineer’s worst nightmare: <strong>support tickets</strong>. On that first job I was telling you about, every engineer on my team was eventually spending one to two scheduled days a week just triaging cries for help in Jira. Hundreds of cloud newbies who didn’t know EC2 from S3, who couldn’t figure out how to use the AWS CLI, were consuming up to 50% of our brightest (and best-paid) engineers’ time.</p><div><figure><img loading="lazy" width="1784" height="953" src="https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2020/06/blob.png" alt="Reality vs Expectation" srcset="https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2020/06/blob.png 1784w, https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2020/06/blob.png 300w, https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2020/06/blob.png 1024w, https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2020/06/blob.png 768w, https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2020/06/blob.png 1536w" sizes="(max-width: 1784px) 100vw, 1784px"></figure></div><p>And as the central cloud team’s time and energy to handle support requests decreases, it has a cascading effect on the broader team’s ability to succeed.</p><p>If you’re looking for a shorthand metric to gauge the success of your cloud transformation, try <i>support ticket volume. </i>That’s why your best engineers get burned out and leave. It’s why cloud migrations stall. It’s a leading indicator of how successful your central team (the experts) have been at transferring their knowledge to the organization as a whole.</p><p>To paraphrase the great cloud architect William Butler Yeats: things fall apart, the central cloud team cannot hold.</p><h3 id="h-the-solution-comprehensive-fluency">The solution: comprehensive fluency</h3><p>The only way I know of to save the central cloud team’s sanity and speed up cloud adoption is <a href="https://info.acloud.guru/resources/how-many-cloud-training" target="_blank" rel="nofollow noopener noreferrer"><i>comprehensive fluency</i></a><i>.&nbsp;</i></p><p>That’s right: your goal should be for everybody who builds stuff headed for the cloud, central experts or otherwise, to be able to speak cloud like it’s their native language. And I’ve got years of failures in my past to attest: that doesn’t just happen.</p><p>If central cloud teams start off small, high-performing, and autonomous, the rest of your organization is large, slow-moving, and interdependent. Everyone has legacy technologies to worry about. Cloud fluency doesn’t happen unless you make it happen.</p><p>And that’s where the central cloud team can finally shine.</p><h3 id="h-enabling-fluency">Enabling fluency</h3><p>Like any other organism, your cloud center of excellence will die unless it can figure out how to reproduce. Fundamentally, the central cloud team has to change their mission from <i>expertise </i>(knowing the most about cloud) to <i>enablement </i>(creating more people who know about cloud)<i>.&nbsp;</i></p><p><strong>Embed experts with product teams</strong></p><p>You wouldn’t put all your servers in one availability zone, so don’t put all your knowledge on the central cloud team.</p><p>Set up a rotation that assigns your central cloud experts directly to the product teams that need them most. Through daily standups, code reviews, and pair programming, they can help raise the maturity of your teams in real time.</p><p>The USA used the same tactic to win the air war in World War II: they <a href="https://books.google.com/books?id" target="_blank" rel="noopener noreferrer">sent their best pilots home to train others</a>, while the Germans kept their aces on the front lines until they were shot down.</p><p>When one team reaches an acceptable baseline of fluency, you can reassign the expert, like a flight instructor moving on to a new crop of recruits. Channeling expertise into enablement keeps your entire team in fighting shape.</p><div><figure><img loading="lazy" width="2144" height="2028" src="https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2020/06/blob-1.png" alt="comic - Are you investing in cloud translation" srcset="https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2020/06/blob-1.png 2144w, https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2020/06/blob-1.png 300w, https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2020/06/blob-1.png 1024w, https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2020/06/blob-1.png 768w, https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2020/06/blob-1.png 1536w, https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2020/06/blob-1.png 2048w" sizes="(max-width: 2144px) 100vw, 2144px"></figure></div><p><strong>Create baselines of cloud fluency</strong></p><p>The central cloud team’s expertise is still essential to define that fluency baseline, so lay out your security, automation, and tooling standards in a place where everyone can access them.</p><p>You should also identify the cloud services everyone is expected to understand. If you’re in AWS, a good shortlist would likely include foundational services like IAM, VPC, and CloudFormation.</p><p>Many teams I’ve worked with require product teams to demonstrate their grasp of these baselines before they get access to their own cloud environments. But you need more validation than just a “thumbs up, I read the docs”. That’s where certification comes in.</p><p><strong>Provide clear training and certification paths</strong></p><p>Many ACG customers assign a foundational course like the AWS Certified Solutions Architect Associate course as part of onboarding for all new engineering hires. Whether or not everyone goes on to sit the certification exam, working through the course material creates a “lingua franca” for the cloud — a shared language everybody speaks.</p><p>But beyond the shared baseline, it’s also a good idea to match the right training to the right roles. Architects and developers need different expertise than data engineers or infosec. Look at something like ACG’s Learning Paths to lay out standardized plans that fit a variety of competencies.</p><p><strong>Establish support and incentives</strong></p><p>People learn best when they are motivated to succeed. So establish study groups and Slack channels to support your learners. Lean into goofy ceremonies to congratulate people who achieve the big cert. (A Cloud Guru has a “Wall of Fame” in our Austin office where we post Polaroids of every employee who achieves a certification.) Positive peer pressure works wonders!</p><p>The best incentive, though, is the cloud itself. You hired good people, they want to get work done in the cloud. Set the prerequisites for them to demonstrate competence, and they’ll rise to the challenge — <a href="https://info.acloud.guru/resources/value-cloud-certifications" target="_blank" rel="nofollow noopener noreferrer">93% of IT managers find</a> that certified employees provide value above and beyond the cost of skilling up.</p><hr><div><figure><a href="https://go.acloudguru.com/value-of-cloud-certifications-ebook"><img loading="lazy" width="262" height="243" src="https://res.cloudinary.com/acloud-guru/image/fetch/c_thumb,f_auto,q_auto/https://acg-wordpress-content-production.s3.us-west-2.amazonaws.com/app/uploads/2020/06/why-cloud-certs-banners.png" alt="Book - Why Cloud Certs? Why Now?"></a></figure></div><h2 id="h-why-cloud-certs-why-now"><strong><a href="https://go.acloudguru.com/value-of-cloud-certifications-ebook" target="_blank" rel="noreferrer noopener">Why cloud certs, why now?</a></strong></h2><p>Download <i><a href="https://go.acloudguru.com/value-of-cloud-certifications-ebook" target="_blank" rel="noreferrer noopener">Why Cloud Certs, Why Now?</a></i> to discover the many ways cloud certifications create value for your business. <a href="https://go.acloudguru.com/value-of-cloud-certifications-ebook" target="_blank" rel="noreferrer noopener">Get the goods!</a></p><hr><h3 id="h-the-surprising-conclusion">The surprising conclusion</h3><p>The most surprising thing I’ve learned about cloud adoption in the last ten years? <strong>Your experts need training just as much as anyone else.</strong> Not so much for their own information, but for the teams they are guiding to the cloud. It’s the only way to protect their sanity and scale your cloud adoption.</p><p>To quote another cloud engineer, Robert Frost: some say the world will end in fire, some say in ice. But without comprehensive fluency, the central cloud team is sure to drown in support requests.</p><p>Take my word for it. I think I’m still assigned a Jira ticket from 2015 that just says “need help with AWS.”</p><p><i>Forrest Brazeal is an AWS Serverless Hero</i> <i>and enterprise architect who has led cloud adoption initiatives for companies ranging from startups to the Fortune 50.</i></p><hr><p><em>ACG for Business now includes <a href="https://acloud.guru/cloud-playground" target="_blank" rel="noopener noreferrer">Cloud Playground</a>: fast, fresh, throwaway cloud environments so your team can learn by doing.</em></p></div></div></div>]]>
            </description>
            <link>https://acloudguru.com/blog/engineering/why-central-cloud-teams-fail-and-how-to-save-yours</link>
            <guid isPermaLink="false">hacker-news-small-sites-26229021</guid>
            <pubDate>Mon, 22 Feb 2021 19:12:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[(Texas) – Griddy: Why energy prices were sky high this week]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26228930">thread link</a>) | @rluhar
<br/>
February 22, 2021 | https://www.griddy.com/post/griddy-update-why-energy-prices-were-sky-high-this-week | <a href="https://web.archive.org/web/*/https://www.griddy.com/post/griddy-update-why-energy-prices-were-sky-high-this-week">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><p>At Griddy, transparency has always been our goal. We know you are angry and so are we. Pissed, in fact. Here’s what’s been going down: </p><p>On Monday evening the Public Utility Commission of Texas (PUCT) cited its “<a href="http://www.puc.texas.gov/51617WinterERCOTOrder.pdf" target="_blank">complete authority over ERCOT</a>” to direct that ERCOT set pricing at $9/kWh until the grid could manage the outage situation after being ravaged by the freezing winter storm. &nbsp;</p><p>Under ERCOT's market rules, such a pricing scenario is only enforced when available generation is about to run out (they usually leave a cushion of around 1,000 MW). This is the energy market that Griddy was designed for – one that allows consumers the ability to plan their usage based on the highs and lows of wholesale energy and shift their usage to the cheapest time periods. &nbsp;</p><p>However, the PUCT changed the rules on Monday. &nbsp;</p><p>As of today (Thursday), 99% of homes have their power restored and available generation was well above the 1,000 MW cushion. Yet, the PUCT left the directive in place and continued to force prices to $9/kWh, approximately 300x higher than the normal wholesale price. For a home that uses 2,000 kWh per month, prices at $9/kWh work out to over $640 per day in energy charges. By comparison, that same household would typically pay $2 per day. &nbsp;</p><p>See (below) the difference between the price set by the market's supply-and-demand conditions and the price set by the PUCT's “complete authority over ERCOT.” The PUCT used their authority to ensure a $9/kWh price for generation when the market's true supply and demand conditions called for far less. Why? &nbsp;</p><figure><p><img src="https://assets-global.website-files.com/5df01ca286f5a984f50cd9e2/602f3547d3cfcdac85d1087a_Griddy%20vs%20PUCT%20real-time%20prices.jpg" loading="lazy" alt=""></p></figure><p>The CEO of a fellow innovative retailer shared his distress with the PUCT <a href="http://www.energychoicematters.com/stories/20210218aa.html" target="_blank">here</a>. “Customers blame ERCOT, PUC, TDSPs, and retailers. The one entity that they don’t blame are the generators because they don’t have a face to the customer but they make all the money in these types of events. If you follow the money you will find that generators make all the money.” </p><p>That’s one explanation. Everyone is still trying to figure that out. But here is what we do know: </p><p>The market is supposed to set the prices, not political appointees. </p><p>And here is what we are going to do: </p><p>We intend to fight this for, and alongside, our customers for equity and accountability – to reveal why such price increases were allowed to happen as millions of Texans went without power. </p><p>More to come. &nbsp;</p><p>‍</p></div></div></div>]]>
            </description>
            <link>https://www.griddy.com/post/griddy-update-why-energy-prices-were-sky-high-this-week</link>
            <guid isPermaLink="false">hacker-news-small-sites-26228930</guid>
            <pubDate>Mon, 22 Feb 2021 19:06:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Questions to ask when choosing a programming language]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26228886">thread link</a>) | @feross
<br/>
February 22, 2021 | https://shekhargulati.com/2021/02/12/questions-to-ask-when-choosing-a-programming-language/ | <a href="https://web.archive.org/web/*/https://shekhargulati.com/2021/02/12/questions-to-ask-when-choosing-a-programming-language/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-6502">
	<!-- .entry-header -->

	
	
	<div>
		
<p>This week I had a discussion with one of my friends on how to choose a programming language. It was triggered by multiple discussions I had with our customers on their engineering strategy in the last six months and one question that came multiple times was should we use X programming language for our new initiatives. Some customers were thinking of moving from .NET stack to Java, some banks were thinking about moving to Golang because their technical leaders have watched Monzo talks on Golang, for some it was from Java to Kotlin, and some were thinking of dumping JavaScript for Typescript.</p>



<p>To come up with the answer I try to find answers to following questions in context of the organization:</p>



<ul><li>What is the maturity of the programming language with respect to its community and ecosystem? Should they spend their one innovation token on this language?</li><li>How easy it is to find available talent in the market for that programming language?&nbsp;</li><li>How easy it is for the organization to acquire production engineering know-how for a programming language?</li><li>What are the productivity and efficiency gains that can be achieved from using a programming language? Are those gains aligned with the organization goals?</li><li>What are the use-cases an organization wants to solve with the programming language?</li><li>What is the future of a programming language? For a big enterprise it is important if the language can last for a decade.</li><li>What is the learning curve of the programming language? Can existing staff be upskilled?</li></ul>



<p>There is no correct answer to these questions. Most engineering organizations will end up using multiple programming languages. For example an organization may choose Golang as a general purpose language to build backend services, Python for scripting and data related work, Typescript for building web frontend. It is also possible that an engineering organization might choose Python for building most services and for few where performance and efficiency is important it chooses Golang. I think the important point is defining a small list of programming languages for the organization and documenting when you will choose which programming language.&nbsp;</p>



<p>I use a decision matrix like the one shown below to come up with one possible answer. Depending on which factors are important to the organization they can give them weights and that will impact the score of the language. In the image shown below, language 1 is the winner.</p>



<figure><img src="https://lh6.googleusercontent.com/WGB2gGPDUN24IZXig5kZSGzKwcpwSIfP5dzPMp5gxTpIGw5g5rRlqcsWPSTYDY0QWf6U0D14oL9dfTeRZlDanEwK54XjJIirmMeIGTL62skChi8YxVTBZcpeJyRbnveFCKYW3d7U" alt=""></figure>



<p>As I was writing this post a few more questions came to my mind.&nbsp;</p>



<ul><li>Does a programming language help us write less buggy software?</li><li>Does a programming language have some constructs that help us reduce the essential complexity of the system?</li><li>What constraints does a programming language impose and how do they impact the business goals?</li><li>Can a programming language be a competitive advantage for an organization?</li><li>Can a programming language influence the quality of the development team, the quality of code, and practices they follow?</li><li>Does a programming language influence engineering organization culture?</li><li>Can a programming language over time help average Joe become a good software engineer?</li><li>What makes a programming language future safe? Can we predict it to safeguard us?</li><li>Should a language choice depend on NFRs that you want to achieve?</li><li>How does a programming language influence behavior of a team?</li></ul>



<p>I don’t have answers to all of the above mentioned questions. I am hoping there is academic research done on the above but I am yet to read those papers.</p>



<p>I did some research on why different organizations choose certain languages and I found the following key points.</p>



<ol><li>Gitlab – Ruby – <a href="https://about.gitlab.com/blog/2018/10/29/why-we-use-rails-to-build-gitlab/">Link</a><ul><li>GitHub, a source of inspiration for GitLab, was also based on Rails, making it a logical pick considering his interest in the framework.</li><li>Ruby on Rails ecosystem allows you to shape a lot of functionality at a high quality</li><li>We need a lot of functionality and Ruby on Rails is a way to do it</li><li>Consistent coding practices. You are guided to do the right thing.</li><li>Big community of Ruby gems</li></ul></li><li>CockroachDB – Golang – <a href="https://www.cockroachlabs.com/blog/why-go-was-the-right-choice-for-cockroachdb/">Link</a><ul><li>its support for libraries, interfaces, and tooling positioned it as the right choice for CockroachDB</li><li>Go was designed to scale to large code bases with an emphasis on simplicity and orthogonality of features. The enforced code style, the simple imports and automated import management, the wide variety of linters, the straightforward (and minimal) set of programmatic idioms…all of these attributes of Go are important for clean, understandable code.</li><li>When comparing to Java, we appreciate the tight focus on implementation instead of OOP and abstraction: interfaces can be added when needed, not as an initial, often unnecessary, step.&nbsp;</li><li>When comparing to C++, we appreciate automatic memory management and how there’s rarely more than one way to get something done, for example with static and one-time initializers.</li><li>Go gives better control over memory allocation that impacts garbage collection.</li></ul></li><li>Asana – TypeScript – <a href="https://blog.asana.com/2014/11/asana-switching-typescript/">Link</a><ul><li>Clean JS</li><li>Community Support</li><li>Errors at compile time instead of runtime</li><li>Static typing</li></ul></li><li>American Express – Golang – <a href="https://go.dev/solutions/americanexpress/">Link</a> and <a href="https://americanexpress.io/choosing-go/">Link</a><ul><li>For their assessment, they chose to build a microservice in four different programming languages. They then compared the four languages for speed/performance, tooling, testing, and ease of development.</li><li>While Go may not have been the fastest language tested, its powerful tooling helped bolster its overall results. Go’s built-in testing framework, profiling capabilities, and benchmarking tools impressed the team.</li><li>Reasons<ul><li>Simple and straightforward</li><li>Encourage best practices</li><li>Concurrency</li><li>Tooling</li></ul></li></ul></li><li>Nubank – Clojure – <a href="https://building.nubank.com.br/working-with-clojure-at-nubank/">Link</a><ul><li>Nubank provides services in the finance domain, which is very close to mathematical functions — and functional programming is an excellent fit for both scenarios.</li><li>Clojure, on the other hand, has simple constructs that allow us to focus on the problem we are solving, making evolving the system a small incremental challenge, which doesn’t get that much harder over time.</li><li>Most of our codebase can be understood locally, looking at any given pure function, understanding its outputs for any given set of inputs. There’s rarely any need to reason about or recreate the internal state of objects. Data moves through the system in a composable, inspectable, consistent, and immutable way (without hiding it inside of objects).</li><li>Functional code is much easier to test, and that gives us the confidence to deploy an average of over 50 changes per day in a mission-critical domain.</li><li>Nubank has acquired Cognitect, the US-based software consultancy behind the Clojure programming language and the Datomic database</li></ul></li><li>Janestreet – Ocaml – <a href="https://www.youtube.com/watch?v=v1CmGbOGb2I">Link</a> , <a href="https://queue.acm.org/detail.cfm?id=2038036">Link</a> , and <a href="https://discuss.ocaml.org/t/does-jane-street-use-other-programming-languages-aside-from-ocaml/2761/5">Link</a><ul><li>Brevity of the language and the powerful type system that makes OCaml code very readable</li><li>Powerful abstraction capabilities that reduce boilerplates</li><li>Static type system for ensuring code correctness</li><li>He spoke about some of the fancy type tricks like parametric polymorphism, algebraic data types, type inference, phantom types and type indexed values that add to the expressivity of code.</li><li>Also OCaml hits the sweet spot between expressiveness of code and the performance numbers. The very much tunable GC makes things easier to control.</li></ul></li><li>Starling Bank – Java – <a href="https://www.infoq.com/presentations/starling-bank/">Link</a><ul><li>Exceptions are noisy and difficult to ignore</li><li>Reliable ecosystem (user base, tooling, job market, etc)&nbsp;</li><li>Integrations with legacy third parties (SOAP etc)</li></ul></li><li>KhanAcademy – Golang – <a href="https://blog.khanacademy.org/go-services-one-goliath-project/">Link</a><ul><li>Kotlin was more performant</li><li>Golang used much less memory</li></ul></li><li>Lyft – TypeScript – <a href="https://eng.lyft.com/typescript-at-lyft-64f0702346ea">Link</a><ul><li>Popularity</li><li>Type safety</li><li>Less Bugs</li><li>Productivity</li></ul></li><li>Medium – Golang – <a href="https://medium.engineering/rex-mediums-go-recommendation-microservice-e077bc9582a">Link</a><ul><li>More efficient use of the CPU. While Node is single-threaded, Go is much better suited for the combination of I/O and CPU-intensive operations required to build a ranked feed. Splitting our work onto separate Goroutines means we can avoid the issue of the CPU getting hogged by one single request and other requests getting starved.</li><li>Opinionated. Go makes it pretty hard to write “bad” code. A typed language that is also highly opinionated in terms of code styling means that even a newbie to Go (which I was when we started writing Rex) can quickly start writing clean and readable code.</li><li>Prior experience with Go. While much of Medium’s codebase is written in Node, we already had a few smaller-purpose microservices in Go. Adding another microservice in a language that we as a company have familiarity with makes building and maintaining this new service much easier.</li></ul></li><li>Instagram – Python – <a href="https://instagram-engineering.com/web-service-efficiency-at-instagram-with-python-4976d078e366">Link</a><ul><li>Simplicity</li><li>Practicality</li></ul></li></ol>



<p>I hope this post helps you understand that there is much more to choosing a programming language.</p>



<h5>You can also support me and my work by&nbsp;following me on  <a rel="noreferrer noopener" href="https://softwareleadweekly.us6.list-manage.com/track/click?u=1a258e0fefbb23214c59c5a8d&amp;id=278bdedb31&amp;e=219517c74f" target="_blank"></a><a rel="noreferrer noopener" href="https://twitter.com/shekhargulati" target="_blank">https://twitter.com/shekhargulati</a>.&nbsp;Thank you&nbsp;</h5>



<hr>




	</div><!-- .entry-content -->

	<!-- .entry-footer -->
</article></div>]]>
            </description>
            <link>https://shekhargulati.com/2021/02/12/questions-to-ask-when-choosing-a-programming-language/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26228886</guid>
            <pubDate>Mon, 22 Feb 2021 19:04:00 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[People Who Wear Spectacles Are About Three Times Less Likely to Catch Covid-19]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 1 (<a href="https://news.ycombinator.com/item?id=26228823">thread link</a>) | @throwawaysea
<br/>
February 22, 2021 | https://www.ibtimes.sg/people-who-wear-spectacles-are-about-three-times-less-likely-catch-covid-19-finds-study-55744 | <a href="https://web.archive.org/web/*/https://www.ibtimes.sg/people-who-wear-spectacles-are-about-three-times-less-likely-catch-covid-19-finds-study-55744">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody" id="v_main"><p>A new study has revealed that people who wear glasses are up to three times less likely to catch novel Coronavirus infections. It was found that the eye protection was "statistically significant" to fight against the SARS-CoV-2 caused disease, COVID-19.</p><p>The study, which was also conducted in India, showed that poor and uneducated people were more likely to contract the novel Coronavirus. According to the research, this was because "they do not follow the preventive guidelines properly" and useless spectacles than the educated people.</p>
<figure itemscope="" itemprop="associatedMedia image" itemtype="https://schema.org/ImageObject"><div>
<picture>
<!--[if IE 9]><video
style="display: none;"><![endif]--><source media="(min-width: 1280px)" sizes="640px" srcset="https://data.ibtimes.sg/en/full/47011/spectacles.jpg?w=640 640w"><source media="(min-width: 1024px)" sizes="640px" srcset="https://data.ibtimes.sg/en/full/47011/spectacles.jpg?w=640 640w"><source media="(min-width: 768px)" sizes="640px" srcset="https://data.ibtimes.sg/en/full/47011/spectacles.jpg?w=640 640w"><source media="(min-width: 480px)" sizes="480px" srcset="https://data.ibtimes.sg/en/full/47011/spectacles.jpg?w=480 480w"><source media="(min-width: 0px)" sizes="400px" srcset="https://data.ibtimes.sg/en/full/47011/spectacles.jpg?w=400 400w"><!--[if IE 9]></video><![endif]-->                    <img id="i47011" src="https://data.ibtimes.sg/en/full/47011/spectacles.jpg?w=640" alt="Spectacles " title="Spectacles " width="640" itemprop="contentUrl">
</picture><meta itemprop="url" content="https://data.ibtimes.sg/en/full/47011/spectacles.jpg"><meta itemprop="width" content="640"><meta itemprop="height" content="427">
<figcaption>
<span itemprop="caption">Glasses wearers up to three times less likely to catch Covid</span>
<span itemprop="copyrightHolder">Pixabay</span>
</figcaption></div>
</figure><h3><strong>Spectacles and COVID-19</strong></h3><p>The research head, Amit Kumar Saxena, said the new study showed that the risk of <a href="https://www.ibtimes.sg/new-traffic-signal-lookalike-technology-could-help-reopen-international-airports-safely-55731" target="_blank">COVID-19</a> was two to three times less in spectacles wearing population while compared to those who do not wear glasses.</p><p>"Protective role of the spectacles was found statistically significant if those were used for a long period of the day. Touching and rubbing of the eyes with contaminated hands may be a significant route of infection," added Saxena.</p><p>During the study, it was also found that people touch their face on average 23 times in an hour and the eyes three times per hour. "Transmission occurs by touching the face, nose, mouth and eyes. Touching one's nose and mouth is significantly reduced when wearing a face mask properly. But wearing a face mask does not protect the eyes," said the study.</p><h3><strong>The COVID-19 Research</strong></h3>
<figure itemscope="" itemprop="associatedMedia image" itemtype="https://schema.org/ImageObject"><div>
<picture>
<!--[if IE 9]><video
style="display: none;"><![endif]--><source media="(min-width: 1280px)" sizes="640px" srcset="https://data.ibtimes.sg/en/full/45807/coronavirus.jpg?w=640 640w"><source media="(min-width: 1024px)" sizes="640px" srcset="https://data.ibtimes.sg/en/full/45807/coronavirus.jpg?w=640 640w"><source media="(min-width: 768px)" sizes="640px" srcset="https://data.ibtimes.sg/en/full/45807/coronavirus.jpg?w=640 640w"><source media="(min-width: 480px)" sizes="480px" srcset="https://data.ibtimes.sg/en/full/45807/coronavirus.jpg?w=480 480w"><source media="(min-width: 0px)" sizes="400px" srcset="https://data.ibtimes.sg/en/full/45807/coronavirus.jpg?w=400 400w"><!--[if IE 9]></video><![endif]-->                    <img id="i45807" src="https://data.ibtimes.sg/en/full/45807/coronavirus.jpg?w=640" alt="Coronavirus " title="Coronavirus " width="640" itemprop="contentUrl">
</picture><meta itemprop="url" content="https://data.ibtimes.sg/en/full/45807/coronavirus.jpg"><meta itemprop="width" content="640"><meta itemprop="height" content="360">
<figcaption>
<span itemprop="caption">Novel Coronavirus infection</span>
<span itemprop="copyrightHolder">Pixabay</span>
</figcaption></div>
</figure><p>The study, which was published in <a href="https://www.medrxiv.org/content/10.1101/2021.02.12.21249710v1" rel="nofollow" target="_blank">medRxiv</a>, included 304 Coronavirus patients. Their glasses-wearing behavior was assessed through a questionnaire. The answers were compared with existing studies of the general population.</p><p>As per the findings of the study, a total of 58 patients showed the behavior of using glasses continuously during the daytime and always on outdoor activities. The risk of Coronavirus infection was found 0.48 in spectacles wearing population as compared to 1.35 in the population not using them.</p><p>"The calculated risk ratio was 0.36. The protective effects of the spectacles were found statistically significant," said the study.</p><p>However, based on the findings it would be ideal for the healthcare workers to use face shields and wear goggles to protect their eyes while treating a <a href="https://www.ibtimes.sg/john-hopkins-expert-predicts-end-coronavirus-sufferings-by-april-us-55730" target="_blank">COVID-19 patient</a>. Scientists also said that wearing glasses does not protect the eyes as much as googles but it could provide some sort of protection.</p></div></div>]]>
            </description>
            <link>https://www.ibtimes.sg/people-who-wear-spectacles-are-about-three-times-less-likely-catch-covid-19-finds-study-55744</link>
            <guid isPermaLink="false">hacker-news-small-sites-26228823</guid>
            <pubDate>Mon, 22 Feb 2021 18:59:53 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[ReScript 9.0]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26228682">thread link</a>) | @jesperlang
<br/>
February 22, 2021 | https://rescript-lang.org/blog/release-9-0 | <a href="https://web.archive.org/web/*/https://rescript-lang.org/blog/release-9-0">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><h2>Introduction<span><span><a href="#introduction"><svg height="0.8em" width="0.8em" viewBox="0 0 20.003 19.944"><path d="M11.927 7.908a4.819 4.819 0 00-3.968-1.3 5.091 5.091 0 00-2.921 1.508L1.47 11.684a4.82 4.82 0 00.192 7.122 4.994 4.994 0 006.76-.4l3.7-3.776a.109.109 0 00-.067-.184s-.649.029-1.132.006a10.116 10.116 0 01-1.35-.226.308.308 0 00-.243.088l-2.529 2.609a2.733 2.733 0 01-3.583.319 2.64 2.64 0 01-.247-3.951l3.755-3.753a2.7 2.7 0 013.654-.073.108.108 0 00.15 0l1.4-1.4a.114.114 0 00-.003-.157z"></path><path d="M8.076 12.036a4.822 4.822 0 003.967 1.3 5.089 5.089 0 002.922-1.509l3.568-3.568a4.818 4.818 0 00-.192-7.121 5 5 0 00-6.761.4l-3.7 3.777a.108.108 0 00.067.183s.648-.028 1.132-.006a10.151 10.151 0 011.35.226.3.3 0 00.243-.088l2.529-2.608a2.732 2.732 0 013.581-.319 2.638 2.638 0 01.249 3.95l-3.755 3.754a2.706 2.706 0 01-3.654.073.107.107 0 00-.15 0l-1.4 1.4a.113.113 0 00.004.156z"></path></svg></a><a id="introduction"></a></span></span></h2><p>We are happy to announce ReScript 9.0!</p><p>ReScript is a robustly typed language that compiles to efficient and human-readable JavaScript. It comes with one of the fastest build toolchains and offers first class support for interoperating with ReactJS and other existing JavaScript code.</p><p>Use <code>npm</code> to install the newest <a href="https://www.npmjs.com/package/bs-platform/v/9.0.1" rel="noopener noreferrer">9.0.1 release</a> with the following command:</p><pre><div><div><p><code>npm install bs-platform@9.0.1 --save-dev
</code></p></div></div></pre><p>You can also try our new release in the <a rel="noopener noreferrer" href="https://rescript-lang.org/try">Online Playground</a>.</p><p>In this post we will highlight the most notable changes. The full changelog for this release can be found <a href="https://github.com/rescript-lang/rescript-compiler/blob/master/Changes.md#90" rel="noopener noreferrer">here</a>. </p><h2>Compiler Improvements<span><span><a href="#compiler-improvements"><svg height="0.8em" width="0.8em" viewBox="0 0 20.003 19.944"><path d="M11.927 7.908a4.819 4.819 0 00-3.968-1.3 5.091 5.091 0 00-2.921 1.508L1.47 11.684a4.82 4.82 0 00.192 7.122 4.994 4.994 0 006.76-.4l3.7-3.776a.109.109 0 00-.067-.184s-.649.029-1.132.006a10.116 10.116 0 01-1.35-.226.308.308 0 00-.243.088l-2.529 2.609a2.733 2.733 0 01-3.583.319 2.64 2.64 0 01-.247-3.951l3.755-3.753a2.7 2.7 0 013.654-.073.108.108 0 00.15 0l1.4-1.4a.114.114 0 00-.003-.157z"></path><path d="M8.076 12.036a4.822 4.822 0 003.967 1.3 5.089 5.089 0 002.922-1.509l3.568-3.568a4.818 4.818 0 00-.192-7.121 5 5 0 00-6.761.4l-3.7 3.777a.108.108 0 00.067.183s.648-.028 1.132-.006a10.151 10.151 0 011.35.226.3.3 0 00.243-.088l2.529-2.608a2.732 2.732 0 013.581-.319 2.638 2.638 0 01.249 3.95l-3.755 3.754a2.706 2.706 0 01-3.654.073.107.107 0 00-.15 0l-1.4 1.4a.113.113 0 00.004.156z"></path></svg></a><a id="compiler-improvements"></a></span></span></h2><h3>New External Stdlib Configuration<span><span><a href="#new-external-stdlib-configuration"><svg height="0.8em" width="0.8em" viewBox="0 0 20.003 19.944"><path d="M11.927 7.908a4.819 4.819 0 00-3.968-1.3 5.091 5.091 0 00-2.921 1.508L1.47 11.684a4.82 4.82 0 00.192 7.122 4.994 4.994 0 006.76-.4l3.7-3.776a.109.109 0 00-.067-.184s-.649.029-1.132.006a10.116 10.116 0 01-1.35-.226.308.308 0 00-.243.088l-2.529 2.609a2.733 2.733 0 01-3.583.319 2.64 2.64 0 01-.247-3.951l3.755-3.753a2.7 2.7 0 013.654-.073.108.108 0 00.15 0l1.4-1.4a.114.114 0 00-.003-.157z"></path><path d="M8.076 12.036a4.822 4.822 0 003.967 1.3 5.089 5.089 0 002.922-1.509l3.568-3.568a4.818 4.818 0 00-.192-7.121 5 5 0 00-6.761.4l-3.7 3.777a.108.108 0 00.067.183s.648-.028 1.132-.006a10.151 10.151 0 011.35.226.3.3 0 00.243-.088l2.529-2.608a2.732 2.732 0 013.581-.319 2.638 2.638 0 01.249 3.95l-3.755 3.754a2.706 2.706 0 01-3.654.073.107.107 0 00-.15 0l-1.4 1.4a.113.113 0 00.004.156z"></path></svg></a><a id="new-external-stdlib-configuration"></a></span></span></h3><p>This is a long-awaited <a href="https://github.com/rescript-lang/rescript-compiler/pull/2171" rel="noopener noreferrer">feature request</a>. </p><p>Our compiler comes with a set of stdlib modules (such as <code>Belt</code>, <code>Pervasives</code>, etc.) for core functionality. Compiled ReScript code relies on the JS runtime version of these stdlib modules.</p><p>In previous versions, users couldn't ship their compiled JS code without defining a <code>package.json</code> dependency on <code>bs-platform</code>. Whenever a ReScript developer wanted to publish a package just for pure JS consumption / lean container deployment, they were required to use a bundler to bundle up their library / stdlib code, which made things way more complex and harder to understand.</p><p>To fix this problem, we now publish our pre-compiled stdlib JS files as a separate npm package called <a href="https://www.npmjs.com/package/@rescript/std" rel="noopener noreferrer"><code>@rescript/std</code></a>. Each new <code>bs-platform</code> release has a matching <code>@rescript/std</code> release for runtime compatibility.</p><p>We also introduced a new configuration within our <code>bsconfig.json</code> file to tell the compiler to use our pre-compiled package instead:</p><pre><div><div><p>JSON</p><p><code>{
  
  <span>"external-stdlib"</span> : <span>"@rescript/std"</span>
}
</code></p></div></div></pre><p>With this configuration set, compiled JS code will now point to the defined <code>external-stdlib</code> path:</p><div><div><div><pre><code><span>Belt</span>.<span>Array</span>.forEach(<span>[</span><span>1</span>, <span>2</span>, <span>3</span><span>]</span>, (num) <span>=&gt;</span> <span>Js</span>.log(num))
</code></pre></div></div></div><p>The JavaScript output above was compiled with an <code>es6</code> target, but will also work with <code>commonjs</code>.</p><p><strong>Important:</strong> When using this option, you need to make sure that the version number of <code>bs-platform</code> and <code>@rescript/std</code> matches with the same version number in your <code>package.json</code> file, otherwise you'll eventually run into runtime problems due to mismatching stdlib behavior!</p><p>To prevent unnecessary complications, only use this feature when...</p><ul><li><p>You want to ship a library for JS / TS consumers without making them depend on <code>bs-platform</code></p></li><li><p>You can't depend on <code>bs-platform</code> due to toolchain size (docker containers, low-storage deployment devices, etc)</p></li></ul><h3>Less Bundle Bloat when Adding ReScript<span><span><a href="#less-bundle-bloat-when-adding-rescript"><svg height="0.8em" width="0.8em" viewBox="0 0 20.003 19.944"><path d="M11.927 7.908a4.819 4.819 0 00-3.968-1.3 5.091 5.091 0 00-2.921 1.508L1.47 11.684a4.82 4.82 0 00.192 7.122 4.994 4.994 0 006.76-.4l3.7-3.776a.109.109 0 00-.067-.184s-.649.029-1.132.006a10.116 10.116 0 01-1.35-.226.308.308 0 00-.243.088l-2.529 2.609a2.733 2.733 0 01-3.583.319 2.64 2.64 0 01-.247-3.951l3.755-3.753a2.7 2.7 0 013.654-.073.108.108 0 00.15 0l1.4-1.4a.114.114 0 00-.003-.157z"></path><path d="M8.076 12.036a4.822 4.822 0 003.967 1.3 5.089 5.089 0 002.922-1.509l3.568-3.568a4.818 4.818 0 00-.192-7.121 5 5 0 00-6.761.4l-3.7 3.777a.108.108 0 00.067.183s.648-.028 1.132-.006a10.151 10.151 0 011.35.226.3.3 0 00.243-.088l2.529-2.608a2.732 2.732 0 013.581-.319 2.638 2.638 0 01.249 3.95l-3.755 3.754a2.706 2.706 0 01-3.654.073.107.107 0 00-.15 0l-1.4 1.4a.113.113 0 00.004.156z"></path></svg></a><a id="less-bundle-bloat-when-adding-rescript"></a></span></span></h3><p>With each release we keep a close eye on generating code that is optimized for tree-shaking. We also believe that we reached a milestone where ReScript reliably produces output that has almost no impact on our final JS bundle-sizes (this is what we call our "zero-cost" philosophy).</p><p>The bundled code is almost ReScript runtime free because our generated library code fits the tree-shaking principle really well. Tools like <code>esbuild</code> can easily drop unnecessary code and make sure that the final code stays lean.</p><p>We made a small <a href="https://github.com/bobzhang/zero-cost-rescript" rel="noopener noreferrer">demo repo</a> and added the precompiled JS bundles to demonstrate what we've achieved. Check it out!</p><h3>Improved Code Generation for Pattern Matching<span><span><a href="#improved-code-generation-for-pattern-matching"><svg height="0.8em" width="0.8em" viewBox="0 0 20.003 19.944"><path d="M11.927 7.908a4.819 4.819 0 00-3.968-1.3 5.091 5.091 0 00-2.921 1.508L1.47 11.684a4.82 4.82 0 00.192 7.122 4.994 4.994 0 006.76-.4l3.7-3.776a.109.109 0 00-.067-.184s-.649.029-1.132.006a10.116 10.116 0 01-1.35-.226.308.308 0 00-.243.088l-2.529 2.609a2.733 2.733 0 01-3.583.319 2.64 2.64 0 01-.247-3.951l3.755-3.753a2.7 2.7 0 013.654-.073.108.108 0 00.15 0l1.4-1.4a.114.114 0 00-.003-.157z"></path><path d="M8.076 12.036a4.822 4.822 0 003.967 1.3 5.089 5.089 0 002.922-1.509l3.568-3.568a4.818 4.818 0 00-.192-7.121 5 5 0 00-6.761.4l-3.7 3.777a.108.108 0 00.067.183s.648-.028 1.132-.006a10.151 10.151 0 011.35.226.3.3 0 00.243-.088l2.529-2.608a2.732 2.732 0 013.581-.319 2.638 2.638 0 01.249 3.95l-3.755 3.754a2.706 2.706 0 01-3.654.073.107.107 0 00-.15 0l-1.4 1.4a.113.113 0 00.004.156z"></path></svg></a><a id="improved-code-generation-for-pattern-matching"></a></span></span></h3><p>We fine-tuned our pattern matching engine to optimize the JS output even more. Here is an example of a pretty substantial optimization, based on <a href="https://github.com/rescript-lang/rescript-compiler/issues/4924" rel="noopener noreferrer">this issue</a>:</p><pre><div><div><p>RES</p><p><code><span>type</span> test <span>=</span>
  | <span>NoArg</span>
  | <span>AnotherNoArg</span>
  | <span>OtherArg</span>(int)

<span>let</span> test <span>=</span> x <span>=&gt;</span>
  <span>switch</span> x {
  | <span>NoArg</span> <span>=&gt;</span> <span>true</span>
  | _ <span>=&gt;</span> <span>false</span>
  }
</code></p></div></div></pre><p>The snippet above will compile to the following JS output:</p><div><div><div><pre><code><span><span>function</span> <span>test</span>(<span>x</span>)</span>{
  <span>return</span> x === <span>0</span>
}
</code></pre></div></div></div><p>As you can see, the 9.0 compiler removes all the unnecessary <code>typeof</code> checks!</p><p>This is possible because our optimizer will try to analyze several predicates and get rid of redundant ones. More diffs can be found <a href="https://github.com/rescript-lang/rescript-compiler/pull/4927/files?file-filters%5B%5D=.js" rel="noopener noreferrer">here</a>.</p><p>Another important improvement is that we fixed the pattern match offset issue, which lead to the consequence that magic numbers will not be generated for complex pattern matches anymore.</p><p>For those interested in the details, here is a representative diff resulting from this cleanup:</p><pre><div><div><p>DIFF</p><p><code>function is_space(param){
<span>- var switcher = param - 9 | 0;</span>
<span>- if (switcher &gt; 4 || switcher &lt; 0) {</span>
<span>-    return switcher == 23 ; </span>
<span>+ if (param &gt; 13 || param &lt; 9) {</span>
<span>+    return param === 32;</span>
  } else {
<span>-    return switcher !== 2;     </span>
<span>+    return param != 11;</span>
  }    
}
</code></p></div></div></pre><h2>Syntax Improvements<span><span><a href="#syntax-improvements"><svg height="0.8em" width="0.8em" viewBox="0 0 20.003 19.944"><path d="M11.927 7.908a4.819 4.819 0 00-3.968-1.3 5.091 5.091 0 00-2.921 1.508L1.47 11.684a4.82 4.82 0 00.192 7.122 4.994 4.994 0 006.76-.4l3.7-3.776a.109.109 0 00-.067-.184s-.649.029-1.132.006a10.116 10.116 0 01-1.35-.226.308.308 0 00-.243.088l-2.529 2.609a2.733 2.733 0 01-3.583.319 2.64 2.64 0 01-.247-3.951l3.755-3.753a2.7 2.7 0 013.654-.073.108.108 0 00.15 0l1.4-1.4a.114.114 0 00-.003-.157z"></path><path d="M8.076 12.036a4.822 4.822 0 003.967 1.3 5.089 5.089 0 002.922-1.509l3.568-3.568a4.818 4.818 0 00-.192-7.121 5 5 0 00-6.761.4l-3.7 3.777a.108.108 0 00.067.183s.648-.028 1.132-.006a10.151 10.151 0 011.35.226.3.3 0 00.243-.088l2.529-2.608a2.732 2.732 0 013.581-.319 2.638 2.638 0 01.249 3.95l-3.755 3.754a2.706 2.706 0 01-3.654.073.107.107 0 00-.15 0l-1.4 1.4a.113.113 0 00.004.156z"></path></svg></a><a id="syntax-improvements"></a></span></span></h2><h3><code>when</code> -&gt; <code>if</code><span><span><a href="#when---if"><svg height="0.8em" width="0.8em" viewBox="0 0 20.003 19.944"><path d="M11.927 7.908a4.819 4.819 0 00-3.968-1.3 5.091 5.091 0 00-2.921 1.508L1.47 11.684a4.82 4.82 0 00.192 7.122 4.994 4.994 0 006.76-.4l3.7-3.776a.109.109 0 00-.067-.184s-.649.029-1.132.006a10.116 10.116 0 01-1.35-.226.308.308 0 00-.243.088l-2.529 2.609a2.733 2.733 0 01-3.583.319 2.64 2.64 0 01-.247-3.951l3.755-3.753a2.7 2.7 0 013.654-.073.108.108 0 00.15 0l1.4-1.4a.114.114 0 00-.003-.157z"></path><path d="M8.076 12.036a4.822 4.822 0 003.967 1.3 5.089 5.089 0 002.922-1.509l3.568-3.568a4.818 4.818 0 00-.192-7.121 5 5 0 00-6.761.4l-3.7 3.777a.108.108 0 00.067.183s.648-.028 1.132-.006a10.151 10.151 0 011.35.226.3.3 0 00.243-.088l2.529-2.608a2.732 2.732 0 013.581-.319 2.638 2.638 0 01.249 3.95l-3.755 3.754a2.706 2.706 0 01-3.654.073.107.107 0 00-.15 0l-1.4 1.4a.113.113 0 00.004.156z"></path></svg></a><a id="when---if"></a></span></span></h3><p>Starting from 9.0, <a rel="noopener noreferrer" href="https://rescript-lang.org/docs/manual/latest/pattern-matching-destructuring#when-clause"><code>when</code> clauses</a> within a <code>switch</code> statement will automatically convert to the <code>if</code> keyword instead.</p><div><div><div><pre><code><span>switch</span> person1 {
| <span>Student</span>({reportCard: {gpa}}) <span>if</span> gpa &lt; <span>0.5</span> <span>=&gt;</span>
  <span>Js</span>.log(<span>"What's happening"</span>)
| _ <span>=&gt;</span> () 
}
</code></pre></div></div></div><p>The <code>when</code> keyword is deprecated. The syntax will continue supporting it and the formatter will automatically convert to <code>if</code>, for a pain-free upgrade.</p><h3>Cleaner Polyvariant Syntax<span><span><a href="#cleaner-polyvariant-syntax"><svg height="0.8em" width="0.8em" viewBox="0 0 20.003 19.944"><path d="M11.927 7.908a4.819 4.819 0 00-3.968-1.3 5.091 5.091 0 00-2.921 1.508L1.47 11.684a4.82 4.82 0 00.192 7.122 4.994 4.994 0 006.76-.4l3.7-3.776a.109.109 0 00-.067-.184s-.649.029-1.132.006a10.116 10.116 0 01-1.35-.226.308.308 0 00-.243.088l-2.529 2.609a2.733 2.733 0 01-3.583.319 2.64 2.64 0 01-.247-3.951l3.755-3.753a2.7 2.7 0 013.654-.073.108.108 0 00.15 0l1.4-1.4a.114.114 0 00-.003-.157z"></path><path d="M8.076 12.036a4.822 4.822 0 003.967 1.3 5.089 5.089 0 002.922-1.509l3.568-3.568a4.818 4.818 0 00-.192-7.121 5 5 0 00-6.761.4l-3.7 3.777a.108.108 0 00.067.183s.648-.028 1.132-.006a10.151 10.151 0 011.35.226.3.3 0 00.243-.088l2.529-2.608a2.732 2.732 0 013.581-.319 2.638 2.638 0 01.249 3.95l-3.755 3.754a2.706 2.706 0 01-3.654.073.107.107 0 00-.15 0l-1.4 1.4a.113.113 0 00.004.156z"></path></svg></a><a id="cleaner-polyvariant-syntax"></a></span></span></h3><p>Polyvariants with invalid identifier names (e.g. names including hypens <code>-</code>), don't require any special escaping syntax anymore:</p><div><div><div><pre><code><span>type</span> animation <span>=</span> <span>[</span> #<span>"ease-in"</span> | #<span>"ease-out"</span> <span>]</span>
</code></pre></div></div></div><p>We introduced this change to allow easier interop with existing JS string enums. In pure ReScript code, we'd still recommend our users to stick with valid identifier names instead (e.g. <code>easeIn</code> instead of <code>ease-in</code>).</p><h2>Breaking Changes<span><span><a href="#breaking-changes"><svg height="0.8em" width="0.8em" viewBox="0 0 20.003 19.944"><path d="M11.927 7.908a4.819 4.819 0 00-3.968-1.3 5.091 5.091 0 00-2.921 1.508L1.47 11.684a4.82 4.82 0 00.192 7.122 4.994 4.994 0 006.76-.4l3.7-3.776a.109.109 0 00-.067-.184s-.649.029-1.132.006a10.116 10.116 0 01-1.35-.226.308.308 0 00-.243.088l-2.529 2.609a2.733 2.733 0 01-3.583.319 2.64 2.64 0 01-.247-3.951l3.755-3.753a2.7 2.7 0 013.654-.073.108.108 0 00.15 0l1.4-1.4a.114.114 0 00-.003-.157z"></path><path d="M8.076 12.036a4.822 4.822 0 003.967 1.3 5.089 5.089 0 002.922-1.509l3.568-3.568a4.818 4.818 0 00-.192-7.121 5 5 0 00-6.761.4l-3.7 3.777a.108.108 0 00.067.183s.648-.028 1.132-.006a10.151 10.151 0 011.35.226.3.3 0 00.243-.088l2.529-2.608a2.732 2.732 0 013.581-.319 2.638 2.638 0 01.249 3.95l-3.755 3.754a2.706 2.706 0 01-3.654.073.107.107 0 00-.15 0l-1.4 1.4a.113.113 0 00.004.156z"></path></svg></a><a id="breaking-changes"></a></span></span></h2><p>This release comes with a minor breaking change that shouldn't have much impact on the upgrade of existing codebases.</p><h3>Nested Records within Objects<span><span><a href="#nested-records-within-objects"><svg height="0.8em" width="0.8em" viewBox="0 0 20.003 19.944"><path d="M11.927 7.908a4.819 4.819 0 00-3.968-1.3 5.091 5.091 0 00-2.921 1.508L1.47 11.684a4.82 4.82 0 00.192 7.122 4.994 4.994 0 006.76-.4l3.7-3.776a.109.109 0 00-.067-.184s-.649.029-1.132.006a10.116 10.116 0 01-1.35-.226.308.308 0 00-.243.088l-2.529 2.609a2.733 2.733 0 01-3.583.319 2.64 2.64 0 01-.247-3.951l3.755-3.753a2.7 2.7 0 013.654-.073.108.108 0 00.15 0l1.4-1.4a.114.114 0 00-.003-.157z"></path><path d="M8.076 12.036a4.822 4.822 0 003.967 1.3 5.089 5.089 0 002.922-1.509l3.568-3.568a4.818 4.818 0 00-.192-7.121 5 5 0 00-6.761.4l-3.7 3.777a.108.108 0 00.067.183s.648-.028 1.132-.006a10.151 10.151 0 011.35.226.3.3 0 00.243-.088l2.529-2.608a2.732 2.732 0 013.581-.319 2.638 2.638 0 01.249 3.95l-3.755 3.754a2.706 2.706 0 01-3.654.073.107.107 0 00-.15 0l-1.4 1.4a.113.113 0 00.004.156z"></path></svg></a><a id="nested-records-within-objects"></a></span></span></h3><p>Previously, if you wrote <code>{"user": {age: 10}}</code>, the inner record was interpreted as an object instead of a record (<code>{"user": {"age": 10}}</code>); this is a byproduct of some internal interop transformation details; with the ReScript syntax, this went from understandable to confusing, so we're changing it so that the inner record is indeed now treated as a record. This is an obvious fix, but a breaking change if you were accidentally leveraging that nested record as object.</p><p>Here is a code example before and after the change. Note how the <code>user</code> record secretly turns into a ReScipt object in the previous version:</p><div><div><div><pre><code><span>type</span> user <span>=</span> {
  age: int 
}

<span>let</span> data <span>=</span> {
  <span>"user"</span>: {
    age: <span>1</span> 
  }
}


<span>let</span> age <span>=</span> <span>data[<span>"user"</span>]</span>.age
</code></pre></div></div></div><p>More discussions on this change can be found <a href="https://forum.rescript-lang.org/t/fixing-the-semantics-of-nested-objects-breaking-changes/976" rel="noopener noreferrer">here</a>.</p><h2>Closing Note<span><span><a href="#closing-note"><svg height="0.8em" width="0.8em" viewBox="0 0 20.003 19.944"><path d="M11.927 7.908a4.819 4.819 0 00-3.968-1.3 5.091 5.091 0 00-2.921 1.508L1.47 11.684a4.82 4.82 0 00.192 7.122 4.994 4.994 0 006.76-.4l3.7-3.776a.109.109 0 00-.067-.184s-.649.029-1.132.006a10.116 10.116 0 01-1.35-.226.308.308 0 00-.243.088l-2.529 2.609a2.733 2.733 0 01-3.583.319 2.64 2.64 0 01-.247-3.951l3.755-3.753a2.7 2.7 0 013.654-.073.108.108 0 00.15 0l1.4-1.4a.114.114 0 00-.003-.157z"></path><path d="M8.076 12.036a4.822 4.822 0 003.967 1.3 5.089 5.089 0 002.922-1.509l3.568-3.568a4.818 4.818 0 00-.192-7.121 5 5 0 00-6.761.4l-3.7 3.777a.108.108 0 00.067.183s.648-.028 1.132-.006a10.151 10.151 0 011.35.226.3.3 0 00.243-.088l2.529-2.608a2.732 2.732 0 013.581-.319 2.638 2.638 0 01.249 3.95l-3.755 3.754a2.706 2.706 0 01-3.654.073.107.107 0 00-.15 0l-1.4 1.4a.113.113 0 00.004.156z"></path></svg></a><a id="closing-note"></a></span></span></h2><p>We only highlighted a few user-facing features, but there are also some pretty interesting internal changes happening right now.</p><p>For example, we are tinkering with the idea on using WASM to replace Camlp4, and we are also working on a generalized visitor pattern that doesn't require objects.</p><p>We will discuss these topics in a separate development post, but we are already excited about the new possibilities this will bring within the compiler toolchain.</p><p>Happy Hacking!</p></div></div></div>]]>
            </description>
            <link>https://rescript-lang.org/blog/release-9-0</link>
            <guid isPermaLink="false">hacker-news-small-sites-26228682</guid>
            <pubDate>Mon, 22 Feb 2021 18:52:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Machine learnning for cloud removal in satellite images]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26228564">thread link</a>) | @sowmyay
<br/>
February 22, 2021 | https://eng.ruumi.io/post/seeing-through-clouds.html | <a href="https://web.archive.org/web/*/https://eng.ruumi.io/post/seeing-through-clouds.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div>
        <p>During the 2020 lockdown, I embarked on building machine learning applications using satellite images with the lovely folks from <a href="https://ruumi.io/">ruumi.io</a>.
We worked on machine learning models to look through clouds, in collaboration with <a href="https://github.com/daniel-j-h">daniel-j-h</a>.</p>
<p>We recently presented <a href="https://docs.google.com/presentation/d/1lEIQoCLEv-GUp49OkKa0xu1q3-DXChqQOwjfAru2nd4/edit?usp=sharing">our approach</a> on cloud see-thru at the <a href="https://www.meetup.com/Geo-Berlin/events/275863507/">GeoBerlin</a> meetup, and received a lot of interest and questions.
And so, we decided to write this blog post detailing our methodology, and challenges we faced on the way.</p>
<h2 id="the-challenges-with-using-satellite-images">The Challenges with using Satellite Images<a href="#the-challenges-with-using-satellite-images" arialabel="Anchor">⌗</a> </h2>
<p>At ruumi, we work on rotational grazing: a managed grazing technique to improve biodiversity, replenish the soil, and be profitable for farmers at the same time.
However, many farmers are apprehensive about adopting such regenerative agriculture techniques, as they are labor-intensive and require complex planning.</p>
<p><img src="https://eng.ruumi.io/img/post/0002-rotational-grazing.jpg" alt="Picture showing rotational grazing"></p>
<p>This is where ruumi comes in: we are using multi-spectral and radar satellite images, soil maps, seed mix data, and state-of-the-art remote sensing and machine learning techniques to assist farmers in transitioning to regenerative agriculture.</p>
<p>One of the main challenges in using satellite data for land monitoring applications is cloud obstruction.
These obstructions can be twofold: clouds may block the view of the farmland underneath and their shadows may obscure or distort features on the ground.</p>
<p>Over the last few years, several cloud removal techniques have been developed: <a href="https://eox.at/2017/03/sentinel-2-cloudless/">Sentinel-2 Cloudless</a>, <a href="https://www.wired.com/2013/05/a-cloudless-atlas/">Mapbox Cloudless Atlas</a> and <a href="https://www.theatlantic.com/technology/archive/2016/06/google-maps-gets-a-satellite-makeover-mosaic-700-trillion/488939/">Google’s Cloudless Satellite Map</a>.
These techniques are sifting through multiple years of images to create cloud-free scenes.
This works fine if the goal is to improve the aesthetics of the satellite images, but the trade-off is recency.
In applications like ours, where we rely on the most recent images to monitor changes on the land, these techniques cannot be used.</p>
<p>Since we haven’t found any cloud removal techniques out there that prioritize recency, we set out to develop our own state-of-the-art machine learning model to do just that.
Our goal is not to hallucinate what we can’t see, but instead, to remove those pesky translucent clouds and accurately reconstruct what’s underneath.</p>
<p><img src="https://eng.ruumi.io/img/post/0002-nocloud.gif" alt="Animation showing before and after cloud removal">
<em>Before and after pictures showing our cloud removal technique</em></p>
<h2 id="sentinel-2-constellation">Sentinel-2 Constellation<a href="#sentinel-2-constellation" arialabel="Anchor">⌗</a> </h2>
<p>Over the past decade, the European Space Agency (ESA) has been developing and putting into orbit, a network of satellites called Sentinels.
These satellites carry a range of sensors, such as radar and multi-spectral imaging instruments for land, ocean, and atmospheric monitoring at a global scale.</p>
<p>The Sentinel-2 satellite mission is part of this ongoing effort and is of particular interest to us.
This mission consists of twin satellites flying in the same orbit around the earth, but phased at 180°.
They provide high resolution multi-spectral images with a planet wide coverage, and a revisit frequency of 3-5 days i.e. the satellite sensors revisit the same patch on the earth every 3-5 days.</p>
<p><img src="https://eng.ruumi.io/img/post/0002-sentinel-in-orbit.jpg" alt="Picture showing the Sentinel-2 satellites">
<em>Sentinel 2 satellite, mapping the earth’s surface. Photo by <a href="https://www.esa.int/ESA_Multimedia/Images/2012/02/Sentinel-2">ESA</a></em></p>
<p>The Sentinel-2 satellites carry a multi-spectral instrument (MSI) which works passively, by collecting sunlight reflected from the Earth’s surface.
New data is acquired as the satellite moves along its orbital path.
The incoming light beam is split into 12 spectral bands, in the visible, near infrared and short wave infrared spectral range.
Of these 12 bands, 4 bands are at 10m, 6 bands at 20m and 2 bands at 60m spatial resolution.
Here, spatial resolution refers to the ground area covered by one pixel.
This means that in an image of 10m resolution, each pixel corresponds to a square of 10m x 10m on the ground.</p>
<p>Apart from these 12 spectral bands, Sentinel-2 data also includes a scene classification mask at 60m resolution.
With the help of this band we can easily identify defective pixels, cloud pixels (both thick and thin or cirrus), snow or water pixels, and vegetation and non-vegetation pixels in the image.</p>
<p>For more information on the Sentinel-2 mission, please refer to <a href="https://sentinel.esa.int/web/sentinel/user-guides/sentinel-2-msi">their user guides</a>.</p>
<h2 id="cloud-mask">Cloud Mask<a href="#cloud-mask" arialabel="Anchor">⌗</a> </h2>
<p>Clouds can be broadly classified into two types: dense, and thin or cirrus clouds.</p>
<p>Dense clouds do not allow the penetration of visible spectral radiation from the ground and tend to cast a shadow on the ground.
It is hard to predict the pixel values under these clouds with high confidence as none of the Sentinel-2 spectral bands are able to penetrate through them.</p>
<p>Thin or cirrus clouds on the other hand are transparent or semi-transparent clouds.
Most spectral bands can partially see through these clouds. This is key to accurate de-clouding with our model.</p>
<p>The scene classification mask provided in Sentinel-2 data is able to classify cloud pixels and cloud shadows in the image.
It is also able to distinguish between thick and cirrus clouds, as described <a href="https://sentinel.esa.int/web/sentinel/technical-guides/sentinel-2-msi/level-2a/algorithm">here</a>.</p>
<p><strong>Our aim is to reconstruct pixels classified as cirrus clouds and cloud shadows, while guaranteeing image recency and accuracy.</strong></p>
<h2 id="our-approach">Our Approach<a href="#our-approach" arialabel="Anchor">⌗</a> </h2>
<p>Our main contribution is to create a training dataset based on (cloud, no-cloud) pairs for the same geography but from different days.
The machine learning model then learns to re-construct the no-cloud sample from the cloud sample.</p>
<p>The basic assumption is that between cloud and no-cloud scenes only cloud pixels will change.
This simple approach worked very well for us.</p>
<p>Inspired by the paper <a href="https://arxiv.org/abs/1804.07723">Image inpainting for irregular holes using partial convolutions</a>, we developed a machine learning model that can reconstruct pixels that are otherwise distorted by the semi-transparent clouds and cloud shadows.
We pass image data from the 12 multi-spectral bands, and the cloud mask extracted from the scene classification mask to our model.
Our model, referred to as SensrUnet, works by extracting relevant information from the multi-spectral image data and reconstructing a de-clouded image from the information.</p>
<p>To prepare our training dataset, we began by downloading Sentinel-2 tiles from the past two years over the geographical region of Germany.
There are multiple Sentinel-2 tiles covering Germany and we use all tiles to build a dataset to train our model.</p>
<p><img src="https://eng.ruumi.io/img/post/0002-tile-cover-germany.jpg" alt="Picture showing the Sentinel-2 tiles overlayed on a map of Germany">
<em>Sentinel tile cover over Germany</em></p>
<p>The spatial resolutions of the multi-spectral bands varied between 10m, 20m and 60m resolution.
To stack all the bands together, so that it can be passed to the model as a single input unit, bands have to be scaled to the same resolution.
Scaling bands from lower resolution to a higher resolution, inadvertently results in introducing blurriness.
To keep this to a minimum and since most of the bands were in the 20m and 60m spatial resolution, we chose to scale all the bands to 20m resolution.</p>
<p><img src="https://eng.ruumi.io/img/post/0002-image-chunking.jpg" alt="Picture showing how to chunk an image into smaller parts">
<em>Cutting out smaller images from the spectral band stacks</em></p>
<p>We stacked all the 12 resampled spectral bands, and the cloud mask generated from the scene classification mask.
Next we cut out smaller images of size 256x256x13 from these spectral band stacks and constructed (cloud, no-cloud) image pairs.</p>
<p>To assure recency between the cloud and no-cloud images, we constrained the time difference between these images to be at most two weeks.</p>
<h2 id="network-architecture">Network Architecture<a href="#network-architecture" arialabel="Anchor">⌗</a> </h2>
<p>SensrUNet is an improved UNet architecture with a pre-trained encoder following the approach from the <a href="https://arxiv.org/abs/1801.05746">TernausNet paper</a>.
The decoder blocks perform pixel shuffle up-scaling with <a href="https://arxiv.org/abs/1609.05158">ICNR initialization</a> to avoid any artifacts in the up-scaled image.</p>
<p>We also added <a href="https://arxiv.org/abs/1904.11492">global context attention blocks</a> after each encoder and decoder layer.
These attention blocks enable the model to reconstruct different parts of the image effectively as they retain a global context of the image.
We observed significant improvements in output image quality with the attention blocks, while the number of parameters increased only by a small percentage of 1.6%.</p>
<p><img src="https://eng.ruumi.io/img/post/0002-model-arch.jpg" alt="Picture showing a Unet’ish model architecture">
<em>SensrUnet: model architecture</em></p>
<h3 id="loss-function-and-model">Loss Function and Model<a href="#loss-function-and-model" arialabel="Anchor">⌗</a> </h3>
<p>Our training dataset consists of (cloud, no-cloud) pairs.
We pass the cloudy image (with 13 multi-spectral bands) to our model, and it outputs a de-clouded image, which we compare to the no-cloud image (target image).</p>
<p>Since we don’t want the model to strongly replicate the pixel values in the target image, we focus on loss computation only on pixels where thin or cirrus clouds are present.
We uniquely designed our loss function to be a combination of pixel loss, perceptual, texture losses and total variation loss, similar to the loss function used in <a href="https://arxiv.org/pdf/1804.07723.pdf">this paper</a>.</p>
<p>The perceptual and texture losses allow for minor fluctuations between the output and target, while capturing the similarity in style and the features of the two images.
After all, these are satellite images of two different days and minor variations in the input and target images are expected.</p>
<p>The total variation loss, acts as a smoothing penalty on the composite image.
Collectively, these loss functions ensure smooth reconstruction of the image under thin or cirrus cloud pixels.</p>
<div><pre><code data-lang="python">composite <span>=</span> cloud_mask <span>*</span> out <span>+</span> (<span>1</span> <span>-</span> cloud_mask) <span>*</span> target                                                                       

loss <span>=</span> a <span>*</span> PixelwiseLoss(cloud_mask <span>*</span> out, cloud_mask <span>*</span> target)
     <span>+</span> b <span>*</span> PerceptualLoss(composite, target)
     <span>+</span> c <span>*</span> TextureLoss(composite, target)
     <span>+</span> d <span>*</span> VariationLoss(composite)
</code></pre></div><p><em>Weighted combination of individual loss-terms</em></p>
<p>We trained the model for 48 hours on our workstation with 2x NVIDIA RTX 2080 TIs.</p>
<p>During inference, we ensure that we modify only pixels where thin or cirrus clouds are present and copy the other pixel values as is from the input image.
The reason for this is, that we don’t want to introduce prediction uncertainty in pixels where there were no clouds present, and leave those pixels untouched.</p>
<p><img src="https://eng.ruumi.io/img/post/0002-nocloud.gif" alt="Picture showing before and after cloud removal">
<em>Before and after pictures showing our cloud removal technique</em></p>
<h2 id="future-work">Future Work<a href="#future-work" arialabel="Anchor">⌗</a> </h2>
<p>We set out to develop a state-of-the art machine learning model that can de-cloud satellite images while prioritizing image recency.
Using SensrUnet, and the scene classification mask, we are able to successfully see through thin or cirrus clouds.</p>
<p>However, we found that the scene classification mask …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://eng.ruumi.io/post/seeing-through-clouds.html">https://eng.ruumi.io/post/seeing-through-clouds.html</a></em></p>]]>
            </description>
            <link>https://eng.ruumi.io/post/seeing-through-clouds.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26228564</guid>
            <pubDate>Mon, 22 Feb 2021 18:45:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Using Makesite.py]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26228172">thread link</a>) | @susam
<br/>
February 22, 2021 | https://www.swilliams.io/w/using-makesite-py/ | <a href="https://web.archive.org/web/*/https://www.swilliams.io/w/using-makesite-py/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="article">
            <article id="article">
                
<em>2021-01-12</em>
<p>I think every software developer, or even everyone entirely, should have a personal website. They are great opportunities to learn new technologies, get your
    writing online to where people can see it, and give back to the online community no doubt you rely on.</p>
<p>I've had a personal website for a few years now and I've been using a static site that's been manually maintained and curated, which hasn't been ideal as
    I've had to spend an equal amount of time in boring upkeep as I did actually producing content. So, I'm rather pleased to have recently found <a href="https://github.com/sunainapai/makesite">makesite.py</a> - an elegant, lightweight, and friendly little tool to generate static
    websites.</p>
<p>The primary design philoshopy behind makesite.py is its simplicity. The entire engine behind it is a single 250 line Python file which is easy for the even
    beginner programmers to understand. This engine uses templates you've made and combines them dynamically with content and data you've written, compiling it
    into a static website ready to be hosted. Running the code takes less than a second so the results are immediate, and so it fits nicely into my current
    workflow using VS Code.</p>
<p>Before now, I was manually maintaining the website's headers, page meta tags, RSS feed, etc. Now, I barely even have to think about that stuff and I'm more
    freed to write content without managing quite so much of the chores.</p>
<p>There are of course other static site generators out there: <a href="https://github.com/jekyll/jekyll">Jekyll</a>, <a href="https://github.com/getpelican/pelican">Pelican</a>, and <a href="https://github.com/gohugoio/hugo">Hugo</a> to name a few. But I think most
    struggle to come close to the simplicity of <a href="https://github.com/sunainapai/makesite">makesite.py</a>. I recommend makesite.py to anyone who's
    looking for a static site generator that is as simple as possible or anyone willing to dig a little into the code to really get a bespoke solution.</p>

            </article>
        </div></div>]]>
            </description>
            <link>https://www.swilliams.io/w/using-makesite-py/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26228172</guid>
            <pubDate>Mon, 22 Feb 2021 18:23:22 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I built a telnet chat server in 2021 with WebAssembly]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 1 (<a href="https://news.ycombinator.com/item?id=26227970">thread link</a>) | @todsacerdoti
<br/>
February 22, 2021 | https://lunatic.solutions/blog/lunatic-chat/ | <a href="https://web.archive.org/web/*/https://lunatic.solutions/blog/lunatic-chat/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section itemprop="articleBody"><p><span>
      <a href="https://lunatic.solutions/static/39135b82cc936f07e9a4f3a555d03383/8963a/terminal.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Terminal screenshot" title="Terminal screenshot" src="https://lunatic.solutions/static/39135b82cc936f07e9a4f3a555d03383/f058b/terminal.png" srcset="https://lunatic.solutions/static/39135b82cc936f07e9a4f3a555d03383/c26ae/terminal.png 158w,
https://lunatic.solutions/static/39135b82cc936f07e9a4f3a555d03383/6bdcf/terminal.png 315w,
https://lunatic.solutions/static/39135b82cc936f07e9a4f3a555d03383/f058b/terminal.png 630w,
https://lunatic.solutions/static/39135b82cc936f07e9a4f3a555d03383/40601/terminal.png 945w,
https://lunatic.solutions/static/39135b82cc936f07e9a4f3a555d03383/78612/terminal.png 1260w,
https://lunatic.solutions/static/39135b82cc936f07e9a4f3a555d03383/8963a/terminal.png 1918w" sizes="(max-width: 630px) 100vw, 630px" loading="lazy">
  </a>
    </span></p>
<p>I love the aesthetics of terminals and I’m not the only one, there is a whole
<a href="https://www.reddit.com/r/unixporn/">subreddit</a> dedicated to people sharing their desktops and
showcasing different terminal setups. Last year I spent working on
<a href="https://github.com/lunatic-solutions/lunatic">an innovative WebAssembly runtime called Lunatic</a>.
Recently we landed TCP support and I was super excited to start building real world applications
with it, and what would be a better fit than a terminal based chat server with a
<a href="https://thenewstack.io/the-lost-worlds-of-telnet/">retro vibe</a>?</p>
<p>It took me around a week to build it with Rust +
<a href="https://github.com/lunatic-solutions/lunatic">Lunatic</a> and you can check out the code
<a href="https://github.com/lunatic-solutions/chat">here</a>. If you would like to try it out you
can connect to it with:</p>
<div data-language="bash"><pre><code>
<span>&gt;</span> telnet lunatic.chat

<span>&gt;</span> telnet eu.lunatic.chat</code></pre></div>
<p>While writing the server I ran into many interesting problems and would like to share here how I
leveraged the power of Lunatic to overcome them.</p>
<h2>Architecture</h2>
<p>The reason I picked telnet is that the <a href="https://tools.ietf.org/html/rfc854">specification</a> is
simple enough to read through and implement in a short time. It’s a small layer on top of TCP and
as mentioned before we had TCP already working. On the other hand, telnet is a really limiting
protocol and I needed to get creative while building a chat application on top of it.</p>
<p>The first issue I encountered was the line based nature of terminals. You write a command, hit
enter and the terminal prints out some text. This doesn’t go well with the UI of a chat app where
messages can come in at any time. What are you supposed to do when new text arrives and the user
has already partially written her own message? Override the user’s input? Print the new message
after the input?</p>
<p>One solution would be to buffer all messages until the user hits enter and then just dump all
the ones that arrived in the meantime at once, but this can’t work as we would rely on the user
to keep hitting enter to read new messages.</p>
<p>It became clear that I needed to use some kind of terminal user interface where I render
separately all the incoming messages from the user who is currently typing. It’s possible to do
this by using <a href="https://tools.ietf.org/html/rfc1073">a few</a> <a href="https://tools.ietf.org/html/rfc1184">extensions</a>
to the telnet protocol. Once the telnet client connects I send it the following instructions:</p>
<ol>
<li>Don’t echo anything that the user is typing, let me be in charge of printing in the terminal.</li>
<li>Don’t buffer messages, send each keystroke to the server.</li>
<li>Report size changes of the terminal.</li>
</ol>
<p>This allows me to construct the UI on the server and just send a sequence of terminal escape
characters back to bring the user’s terminal up to date. On each keystroke or message received
the UI is updated.</p>
<h3>Massive concurrency</h3>
<p>For this to work we need to permanently keep the telnet connection open and periodically send
data through it. This is a perfect use case for Lunatic’s Processes, they are designed for massive
concurrency. Each client’s connection is handled in a separate Process.</p>
<p>Not to be confused with Operating System processes, Lunatic’s Processes are lightweight and also
known as green threads (but isolated) or <a href="https://golangbot.com/goroutines">go-routines</a> in other
runtimes. They are fast to create, have a small memory footprint and a low scheduling overhead.
All Processes are preemptively scheduled and they can’t spend too much time running without
yielding and giving others a fair share of the resources. This keeps all connections responsive
in an environment where most of the time is spent waiting on I/O.</p>
<h3>Interop with existing libraries</h3>
<p>Luckily I could make use of existing Rust libraries and didn’t need to reinvent the wheel. I used:</p>
<ol>
<li><a href="https://github.com/djc/askama">Askma</a> as a templating engine.</li>
<li><a href="https://github.com/fdehau/tui-rs">TUI</a> as the rendering engine.</li>
<li><a href="https://github.com/chronotope/chrono">Chrono</a> for date formatting.</li>
</ol>
<p>They all compiled to WebAssembly without issues. I just needed to provide a telnet backend for TUI,
but I could reuse most of the code from the <a href="https://github.com/redox-os/termion">termion</a> crate
(sadly it has no Windows support for now).</p>
<p>TUI works in a somewhat similar way to React.js, you update your state and just call a render method.
It will re-render the UI and send back to the client the minimal amount of changes in the form of
terminal escape characters.</p>
<h3>State Management</h3>
<p>A big part of programming is state management. Your application getting into a state that you
couldn’t predict while writing the code is a big source of bugs, and Lunatic tries to simplify
this by allowing you to isolate the state into separate processes.</p>
<p><span>
      <a href="https://lunatic.solutions/static/0e5c431d6d9ea83a50f7f5c79d3db764/636d3/processes.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Processes visualization" title="Processes visualization" src="https://lunatic.solutions/static/0e5c431d6d9ea83a50f7f5c79d3db764/f058b/processes.png" srcset="https://lunatic.solutions/static/0e5c431d6d9ea83a50f7f5c79d3db764/c26ae/processes.png 158w,
https://lunatic.solutions/static/0e5c431d6d9ea83a50f7f5c79d3db764/6bdcf/processes.png 315w,
https://lunatic.solutions/static/0e5c431d6d9ea83a50f7f5c79d3db764/f058b/processes.png 630w,
https://lunatic.solutions/static/0e5c431d6d9ea83a50f7f5c79d3db764/40601/processes.png 945w,
https://lunatic.solutions/static/0e5c431d6d9ea83a50f7f5c79d3db764/636d3/processes.png 1222w" sizes="(max-width: 630px) 100vw, 630px" loading="lazy">
  </a>
    </span></p>
<p>From the perspective of a process, it owns the whole memory and can’t influence the memory of
other processes in any way, not even by unsafe pointer dereferencing. This is a result of building
them on top of WebAssembly instances. The only way processes can talk between each other is through
message passing.</p>
<p>This greatly simplifies reasoning about state changes. You only need to think about what state you
are in and how the next message will influence the state change. It makes it a lot easier to debug
once you find yourself in an undesirable state. Let’s look at a concrete code sample from the
implementation:</p>
<div data-language="rust"><pre><code>


<span>pub</span> <span>fn</span> <span>server_process</span><span>(</span>state_receiver<span>:</span> <span>Receiver</span><span>&lt;</span><span>ServerMessage</span><span>&gt;</span><span>)</span> <span>{</span>
    <span>let</span> <span>mut</span> state <span>=</span> <span>ServerState</span> <span>{</span>
        clients<span>:</span> <span>0</span><span>,</span>
        channels<span>:</span> <span>HashMap</span><span>::</span><span>new</span><span>(</span><span>)</span><span>,</span>
    <span>}</span><span>;</span>

    <span>let</span> <span>mut</span> username_generator<span>:</span> <span>i64</span> <span>=</span> <span>0</span><span>;</span>
    <span>let</span> <span>mut</span> all_usernames <span>=</span> <span>HashSet</span><span>::</span><span>new</span><span>(</span><span>)</span><span>;</span>

    <span>loop</span> <span>{</span>
        <span>match</span> state_receiver<span>.</span><span>receive</span><span>(</span><span>)</span><span>.</span><span>unwrap</span><span>(</span><span>)</span> <span>{</span>
            <span>ServerMessage</span><span>::</span><span>Joined</span><span>(</span>client<span>)</span> <span>=&gt;</span> <span>{</span>
                
                state<span>.</span>clients <span>+=</span> <span>1</span><span>;</span>
                
                username_generator <span>+=</span> <span>1</span><span>;</span>
                <span>let</span> username <span>=</span> <span>format!</span><span>(</span><span>"User_{}"</span><span>,</span>
                                   username_generator<span>)</span><span>;</span>
                all_usernames<span>.</span><span>insert</span><span>(</span>username<span>.</span><span>clone</span><span>(</span><span>)</span><span>)</span><span>;</span>
                
                <span>let</span> server_info <span>=</span> <span>ServerInfo</span> <span>{</span>
                    clients<span>:</span> state<span>.</span>clients<span>,</span>
                    username<span>,</span>
                <span>}</span><span>;</span>
                <span>let</span> _ <span>=</span> client<span>.</span><span>send</span><span>(</span>server_info<span>)</span><span>;</span>
            <span>}</span>
            <span>ServerMessage</span><span>::</span><span>List</span><span>(</span>client<span>)</span> <span>=&gt;</span> <span>{</span>
                …<span>.</span>
            <span>}</span>
            <span>ServerMessage</span><span>::</span><span>ChangeName</span><span>(</span>from<span>,</span> to<span>,</span> client<span>)</span> <span>=&gt;</span> <span>{</span>
                <span>if</span> all_usernames<span>.</span><span>contains</span><span>(</span><span>&amp;</span>to<span>)</span> <span>{</span>
                    
                    <span>let</span> _ <span>=</span> client<span>.</span><span>send</span><span>(</span><span>false</span><span>)</span><span>;</span>
                <span>}</span> <span>else</span> <span>{</span>
                    all_usernames<span>.</span><span>remove</span><span>(</span><span>&amp;</span>from<span>)</span><span>;</span>
                    all_usernames<span>.</span><span>insert</span><span>(</span>to<span>)</span><span>;</span>
                    <span>let</span> _ <span>=</span> client<span>.</span><span>send</span><span>(</span><span>true</span><span>)</span><span>;</span>
                <span>}</span>
            <span>}</span>
            <span>...</span>
    <span>}</span>
<span>}</span></code></pre></div>
<p>We can see that the process has a few local variables to keep track of its state:</p>
<ul>
<li><em>How many clients are connected.</em></li>
<li><em>Which channels are available.</em></li>
<li><em>If a new user joins what username should be assigned.</em></li>
<li><em>Which usernames are taken.</em></li>
</ul>
<p>Afterwards the Process just runs in a loop waiting on messages. If a new client is connected the
server receives a <code>ServerMessage::Joined</code> message. It will then update the total count of users,
assign a new username to the client and send back a message notifying the client about the assigned
username.</p>
<p>The client’s process is similarly structured, it keeps a state of the current input box for each
channel and all received messages for the channels. The client’s process can receive 2 types of
messages:</p>
<ul>
<li><em>Keystrokes coming from the telnet connection.</em></li>
<li><em>New chat messages coming from all subscribed channels.</em></li>
</ul>
<p>For each keystroke we update the current channel’s input box or attach the new message to the
history of messages in the channel.</p>
<p>If we have such an architecture and run into a bug, let’s say the number of connected users shown
is wrong, there is only one source of truth here and we know exactly where this information came
from. We just need to figure out how we got into this state.</p>
<h3>Other benefits</h3>
<p>There are some not so obvious additional benefits that we get from Lunatic.</p>
<p>If a client’s process receives some malicious data from the telnet connection and crashes, it
will only terminate the existing connection. It can’t access the state of any other Processes.
In my first implementation I was often using <code>.unwrap</code> in the code, following
<a href="https://verraes.net/2014/12/erlang-let-it-crash/">Erlang’s let it crash philosophy</a> and knowing
that if I see any crashes in the logs I can always later investigate why they happened, but the
application should continue running.</p>
<p>The message sending implementation uses <a href="https://docs.rs/smol/1.2.5/smol/channel/index.html">Smol’s channels</a>
underneath, but you may be surprised not to see any <code>async</code> or <code>.await</code> keywords in the code.
The reason for this is that Lunatic abstracts away the asynchronous code and you can just write
seemingly blocking code, but it actually never blocks the underlying thread and takes full
advantage of async Rust. This is a whole topic on its own so I will leave it for another blog post.</p>
<p>Lunatic works with any code that can compile to WebAssembly, and as I have shown earlier a lot of
libraries just work out of the box. You can also
<a href="https://users.rust-lang.org/t/how-to-static-link-c-lib-to-wasm/36558/5">link C code into your Rust application</a>
while compiling to WebAssembly. One big pain point when using C from Erlang is that you need to be
extremely careful in your code, because if something crashes it will take the whole VM down. Or if
you spend too much time in the C part it will block the scheduler from using the thread and endanger
the responsiveness of your system. Lunatic solves both of these problems. The reduction counter is
inserted before the WebAssembly code is JIT translated to machine code and will also be part of the
“native” C code, allowing the scheduler to preempt it. A crash still stays isolated thanks to
WebAssembly sandboxing properties.</p>
<h2>Conclusion</h2>
<p>In the end the chat server will remain a nice toy application and you should not use it for more
serious use cases as telnet doesn’t encrypt any of the data sent to the server.</p>
<p>However, it’s a really good feeling to get something like this running on a runtime you have built.
While developing the chat application I found a few bugs in the runtime itself, so it was totally
worth creating this app . I’m really looking forward to gradually moving away from building Lunatic
and building amazing applications with it. I was also positively surprised how well the chat app is
working, being the first real word app built on Lunatic.</p>
<p>I think that we are finally at the point where WebAssembly is mature enough to be used in serious
applications, and I strongly believe that WebAssembly on the backend …</p></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://lunatic.solutions/blog/lunatic-chat/">https://lunatic.solutions/blog/lunatic-chat/</a></em></p>]]>
            </description>
            <link>https://lunatic.solutions/blog/lunatic-chat/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26227970</guid>
            <pubDate>Mon, 22 Feb 2021 18:10:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Rethinking the IDE for the 2020s]]>
            </title>
            <description>
<![CDATA[
Score 11 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26227466">thread link</a>) | @fsynced
<br/>
February 22, 2021 | https://movingfulcrum.com/rethinking-the-ide-for-2020s/ | <a href="https://web.archive.org/web/*/https://movingfulcrum.com/rethinking-the-ide-for-2020s/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="wrapper">
    
    


    <div id="ajax-container">
        
<div>
<article>
    

    <div>

        <p>Intellij IDEA has been an amazing professional-grade IDE for the last 20 years. However, as computer programs evolve, so must the IDE keep pace to remain a useful tool.</p><figure><blockquote><div lang="en" dir="ltr"><p>major IDE evolutions as I see:</p><p>2000s: using AST to represent text and building features around that. Intellij nailed this.</p><p>2010s: doing the same, but polyglot. Again Jetbrain's suite of IDEs adopted well in time.</p><p>2020s: support massive codebases across huge number of projects</p></div>— Prashant Deva (@pdeva) <a href="https://twitter.com/pdeva/status/1337817841567899649?ref_src=twsrc%5Etfw">December 12, 2020</a></blockquote>

</figure><h3 id="what-s-changed">What's Changed?</h3><p>A typical organization in the 2020s has:</p><ol><li>Hundreds of microservices</li><li>Hundreds of git repos</li><li>Polyglot codebase</li><li>APIs defined with HTTP/JSON/GRPC, not just programming language interfaces</li><li>Runtime service inter-dependencies</li><li>Cloud service dependencies</li></ol><p>Let's go through how each of these could impact the design of the IDE of the future. I will be using Intellij for comparison since it's the most advanced IDE currently.</p><h3 id="big-code-is-the-new-big-data">Big Code is the new Big Data</h3><p>With huge amounts of code across hundreds or even thousands of repos, the IDE has to now deal with 'Big Code'. It's big not just due to the sheer lines of code. It's the fact that it's divided into microservices, each of which has a separate set of dependencies, which the IDE now has to separately index. This can exponentially increase the amount of code to index compared to a single large codebase without so many external dependencies like the Linux kernel.</p><p>All operations in the IDE must assume huge amounts of code across hundreds of repositories, not all of them might be checked out locally. So things like Refactoring, Find Usages, Call hierarchy, etc have to be re-architected to run as long-running operations over code that could be both local or remote and still give users a seamless experience.</p><h3 id="refactoring">Refactoring</h3><p>Refactoring so far has really been a single repo feature. But what if the code you are refactoring is called by code in 100 other repos in your organization? What if those repos are not even checked out locally? The modern IDE needs to evolve beyond single repo operations. Maybe that rename refactoring now becomes a long-running operation that creates Pull Requests in various repos. This is not an easy problem to solve. Google even has a paper on this:</p><figure><blockquote><p lang="en" dir="ltr">Yes and in large monorepos refactoring involves mapreduce operations <a href="https://t.co/yc92gX1qem">https://t.co/yc92gX1qem</a></p>— Nagesh Susarla (@nageshs) <a href="https://twitter.com/nageshs/status/1337843676324589568?ref_src=twsrc%5Etfw">December 12, 2020</a></blockquote>

</figure><p>The complexity grows as API calls happen across services in various languages now and use HTTP/JSON or GRPC/ProtoBuf. Renaming a <code>struct</code> in one repo that gets serialized to JSON during an http api call might require renaming a similar <code>struct</code> in a whole different language that deserializes said JSON. This is way more complex than a simple Java function rename refactoring.</p><h3 id="version-control-ui">Version Control UI</h3><p>Version Control features in IDEs are really built around browsing/editing one git repo at a time. This simply doesn't work when your codebase is spread across hundreds of repos. The fundamental interface for the Git UI in Intellij (and other IDEs) needs to be rethought to deal with a large number of repos.</p><figure><blockquote><p lang="en" dir="ltr">The Git Log view in <a href="https://twitter.com/intellijidea?ref_src=twsrc%5Etfw">@intellijidea</a> is poorly designed with respect to multiple repositories. You need to use the mouse to get to this dropdown list in the view. Then you need to first *deselect* the current repo and select your new repo. <br>Dont think this was dogfooded by the devs. <a href="https://t.co/lZ0YHdrUth">pic.twitter.com/lZ0YHdrUth</a></p>— Prashant Deva (@pdeva) <a href="https://twitter.com/pdeva/status/1349137311619981312?ref_src=twsrc%5Etfw">January 12, 2021</a></blockquote>

</figure><h3 id="rethinking-the-project-model">Rethinking the 'Project' model</h3><p>Currently, an <a href="http://astradot.com/">Astradot</a> engineer has to check out a git repo, eg <a href="https://github.com/astradot/kafka-schema-sync">https://github.com/astradot/kafka-schema-sync</a>, open the IDE, point to it, which will create an Intellij 'project' for the repo or a 'module' for an existing project. This is backward. The IDE should ask for the Github org eg, <a href="http://github.com/org">github.com/astradot</a> and it should create a single project that contains all the repo as modules. It should then manage lazy-loading/lazy-checkout or whatever is needed to give me a seamless experience browsing the code of my entire org.</p><h3 id="-run-button">'Run' button</h3><p>The 'Run' button will need to have more intelligence than simply running your app. In a microservice world, your service might depend upon an 'auth' service which might require a Postgres database and Redis instance initialized to some state. The services may rely on k8s service names to communicate, thus requiring running inside k8s. The IDE will need to be aware of the environment where you want to run your services and initialize the dependencies appropriately when you hit the 'Run' button.</p><p>The traditional debugger though is not going anywhere anytime soon. Take that from a guy who wrote a <a href="https://www.youtube.com/watch?v=LpfmKIxusZY">time-traveling one</a>, once upon a time. Though IDEs could take a page from APMs and benefit from showing a distributed trace in addition to breakpoint-based debugging.</p><h3 id="what-s-not-the-future">What's not the future</h3><p>Silicon Valley has been obsessed with making 'IDE in the Cloud' happen for the last decade. Every year a new set of cloud IDE startups is funded while the old ones die off. None of the problems they are solving help professional engineers.</p><figure><blockquote><div lang="en" dir="ltr"><p>Annual IDE startup bingo card:</p><p>- Downgrade 'IDE' part from Intellij to VSCode but hey, it opens in browser!</p><p>- See every keystroke of other engineers - 'Collaboration/Live coding'</p><p>- Code runs in tiny ec2 instance with horsepower of 90s laptop instead of your 12 core AMD pc</p></div>— Prashant Deva (@pdeva) <a href="https://twitter.com/pdeva/status/1363206351128723457?ref_src=twsrc%5Etfw">February 20, 2021</a></blockquote>

</figure><p>From an <a href="http://astradot.com/">Astradot</a> engineer's perspective:</p><ul><li>Downloading and Installing Intellij is a non-issue</li><li>We have scripts to setup your workstation environment within minutes with all the needed compilers, tooling, etc.</li><li>We never need to see each other live code. That would be annoying/intruding on the other engineer's privacy.</li><li>Workstations are powerful enough that they can run the entire <a href="http://astradot.com/">Astradot</a> locally.</li></ul><p>We would love to buy all our engineers 64 core Threadrippers w 128Gb ram if the IDE could make use of it.</p><h3 id="conclusion">Conclusion</h3><p>The IDE of the future is very different from what Intellij is today, both in terms of its architecture and UI. It requires solving some hard computer science problems. Jetbrains seems more focused on making just evolutionary changes to its IDEs to keep it ahead of VS Code. This is an opportunity for a new startup to rise.</p>
    </div>

    
</article></div>
    </div>
</div></div>]]>
            </description>
            <link>https://movingfulcrum.com/rethinking-the-ide-for-2020s/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26227466</guid>
            <pubDate>Mon, 22 Feb 2021 17:39:54 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Microsoft and European news for Australian-style arbitration mechanism in Europe]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26227215">thread link</a>) | @alexrustic
<br/>
February 22, 2021 | https://www.epceurope.eu/post/europe-s-press-publishers-microsoft-call-for-australian-style-arbitration-mechanism-in-europe | <a href="https://web.archive.org/web/*/https://www.epceurope.eu/post/europe-s-press-publishers-microsoft-call-for-australian-style-arbitration-mechanism-in-europe">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-hook="post-description"><article><div><div data-rce-version="8.17.5"><div dir="ltr"><div><p id="viewer-ct2co"><span> <strong>PRESS RELEASE</strong> </span></p><p id="viewer-adra"><span><em>Brussels, 22 February 2021</em> </span></p><p id="viewer-3s19g"><span><strong>Europe’s press publishers &amp; Microsoft call for Australian-style arbitration mechanism in Europe to ensure tech gatekeepers remunerate press publishers fairly for use of content.</strong></span></p><p id="viewer-6cvbf"><span>Europe’s press publishers and Microsoft today agreed to work together on a solution to ensure that Europe’s press publishers get paid for the use of their content by gatekeepers that have dominant market power in line with the objectives of the new neighbouring right in the EU Digital Single Market Copyright Directive, which comes into force this June and to take inspiration from the new Australian legislation that requires the tech gatekeepers covered by that law to share revenue with news organisations. </span></p><p id="viewer-9tsm9"><span>The solution should mandate payments for the use of press publishers’ content by these gatekeepers and should include arbitration provisions, to ensure that fair agreements are negotiated. Such provisions should consider the model established by the Australian law, which enables an arbitral panel to establish a fair price based on an assessment of the benefits derived by each side in having the news content included on these gatekeepers’ platforms, the costs of producing this content, and any undue burden an amount would place on the platforms themselves. </span></p><p id="viewer-c0s1p"><span>Although press publishers have been granted a neighbouring right in the EU, negotiations with such gatekeepers will not produce fair outcomes unless additional regulatory measures are brought forward to address gatekeepers with dominant market power, through appropriate regulatory frameworks such as the Digital Markets Act, Digital Services Act or other national laws. </span></p><p id="viewer-1lvj8"><span>EMMA, ENPA, EPC, NME &amp; Microsoft therefore call for an arbitration mechanism to be implemented in European or national law requiring such gatekeepers to pay for press content in full respect of the Publisher‘s Right set out in Directive 2019/790. We welcome proposals made by several Members of the European Parliament to introduce a final arbitration mechanism into relevant regulation. This is needed to prevent undermining the scope of the Publishers’ Right and to create legal certainty. Otherwise, even though press publishers have a neighbouring right, they might not have the economic strength to negotiate fair and balanced agreements with these gatekeeper tech companies, who might otherwise threaten to walk away from negotiations or exit markets entirely. </span></p><p id="viewer-c4u4c"><span><strong>Christian Van Thillo, Chairman of the European Publishers Council </strong>said “We welcome Microsoft’s recognition of the value that our content brings to the core businesses of search engines and social networks because this is where Google and Facebook generate the vast majority of their revenues. It is crucial that our regulators recognise this key point, and don’t get misled into thinking that side deals on the basis of a stand-alone product are the same thing, because they are not at all and undermine the neighbouring rights that we have been granted. All publishers should get an agreement – no one should be left out”. </span></p><p id="viewer-93h7p"><span><strong>Fernando de Yarza, President of News Media Europe </strong>said “The experiences in France and Australia have shown us that there’s a real need for a binding instrument to address inherent imbalances in bargaining power with gatekeepers, which undermine the potential of Europe’s press sector. We look forward to working with Microsoft and others on a solution that allows for a healthy and diverse online news media ecosystem”. </span></p><p id="viewer-71g89"><span><strong>Jean-Pierre de Kerraoul, President of ENPA </strong>said: “Independent journalism is vital to the social cohesion that is essential for democracy. But the internet and social media have not been kind to the free press with most outlets hit hard. A fully functioning and competitive ecosystem will strengthen media pluralism and will ultimately strengthen democratic discourse. Democracy relies on a free press to make it through difficult times. Any legislative proposal that strengthens democracy and supports a free press should be promoted by the technology industry, which is a product of the very same freedoms and values.” </span></p><p id="viewer-q7q0"><span><strong>Xavier Bouckaert, President of EMMA </strong>said: “The DMA or other binding regulation should entail a specific obligation for the gatekeepers to grant all legal publications and offerings non-discriminatory access and fair terms and conditions to their services. This must include an obligation for market dominant platforms to enter into negotiations with all rightsholders of the Publishers’ right and offer fair payment for their content. We therefore welcome today’s commitment, as it covers newspaper and magazine publishers alike.” </span></p><p id="viewer-fspng"><span><strong>Casper Klynge, Vice President, Microsoft</strong>, said “Access to fresh, broad and deep press coverage is critical to the success of our democracies. Our commitment to preserving and promoting journalism isn’t new. In October 2020, we launched a new initiative to invest in and support local media and, through Microsoft News, we have been sharing a large portion of revenue with press publishers. This initiative is a logical next step.” </span></p><p id="viewer-fhtvn"><span>Pdf version of the Press Release, with press contact details, found below. </span></p><div id="viewer-b2pe9"><div><div data-hook="fileUploadViewer"><div role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="40" height="42" viewBox="0 0 40 42"><g fill="none" fill-rule="evenodd"><path d="M0 0H4823V3877H0z" transform="translate(-1717 -3612)"></path><path d="M1717 3612H1757V3654H1717z" transform="translate(-1717 -3612)"></path><g><path d="M13.6 0.9L7.9 8.3 4.9 4.7 0.5 9.9 6.7 9.9 9.2 9.9 20.5 9.9z" transform="translate(-1717 -3612) translate(1720 3613) translate(6 21)"></path><path fill-rule="nonzero" d="M3.3 11.5L0.6 11.5 4.2 5.9 0.8 0.6 3.5 0.6 5.5 4.1 7.5 0.6 10.2 0.6 6.8 5.9 10.4 11.5 7.6 11.5 5.5 7.8z" transform="translate(-1717 -3612) translate(1720 3613) translate(11 19)"></path><path fill-rule="nonzero" d="M18.4 23.2c0-.6-.2-1-.5-1.2-.3-.2-.7-.4-1.3-.4h-2.1v3.2h2.1c.5 0 1-.1 1.3-.4.4-.3.5-.7.5-1.2zm2.3-.1c0 1.3-.3 2.2-1 2.8-.7.5-1.6.8-2.8.8h-2.3v4h-2.3v-11h4.8c1.1 0 2 .3 2.6.9.7.5 1 1.4 1 2.5z" transform="translate(-1717 -3612) translate(1720 3613)"></path><path d="M10.3 5.8L0.2 10.9 0.2 0.7z" transform="translate(-1717 -3612) translate(1720 3613) translate(12 19)"></path><g stroke-linejoin="bevel" stroke-width="1.003"><path fill="currentColor" d="M32.5 41.5L0.5 41.5 0.5 0.5 20 0.5 32.5 13.4z" transform="translate(-1717 -3612) translate(1720 3613)"></path><path d="M19 13.4L31.9 13.4 31.3 12.7 20 1.2 19.4 0.5z" transform="translate(-1717 -3612) translate(1720 3613)"></path></g><g><path d="M.5 4H10.5V5H.5zM.5 0H10.5V1H.5zM5.5 12L10.5 8 .5 8z" transform="translate(-1717 -3612) translate(1720 3613) translate(10.963 19.09)"></path></g></g></g></svg><div><div><p>Press Release 22 February 2021_EMBARGO_F</p><p>. </p></div><p>Download   • 149KB</p></div></div></div></div></div></div></div></div></div></article></div></div>]]>
            </description>
            <link>https://www.epceurope.eu/post/europe-s-press-publishers-microsoft-call-for-australian-style-arbitration-mechanism-in-europe</link>
            <guid isPermaLink="false">hacker-news-small-sites-26227215</guid>
            <pubDate>Mon, 22 Feb 2021 17:24:56 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Totality – FP Explained]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26227055">thread link</a>) | @vrom911
<br/>
February 22, 2021 | https://kowainik.github.io/posts/totality | <a href="https://web.archive.org/web/*/https://kowainik.github.io/posts/totality">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            <div>
              <div>
              <p>Along with the popular Functional Programming (FP) concepts such as purity or immutability, there is a significant one, which couldn’t boast much discussion around it — <strong>totality</strong>. Totality is an exceptionally interesting notion in the FP context. And you probably already use it and are aware of some pitfalls of not having totality in your code, but maybe the terminology doesn’t ring a bell.</p>
<p>In this blog post, we want to provide a comprehensive guide to the concept of totality in Functional Programming by demystifying its meaning, giving a lot of examples, and recommending how to get tools to help you write maintainable, testable code. You will find this blog post helpful if you are interested in understanding the fundamentals of FP.</p>
<blockquote>
<p>Note: we will use Haskell to demonstrate and explain the totality, but the involved concepts apply to any programming language.</p>
</blockquote>
<p>Ready?</p>
<figure>
<img src="https://kowainik.github.io/images/totality/mortal-kompose-start.gif" alt="Press Start">
</figure>
<h2 id="definition">Definition<a href="#definition">🔗</a></h2>
<p><strong>Functions</strong> are core elements of Functional Programming. Functions are expressions that have a type that could be primitive or more complex. We can say that each function has an input, 0 or more arguments of some type and only one output, returning type. In other words, a function transforms its inputs to the output, and the function definition describes what actual work a function does to produce its result.</p>
<p>A function is <strong>total</strong> if it is defined for all inputs of its corresponding type, or in other words, if a function returns the output on any possible values of the input types.</p>
<p>For example, the following function that checks if the given integer is zero is total:</p>
<div id="cb1"><pre><code><span id="cb1-1"><span>isZero ::</span> <span>Integer</span> <span>-&gt;</span> <span>Bool</span></span>
<span id="cb1-2">isZero n <span>=</span> n <span>==</span> <span>0</span></span></code></pre></div>
<p>The above function is defined for any value of type <code>Integer</code>. No matter what it would receive as the argument, it will always return the answer – <code>True</code> or <code>False</code>.</p>
<p>On the other hand, a function like “integral division” is <strong>partial</strong> (non-total). Though the division works perfectly on most of the inputs, the result of division by zero is not defined; therefore, there is an argument on which the function can’t return a reasonable result (and that’s why you may find the <code>isZero</code> function helpful). Partial functions are not defined on all their inputs and usually blow up when given something they cannot handle.</p>
<figure>
<img src="https://kowainik.github.io/images/totality/totality-functional.png" alt="Partial Functions">
</figure>
<h3 id="is-that-math">Is that math?<a href="#is-that-math">🔗</a></h3>
<figure>
<img src="https://kowainik.github.io/images/totality/fp-vs-math.png" alt="FP vs Math">
</figure>
<p>The definition of totality originated in math. It has a similar formulation to what we provided above and could be understood in the same way.</p>
<p>Total function is a function defined for all elements in its domain. The domain is the set of x-values that can be put into a function. In other words, it’s the set of all possible values of the independent variable.</p>
<p>We can notice that the math function domain is the same as the input of our functions.</p>
<p>The following image is the canonical way to represent math functions, mapping values from domain A to values of range B. If it were a programming function, we would say that it maps type A to type B. Both total and partial way could be illustrated in that manner:</p>
<figure>
<img src="https://kowainik.github.io/images/totality/totality-math.png" alt="Totality in math">
</figure>
<p>However, despite all similarities, functions in programming are a bit different because they have a notion of <em>computation</em>.</p>
<p>Math doesn’t consider how long it takes for a function to calculate or how much memory it needs. Moreover, functions in programming can hang, and it is vital to keep this in mind when you write code.</p>
<p>Functions in math also don’t have side-effects, e.g.&nbsp;reading from file. But all these concerns are valid in the context of programming.</p>
<p>To summarise, here is a short list of possible things that functions in programming can do, and math functions cannot:</p>
<ul>
<li>Hang (loop indefinitely or takes unreasonable time to compute)</li>
<li>Throw exceptions</li>
<li>Terminate before producing a result in case of insufficient memory</li>
<li>Have side-effects (read from files, send requests to web services, etc.)</li>
</ul>
<p>Functional Programming is closer to the original math definition in the sense that its functions are pure – they have no side-effects. This essential FP paradigm allows us to talk about the concept of totality in a programming context, even though programming functions are very different from math functions.</p>
<h3 id="termination">Termination*<a href="#termination">🔗</a></h3>
<p><em>advanced section, could be skipped</em></p>
<p>As we look at totality from the programming point of view, we need to describe a very close concept to totality — <strong>termination</strong>. Termination gives the guarantee that function does produce a result in a finite amount of time. Usually, when people talk about <strong>total functional programming</strong>, they mean programming with <em>total</em> functions that <em>terminate successfully</em>.</p>
<hr>
<p>This concept of termination is more relevant to advanced languages called <em>proof-assistants</em> used to write proofs as programming functions. In such languages, successful proof must be a total and terminating function. Proof-assistants are invaluable in the software verification areas.</p>
<p>However, in real-life software, not all functions must be total. For instance, a Read-Eval-Print-Loop (REPL) or a Web Backend are not supposed to terminate eventually on their own. They should run infinitely and respond to requests in a timely manner.</p>
<hr>
<p>The terminology of totality is a bit ambiguous in programming due to the different use-cases. In some places, total functions are required to terminate, while others require only to handle all inputs’ values. We will try to cover the most common definition of totality in this post.</p>
<p>Note as well that totality is not a straightforward and universally-applicable idea. We will make a few common simplifications in this post regarding totality in modern languages. But bear in mind that there are a lot of specifics that need to be kept in view. There are different methods of making functions total and guaranteeing this property for compiled vs interpreted, for typed vs untyped languages.</p>
<hr>
<p>Another aspect — <strong>laziness</strong>; it also affects the work of pure functions in different ways. Infinite functions could be total (with termination notion) due to laziness. For instance, the following recursive function produces an infinite list. And if you’ll try to print the resulting list to the terminal, you will wait indefinitely for this function to finish:</p>
<div id="cb2"><pre><code><span id="cb2-1"><span>multipliedByTwo ::</span> <span>Int</span> <span>-&gt;</span> [<span>Int</span>]</span>
<span id="cb2-2">multipliedByTwo x <span>=</span> x <span>:</span> multipliedByTwo (x <span>*</span> <span>2</span>)</span></code></pre></div>
<p>However, due to laziness, you still can take the first five elements of the list and get the result:</p>
<div id="cb3"><pre><code><span id="cb3-1">ghci<span>&gt;</span> <span>take</span> <span>5</span> (multipliedByTwo <span>3</span>)</span>
<span id="cb3-2">[<span>3</span>, <span>6</span>, <span>12</span>, <span>24</span>, <span>48</span>]</span></code></pre></div>
<hr>
<p>If being absolutely honest, Haskell is not a total functional programming language by default. It has a special value called <em>“bottom”</em> (⊥) that can be passed to any pure function. When such a value is being evaluated, it throws a runtime error. You can use standard Haskell functions <code>undefined</code> or <code>error</code> at the bottom. It means that even the pure and total function <code>isZero</code> we defined above could fail in runtime if used on bottom elements:</p>
<div id="cb4"><pre><code><span id="cb4-1">ghci<span>&gt;</span> isZero <span>undefined</span></span>
<span id="cb4-2"><span>***</span> <span>Exception</span><span>:</span> Prelude.undefined</span>
<span id="cb4-3"><span>CallStack</span> (from <span>HasCallStack</span>)<span>:</span></span>
<span id="cb4-4">  <span>error</span>, called at libraries<span>/</span>base<span>/</span><span>GHC</span><span>/</span>Err.hs<span>:</span><span>79</span><span>:</span><span>14</span> <span>in</span> base<span>:</span><span>GHC.Err</span></span>
<span id="cb4-5">  <span>undefined</span>, called at <span>&lt;</span>interactive<span>&gt;:</span><span>2</span><span>:</span><span>8</span> <span>in</span> interactive<span>:</span><span>Ghci1</span></span>
<span id="cb4-6"></span>
<span id="cb4-7">ghci<span>&gt;</span> isZero (<span>error</span> <span>"I'm a banana, I do what I wanna"</span>)</span>
<span id="cb4-8"><span>***</span> <span>Exception</span><span>:</span> <span>I'm</span> a banana, <span>I</span> <span>do</span> what <span>I</span> wanna</span>
<span id="cb4-9"><span>CallStack</span> (from <span>HasCallStack</span>)<span>:</span></span>
<span id="cb4-10">  <span>error</span>, called at <span>&lt;</span>interactive<span>&gt;:</span><span>3</span><span>:</span><span>9</span> <span>in</span> interactive<span>:</span><span>Ghci1</span></span></code></pre></div>
<p>To design a complete total system, we need to have the input set without bottom (⊥) elements. An example of pure total language is <a href="https://dhall-lang.org/">Dhall</a> — a configuration language where all functions must be pure, total and terminating.</p>
<p>If interested, you can read more about research in total functional programming (see in <a href="#links">Links</a>).</p>
<h2 id="not-totally-total">Not totally total<a href="#not-totally-total">🔗</a></h2>
<figure>
<img src="https://kowainik.github.io/images/totality/Fight.gif" alt="Fight">
</figure>
<p>Usually, in FP, you don’t use the phrase “total function” as, by default, functions are considered to be total. However, this can’t be the case all the time; it would be too easy. There is an opposite concept of <em>totality</em> – <strong>partiality</strong>, which means that a function is <strong>not</strong> defined for all inputs of its type.</p>
<p>Here are a few examples of common partial functions:</p>
<ul>
<li><strong>Taking a list element by index.</strong> The index can be negative or be outside the list bounds, so it’s impossible to get the element in such cases.</li>
<li><strong>Parsing string to an integer.</strong> Not every string represents a valid numeric number, so a parsing function fails in such cases.</li>
<li><strong>Printf-like pretty-printing.</strong> If you specify the formatting with a separate string, it may fail at runtime on a wrong number of arguments or when types of arguments don’t match.</li>
<li><strong>Mathematical functions</strong>: division by zero, square root of a negative number, etc.</li>
<li><strong>Multiplication of matrices.</strong> If the dimensions of the two matrices are not aligned, it is impossible to multiply one matrix with another.</li>
<li>Laziness brings more interesting partiality cases to the table. When you can have infinite lists, functions like <code>sum</code>, <code>sort</code> or <code>reverse</code> become partial because they hang on infinite lists.</li>
</ul>
<h2 id="why-should-we-care">Why should we care?<a href="#why-should-we-care">🔗</a></h2>
<p>As a developer, you have to deal with runtime exceptions all the time. Programming with total functions helps to avoid some of the runtime exceptions by ultimately preventing them from happening in the first place. But, at the same time, it requires time and discipline to write total functions. So you may think that writing total functions is a big price to pay for reducing the number of runtime exceptions since total functions won’t remove all exceptions entirely. But we believe that you actually should care about totality due to the few other perks as well:</p>
<ul>
<li>Even when writing a huge number of unit tests, you still can miss some cases. Total functional programming gives more guarantees about code correctness.</li>
<li>Debugging runtime exceptions can be tedious for developers. But users of buggy products are frustrated even more. Spending more time on making functions total pays off in the long run.</li>
<li>Total functional programming results in more maintainable, modular and composable programming. The composition of total functions is total. It means that you can refactor your code painlessly, split it into smaller and reusable parts, combine different components, and still be confident that it works. While working with partial functions, you may …</li></ul></div></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://kowainik.github.io/posts/totality">https://kowainik.github.io/posts/totality</a></em></p>]]>
            </description>
            <link>https://kowainik.github.io/posts/totality</link>
            <guid isPermaLink="false">hacker-news-small-sites-26227055</guid>
            <pubDate>Mon, 22 Feb 2021 17:14:13 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[F# Units of Measure – A Worked Example]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26226932">thread link</a>) | @dunefox
<br/>
February 22, 2021 | http://stevenpemberton.net/blog/2015/03/11/FSharp-Units-Of-Measure/ | <a href="https://web.archive.org/web/*/http://stevenpemberton.net/blog/2015/03/11/FSharp-Units-Of-Measure/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
            

<p>Have you ever wished that you could have type safe calculations throughout your application?<br>
Well through the use of F# Units of Measure (UoM), now you can!</p>

<p>In this post I will explore the various ways of using F# Units of Measure and the benefits they bring.<br>
Before I started writing this article, I had not used units of measure before. I therefore thought it would be a good idea to aid the learning process by applying them in a small project.
To do this, I decided to put UoM to the test by applying them in a real world example; The calculations required to brew beer.</p>

<p>What follows is a thorough walkthrough of using Units of Measure based upon my experiences while implementing a library of calculations for use in the various stages of brewing beer.<br>
I aim to highlight how, through the use of units of measure, we can increase the robustness of our code and hopefully eliminate potential runtime errors.</p>

<!-- more -->

<h2>Units of Measure - An introduction</h2>

<p>Units of measure in F# are a type of metadata that can be associated with floating point or signed integer values.<br>
By associating a UoM with a quantity value it allows the F# compiler to perform additional type checking on the use of these units, enforcing relationships between units in arithmetic and reducing potential errors.</p>

<p>To declare a unit of measure you use the <code>[&lt;Measure&gt;]</code> attribute, followed by the <code>type</code> keyword and the name we want to give the measure.</p>

<p>For example, we can declare units of measure for some of the measures of volume we will need when calculating the ingredients in beer recipes.</p>

<table><tbody><tr><td><pre><span>1: </span>
<span>2: </span>
<span>3: </span>
<span>4: </span>
<span>5: </span>
</pre>
</td>
<td><pre><span>///Litre (or Liter in the US)</span>
[&lt;<span onmouseout="hideTip(event, 'fs9', 16)" onmouseover="showTip(event, 'fs9', 16)">Measure</span>&gt;] <span>type</span> <span onmouseout="hideTip(event, 'fs13', 17)" onmouseover="showTip(event, 'fs13', 17)">L</span>

<span>///Us Gallon</span>
[&lt;<span onmouseout="hideTip(event, 'fs9', 18)" onmouseover="showTip(event, 'fs9', 18)">Measure</span>&gt;] <span>type</span> <span onmouseout="hideTip(event, 'fs14', 19)" onmouseover="showTip(event, 'fs14', 19)">usGal</span>
</pre>
</td>
</tr>
</tbody></table>

<p>Using a measure is as simple as annotating a float literal.</p>

<table><tbody><tr><td><pre><span>1: </span>
<span>2: </span>
</pre>
</td>
<td><pre><span>//Volume in litres</span>
<span>let</span> <span onmouseout="hideTip(event, 'fs15', 20)" onmouseover="showTip(event, 'fs15', 20)">volume</span> <span>=</span> <span>120.0</span><span>&lt;</span><span onmouseout="hideTip(event, 'fs13', 21)" onmouseover="showTip(event, 'fs13', 21)">L</span><span>&gt;</span>
</pre>
</td>
</tr>
</tbody></table>

<p>A value that has a unit of measure associated with it is said to have a <em>dimension</em> or be <em>dimensioned</em>.</p>

<p>Units of measure can be utilised in a number of ways.</p>

<ul>
<li>To constrain the values involved in calculations to particular measures</li>
<li>To provide type safe conversions between measures</li>
<li>For defining new units of measure in terms of the original</li>
<li>In types to create associations between different measures</li>
</ul>

<p>This is in no way a definitive list. They are just a few of the ways I found measures particularly beneficial during my time exploring there usage.<br>
If anyone has any other useful applications I would love to hear about them.</p>

<h2>Defining units of measure in terms of others</h2>

<p>Sometimes, it can be a useful technique to define a unit of measure in terms of other previously defined measures.<br>
Doing so allows us to use the <em>derived</em> measures in place of inferred results of calculations which can increase code clarity.</p>

<p>Let's take an example from the brewing process.</p>

<p>We often need to associate something called gravity points, with a volume of liquid.<br>
Gravity points are a very simplified definition of the amount of sugar in liquid. Obviously, this liquid could be in any number of units, and it is paramount that we ensure we do not mix measures of volume, for example, during recipe planning.</p>

<p>One common measurement used in home brewing circles is that of points per gallon (or points per pound per gallon - PPG) so let's define a measure for that.<br>
Firstly, we need to define a measure for gravity points.</p>

<table><tbody><tr><td><pre><span>1: </span>
<span>2: </span>
</pre>
</td>
<td><pre><span>///Gravity Point - A Simplified brewing unit for amount of sugar dissolved in solution</span>
[&lt;<span onmouseout="hideTip(event, 'fs9', 22)" onmouseover="showTip(event, 'fs9', 22)">Measure</span>&gt;] <span>type</span> <span onmouseout="hideTip(event, 'fs16', 23)" onmouseover="showTip(event, 'fs16', 23)">gp</span>
</pre>
</td>
</tr>
</tbody></table>

<p>Next up, we define the <em>association</em> between gravity points and US gallons.</p>

<table><tbody><tr><td><pre><span>1: </span>
</pre>
</td>
<td><pre>[&lt;<span onmouseout="hideTip(event, 'fs9', 24)" onmouseover="showTip(event, 'fs9', 24)">Measure</span>&gt;] <span>type</span> <span onmouseout="hideTip(event, 'fs17', 25)" onmouseover="showTip(event, 'fs17', 25)">ppg</span> <span>=</span> <span onmouseout="hideTip(event, 'fs16', 26)" onmouseover="showTip(event, 'fs16', 26)">gp</span> <span>/</span> <span onmouseout="hideTip(event, 'fs14', 27)" onmouseover="showTip(event, 'fs14', 27)">usGal</span>
</pre>
</td>
</tr>
</tbody></table>

<p>Our <code>ppg</code> measure can now be used in our calculations, which we'll get to in a minute.<br>
But first, a few quick points on this type of measure.</p>

<ul>
<li>The formulas that represent the measures can be written in various equivalent ways. This sometimes manifests in the results of expressions being inferred differently than we would expect - more on this later.</li>
<li>Equivalent formulas are compiled into a common representation and can therefore be substituted freely.</li>
<li>You cannot use numeric values in these formulae. However, we can declare conversion constants which we will also explore later.</li>
<li>You can use <code>1</code> in these formulae. <code>1</code> represents a <em>dimensionless</em> value. I will touch on dimensionless values when discussing error prevention in the next section.</li>
</ul>

<p>These points and others are explained in detail on the <a href="https://msdn.microsoft.com/en-us/library/dd233243.aspx">MSDN</a> page for Units of Measure.</p>

<h2>Using Units of Measure for error prevention</h2>

<p>Units of measure come in extremely handy for preventing us introducing errors into our code by using a value with an incorrect unit in a calculation or function.<br>
As an example taken from the world of brewing, we wouldn't want to mix up the units when making calculations about how much grain we need.</p>

<p>Below is an example of a function that can only take values with the specified dimensions.</p>

<table><tbody><tr><td><pre><span>1: </span>
<span>2: </span>
<span>3: </span>
</pre>
</td>
<td><pre><span>///Converts a points per gal (gp / usGal) and volume into total gravity points in that volume</span>
<span>let</span> <span onmouseout="hideTip(event, 'fs18', 28)" onmouseover="showTip(event, 'fs18', 28)">TotalGravityPoints</span> (<span onmouseout="hideTip(event, 'fs19', 29)" onmouseover="showTip(event, 'fs19', 29)">potential</span><span>:</span><span onmouseout="hideTip(event, 'fs20', 30)" onmouseover="showTip(event, 'fs20', 30)">float</span><span>&lt;</span><span onmouseout="hideTip(event, 'fs16', 31)" onmouseover="showTip(event, 'fs16', 31)">gp</span> <span>/</span> <span onmouseout="hideTip(event, 'fs14', 32)" onmouseover="showTip(event, 'fs14', 32)">usGal</span><span>&gt;</span>) (<span onmouseout="hideTip(event, 'fs21', 33)" onmouseover="showTip(event, 'fs21', 33)">vol</span> <span>:</span> <span onmouseout="hideTip(event, 'fs20', 34)" onmouseover="showTip(event, 'fs20', 34)">float</span><span>&lt;</span><span onmouseout="hideTip(event, 'fs14', 35)" onmouseover="showTip(event, 'fs14', 35)">usGal</span><span>&gt;</span>) <span>=</span>  
    <span onmouseout="hideTip(event, 'fs19', 36)" onmouseover="showTip(event, 'fs19', 36)">potential</span> <span>*</span> <span onmouseout="hideTip(event, 'fs21', 37)" onmouseover="showTip(event, 'fs21', 37)">vol</span>
</pre>
</td>
</tr>
</tbody></table>

<p>As you can see, this function is declared with explicit type annotations specifying the dimensions of the parameters.<br>
The F# compiler will now prevent you from using this function with either dimensionless floats, or floats with the the wrong dimension (and of course, non float values).</p>

<p>Consider the following example where we attempt to call the function with dimensionless values:</p>

<table><tbody><tr><td><pre><span>1: </span>
</pre>
</td>
<td><pre><span>let</span> <span onmouseout="hideTip(event, 'fs22', 38)" onmouseover="showTip(event, 'fs22', 38)">totalGravPoints</span> <span>=</span> <span onmouseout="hideTip(event, 'fs18', 39)" onmouseover="showTip(event, 'fs18', 39)">TotalGravityPoints</span> <span>240.0</span> <span>5.0</span>
</pre>
</td>
</tr>
</tbody></table>

<p>Attempting to compile this line of code produces the following error notifying us that we haven't satisfied the type constraints and preventing us from introducing an error into our code.</p>

<div><pre lang="output">error FS0001: This expression was expected to have type
float&lt;gp/usGal&gt;    
    but here has type
float 
</pre></div>

<p>Likewise the compiler will stop us from passing different <code>UoM</code> to the function. 
Suppose we attempted to use a volume in Litres instead of the expected US Gallons.</p>

<p>We receive a similar error.</p>

<table><tbody><tr><td><pre><span>1: </span>
</pre>
</td>
<td><pre><span>let</span> <span onmouseout="hideTip(event, 'fs22', 40)" onmouseover="showTip(event, 'fs22', 40)">totalGravPoints</span> <span>=</span> <span onmouseout="hideTip(event, 'fs18', 41)" onmouseover="showTip(event, 'fs18', 41)">TotalGravityPoints</span> <span>240.0</span><span>&lt;</span><span onmouseout="hideTip(event, 'fs16', 42)" onmouseover="showTip(event, 'fs16', 42)">gp</span> <span>/</span> <span onmouseout="hideTip(event, 'fs14', 43)" onmouseover="showTip(event, 'fs14', 43)">usGal</span><span>&gt;</span> <span>5.0</span><span>&lt;</span><span onmouseout="hideTip(event, 'fs13', 44)" onmouseover="showTip(event, 'fs13', 44)">L</span><span>&gt;</span>
</pre>
</td>
</tr>
</tbody></table>

<div><pre lang="output">error FS0001: Type mismatch. Expecting a
    float&lt;usGal&gt;    
but given a
    float&lt;L&gt;    
The unit of measure 'usGal' does not match the unit of measure 'L'
</pre></div>

<p>This example may be quite contrived, but it highlights the type safety provided by units of measure.
The F# compiler will also prevent us introducing arithmetic errors such as attempting to add/subtract a different or dimensionless unit from another.<br>
For instance, the following would not compile, returning the errors shown.</p>

<table><tbody><tr><td><pre><span>1: </span>
</pre>
</td>
<td><pre><span>let</span> <span onmouseout="hideTip(event, 'fs15', 45)" onmouseover="showTip(event, 'fs15', 45)">volume</span> <span>=</span> <span>5.0</span><span>&lt;</span><span onmouseout="hideTip(event, 'fs14', 46)" onmouseover="showTip(event, 'fs14', 46)">usGal</span><span>&gt;</span> <span>+</span> <span>5.0</span><span>&lt;</span><span onmouseout="hideTip(event, 'fs13', 47)" onmouseover="showTip(event, 'fs13', 47)">L</span><span>&gt;</span>
</pre>
</td>
</tr>
</tbody></table>

<div><pre lang="output">//result
error FS0001: The unit of measure 'L' does not match the unit of measure 'usGal'
</pre></div>

<p>Likewise attempting to use a dimensionless value would also fail.</p>

<table><tbody><tr><td><pre><span>1: </span>
</pre>
</td>
<td><pre><span>let</span> <span onmouseout="hideTip(event, 'fs15', 48)" onmouseover="showTip(event, 'fs15', 48)">volume</span> <span>=</span> <span>5.0</span><span>&lt;</span><span onmouseout="hideTip(event, 'fs14', 49)" onmouseover="showTip(event, 'fs14', 49)">usGal</span><span>&gt;</span> <span>+</span> <span>5.0</span>
</pre>
</td>
</tr>
</tbody></table>

<div><pre lang="output">//result
error FS0001: The type 'float' does not match the type 'float&lt;usGal&gt;'
</pre></div>

<p>A dimensionless value can either be declared simply with no measure, as above, or with the explicit measure of <code>1</code> like so:</p>

<table><tbody><tr><td><pre><span>1: </span>
</pre>
</td>
<td><pre><span>let</span> <span onmouseout="hideTip(event, 'fs15', 50)" onmouseover="showTip(event, 'fs15', 50)">volume</span> <span>=</span> <span>5.0</span><span>&lt;</span><span onmouseout="hideTip(event, 'fs14', 51)" onmouseover="showTip(event, 'fs14', 51)">usGal</span><span>&gt;</span> <span>+</span> <span>5.0</span><span>&lt;</span><span>1</span><span>&gt;</span>
</pre>
</td>
</tr>
</tbody></table>

<div><pre lang="output">//result
error FS0001: The type 'float' does not match the type 'float&lt;usGal&gt;'
</pre></div>

<p>Although we cannot add/subtract different or dimensionless values from an already dimensioned value, we can multiply or divide them.<br>
Multiplying or dividing by a dimensionless value will result in same measure, however using a different measure in the calculation will result in a different (potentially new) measure.</p>

<p>This brings us nicely to our next section.</p>

<h2>Effects of multiplication and division</h2>

<p>By multiplying or dividing a value that either has a measure already, or is dimensionless, we can create new units of measure.<br>
The result of this process is effectively to <em>combine</em> two different units of measure (remember, a dimensionless value can be thought of as having a measure of 1).</p>

<p>We have already declared a unit of measure that can be used to demonstrate this, our <code>ppg</code> measure.<br>
A <code>ppg</code> value is simply a <code>gp</code> value divided by a <code>usGal</code> value.</p>

<table><tbody><tr><td><pre><span>1: </span>
<span>2: </span>
<span>3: </span>
</pre>
</td>
<td><pre><span>let</span> <span onmouseout="hideTip(event, 'fs23', 52)" onmouseover="showTip(event, 'fs23', 52)">totalGravityPoints</span> <span>=</span> <span>240.0</span><span>&lt;</span><span onmouseout="hideTip(event, 'fs16', 53)" onmouseover="showTip(event, 'fs16', 53)">gp</span><span>&gt;</span>
<span>let</span> <span onmouseout="hideTip(event, 'fs24', 54)" onmouseover="showTip(event, 'fs24', 54)">beerVolume</span> <span>=</span> <span>5.0</span><span>&lt;</span><span onmouseout="hideTip(event, 'fs14', 55)" onmouseover="showTip(event, 'fs14', 55)">usGal</span><span>&gt;</span>
<span>let</span> <span onmouseout="hideTip(event, 'fs25', 56)" onmouseover="showTip(event, 'fs25', 56)">pointsPerGallon</span> <span>=</span> <span onmouseout="hideTip(event, 'fs23', 57)" onmouseover="showTip(event, 'fs23', 57)">totalGravityPoints</span> <span>/</span> <span onmouseout="hideTip(event, 'fs24', 58)" onmouseover="showTip(event, 'fs24', 58)">beerVolume</span>
</pre>
</td>
</tr>
</tbody></table>

<p>The value of pointsPerGallon above is just what we would expect.</p>

<div><pre lang="output">val pointsPerGallon : float&lt;gp/usGal&gt; = 48.0
</pre></div>

<p>The exact same principle works for multiplication too and don't forget, two or more units of measure can be considered equal.</p>

<h2>Type inference and measure equality</h2>

<p>Lets take the following function as an example;</p>

<table><tbody><tr><td><pre><span>1: </span>
<span>2: </span>
<span>3: </span>
</pre>
</td>
<td><pre><span>///Calculates the maximum potential gravity points for a given weight of grain with the given potential and target volume</span>
<span>let</span> <span onmouseout="hideTip(event, 'fs26', 59)" onmouseover="showTip(event, 'fs26', 59)">MaxPotentialPoints</span> (<span onmouseout="hideTip(event, 'fs27', 60)" onmouseover="showTip(event, 'fs27', 60)">grainPotential</span><span>:</span><span onmouseout="hideTip(event, 'fs20', 61)" onmouseover="showTip(event, 'fs20', 61)">float</span><span>&lt;</span><span onmouseout="hideTip(event, 'fs16', 62)" onmouseover="showTip(event, 'fs16', 62)">gp</span><span>/</span><span onmouseout="hideTip(event, 'fs11', 63)" onmouseover="showTip(event, 'fs11', 63)">lb</span><span>&gt;</span>) (<span onmouseout="hideTip(event, 'fs28', 64)" onmouseover="showTip(event, 'fs28', 64)">grain</span><span>:</span><span onmouseout="hideTip(event, 'fs20', 65)" onmouseover="showTip(event, 'fs20', 65)">float</span><span>&lt;</span><span onmouseout="hideTip(event, 'fs11', 66)" onmouseover="showTip(event, 'fs11', 66)">lb</span><span>&gt;</span>) (<span onmouseout="hideTip(event, 'fs21', 67)" onmouseover="showTip(event, 'fs21', 67)">vol</span><span>:</span><span onmouseout="hideTip(event, 'fs20', 68)" onmouseover="showTip(event, 'fs20', 68)">float</span><span>&lt;</span><span onmouseout="hideTip(event, 'fs14', 69)" onmouseover="showTip(event, 'fs14', 69)">usGal</span><span>&gt;</span>) <span>=</span> 
    (<span onmouseout="hideTip(event, 'fs27', 70)" onmouseover="showTip(event, 'fs27', 70)">grainPotential</span> <span>*</span> <span onmouseout="hideTip(event, 'fs28', 71)" onmouseover="showTip(event, 'fs28', 71)">grain</span>) <span>/</span> <span onmouseout="hideTip(event, 'fs21', 72)" onmouseover="showTip(event, 'fs21', 72)">vol</span>
</pre>
</td>
</tr>
</tbody></table>

<p>The F# compiler correctly infers that the result of this function is of the type <code>float&lt;gp/usGal&gt;</code> (hover over the function above to see this)</p>

<p>We also know that equivalent measures are interchangeable.<br>
This means, we could alternatively declare this function as returning a <code>&lt;ppg&gt;</code> measure explicitly like so.</p>

<table><tbody><tr><td><pre><span>1: </span>
<span>2: </span>
<span>3: </span>
</pre>
</td>
<td><pre><span>//Explicit return type</span>
<span>let</span> <span onmouseout="hideTip(event, 'fs29', 73)" onmouseover="showTip(event, 'fs29', 73)">MaxPotentialPoints</span> (<span onmouseout="hideTip(event, 'fs27', 74)" onmouseover="showTip(event, 'fs27', 74)">grainPotential</span><span>:</span><span onmouseout="hideTip(event, 'fs20', 75)" onmouseover="showTip(event, 'fs20', 75)">float</span><span>&lt;</span><span onmouseout="hideTip(event, 'fs16', 76)" onmouseover="showTip(event, 'fs16', 76)">gp</span><span>/</span><span onmouseout="hideTip(event, 'fs11', 77)" onmouseover="showTip(event, 'fs11', 77)">lb</span><span>&gt;</span>) (<span onmouseout="hideTip(event, 'fs28', 78)" onmouseover="showTip(event, 'fs28', 78)">grain</span><span>:</span><span onmouseout="hideTip(event, 'fs20', 79)" onmouseover="showTip(event, 'fs20', 79)">float</span><span>&lt;</span><span onmouseout="hideTip(event, 'fs11', 80)" onmouseover="showTip(event, 'fs11', 80)">lb</span><span>&gt;</span>) (<span onmouseout="hideTip(event, 'fs21', 81)" onmouseover="showTip(event, 'fs21', 81)">vol</span><span>:</span><span onmouseout="hideTip(event, 'fs20', 82)" onmouseover="showTip(event, 'fs20', 82)">float</span><span>&lt;</span><span onmouseout="hideTip(event, 'fs14', 83)" onmouseover="showTip(event, 'fs14', 83)">usGal</span><span>&gt;</span>) <span>:</span><span onmouseout="hideTip(event, 'fs20', 84)" onmouseover="showTip(event, 'fs20', 84)">float</span><span>&lt;</span><span onmouseout="hideTip(event, 'fs17', 85)" onmouseover="showTip(event, 'fs17', 85)">ppg</span><span>&gt;</span> <span>=</span> 
    (<span onmouseout="hideTip(event, 'fs27', 86)" onmouseover="showTip(event, 'fs27', 86)">grainPotential</span> <span>*</span> <span onmouseout="hideTip(event, 'fs28', 87)" onmouseover="showTip(event, 'fs28', 87)">grain</span>) <span>/</span> <span onmouseout="hideTip(event, 'fs21', 88)" onmouseover="showTip(event, 'fs21', 88)">vol</span>
</pre>
</td>
</tr>
</tbody></table>

<p>The F# type system will allow us to use either of these functions where the alternative dimension is required (i.e. a <code>ppg</code> where a <code>gp/usGal</code> is expected).
I did find however, that in certain situations, it can make code much clearer to be explicit about the return type.</p>

<p>Consider the following example where we use a <code>pgp</code> measure instead of the <code>gp/lb</code> for the grainPotential:</p>

<table><tbody><tr><td><pre><span>1: </span>
<span>2: </span>
<span>3: </span>
<span>4: </span>
<span>5: </span>
</pre>
</td>
<td><pre><span>///Potential Gravity Points - The number of Gravity points in a lb of a particular malt</span>
[&lt;<span onmouseout="hideTip(event, 'fs9', 89)" onmouseover="showTip(event, 'fs9', 89)">Measure</span>&gt;] <span>type</span> <span onmouseout="hideTip(event, 'fs30', 90)" onmouseover="showTip(event, 'fs30', 90)">pgp</span> <span>=</span> <span onmouseout="hideTip(event, 'fs16', 91)" onmouseover="showTip(event, 'fs16', 91)">gp</span> <span>/</span> <span onmouseout="hideTip(event, 'fs11', 92)" onmouseover="showTip(event, 'fs11', 92)">lb</span>

<span>let</span> <span onmouseout="hideTip(event, 'fs31', 93)" onmouseover="showTip(event, 'fs31', 93)">MaxPotentialPoints</span> (<span onmouseout="hideTip(event, 'fs32', 94)" onmouseover="showTip(event, 'fs32', 94)">grainPotential</span><span>:</span><span onmouseout="hideTip(event, 'fs20', 95)" onmouseover="showTip(event, 'fs20', 95)">float</span><span>&lt;</span><span onmouseout="hideTip(event, 'fs30', 96)" onmouseover="showTip(event, 'fs30', 96)">pgp</span><span>&gt;</span>) (<span onmouseout="hideTip(event, 'fs28', 97)" onmouseover="showTip(event, 'fs28', 97)">grain</span><span>:</span><span onmouseout="hideTip(event, 'fs20', 98)" onmouseover="showTip(event, 'fs20', 98)">float</span><span>&lt;</span><span onmouseout="hideTip(event, 'fs11', 99)" onmouseover="showTip(event, 'fs11', 99)">lb</span><span>&gt;</span>) (<span onmouseout="hideTip(event, 'fs21', 100)" onmouseover="showTip(event, 'fs21', 100)">vol</span><span>:</span><span onmouseout="hideTip(event, 'fs20', 101)" onmouseover="showTip(event, 'fs20', 101)">float</span><span>&lt;</span><span onmouseout="hideTip(event, 'fs14', 102)" onmouseover="showTip(event, 'fs14', 102)">usGal</span><span>&gt;</span>) <span>=</span> 
    (<span onmouseout="hideTip(event, 'fs32', 103)" onmouseover="showTip(event, 'fs32', 103)">grainPotential</span> <span>*</span> <span onmouseout="hideTip(event, 'fs28', 104)" onmouseover="showTip(event, 'fs28', 104)">grain</span>) <span>/</span> <span onmouseout="hideTip(event, 'fs21', 105)" onmouseover="showTip(event, 'fs21', 105)">vol</span>
</pre>
</td>
</tr>
</tbody></table>

<p>If you look at the inferred return type of this function you will see it is <code>float&lt;lb pgp/usGal&gt;</code>.<br>
While this is perfectly correct, it can be confusing.</p>

<p>We can clearly see that the <code>pgp</code> measure is equivalent to that of <code>gp…</code></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://stevenpemberton.net/blog/2015/03/11/FSharp-Units-Of-Measure/">http://stevenpemberton.net/blog/2015/03/11/FSharp-Units-Of-Measure/</a></em></p>]]>
            </description>
            <link>http://stevenpemberton.net/blog/2015/03/11/FSharp-Units-Of-Measure/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26226932</guid>
            <pubDate>Mon, 22 Feb 2021 17:06:07 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Living Like It's 99: No Social Media, No Smartphone]]>
            </title>
            <description>
<![CDATA[
Score 90 | Comments 143 (<a href="https://news.ycombinator.com/item?id=26226864">thread link</a>) | @betaman0
<br/>
February 22, 2021 | https://www.alvarez.io/posts/living-like-it-s-99/ | <a href="https://web.archive.org/web/*/https://www.alvarez.io/posts/living-like-it-s-99/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
				<p><img src="https://www.alvarez.io/img/color/l99.jpg" alt="Robot"></p>
<p>At the time of writing this article, I’ve been living without social media for 3 years and without a smartphone for 2 years. Everything started as an experiment motivated by my privacy concerns. I ended up living like that for an entire different reason: peace of mind. You can find a lot of people on internet that have tried this experiment, from a couple of days to an <a href="https://www.youtube.com/watch?v=B0RVWU_nROk">entire month</a>. However I discovered that the brain dependencies created by social media and smartphones take a lot longer to go away (30 days for me). You can’t really see the effects it has on your life unless you try this kind of experiment for a long time, because you will be stuck in the withdrawal phase that makes you crave dopamine.</p>
<p>Contrary to popular belief, I do not live in a cave where I spend my time coding without any social life, sorry guys ;) I did this experiment while having a busy professional and personal life: I traveled around the world, moved to a new city without knowing anyone, ran <a href="https://www.duple.io/">my own software startup</a>, met new people and made new friends, etc… So it is possible to live your life the same way, or even better, without a smartphone or social media.</p>
<p>I will share with you my experience leaving social media and my smartphone, the tools I replaced them with, some tips and tricks, people’s reactions to my experiment, as well as some funny anecdotes.</p>
<h2 id="lets-start-with-why">Let’s Start With Why</h2>
<h3 id="privacy">Privacy</h3>
<p>The original motivation behind this experiment was privacy. I’m a professional hacker, the things I can do are scary and I’m far from being the only one with these skills. <strong>Smartphones are a dream come true for people like me, little spy devices that are 24/7 on you, remotely accessible from anywhere around the world</strong>. Throw social media into the equation, and you can get inside the head of anybody, and make them do whatever you want. Yes, you should be scared. And that’s even without mentioning all the other <a href="https://lithub.com/what-does-privacy-really-mean-under-surveillance-capitalism/">privacy</a> and <a href="https://the.ink/p/we-can-have-democracy-or-we-can-have">freedom</a> issues that come with <a href="https://techcrunch.com/2019/09/04/facebook-phone-numbers-exposed/">social media</a> and <a href="https://nrkbeta.no/2020/12/03/my-phone-was-spying-on-me-so-i-tracked-down-the-surveillants/">smartphones</a>.</p>
<h3 id="curiosity">Curiosity</h3>
<p>Another reason, which is less dark, was curiosity. I like to experiment and try new things in my life. I was curious about the idea of living without a smartphone and social media especially in a world more connected than ever. And if I didn’t like the experiment, I could always go back to <a href="https://www.meta-nomad.net/avoiding-the-global-lobotomy/">zombieland</a>.</p>
<h3 id="planned-obsolescence">Planned Obsolescence</h3>
<p>The cherry on top was to stop paying each year for a new smartphone, that does nothing more than the previous one, just because the providers decided to <a href="https://en.wikipedia.org/wiki/Planned_obsolescence">sabotage old models</a> so they <a href="https://en.wikipedia.org/wiki/Batterygate">stop working</a>.</p>
<h3 id="peace-of-mind">Peace of mind</h3>
<p>This is for me the most important reason (even though I discovered it afterwards). The positive effects on your mind, being free from social media and smartphones, are incredible. More on it later.</p>

<blockquote>
<p>“Technology has solved old economics problems by giving us new psychological problems.”<br>
Mark Manson, The Subtle Art of Not Giving a F*ck</p>
</blockquote>
<p>In 2018 I deleted my accounts from Twitter, Facebook, Instagram and WhatsApp. No coming back, no temptation to reactivate them later on. I kept LinkedIn on standby for professional use although it came close to being deleted as well. WhatsApp got replaced by <a href="https://signal.org/">Signal</a> because Facebook bought them, plus <a href="https://www.forbes.com/sites/parmyolson/2018/09/26/exclusive-whatsapp-cofounder-brian-acton-gives-the-inside-story-on-deletefacebook-and-why-he-left-850-million-behind/">they’re not really big fan s of privacy</a>.</p>
<p>During that year I kept my smartphone, as I wanted to do the experiment gradually. This decision allowed me to discover something quite counter intuitive about social media and smartphones (more on it later).</p>
<p>From that point on I was reachable by SMS, call, email and Signal. I wasn’t ready for what happened next. Fasten your seatbelts.</p>
<h3 id="people-thought-i-was-dead">People Thought I Was Dead</h3>
<p>The first reaction people had was to think something bad had happened to me, some of them even thought I was dead. Then something socially curious happened: everybody started speaking to each other on Facebook and WhatsApp to try to figure out what was wrong. Some of them even contacted my family multiple times. They all had my phone number, email address and other ways of contacting me. <strong>However, none of them did</strong>. It was like I had exited the matrix, and was living in another reality.</p>
<h3 id="trustworthiness">Trustworthiness</h3>
<p>I was told that I couldn’t be trusted since people can’t check online what I’m doing when I’m not around.</p>
<p>Yeah, you read that right.</p>
<p><strong>Society has been brainwashed to believe that privacy is something criminal. Sorry to disappoint, but privacy is a basic fundament of freedom and democracy. That’s why the voting system is anonymous</strong> [1]. When people tell you “<a href="https://write.privacytools.io/freddy/why-privacy-matters-even-if-you-have-nothing-to-hide">If you have nothing to hide, you have nothing to fear</a>”, what they really mean is “democracy is overrated, get over it”.</p>
<p><em>[1] Privacy: you know who I am but not what I do. Anonymity: You know what I do but not who I am. The voting system uses both, privacy when you go vote, anonymity when they count the results.</em></p>
<h3 id="whatsapp">WhatsApp</h3>
<p>This was for me the biggest problem. <a href="https://seirdy.one/2021/01/27/whatsapp-and-the-domestication-of-users.html">Getting people out of WhatsApp</a> if they wanted to talk to me created a lot of friction. Some of them even stopped texting me because they had to open another app on their phone in order to write to me.</p>
<p>Yeah, you also read that right.</p>
<p>I don’t miss WhatsApp, nor do I miss its endless group talks without anything useful. If you leave one of these groups people look at you as if you did something wrong. In the end I still had access to all the event’s information I needed despite them being organized on WhatsApp. In regards to this, not having the app didn’t change my life much.</p>
<p>A trick I’ve developed, when giving my contact info to new people, is to enter my phone number on their smartphone myself, and install Signal for them. This removed a lot of friction. I would then explain my experiment to them and tell them I can only be contacted via this app. I’ve always had a positive reaction. Everybody’s been curious and asking a lot of questions.</p>
<h3 id="and-then-nothing-happened">And Then Nothing Happened</h3>
<p>During the first weeks without social media, I felt off. As if I was missing out on something big that was happening. Like everybody was having fun except me. Once the <a href="https://joshcsimmons.com/quit-social-media/">withdrawal phase</a> went away, I realized that my life hadn’t changed that much. I was still doing the same things, talking to the same people, going to the same parties, etc… It was just more quiet and peaceful.</p>
<p><strong>I was no longer bombarded with pictures of everybody trying to fake a life they’re not living for the sole purpose of impressing someone else: <a href="https://hbr.org/2017/04/a-new-more-rigorous-study-confirms-the-more-you-use-facebook-the-worse-you-feel">my life had just upgraded</a>.</strong></p>
<p>In the end, after everybody got over their initial shock and calmed down, it became normal for them to contact me using Signal, and life went on as usual.</p>
<h2 id="round-2-goodbye-smartphone">Round 2: Goodbye Smartphone</h2>
<p><img src="https://www.alvarez.io/img/color/l99-2.jpg" alt="Phone"></p>
<p>Unlike social media, smartphones are a lot harder to get rid off. They handle many more things than just simply communicating with people.  I work all day long with a computer, most of what the smartphone was doing could be handled by my laptop. For the rest, I narrowed down my bare essential to Music, Pictures, GPS navigation and of course GSM calls.</p>
<h3 id="the-hardware">The Hardware</h3>
<p>You could solve these problems quite easily using multiple devices, however I wanted to be smart about it and not walk around with a luggage just to carry around all this stuff. After doing some research I figured out the perfect combination and I was even able to reduce the amount of things I had in my pockets.</p>

			</div></div>]]>
            </description>
            <link>https://www.alvarez.io/posts/living-like-it-s-99/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26226864</guid>
            <pubDate>Mon, 22 Feb 2021 17:02:29 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[It's ok to take a walk without headphones]]>
            </title>
            <description>
<![CDATA[
Score 35 | Comments 58 (<a href="https://news.ycombinator.com/item?id=26226862">thread link</a>) | @khehy
<br/>
February 22, 2021 | https://radreads.co/telic/ | <a href="https://web.archive.org/web/*/https://radreads.co/telic/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><h2 itemprop="name"><span itemprop="dateCreated">20 Feb<meta itemprop="interactionCount" content="UserComments: 0"></span> It’s ok to take a walk without headphones</h2><p>The Big Sur MacOS update delivers a delightful <em>Easter Egg</em>.</p><p>Your AirPods now magically follow you across devices. Gone are the awkward transitions (“hold on, let me connect my AirPods”) while fiddling with your Bluetooth settings and pressing that random button on the white case.</p><p>Now you can gracefully glide from podcast, to Zoom call, to Discover Weekly, to Clubhouse, to audiobooks while enlisting Siri’s help. <strong>Seamlessly and without interruption.</strong></p><p>Yet it turns out that our headphones have been following us for much longer than a MacOS update.</p><div><figure><img src="https://i.insider.com/5273e2a669bedd7c06afe99f?width=1100&amp;format=jpeg&amp;auto=webp" alt="Image result for original ipod add" data-old-src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7"></figure></div><p>Remember that original promise <strong>“1,000 songs in your Pocket?”</strong> <em>That was 20 years ago.</em></p><p><strong>“All of humanity’s problems stem from man’s inability to sit quietly in a room alone,”</strong> wrote the 17th century philosopher Blaise Pascal. Yet thanks to these white little earbuds, we never need to spend that quiet time alone.</p><hr><p>Do all of life’s moments need to be productive moments?</p><p>To answer that question, let’s distinguish between two types of activities: <strong>telic </strong>and<strong> atelic.</strong></p><p>Stemming from the Greek term <em>Telos </em>(“Having an inherent purpose”), <strong>telic activities</strong> are directed towards an end goal. Conversely, <strong>atelic</strong> activities are pursued for their own sake.</p><p>Telic activities include writing a novel, learning a new skill and building a house all have <strong>specific outcomes. </strong></p><p>On the other hand atelic activities – going for a walk, a long talk with friends, making love, and listening to your favorite songs – <strong>have no end goal.</strong> You derive joy from the activity itself.</p><p>Here’s the rub. <strong>Atelic activities are not “productive.”</strong></p><p>You do not move yourself closer to your goals when you do a puzzle with your toddler. Or when you pause to observe the beauty of a sunset. <strong>These aren’t productive activities.</strong></p><p>And since my Type-A self finds these activities very uncomfortable, my brain does a little mental <strong><em>jiu jitsu</em></strong> to make them more comfortable. I call them <em>Telic Transformations.</em></p><p>As I watch Soul with the fam, I’m <a href="https://twitter.com/khemaridh/status/1342864974771732480">processing ideas</a> for upcoming blog posts. (And when I watch Toy Story, looking to identify the <em>Hero’s Journey </em>narrative arc.)</p><p>When I surf, I’m constantly thinking of the next maneuver to learn (<em>cutbacks</em>), or the next board I could buy.</p><p>Heck, even when sitting on the John (without an iPhone), I’ll grab one of the cleaning products and look for examples of good copy, logo design, or color pairings.</p><p>As Pascal says, those quiet moments with myself can be <strong>quite uncomfortable.</strong></p><p>And then there’s the ultimate telic transformation: <strong>the podcast.</strong></p><p>Thanks to this venerable audio format, the last bastion of atelic activities (a beach walk, cooking a family dinner, getting your kids to sleep) can become <strong>instantly productive</strong> with the most recent episode of <em>The Tim Ferriss show.</em></p><p>Now this isn’t a critique against learning. Nor one against continuous self-improvement. Or about pursuing one’s insatiable curiosity.</p><p>But isn’t this pull to turn <strong>everything</strong> <strong>into an</strong> <strong>outcome</strong> quite peculiar?</p><p>And here comes a conundrum. Telic activities end. Yet the desire lives on. So we <a href="https://radreads.co/when-then-trap/">move the goal line</a>. Another goal. Another outcome.</p><p>And one starts laying the bricks for the hedonic treadmill.</p><p>In <a href="https://www.newyorker.com/books/page-turner/the-philosophy-of-the-midlife-crisis">The Philosophy of the Midlife Crisis</a>, the philosopher Kieran Setiya writes that “there’s something intrinsically self-defeating about getting things done.” Once you do the thing, it can’t be done again. Setiya continues:</p><blockquote><p><em>“Having a child, writing a book, saving a life—the completion of your project may be of value, but it means that the project can no longer be your guide. In pursuing a goal, you are trying to exhaust your interaction with something good, as if you were to make friends for the sake of saying goodbye.”</em></p></blockquote><p>Setiya concludes that “being consumed by plans” can be problematic:</p><blockquote><p><em>“They are schemes for which success can only mean cessation.”</em></p></blockquote><p>We’re not human doings. We’re human beings. Personally, I suspect that my <em>telic transformations</em> come from a place of fear. The fear of not <em>doing enough</em>, comes from the fear of <em>not being enough</em>. Confusing <a href="https://radreads.co/identity-achievement/">identity and achievement</a> becomes a slippery slope that robs me from the present and the beauty and love that surround me.</p><p>So I’ll heed Pascal’s advice – and ditch the AirPods during my next beach walk.</p> </div></div></div>]]>
            </description>
            <link>https://radreads.co/telic/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26226862</guid>
            <pubDate>Mon, 22 Feb 2021 17:02:19 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How Ahrefs designed content to grow millions in revenue]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26226832">thread link</a>) | @hello123benj
<br/>
February 22, 2021 | https://www.thefxck.com/interviews/how-to-design-seo-content-that-drives-business-growth | <a href="https://web.archive.org/web/*/https://www.thefxck.com/interviews/how-to-design-seo-content-that-drives-business-growth">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Kicking off season two with a bang, we've got Tim Soulo, CMO of Ahrefs.com.</p><p>‍</p><p>In this episode, we cover:</p><p>• Growing the Ahrefs blog to 1.5 million monthly visitors</p><p>• Why traffic doesn't matter (and why Ahrefs has never used Google Analytics or retargeting pixels)</p><p>• How to design content that drives sign ups (and why you shouldn't waste your time on anything else)</p><p>• How to rank #1 in Google (like Ahrefs does for everything)</p><p>• What type of content Ahrefs writes and why it works so well</p><p>• How to make 'you' focused content that also adds a lot of value</p><p>• What are the limitations to this approach?</p><p>• How do Ahrefs promote their content? How do they build backlinks?</p><p>• What are the six ways content marketing at Ahrefs contributes to revenue growth?</p><p>• Are backlinks really important? If so, why?</p><p>‍</p><h3>Join the How the Fxck community to get instant access to this episode summarised into a PDF playbook. <a href="https://www.thefxck.com/playbooks/tim-soulo-content-marketing">Download here</a>.</h3><p>‍</p><blockquote>"You can be successful with a few simple fundamentals and by getting rid of non-essential stuff. Create a product that's genuinely useful to people. Forget growth hacks, make your product indispensable."</blockquote><h3>Resources mentioned</h3><p><a href="https://ahrefs.com/academy/blogging-for-business">Blogging for business</a> by Ahrefs</p><p><a target="_blank" href="https://www.amazon.com/gp/product/1451686587/ref=as_li_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1451686587&amp;linkCode=as2&amp;tag=howthefxckus-20&amp;linkId=d5202612b6821704bdf7b1f534b8a1eb"><img src="https://ws-na.amazon-adsystem.com/widgets/q?_encoding=UTF8&amp;MarketPlace=US&amp;ASIN=1451686587&amp;ServiceVersion=20070822&amp;ID=AsinImage&amp;WS=1&amp;Format=_SL250_&amp;tag=howthefxckus-20"></a></p><p>‍</p><h3>Watch on YouTube</h3><p><iframe width="560" height="315" src="https://www.youtube.com/embed/TPnbdK7kmjM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></p></div></div>]]>
            </description>
            <link>https://www.thefxck.com/interviews/how-to-design-seo-content-that-drives-business-growth</link>
            <guid isPermaLink="false">hacker-news-small-sites-26226832</guid>
            <pubDate>Mon, 22 Feb 2021 17:00:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Abundant Capital]]>
            </title>
            <description>
<![CDATA[
Score 348 | Comments 183 (<a href="https://news.ycombinator.com/item?id=26226723">thread link</a>) | @tomhoward
<br/>
February 22, 2021 | https://blog.aaronkharris.com/abundant-capital | <a href="https://web.archive.org/web/*/https://blog.aaronkharris.com/abundant-capital">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
    
      <div><p>The venture capital industry was built on the premise that both capital and high quality companies are scarce. For most of the history of the industry, this has been true. I remember sitting at demo day in 2011 and marveling at the fact that the combined capital of all the VCs in the room was less than that controlled by the hedge fund at which I had worked. But the model is wrong. Venture capital is abundant, and that fact should fundamentally change how founders fundraise.<br></p><p>This scarcity model has shaped the structure of startups and VCs - most of what an early stage startup does is designed to convince a VC to invest. Companies treat VCs as a limited resource that is both hard to access and hard to convince. Investors do their best to perpetuate this idea because it allows them to retain control of the pitch and fund dynamic.[1]</p><p>Something interesting happens, though, whenever a company has a signifier of quality - a YC demo day slot, a high quality angel, pedigreed founders, or, even better, strong growth. In these cases, there are investor feeding frenzies, leading to oversubscribed rounds, ever climbing prices, and investors willing to accept ownership targets they - until recently - would have termed unacceptable.</p><p>To be sure, there have always been bidding wars in private equity (of which venture is a subset), but these bidding wars are so frequent now as to be approaching the norm. If capital was actually scarce, this wouldn’t happen, there wouldn’t be enough money to create so many bidding wars.[2]</p><p>Bidding wars aren’t the only evidence of capital abundance. The VCs are changing their businesses because of this abundance, whether or not they admit the reason. The evidence is in the new funds that seem to launch on a daily basis, the multi-billion dollar growth funds that have become increasingly common, and the ownership targets at various rounds that continue to drop.</p><p>At the same time that capital has become more abundant, founders have become smarter about fundraising. There are now a huge number of blogs, classes, essays, guides, and advisers ready to help founders navigate the previously opaque world of fundraising. As a result, founders can approach each funding event with a clear plan of how to run a process. Running an orderly process further increases the chances that a company will see competitive bids.</p><p>As a thought experiment, assume that the abundance model is here to stay. It is also safe to assume that founders will not suddenly forget their newfound knowledge about process. I think this should encourage founders to think about changing fundraising in a few major ways:</p><ol>
<li><p>Founders should approach every fundraising as an auction. This is what each process already is, but the auction is inefficient. There’s lots of language and pseudo-moral arguments about why this is bad, but most of those fall apart if capital is abundant.</p></li>
<li><p>Founders should expand their funnels beyond the traditional VCs. These VCs hold a marketing and branding advantage, much of which is built around the signal to later rounds. If, however, each round is an auction, this benefit evaporates. YC’s demo day proved this funnel expansion works at seed, and there’s no logical reason it should fail at later rounds.</p></li>
<li><p>Once a founder has the information produced by this process, she can decide whether to minimize dilution, maximize price, or optimize around the partner. The answer will change based on the situation, but having access to the choice is important.</p></li>
</ol><p>Founders are hesitant to run this model because they fear that running an auction will create a negative quality signal. Investors encourage this belief because it allows them to keep deal flow proprietary. This is flawed logic. The quality of a company can’t be determined by the investors to whom that company talks when raising money. The quality of a company is determined by whether or not the company is good, and good companies should take advantage of abundant capital markets.[3]</p><p><i>Thanks to Adora Cheung, Janelle Tam, Ilya Sukhar, and Nabeel Hyatt for helping me think this through, even though our conclusions might differ.<br></i></p><p>__</p><p>[1] Perhaps more importantly to the investors’ business model is that this dynamic creates a reason for the existence of VCs. If founders and LPs both internalized how non-scarce capital actually is, they could find one another directly, bypassing VCs.</p><p>[2] It’s important to remember that, even though capital is abundant, it remains unevenly distributed. There are companies that struggle to raise money - some of these may be bad investments, but many are good. This is a problem of access rather than capacity, which is a whole different issue.</p><p>[3] When a company IPOs, it opens ownership up to anyone who can afford a share. Imagine, for a second, an investor arguing that this is a sign of low quality.</p></div>
    
  </div></div>]]>
            </description>
            <link>https://blog.aaronkharris.com/abundant-capital</link>
            <guid isPermaLink="false">hacker-news-small-sites-26226723</guid>
            <pubDate>Mon, 22 Feb 2021 16:54:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: I made a reader for HN with Angular]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 2 (<a href="https://news.ycombinator.com/item?id=26226439">thread link</a>) | @izquiratops
<br/>
February 22, 2021 | https://izquiratops.github.io/hacker-reader/ | <a href="https://web.archive.org/web/*/https://izquiratops.github.io/hacker-reader/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://izquiratops.github.io/hacker-reader/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26226439</guid>
            <pubDate>Mon, 22 Feb 2021 16:33:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Haskell Database Implementation – Part 2, Domain Specific Language and Transact]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26225863">thread link</a>) | @todsacerdoti
<br/>
February 22, 2021 | https://dfithian.github.io/2021/02/18/database-implementation-part-2.html | <a href="https://web.archive.org/web/*/https://dfithian.github.io/2021/02/18/database-implementation-part-2.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p><span>This site uses third party cookies and scripts to improve the functionality of this website.</span><a id="cookie-notice-accept">Approve</a><a href="https://dfithian.github.io/faq/#cookies">More Info</a></p><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>This post is part 2 in a series on database implementation, part 1 is
<a href="https://dfithian.github.io/2021/02/15/database-implementation-part-1.html">here</a>.</p>

<p>In the last post I wrote about creating an underlying tree structure. This post is about creating a DSL and managing
transactions.</p>

<p>If you’d like to skip ahead and read the code, it’s <a href="https://github.com/dfithian/dfdb">here</a>.</p>

<p>I could write an entire post on parsing, but I’ll leave that for another day. For now, we’ll assume that we can parse
all user input into our domain specific language.</p>

<h2 id="the-dsl">The DSL</h2>

<p>In general, I wanted a DSL that could add and remove tables, and read and write data, and create and use indexes.</p>

<div><div><pre><code><span>newtype</span> <span>TableName</span> <span>=</span> <span>TableName</span> <span>{</span> <span>unTableName</span> <span>::</span> <span>Text</span> <span>}</span>

<span>data</span> <span>AtomType</span>
  <span>=</span> <span>AtomTypeInt</span>
  <span>|</span> <span>AtomTypeString</span>
  <span>|</span> <span>AtomTypeBool</span>

<span>data</span> <span>Atom</span>
  <span>=</span> <span>AtomInt</span> <span>Int</span>
  <span>|</span> <span>AtomString</span> <span>Text</span>
  <span>|</span> <span>AtomBool</span> <span>Bool</span>

<span>newtype</span> <span>Row</span> <span>=</span> <span>Row</span> <span>{</span> <span>unRow</span> <span>::</span> <span>[</span><span>Atom</span><span>]</span> <span>}</span>

<span>newtype</span> <span>ColumnName</span> <span>=</span> <span>ColumnName</span> <span>{</span> <span>unColumnName</span> <span>::</span> <span>Text</span> <span>}</span>

<span>newtype</span> <span>IndexName</span> <span>=</span> <span>IndexName</span> <span>{</span> <span>unIndexName</span> <span>::</span> <span>Text</span> <span>}</span>

<span>data</span> <span>WhereClause</span> <span>=</span> <span>WhereClause</span>
  <span>{</span> <span>_whereClauseColumn</span> <span>::</span> <span>ColumnName</span>
  <span>,</span> <span>_whereClauseValue</span>  <span>::</span> <span>Atom</span>
  <span>}</span>

<span>data</span> <span>ColumnDefinition</span> <span>=</span> <span>ColumnDefinition</span>
  <span>{</span> <span>_columnDefinitionName</span> <span>::</span> <span>ColumnName</span>
  <span>,</span> <span>_columnDefinitionType</span> <span>::</span> <span>AtomType</span>
  <span>}</span>

<span>data</span> <span>Statement</span>
  <span>=</span> <span>StatementSelect</span> <span>[</span><span>ColumnName</span><span>]</span> <span>TableName</span> <span>[</span><span>WhereClause</span><span>]</span>
  <span>|</span> <span>StatementInsert</span> <span>Row</span> <span>TableName</span>
  <span>|</span> <span>StatementCreate</span> <span>TableName</span> <span>[</span><span>ColumnDefinition</span><span>]</span>
  <span>|</span> <span>StatementCreateIndex</span> <span>IndexName</span> <span>TableName</span> <span>[</span><span>ColumnName</span><span>]</span>
  <span>|</span> <span>StatementDrop</span> <span>TableName</span>
  <span>|</span> <span>StatementDropIndex</span> <span>IndexName</span>
</code></pre></div></div>

<p>I made a few important decisions for simplicity’s sake:</p>

<ol>
  <li>A <code>SELECT</code> statement filters only using <code>AND</code>, and all comparisons must use equality</li>
  <li><code>INSERT</code> statements must specify every columnar value matching the order of the columns in the internal store</li>
  <li><code>DELETE</code> and <code>UPDATE</code> are not implemented</li>
</ol>

<p>I’m sure there’s a better way to enforce type safety internally than using <code>Atom</code> and <code>AtomType</code> but because I was
moving fast I didn’t spend too much time on it.</p>

<h2 id="the-database-state">The database state</h2>

<div><div><pre><code><span>newtype</span> <span>PrimaryKey</span> <span>=</span> <span>PrimaryKey</span> <span>{</span> <span>unPrimaryKey</span> <span>::</span> <span>Int</span> <span>}</span>

<span>data</span> <span>Table</span> <span>=</span> <span>Table</span>
  <span>{</span> <span>_tableName</span>           <span>::</span> <span>TableName</span>
  <span>,</span> <span>_tableDefinition</span>     <span>::</span> <span>[</span><span>ColumnDefinition</span><span>]</span>
  <span>,</span> <span>_tableRows</span>           <span>::</span> <span>TreeMap</span> <span>PrimaryKey</span> <span>Row</span>
  <span>,</span> <span>_tableNextPrimaryKey</span> <span>::</span> <span>PrimaryKey</span>
  <span>,</span> <span>_tableIndices</span>        <span>::</span> <span>[</span><span>IndexName</span><span>]</span>
  <span>}</span>

<span>data</span> <span>Index</span> <span>=</span> <span>Index</span>
  <span>{</span> <span>_indexName</span>    <span>::</span> <span>IndexName</span>
  <span>,</span> <span>_indexTable</span>   <span>::</span> <span>TableName</span>
  <span>,</span> <span>_indexColumns</span> <span>::</span> <span>[</span><span>ColumnName</span><span>]</span>
  <span>,</span> <span>_indexData</span>    <span>::</span> <span>TreeMap</span> <span>[</span><span>Atom</span><span>]</span> <span>[</span><span>Row</span><span>]</span>
  <span>}</span>

<span>data</span> <span>Database</span> <span>=</span> <span>Database</span>
  <span>{</span> <span>_databaseTables</span>  <span>::</span> <span>Map</span> <span>TableName</span> <span>Table</span>
  <span>,</span> <span>_databaseIndices</span> <span>::</span> <span>Map</span> <span>IndexName</span> <span>Index</span>
  <span>}</span>
</code></pre></div></div>

<p>A <code>Table</code> consists of a name and definition, plus the actual data, and some helpers like the next primary key and the
names of the indexes defined on this table.</p>

<p>An <code>Index</code> refers to a subset of columns on a table, and, for simplicity, duplicates the data in the table (stores
<code>[Row]</code>) instead of using pointers.</p>

<p>A <code>Database</code> consists of tables and indexes.</p>

<h2 id="transactionality">Transactionality</h2>

<p>Having specified the DSL for the database, I was interested in how transactions on the database would work. Enumerating
some of the key features of transactions allowed me to investigate which ones I wanted to implement.</p>

<ol>
  <li>Primitive operations like <code>BEGIN</code>, <code>COMMIT</code>, and <code>ROLLBACK</code></li>
  <li>Concurrency, and relatedly, isolation levels</li>
</ol>

<p>Because I had already made the decision to keep the database in memory in part 1, concurrency and isolation levels
didn’t make much sense to implement. Instead I focused on primitive operations after implementing autocommit.</p>

<h3 id="naive-autocommit-implementation">Naive autocommit implementation</h3>

<p>My first pass on transactionality was to implement autocommit. This was helpful in the case where a table had one or
more indexes that needed to be updated during an insert, and it provided a way to abstract transactions from the
underlying code.</p>

<div><div><pre><code><span>data</span> <span>StatementFailureCode</span>
  <span>=</span> <span>StatementFailureCodeSyntaxError</span> <span>Text</span>
  <span>|</span> <span>StatementFailureCodeInternalError</span> <span>Text</span>

<span>newtype</span> <span>Transaction</span> <span>a</span> <span>=</span> <span>Transaction</span> <span>(</span><span>StateT</span> <span>Database</span> <span>(</span><span>Except</span> <span>StatementFailureCode</span><span>)</span> <span>a</span><span>)</span>
  <span>deriving</span> <span>(</span><span>Functor</span><span>,</span> <span>Applicative</span><span>,</span> <span>Monad</span><span>)</span>

<span>type</span> <span>MonadDatabase</span> <span>m</span> <span>=</span> <span>(</span><span>MonadState</span> <span>Database</span> <span>m</span><span>,</span> <span>MonadError</span> <span>StatementFailureCode</span> <span>m</span><span>)</span>

<span>runTransaction</span> <span>::</span> <span>(</span><span>MonadState</span> <span>Database</span> <span>m</span><span>)</span> <span>=&gt;</span> <span>Transaction</span> <span>a</span> <span>-&gt;</span> <span>m</span> <span>(</span><span>Either</span> <span>StatementFailureCode</span> <span>a</span><span>)</span>
<span>runTransaction</span> <span>(</span><span>Transaction</span> <span>mx</span><span>)</span> <span>=</span> <span>do</span>
  <span>pre</span> <span>&lt;-</span> <span>get</span>
  <span>let</span> <span>result</span> <span>=</span> <span>runExcept</span> <span>$</span> <span>runStateT</span> <span>mx</span> <span>pre</span>
  <span>traverse</span> <span>(</span><span>\</span><span>(</span><span>out</span><span>,</span> <span>post</span><span>)</span> <span>-&gt;</span> <span>put</span> <span>post</span> <span>&gt;&gt;</span> <span>pure</span> <span>out</span><span>)</span> <span>result</span>
</code></pre></div></div>

<p>As an example, execute any sequence of statements using this underlying monad:</p>

<div><div><pre><code><span>newtype</span> <span>Output</span> <span>=</span> <span>Output</span> <span>{</span> <span>unOutput</span> <span>::</span> <span>Text</span> <span>}</span>

<span>execute</span> <span>::</span> <span>MonadDatabase</span> <span>m</span> <span>=&gt;</span> <span>Statement</span> <span>-&gt;</span> <span>m</span> <span>Output</span>
<span>execute</span> <span>=</span> <span>\</span><span>case</span>
  <span>StatementSelect</span> <span>cols</span> <span>tableName</span> <span>wheres</span> <span>-&gt;</span> <span>...</span>
  <span>StatementInsert</span> <span>row</span> <span>tableName</span> <span>-&gt;</span> <span>...</span>
  <span>StatementCreate</span> <span>tableName</span> <span>cols</span> <span>-&gt;</span> <span>...</span>
  <span>StatementCreateIndex</span> <span>indexName</span> <span>tableName</span> <span>cols</span> <span>-&gt;</span> <span>...</span>
  <span>StatementDrop</span> <span>tableName</span> <span>-&gt;</span> <span>...</span>
  <span>StatementDropIndex</span> <span>indexName</span> <span>-&gt;</span> <span>...</span>
</code></pre></div></div>

<p>And run it:</p>

<div><div><pre><code>  <span>runTransaction</span> <span>(</span><span>Transaction</span> <span>(</span><span>execute</span> <span>statement</span><span>))</span> <span>&gt;&gt;=</span> <span>\</span><span>case</span>
    <span>Right</span> <span>output</span> <span>-&gt;</span> <span>putStrLn</span> <span>$</span> <span>unOutput</span> <span>output</span>
    <span>Left</span> <span>code</span> <span>-&gt;</span> <span>case</span> <span>code</span> <span>of</span>
      <span>StatementFailureCodeSyntaxError</span> <span>err</span> <span>-&gt;</span> <span>putStrLn</span> <span>err</span>
      <span>StatementFailureCodeInternalError</span> <span>err</span> <span>-&gt;</span> <span>putStrLn</span> <span>err</span>
</code></pre></div></div>

<p>While autocommit is simple and prevents database corruption, it doesn’t provide a basic feature set, namely the
primitives <code>BEGIN</code>, <code>ROLLBACK</code>, or <code>COMMIT</code>.</p>

<h3 id="less-naive-implementation">Less naive implementation</h3>

<h4 id="transaction-lifecycle">Transaction lifecycle</h4>

<p>In order to implement these underlying primitives, I added constructors to the <code>Statement</code> DSL, created a state machine
for a transaction, and further abstracted the database away from the interpreter.</p>

<div><div><pre><code><span>-- Same as before, plus three operations</span>
<span>data</span> <span>Statement</span>
  <span>...</span>
  <span>|</span> <span>StatementBegin</span>
  <span>|</span> <span>StatementCommit</span>
  <span>|</span> <span>StatementRollback</span>

<span>data</span> <span>TransactionStatus</span>
  <span>=</span> <span>TransactionStatusBegin</span>
  <span>|</span> <span>TransactionStatusAborted</span>
  <span>|</span> <span>TransactionStatusCommit</span>
  <span>|</span> <span>TransactionStatusRollback</span>

<span>data</span> <span>TransactionalDatabase</span> <span>=</span> <span>TransactionalDatabase</span>
  <span>{</span> <span>_transactionalDatabaseLastSavepoint</span> <span>::</span> <span>Database</span>
  <span>,</span> <span>_transactionalDatabaseInner</span>         <span>::</span> <span>Maybe</span> <span>(</span><span>TransactionStatus</span><span>,</span> <span>Database</span><span>)</span>
  <span>}</span>
</code></pre></div></div>

<p>The interpreter still operated on a <code>Database</code>, but <em>which</em> database is determined by whether or not there’s a currently
executing transaction. The transaction runner changed to read the status and perform the appropriate operations. The
interpreter was modified to change the transaction status based on which <code>Statement</code> constructor was passed in.</p>

<div><div><pre><code><span>newtype</span> <span>Transaction</span> <span>a</span> <span>=</span> <span>Transaction</span> <span>(</span><span>StateT</span> <span>(</span><span>Database</span><span>,</span> <span>Maybe</span> <span>TransactionStatus</span><span>)</span> <span>(</span><span>Except</span> <span>StatementFailureCode</span><span>)</span> <span>a</span><span>)</span>
  <span>deriving</span> <span>(</span><span>Functor</span><span>,</span> <span>Applicative</span><span>,</span> <span>Monad</span><span>)</span>

<span>type</span> <span>MonadDatabase</span> <span>m</span> <span>=</span> <span>(</span><span>MonadState</span> <span>(</span><span>Database</span><span>,</span> <span>Maybe</span> <span>TransactionStatus</span><span>)</span> <span>m</span><span>,</span> <span>MonadError</span> <span>StatementFailureCode</span> <span>m</span><span>)</span>
</code></pre></div></div>

<p>The autocommit branch works mostly like it used to, modifying the last savepoint, but will also detect changes to the
transaction status and initialize the transaction.</p>

<div><div><pre><code><span>runAutocommit</span> <span>::</span> <span>(</span><span>MonadState</span> <span>DFDB</span><span>.</span><span>Types</span><span>.</span><span>TransactionalDatabase</span> <span>m</span><span>)</span> <span>=&gt;</span> <span>DFDB</span><span>.</span><span>Types</span><span>.</span><span>Database</span> <span>-&gt;</span> <span>Transaction</span> <span>a</span> <span>-&gt;</span> <span>m</span> <span>(</span><span>Either</span> <span>DFDB</span><span>.</span><span>Types</span><span>.</span><span>StatementFailureCode</span> <span>a</span><span>)</span>
<span>runAutocommit</span> <span>pre</span> <span>(</span><span>Transaction</span> <span>mx</span><span>)</span> <span>=</span> <span>case</span> <span>runExcept</span> <span>$</span> <span>runStateT</span> <span>mx</span> <span>(</span><span>pre</span><span>,</span> <span>Nothing</span><span>)</span> <span>of</span>
  <span>Left</span> <span>err</span> <span>-&gt;</span> <span>pure</span> <span>$</span> <span>Left</span> <span>err</span>
  <span>Right</span> <span>(</span><span>out</span><span>,</span> <span>(</span><span>post</span><span>,</span> <span>postStatusMay</span><span>))</span> <span>-&gt;</span> <span>do</span>
    <span>case</span> <span>postStatusMay</span> <span>of</span>
      <span>Nothing</span> <span>-&gt;</span> <span>assign</span> <span>DFDB</span><span>.</span><span>Types</span><span>.</span><span>transactionalDatabaseLastSavepoint</span> <span>post</span>
      <span>Just</span> <span>postStatus</span> <span>-&gt;</span> <span>do</span>
        <span>put</span> <span>DFDB</span><span>.</span><span>Types</span><span>.</span><span>TransactionalDatabase</span>
          <span>{</span> <span>_transactionalDatabaseLastSavepoint</span> <span>=</span> <span>pre</span>
          <span>,</span> <span>_transactionalDatabaseInner</span> <span>=</span> <span>Just</span> <span>(</span><span>postStatus</span><span>,</span> <span>post</span><span>)</span>
          <span>}</span>
    <span>pure</span> <span>$</span> <span>Right</span> <span>out</span>
</code></pre></div></div>

<p>The transaction runner branch passes in the transient inner database, reverts when rolled back, and overwrites the last
savepoint when committed.</p>

<div><div><pre><code><span>runInner</span> <span>::</span> <span>(</span><span>MonadState</span> <span>TransactionalDatabase</span> <span>m</span><span>)</span> <span>=&gt;</span> <span>TransactionStatus</span> <span>-&gt;</span> <span>Database</span> <span>-&gt;</span> <span>Transaction</span> <span>a</span> <span>-&gt;</span> <span>m</span> <span>(</span><span>Either</span> <span>StatementFailureCode</span> <span>a</span><span>)</span>
<span>runInner</span> <span>preStatus</span> <span>pre</span> <span>(</span><span>Transaction</span> <span>mx</span><span>)</span> <span>=</span> <span>case</span> <span>runExcept</span> <span>$</span> <span>runStateT</span> <span>mx</span> <span>(</span><span>pre</span><span>,</span> <span>Just</span> <span>preStatus</span><span>)</span> <span>of</span>
  <span>Left</span> <span>err</span> <span>-&gt;</span> <span>do</span>
    <span>assign</span> <span>(</span><span>transactionalDatabaseInner</span> <span>.</span> <span>_Just</span> <span>.</span> <span>_1</span><span>)</span> <span>TransactionStatusAborted</span>
    <span>pure</span> <span>$</span> <span>Left</span> <span>err</span>
  <span>Right</span> <span>(</span><span>out</span><span>,</span> <span>(</span><span>post</span><span>,</span> <span>postStatusMay</span><span>))</span> <span>-&gt;</span> <span>do</span>
    <span>case</span> <span>postStatusMay</span> <span>of</span>
      <span>Nothing</span> <span>-&gt;</span> <span>put</span> <span>TransactionalDatabase</span>
        <span>{</span> <span>_transactionalDatabaseLastSavepoint</span> <span>=</span> <span>post</span>
        <span>,</span> <span>_transactionalDatabaseInner</span> <span>=</span> <span>Nothing</span>
        <span>}</span>
      <span>Just</span> <span>TransactionStatusBegin</span> <span>-&gt;</span> <span>assign</span> <span>(</span><span>transactionalDatabaseInner</span> <span>.</span> <span>_Just</span> <span>.</span> <span>_2</span><span>)</span> <span>post</span>
      <span>Just</span> <span>TransactionStatusAborted</span> <span>-&gt;</span> <span>pure</span> <span>()</span>
      <span>Just</span> <span>TransactionStatusCommit</span> <span>-&gt;</span> <span>put</span> <span>TransactionalDatabase</span>
        <span>{</span> <span>_transactionalDatabaseLastSavepoint</span> <span>=</span> <span>post</span>
        <span>,</span> <span>_transactionalDatabaseInner</span> <span>=</span> <span>Nothing</span>
        <span>}</span>
      <span>Just</span> <span>TransactionStatusRollback</span> <span>-&gt;</span> <span>assign</span> <span>transactionalDatabaseInner</span> <span>Nothing</span>
    <span>pure</span> <span>$</span> <span>Right</span> <span>out</span>
</code></pre></div></div>

<p>Finally, <code>runTransaction</code> branches based on whether there’s a currently executing transaction.</p>

<div><div><pre><code><span>runTransaction</span> <span>::</span> <span>(</span><span>MonadState</span> <span>.</span><span>TransactionalDatabase</span> <span>m</span><span>)</span> <span>=&gt;</span> <span>Transaction</span> <span>a</span> <span>-&gt;</span> <span>m</span> <span>(</span><span>Either</span> <span>StatementFailureCode</span> <span>a</span><span>)</span>
<span>runTransaction</span> <span>tx</span> <span>=</span> <span>do</span>
  <span>pre</span> <span>&lt;-</span> <span>get</span>
  <span>case</span> <span>view</span> <span>transactionalDatabaseInner</span> <span>pre</span> <span>of</span>
    <span>Nothing</span> <span>-&gt;</span> <span>runAutocommit</span> <span>(</span><span>view</span> <span>transactionalDatabaseLastSavepoint</span> <span>pre</span><span>)</span> <span>tx</span>
    <span>Just</span> <span>(</span><span>status</span><span>,</span> <span>innerPre</span><span>)</span> <span>-&gt;</span> <span>runInner</span> <span>status</span> <span>innerPre</span> <span>tx</span>
</code></pre></div></div>

<h4 id="transaction-interpretation">Transaction interpretation</h4>

<p>The <code>execute</code> function, also known as the interpreter, added three branches. The branches enforce the state machine
transitions for <code>TransactionStatus</code>, and otherwise allow the transaction runner to handle success and failure.</p>

<div><div><pre><code><span>-- Same as before, plus three branches</span>
<span>execute</span> <span>::</span> <span>MonadDatabase</span> <span>m</span> <span>=&gt;</span> <span>Statement</span> <span>-&gt;</span> <span>m</span> <span>Output</span>
<span>execute</span> <span>=</span> <span>\</span><span>case</span>
  <span>...</span>
  <span>StatementBegin</span> <span>-&gt;</span> <span>do</span>
    <span>whenM</span> <span>(</span><span>uses</span> <span>_2</span> <span>(</span><span>has</span> <span>_Just</span><span>))</span> <span>$</span>
      <span>throwError</span> <span>$</span> <span>StatementFailureCodeInternalError</span> <span>"Already in a transaction"</span>
    <span>assign</span> <span>_2</span> <span>(</span><span>Just</span> <span>TransactionStatusBegin</span><span>)</span>
    <span>pure</span> <span>$</span> <span>Output</span> <span>"BEGIN"</span>

  <span>StatementCommit</span> <span>-&gt;</span> <span>do</span>
    <span>use</span> <span>_2</span> <span>&gt;&gt;=</span> <span>\</span> <span>case</span>
      <span>Nothing</span> <span>-&gt;</span> <span>throwError</span> <span>$</span> <span>StatementFailureCodeInternalError</span> <span>"Not in a transaction"</span>
      <span>Just</span> <span>TransactionStatusBegin</span> <span>-&gt;</span> <span>do</span>
        <span>assign</span> <span>_2</span> <span>(</span><span>Just</span> <span>TransactionStatusCommit</span><span>)</span>
        <span>pure</span> <span>$</span> <span>Output</span> <span>"COMMIT"</span>
      <span>Just</span> <span>_</span> <span>-&gt;</span> <span>throwError</span> <span>$</span> <span>StatementFailureCodeInternalError</span> <span>"Transaction in a funky state; must roll back"</span>

  <span>StatementRollback</span> <span>-&gt;</span> <span>do</span>
    <span>use</span> <span>_2</span> <span>&gt;&gt;=</span> <span>\</span> <span>case</span>
      <span>Nothing</span> <span>-&gt;</span> <span>throwError</span> <span>$</span> <span>StatementFailureCodeInternalError</span> <span>"Not in a transaction"</span>
      <span>Just</span> <span>_</span> <span>-&gt;</span> <span>do</span>
   …</code></pre></div></div></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://dfithian.github.io/2021/02/18/database-implementation-part-2.html">https://dfithian.github.io/2021/02/18/database-implementation-part-2.html</a></em></p>]]>
            </description>
            <link>https://dfithian.github.io/2021/02/18/database-implementation-part-2.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26225863</guid>
            <pubDate>Mon, 22 Feb 2021 15:51:05 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why an interpretable Covid mortality prediction model failed in the real world]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 1 (<a href="https://news.ycombinator.com/item?id=26225784">thread link</a>) | @stuartbman
<br/>
February 22, 2021 | https://explainthispaper.com/ai-covid-prognosis-predictor/ | <a href="https://web.archive.org/web/*/https://explainthispaper.com/ai-covid-prognosis-predictor/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p><i>article</i></p>
<div>
<p>An interpretable mortality prediction model for COVID-19 patients</p>
<p>May 14, 2020</p>
<p>Li Yan, Hai-Tao Zhang, Jorge Goncalves, Yang Xiao, Maolin Wang, Yuqi Guo, Chuan Sun, Xiuchuan Tang, Liang Jing, Mingyang Zhang ... Yong Zhang, Ailin Luo, Laurent Mombaerts, Junyang Jin, Zhiguo Cao, Shusheng Li*, Hui Xu* &amp; Ye Yuan</p>
<p><a href="https://www.nature.com/articles/s42256-020-0180-7">Nature Machine Intelligence</a>
</p></div>
</div><div>
<div data-contentpath-field="block">
<h3>Clinical Need</h3><p>COVID-19 is overwhelming healthcare systems worldwide. One reason for this is that COVID-19 causes a spectrum of disease ranging from mild infection to critical illness, and its hard for doctors to anticipate which COVID-19 patients will need more immediate medical attention.</p><p>Having a prediction tool would allow hospitals to quickly triage coronavirus patients into high and low risk levels. In this way, hospital resources can be more adequately allocated to the higher risk coronavirus patients.</p><h3>What did they do?</h3><p>They looked back at blood test results from coronavirus patients in Tangji Hospital in Wuhan, China. They used the latest blood tests to train a machine learning model to predict one of two outcomes: death or survival.</p>
</div>
<div data-contentpath-field="block">
<section>
<div>
<p><span data-contentpath-field="block">
<h3>How was the model made?</h3><p>The group used a popular <sample>classification model</sample> called XGBoost 🤖</p>
</span>
</p>
<div data-contentpath-field="right">

<p>In machine learning, classification is a technique that categorises data into a given number of classes. These classes can have any sort of label e.g. cancer or no cancer. A classification model attempts to reach a conclusion about the input values during training by assigning the input to one of the classes.</p>
<p><img alt="omermohamed.jpg" height="170" src="https://explainthispaper.s3.amazonaws.com/images/image.2e16d0ba.fill-200x200.jpg" width="169">
</p>

</div>
</div>
</section>
</div>
<div data-contentpath-field="block">
<section>
<div>
<p><span data-contentpath-field="block">
<p>This is a type of <sample>decision tree ensemble</sample>.</p>
</span>
</p>
<div data-contentpath-field="right">

<p>Ensemble Learning is a powerful method of Machine Learning that trains and predicts with many models (ie many decision trees) at once to produce a single superior output.</p><p>Think of it as trying out a few different routes to a single location you’ve never been to; as you use all of the routes, you begin to learn which traffic lights take longer, when and how the time of day impacts one route over the other — allowing you to craft the perfect route. You experimented with and combined a few different models to reach an optimal conclusion. Ensemble learning is similar!</p>
<p><img alt="omermohamed.jpg" height="170" src="https://explainthispaper.s3.amazonaws.com/images/image.2e16d0ba.fill-200x200.jpg" width="169">
</p>
<p><span><b>
Omer Mohamed</b>
</span><br>
<span>Medical Student</span>
</p>
</div>
</div>
</section>
</div>
<div data-contentpath-field="block">
<p>The model output was set as either class 0 (death) or class 1 (survival).</p><p>As is usual for training classification models, the model was trained by comparing its predictions to the true outcomes of over 300 respective patients and then altering its decision steps accordingly.</p><p>Eventually, the trained model was tested on a separate set of patient data (the test set), from 110 new patients.</p>
</div>
<div data-contentpath-field="block">
<section>
<div>
<p><span data-contentpath-field="block">
<h3>How did the model do?</h3><p>The algorithm performed well 🎯. On the test set, it achieved an <sample>F1 score of 0.98</sample> for predicting survival and 0.90 for predicting death.</p>
</span>
</p>
<div data-contentpath-field="right">

<p>This is a score for assessing how well the model is making the correct prediction, ranging from 0 (really bad) to 1 (really good). It balances precision (if the algorithm makes a prediction, how confident can we be?) with sensitivity (of the outcome of interest, how many did it pick up correctly?)</p>
<p><img alt="chris.jpg" height="200" src="https://explainthispaper.s3.amazonaws.com/images/chris.2e16d0ba.fill-200x200.jpg" width="200">
</p>
<p><span><b>
Chris Lovejoy</b>
</span><br>
<span>Doctor and Data Scientist</span>
</p>
</div>
</div>
</section>
</div>
<div data-contentpath-field="block">
<section>
<div>
<p><span data-contentpath-field="block">
<p>The algorithm revealed that three key biomarkers in the blood had the biggest influence on predictions: <sample>LDH, hs-CRP and lymphocyte counts</sample>.</p>
</span>
</p>
<div data-contentpath-field="right">

<p>LDH and hs-CRP are proteins while lymphocytes are a type of immune cell. Basically, when the level of these go up, the more likely it is that the infection is serious.</p>
<p><img alt="chris.jpg" height="200" src="https://explainthispaper.s3.amazonaws.com/images/chris.2e16d0ba.fill-200x200.jpg" width="200">
</p>
<p><span><b>
Chris Lovejoy</b>
</span><br>
<span>Doctor and Data Scientist</span>
</p>
</div>
</div>
</section>
</div>
<div data-contentpath-field="block">
<p>They found F1 scores were highest when blood tests were taken close to the day of final outcome (death or survival). Even as far as 18 days before final outcome, the cumulative F1 scores were over 0.90.</p><h3>Want the nitty gritty? 🧐</h3><p>XGBoost is a type of tree ensembling method. It generates many different 'decision trees' then aggregates them together.</p><p>An advantage is that this can be very interpretable. The team produced two simple rules that would predict death in this population:</p><ol><li>LDH &gt; 365 or</li><li>hs-CRP &gt;41.2 and Lymphocyte &lt;14.7%</li></ol>
</div>
<div data-contentpath-field="block">
<h3>Where it all went wrong 😬</h3><p>The group didn't validate their model externally i.e. in another hospital. This is crucial when developing a model as it reduces overfitting of the model to one specific dataset. <a href="https://www.nature.com/articles/s42256-020-00254-2">Another group</a> tested the model in New York and the F1 score for predicting death was 0.41. Similarly low accuracy was reported in <a href="https://www.nature.com/articles/s42256-020-00252-4">France</a> and the <a href="https://www.nature.com/articles/s42256-020-00253-3">Netherlands</a>.</p><p>Also, this model's accuracy depended on how close to the final outcome it was applied. In real-life, doctors wouldn't know when the final outcome will happen.</p><p>The group didn't make it clear whether the model can be used as a triage tool at the first point of presentation to hospital. When the New York group applied the model at initial triage point, the F1 score for predicting death was still low, at 0.56.</p><p>(These performance issues are not confined to this particular model. A <a href="https://erj.ersjournals.com/content/56/6/2003498">review</a> of existing predictive models for COVID-19 found that none of them met the standard of accuracy that you get from simpler measures like oxygen levels. The review recommended that, in future, model developers must always externally validate their models.)</p><h3>So what?</h3><p>This paper made a lot of waves in the few months after it came out, with several direct replies and over 90 citations. There's clearly a lot of interest in predictive models for COVID-19. However, with the limitations highlighted above, it's unlikely that this exactly model will be used on a widespread scale.</p>
</div>
</div></div>]]>
            </description>
            <link>https://explainthispaper.com/ai-covid-prognosis-predictor/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26225784</guid>
            <pubDate>Mon, 22 Feb 2021 15:44:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: A paypal.me clone using Stripe]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26225504">thread link</a>) | @thomasisaac
<br/>
February 22, 2021 | https://tillypay.com/blog/open-payment-link-with-stripe/ | <a href="https://web.archive.org/web/*/https://tillypay.com/blog/open-payment-link-with-stripe/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section>
      <p>TL;dr If you wish to create your own Stripe Open Payment Link, like Paypal.me, you can use <a href="https://tillypay.com/">tillypay.com</a></p><p>One killer feature that Stripe hasn't made is a response to PayPal.me, this would be when the end client or user could enter the amount and continue to pay that amount.</p><p>It would be great if you could have the option to do this, so we built it! TillyPay is a verified Stripe Partner, this means we can control your Stripe platform on your behalf adding this much needed <strong>Open Pay Link</strong>. We call it OpenPay.</p><h3 id="introducing-openpay">Introducing OpenPay</h3><p>OpenPay will allow your customers to write in exactly <strong>amount</strong> they wish to pay and in the <strong>currency</strong> they wish to pay in.</p><p>You also operate this under your <strong>own domain</strong> rather than using ours.<br><strong>pay.yourcompany.com/open</strong> could lead to the page like below:</p><figure><img src="https://tillypay.com/blog/content/images/2020/11/Screenshot-2020-11-05-at-12.21.12.png"><figcaption>Example of an Open Payment Link for Stripe</figcaption></figure><p>The payment will be entered and created into your existing Stripe dashboard.</p><p><strong>Payment Methods</strong><br>We allow any payment method that Stripe uses, including Apple &amp; Google Pay, you can see this above under the "Pay Now&gt;" button will lead them to any system that they currently have installed on their system.</p><p>This is a lot more variety than PayPal's system, under business transactions this solution is cheaper.</p><h2 id="free-to-try-out">Free to try out</h2><p>To get started with TillyPay's OpenPay system, head over to our web app and get started</p><!--kg-card-begin: html--><a href="https://app.tillypay.com/signup?utm_source=blog-openpay">
  <svg width="309px" height="68px" viewBox="0 0 309 68" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
      <g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
          <g id="Artboard" transform="translate(-31.000000, -46.000000)">
              <g id="Group-31" transform="translate(31.000000, 46.000000)">
                  <rect id="Rectangle" fill="#0F24CE" x="0" y="0" width="309" height="68" rx="4"></rect>
                  <text id="Create-an-Account" font-family="KohinoorBangla-Bold, Kohinoor Bangla" font-size="30" font-weight="bold" fill="#FFFFFF">
                      <tspan x="23.305" y="44">Create an Account</tspan>
                  </text>
              </g>
          </g>
      </g>
  </svg>
</a><!--kg-card-end: html-->
    </section></div>]]>
            </description>
            <link>https://tillypay.com/blog/open-payment-link-with-stripe/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26225504</guid>
            <pubDate>Mon, 22 Feb 2021 15:21:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Practical Color Theory for People Who Code (2016)]]>
            </title>
            <description>
<![CDATA[
Score 80 | Comments 28 (<a href="https://news.ycombinator.com/item?id=26225339">thread link</a>) | @martinlaz
<br/>
February 22, 2021 | http://tallys.github.io/color-theory/ | <a href="https://web.archive.org/web/*/http://tallys.github.io/color-theory/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><header>
	
	<h3>Natalya Shelburne
		<br>
		<a href="https://twitter.com/natalyathree">@natalyathree</a>
	</h3>
</header>
	
		<div>
			<div>
				<div>
					

					<div><p>
						Hi! I'm Natalya! <img src="http://tallys.github.io/color-theory/images/tally-pic.jpg" title="natalya-profile" description="vector art self portrait">I'm a classically trained fine artist who spent 6 years teaching people how to paint, draw, and grow their creativity. I am now a front end developer, and I love writing code as much as I love painting. </p><p>I have a degree in Studio Art, a bachelor's in Developmental Psychology, and a master's degree in Creativity and Talent Development. But, most importantly, I have mixed gallons and gallons of paint. </p><p>I abstracted my domain knowledge as a fine artist into variables and functions in order to reveal color selection as being logical, predictable, and driven by principles anyone can learn. Sass color functions give you the same creative power as owning a set of paints, brushes, and canvas. </p><p>This is a demo of my functions for a complementary color scheme - pick any color on the color wheel and the functions will make sure that the scheme will still work! 🎨</p></div>
					</div>
					<div>
						
					
					<h4>Completely new to this? Check out these resources first:</h4>
					
				</div>
			</div>
		</div>
		<a href="#" name="start"></a>
<section>
	<h2>Let's build a Complementary Color Scheme!</h2>
	<img src="http://tallys.github.io/color-theory/images/color-circle.png" alt="color-wheel" title="color wheel" description="the color wheel with corresponding hsl degrees">
	<p>This is the color wheel, consider this the documentation for using color. <br> Notice that the degrees on the color wheel correspond to colors.</p>
	
</section>

<section>
	<h2>Pick a color <span> hsl($hue, $saturation, $lightness)</span></h2>
	<div>
		
		<p>Pick any color by selecting its hue (0-360) on the color wheel at full saturation (100%) and at half lightness (50%) - this way you start with the 'most colorful color' you can get.</p>
		<p>This is what your website will look like if you set every element to this one color. Notice how you can't tell one item from another. Color is information.</p>
	</div>
	<div>

		<div>

	
	
	<div>
		<p> tiny lorem ipsum tiny lorem ipsum tin rem ipsum ?</p>
	<p> tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tinyny lorem ipsum </p>
	<p>cta
	</p></div>
	
</div>


	</div>
	<div>
		
		<div>
			<p>Why pick a fully saturated color, but only half lightness?</p>
			<p>Ever mix up a bunch of colors only to end up with a gray blob? In the real world, you can't mix a color to be more saturated - you only "lose" color information as you mix. So, the practice is to start with the most saturated colors at their most chromatic so you can still have a full range of mixing opportunities.</p>
		</div>
		
	</div>
</section>


<section>
	<h2>Generate Complementary Color<span>complement( );</span></h2>
	<div>
		
		<p>Generate your second color without having to guess what will work. Thanks to science and wavelengths, we know that this works. The opposition of these two colors stimulate your photoreceptor cells in a good way!</p>
		<p>Finally! A different color - now hue separates elements from each other and a layout can be seen.</p>
	</div>
	<div>
		<div>

	
	
	<div>
		<p> tiny lorem ipsum tiny lorem ipsum tin rem ipsum ?</p>
	<p> tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tinyny lorem ipsum </p>
	<p>cta
	</p></div>
	
</div>

	</div>
	<div>
		
		<p>The complement to any color is always 180 degrees across on the other side of the color wheel. If you've ever wondered why it's the color wheel instead of a color line, this is part of that answer. Remember that the color wheel is our visual documentation for color relationships. <br> Fun fact, if you mix complementary colors, they'll cancel each other out and you'll end up with a neutral gray.</p>
		
	</div>
</section>

<section>
	<h2>Color Relationship Established by Mixing<span>harmonious-mix( );</span></h2>
	<div>
		
		
		<p>Establish a color relationship by mixing them together. This makes the colors look like they're under similar lighting conditions.</p>
		<p>Here, you see less saturated hues, with a clear relationship between each other.</p>
		</div>
		<div>
			<div>

	
	
	<div>
		<p> tiny lorem ipsum tiny lorem ipsum tin rem ipsum ?</p>
	<p> tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tinyny lorem ipsum </p>
	<p>cta
	</p></div>
	
</div>

		</div>
		<div>
			
			<div>
				<p>Our eyes may be legacy systems, but they are really good at spotting patterns, especially things that look like they don't belong in a set.</p>
				<p>This mixing method is another way we can simulate lighting and color found in nature. It's a bit more complicated, but my favorite way of explaining it is this: Everything outside has some "yellow" mixed in from the sunshine during the day. Same thing happens when everyone and everything looks bad in photos taken under the glow of green flourescent lights - there is green added to all of the colors you're looking at, including adding a green glow to your skin if you're standing under the flourescent light yourself. Whether it looks good or bad, this color (light) mixing creates a visual harmony. We don't really notice it when it's there, but we really notice its absence.</p>
				<p>When you're painting, you want to simulate similar lighting conditions for a scene, and that effect is accomplished mixing a bit of one color into the other. Mixing different ratios of the same colors will usually generate a matching color palette.</p>
				<p>How do I decide what to do? Thanks to art school and science, I know that cool colors have lower luminosity than warm colors, and will dominate in mixes, with yellow being the lightest color. For example, a touch of blue will really affect yellow, whereas you can add a lot of yellow to blue before it is affected. So, here is this decision making in function form - I am weighing different colors differently when mixing.</p>
	</div>
		
		
		
</div>
</section>

<section>
	<h2>Create Neutrals<span>mix-neutral( ); lighten( ); darken( );</span></h2>
	<div>
		
		
		
		<p>Let your chosen color pop (in other words, don't exhaust your eyes by making them process non-stop intense chromatic colors!) by surrounding it with neutrals. Desaturate the painter way: by mixing complementary colors! Then, vary that neutral's lightness to create a highlight and a shadow.</p>
		<p>Making the complementary color neutrals will help the 'call to action' color you selected stand out - notice how much the button in the top right "pops" all of a sudden. Make things "pop" by making other things around them not pop.</p>
	</div>
	<div>
		<div>

	
	
	<div>
		<p> tiny lorem ipsum tiny lorem ipsum tin rem ipsum ?</p>
	<p> tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tinyny lorem ipsum </p>
	<p>cta
	</p></div>
	
</div>

	</div>
	<div>
		
		
		
		<p>We can do the same for our primary color too. More neutrals to work with.</p>
		<p>Doesn't it seem like we went too far with this whole making things neutral? Now, nothing "pops"! But, on the plus side, none of this seems to be irritating any eyeballs, either. Remember that our eyes don't like to handle seeing everything our computers are capable of rendering.</p>
	</div>
	<div>
		<div>

	
	
	<div>
		<p> tiny lorem ipsum tiny lorem ipsum tin rem ipsum ?</p>
	<p> tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tiny lorem ipsum tinyny lorem ipsum </p>
	<p>cta
	</p></div>
	
</div>

	</div>
	<div>
		
		<div>
			<p>The colors are way too intense at their full saturation for our eyes to handle. If all the colors are bright, none of the colors are bright.</p>
			<p>I picked my starting color for a reason -  I want that to be the heart of my design! That means the complementary color should support the chosen color, and we do that by mixing neutrals. Our eyes don't really handle saturated colors very well next to other saturated colors. Our eyes get confused at the edges - is it this color or the other one? Ahh both! We end up seeing optical illusions. Just give your eyes a break between super strong saturated colors with neutrals so they don't stress out about that much visual information. At the very minimum a rule of thumb is that your viewport should have 33% neutral space (white, desaturated, or black colors) so your poor eyes can have a break and process the information right. Otherwise you get eye strain!</p>
		</div>
		
	</div>
</section>

<section>
	<h2>Why not just desaturate?<span>mix-neutral( ); lighten( ); darken( );</span></h2>
		<p>You totally can! Desaturate does a great job. But, I think not only is it important to understand what "desaturate" means. How would you desaturate a real color in the real world? Remember, you want "ugly" colors for your neutrals! You want to create bland and forgettable colors that recede into the background. These ugly duckling colors are how you get those other call to actions and buttons "pop"!</p>

		<h3>Primary and complementary Colors</h3>
			
			
		<h3>Mixed neutrals</h3>
			
			


		<h3>Desaturated neutrals</h3>
			
			

		<div>
			
			<p>Even though these may look "ugly", neutral colors are the heart of any painting, and that is the case on the web, too. Notice that the same decisions are …</p></div></section></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="http://tallys.github.io/color-theory/">http://tallys.github.io/color-theory/</a></em></p>]]>
            </description>
            <link>http://tallys.github.io/color-theory/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26225339</guid>
            <pubDate>Mon, 22 Feb 2021 15:07:35 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Public stocks like Tesla and Square gain $5B on their Bitcoin]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26225267">thread link</a>) | @senor_lecce
<br/>
February 22, 2021 | https://protos.com/bitcoin-treasuries-tesla-square-crypto-stocks-investment/ | <a href="https://web.archive.org/web/*/https://protos.com/bitcoin-treasuries-tesla-square-crypto-stocks-investment/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-2694">
<header>


</header>
<figure>
<img width="889" height="500" src="https://protos.com/wp-content/uploads/2021/02/Protos-Artwork-Bitcoin50.jpg" alt="stocks, bitcoin" loading="lazy" srcset="https://protos.com/wp-content/uploads/2021/02/Protos-Artwork-Bitcoin50.jpg 1920w, https://protos.com/wp-content/uploads/2021/02/Protos-Artwork-Bitcoin50-300x169.jpg 300w, https://protos.com/wp-content/uploads/2021/02/Protos-Artwork-Bitcoin50-1024x576.jpg 1024w, https://protos.com/wp-content/uploads/2021/02/Protos-Artwork-Bitcoin50-768x432.jpg 768w, https://protos.com/wp-content/uploads/2021/02/Protos-Artwork-Bitcoin50-1536x864.jpg 1536w" sizes="(max-width: 889px) 100vw, 889px" data-pagespeed-url-hash="2854192597" onload="pagespeed.CriticalImages.checkImageForCriticality(this);">

</figure>
<main>
<div>
<p>Publicly traded stocks like Tesla and Square are <strong>up over $5 billion</strong> on their Bitcoin investments.</p>
<p>According to <a href="https://bitcointreasuries.org/" target="_blank" rel="noreferrer noopener">Bitcoin Treasuries</a>, public companies have so far together spent $3 billion to acquire 151,919 BTC — worth $8 billion at current prices.</p>
<ul><li>Michael Saylor’s <a href="https://www.protos.com/microstrategy-grows-bitcoin-holding/" target="_blank" rel="noreferrer noopener">MicroStrategy</a> <strong>spent $1.15 billion</strong> on 71,079 BTC (now worth $3.9 billion).</li><li><strong>Tesla’s up 70%</strong> on its 48,000 BTC (bought for $1.5 billion, now worth $2.5 billion).</li><li>Canada’s Galaxy Digital is third, having bought <strong>16,402 BTC for $134 million</strong> — now worth just under $900 million.</li></ul>
<p>Jack Dorsey’s <a href="https://protos.com/square-bitcoin-jack-dorsey-whitepaper-craig-wright-crypto/" target="_blank" rel="noreferrer noopener">fintech Square</a> gets a notable mention. The firm behind Cash App <strong>spent $50 million</strong> on 4,709 BTC <a href="https://www.theverge.com/2020/10/8/21507533/square-50-million-bitcoin-dorsey-cryptocurrency" target="_blank" rel="noreferrer noopener">last October</a> — now it’s worth $250 million, a 400% increase in five months.</p>
<p><a href="https://public.flourish.studio/visualisation/5364437/?utm_source=embed&amp;utm_campaign=visualisation/5364437" target="_top" rel="noopener"><img alt="Made with Flourish" src="https://public.flourish.studio/resources/made_with_flourish.svg" data-pagespeed-url-hash="1225504988" onload="pagespeed.CriticalImages.checkImageForCriticality(this);"> </a></p>
<p>It should be noted these figures only make sense if the companies <strong>have held their Bitcoin</strong>. Public stocks often disclose asset sales months after the fact.</p>
<p>In any case, investors in <strong>crypto-specific stocks</strong> like Voyager Digital, Riot Blockchain, and Marathon Patent Group are even better off. </p>
<p>Bitcoin’s historic rallies have pushed their <strong>share prices up thousands of percent</strong> over the past year.</p>
<p>For scale, Bitcoin itself is up around 480% while the S&amp;P 500 has gained 15%.</p>
<p><a href="https://public.flourish.studio/visualisation/5364733/?utm_source=embed&amp;utm_campaign=visualisation/5364733" target="_top" rel="noopener"><img alt="Made with Flourish" src="https://public.flourish.studio/resources/made_with_flourish.svg" data-pagespeed-url-hash="1225504988" onload="pagespeed.CriticalImages.checkImageForCriticality(this);"> </a></p>
<p><em>[Read more: <a href="https://protos.com/bitcoin-price-candles-tesla-crypto-history/" target="_blank" rel="noreferrer noopener">Chasing candles — here’s where Bitcoin’s ‘Tesla pump’ ranks in history</a>]</em></p>
<p>But while Tesla’s Bitcoin buy <a href="https://protos.com/bitcoin-tesla-elon-musk-price-record-crypto-purchase/" target="_blank" rel="noreferrer noopener">captured the world’s attention</a>, the influence of BTC’s volatility <strong>could be more apparent</strong> in MicroStrategy’s share price.</p>
<p>After all, more than <strong>40% of MicroStrategy’s market value</strong> is directly derived from its Bitcoin stash. If Bitcoin corrects — so might MSTR stock.</p>
</div>
</main>
</article></div>]]>
            </description>
            <link>https://protos.com/bitcoin-treasuries-tesla-square-crypto-stocks-investment/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26225267</guid>
            <pubDate>Mon, 22 Feb 2021 15:01:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Purchasing Power Parity: fair pricing for [a] SaaS product]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26225110">thread link</a>) | @moviuro
<br/>
February 22, 2021 | https://scastiel.dev/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/ | <a href="https://web.archive.org/web/*/https://scastiel.dev/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article>
      <header>
<picture><source srcset="https://d33wubrfki0l68.cloudfront.net/5031f41aa35ca0f024a011e593598bd643121eb1/97562/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/cover.200.webp 200w, https://d33wubrfki0l68.cloudfront.net/856e13c61ac6fd2ad6b524d6f58ae7ba58a95ad7/559cb/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/cover.450.webp 450w, https://d33wubrfki0l68.cloudfront.net/9f98b77279c3dfa86e83912a0400edff9229768b/87f5d/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/cover.700.webp 700w, https://d33wubrfki0l68.cloudfront.net/04d30fc4482a7323c4bdae17a59b336b6cb6f393/2be42/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/cover.950.webp 950w, https://d33wubrfki0l68.cloudfront.net/a2c402ca694d8221ecfb49a1e6c6e0a10e87d00d/319ab/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/cover.1200.webp 1200w" sizes="100vw" type="image/webp"><source srcset="https://d33wubrfki0l68.cloudfront.net/e7ac2ac5bd77ff6af09c5a6522516b7c4908eed9/5b7d8/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/cover.200.jpg 200w, https://d33wubrfki0l68.cloudfront.net/3816c586aed20fc298e4e9ad4505ea6d3e879cdc/762f4/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/cover.450.jpg 450w, https://d33wubrfki0l68.cloudfront.net/1abd4a439b43146b37052fa3e08c1bcf2d8bd9a2/36a9e/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/cover.700.jpg 700w, https://d33wubrfki0l68.cloudfront.net/11786dbadf9847aed0650ae2d01c8dedc181845b/3e0f3/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/cover.950.jpg 950w, https://d33wubrfki0l68.cloudfront.net/a1c69f33676ccea0c5e9cf5aa2893666af2a2f87/0cec5/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/cover.1200.jpg 1200w" sizes="100vw" type="image/jpeg"><img src="https://scastiel.dev/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/cover.jpg" alt="Purchasing Power Parity: fair pricing for your SaaS product" height="800" width="1200" loading="lazy"></picture>

<p><small>22 Feb 2021 — 10 min read</small></p>
</header>
<p>I released <a href="https://useeffect.dev/?ref=PostPPP">my course about React hooks</a> a couple of weeks ago. As with any SaaS product, I asked myself the usual question: what should be my product's price?</p>
<p>Economists would say that the right price should be when the offer curve crosses the demand one. But I’m not an economist, and this model seems to be valid for a given place (country), not when you sell something worldwide.</p>
<p>Using Purchasing Power Parity will help me solve this problem. After I explained what it is, I’ll show you how I implemented it on my course’s selling page using serverless functions and a little bit of React.</p>
<h2 id="what-is-purchasing-power-parity%3F">What is Purchasing Power Parity?</h2>
<p>At the core of the <a href="https://www.investopedia.com/updates/purchasing-power-parity-ppp/">Purchasing Power Parity</a> (PPP) is the following idea: a fair price in Switzerland will have no chance to convince anyone to buy in Somalia. It is an extreme example, but you get the idea.</p>
<p>And it isn’t new when you think about it: the same Spotify subscription doesn’t cost the same depending on where you are:</p>
<figure><picture><source srcset="https://d33wubrfki0l68.cloudfront.net/89052b2b947fbf8259d51c2751dcaa06bdaee0c3/74270/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/spotify_pricing.200.webp 200w, https://d33wubrfki0l68.cloudfront.net/ab8b58aad667c00361d026347fba6d2bbd9a0ba1/7d69b/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/spotify_pricing.450.webp 450w, https://d33wubrfki0l68.cloudfront.net/d5ed0dc7c42efbfd2e40caf895d2d82ec78be76c/ca1bd/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/spotify_pricing.700.webp 700w, https://d33wubrfki0l68.cloudfront.net/6b857abc4496b45c09e17e35226885e61ef65b7f/79d43/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/spotify_pricing.950.webp 950w, https://d33wubrfki0l68.cloudfront.net/414f767240038f5cac871a24846b591ec13603bf/72843/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/spotify_pricing.1200.webp 1200w" sizes="100vw" type="image/webp"><source srcset="https://d33wubrfki0l68.cloudfront.net/b00584bea4aec8818aaee8b3c6a5e7fab992765b/b54b8/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/spotify_pricing.200.png 200w, https://d33wubrfki0l68.cloudfront.net/89df9581a5ac55a702c38b30e0d858c8ed3e622f/75146/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/spotify_pricing.450.png 450w, https://d33wubrfki0l68.cloudfront.net/34d5759d7a1c0470835381b7dd7ec59c577b286b/8c33a/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/spotify_pricing.700.png 700w, https://d33wubrfki0l68.cloudfront.net/31fb64ed77254700386e3845e01ff320d71dcab2/dcb2a/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/spotify_pricing.950.png 950w, https://d33wubrfki0l68.cloudfront.net/a7c3dc38114cb28bfb656cb12eca39cd8b1876f9/ed710/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/spotify_pricing.1200.png 1200w" sizes="100vw" type="image/png"><img src="https://scastiel.dev/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/spotify_pricing.png" alt="" height="731.3432835820896" width="1200" loading="lazy"></picture><figcaption>In Denmark, a Spotify subscription costs $18, but less than $3 in Philippines. Source: <a href="https://mts.io/projects/spotify-pricing/">Spotify Premium Index (2014)</a></figcaption></figure>
<p>And the reason for this difference is obvious: purchasing power is not the same across the world. Several indices exist to measure this phenomenon, the most famous in popular culture being the <a href="https://en.wikipedia.org/wiki/Big_Mac_Index">Big Mac Index</a>.</p>
<p>Big companies such as Spotify can adjust their pricing in every country they are in, but what about you with your SaaS? You don’t have headquarters in all countries, yet you sell your product worldwide.</p>
<p>To solve this problem, a tendency emerged these past few months: using PPP to offer your product at a lower price where the purchasing power is lower.</p>
<p>For instance, you can purchase my course at its base price of $50 (when I’m writing this post) in Switzerland or Denmark, $25 in Romania or Albania, and $5 in Somalia or Liberia.</p>
<p>To me, using PPP for your pricing has two advantages:</p>
<ul>
<li>it is <strong>fair</strong>: I want my course to be available to the most people, especially people who could use it to improve their economic situation by learning new skills;</li>
<li>it is <strong>profitable</strong>: even if low-PPP countries won’t be the ones bringing you the most significant revenue, it is still better than nothing, i.e., a pricing way too high for these countries.</li>
</ul>
<p><em>Note that I’m not yet able to confirm the second hypothesis for my case since I just released my course. And even then, confirming it would require an experimental system with a large sample.</em></p>
<p>Let’s say I convinced you to use PPP for your product’s pricing; how can you implement it?</p>
<h2 id="how-to-add-ppp-to-your-site%3F">How to add PPP to your site?</h2>
<p>For the rest of this post, I’ll suppose you have a product, and its selling page uses Stripe, React and serverless functions (e.g. with <a href="https://nextjs.org/">Next.js</a> or <a href="https://functions.netlify.com/">Netlify</a> functions).</p>
<p>It’s quite a big assumption, but it shouldn’t be too much trouble to use the principles to adapt them to your use case.</p>
<p>I created a small demo website to understand what it can look like when you offer purchasing power parity. <a href="https://github.com/scastiel/parity.coffee">Its source code is available on GitHub</a>, and it presents an implementation for the two first options I will describe here.</p>
<figure><picture><source srcset="https://d33wubrfki0l68.cloudfront.net/d47327fa9522392a5009fef25b1f836981abaa1f/3d937/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/parity.coffee-screenshot.200.webp 200w, https://d33wubrfki0l68.cloudfront.net/a2c094b1d8b44c5f1ea3ef44d391d0dec2c068cb/3ec42/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/parity.coffee-screenshot.450.webp 450w, https://d33wubrfki0l68.cloudfront.net/9bdd1b77ea9ee0ed0e1fc079ed510eb9a3121f62/17699/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/parity.coffee-screenshot.700.webp 700w, https://d33wubrfki0l68.cloudfront.net/3f4034f0f2b2f2b83c6eb424d79bdf5522b1687f/b5bde/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/parity.coffee-screenshot.950.webp 950w, https://d33wubrfki0l68.cloudfront.net/06ffd1a5f92914ce63c4b058beec8557a8b3407f/cd12d/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/parity.coffee-screenshot.1200.webp 1200w" sizes="100vw" type="image/webp"><source srcset="https://d33wubrfki0l68.cloudfront.net/a7125c64cb2e8a9a09dc8edde1fedda06432effe/6d558/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/parity.coffee-screenshot.200.png 200w, https://d33wubrfki0l68.cloudfront.net/f8f6ee8c9f90d1d40d3c433612a117a110cba327/87cb2/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/parity.coffee-screenshot.450.png 450w, https://d33wubrfki0l68.cloudfront.net/e6102bfa6ed5a21a19b77cff29e09daf536e8ce9/bd307/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/parity.coffee-screenshot.700.png 700w, https://d33wubrfki0l68.cloudfront.net/9b9b3ea48371b45602f9534f281c2419a00818d3/b9b83/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/parity.coffee-screenshot.950.png 950w, https://d33wubrfki0l68.cloudfront.net/34f8205cb9ad084242039ab7da06b85be56b2d1f/88bfe/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/parity.coffee-screenshot.1200.png 1200w" sizes="100vw" type="image/png"><img src="https://scastiel.dev/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/parity.coffee-screenshot.png" alt="" height="822.2029488291414" width="1200" loading="lazy"></picture><figcaption><a href="https://parity.coffee/">parity.coffee</a>: a demo site using PPP</figcaption></figure>
<p>For the payment with Stripe, I tried to be as close as possible to what they describe in <a href="https://stripe.com/docs/checkout/integration-builder">their documentation about accepting a payment</a>.</p>
<h3 id="option-%231%3A-adjust-the-price-automatically">Option #1: adjust the price automatically</h3>
<p>The first solution I tried on my course was to offer a price automatically inferred from the user’s country, using geolocation of their IP address.</p>
<p>This can be done using <a href="https://www.npmjs.com/package/request-ip"><code>request-ip</code></a> and <a href="https://www.npmjs.com/package/geoip-lite"><code>geoip-lite</code></a> packages with Node.js:</p>
<pre><code><span>async</span> <span>function</span> <span>getCountryForRequest</span><span>(</span><span>req</span><span>)</span> <span>{</span><br>  <span>try</span> <span>{</span><br>    <span>const</span> clientIp <span>=</span> requestIp<span>.</span><span>getClientIp</span><span>(</span>req<span>)</span><br>    <span>return</span> geoip<span>.</span><span>lookup</span><span>(</span>clientIp<span>)</span><span>.</span>country <span>||</span> <span>null</span><br>  <span>}</span> <span>catch</span> <span>(</span>err<span>)</span> <span>{</span><br>    console<span>.</span><span>error</span><span>(</span>err<span>)</span><br>    <span>return</span> <span>null</span><br>  <span>}</span><br><span>}</span></code></pre>
<p>When you have the country, the next step is to get the <em>PPP conversion factor</em> for this country. I used an <a href="https://purchasing-power-parity.com/">API</a> created by <a href="https://twitter.com/rwieruch">Robin Wieruch</a>. Still, you can also use any list you find on the Internet with countries associated with their PPP conversion factor (or any other index: Spotify subscription price, Big Mac index…)</p>
<pre><code><span>async</span> <span>function</span> <span>getConversionFactorForCountry</span><span>(</span><span>country</span><span>)</span> <span>{</span><br>  <span>try</span> <span>{</span><br>    <span>const</span> url <span>=</span> <span><span>`</span><span>https://api.purchasing-power-parity.com/?target=</span><span><span>${</span>country<span>}</span></span><span>`</span></span><br>    <span>const</span> res <span>=</span> <span>await</span> <span>fetch</span><span>(</span>url<span>)</span><br>    <span>const</span> <span>{</span> ppp <span>}</span> <span>=</span> <span>await</span> res<span>.</span><span>json</span><span>(</span><span>)</span><br>    <span>return</span> ppp<span>.</span>pppConversionFactor<br>  <span>}</span> <span>catch</span> <span>(</span>err<span>)</span> <span>{</span><br>    console<span>.</span><span>error</span><span>(</span>err<span>)</span><br>    <span>return</span> <span>1</span><br>  <span>}</span><br><span>}</span></code></pre>
<p>Last step: we use the conversion factor to calculate your product's price dynamically from the user’s request.</p>
<pre><code><span>import</span> <span>{</span> <span>BASE_PRICE</span> <span>}</span> <span>from</span> <span>'../prices'</span><br><p><span>async</span> <span>function</span> <span>getPriceFromRequest</span><span>(</span><span>req</span><span>)</span> <span>{</span><br>  <span>const</span> country <span>=</span> <span>await</span> <span>getCountryForRequest</span><span>(</span>req<span>)</span><br>  <span>const</span> pppConversionFactor <span>=</span><br>    <span>(</span>country <span>&amp;&amp;</span> <span>(</span><span>await</span> <span>getConversionFactorForCountry</span><span>(</span>country<span>)</span><span>)</span><span>)</span> <span>||</span> <span>1</span><br>  <span>if</span> <span>(</span>pppConversionFactor <span>&gt;=</span> <span>1</span><span>)</span> <span>{</span><br>    <br>    <br>    <span>return</span> <span>{</span> price<span>:</span> <span>BASE_PRICE</span><span>,</span> discount<span>:</span> <span>0</span><span>,</span> country <span>}</span><br>  <span>}</span><br>  <span>const</span> price <span>=</span> Math<span>.</span><span>round</span><span>(</span><span>BASE_PRICE</span> <span>*</span> pppConversionFactor<span>)</span><br>  <span>const</span> discount <span>=</span> Math<span>.</span><span>round</span><span>(</span><span>100</span> <span>*</span> <span>(</span><span>1</span> <span>-</span> price <span>/</span> <span>BASE_PRICE</span><span>)</span><span>)</span><br>  <span>return</span> <span>{</span> price<span>,</span> discount<span>,</span> country <span>}</span><br><span>}</span></p></code></pre>
<p>This function has to be called at least in two places:</p>
<ul>
<li>in an endpoint called to display the price on your page, and</li>
<li>in the endpoint used to create the checkout session.</li>
</ul>
<p>To display the price on my course page, I created an endpoint <code>/api/get-price</code>, with this function:</p>
<pre><code><span>async</span> <span>function</span> <span>getPrice</span><span>(</span><span>req<span>,</span> res</span><span>)</span> <span>{</span><br>  <span>const</span> <span>{</span> price<span>,</span> discount<span>,</span> country <span>}</span> <span>=</span> <span>await</span> <span>getPriceFromRequest</span><span>(</span>req<span>)</span><br>  res<span>.</span><span>send</span><span>(</span><span>{</span> price<span>,</span> discount<span>,</span> country <span>}</span><span>)</span><br><span>}</span></code></pre>
<p>Notice that I also return the discount (vs. the base price) and the country; they will be useful, as we’ll see in a minute.</p>
<p>I call this endpoint in my component, using a combination of <code>useState</code> and <code>useEffect</code> hooks:</p>
<pre><code><span>const</span> <span>[</span>priceInfo<span>,</span> setPriceInfo<span>]</span> <span>=</span> <span>useState</span><span>(</span><span>null</span><span>)</span><p><span>useEffect</span><span>(</span><span>(</span><span>)</span> <span>=&gt;</span> <span>{</span><br>  <span>fetch</span><span>(</span><span>'/api/get-price'</span><span>)</span><br>    <span>.</span><span>then</span><span>(</span><span>(</span><span>res</span><span>)</span> <span>=&gt;</span> res<span>.</span><span>json</span><span>(</span><span>)</span><span>)</span><br>    <span>.</span><span>then</span><span>(</span><span>(</span><span>priceInfo</span><span>)</span> <span>=&gt;</span> <span>setPriceInfo</span><span>(</span>price<span>)</span><span>)</span><br>    <span>.</span><span>catch</span><span>(</span><span>(</span><span>err</span><span>)</span> <span>=&gt;</span> console<span>.</span><span>error</span><span>(</span>err<span>)</span><span>)</span><br><span>}</span><span>,</span> <span>[</span><span>]</span><span>)</span></p></code></pre>
<p>I can then display this price in my page:</p>
<pre><code><span><span><span>&lt;</span>span</span><span>&gt;</span></span><span><br>  </span><span>{</span>priceInfo <span>?</span> <span>(</span><br>    <br>    <span><span><span>&lt;</span>span</span><span>&gt;</span></span><span>$</span><span>{</span><span>(</span>priceInfo <span>/</span> <span>100</span><span>)</span><span>.</span><span>toFixed</span><span>(</span><span>2</span><span>)</span><span>}</span><span><span><span>&lt;/</span>span</span><span>&gt;</span></span><br>  <span>)</span> <span>:</span> <span>(</span><br>    <span><span><span>&lt;</span>span</span><span>&gt;</span></span><span>Loading…</span><span><span><span>&lt;/</span>span</span><span>&gt;</span></span><br>  <span>)</span><span>}</span><span><br></span><span><span><span>&lt;/</span>span</span><span>&gt;</span></span></code></pre>
<p>I also asked the question (to myself and on Twitter): should I inform the user that they got a discount because of their location? And the answer I got (and it seems very wise) was <a href="https://twitter.com/scastiel/status/1360224976339664897">a big YES</a>.</p>
<p>So we can use the <code>country</code> and the <code>discount</code> returned by the endpoint to display a customized message:</p>
<pre><code><span>{</span><br>  priceInfo<span>.</span>discount <span>&gt;</span> <span>0</span> <span>&amp;&amp;</span> <span>(</span><br>    <span><span><span>&lt;</span>p</span><span>&gt;</span></span><span><br>      </span><span>{</span>countryEmoji<span>}</span><span> Hey! It looks like you are from </span><span>{</span>countryName<span>}</span><span>. We support<br>      Purchasing Power Parity so we automatically adjusted the price, adding a<br>      discount of </span><span>{</span>priceInfo<span>.</span>discount<span>}</span><span>%.<br>    </span><span><span><span>&lt;/</span>p</span><span>&gt;</span></span><br>  <span>)</span><br><span>}</span></code></pre>
<p><a href="https://github.com/scastiel/parity.coffee/blob/main/lib/country.js">Have a look here to know how to get the country name and emoji 😉</a></p>
<figure><picture><source srcset="https://d33wubrfki0l68.cloudfront.net/d9c55dadd3e74061e41eec9b785859ce648aa1e7/46cf9/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/message1.200.webp 200w, https://d33wubrfki0l68.cloudfront.net/53dbed2b8b4a3deafe64b4313ddaf00030e60de5/799f8/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/message1.450.webp 450w, https://d33wubrfki0l68.cloudfront.net/4b0af02954f41c3463c70e3d9e9762496e88fa8d/0139a/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/message1.700.webp 700w, https://d33wubrfki0l68.cloudfront.net/10df8651b1d64711bc15d0aadbbb85ed80efebf2/00c3d/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/message1.950.webp 950w, https://d33wubrfki0l68.cloudfront.net/5ce3cb2a10735adc4679904e8cdb9a49719906d6/1a406/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/message1.976.webp 976w" sizes="100vw" type="image/webp"><source srcset="https://d33wubrfki0l68.cloudfront.net/550dbcc50c62bef3fb76b8a68aa084de7d1f290a/3d702/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/message1.200.png 200w, https://d33wubrfki0l68.cloudfront.net/ed1110c20f7bf5de6df8eca4dd23c254c3609edf/4d449/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/message1.450.png 450w, https://d33wubrfki0l68.cloudfront.net/2e6b14fccd9ce771d42c8bdd2e6d6afbd6bf2059/6f53e/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/message1.700.png 700w, https://d33wubrfki0l68.cloudfront.net/217540241a39494ca1e86dac8b1920706401ed1d/43f1e/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/message1.950.png 950w, https://d33wubrfki0l68.cloudfront.net/e551a64feaab7e852c15413b8d2fc68f273d049b/304ac/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/message1.976.png 976w" sizes="100vw" type="image/png"><img src="https://scastiel.dev/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/message1.png" alt="" height="326" width="976" loading="lazy"></picture><figcaption>Displaying a message to inform the user a discount was applied.</figcaption></figure>
<p>Ok, the price is automatically adjusted on the page, but we still need to adjust it when the user goes to the checkout page on Stripe. To do this, we have to call our <code>getPriceFromRequest</code> function in the <code>create-checkout-session</code> endpoint used to <a href="https://stripe.com/docs/api/checkout/sessions/create">create the Stripe session</a>:</p>
<pre><code><span>export</span> <span>default</span> <span>async</span> <span>function</span> <span>createCheckoutSession</span><span>(</span><span>req<span>,</span> res</span><span>)</span> <span>{</span><br>  <span>const</span> <span>{</span> price <span>}</span> <span>=</span> <span>await</span> <span>getPriceFromRequest</span><span>(</span>req<span>)</span><p>  <span>const</span> session <span>=</span> <span>await</span> stripe<span>.</span>checkout<span>.</span>sessions<span>.</span><span>create</span><span>(</span><span>{</span><br>    payment_method_types<span>:</span> <span>[</span><span>'card'</span><span>]</span><span>,</span><br>    line_items<span>:</span> <span>[</span><br>      <span>{</span><br>        price_data<span>:</span> <span>{</span><br>          currency<span>:</span> <span>'usd'</span><span>,</span><br>          product_data<span>:</span> <span>{</span> name<span>:</span> <span>'One coffee'</span> <span>}</span><span>,</span><br>          unit_amount<span>:</span> price<span>,</span><br>        <span>}</span><span>,</span><br>        quantity<span>:</span> <span>1</span><span>,</span><br>      <span>}</span><span>,</span><br>    <span>]</span><span>,</span><br>    mode<span>:</span> <span>'payment'</span><span>,</span><br>    success_url<span>:</span> <span><span>`</span><span><span>${</span>process<span>.</span>env<span>.</span><span>DOMAIN</span><span>}</span></span><span>?success=true</span><span>`</span></span><span>,</span><br>    cancel_url<span>:</span> <span><span>`</span><span><span>${</span>process<span>.</span>env<span>.</span><span>DOMAIN</span><span>}</span></span><span>?cancel=true</span><span>`</span></span><span>,</span><br>  <span>}</span><span>)</span><br>  res<span>.</span><span>json</span><span>(</span><span>{</span> id<span>:</span> session<span>.</span>id <span>}</span><span>)</span><br><span>}</span></p></code></pre>
<p>With this solution, the discount is almost transparent for the user (we still display a message to inform them). They could almost believe that the price they see is the same as everyone sees.</p>
<p>When I thought about it, something didn’t feel perfect with this solution. I felt the discount would be more convincing if the user had to enter a code during the checkout, so I tested the second option.</p>
<h2 id="option-%232%3A-offer-a-discount-code">Option #2: offer a discount code</h2>
<p>This option’s implementation is not very different from the first one. First, we’ll need the same functions <code>getCountryForRequest</code> and <code>getConversionFactorForCountry</code>. But, whereas in the previous implementation, we calculated the price using the conversion factor, this time, we’ll infer a discount code from this factor.</p>
<p>The idea is to define that for a factor between 0.9 and 1 there will be a code, another one for factors between 0.8 and 0.9, etc. You can put this association in a database for instance, but for my case it was good enough to hardcode them:</p>
<pre><code><span>function</span> <span>getDiscountForConversionFactor</span><span>(</span><span>pppConversionFactor</span><span>)</span> <span>{</span><br>  <span>if</span> <span>(</span>pppConversionFactor <span>&lt;=</span> <span>0.1</span><span>)</span> <span>return</span> <span>{</span> code<span>:</span> <span>'CSVVSDVV'</span><span>,</span> discount<span>:</span> <span>90</span> <span>}</span><br>  <span>if</span> <span>(</span>pppConversionFactor <span>&lt;=</span> <span>0.2</span><span>)</span> <span>return</span> <span>{</span> code<span>:</span> <span>'LLSJDLWF'</span><span>,</span> discount<span>:</span> <span>80</span> <span>}</span><br>  <span>if</span> <span>(</span>pppConversionFactor <span>&lt;=</span> <span>0.3</span><span>)</span> <span>return</span> <span>{</span> code<span>:</span> <span>'KRUFLDLF'</span><span>,</span> discount<span>:</span> <span>70</span> <span>}</span><br>  <span>if</span> <span>(</span>pppConversionFactor <span>&lt;=</span> <span>0.4</span><span>)</span> <span>return</span> <span>{</span> code<span>:</span> <span>'PJLKHJHI'</span><span>,</span> discount<span>:</span> <span>60</span> <span>}</span><br>  <span>if</span> <span>(</span>pppConversionFactor <span>&lt;=</span> <span>0.5</span><span>)</span> <span>return</span> <span>{</span> code<span>:</span> <span>'AGEFDXSL'</span><span>,</span> discount<span>:</span> <span>50</span> <span>}</span><br>  <span>if</span> <span>(</span>pppConversionFactor <span>&lt;=</span> <span>0.6</span><span>)</span> <span>return</span> <span>{</span> code<span>:</span> <span>'FDJGFYLX'</span><span>,</span> discount<span>:</span> <span>40</span> <span>}</span><br>  <span>if</span> <span>(</span>pppConversionFactor <span>&lt;=</span> <span>0.7</span><span>)</span> <span>return</span> <span>{</span> code<span>:</span> <span>'SYSDJSMF'</span><span>,</span> discount<span>:</span> <span>30</span> <span>}</span><br>  <span>if</span> <span>(</span>pppConversionFactor <span>&lt;=</span> <span>0.8</span><span>)</span> <span>return</span> <span>{</span> code<span>:</span> <span>'WUEJCCFJ'</span><span>,</span> discount<span>:</span> <span>20</span> <span>}</span><br>  <span>if</span> <span>(</span>pppConversionFactor <span>&lt;=</span> <span>1.0</span><span>)</span> <span>return</span> <span>{</span> code<span>:</span> <span>'DHFVUFKE'</span><span>,</span> discount<span>:</span> <span>10</span> <span>}</span><br>  <span>return</span> <span>{</span> code<span>:</span> <span>null</span><span>,</span> discount<span>:</span> <span>null</span> <span>}</span><br><span>}</span></code></pre>
<p>Note that I offer a discount for countries with a PPP of 1 (but not higher). I think being offered a discount code is always well-perceived by visitors.</p>
<p>Of course, the codes you have here must exist for your Stripe product, and they must remain secret! (These are for <a href="https://parity.coffee/">parity.coffee</a> and can be used freely 😉.)</p>
<p>This time, you don’t have to fetch the price from an endpoint (it will be the same for everyone), but you have to fetch the user’s country's discount code.</p>
<pre><code><span>async</span> <span>function</span> <span>getPrice</span><span>(</span><span>req<span>,</span> res</span><span>)</span> <span>{</span><br>  <span>const</span> country <span>=</span> <span>await</span> <span>getCountryForRequest</span><span>(</span>req<span>)</span><br>  <span>const</span> pppConversionFactor <span>=</span><br>    <span>(</span>country <span>&amp;&amp;</span> <span>(</span><span>await</span> <span>getConversionFactorForCountry</span><span>(</span>country<span>)</span><span>)</span><span>)</span> <span>||</span> <span>1</span><br>  <span>const</span> <span>{</span> discount<span>,</span> code <span>}</span> <span>=</span> <span>await</span> <span>getDiscountForConversionFactor</span><span>(</span><br>    pppConversionFactor<br>  <span>)</span><br>  res<span>.</span><span>send</span><span>(</span><span>{</span> discount<span>,</span> code<span>,</span> country <span>}</span><span>)</span><br><span>}</span></code></pre>
<p>Same as before, let’s …</p></article></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://scastiel.dev/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/">https://scastiel.dev/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/</a></em></p>]]>
            </description>
            <link>https://scastiel.dev/posts/2021-02-22-implement-ppp-fair-pricing-for-your-product/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26225110</guid>
            <pubDate>Mon, 22 Feb 2021 14:48:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Are these online pleas for humanitarian aid or ISIS fundraising?]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26224980">thread link</a>) | @gbseventeen3331
<br/>
February 22, 2021 | https://restofworld.org/2021/humanitarian-aid-or-isis-fundraising/ | <a href="https://web.archive.org/web/*/https://restofworld.org/2021/humanitarian-aid-or-isis-fundraising/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

			<!-- Article Start -->
			
<p><span>I</span>t’s the kind of picture that most Facebook users would scroll by without pause: the sun hanging low in a blue sky, half-hidden by the silhouette of a tent. “Donate [for the sake of Allah] to free your imprisoned sisters,” a line of embedded text reads, with the hashtag #CampHol. “Contact to help us,” the caption adds, including the name of a Telegram account.</p>



<p>Even a user who did take the time to look closer might assume the post was related to an Islamic charity or perhaps a fundraiser for human trafficking victims, then move on. Its intended audience, however, would immediately recognize the link to ISIS.</p>



<p>Al-Hol refugee camp in Kurdish-controlled northeastern Syria is home to the women and children who lived in ISIS’s last remaining pockets of territory before they were retaken by the Syrian Democratic Forces in March 2019. The majority of residents are Iraqi and Syrian, but there is also a separate annex for women who traveled to live in the so-called caliphate. These women, some of whom emigrated from Europe, Asia, and Africa, are seen by locals as more fanatical than those in other parts of the camp. A United Nations <a href="https://reliefweb.int/report/syrian-arab-republic/syrian-arab-republic-north-east-syria-al-hol-camp-21-november-2019">report</a> from November 2019 found that foreigners made up 15% of the camp’s total population of 70,000, although some have since been repatriated or moved to more secure facilities.</p>



<p>A sprawling mass of dusty tents surrounded by fences and armed guards, al-Hol’s foreigners’ annex has become a place of radicalization and extremism. There, hard-line Islamists have reimposed ISIS’s draconian rules <strong>—</strong> any woman or girl over eight must be fully veiled in black; communication with authorities or journalists is forbidden; daily prayer is mandatory —<strong> </strong>and been known to beat and murder transgressors. With resources already overextended, the camp staff have little ability to intervene.</p>



<p>Last year, online fundraisers began to appear on behalf of al-Hol residents. Many were seeking to finance escapes, others to pay for food and supplies. (While some donations have likely gone toward terrorism, the campaigns are careful to avoid mentioning violence.) The petitions spread via social networks, including Facebook, Instagram, and Twitter, and often involved PayPal and other payment systems as well as messaging apps, like WhatsApp and Telegram. Before long, intelligence and law enforcement agencies began to monitor them.</p>


		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/Screenshot-2020-12-11-at-23.45.57-40x72.png" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/Screenshot-2020-12-11-at-23.45.57-600x1066.png" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/Screenshot-2020-12-11-at-23.45.57-400x724.png 400w, https://restofworld.org/wp-content/uploads/2021/02/Screenshot-2020-12-11-at-23.45.57-600x1086.png 600w, " sizes="300px" alt="Last year, fundraisers started appearing on social media on behalf of al-Hol residents.">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				<span itemprop="copyrightHolder">Facebook</span>
			</figcaption>
		</figure>


<p>Media reports followed, and any platform that was not already aware of the campaigns’ existence was soon clued in. Even so, several months later, this kind of content remains relatively easy to find. <em>Rest of World </em>was able to identify dozens of Facebook accounts claiming to be linked to al-Hol, many of which post comments glorifying ISIS or soliciting funds. “Do not fear the imprisonment of the [unbelievers] for helping your sisters,” reads one, inviting supporters to message privately for more information. On Instagram, an account consisting mostly of images and videos from al-Hol included a picture of a figure clad all in black holding up a cardboard sign. It reads, “WE ARE TWO SISTERS FROM CAMP AL HOL AND WE ARE TRYING TO ESCAPE … WE COLLECTED 13,000$ AND WE NEED 3000$ PLEASE WE BEG THE UMMAH TO HELP US AND DONATE AS MUCH AS THEY CAN.” Below, a caption elaborating on the message is translated into Turkish, English, Russian, and French.</p>



<p>Accounts like these often remain active for months. This is possible, in part, because campaigns extend across multiple networks and payment platforms, creating a complex and opaque ecosystem that sometimes mixes illegal payment solicitations with requests for legitimate charitable giving. As major social media companies scramble to figure out policies around hate speech and disinformation, ISIS-related fundraisers have continued to slip through.</p>


		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/al-hol-04854-40x27.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/al-hol-04854-768x432.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/al-hol-04854-400x267.jpg 400w, https://restofworld.org/wp-content/uploads/2021/02/al-hol-04854-600x401.jpg 600w, https://restofworld.org/wp-content/uploads/2021/02/al-hol-04854-1000x668.jpg 1000w, https://restofworld.org/wp-content/uploads/2021/02/al-hol-04854-1600x1069.jpg 1600w, https://restofworld.org/wp-content/uploads/2021/02/al-hol-04854-2800x1870.jpg 2800w, " sizes="(max-width: 640px) 100vw, (max-width: 992px) calc(100vw - 40px), (max-width: 1140px) calc(100vw - 290px), calc(100vw - ((100vw - 640px)/2))" alt="Women stand at a registration office at al-Hol camp, looking for documents in their phones they need to print to apply for permission to leave the camp.">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				
			</figcaption>
		</figure>


<hr>



<p><strong>In the more</strong> than five years that ISIS held territory in Iraq and Syria, it generated vast sums of money — from oil fields, plundering banks, drug and artifact smuggling, taxing people living under its regime, and taking hostages. The French government alone was <a href="http://www.focus.de/politik/ausland/krise-in-der-arabischen-welt/syrien/paris-zahlt-loesegeld-18-millionen-dollar-fuer-entfuehrte-journalisten_id_3800633.html">reported</a> to have paid an $18 million ransom to secure the release of four journalists. (A government spokesman denied this account.)</p>



<p>Although the revenues generated by online campaigns are minuscule in comparison, they are not insignificant — Audrey L. Alexander, a researcher and instructor at West Point’s Combating Terrorism Center, told <em>Rest of World</em> that she regularly encounters crowdfunding efforts by terrorists, some of which bring in as much as $2,000 — and this income likely helps keep ISIS activities going. It’s impossible to say with any certainty how much ISIS receives in online donations, but the money has been linked to escapes, weapons purchases, and propaganda produced by al-Hol-linked accounts, which have replaced the high-production-value videos once released by ISIS’s media arm.</p>


		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/IMG_1496-40x87.png" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/IMG_1496-600x1066.png" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/IMG_1496-400x866.png 400w, https://restofworld.org/wp-content/uploads/2021/02/IMG_1496-600x1299.png 600w, " sizes="(max-width: 640px) 100vw, 300px" alt="As major social media companies scramble to figure out policies around hate speech and disinformation, ISIS-related fundraisers have continued to slip through.">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				<span itemprop="copyrightHolder">Instagram</span>
			</figcaption>
		</figure>


<p>In a March <a href="https://media.defense.gov/2020/May/13/2002298979/-1/-1/1/LIG_OIR_Q2_MAR2020_GOLD_508_0513.PDF">report</a> to Congress, the American-led coalition against ISIS, Operation Inherent Resolve, detailed how women within the camps, and particularly al-Hol, have built up a network for smuggling people and supplies, while simultaneously recruiting and indoctrinating new members. “Using funds received via wire transfers,” the authors noted, “female ISIS members continued to conduct operations — such as attacks against camp security personnel.” While this amplified existing pressure on social media companies to better monitor their platforms, it didn’t appear to have any major policy impacts.</p>



<p>The architects of these networks tailor their messages and methods to geography, specific donors and goals, and national laws and platform regulations. Of the Facebook accounts identified by <em>Rest of World</em> that claim links to al-Hol, only some explicitly asked for donations. Others disseminated pictures or news from the camp in different languages, alongside Islamic scripture and memes. A few users fondly reminisced about their time in the caliphate. Facebook disables and deletes accounts that share terrorist propaganda, so ISIS was never explicitly mentioned. Instead, references to the organization were camouflaged by alternative spellings. “I miss the Dawl@,” one said, with a crying emoji, referencing the Arabic word for “state” in ISIS’s full name.</p>



<p>Facebook says it has been making major investments to combat the proliferation of terrorist content on its platform. Much of that relies on AI and on media-matching software that finds images, text, or videos that are either identical or near identical to content that has already been taken down. Once identified, they are removed nearly instantly.</p>



<p>“Facebook has no tolerance for terrorist propaganda or content fundraising for terrorist groups. We take this extremely seriously and we are investing heavily to keep people safe,” a spokesperson told <em>Rest of World</em> in an emailed statement. “Over the last few years we’ve tripled the size of our safety and security team to 35,000 and built artificial intelligence technology to find and remove this content before people see it and report it to us. From June to September 2020, we removed over 9.7 million pieces of terrorist content on Facebook, 99% of which we detected proactively.”</p>



<p>While undoubtedly effective, aggressive AI-based content removal isn’t perfect. Journalists across the Middle East and North Africa regularly have their Facebook and Twitter accounts <a href="https://www.nbcnews.com/tech/tech-news/facebook-doesn-t-care-activists-say-accounts-removed-despite-zuckerberg-n1231110">suspended</a> after posting material related to conflicts or human rights abuses. Charities working with Syrian refugees have for years complained about <a href="https://slate.com/technology/2020/02/paypal-venmo-iran-syria-sanctions-crime-detection-system.html">PayPal’s keyword filters</a> blocking donations. And as social media companies get faster at taking down content, often without anyone having seen it, <a href="https://www.hrw.org/report/2020/09/10/video-unavailable/social-media-platforms-remove-evidence-war-crimes">human rights groups</a> and activists worry that they may be erasing vital evidence of war crimes that could be of use in future trials.</p>



<p>AI has other weaknesses too. Accounts that are more subtle about illegal affiliations may go unnoticed, allowing disguised ISIS content to slip through. Of the more than 40 apparently al-Hol-linked Facebook accounts found by <em>Rest of World</em> in October, only around half had been removed by December. Additionally, account administrators often maintain a network of duplicate or backup accounts, so that if one is blocked or removed, others are ready to take its place.</p>


		<figure>
			<div>
				<p><img src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/al-hol-05136-40x27.jpg" data-src="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/al-hol-05136-768x432.jpg" data-srcset="https://149346090.v2.pressablecdn.com/wp-content/uploads/2021/02/al-hol-05136-400x267.jpg 400w, https://restofworld.org/wp-content/uploads/2021/02/al-hol-05136-600x401.jpg 600w, https://restofworld.org/wp-content/uploads/2021/02/al-hol-05136-1000x668.jpg 1000w, https://restofworld.org/wp-content/uploads/2021/02/al-hol-05136-1600x1069.jpg 1600w, https://restofworld.org/wp-content/uploads/2021/02/al-hol-05136-2800x1870.jpg 2800w, " sizes="(max-width: 640px) 100vw, calc(100vw - 40px)" alt="Women and children walk through al-Hol camp.">
					
				</p>
			</div>
						<figcaption itemprop="caption description">
				
				
			</figcaption>
		</figure>


<hr>



<p><strong>In contrast to</strong> Facebook, Telegram has relatively few safeguards against terrorist content. Launched in 2013 by Russians Nikolai and Pavel Durov, the Dubai-based company is <a href="https://restofworld.org/2020/silk-road-is-dead-long-live-silk-road/">known for prioritizing privacy and free speech over nearly all other concerns.</a> This stance has made it popular among people living in authoritarian regimes, and it has also made it the app of choice for violent extremists. The company has conducted coordinated sweeps to remove ISIS content, but channels openly devoted to ISIS news and propaganda still regularly appear — including ones that raise money for escape attempts from al-Hol. (Telegram did not respond to requests for comment.)</p>



<p>While many jurisdictions hold the person who posts illegal content — rather than the platform itself — accountable, that may be changing in the U.K. and the E.U., where new draft legislation being considered would place responsibility on platforms to police themselves.</p>



<p>Vera Mironova, a visiting fellow at Harvard University who has extensively monitored online terrorist fundraising campaigns, notes that posts follow the mores of their host platform. “So secretive campaigns would not be posted on Facebook, or if they were, they would sound more humanitarian and not use words like ‘ISIS.’ But the ones on Telegram go full hurrah,” she explained. This same dynamic plays out on a country-by-country level, Mironova added, and is especially apparent on payment platforms. “Some countries — let’s say Russia or parts of …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://restofworld.org/2021/humanitarian-aid-or-isis-fundraising/">https://restofworld.org/2021/humanitarian-aid-or-isis-fundraising/</a></em></p>]]>
            </description>
            <link>https://restofworld.org/2021/humanitarian-aid-or-isis-fundraising/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26224980</guid>
            <pubDate>Mon, 22 Feb 2021 14:36:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[AWS Parameter Store vs. Secrets Manager]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=26224915">thread link</a>) | @skwashd
<br/>
February 22, 2021 | https://www.davehall.com.au/blog/2021/02/22/parameter-store-vs-secrets-manager/ | <a href="https://web.archive.org/web/*/https://www.davehall.com.au/blog/2021/02/22/parameter-store-vs-secrets-manager/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article><section><div><div><p>The handling of secrets in AWS is up there with tabs vs spaces and vim vs emacs
in terms of technical debates. In one corner we have Amazon’s original secrets
store <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html">System Manager Parameter
Store</a>.
In the other is the new(er) challenger, <a href="https://aws.amazon.com/secrets-manager/">Secrets
Manager</a>. Let’s see how they compare.</p><p>We won’t be looking at HashiCorp’s Vault in this comparison, because the focus
of this post is to compare AWS’ managed service. This is one of those occasions
where it is cheaper to accept some vendor lock in and avoid the hassle of
managing a cluster of Consul nodes.</p><p>This comparison won’t cover every little detail. We will stick to the key
differences between the two tools with the aim of helping you choose the best
one for your use case.</p><h2 id="round-1-key-value-store">Round 1: Key Value Store</h2><p>At the heart of both services is a managed key value store. You send your
sensitive data to Amazon and they store it until you need it. Each value is
referenced via a unique key that you define.</p><p>Both services allow you to name your secrets using simple strings. Parameter
store allows keys to be any mix of <code>a-zA-Z0-9_.-</code> up to 966 characters, while
secrets manager’s limit is 512 unicode characters.</p><p>Parameter store allows you to store your secrets in a hierarchy. By using a path
structure you build up the structure. So instead of simple names such as
<code>DB_URI</code> you can use something more complex like <code>/myapp/DB_URI</code>. The Parameter
Store API allows you to fetch all the values in the hierarchy with a single
call. This is really handy when you have multiple values stored for an
application.</p><p><strong>Winner:</strong> Parameter Store for supporting hierarchical structures.</p><h2 id="round-2-storage-limitations">Round 2: Storage Limitations</h2><p>Both services allow users to store any unicode string. Standard SSM Parameters
are limited to 4Kb, while their advanced siblings can be up to 8Kb. Secrets
manager allows values up to 64Kb. Depends on your data storage needs these
limits may impact choice of service.</p><p>Both services retain 100 revisions of your secret. That can be handy if someone
accidentally overwrites the wrong value.</p><p><strong>Winner:</strong> Secrets Manager for higher value limits</p><h2 id="round-3-encryption">Round 3: Encryption</h2><p>Both parameter store and secrets manager store your secrets in an encrypted
state using KMS encryption keys. This ensures your sensitive credentials are
kept secure.</p><p>Unlike secrets manager, parameter store allows to decide if you want your values
to be stored unencrypted. While this isn’t advisable for secrets it can useful
for non sensitive information. If you hit a modified time stamp, check sum or
other non sensitive value option, this can be useful. It reduces the number of
KMS API calls and leads to faster response times. This makes it easier to use
parameter store as your single solution for application configuration
management.</p><p><strong>Winner:</strong> Parameter Store for the extra flexibility</p><h2 id="round-4-rotation">Round 4: Rotation</h2><p>Rotating credentials can be a tedious task that can result in downtime. Amazon
promotes the credentials rotation feature in Secrets Manager. This is mostly
marketing hype. The feature is limited to databases and it is really just an
easy way to deploy a Lambda function that does the rotation. There are similar
Lambdas available to do this with Parameter Store.</p><p><strong>Winner:</strong> Draw, we won’t reward over hyping features</p><h2 id="round-5-cost">Round 5: Cost</h2><p>Parameter Store has two flavours of parameters, standard and advanced. Standard
parameters don’t incur any monthly storage fees. Adding to the complexity there
are two price tiers for interacting with the Parameter Store API, standard and
high throughput. As you have already guessed you pay for the <a href="https://docs.aws.amazon.com/general/latest/gr/ssm.html">higher
quotas</a>. While standard
is free, the high rate will cost you 0.05USD per 10000 interactions. The higher
throughput is an <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/parameter-store-throughput.html">account wide
setting</a>.</p><p>For advanced parameters you pay 0.05USD per parameter per month. You decide when
creating a parameter if it will be an advanced. You pay 0.05USD per 10000 API
interactions with advanced parameters. The one consolation is that you won’t pay
extra for enabling high throughput when fetching advanced parameters.</p><p>With Secrets Manager you pay for everything. You will pay 0.40USD per secret per
month, then 0.05USD per 10000 API interactions. The <a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/reference_limits.html">Secrets Manager API
quotas</a>
are the highest of the 3 options.</p><p>On top of these costs, you will <a href="https://aws.amazon.com/kms/pricing/">pay 0.03USD for 10000 KMS API
requests</a>. Even if you fetch multiple
parameters in a single API call, you will pay to decrypt each one individually.</p><p><strong>Winner:</strong> Parameter Store standard cos you can’t beat free.</p><h2 id="the-verdict">The Verdict</h2><p>There is no knock out winner in this contest. It is going to come down to a
points decision. Our judges have awarded the win to Parameter Store.</p><p>While Secrets Manager can be a better fit for some use cases, often it is
overkill. Unless you require the larger storage limits or very high throughput
you’re wasting money on secrets manager.</p><p>If you can work within the constraints of the standard tier of Parameter Store
it is a very cost effective tool for managing your secrets and other application
configuration.</p><p>Our earlier blog post on <a href="https://www.davehall.com.au/blog/2018/08/26/aws-parameter-store/">AWS System Manager Parameter
Store</a> is a great introduction for new
users.</p></div></div></section></article></div>]]>
            </description>
            <link>https://www.davehall.com.au/blog/2021/02/22/parameter-store-vs-secrets-manager/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26224915</guid>
            <pubDate>Mon, 22 Feb 2021 14:29:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Microsoft joins forces with European news publishers]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26224860">thread link</a>) | @samizdis
<br/>
February 22, 2021 | https://www.techregister.co.uk/microsoft-joins-forces-with-european-news-publishers/ | <a href="https://web.archive.org/web/*/https://www.techregister.co.uk/microsoft-joins-forces-with-european-news-publishers/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<p>Microsoft has joined forces with Europe’s publishers to deepen the troubles of Google and Facebook, launching a project to develop an Australian-style arbitration system for the EU that would force Big Tech to pay for news.</p>
<p>The move by the Seattle-based company is one of its most brazen yet to align with the press industry, exploit the difficulties of its Silicon Valley rivals and promote its own search engine Bing as a copyright-friendly alternative for news. </p>
<p>The project announced on Monday will involve Microsoft working with Europe’s four leading lobby groups for news publishers to develop a legal solution to “mandate payments” for the use of content by “gatekeepers that have dominant market power”. </p>
<p>The informal coalition, which will propose that the plan is added to upcoming EU legislation on Big Tech, includes the European Publishers Council, News Media Europe, and the associations for European magazine and newspaper publishers, which together represent thousands of news outlets.</p>
<p>Microsoft and the publishers said on Monday that they will support a form of arbitration, and will look closely at the model developed in Australia, which has prompted Google to strike a flurry of licensing deals and Facebook to stop sharing Australian news on its service.</p>
<p>Christian Van Thillo, a Belgian media executive who is chair of the European Publishers Council, welcomed “Microsoft’s recognition” of the value “our content brings to the core business of search engines and social networks”.</p>
<p>“It is crucial that our regulators recognise this key point, and don’t get misled into thinking that side deals on the basis of a standalone product are the same thing,” he said, adding: “All publishers should get an agreement — no one should be left out.”</p>
<p>Microsoft has offered vocal public support for the Australian reforms and has urged other governments to follow suit, much to the chagrin of its rivals. </p>
<p>Unveiling the project with European publishers, Casper Klynge, a vice-president of Microsoft, said access to quality news was “critical to the success of our democracies”. </p>
<p>The Australian system has caught the eye of regulators around the world, who are also looking for ways to empower publishers in licensing negotiations with Google and Facebook. </p>
<p>Canada is preparing Australia-style laws, and the EU and UK are looking at importing elements of the system into upcoming laws. It remains unclear whether the calculations of lawmakers have been changed by Facebook’s decision to boycott news in Australia.</p>
<p>EU governments are in the process of implementing a recent overhaul of copyright law, which strengthened the claim of publishers to seek compensation for the use of news snippets by Google. </p>
<p>But industry executives and some MEPs are concerned that the provisions, which do not include any arbitration system to resolve disputes, are too easy for Big Tech groups to sidestep. Google recently reached a licensing deal with French publishers, but paid much smaller sums than the settlements agreed with Australian publishers.</p>
<p>Fernando de Yarza, president of News Media Europe, said: “The experiences in France and Australia have shown us that there’s a real need for a binding instrument.”</p>
<p>The Financial Times has reached commercial agreements for news with both Google and Facebook. The FT is not a member of any of the associations involved in the Microsoft initiative.</p>
<p>Google and Facebook both strongly criticise the Australian reforms as unworkable and unfair. Neither company had commented on Microsoft’s initiative in Europe by the time of publication. </p>
</div></div>]]>
            </description>
            <link>https://www.techregister.co.uk/microsoft-joins-forces-with-european-news-publishers/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26224860</guid>
            <pubDate>Mon, 22 Feb 2021 14:24:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[What Do We Talk About When We Talk About Dashboards? (2018)]]>
            </title>
            <description>
<![CDATA[
Score 64 | Comments 11 (<a href="https://news.ycombinator.com/item?id=26224846">thread link</a>) | @sebg
<br/>
February 22, 2021 | https://alper.datav.is/publications/dashboards/ | <a href="https://web.archive.org/web/*/https://alper.datav.is/publications/dashboards/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
      <p>Dashboards have long been the much maligned visualization vehicle of choice for decision-making in commercial and governmental situations.  While the visualization research community has concentrated much of its effort on visual analytics, the commercial success and widespread use of dashboards begs more attention.  Critically, dashboards are becoming many peoples’ direct connection to “big data” sources, enabling data democratization and wider access to data.</p>

<p>In this paper, we explore the genre of dashboards through a two-prong approach.  We survey the existing literature in business, marketing, and related fields to capture the relevant factors to consider when designing appropriate dashboards and their tools for consumption by different parties, all of which have differing levels of visualization literacy, data literacy, and decision agency.  We also collect examples of dashboard designs based on the dimensions derived from our literature search, and identify different clusters of dashboard designs with similar analysis goals, audiences, and decision support.</p>

<p>We call ourselves the “dashboard conspiracy:” a truly diverse collection of authors across Tableau Research, Microsoft Research, and Simon Fraiser University.</p>

<p><em>This work was presented at <a href="http://ieeevis.org/year/2018/welcome">IEEE VIS 2018</a> in Berlin, Germany.</em></p>

<p><em>This work was discussed in an half-hour datastori.es podcast, <a href="https://datastori.es/135-the-dashboard-conspiracy-with-lyn-bartram-and-alper-sarikaya/">give it a listen</a>!</em></p>

    </div></div>]]>
            </description>
            <link>https://alper.datav.is/publications/dashboards/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26224846</guid>
            <pubDate>Mon, 22 Feb 2021 14:22:15 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The State of GraphQL 2020]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26224536">thread link</a>) | @lukzar
<br/>
February 22, 2021 | https://blog.graphqleditor.com/state-of-graphql-2020 | <a href="https://web.archive.org/web/*/https://blog.graphqleditor.com/state-of-graphql-2020">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>The State of JavaScript is an annual survey that collects data from JS professionals from across the globe. This year’s edition questioned  23,765 developers in 137 countries about development areas such as:</p>
<ul>
<li>Front-end frameworks,</li>
<li>Back-end framework</li>
<li>JavaScript flavors,</li>
<li>Testing libraries,</li>
<li>Build tools,</li>
<li><em>Data layer</em>.</li>
</ul>
<p>Let’s take a look at GraphQL data concluded in the Data layer part of the survey.</p>
<h2>Data layer report</h2>
<p>The data layer part covers technologies used to transmit and manage data. The users were asked about their awareness, interest, usage experience, and satisfaction with various data layer libraries (including GraphQ) and here are the results.</p>
<h4>The awareness and interest</h4>
<p>Since becoming publicly available in 2015 GraphQL has received a lot of coverage on the Internet, both positive and negative. The awareness of GraphQL is constantly growing (from 97% to 98% comparing to the previous year) while the interest graph shows a little decline (from 90% to 87%), which seems to be pretty natural for maturing technology.</p>
<h4>The usage of GraphQL</h4>
<p>The growth of GraphQL usage among survey responders was the biggest between 2018 and 2019 and it amounted to 40% (from 22%) so it’s unrealistic to expect the same pace of growth. <strong>In 2020 the usage of GraphQL has gained 6%</strong> which seems to be a fine result, especially when thinking about GraphQL as a somehow mature technology.</p>
<p><span>
      <a href="https://blog.graphqleditor.com/static/65f03c030e0fc5e58d888ed7b0b11c4f/52ab5/usage.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Usage" title="Usage" src="https://blog.graphqleditor.com/static/65f03c030e0fc5e58d888ed7b0b11c4f/fcda8/usage.png" srcset="https://blog.graphqleditor.com/static/65f03c030e0fc5e58d888ed7b0b11c4f/12f09/usage.png 148w,
https://blog.graphqleditor.com/static/65f03c030e0fc5e58d888ed7b0b11c4f/e4a3f/usage.png 295w,
https://blog.graphqleditor.com/static/65f03c030e0fc5e58d888ed7b0b11c4f/fcda8/usage.png 590w,
https://blog.graphqleditor.com/static/65f03c030e0fc5e58d888ed7b0b11c4f/efc66/usage.png 885w,
https://blog.graphqleditor.com/static/65f03c030e0fc5e58d888ed7b0b11c4f/c83ae/usage.png 1180w,
https://blog.graphqleditor.com/static/65f03c030e0fc5e58d888ed7b0b11c4f/52ab5/usage.png 1420w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span></p>
<h4>The satisfaction of GraphQL</h4>
<p>The satisfaction of GraphQL remains and nearly the same level.
The advantages and flaws of GraphQL are factually described in various articles, blog posts and talks so users deciding to give GraphQL know what they are signing for. GraphQL has a great community standing behind it, working hard every day to provide solutions, tools and different ways to overcome all its shortcomings. </p>
<p><span>
      <a href="https://blog.graphqleditor.com/static/b5abe7c825191f7594e8e03b16eff45e/6b95e/satisfaction.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Satisfaction" title="Satisfaction" src="https://blog.graphqleditor.com/static/b5abe7c825191f7594e8e03b16eff45e/fcda8/satisfaction.png" srcset="https://blog.graphqleditor.com/static/b5abe7c825191f7594e8e03b16eff45e/12f09/satisfaction.png 148w,
https://blog.graphqleditor.com/static/b5abe7c825191f7594e8e03b16eff45e/e4a3f/satisfaction.png 295w,
https://blog.graphqleditor.com/static/b5abe7c825191f7594e8e03b16eff45e/fcda8/satisfaction.png 590w,
https://blog.graphqleditor.com/static/b5abe7c825191f7594e8e03b16eff45e/efc66/satisfaction.png 885w,
https://blog.graphqleditor.com/static/b5abe7c825191f7594e8e03b16eff45e/c83ae/satisfaction.png 1180w,
https://blog.graphqleditor.com/static/b5abe7c825191f7594e8e03b16eff45e/6b95e/satisfaction.png 1458w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span></p>
<h4>The GraphQL Experience</h4>
<p>The general experience observed in past years shows a positive tone. The number of people that never heard, are not interested or wound not use GraphQL has decreased significantly and the latest results show that 88.1% of respondents are either interested in GraphQL or declares that they have already worked with and would do it again.</p>
<p><span>
      <a href="https://blog.graphqleditor.com/static/37fa2eaba70849065382025ad56c825e/82b28/graphql.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="GraphQL Experience over time" title="GraphQL Experience over time" src="https://blog.graphqleditor.com/static/37fa2eaba70849065382025ad56c825e/fcda8/graphql.png" srcset="https://blog.graphqleditor.com/static/37fa2eaba70849065382025ad56c825e/12f09/graphql.png 148w,
https://blog.graphqleditor.com/static/37fa2eaba70849065382025ad56c825e/e4a3f/graphql.png 295w,
https://blog.graphqleditor.com/static/37fa2eaba70849065382025ad56c825e/fcda8/graphql.png 590w,
https://blog.graphqleditor.com/static/37fa2eaba70849065382025ad56c825e/efc66/graphql.png 885w,
https://blog.graphqleditor.com/static/37fa2eaba70849065382025ad56c825e/82b28/graphql.png 931w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span></p>
<p>The below chart presents the Positive vs Negative responses split and the GraphQL results come out very positive. The GraphQL wins significantly in the data layer category.</p>
<p><span>
      <a href="https://blog.graphqleditor.com/static/d4d4513fcd0d7090b77163136ac7ab3e/c0566/posneg.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="Positive vs Negative split" title="Positive vs Negative split" src="https://blog.graphqleditor.com/static/d4d4513fcd0d7090b77163136ac7ab3e/fcda8/posneg.png" srcset="https://blog.graphqleditor.com/static/d4d4513fcd0d7090b77163136ac7ab3e/12f09/posneg.png 148w,
https://blog.graphqleditor.com/static/d4d4513fcd0d7090b77163136ac7ab3e/e4a3f/posneg.png 295w,
https://blog.graphqleditor.com/static/d4d4513fcd0d7090b77163136ac7ab3e/fcda8/posneg.png 590w,
https://blog.graphqleditor.com/static/d4d4513fcd0d7090b77163136ac7ab3e/efc66/posneg.png 885w,
https://blog.graphqleditor.com/static/d4d4513fcd0d7090b77163136ac7ab3e/c83ae/posneg.png 1180w,
https://blog.graphqleditor.com/static/d4d4513fcd0d7090b77163136ac7ab3e/c0566/posneg.png 1544w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span></p>
<h2>The summary</h2>
<p>The data layer space is still in constant movement which makes selecting the right technology for your needs a bit tricky. The survey administrators decided to prepare a data graph that could possibly really help you decide if the technology you are looking into is going in the right direction and ease the process of the decision if you should start seriously thinking about adopting it.</p>
<p><span>
      <a href="https://blog.graphqleditor.com/static/50c2e320890d44205359412979481f03/8733b/change-ot.png" target="_blank" rel="noopener">
    <span></span>
  <img alt="GraphQL over last years graph" title="GraphQL over last years graph" src="https://blog.graphqleditor.com/static/50c2e320890d44205359412979481f03/fcda8/change-ot.png" srcset="https://blog.graphqleditor.com/static/50c2e320890d44205359412979481f03/12f09/change-ot.png 148w,
https://blog.graphqleditor.com/static/50c2e320890d44205359412979481f03/e4a3f/change-ot.png 295w,
https://blog.graphqleditor.com/static/50c2e320890d44205359412979481f03/fcda8/change-ot.png 590w,
https://blog.graphqleditor.com/static/50c2e320890d44205359412979481f03/efc66/change-ot.png 885w,
https://blog.graphqleditor.com/static/50c2e320890d44205359412979481f03/c83ae/change-ot.png 1180w,
https://blog.graphqleditor.com/static/50c2e320890d44205359412979481f03/8733b/change-ot.png 1495w" sizes="(max-width: 590px) 100vw, 590px" loading="lazy">
  </a>
    </span></p>
<p>Each of the lines represents different technology and is filled by the data from 2016 to 2020. The higher position on the Y-axis means that the technology has been used by more people, and a point further to the right on the X-axis means more users have used it and would use it again or are interested in learning more about it.</p>
<p>Over a couple of last years, GraphQL has ranked up from technology worth keeping an eye on (with low usage, but high satisfaction) to a date later characterizing in high usage and satisfaction which makes it a safe technology to adopt. The general conclusion is that GraphQL and all the technologies, libraries, tools its fuelling are here to stay. </p>
<hr>
<p><em>All graphs and data comes from the StateofJs.com, if you are interested in more details regarding data layer or other JS aspects make sure to visit <a href="https://2020.stateofjs.com/en-US/technologies/datalayer/">2020.stateofjs.com</a></em></p></div><p>The GraphQL Editor is a supportive tool for both advanced GraphQL users as well as those taking their first steps with GraphQL APIs. Our all-in-one development environment for GraphQL will help you build, manage &amp; deploy your GraphQL API much faster thanks to dozens of built-in micro features. Its graphical interface will also fix communication within your product team. Visualization is the key!</p></div>]]>
            </description>
            <link>https://blog.graphqleditor.com/state-of-graphql-2020</link>
            <guid isPermaLink="false">hacker-news-small-sites-26224536</guid>
            <pubDate>Mon, 22 Feb 2021 13:57:17 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Monitor the services you use, from your menu bar]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26224534">thread link</a>) | @alollou
<br/>
February 22, 2021 | https://instatus.com/out | <a href="https://web.archive.org/web/*/https://instatus.com/out">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><a href="https://instatus.com/out/download/mac"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M376 160H272v153.37l52.69-52.68a16 16 0 0122.62 22.62l-80 80a16 16 0 01-22.62 0l-80-80a16 16 0 0122.62-22.62L240 313.37V160H136a56.06 56.06 0 00-56 56v208a56.06 56.06 0 0056 56h240a56.06 56.06 0 0056-56V216a56.06 56.06 0 00-56-56zM272 48a16 16 0 00-32 0v112h32z"></path></svg>Get on mac OS</a><a href="https://instatus.com/out/video" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M464 384.39a32 32 0 01-13-2.77 15.77 15.77 0 01-2.71-1.54l-82.71-58.22A32 32 0 01352 295.7v-79.4a32 32 0 0113.58-26.16l82.71-58.22a15.77 15.77 0 012.71-1.54 32 32 0 0145 29.24v192.76a32 32 0 01-32 32zM268 400H84a68.07 68.07 0 01-68-68V180a68.07 68.07 0 0168-68h184.48A67.6 67.6 0 01336 179.52V332a68.07 68.07 0 01-68 68z"></path></svg>Watch intro video</a><div><p>Select services you depend on</p><p>Check their status in your menu bar</p><p>Get notified when they change their status</p></div></div></div>]]>
            </description>
            <link>https://instatus.com/out</link>
            <guid isPermaLink="false">hacker-news-small-sites-26224534</guid>
            <pubDate>Mon, 22 Feb 2021 13:57:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Calls for Bitcoin Plunge Emerge over Mysterious $1.5 Bn BTC Transfer]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26224353">thread link</a>) | @nwotnagrom
<br/>
February 22, 2021 | https://www.bitcoinprofit.app/news/calls-for-bitcoin-plunge-emerge-over-mysterious-1-5-bn-btc-transfer/ | <a href="https://web.archive.org/web/*/https://www.bitcoinprofit.app/news/calls-for-bitcoin-plunge-emerge-over-mysterious-1-5-bn-btc-transfer/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p> A high-volumed transfer to a Bitcoin exchange purse made on February 21 has actually increased calls for a wider rate improvement amongst risk-averse investors.</p><p>An entity (or a team of entities) attributed regarding 28,000 BTC worth over $1.5 billion to an address that apparently comes from OKEx’s over the counter solutions. A Twitterati kept in mind that the OTC address better attributed BTC right into numerous pocketbooks, among which apparently comes from a “rich” address that has actually revealed organizations with several cloud mining rip-offs as well as cash laundering tasks in Asia.</p><div id="attachment_146788"><p><img aria-describedby="caption-attachment-146788" loading="lazy" src="https://www.bitcoinprofit.app/wp-content/uploads/2021/02/Calls-for-Bitcoin-Plunge-Emerge-Over-Mysterious-15bn-BTC-Transfer.jpeg" alt="Bitcoin, cryptocurrency, BTCUSD, BTCUSDT" width="909" height="660"></p><p id="caption-attachment-146788">The highlighted address apparently comes from a bitcoin scammer. Source: <a href="https://twitter.com/thisisbullish/status/1363622946112237573/photo/1" target="_blank" rel="noopener follow" data-wpel-link="exclude">This Is Bullish</a></p></div><p>Analysts regard bigger crypto transfers to exchanges as well as their linked solutions as an indication of impending marketing stress. An investor probably down payments bitcoins to public pocketbooks when s/he plans to market them for cash money or exchange them for various other cryptocurrency symbols.</p><p>Conversely, bigger withdrawals indicate their objective of not selling/exchanging yet holding the bitcoins.</p><h2>Bitcoin Liquidity</h2><p>Of late, information on exchanges revealed substantial decrease in exchanges’ BTC books, coming by around 635,000 from its March 2020 top, simply timid of 3 million. They mostly accompanied a remarkable increase in the BTC/ USD currency exchange rate, which increased by around 1,200 percent in the exact same duration.</p><div id="attachment_146789"><p><img aria-describedby="caption-attachment-146789" loading="lazy" src="https://www.bitcoinprofit.app/wp-content/uploads/2021/02/Calls-for-Bitcoin-Plunge-Emerge-Over-Mysterious-15bn-BTC-Transfer.png" alt="Bitcoin, cryptocurrency, BTCUSD, BTCUSDT" width="893" height="660"></p><p id="caption-attachment-146789">Bitcoin books on all exchanges dove greatly given that March 2020. Source: <a href="https://cryptoquant.com/overview/btc-exchange-flows" target="_blank" rel="noopener nofollow external" data-wpel-link="external">CryptoQuant</a></p></div><p>The OKEx down payment, as stated over, at the same time, showed up when Bitcoin was revealing indications of peaking. On Sunday, the cryptocurrency attained a brand-new rate turning point over $58,000, leaving the Twitterati worried regarding an unavoidable sell-off in advance.</p><blockquote><p><span>“The ‘OKEx Whale’ is ‘LOUD’ in the way they conduct business, they don’t care about </span><span>#hodl</span><span> or </span><span>#lazereyes,” the pseudonymous blockchain private investigator clarified.</span><span> “[It is] happy to market dump on you. This coin flow tells us they now have ammo to increase sell-pressure in the future.”</span></p></blockquote><h2> A Short- term Shock?</h2><p>There are likewise opportunities that the marketplace winds up soaking up the marketing stress as Bitcoin becomes conventional capitalists’ principles as a safe-haven property.</p><p>Ben Lilly, a cryptocurrency financial expert, <a href="https://jarvislabs.substack.com/p/bitcoins-current-crisis" target="_blank" rel="noopener nofollow external" data-wpel-link="external">penned a paper</a> that concentrated on a recurring liquidity dilemma in the Bitcoin market. He mentioned that 3 fields: crypto-enabled investment company, corporations/institutions, as well as decentralized money, have actually been proactively drawing Bitcoin’s supply out of the exchanges.</p><blockquote data-width="500" data-dnt="true"><p lang="en" dir="ltr">Corporations/Institutions<a href="https://twitter.com/michael_saylor?ref_src=twsrc%5Etfw" data-wpel-link="exclude" target="_blank" rel="follow noopener">@michael_saylor</a> at MicroStrategy: 71k BTC throughout this period.<a href="https://twitter.com/elonmusk?ref_src=twsrc%5Etfw" data-wpel-link="exclude" target="_blank" rel="follow noopener">@elonmusk</a> at Tesla: Let’s state regarding 42k BTC utilizing an avg rate of $35k/BTC</p><p>Square, Bitwise, Stone Ridge Holdings, Ruffer (of course offered back some, yet still appropriate): 72k</p><p> 185k BTC to this team</p><p>— Ben Lilly (@MrBenLilly) <a href="https://twitter.com/MrBenLilly/status/1362079633160683520?ref_src=twsrc%5Etfw" data-wpel-link="exclude" target="_blank" rel="follow noopener">February 17, 2021</a></p></blockquote><blockquote><p>“It means bitcoin is in fact becoming scarce. If this continues, a liquidity crisis will transpire pushing prices considerably higher.”</p></blockquote><p>Technically, Bitcoin anticipates to prolong its temporary benefit predisposition because of an affordable loved one toughness indication analysis as well as distinct assistance degrees in its 20- as well as 50-4H relocating standards.</p></div></div>]]>
            </description>
            <link>https://www.bitcoinprofit.app/news/calls-for-bitcoin-plunge-emerge-over-mysterious-1-5-bn-btc-transfer/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26224353</guid>
            <pubDate>Mon, 22 Feb 2021 13:40:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[JSON with Commas and Comments]]>
            </title>
            <description>
<![CDATA[
Score 106 | Comments 155 (<a href="https://news.ycombinator.com/item?id=26224255">thread link</a>) | @todsacerdoti
<br/>
February 22, 2021 | https://nigeltao.github.io/blog/2021/json-with-commas-comments.html | <a href="https://web.archive.org/web/*/https://nigeltao.github.io/blog/2021/json-with-commas-comments.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      
      
      

      

<p><em>Summary: JWCC is a minimal extension to the widely used JSON file format with
(1) optional commas after the final element of arrays and objects and (2) C/C++
style comments. These two features make it more suitable for human-editable
configuration files, without adding so many features that it’s incompatible
with numerous other (deliberate and accidental) existing JSON extensions.</em></p>

<h2 id="extensibility">Extensibility</h2>

<p>The Peter Principle is the half-joking, half-serious observation that people
get promoted to their level of incompetence, because being competent at level
<code>N</code> leads to being promoted to level <code>N+1</code>.</p>

<p>My colleague Simon Morris made a similar observation about software complexity:</p>

<blockquote>
  <p>Software has a Peter Principle. If a piece of code is comprehensible, someone
will extend it, so they can apply it to their own problem. If it’s
incomprehensible, they’ll write their own code instead. Code tends to be
extended to its level of incomprehensibility.</p>
</blockquote>

<h3 id="the-many-json-extensions">The Many JSON Extensions</h3>

<p>There’s a similar story with file formats. If they’re comprehensible, they’ll
get extended. JSON (JavaScript Object Notation) is this article’s example. The
<a href="https://json.org/">original specification</a> fits on a single page, either as
text or diagrams. The file format is simple and ubiquitous. Therefore, there
are many extensions - supersets of JSON. Here’s just a few (including two
slightly different extensions both called “JSONC”):</p>

<ul>
  <li><a href="https://json5.org/">JSON5</a></li>
  <li><a href="https://komkom.github.io/">JSONC</a> #1</li>
  <li><a href="https://code.visualstudio.com/docs/languages/json#_json-with-comments">JSONC</a> #2</li>
  <li><a href="https://hjson.github.io/">HJSON</a></li>
  <li><a href="https://github.com/lightbend/config/blob/master/HOCON.md">HOCON</a></li>
</ul>

<p>Suprisingly, <a href="https://yaml.org/">YAML</a> is also a superset of JSON. Not just
conceptually, but also in the sense that valid JSON files are also valid YAML
files (although there’s some divergence about whether duplicate keys are
legitimate). As a bonus, if you use YAML, then to paraphrase <a href="http://regex.info/blog/2006-09-15/247">Jamie
Zawinski</a>: now you have <a href="https://noyaml.com/">NO
problems</a>.</p>

<h3 id="wandering-off-the-specification">Wandering Off the Specification</h3>

<p>There are also informal supersets-of-JSON in widespread use, sometimes more by
accident than by design. The Chromium web browser’s <a href="https://source.chromium.org/chromium/chromium/src/+/master:base/json/json_reader.h;l=27;drc=d0919138b7951c1a154cf802a68aad7904b6f4c9">JSON parser goes
off-spec</a>
in a number of ways. The timeline could have been:</p>

<ol>
  <li>Some developer long ago (perhaps in a yak-shaving hurry) wrote or
copy/pasted some parsing code that was accidentally too lenient, allowing a
superset-of-JSON. Perhaps they re-used existing code that handled C-style
string escapes, like the <code>"\n"</code> in <code>"line\nbreak"</code>, without realizing that
it also unescaped <code>"\v"</code>, valid in a C string but not a JSON string.</li>
  <li>People use the software. They write first-party and third-party JSON for it.
Some of it is actually malformed (e.g. they have <code>"\v"</code> inside strings) but
tests (manual and automatic) usually check that new features work, not that
all the slightly-incorrect things are rejected. Nobody notices at the time.</li>
  <li>Years pass. <a href="https://www.hyrumslaw.com/">Hyrum’s Law</a> slowly kicks in. We
can no longer tighten this custom JSON parser implementation to follow the
spec more strictly because too many things (in unknown places) will break.</li>
</ol>

<p>This also affects our ability to replace one JSON library with another. For
example, we might want to switch from a C++-based JSON parser to a Rust-based
one, because of its security benefits. If the upstream Rust library chooses to
follow the spec diligently (which is a perfectly reasonable position) then it
would ‘break’ our apps that have inadvertently relied on the previous
looser-than-the-spec implementation.</p>

<p>We could carry local patches, but that isn’t free. Upstream fuzz-testing
infrastructure only exercises the unmodified library, not our patched flavor.
Future upstream changes may also invalidate the downstream patch, possibly in
subtle ways. An upstream “this new unsafe block is OK because it’s a private
implementation detail and nothing in this crate does X” comment might not be
aware that our out-of-tree patch does X to its internals.</p>

<h3 id="quirks">Quirks</h3>

<p>The Wuffs library approach is to expose
<a href="https://github.com/google/wuffs/blob/3d6c609dc12de3c81e1b8079ceecf96370b086a2/doc/note/quirks.md">quirks</a>: runtime
configuration options to go off-spec in various ways so that Wuffs’
implementation can be a drop-in replacement for other implementations, without
the need for downstream patches.</p>

<p>Wuffs has <a href="https://github.com/google/wuffs/blob/3d6c609dc12de3c81e1b8079ceecf96370b086a2/std/json/decode_quirks.wuffs">20 JSON
quirks</a>
so far. As always, there are trade-offs. They’re not free (in terms of
maintenance cost) and have super-linear complexity: that file’s comments also
has 12 call-outs to the subtleties of combining two particular quirks.</p>

<p>Here’s an example of the emergent complexity when combining two simple-sounding
JSON extensions. The first one adds C++-style <code>/* slash-star block comments */</code>
and <code>// double-slash line comments</code>. The second one packs multiple top-level
values in a single stream, separated by line breaks.</p>

<p>That second extension - by itself and when holding minified, whitespace-free
‘vanilla’ (non-extended) JSON - plays well with Unix’s traditional
line-oriented tools. It is sometimes known as Line-Delimited JSON
(<a href="https://en.wikipedia.org/wiki/JSON_streaming#Line-delimited_JSON">LDJSON</a>),
Newline-Delimited JSON (<a href="http://ndjson.org/">NDJSON</a>) and JSON Lines
(<a href="http://jsonlines.org/">JSONL</a>). But “one value per line” tools’ assumptions
can break if slash-star comments can also contain blank lines.</p>

<p>Here’s another question (let’s call it the ‘end of comment’ question). Is the
<code>'\n'</code> at the end of of a <code>// double-slash line comment</code> actually part of the
comment? At first, this sounds merely philosophical. Comments are ignored and,
in ‘vanilla’ JSON, all whitespace is ignored, so why the distinction?</p>

<p>The ‘right’ answer to that ‘end of comment’ question isn’t obvious, but it can
affect whether a line comment at the end of a multi-value stream should end in
1 or 2 <code>'\n'</code> bytes. Ideally the answer should be self-consistent with whether
a line comment at the end of file must end with the <code>'\n'</code> or whether the
implicit EOF (end-of-file) alone suffices. See also the <a href="http://seriot.ch/parsing_json.php">“Parsing JSON is a
Minefield”</a> and <a href="https://nullprogram.com/blog/2019/12/28/">“Unintuitive JSON
Parsing”</a> articles for how subtle a
‘simple’ format like JSON can be.</p>

<p>Wuffs makes one particular choice for that ‘end of comment’ question. Its
particular choice probably isn’t that important, more that it made a concious
and documented choice.</p>

<h3 id="clarity-not-terseness">Clarity, not Terseness</h3>

<p>Some general advice, when designing a new file format or extending an existing
one, is keep some room for future extensions. For example, allowing unquoted
strings (writing <code>foo</code> instead of <code>"foo"</code>), is certainly convenient, but
re-defining <code>undefined</code> or <code>datetime</code> without quotes, from invalid JSON syntax
to valid some-extension-of-JSON strings, rules out a future extension adding
new ‘keywords’.</p>

<p><a href="https://cbor.io/">CBOR</a> is binary at the wire format level (unlike textual
JSON) but naturally extends JSON at the object model level. It also has an
<code>undefined</code> concept separate from <code>null</code>, and <code>undefined</code> can be a map key. We
couldn’t do the ‘obvious’ CBOR-to-some-extended-JSON conversion if <code>undefined</code>,
without quotes, was already repurposed to mean a string.</p>

<p>I find it suprising that, <a href="https://github.com/lightbend/config/blob/master/HOCON.md#unquoted-strings">in
HOCON</a>,
“<code>truefoo</code> parses as the boolean token <code>true</code> followed by the unquoted string
<code>foo</code>. However, <code>footrue</code> parses as the unquoted string <code>footrue</code>”.</p>

<p>It can also be helpful for a <a href="https://github.com/search?q=return.flase+extension%3Apy">typo like
<code>flase</code></a> to be picked
up early as a syntax error (without needing schemas or type checking) instead
of silently accepted (as a string, not a bool). This can otherwise be
especially dangerous if further processed in a weakly-typed programming
language where any non-empty string is ‘truthy’.</p>

<p><code>[a b c]</code> is invalid ‘vanilla’ JSON syntax, but in the various extended-JSON
variants, is it a list with three 1-byte strings or one 5-byte string? Or is it
one 3-byte string because three 1-byte strings are implicitly
whitespace-delimited and also then implicitly concatenated? Any particular
answer can be consistent in its own world, but different JSON extensions make
different choices. This can be confusing when software grows large enough (or
gains enough transitive dependencies) to have to speak multiple JSON
extensions.</p>

<p>These days, when I’m programming in C/C++ or Go, I often add unnecessary
parentheses in expressions like <code>(a * b) + c</code>. Even though they’re redundant
because of well-defined operator precedence rules, different programming
languages have different precedence rules and getting the precedence wrong can
lead to <a href="https://github.com/jbangert/nail/issues/7">hard-to-spot bugs</a>. The
Wuffs language actually <a href="https://github.com/google/wuffs/blob/3d6c609dc12de3c81e1b8079ceecf96370b086a2/doc/wuffs-the-language.md#operators">rejects a bare <code>a * b +
c</code></a>
and you have to parenthesize the multiplication or the addition.</p>

<p>Similarly, for JSON-like documents, I prefer the clarity of either <code>["a", "b",
"c"]</code> or <code>["a b c"]</code>, even if it means a little extra typing. Reading is more
important than writing for code and configuration, especially when multiple
people or long periods of time are involved.</p>

<h2 id="introducing-jwcc">Introducing JWCC</h2>

<p>Having said all of that, here is yet another superset-of-JSON, called JWCC
(JSON With Commas and Comments). It is a minimal extension. As its name
suggests, there are only two new features:</p>

<ul>
  <li>“Commas” lets you optionally have a comma after the final element of an array
or an object: <code>[1,2,3,]</code>. When you format one element per line, it’s easier
to insert and remove elements (and eyeball the diffs) when you don’t have to
fiddle with any commas (or lack of commas) on adjacent but otherwise
unrelated lines.</li>
  <li>“Comments” lets you have C++-style <code>/* slash-star block comments */</code> and <code>//
double-slash line comments</code>, anywhere where ‘vanilla’ JSON allows whitespace.
Line comments must end with a <code>'\n'</code> byte, even at the end of the file.</li>
</ul>

<p>To be clear, while every JSON file is valid JWCC, this is a new file format. It
just happens to be very familiar if you (or your software) already speak JSON.
Yes, Doug Crockford <a href="https://web.archive.org/web/20150105080225if_/https://plus.google.com/+DouglasCrockfordEsq/posts/RK8qyGVaGSr">deliberately removed comments from
JSON</a>
but people keep putting them back in. If we’re going to have comment-enriched
JSON (e.g. for human-editable configuration files), we might as well have a
standard one. Cue <a href="https://xkcd.com/927/">XKCD #927 “Standards”</a>.</p>

<h3 id="cc-implementation">C/C++ Implementation</h3>

<p><a href="https://github.com/google/wuffs">Wuffs</a>’ JSON library (availble as a C or C++
API) can decode either ‘vanilla’ JSON or JWCC, using its quirks mechanism.
<a href="https://github.com/google/wuffs/tree/3d6c609dc12de3c81e1b8079ceecf96370b086a2/example/jsonptr"><code>jsonptr</code></a>
is a command line tool (a JSON formatter) that uses this library. By default,
it speaks spec-compliant ‘vanilla’ JSON:</p>

<div><div><pre><code>$ echo '[1,2,/*hello*/3,]' | jsonptr
[
    1,
    2
json: bad input
</code></pre></div></div>

<p>It has a JWCC mode:</p>

<div><div><pre><code>$ echo '[1,2,/*hello*/3,]' | jsonptr -jwcc
[
    1,
    2,
    /*hello*/
    3,
]
</code></pre></div></div>

<p>It can also convert from JWCC syntax to …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://nigeltao.github.io/blog/2021/json-with-commas-comments.html">https://nigeltao.github.io/blog/2021/json-with-commas-comments.html</a></em></p>]]>
            </description>
            <link>https://nigeltao.github.io/blog/2021/json-with-commas-comments.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26224255</guid>
            <pubDate>Mon, 22 Feb 2021 13:30:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Write Code, Not Configuration]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26224186">thread link</a>) | @meebob
<br/>
February 22, 2021 | http://catern.com/config.html | <a href="https://web.archive.org/web/*/http://catern.com/config.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
When you write a program which might use one of multiple implementations,
or needs to connect to some URL,
or needs some information to function,
your first step should be to just hardcode it.
Use one implementation, hardcode one URL, use one specific piece of information.
<p>
If, after doing so, you find that you need another program
with different hardcoded information,
turn your program into a function.
Take the implementation as an argument,
take the URL as an argument,
take the information you need as an argument.
</p><p>
And then just call that function with the arguments you prefer,
each time you need a program with different "configuration".
</p><p>
You don't need to load those arguments from some file on disk,
or by querying some database or service,
or even by taking them as command line parameters.
</p><p>
Just make a separate program for each case.
</p><p>
For a batch job, this might mean a few different executables,
tweaked and rebuilt frequently over time as one's needs change.
</p><p>
For a user-facing application, this might mean that each user runs their own custom executable,
compiled for or by them, with pre-compiled shared libraries common between all users.
</p><p>
For a daemon providing some network service and connecting to other services,
this might mean 10 or so different executables,
which are run in the 10 or so different availability zones or datacenters or sorts of machines
on which this daemon is deployed.
Or for a more heterogeneous deployment,
it might mean many thousands of different executables,
deployed to many thousands of different environments,
each layered on top of a common image with shared libraries identical between all deployments.
</p><p>
If you want to share information between multiple programs with different configurations,
share it in the same way you share code: with a library.
</p><p>
If you want to make your configuration more dynamic,
write code to dynamically determine the arguments to pass.
</p><p>
If you want to make rapid changes and don't want to wait for builds,
call the function from a REPL, or rely on fast incremental builds.
</p><p>
If you want to see what arguments are being passed to your function in your program,
use logging and debuggers, according to your preference.
</p><p>
Code written in this way in your actual programming language
is far more expressive than code written in a configuration DSL,
like JSON, YAML, or Dhall.
</p><p>
In a configuration DSL,
one would select from multiple implementations by manipulating some identifier for the implementations;
a string or a sum type, perhaps.
This might be formatted in an invalid way,
or might be from the wrong version,
or might be incompatible,
or any number of possible issues.
</p><p>
But in any general-purpose programming language,
implementations are first class values which can be passed around and manipulated,
whether as an object, a module, a struct of function pointers, or something else.
One can write a function (or a template, a functor, a macro, or something else)
which takes the implementation directly as an argument,
with no possibility of issues due to mismatches between a string identifier and the actually available implementations.
</p><p>
In a typical configuration DSL,
we would specify a set of key-value pairs to configure some component.
We might run some validator over the DSL to ensure it matches some schema
which we know our code will eventually load.
</p><p>
But static checks in general purpose programming languages, such as type checkers,
perform "validation" of the "schema" of our configuration for free,
wherever we pass arguments to functions.
Required arguments must be present, and even must be of the correct type,
and there's no need to keep our code in sync with a schema.
</p><p>
Writing code in your general purpose language
is easier, faster, and better
than writing separate configuration.
</p><p>
Of course, this is all easier with faster and more powerful build systems,
like Nix,
or with interpreted or fast-building languages,
like Python,
or with, at least, shared libraries and a fast link step,
like dynamic libraries in C.
</p><p>
If you're on a slow, weak, and hard-to-use build system,
with a slow-building language,
and you have slow linking,
then you probably want to fix one or more of those issues first;
although if your programs are small, even those issues are not necessarily prohibitive.
</p><p>
And you might also be in a corporate environment,
where code changes require an extensive and painful process,
but configuration changes can be made relatively easily.
If so, consider quitting.
</p><h3>Further reading</h3>
<ul>
  <li><a href="http://mikehadlow.blogspot.com/2012/05/configuration-complexity-clock.html">The Configuration Complexity Clock</a>
  </li><li><a href="http://www.object-oriented-security.org/lets-argue/singletons">Singletons Considered Harmful</a>
  </li><li><a href="http://hackage.haskell.org/package/xmonad-contrib-0.16/docs/XMonad-Doc-Configuring.html">XMonad configuration</a>
  </li><li><a href="https://st.suckless.org/">st configuration</a>
</li></ul>
</div>]]>
            </description>
            <link>http://catern.com/config.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26224186</guid>
            <pubDate>Mon, 22 Feb 2021 13:24:04 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[SubstackDB: Exploiting Lax Upload Validation to Create Parasitic File Servers]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26224143">thread link</a>) | @dgaff
<br/>
February 22, 2021 | http://devingaffney.com/substackdb-exploiting-lax-upload-validation-to-create-parasitic-file-servers/ | <a href="https://web.archive.org/web/*/http://devingaffney.com/substackdb-exploiting-lax-upload-validation-to-create-parasitic-file-servers/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="main">
    <article itemtype="http://schema.org/BlogPosting">
        <section>
            <p>For the past year, I've been increasingly focusing on what I have come to call "sociotechnical security" - whereas "technical security" seeks to identify and remove unintended flaws in the architecture of platforms, "sociotechnical security" is all about identifying and removing the incentives for how worst-faith users may abuse the explicit intent of platform affordances. Making this "sociotechnical" distinction brings into the frame a lot of issues not typically considered to be security issues, but are proving to become existential threats to a bunch of different businesses. Social platforms have misinformation problems due (in part) to fake accounts spreading it, online marketplaces face algorithmic manipulation challenges from sellers jockeying for position, and platforms with weak security around analytics face all sorts of ad and impression count fraud.</p>

<p>Today I want to share an exploit that I spent the last week investigating, and am calling "SubstackDB," after Substack, where I first identified the problem. Specifically, platforms tend to prefer low-friction interfaces, and tend to afford users increasing flexibility in affordances provided. Substack's WYSIWYG editor for drafting posts is overly optimistic in assuming good faith in user behavior, and is exposed to a huge flaw - because there is no validation for the input of files uploaded into the editor, and because their upload functionality has no verification scheme beyond requiring an active user session, their file server can be hijacked for any arbitrary use case. Far from being the only company facing this issue, Discord also suffers from a nearly identical problem. As a proof of concept, I used the unpublished APIs for both Substack and Discord's file uploading capabilities to store copies of GPT2 on their servers, and I provide the necessary scripts for loading and verifying the execution of those models. Additionally, I am providing a ruby implementation of <code>SubstackDB</code> which, given a valid username and password allows a user to upload and download any file of any size.</p>

<h2 id="substackvulnerability">Substack Vulnerability</h2>

<p><img src="https://i.imgur.com/VQhkAow.png" alt="Substack Editor"></p>

<p>This is a picture of Substack's WYSIWYG post editor with an example image uploaded. Here's a look at the <code>cURL</code> request that uploaded the image, and the response back from the server:</p>



<p>And the response:  </p>



<p>It turns out that this "bucketeer" name refers to a Heroku file server plugin, which presumably also indicates that Substack is at least partially hosted there. Regardless, through trial and error, I determined that only a very small portion of the above <code>cURL</code> is required to send a file:</p>

<p>Here, the <code>[BASE 64 BYTES]</code> refers to literally any content that is Base64 encoded. In our case it is an image, but it turns out that any data encoded in Base64 will be treated as valid input. Through more trial and error, I determined that while there seems to be no upper limit to the size of an uploaded file, though it is in practice limited by timeout errors that ultimately invalidate the request. Further, this "image" upload functionality actually returns the original file byte-for-byte, so no compression occurs between upload and receiving the final URL - because of this, we can store any other data relatively easily and just declare the "type" of the content to be one of the valid types required by the upload endpoint.</p>

<p>To prove that any arbitrary content could be uploaded, I downloaded a copy of the GPT2 "medium" model via <code>aitextgen</code>, split the <code>.bin</code> file containing the model into several hundred smaller files of equal size, and then uploaded those to Substack's endpoint. Finally, I wrote a script that reconstructs the model using a final "manifest" JSON file stored on Substack as well:</p>



<h2 id="discordvulnerability">Discord Vulnerability</h2>

<p>Discord's vulnerability seems a bit more intentional as a feature than as a bug per se, but is still ripe for abuse. Using a throwaway account and throwaway server, I was granted a set of credentials - using those credentials, I was able to slightly alter the script I used to upload GPT2.</p>

<p><img src="https://i.imgur.com/rdwMF5D.png" alt="Uploaded 6mb slices of GPT2"></p>

<p>Discord appears to treat non-image uploads as more of a first-order object in their system - clearly, there is some form of intent to allow users to upload files of some nature. What is likely outside the intent, however, is automating this affordance to send gigabytes of content through their platform in a relatively short time frame. Ultimately, I was able to generate a nearly identical script as was deployed in the Substack case.</p>



<h2 id="substackdbscript">SubstackDB Script</h2>

<p>Finally, to prove out the concept of truly using this type of vulnerability as an arbitrary file server, I wrote a generalized <code>SubstackDB</code> class which, in this version of the script, takes as input the username, password, and filepath, and returns a print-out of whether or not the contents of that filepath, once read, uploaded, and downloaded, is identical to the original source file. In practice, one could use this script to be a literal drop-in replacement for many classic file store APIs. </p>



<h2 id="endnote">Endnote</h2>

<p>This is just two examples of a general problem with upload validation. Of note, I also explored exploits like this on Meetup, Indiegogo, Gumroad, and a few others, and while it was still likely <em>technically</em> possible to pull off a similar stunt, it was in no way worth the investment in time that it would take to fully reverse engineer their implementations - generally, the issue was that one-time-use tokens were being employed to validate uploads on a per-upload basis, which proved to be too much of a pain to solve. The point, however, is that this is a demonstration of a systemic issue - by assuming best-faith use, platforms allow worst-faith users the unintended "sociotechnical" affordance of a free fileserver at the cost of the platform. </p>
            
            
            
        </section>
        
    </article>
</div></div>]]>
            </description>
            <link>http://devingaffney.com/substackdb-exploiting-lax-upload-validation-to-create-parasitic-file-servers/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26224143</guid>
            <pubDate>Mon, 22 Feb 2021 13:18:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Desire Covid-19 exposure notification]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26224023">thread link</a>) | @programLyrique
<br/>
February 22, 2021 | https://privatics.inrialpes.fr/desire/ | <a href="https://web.archive.org/web/*/https://privatics.inrialpes.fr/desire/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://privatics.inrialpes.fr/desire/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26224023</guid>
            <pubDate>Mon, 22 Feb 2021 13:03:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The birth of Prolog (1992) [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26223906">thread link</a>) | @alokrai
<br/>
February 22, 2021 | http://alain.colmerauer.free.fr/alcol/ArchivesPublications/PrologHistory/19november92.pdf | <a href="https://web.archive.org/web/*/http://alain.colmerauer.free.fr/alcol/ArchivesPublications/PrologHistory/19november92.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>http://alain.colmerauer.free.fr/alcol/ArchivesPublications/PrologHistory/19november92.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-26223906</guid>
            <pubDate>Mon, 22 Feb 2021 12:51:50 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Microsoft Sculpt Wired Conversion Mod]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26223805">thread link</a>) | @yuribro
<br/>
February 22, 2021 | https://chadaustin.me/2021/02/wired-sculpt/ | <a href="https://web.archive.org/web/*/https://chadaustin.me/2021/02/wired-sculpt/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>I made a control board for the Microsoft Sculpt wireless keyboard that converts it to wired USB, and now my favorite keyboard is even better.</p>

<figure>
<a href="https://chadaustin.me/images/sculpt/finished-board.jpeg"><img src="https://chadaustin.me/images/sculpt/finished-board.jpeg" alt="The finished and installed board."></a>
<figcaption>The finished and installed board.</figcaption>
</figure>

<figure>
<a href="https://chadaustin.me/images/sculpt/messy-desk.jpeg"><img src="https://chadaustin.me/images/sculpt/messy-desk.jpeg" alt="Wired keyboard and the resulting project mess!"></a>
<figcaption>Wired keyboard and the resulting project mess!</figcaption>
</figure>

<figure>
<a href="https://chadaustin.me/images/sculpt/underside.jpeg"><img src="https://chadaustin.me/images/sculpt/underside.jpeg" alt="USB cable and reset button."></a>
<figcaption>USB cable and reset button.</figcaption>
</figure>

<p>The QMK config is available at <a href="https://github.com/chadaustin/qmk_firmware">@chadaustin/qmk_firmware</a> (<a href="https://github.com/chadaustin/qmk_firmware/tree/master/keyboards/handwired/sculpt">keyboards/handwired/sculpt/</a>), and the PCB design files at <a href="https://github.com/chadaustin/wired-sculpt-pcb">@chadaustin/wired-sculpt-pcb</a>.</p>

<p>I’m planning on making at least one more, so if you’d like one, maybe I can help.</p>

<p>It’s a huge improvement. Latency is reduced by about 13 milliseconds, and with full control over the microcontroller’s firmware, you can customize keymaps and layers, and actually use the keyboard’s built-in LEDs.</p>

<h2 id="why">Why?</h2>

<p>Feel free to stop reading here — I am going to tell the sequence of events that led to this project. Besides some exposure to basic voltage and resistance circuits in college, I have very little electronics background. But, in a short time, I went from only barely knowing what a capacitor was to having a working PCB manufactured and assembled, and maybe this will inspire someone else to give it a try.</p>

<p>Since developing RSI in college, I’ve exclusively used Microsoft’s ergonomic keyboards. And when I first tried the Sculpt, I instantly knew it was the best yet. The soft actuation, short key travel, and rigid frame are perfect for my hands. And because the number pad is a separate device, the distance to my mouse is shortened.</p>

<p>My brother went out and bought one too. Not much later, he gave it to me, saying the latency was inconsistent and high, and it was unacceptable for gaming. I thought he was being uniquely sensitive, since I had no problem in either Linux, Windows 7, or macOS. But then I updated to Windows 10 and saw exactly what he meant.</p>

<p>It was like the keyboard would go to sleep if a key wasn’t pressed for a few seconds, and the first keypress after a wake would be delayed or, worse, dropped.</p>

<p>And heaven forbid I use my USB 3 hub, whose EMI would disrupt the 2.4 GHz signal, and <em>every other</em> keypress would be unreliable. I’d gone as far as mounting the wireless transceiver directly under my keyboard, on the underside of my desk, and keys were still dropped.</p>

<p>So, best keyboard ever. But wireless sucks. (But mostly in Windows 10? No idea about that.)</p>

<h2 id="over-the-hump">Over the Hump</h2>

<p>What started this whole thing is that the <a href="https://github.com/facebookexperimental/eden/#edenfs">EdenFS</a> team was a bunch of keyboard enthusiasts. During the pandemic, as we’re all at home burning out and missing each other, we were trying to think of some virtual team offsites. Wez offered to walk everyone through building a <a href="https://www.1upkeyboards.com/instructions-downloads/sweet-16-instructions/">Sweet 16 Macro Pad</a>.</p>

<figure>
<a href="https://chadaustin.me/images/sculpt/sweet-16.jpeg"><img src="https://chadaustin.me/images/sculpt/sweet-16.jpeg" alt="Assembled Sweet 16 underside"></a>
<figcaption>Assembled Sweet 16 underside. This is take two, after resoldering and cleaning the whole thing. Take one was a bit of a mess.</figcaption>
</figure>

<p>So, okay, a keyboard is a matrix, with some diodes used to disambiguate the signalling, and a microcontroller that rapidly polls the matrix and reports events over USB…</p>

<p>So maybe I could fix the Sculpt! I bought a transceiver-less Sculpt off eBay for cheap and <a href="http://emmanuelcontreras.com/how-to/how-to-disassemble-microsoft-sculpt-ergonomic-keyboard-and-make-it-wired/">popped it open (thanks Emmanuel Contreras!)</a>, thinking maybe its controller could be flashed with new firmware that speaks USB. The Sculpt uses a <a href="https://infocenter.nordicsemi.com/pdf/nRF24LE1_PS_v1.6.pdf">Nordic Semiconductor nRF24LE1</a>, but I was nowhere near capable of making use of that information at the time, though it did point me to Samy Kamkar’s horrifying guide on <a href="https://samy.pl/keysweeper/">surreptitiously sniffing keystrokes from nearby (older) Microsoft wireless keyboards</a>.</p>

<p>I almost gave up here, but Per Vognsen <a href="https://twitter.com/pervognsen/status/1322422385174220800">suggested I scan the matrix myself</a> and it turns out Michael Fincham had already <a href="https://www.reddit.com/r/MechanicalKeyboards/comments/bhkgnp/modification_photos_qmk_wired_microsoft_sculpt/">mapped out the matrix and soldered a Teensy 2.0++ board onto the Sculpt’s test pads</a>, showing this was doable!</p>

<p>So I ordered my own microcontroller to try the same thing.</p>

<p>First, I bought an Arduino Pro Micro, like the Sweet 16 uses. Oh hey, 18 GPIO pins isn’t enough to drive the Sculpt’s 26-pin matrix. I looked at using an I2C GPIO expander, but it felt like taking on too much.</p>

<figure>
<a href="https://chadaustin.me/images/sculpt/pro-micro.jpeg"><img src="https://chadaustin.me/images/sculpt/pro-micro.jpeg" alt="Arduino Pro Micro"></a>
<figcaption>Arduino Pro Micro. Wait, you need pins to scan a matrix?</figcaption>
</figure>

<p>More pins? QMK’s Proton C has more pins! So I carefully soldered onto the test pads as Michael had shown was possible… and it worked!</p>

<figure>
<a href="https://chadaustin.me/images/sculpt/proton-c.jpeg"><img src="https://chadaustin.me/images/sculpt/proton-c.jpeg" alt="QMK Proton C"></a>
<figcaption>QMK Proton C. It's a beautiful board.</figcaption>
</figure>

<figure>
<a href="https://chadaustin.me/images/sculpt/test-pads.jpeg"><img src="https://chadaustin.me/images/sculpt/test-pads.jpeg" alt="Soldering test pads to Proton C."></a>
<figcaption>Soldering test pads to Proton C.</figcaption>
</figure>

<figure>
<a href="https://chadaustin.me/images/sculpt/all-test-pads.jpeg"><img src="https://chadaustin.me/images/sculpt/all-test-pads.jpeg" alt="All test pads connected to Proton C. It works!"></a>
<figcaption>All test pads connected to Proton C. It works!</figcaption>
</figure>

<p>Getting those wires to stick to the pads without shorting was tricky. (I hadn’t yet discovered how magical flux is.)</p>

<p>The keyboard worked, but I couldn’t fit the board, its wires, and the new microcontroller into the case, and I wasn’t <em>really</em> happy leaving it in this state, even if I could pack it in somehow.</p>

<p>I thought, all I <em>really</em> need is the ribbon cable connector, so I ordered a 30 pin, 1.0 mm pitch ribbon breakout and the pricier (but tons of pins!) <a href="https://www.pjrc.com/store/teensypp.html">Teensy 2.0++</a>. Looking back, it’s cute that I was trying to save $10 on the microcontroller… You just have to get used to spending money on whatever saves you time.</p>

<figure>
<a href="https://chadaustin.me/images/sculpt/breakout-and-teensy.jpeg"><img src="https://chadaustin.me/images/sculpt/breakout-and-teensy.jpeg" alt="Ribbon cable breakout and Teensy 2.0++"></a>
<figcaption>Ribbon cable breakout and Teensy 2.0++</figcaption>
</figure>

<p>Well, it was almost as annoying to solder, and still didn’t fit. So much for saving money on microcontrollers.</p>

<p>I thought about giving up. Is it really that bad that my keys don’t always register in games? Can I just tolerate some flakiness and latency?</p>

<p>But Jon Watte offered to spend an entire day showing me how to use KiCad, design circuits, layout PCBs, select components on Digi-Key, scan datasheets for the important information, and how to work with a PCB manufacturing house. Of course you never turn down opportunities like that.</p>

<h2 id="designing-the-final-board---schematic">Designing the Final Board - Schematic</h2>

<p>Assuming, like me, you’ve never done this, I’ll summarize the steps.</p>

<p>First you sketch out the circuit schematic.</p>

<figure>
<a href="https://chadaustin.me/images/sculpt/schematic.png"><img src="https://chadaustin.me/images/sculpt/schematic.png" alt="Schematic"></a>
<figcaption>Schematic in KiCad. Most of this was informed by the datasheet and Atmel's design guides.</figcaption>
</figure>

<p>Jon showed me several tricks in KiCad, like global labels, and starting with some standard resistor and capacitor values, but it’s very important that you go through the datasheets, because details can matter a ton.</p>

<p>I knew I wanted the main processor to be the AT90USB1286 controller, and fortunately KiCad already had a symbol for it. Atmel has a comprehensive and accessible data sheet, which showed me I needed some 22 Ω resistors on the USB data lines, which of the ISP programmer lines needed resistors (and appropriate values), and that I needed to either pull HWB low, or provide a physical switch that pulls it low, in order to allow rebooting the device into USB firmware update mode.</p>

<p>There are a bunch of things that are implicitly known to electrical engineers but that were new to me. You want:</p>

<ul>
  <li>a ground plane under the data lines and most of the microcontroller if possible.</li>
  <li>an electrolytic or tantalum bypass capacitor on the main 5V power from USB.</li>
  <li>ceramic filter capacitors on each power pin.</li>
  <li>appropriate values for the resonance capacitors on your crystal.</li>
  <li>electrostatic discharge protection! Turns out transients are common and it’s easy to fry a chip just by plugging it in.</li>
</ul>

<p>And then when you get into concerns like EMI and high-frequency signal integrity, the rabbit hole goes deep.</p>

<p>I kept having to tell myself “it’s just a keyboard”, but it also helped that there are a great number of high-quality resources on these topics just a click away. I spent lots of time on <a href="https://www.eevblog.com/">EEVBlog</a>.</p>

<p>Before finishing the circuit design, Jon had me do a couple smart things. In case the factory-supplied USB bootloader didn’t work out, he suggested I add the footprint (but not a connector!) for an ISP programmer and a debug LED to prove code would work at all.</p>

<h2 id="designing-the-final-board---physical-layout">Designing the Final Board - Physical Layout</h2>

<p>After arranging the schematic and ensuring it passed the electrical rules check, it was time to pick specific components. That is, the reference to a 220 Ω resistor is replaced with the Panasonic ERJ-3EKF2200V, 0603 surface mount.</p>

<p>There are a couple things to keep in mind. For common components, like resistors and ceramic capacitors, there is a huge amount of choice. For example, I see over 1400 surface-mount 220 Ω resistors on digikey. I tried to just stick with one high-quality brand like Panasonic or Samsung for all of that stuff.</p>

<p>The important thing is the physical form factor, which determines the footprint on the board. Once you pick a part, it has a size, and you need to tell KiCad which physical footprint should be assigned to that component. I used 0603 resistors, so I assigned each resistor in the schematic the “Resistor_SMD:R_0603_1608Metric” footprint.</p>

<p>Same for everything else. Jon showed me how to draw my own footprints, but to avoid complexity, I was able to find appropriate footprints in KiCad’s standard libraries for every component I needed.</p>

<p>When you import the schematic into Pcbnew, it’s time to figure out where things go. Where are the edges of the board? Make careful measurements here. Where do the mounting holes go? Where do you want 
the microcontroller? Where do you want the USB port?</p>

<figure>
<a href="https://chadaustin.me/images/sculpt/dimensions.jpeg"><img src="https://chadaustin.me/images/sculpt/dimensions.jpeg" alt="Measuring dimensions and mounting holes"></a>
<figcaption>Measuring dimensions and mounting holes</figcaption>
</figure>

<p>Also, you have to pick through-hole sizes and trace widths. Jon had me use .250 mm for the narrow traces and .500 mm for the wider ones, presumably from experience. I used the narrow traces for signalling and wide traces for power, though I’ve since heard it’s a good idea to use narrow traces between filter capacitors and VBUS.</p>

<figure>
<a href="https://chadaustin.me/images/sculpt/pcb-layout.svg"><img src="https://chadaustin.me/images/sculpt/pcb-layout.svg" alt="Schematic"></a>
<figcaption>PCB layout in KiCad</figcaption>
</figure>

<p>Of course, there’s some iteration between the schematic and the PCB. After physically placing the ribbon cable connector and MCU, the traces all crossed over each other, so I had to reassign all the pins so it made sense physically.</p>

<p>There are also physical constraints about how USB data lines are run, and how the electrostatic protection chip wants to be placed for the most protection.</p>

<p>So, as simple as this board is, I spent a fair amount of time getting all of that right.</p>

<p>I found myself getting lost in the abstractness of holes and traces and footprints, so it was helpful to ground myself by occasionally loading the PCB in KiCad’s 3D viewer.</p>

<figure>
<a href="https://chadaustin.me/images/sculpt/3d-view.png"><img src="https://chadaustin.me/images/sculpt/3d-view.png" alt="Schematic"></a>
<figcaption>3D View</figcaption>
</figure>

<h2 id="designing-the-final-board---manufacturing-and-testing-physical-fit">Designing the Final Board - Manufacturing and Testing Physical Fit</h2>

<p>I tried to …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://chadaustin.me/2021/02/wired-sculpt/">https://chadaustin.me/2021/02/wired-sculpt/</a></em></p>]]>
            </description>
            <link>https://chadaustin.me/2021/02/wired-sculpt/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26223805</guid>
            <pubDate>Mon, 22 Feb 2021 12:39:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Calculating your travel buffer with Python]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26223690">thread link</a>) | @staplebattery
<br/>
February 22, 2021 | https://pythoncharmers.com/blog/travel-distance-python-with-geopandas-folium-alphashape-osmnx-buffer.html | <a href="https://web.archive.org/web/*/https://pythoncharmers.com/blog/travel-distance-python-with-geopandas-folium-alphashape-osmnx-buffer.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div role="main"><div id="content">






<div>
    <div>
        <p><span>11 Feb 2021</span> 
            <span>(30 minutes read)</span>
        </p>
    </div>
    <table>
<!--         <tr>
            <td>Author</td>
            <td>Henry Walshaw</td>
        </tr>
        <tr>
            <td>Date</td>
            <td>11 Feb 2021</td>
        </tr> -->
        <tbody><tr>
            <td>Level</td>
            <td>Intermediate</td>
        </tr>
<!--         <tr>
            <td>Time to Read</td>
            <td>30 minutes</td>
        </tr> -->
        <tr>
            <td>Tools</td>
            <td>scikit-learn v0.23+, Python 3.7+</td>
        </tr>
        <tr>
            <td>Prerequisites</td>
            <td>Machine learning experience with scikit-learn</td>
        </tr>
    </tbody></table>

</div>




<p>Last year, many cities around the world imposed restrictions on how far
you can travel from home. Here we'll show how you can calculate your a
travel bubble around your home using some of the best Python tools for
spatial analysis selected from Python Charmers' <a href="https://pythoncharmers.com/training/python-for-geospatial-analysis/">Python for Geospatial Analysis</a> course.</p>

<p>First some setup. In Python, we rely on building on third party libraries for our analysis (and hopefully contributing back some of our own). Installation of the spatial libraries can sometimes be a challenge due to the compilation requirements but if you're using the <a href="https://www.anaconda.com/">Anaconda Python distribution</a> it is quite simple to get started. First do some installations with the <code>conda</code> install tool, followed by some extras with <code>pip</code>:</p>
<div><pre><span></span><code>conda install gdal fiona geopandas rtree descartes pyproj

pip install -U alphashape geopy osmnx folium
</code></pre></div>

<h2 id="imports">Imports</h2>
<p>The following are the imports we'll use for this example. For now don't worry about the individual imports - some may be familiar, and some may not - we'll discuss them in more depth as we go through the notebook.</p>
<div><pre><span></span><code><span>import</span> <span>alphashape</span>
<span>from</span> <span>descartes</span> <span>import</span> <span>PolygonPatch</span>
<span>import</span> <span>folium</span>
<span>import</span> <span>geopandas</span> <span>as</span> <span>gpd</span>
<span>from</span> <span>geopy.geocoders</span> <span>import</span> <span>Nominatim</span>
<span>from</span> <span>ipywidgets</span> <span>import</span> <span>interact</span><span>,</span> <span>fixed</span><span>,</span> <span>widgets</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>
<span>import</span> <span>networkx</span> <span>as</span> <span>nx</span>
<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>osmnx</span> <span>as</span> <span>ox</span>
<span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>
<span>from</span> <span>shapely</span> <span>import</span> <span>geometry</span>
</code></pre></div>

<div><pre><span></span><code><span>%</span><span>config</span> <span>InlineBackend</span><span>.</span><span>figure_format</span> <span>=</span> <span>'retina'</span>
<span>plt</span><span>.</span><span>rcParams</span><span>[</span><span>'figure.figsize'</span><span>]</span> <span>=</span> <span>(</span><span>10</span><span>,</span> <span>10</span><span>)</span>
</code></pre></div>


<h2 id="geocoding-an-address">Geocoding an address</h2>
<p>First we need a location. To do this we perform a process called "geocoding". This is the process of matching an address to a location on the ground. i.e. where is your house? You can use any address that you like, but for now I'll use my local Melbourne train station: Footscray.</p>
<p>The <code>geopy</code> library is a really convenient library that wraps up a bunch of online geocoding services and APIs into a simple consistent package. This means if you have an API key for Google you can use the Google Maps API via Python to get the location of an address, or if you have a Bing API key you can do the same. Importantly the address will be returned in a consistent way.</p>
<p>Normally I'd use the <a href="https://developer.here.com/documentation/geocoder/dev_guide/topics/quick-start-geocode.html">Here maps</a> API - it's a good one with a generous amount of free address searches when you sign up. For this example though I'll use the open <a href="https://nominatim.org/release-docs/develop/">Nominatim</a> geocoding API built on Open Street Map as it doesn't require any signup - you just need to set the user agent to tell them who you are.</p>
<div><pre><span></span><code><span>address</span> <span>=</span> <span>'Footscray Railway Station Victoria, 3011, Australia'</span>
<span>geocoder</span> <span>=</span> <span>Nominatim</span><span>(</span><span>user_agent</span><span>=</span><span>'Isochrone calculator'</span><span>)</span>
<span>location</span> <span>=</span> <span>geocoder</span><span>.</span><span>geocode</span><span>(</span><span>address</span><span>)</span>
<span>location</span>
</code></pre></div>

<div><pre><span></span><code>Location(Footscray, Hyde Street, Footscray, City of Maribyrnong, Victoria, 3011, Australia, (-37.8015202, 144.9025869, 0.0))
</code></pre></div>

<p>This works well and gives me be a <code>Location</code> that I can use to get the latitude and longitude values for the address easily.</p>
<div><pre><span></span><code><span>location</span><span>.</span><span>latitude</span><span>,</span> <span>location</span><span>.</span><span>longitude</span>
</code></pre></div>

<div><pre><span></span><code>(-37.8015202, 144.9025869)
</code></pre></div>

<p>As a side note, I can use this with the <code>folium</code> library to embed a quick interactive map in my notebook so I can see the address is in the place that I expect:</p>
<div><pre><span></span><code><span>m</span> <span>=</span> <span>folium</span><span>.</span><span>Map</span><span>((</span><span>location</span><span>.</span><span>latitude</span><span>,</span> <span>location</span><span>.</span><span>longitude</span><span>),</span> <span>max_zoom</span><span>=</span><span>20</span><span>,</span> <span>zoom_start</span><span>=</span><span>16</span><span>)</span>
<span>folium</span><span>.</span><span>Marker</span><span>((</span><span>location</span><span>.</span><span>latitude</span><span>,</span> <span>location</span><span>.</span><span>longitude</span><span>),</span> <span>popup</span><span>=</span><span>address</span><span>)</span><span>.</span><span>add_to</span><span>(</span><span>m</span><span>)</span>
<span>m</span>
</code></pre></div>



<p>At this point I could go and create a geometry and build this into a point I can use to help me find my travel bubble radius (in my case, 5km), but there are some nice shortcuts using Pandas and GeoPandas. First I create a Pandas <code>DataFrame</code> that contains the address I wish to geocode:</p>
<div><pre><span></span><code><span>home</span> <span>=</span> <span>pd</span><span>.</span><span>DataFrame</span><span>([{</span><span>'address'</span><span>:</span> <span>address</span><span>}])</span>
<span>home</span>
</code></pre></div>

<div>

<table>
  <thead>
    <tr>
      <th></th>
      <th>address</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Footscray Railway Station Victoria, 3011, Aust...</td>
    </tr>
  </tbody>
</table>
</div>

<p>Not very exciting, but if we wanted to we could use this to find the address of many points of interest. For example you might want to calculate a travel buffer around your home, your parents' home and your brothers and sisters' homes. For now we'll stick with one.</p>
<p>The <a href="https://geopandas.readthedocs.io/">GeoPandas</a> library is a Python library that extends the Pandas dataframe to work with spatial vector data (points, lines and polygons, or for example: addresses, roads, and suburb boundaries). It includes tools that will let you geocode your data using the geopy library as the back-end:</p>
<div><pre><span></span><code><span>home</span> <span>=</span> <span>gpd</span><span>.</span><span>tools</span><span>.</span><span>geocode</span><span>(</span><span>home</span><span>[</span><span>'address'</span><span>],</span> <span>Nominatim</span><span>,</span> <span>user_agent</span><span>=</span><span>'Isochrone calculator'</span><span>)</span>
</code></pre></div>

<p>Note here that we supply the geocoder class object and any parameters it requires as keyword arguments to this function. This returns a GeoPandas <code>GeoDataFrame</code> - a dataframe with a geometry that represents the point for each address.</p>


<div>

<table>
  <thead>
    <tr>
      <th></th>
      <th>geometry</th>
      <th>address</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>POINT (144.90259 -37.80152)</td>
      <td>Footscray, Hyde Street, Footscray, City of Mar...</td>
    </tr>
  </tbody>
</table>
</div>

<p>That's really handy, and we can easily put this on the same map as above:</p>
<div><pre><span></span><code><span>m</span> <span>=</span> <span>folium</span><span>.</span><span>Map</span><span>((</span><span>location</span><span>.</span><span>latitude</span><span>,</span> <span>location</span><span>.</span><span>longitude</span><span>),</span> <span>max_zoom</span><span>=</span><span>20</span><span>,</span> <span>zoom_start</span><span>=</span><span>16</span><span>)</span>
<span>folium</span><span>.</span><span>GeoJson</span><span>(</span><span>home</span><span>,</span> <span>tooltip</span><span>=</span><span>folium</span><span>.</span><span>GeoJsonTooltip</span><span>([</span><span>'address'</span><span>]))</span><span>.</span><span>add_to</span><span>(</span><span>m</span><span>)</span>
<span>m</span>
</code></pre></div>



<p>Note that in this case the tooltip can be more complex: if we have more fields we want to see we can place them all in the list and display it on our map.</p>
<h2 id="getting-the-street-network-within-a-given-distance-your-home">Getting the street network within a given distance your home</h2>
<p>In my case, I could move 5km from my home. However this not 5km as the crow flies (in a straight line), it is 5km of travel. So it's actually 5km as we move through a street network. To calculate what this is we need to download some of that street network data. First we'll need to find out what a boundary is around our home to download just the data we need.</p>
<p>How do we get this 5km bubble? In GIS, a buffer is a standard operation to expand out from a geometry by a set value. However we have a bit of an issue here: our data is in latitude and longitude (which is not metres) - if we try to buffer by 5000 we will get a very wrong answer! First we must re-project the data to a projection based in metres, perform the buffer, the project back.</p>
<p>A later blog post will discuss map projections and how to use them in Python, but for now we will create our bubble by projecting to the GDA2020 MGA Zone 55 projection (EPSG code 7855) to create the buffer before re-projecting back to the original GCS WGS84 projection.</p>
<div><pre><span></span><code><span>buffer</span> <span>=</span> <span>home</span><span>.</span><span>to_crs</span><span>(</span><span>epsg</span><span>=</span><span>7855</span><span>)</span><span>.</span><span>buffer</span><span>(</span><span>5000</span><span>)</span><span>.</span><span>to_crs</span><span>(</span><span>epsg</span><span>=</span><span>4326</span><span>)</span>
</code></pre></div>

<p>It's definitely worth seeing this on a map - this will be a 5000 metre circle around your home. If for example you used the interactive tool from <a href="https://www.theage.com.au/national/victoria/interactive-see-where-your-5km-lockdown-limit-ends-20200801-p55hns.html">the Age newspaper</a> you would get the same map.</p>
<div><pre><span></span><code><span>m</span> <span>=</span> <span>folium</span><span>.</span><span>Map</span><span>((</span><span>location</span><span>.</span><span>latitude</span><span>,</span> <span>location</span><span>.</span><span>longitude</span><span>),</span> <span>max_zoom</span><span>=</span><span>20</span><span>,</span> <span>zoom_start</span><span>=</span><span>12</span><span>)</span>
<span>folium</span><span>.</span><span>GeoJson</span><span>(</span><span>buffer</span><span>)</span><span>.</span><span>add_to</span><span>(</span><span>m</span><span>)</span>
<span>folium</span><span>.</span><span>GeoJson</span><span>(</span><span>home</span><span>,</span> <span>tooltip</span><span>=</span><span>folium</span><span>.</span><span>GeoJsonTooltip</span><span>([</span><span>'address'</span><span>]))</span><span>.</span><span>add_to</span><span>(</span><span>m</span><span>)</span>
<span>m</span>
</code></pre></div>



<p>Sure looks like I can go a long way! If only. Don't forget: every map is lying to you, even if it's not deliberate!</p>
<p>We'll use the bounds from this along with the <a href="https://osmnx.readthedocs.io/en/stable/">osmnx</a> library to extract the road network data from <a href="https://openstreetmap.org/">Open Street Map</a>. The osmnx library is a really powerful tool to extract and use data in Python from Open Street Map (the Wikipedia of maps). We'll talk more about how it works as we go on but for now let's grab the data and draw a simple plot:</p>
<div><pre><span></span><code><span>bounds</span> <span>=</span> <span>buffer</span><span>.</span><span>bounds</span><span>.</span><span>loc</span><span>[</span><span>0</span><span>]</span>
<span>bounds</span>
</code></pre></div>

<div><pre><span></span><code>minx    144.845830
miny    -37.846556
maxx    144.959347
maxy    -37.756484
Name: 0, dtype: float64
</code></pre></div>

<div><pre><span></span><code><span>region</span> <span>=</span> <span>ox</span><span>.</span><span>graph_from_bbox</span><span>(</span><span>bounds</span><span>[</span><span>'maxy'</span><span>],</span> <span>bounds</span><span>[</span><span>'miny'</span><span>],</span> <span>bounds</span><span>[</span><span>'minx'</span><span>],</span> <span>bounds</span><span>[</span><span>'maxx'</span><span>])</span>
</code></pre></div>



<p><img alt="png" src="https://pythoncharmers.com/blog/extras/Calculate%20your%205km%20travel%20bubble_25_0.png"></p>
<p>Note that this might take a couple of minutes to download from the Open Street Map servers depending on your internet connection. There are options to just download the road network or the walking network if you prefer, but we're looking at anywhere you can reach within 5km.</p>

<p>Conceptually there are two steps to calculating your 5km travel radius.
1. Calculate how much of the network that we can reach within 5km;
2. Draw an accurate boundary around this region.</p>
<h2 id="getting-all-the-nodes-in-the-graph-using-network-algorithms">Getting all the nodes in the graph using network algorithms</h2>
<p>As part ofcalculating the region of our street network graph we can reach first we need to get the nearest node in our graph to our starting point.</p>
<p>As a bonus this will be very simple as the geocoder we used was Nominatim which is built on top of Open Street Map - as long as we found an address we will be able to match it to a node.</p>
<div><pre><span></span><code><span>center_node</span> <span>=</span> <span>ox</span><span>.</span><span>get_nearest_node</span><span>(</span><span>region</span><span>,</span> <span>(</span><span>home</span><span>.</span><span>loc</span><span>[</span><span>0</span><span>,</span> <span>'geometry'</span><span>]</span><span>.</span><span>y</span><span>,</span> <span>home</span><span>.</span><span>loc</span><span>[</span><span>0</span><span>,</span> <span>'geometry'</span><span>]</span><span>.</span><span>x</span><span>))</span>

<span>center_node</span>  <span># this is the node ID</span>
</code></pre></div>



<p>Again the data we have extracted from Open Street Map is uses latitude and longitude as its current coordinate system. We need to project this data so that it's in a system that uses metres as its unit of measurement. Once again we'll use EPSG code 7855 - the GDA2020 MGA Zone 55 projection.</p>
<div><pre><span></span><code><span>region</span> <span>=</span> <span>ox</span><span>.</span><span>project_graph</span><span>(</span><span>region</span><span>,</span> <span>7855</span><span>)</span>
</code></pre></div>

<p>And now we can choose some distances of interest. Because it looks good we might increment all the way up to 5,000 metres in increments of 500 metres. Ultimately we will be interested in just the 5,000 metres. The methodology here was adapted from the osmnx <a href="https://github.com/gboeing/osmnx-examples/blob/master/notebooks/13-isolines-isochrones.ipynb">Isochrones example</a>.</p>
<div><pre><span></span><code><span>distances</span> <span>=</span> <span>np</span><span>.</span><span>arange</span><span>(</span><span>0</span><span>,</span> <span>5001</span><span>,</span> <span>500</span><span>)</span>
<span>distances</span>
</code></pre></div>

<div><pre><span></span><code>array([   0,  500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000])
</code></pre></div>

<div><pre><span></span><code><span># get one color for each isochrone</span>
<span>iso_colors</span> <span>=</span> <span>ox</span><span>.</span><span>plot</span><span>.</span><span>get_colors</span><span>(</span><span>n</span><span>=</span><span>len</span><span>(</span><span>distances</span><span>),</span> <span>cmap</span><span>=</span><span>'plasma'</span><span>,</span> <span>start</span><span>=</span><span>0</span><span>,</span> <span>return_hex</span><span>=</span><span>True</span><span>)</span>
</code></pre></div>

<div><pre><span></span><code><span>node_colors</span> <span>=</span> <span>{}</span>

<span>for</span> <span>trip_time</span><span>,</span> <span>color</span> <span>in</span> <span>zip</span><span>(</span><span>sorted</span><span>(</span><span>distances</span><span>,</span> <span>reverse</span><span>=</span><span>True</span><span>),</span> <span>iso_colors</span><span>):</span>
    <span>subgraph</span> <span>=</span> <span>nx</span><span>.</span><span>ego_graph</span><span>(</span><span>region</span><span>,</span> <span>center_node</span><span>,</span> <span>radius</span><span>=</span><span>trip_time</span><span>,</span> <span>distance</span><span>=</span><span>'length'</span><span>)</span>
    <span>for</span> <span>node</span> <span>in</span> <span>subgraph</span><span>.</span><span>nodes</span><span>():</span>
        <span>node_colors</span><span>[</span><span>node</span><span>]</span> <span>=</span> <span>color</span>

<span>nc</span> <span>=</span> <span>[</span><span>node_colors</span><span>[</span><span>node</span><span>]</span> <span>if</span> <span>node</span> <span>in</span> <span>node_colors</span> <span>else</span> <span>'none'</span> <span>for</span> <span>node</span> <span>in</span> <span>region</span><span>.</span><span>nodes</span><span>()]</span>
<span>ns</span> <span>=</span> <span>[</span><span>15</span> <span>if</span> <span>node</span> <span>in</span> <span>node_colors</span> <span>else</span> <span>0</span> <span>for</span> <span>node</span> <span>in</span> <span>region</span><span>.</span><span>nodes</span><span>()]</span>
<span>fig</span><span>,</span> <span>ax</span> <span>=</span> <span>ox</span><span>.</span><span>plot_graph</span><span>(</span><span>region</span><span>,</span> <span>node_color</span><span>=</span><span>nc</span><span>,</span> <span>node_size</span><span>=</span><span>ns</span><span>,</span> <span>node_alpha</span><span>=</span><span>0.8</span><span>,</span>
                        <span>edge_linewidth</span><span>=</span><span>0.2</span><span>,</span> <span>edge_color</span><span>=</span><span>'#999999'</span><span>)</span>
</code></pre></div>

<p><img alt="png" src="https://pythoncharmers.com/blog/extras/Calculate%20your%205km%20travel%20bubble_34_0.png"></p>
<p>From this visualisation you can see that I definitely can't go out as far as 5km as the crow flies. You can see …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://pythoncharmers.com/blog/travel-distance-python-with-geopandas-folium-alphashape-osmnx-buffer.html">https://pythoncharmers.com/blog/travel-distance-python-with-geopandas-folium-alphashape-osmnx-buffer.html</a></em></p>]]>
            </description>
            <link>https://pythoncharmers.com/blog/travel-distance-python-with-geopandas-folium-alphashape-osmnx-buffer.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26223690</guid>
            <pubDate>Mon, 22 Feb 2021 12:25:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why I’m Losing Trust in Open Source]]>
            </title>
            <description>
<![CDATA[
Score 22 | Comments 31 (<a href="https://news.ycombinator.com/item?id=26223575">thread link</a>) | @bodegajed
<br/>
February 22, 2021 | https://gibson.ws/why-im-losing-trust-in-open-source/ | <a href="https://web.archive.org/web/*/https://gibson.ws/why-im-losing-trust-in-open-source/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="page">

		
					<!-- Image banner -->
					
					<!-- Image banner -->

					<div id="content">
						<div>
							<div>

	<div id="primary">
		<main id="main">

		
<article id="post-146">
			<!-- .entry-header -->

	<div>
		<p>Back when I was starting to code several years ago. I picked up The Cathedral and the Bazaar by Eric Raymond and I was blown away at the idea of free software. Just in case you are not familiar, free software as in freedom and not free beer. Free software back then was this super radical and idealistic concept where as you make a software product commercial or not, but when you distribute it you include the source code of it. The person who got your product would eventually continue to develop it and it will evolve and continually improve as it gets to many users. You then will be looking at their version of your product and will see how it has grown further. Think of it has a community garden where everyone grows their vegetable and anyone would then take pointers on some of your crops and grow their improved version. Eventually you’ll see where you are doing it wrong by looking at how they tend their garden. This is not necessarily free food for everyone – although it’s common. It’s freedom to copy and use my garden setup so we have bigger crops next harvest time.</p>
<p>This what happened to Linux, nodejs, Ruby etc.. I’ve believed in it much so I joined sourceforge joined a team, also started a project myself even. I followed this radical concept through the years and publish my projects openly on github. It was fun and there is some sort of social acceptance when people see your ugly looking code yet they accept it and submit their own ugly looking code as well.</p>
<p>Facebook, Apple, Google these companies are worth trillions of dollars and they all at one point when they are still small companies depended on open source. Their founders built an MVP and took money from VCs and then had to responsibly return their money 10x. They eventually all cashed out and now driving luxury sports cars. Meanwhile present day Linux desktop is still dead. Open source maintainers abandoning projects due to lack of time and interest. They say why not just use GPL but if you license your code using GPL you will not have users. Developers can’t even share the name of the software they are putting your code into because of these NDA they signed. Sometimes it’s just a simple request like attribution and compliance is still uncommon.</p>
<p>Life as a open source maintainer is sometimes a <a href="https://daniel.haxx.se/blog/2021/02/19/i-will-slaughter-you/">life threatening endeavor</a></p>
<p>Society, Conglomerates, and Capitalism killed free software and nobody cared. It’s all about 10x ROI and taking advantage of some poor idiot programmer clueless in business.</p>

			</div><!-- .entry-content -->
</article><!-- #post-146 -->

<!-- #comments -->

		</main><!-- #main -->
	</div><!-- #primary -->


<!-- #secondary -->
</div>
</div><!-- #content -->
</div>
</div></div>]]>
            </description>
            <link>https://gibson.ws/why-im-losing-trust-in-open-source/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26223575</guid>
            <pubDate>Mon, 22 Feb 2021 12:09:34 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Ciitizen and the Patient Data Marketplace]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26223570">thread link</a>) | @bbirnbaum
<br/>
February 22, 2021 | https://outofpocket.health/p/ciitizen-and-the-patient-data-marketplace | <a href="https://web.archive.org/web/*/https://outofpocket.health/p/ciitizen-and-the-patient-data-marketplace">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><h2><strong>TL:DR&nbsp;</strong></h2><p>I wanted to use this as an opportunity to talk a little bit about patient data access, health data exchanges, and what <a href="https://www.ciitizen.com/">Ciitizen</a> is doing to help us take control of our healthcare data (with my take at the end).  </p><p><em>This is a sponsored post - you can read more about my rules/thoughts on sponsored posts <a href="https://outofpocket.health/p/an-update-about-out-of-pocket">here</a>. If you’re interested in having a sponsored post done, email nikhil@outofpocket.health.</em></p><p><a href="https://www.ciitizen.com/">Ciitizen</a> is a personal health record with the goal of making it easier to choose where you want your health data to go. It decided on its name based on what would permanently mess up my autocorrect going forward.</p><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa14a66b-591d-4062-b948-1301cd22bd0f_1576x960.png"><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa14a66b-591d-4062-b948-1301cd22bd0f_1576x960.png" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/fa14a66b-591d-4062-b948-1301cd22bd0f_1576x960.png&quot;,&quot;height&quot;:887,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null}" alt=""></a></figure></div><p>Ciitizen was founded by <a href="https://www.linkedin.com/in/mranilsethi/">Anil Sethi,</a><a href="https://twitter.com/HealthPrivacy?s=20">Deven McGraw, </a><a href="https://www.linkedin.com/in/bcarlsenca/">Brian Carlsen</a>, <a href="https://www.linkedin.com/in/faridvij/">Farid Vij </a>and <a href="https://www.linkedin.com/in/peeyushrai/">Peeyush Rai</a>. Initially they thought about forming a basketball team before landing on Ciitizen. Previously, Anil was the founder of Gliimpse, which is the personal health record company <a href="https://www.fastcompany.com/3062865/apple-acquires-personal-health-data-startup-gliimpse">acquired by Apple</a> which eventually became the underlying technology for Apple Health Records. Deven was previously the Deputy Director, Health Information Privacy at the HHS who wrote much of the HIPAA patient access guidance we operate under today. Brian Carlsen authored the NLM/NIH data standards that became SNOMED and other leading bioinformatics underpinnings. Farid and Peeyush bring the tech experience to the party. As you read more, I think you’ll realize this is as close to founder(s)-market-fit as you can possibly get.</p><p>The 100+ person company has raised more than $27 million so far from investors including Vijay Pande from a16z, Mike Pellini of Section 32, Verily, and Mubadala Ventures.&nbsp;</p><p>I knew the day would come where I’d have to explain a semi-complicated healthcare data product but I was hoping I’d have at least a few more months to live carefree first. Alas that is not the case. Just kidding - talking about Ciitizen is a great chance to explain how healthcare data moves around the ecosystem all without our knowledge.&nbsp;</p><p>Ciitizen gets your health records (think thousands of pages of incomprehensible, repetitive documents in one fat stack of a PDF) from the many parts of the healthcare system. Then it uses its fancy ML pipeline to create research-grade data from an otherwise hellish stack of PDFs and securely stores it under patient control. Simply sign a form that says “Ciitizen is allowed to get my data from hospitals on my behalf.” Finally, Ciitizen hits up hospitals, imaging centers, genetic labs, etc and gets all your records for you.&nbsp;</p><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc36ae850-8415-4557-823e-3f72fd3251db_1421x1600.jpeg"><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc36ae850-8415-4557-823e-3f72fd3251db_1421x1600.jpeg" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/c36ae850-8415-4557-823e-3f72fd3251db_1421x1600.jpeg&quot;,&quot;height&quot;:1600,&quot;width&quot;:1421,&quot;resizeWidth&quot;:560,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null}" alt=""></a></figure></div><p>This is, at its core, what a patient’s HIPAA Right of Access is meant to do. As patients we are allowed total access to our complete health history, but that doesn’t mean it’s easy to get. Sure, you can log into your patient portal if you have one and if you remember the password. But it wouldn’t give you the complete story. If you want all of the records (and are brave enough), you could pester each one of your providers for your data and they might charge you a fee, take a super long time dragging their feet getting the data to you, and likely violate a bunch of HIPAA regulations in the process. Then finally they’ll get a CD-ROM or something and you’ll have to ask any of your friends if they have a CD-ROM drive and they’ll laugh at you. Or they’ll point you to their portal which only has a fragment of your total health record, which sort of defeats the purpose?</p><p>Ciitizen prevents you from getting laughed at by your friends. Ciitizen’s technology automatically bugs providers on your behalf, receives the documents that come in all sorts of formats like faxes, scanned PDFs, mailed boxes of paper (I hate this industry so much), emails, etc. Then Ciitizen gets those documents, structures the clinical narrative into computable data, and gives it to you as a patient so you have your very own complete personal health record. They do this fast (well, healthcare fast) getting all of your records together in a few days vs. weeks or months. Some data comes even in minutes but the complete record takes time. For people with advanced diseases, days versus weeks makes a huge difference.&nbsp;</p><p>Right now they’ve started with cancer and rare neurological conditions and are moving into other areas like autoimmune diseases, eventually serving all patients with all conditions.</p><p>[Below are screenshots of what Anil’s late sister Tania would see in her Ciitizen profile. Data and screenshots shared with permission.]</p><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4ac42c4-3462-4d3b-b23a-c1b8e6684023_1600x1315.png"><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4ac42c4-3462-4d3b-b23a-c1b8e6684023_1600x1315.png" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/c4ac42c4-3462-4d3b-b23a-c1b8e6684023_1600x1315.png&quot;,&quot;height&quot;:1197,&quot;width&quot;:1456,&quot;resizeWidth&quot;:626,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null}" alt=""></a></figure></div><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F56f08f96-8896-4acd-8a6b-05bdb9eb2eb6_1600x1315.png"><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F56f08f96-8896-4acd-8a6b-05bdb9eb2eb6_1600x1315.png" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/56f08f96-8896-4acd-8a6b-05bdb9eb2eb6_1600x1315.png&quot;,&quot;height&quot;:1197,&quot;width&quot;:1456,&quot;resizeWidth&quot;:612,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null}" alt=""></a></figure></div><p><em>All extracted data is easily source verified to the original document.&nbsp;</em></p><p>There are a lot of companies building applications that retrieve and structure patient’s data, so I wanted to talk about some specific things Ciitizen is betting on as differentiation.</p><h3><strong>Data ingestion</strong></h3><p>Ciitizen’s secret sauce begins with the document ingestion and data structuring technology. First, they have some pretty fun automated processes that bother hospitals, clinics and labs at scale until they send over the patient’s health records to Ciitizen. They even made a <a href="http://www.patientrecordscorecard.com/">scorecard</a> on how well each hospital does this. This is my favorite part of the process because I, too, enjoy bothering people at scale.</p><p>Ciitizen then gathers the incoming documents, regardless of format and turns them into readable data using machine learning to understand the different sections of the forms, faxes, PDFs etc. This is necessary to contextualize the data in each section (e.g. histological subtype from pathology report, medications from chemo flow sheets and findings from imaging reports, etc.). The ML pipeline automatically parses unstructured text into semantically normalized data informed by Ciitizen’s multidimensional data models. Ciitizen then leverages clinical experts to quality control and make sure all the important fields are correctly placed and coded. This is that research-grade stuff, Heisenberg quality data.</p><p>Here is a general overview of the process. It has that newsletter-cute handwritten aesthetic. Shout out to Brian C, Ciitizen co-founder and czar of all things bioinformatics who did his best to explain this to my smooth-brained self.&nbsp;</p><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4db403a5-44e9-4857-8109-ca1095b18d22_671x248.png"><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F4db403a5-44e9-4857-8109-ca1095b18d22_671x248.png" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/4db403a5-44e9-4857-8109-ca1095b18d22_671x248.png&quot;,&quot;height&quot;:248,&quot;width&quot;:671,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null}" alt=""></a></figure></div><div><figure><a target="_blank" href="https://cdn.substack.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F819eee1a-8a15-407d-8388-e304ac2b7fd0_1600x870.png"><img src="https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F819eee1a-8a15-407d-8388-e304ac2b7fd0_1600x870.png" data-attrs="{&quot;src&quot;:&quot;https://bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com/public/images/819eee1a-8a15-407d-8388-e304ac2b7fd0_1600x870.png&quot;,&quot;height&quot;:792,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null}" alt=""></a></figure></div><p>By focusing on getting the source documents, Ciitizen is aiming to get deeper data on (initially) targeted groups of patients. By starting with the source documents and choosing how they can structure it, Ciitizen has mapped the data (they called it “ontologies”) so that clinical concepts that mean the same thing are mapped together (e.g. acetaminophen and tylenol are mapped to the same thing, breast cancer and invasive ductal carcinoma will have a parent-child relationship, etc.). No more extremely janky SQL queries with 50 permutations of cancer in your WHERE clause to make sure you got it all.&nbsp;</p><p>One question is whether this technology is defensible, and if improvements in off-the-shelf machine learning packages won’t make it easy for another company to copy. There’s a lot of focus on clinical natural language processing, which tons of companies are doing and which typically results in it finding words without much context. Data standardization is really the key differentiator. While the ability to ingest data and identify common text terms will become a commodity eventually, automating the accurate codification of this data to standard concepts based on the specific context of a patient's history is the hard part to replicate. Good biostatisticians make bank for a reason.</p><p>Ciitizen’s main bet is that their technology will effectively scale with as few humans as possible as more patients with different diseases use Ciitizen. The company started working with cancer patients, and has already moved into several other therapeutic areas. This is one of the core value-propositions of Ciitizen: it must work for everyone, globally. And if it requires a lot of humans to get the data, that’s going to be very expensive. “Calling an Uber on New Year’s” -expensive. So Ciitizen is going to have to leverage its AI and ML to expand into other therapeutic areas and industry verticals if it plans on doing it faster than its competitors.&nbsp;</p><h3><strong>The FHIR Extinguisher (Please don’t unsubscribe).</strong></h3><div><p>Ciitizen is also taking the interesting approach of not building exclusively on FHIR. You can read more about FHIR <a href="https://www.hl7.org/fhir/overview.html">here</a>, but the general gist is that FHIR creates a common data standard for healthcare organizations that outlines the types of data, their formats, the field name for that data, etc. so it’s easy to query via APIs. It’s sort of similar to how your web browser can go to any page and load because it knows we use standardized names for elements. </p><p>FHIR is still a relatively new concept slowly being rolled out. There are FHIR APIs that exist today, but historically there hasn’t been much guidance or pressure on types of data providers’ EMRs have to allow through them. However with new interoperability rules from the ONC, there will be a <a href="https://www.healthit.gov/isa/united-states-core-data-interoperability-uscdi">common set of data types</a> that are mandated to be available through FHIR v4 by 2022. Many apps are choosing to pull patient data out of EMRs using FHIR APIs to build personal health records.</p></div><p>Ciitizen has looked at FHIR and said “😬😬”. Their belief is that the current data that EMRs are required to put out through their APIs are missing research grade data that is relevant for complex diagnoses like cancer, including (but not limited to):</p><ul><li><p>CT/MRI, Images&nbsp;</p></li><li><p>Genetics/genomics&nbsp;</p></li><li><p>Tumor profiling details</p></li><li><p>Pathology and imaging reports&nbsp;</p></li><li><p>ECGs/device reading</p></li><li><p>Clinical Notes</p></li></ul><p>Some of these like clinical notes will be rolled out in 2022, assuming the date sticks and EMRs are compliant. Several of these other data fields are presumably going to be included <a href="https://www.healthit.gov/isa/united-states-core-data-interoperability-uscdi">in round 2</a> of interoperability requirements, but TBD on how long it will take for that to happen and rollout. So the timeline is up in the air on getting all the data types necessary for research, and even when it comes through it will be unstructured. FHIR is also not a global standard - and Ciitizen is thinking about what their eventual international expansion will look like (even if it’s early days).</p><p>Ciitizen’s CEO Anil Sethi kept repeating this line as I asked him to explain things to me so I feel …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://outofpocket.health/p/ciitizen-and-the-patient-data-marketplace">https://outofpocket.health/p/ciitizen-and-the-patient-data-marketplace</a></em></p>]]>
            </description>
            <link>https://outofpocket.health/p/ciitizen-and-the-patient-data-marketplace</link>
            <guid isPermaLink="false">hacker-news-small-sites-26223570</guid>
            <pubDate>Mon, 22 Feb 2021 12:08:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Minesweeper]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 1 (<a href="https://news.ycombinator.com/item?id=26223504">thread link</a>) | @madprops
<br/>
February 22, 2021 | https://madprops.github.io/minesweeper/ | <a href="https://web.archive.org/web/*/https://madprops.github.io/minesweeper/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://madprops.github.io/minesweeper/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26223504</guid>
            <pubDate>Mon, 22 Feb 2021 11:58:44 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Virtual passport app presents real data risk, experts warn]]>
            </title>
            <description>
<![CDATA[
Score 115 | Comments 96 (<a href="https://news.ycombinator.com/item?id=26223347">thread link</a>) | @pseudolus
<br/>
February 22, 2021 | https://www.cbc.ca/news/canada/ottawa/passport-application-online-program-1.5920625 | <a href="https://web.archive.org/web/*/https://www.cbc.ca/news/canada/ottawa/passport-application-online-program-1.5920625">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>Canadian privacy experts are concerned the federal government's plan to develop an online passport application process could put&nbsp;personal information at risk and open a new angle of attack for fraudsters.</p><div><figure><div><p><img loading="lazy" alt="" srcset="" sizes="" src="https://i.cbc.ca/1.4612031.1536417303!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_780/passport.jpg"></p></div><figcaption>IBM Canada's digital passport application platform is expected to begin testing in three months, and could be ready for use as early as 2022.<!-- --> <!-- -->(John Badcock/CBC)</figcaption></figure><p><span><p>Canadian privacy experts are concerned the federal government's plan to develop an online passport application process could put&nbsp;personal information at risk and open a new angle of attack for fraudsters.</p>  <p>IBM Canada has been awarded the&nbsp;$1.5-million contract to create software that would allow Canadians to apply for a passport using their&nbsp;smartphones, tablets or&nbsp;computers.</p>  <p>The new platform would also allow applicants to&nbsp;pay fees and upload their passport photos securely, according to a statement from Immigration, Refugees and Citizenship Canada (IRCC).</p>    <blockquote><span><span><svg version="1.1" focusable="false" x="0px" y="0px" width="30px" height="25px" viewBox="0 0 52.157 39.117" enable-background="new 0 0 52.157 39.117" space="preserve"><g><g><path fill="000000" d="M22.692,10.113c-5.199,1.4-8.398,4.4-8.398,8.801c0,2.4,2,3,3.6,4.199c2.2,1.602,3.4,3.4,3.4,6.602   c0,3.799-3.4,6.799-7.4,6.799c-4.6,0-8.8-3.199-8.8-10.6c0-13.6,7-20,17.599-21.799V10.113z M45.692,10.113   c-5.199,1.4-8.399,4.4-8.399,8.801c0,2.4,2,3,3.601,4.199c2.2,1.602,3.399,3.4,3.399,6.602c0,3.799-3.399,6.799-7.399,6.799   c-4.601,0-8.8-3.199-8.8-10.6c0-13.6,7-20,17.599-21.799V10.113z"></path></g></g><g display="none"><g display="inline"> <path fill="000000" d="M6.648,29.759c5.199-1.4,8.398-4.4,8.398-8.801c0-2.398-2-3-3.599-4.199c-2.2-1.6-3.399-3.4-3.399-6.6   c0-3.801,3.399-6.801,7.399-6.801c4.599,0,8.8,3.201,8.8,10.6c0,13.6-7,20-17.6,21.801V29.759z M29.648,29.759   c5.199-1.4,8.398-4.4,8.398-8.801c0-2.398-2-3-3.599-4.199c-2.2-1.6-3.401-3.4-3.401-6.6c0-3.801,3.401-6.801,7.401-6.801   c4.599,0,8.8,3.201,8.8,10.6c0,13.6-7,20-17.6,21.801V29.759z"></path></g></g></svg>They will attempt to&nbsp;exploit the program very quickly, very intensely to obtain the most fraudulent passports they can in the least amount of time.<svg focusable="false" x="0px" y="0px" width="23px" height="22px" viewBox="0 0 52.157 39.117" enable-background="new 0 0 52.157 39.117" space="preserve"><g display="none"><g display="inline"><path fill="000000" d="M22.692,10.113c-5.199,1.4-8.398,4.4-8.398,8.801c0,2.4,2,3,3.6,4.199c2.2,1.602,3.4,3.4,3.4,6.602   c0,3.799-3.4,6.799-7.4,6.799c-4.6,0-8.8-3.199-8.8-10.6c0-13.6,7-20,17.599-21.799V10.113z M45.692,10.113   c-5.199,1.4-8.399,4.4-8.399,8.801c0,2.4,2,3,3.601,4.199c2.2,1.602,3.399,3.4,3.399,6.602c0,3.799-3.399,6.799-7.399,6.799   c-4.601,0-8.8-3.199-8.8-10.6c0-13.6,7-20,17.599-21.799V10.113z"></path></g></g><g><g><path fill="000000" d="M6.648,29.759c5.199-1.4,8.398-4.4,8.398-8.801c0-2.398-2-3-3.599-4.199c-2.2-1.6-3.399-3.4-3.399-6.6   c0-3.801,3.399-6.801,7.399-6.801c4.599,0,8.8,3.201,8.8,10.6c0,13.6-7,20-17.6,21.801V29.759z M29.648,29.759   c5.199-1.4,8.398-4.4,8.398-8.801c0-2.398-2-3-3.599-4.199c-2.2-1.6-3.401-3.4-3.401-6.6c0-3.801,3.401-6.801,7.401-6.801   c4.599,0,8.8,3.201,8.8,10.6c0,13.6-7,20-17.6,21.801V29.759z"></path></g></g></svg></span><cite>- Benoît&nbsp;Dupont, l'Université de Montréal</cite></span></blockquote>    <p>But privacy and data protection experts worry that personal information may be stored on foreign servers, providing an appealing target to criminals.</p>  <p>Sébastien Gambs, a professor in the information technology department of l'Université de Québec à Montréal&nbsp;and Canada Research Chair on privacy and data protection, said there are real&nbsp;concerns about&nbsp;where the data will be stored, a detail neither the government nor IBM Canada has divulged, though the tender identifies Amazon Web Services (AWS), the cloud computing branch of the American online retail giant.</p>  <p>"Even when we do business with an American company that agrees to store data within Canada, under the [U.S.] CLOUD Act, data could eventually be transferred out of the country," Gambs said in French.</p>  <p>In a statement, AWS said its clients retain full ownership and control of their data, including who may access that information.</p>  <p>In a separate statement, IRCC said "the privacy of Canadians and the safety of their personal information will be an absolute priority."</p>  <p><span><figure><div><p><img loading="lazy" alt="" srcset="https://i.cbc.ca/1.5555269.1588631660!/fileImage/httpImage/image.JPG_gen/derivatives/original_300/tech-cloud.JPG 300w,https://i.cbc.ca/1.5555269.1588631660!/fileImage/httpImage/image.JPG_gen/derivatives/original_460/tech-cloud.JPG 460w,https://i.cbc.ca/1.5555269.1588631660!/fileImage/httpImage/image.JPG_gen/derivatives/original_620/tech-cloud.JPG 620w,https://i.cbc.ca/1.5555269.1588631660!/fileImage/httpImage/image.JPG_gen/derivatives/original_780/tech-cloud.JPG 780w,https://i.cbc.ca/1.5555269.1588631660!/fileImage/httpImage/image.JPG_gen/derivatives/original_1180/tech-cloud.JPG 1180w" sizes="(max-width: 300px) 300px,(max-width: 460px) 460px,(max-width: 620px) 620px,(max-width: 780px) 780px,(max-width: 1180px) 1180px" src="https://i.cbc.ca/1.5555269.1588631660!/fileImage/httpImage/image.JPG_gen/derivatives/original_780/tech-cloud.JPG"></p></div><figcaption>Amazon Web Services (AWS) says its clients retain full control over the data it stores.<!-- --> <!-- -->(Ivan Alvarado/Reuters)</figcaption></figure></span></p>  <h2>Canadian passports highly valued</h2>  <p>Benoît Dupont, a criminology professor at l'Université de Montréal and Canada Research Chair in cybersecurity, said the passport app will likely be a major target for fraudsters&nbsp;eager to get their hands on&nbsp;Canadian passports and the mobility that comes with them.</p>  <p>"That's very attractive for organized crime groups who specialize in human trafficking," Dupont said in French. "They will attempt to&nbsp;exploit the program very quickly, very intensely to obtain the most fraudulent passports they can in the least amount of time."</p>  <p>But&nbsp;Gambs said any virtual application will likely have extra&nbsp;steps built in to protect against hackers.</p>  <p>"As soon as we're doing things remotely, verifying somebody's identity becomes much more difficult," he said. "The government will definitely need to collect more personal information in order to verify an applicant's identity."</p>  <h2>'Vicious cycle': PIPSC</h2>  <p>The Professional Institute of the Public Services (PIPSC) said this tender should never have gone out to the private sector when it could have been developed in-house by public servants, as was done with the online tax portal.</p>  <p>"It's a vicious cycle. Instead of developing resources internally, we go externally," said&nbsp;Stéphane Aubry, vice-president of PIPSC. "Then we don't have the needed expertise internally,&nbsp;which&nbsp;unfortunately, over the years, fades and makes it so we need to contract out."</p>  <p>PIPSC said the project raises <a href="https://www.cbc.ca/news/canada/ottawa/phoenix-pay-system-cost-report-1.5138036">the spectre of the Phoenix pay system fiasco</a>, which also involved IBM.&nbsp;IBM Canada will be required to train and support IRCC employees in running the new passport system, according to the tender documents.</p>  <p>In 2020, the government issued just 897,401 passports, compared to 2.6 million the year before. For the first four months of the pandemic, Service Canada was only providing critical passport services for urgent travel.&nbsp;</p>  <p>Nevertheless, the Canadian Anti-Fraud Centre received 1,806 reports of passport-related fraud last year.</p></span></p></div></div>]]>
            </description>
            <link>https://www.cbc.ca/news/canada/ottawa/passport-application-online-program-1.5920625</link>
            <guid isPermaLink="false">hacker-news-small-sites-26223347</guid>
            <pubDate>Mon, 22 Feb 2021 11:36:28 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[An Anti-Democratic Exercise of Monopolistic Power]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26223310">thread link</a>) | @patrickdavidson
<br/>
February 22, 2021 | https://pancake.nz/blog/fbnewsban/ | <a href="https://web.archive.org/web/*/https://pancake.nz/blog/fbnewsban/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
        
        <p>Saturday, 20 February 2021</p>
        <p>On Thursday morning, in response to the proposed News Media and Digital Platforms Mandatory Bargaining Code in Australia, Facebook (with a "<a href="https://about.fb.com/news/2021/02/changes-to-sharing-and-viewing-news-on-facebook-in-australia/">heavy heart</a>") restricted the ability for any news to be shared by Australian users, and for Australian news to be shared by any users. Visiting an Australian news organisation's Facebook page would simply display "No posts yet".</p>
        <p><img src="https://pancake.nz/blog/fbnewsban/media/no-posts-yet.png" alt="An empty Australian news Facebook page"></p><p>If you look closely at the image above you will notice there are still two hyperlinks to news.com.au's website on the page, so technically it seems this could still be non-compliant with the proposed code.</p>
        <p>Attempting to post a link from an Australian news source, no matter where in the world the user is, would result in a popup, stating that "This post can't be shared".</p>
        <p><img src="https://pancake.nz/blog/fbnewsban/media/post-cant-be-shared.png" alt="Popup restricting the posting of Australian news"></p><p>Many people have been quick to label Facebook's actions as irresponsible and potentially harmful. Most of the initial criticism was due to the fact that whatever tool or algorithm Facebook had developed to remove news content had inadvertently removed content from "necessary" Facebook pages. Such as Government Health pages and the Bureau of Meteorology, which provides severe weather warnings. It doesn't stop there, Facebook wiped the pages of <a href="https://www.smh.com.au/national/facebook-news-ban-hits-emergency-services-and-government-health-departments-20210218-p573ks.html">many organisations</a> which should not have been caught up in the restrictions. Facebook blamed these issues on the broad definition of "news content" as defined in the proposed code, but quickly reinstated the wrongly blocked pages. They stopped short of <a href="https://www.abc.net.au/news/2021-02-18/facebook-unrepentant-scott-morrison-dubs-move-arrogant/13169340">apologising</a>, however.</p>
        <p>The more legitimate harm that Facebook's actions could cause is aiding in the spread of misinformation. Removing all news content from legitimate news sources but leaving up the unregulated content can do nothing but worsen the situation that Facebook has already been struggling to contain.</p>
        <p><img src="https://pancake.nz/blog/fbnewsban/media/fake-news.png" alt="A vandalised ad for Facebook"></p><p>As I mentioned in the first sentence of this post, Facebook took this action in response to the proposed News Media and Digital Platforms Mandatory Bargaining Code. Both <a href="https://about.fb.com/news/2020/08/changes-to-facebooks-services-in-australia/">Facebook</a> and <a href="https://about.google/google-in-australia/an-open-letter/">Google</a> have been strongly opposed to code since it was drafted in the latter half of 2020, and both threatened drastic action if their concerns about the code were not addressed. Their concerns were not addressed. So on Thursday Facebook followed through with their threat.</p>
        <h2>For the Code</h2>
        <p>The goal of the proposed code is to address the bargaining imbalance which exists between digital service providers (Facebook and Google) and news organisations. It aims to achieve this by forcing Facebook and Google to negotiate with news organisations for the right to distribute or link to their content. At the moment, whenever you search something on Google, the results you see not only include a link to the page, but also the title, an excerpt from the page, and in the case of some news content, an image.</p>
        <p><img src="https://pancake.nz/blog/fbnewsban/media/google-search.png" alt="A Google search result showing scraped content"></p><p>Google isn't paying anything to the respective organisations to show this content, and the Australian Government, along with the major media outlets, think they should.</p> 
        <p>Google Search makes money by showing ads alongside the actual search results. At the moment, none of that money is shared with the websites who's pages show up in the search results. The news organisations are only making money when a user actually clicks a link to their website, and gets to look at the news organisation's ads instead.</p>
        <p>Facebook engages in similar practices whenever a user shares a link. They scrape the website for the headline, an excerpt, and an image.</p>
        <p><img src="https://pancake.nz/blog/fbnewsban/media/facebook-scraping.png" alt="A Facebook post showing scraped content"></p><p>An example of an outraged New Zealand MP sharing a link to a news article on Facebook, which Facebook has scraped for the Headline, an excerpt, and an image.</p>
        <p>There is a key difference in Facebook's situation though, and that's the fact that it is not Facebook itself which is posting the links - it's its users. The implications of which I will explain later on.</p>
        <p>The key assumption here is that people are using the scraped content as a substitute for actually clicking the link and reading the article. In which case all the advertising revenue goes to Facebook or Google and none to the news organisation. You may be sitting there reading this thinking you are one of those people who always reads the actual article and not just the headline and therefore are supporting the news organisations directly, and you could be right. However that's almost missing the point. When you search something on Google and a bunch of news articles show up in the results you read most of the headlines and even some of the excerpts, and you definitely look at the images, but you may only click and read one of the articles. You have just consumed multiple organisation's content, but only the one you clicked on will get paid for it. This is what the code is aiming to address.</p>
        <h2>Against the Code</h2>
        <p>So the purpose of the code might actually make a lot of sense, Google and Facebook are directly making money from news organisation's content. So why are both companies so strongly opposed to the law?</p>
        <p>It turns out that when you take a closer look at the proposed code things seem to get a bit extreme. Firstly, the code doesn't just cover when Facebook or Google scrape content and reproduce it on their websites, it covers simply linking to the content. No scraping or reproducing needed. It seems a bit of a stretch that looking at a raw URL can be considered consuming content.</p>
        <p><img src="https://pancake.nz/blog/fbnewsban/media/simply-linking.png" alt="The part of the code which defines how content is made available"></p><p>The code's definition of "Making content available"</p>
        <p>Making a company pay to share links <a href="https://theconversation.com/webs-inventor-says-news-media-bargaining-code-could-break-the-internet-hes-right-but-theres-a-fix-153630">breaks a fundamental principle of the internet</a> - the ability to link freely. It would really suck if I had to pay every company who's website I've linked to in this article. In fact I would probably just not link at all if that were the case, and you would have no idea if I am actually referencing legitimate sources or just making things up as I go. I would effectively take the same action Facebook has, so maybe their position isn't so extreme after all.</p>
        <p>So why does this law only apply to Facebook and Google, and not to Twitter who partakes in the same scraping practices for its links, or even Facebook owned Instagram? Well it's because that's what the <a href="https://ministers.treasury.gov.au/ministers/josh-frydenberg-2018">Treasurer of Australia</a> has decided. Who this code applies to isn't metrics or rules based, it's simply up to who the Treasurer of Australia thinks it should apply to. The only guidelines the code lays out are what I've added below.</p>
        <p><img src="https://pancake.nz/blog/fbnewsban/media/law-applies.png" alt="Who the code applies to"></p><p>This is a lot of power to give to just one Minister, and it has more than just Google and Facebook concerned. Twitter also made a submission to the Australian Parliament, giving reasons why they too oppose the code. You can see all submissions regarding the code, both supporting and opposing, <a href="https://www.aph.gov.au/Parliamentary_Business/Committees/Senate/Economics/TLABNewsMedia/Submissions">here</a>.</p>
        <p>As I mentioned earlier, in Facebook's case, it is not Facebook which is posting links to news content, it's its users. Making Facebook pay for the content which its users post is an uncomfortable precedent to set. Especially when you consider that the users posting these links include the news organisations themselves. So Facebook would essentially be paying news organisations for providing them with a platform to promote themselves. Facebook evidently doesn't see enough value in hosting news content if they also have to pay for it.</p>
        <p><a href="https://about.fb.com/news/2021/02/changes-to-sharing-and-viewing-news-on-facebook-in-australia/">According to Facebook</a>, "News makes up less than 4% of the content people see in their News Feed". So it might not be a huge loss to Facebook if they remove news. Although the 4% quoted by Facebook will likely not directly translate into only a 4% loss of revenue, it's extremely hard to tell what the actual effect will be on Facebook and we will only find out over the coming months, assuming Facebook sticks with its decision, which it could easily not. In the same post, Facebook claims it "generated approximately 5.1 billion free referrals to Australian publishers worth an estimated AU$407 million". I'm no business expert but that seems like a lot of money for the Australian media to miss out on. Facebook also claims to have "delivered A$5.4 million to Australian publishers from revenue share programs, such as In-Stream Ads" not much, but it's worth noting, I guess. Of course the removal of news from Facebook won't mean the people who were clicking the links before will now just not read the news, they may now go directly to the outlet's website, instead of the Facebook page, possibly generating more revenue than if they went through Facebook. But this won't be the case for everyone, and how much of an impact this will have on news organisations is unclear. Founder of New Zealand media company <a href="https://thespinoff.co.nz/editorial-about-us-page/">The Spinoff</a> claims 15%-22% of their traffic comes from Facebook.</p>
        <blockquote data-conversation="none" data-dnt="true" data-theme="light"><p lang="en" dir="ltr">A little glimpse of how much Facebook traffic can impact an individual story / it fades fast, but typically provides 15%-22% of our traffic on a monthly basis. There will be multiple Australian publishers whose businesses will no longer be viable should this not pass. <a href="https://t.co/i8KZ8Sx8IS">pic.twitter.com/i8KZ8Sx8IS</a></p>— Duncan Greive (@duncangreive) <a href="https://twitter.com/duncangreive/status/1362160608502714368?ref_src=twsrc%5Etfw">February 17, 2021</a></blockquote> 
        <p>It's worth noting that The Spinoff is a digital-only media outlet targeted at a younger audience, so its numbers may not be representative of the media industry as a whole, but I wouldn't be surprised if most media outlets were at least in the same ballpark.</p>
        <p>In <a href="https://www.stuff.co.nz/business/industries/122048365/stuff-stops-all-activity-on-facebook-in-trial-inspired-by-principle">July 2020</a>, major New Zealand media outlet Stuff made the decision to stop all activity on Facebook. They claim to have done this for ethical reasons, and had previously stopped advertising on Facebook in response to how Facebook handled the terror attack in Christchurch in 2019. They claim this action was a trial, and six months on, Stuff still doesn't post to Facebook, so things must not have gone too poorly. It is important to note a key difference in this situation though, and that is that stuff.co.nz links are still allowed on Facebook, just Stuff themselves doesn't post them. So they will still have a percentage of their traffic coming from Facebook, although …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://pancake.nz/blog/fbnewsban/">https://pancake.nz/blog/fbnewsban/</a></em></p>]]>
            </description>
            <link>https://pancake.nz/blog/fbnewsban/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26223310</guid>
            <pubDate>Mon, 22 Feb 2021 11:31:57 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Tutorial for secure OTA (over the air) firmware update on the ESP32]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 4 (<a href="https://news.ycombinator.com/item?id=26223293">thread link</a>) | @hamica
<br/>
February 22, 2021 | https://www.lab4iot.com/2021/02/21/esp32-secure-firmware-update-over-the-air-ota/ | <a href="https://web.archive.org/web/*/https://www.lab4iot.com/2021/02/21/esp32-secure-firmware-update-over-the-air-ota/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
			<article>
								
<div><figure><img loading="lazy" width="684" height="912" src="https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_684,h_912/https://www.lab4iot.com/wp-content/uploads/2021/02/firmware_auto_update_esp32.jpg" alt="" title="ESP32 with LCD after automatic update" srcset="https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_684/https://www.lab4iot.com/wp-content/uploads/2021/02/firmware_auto_update_esp32.jpg 684w, https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_225/https://www.lab4iot.com/wp-content/uploads/2021/02/firmware_auto_update_esp32-225x300.jpg 225w" sizes="(max-width: 684px) 100vw, 684px"><figcaption>ESP32 after the automatic firmware update has completed.</figcaption></figure></div>



<p>Once you deploy your IoT device, you won’t have physical access to reprogram or update it. It is critical to plan ahead and to have a secure mechanism for updating your embedded system or IoT device. Sometimes you will have the requirement to update your IoT device because of a new feature update, security issues, bugs, you did not have enough time to finish something on time and you had to ship your device and etc.</p>



<p>Since we want to rely on existing and working software platforms, in this tutorial I will use the NGINX as a web server, “encrypted” traffic by an <strong>SSL certificate from Let’s Encrypt</strong>. I will use my test domain on <em>lab4iot.site</em>, I will describe everything in a <strong>step-by-step</strong> manner so that anyone can replicate and follow it. First, I will describe how to set up your domain and web server, then we will proceed with the ESP32 code. <strong>I present you with a basic Let’s Encrypt setup, it is not a production-ready setup but rather something to start you of. Let’s call it a proof of concept. The core point of the article is on the ESP32 part, however, the webserver setup is required. Keep that in mind!</strong></p>



<p>Before you start, make sure your DNS records are properly set:</p>



<div><pre data-setting="{&quot;mode&quot;:&quot;shell&quot;,&quot;mime&quot;:&quot;text/x-sh&quot;,&quot;theme&quot;:&quot;monokai&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;showPanel&quot;:false,&quot;language&quot;:&quot;Shell&quot;,&quot;modeName&quot;:&quot;shell&quot;}">CNAME record: www to @
A record: * to your server IP address
A record: @ to your server IP address</pre></div>



<figure><img loading="lazy" width="1024" height="515" src="https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_1024,h_515/https://www.lab4iot.com/wp-content/uploads/2021/02/dns_records_setup-1024x515.jpg" alt="" srcset="https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_1024/https://www.lab4iot.com/wp-content/uploads/2021/02/dns_records_setup-1024x515.jpg 1024w, https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_300/https://www.lab4iot.com/wp-content/uploads/2021/02/dns_records_setup-300x151.jpg 300w, https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_768/https://www.lab4iot.com/wp-content/uploads/2021/02/dns_records_setup-768x386.jpg 768w, https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_1213/https://www.lab4iot.com/wp-content/uploads/2021/02/dns_records_setup.jpg 1213w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>My DNS records from my server </figcaption></figure>



<p>In this tutorial, I used Ubuntu Linux, feel free to use any other Linux distribution, maybe it will only differ in the used installation commands.</p>



<h2>Installing Nginx webserver and setting up the firewall</h2>



<p>First, make sure your Ubuntu is updated:</p>



<div><pre data-setting="{&quot;mode&quot;:&quot;shell&quot;,&quot;mime&quot;:&quot;text/x-sh&quot;,&quot;theme&quot;:&quot;monokai&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;language&quot;:&quot;Shell&quot;,&quot;modeName&quot;:&quot;shell&quot;}">sudo apt-get update
sudo apt-get upgrade</pre></div>



<p>If there are packages to be installed, press Y to install them.</p>



<p>Then install Nginx, the webserver I am going to use in this tutorial:</p>







<p>Once you have installed it, check that your website is set up properly by going to it, lab4iot.site (your domain in this case.) You should see the default Nginx site that comes with it when it is installed. You will note, we are not yet using HTTPS but rather the unencrypted old protocol HTTP.</p>



<figure><img loading="lazy" width="1024" height="213" src="https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_1024,h_213/https://www.lab4iot.com/wp-content/uploads/2021/02/webserver_running-1-1024x213.jpg" alt="" srcset="https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_1024/https://www.lab4iot.com/wp-content/uploads/2021/02/webserver_running-1-1024x213.jpg 1024w, https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_300/https://www.lab4iot.com/wp-content/uploads/2021/02/webserver_running-1-300x62.jpg 300w, https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_768/https://www.lab4iot.com/wp-content/uploads/2021/02/webserver_running-1-768x160.jpg 768w, https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_1536/https://www.lab4iot.com/wp-content/uploads/2021/02/webserver_running-1-1536x320.jpg 1536w, https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_1719/https://www.lab4iot.com/wp-content/uploads/2021/02/webserver_running-1.jpg 1719w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Nginx is running and your traffic is not yet encrypted.</figcaption></figure>



<p>Let us set up the firewall so that only <strong>HTTPS (encrypted) traffic</strong> is allowed to and from your server, aside from your SSH connection. We will use the <strong>Uncomplicated Firewall</strong>, UFW. It should come preinstalled on Ubuntu.</p>







<p>Your response should look as the following:</p>



<div><pre data-setting="{&quot;mode&quot;:&quot;shell&quot;,&quot;mime&quot;:&quot;text/x-sh&quot;,&quot;theme&quot;:&quot;monokai&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;showPanel&quot;:false,&quot;language&quot;:&quot;Shell&quot;,&quot;modeName&quot;:&quot;shell&quot;}">Available applications:
  Nginx Full
  Nginx HTTP
  Nginx HTTPS
  OpenSSH</pre></div>



<p>Then enable the following two options, OpenSSH and Nginx HTTPS.</p>



<div><pre data-setting="{&quot;mode&quot;:&quot;shell&quot;,&quot;mime&quot;:&quot;text/x-sh&quot;,&quot;theme&quot;:&quot;monokai&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;language&quot;:&quot;Shell&quot;,&quot;modeName&quot;:&quot;shell&quot;}">sudo ufw allow 'OpenSSH'
sudo ufw allow 'Nginx HTTPS'</pre></div>



<p>You should get some response like the following: </p>



<div><div>
<div><pre data-setting="{&quot;mode&quot;:&quot;shell&quot;,&quot;mime&quot;:&quot;text/x-sh&quot;,&quot;theme&quot;:&quot;monokai&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;showPanel&quot;:false,&quot;language&quot;:&quot;Shell&quot;,&quot;modeName&quot;:&quot;shell&quot;}">Rules updated
Rules updated (v6)</pre></div>



<p>Then, enable the firewall by typing.</p>







<p>You  will get the following response:</p>



<div><pre data-setting="{&quot;mode&quot;:&quot;shell&quot;,&quot;mime&quot;:&quot;text/x-sh&quot;,&quot;theme&quot;:&quot;monokai&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;showPanel&quot;:false,&quot;language&quot;:&quot;Shell&quot;,&quot;modeName&quot;:&quot;shell&quot;}">Command may disrupt existing ssh connections. Proceed with operation (y|n)? y
Firewall is active and enabled on system startup</pre></div>



<p>At this point, no incoming connection may be accepted aside from the one on HTTPS port 443 and SSH on port 22.<br>When you try to go to www.lab4iot.site (your domain), you will just see connecting but actually your browser cannot connect to it, yet.</p>



<p>To check your firewall status, this command may come up as handy:</p>







<p>Your Nginx site content is located in <strong><em>/var/www/html/</em></strong></p>
</div></div>



<p>Your Nginx configuration for your site, since it is the default one, is located in <em><strong>/etc/nginx/sites-available/default</strong></em></p>



<h2>Setting up the Let’s Encrypt SSL certificate</h2>



<p>We will modify the settings in this file soon. Let’s set up the HTTPS Let’s Encrypt SSL certificate, which is free of charge. <strong>For the purpose of this demo, I used a Let’s Encrypt certificate, which needs to be renewed every three months (thanks for the comment THEAMK). Instead, you could use OpenSSL to generate self-signed certificates that you don’t need to renew every three months and download on your device. You can google how it is done with openssl.</strong> </p>



<p>We need to install the Let’s encrypt <strong>certbot</strong> that will install the certificate for us:</p>



<div><pre data-setting="{&quot;mode&quot;:&quot;shell&quot;,&quot;mime&quot;:&quot;text/x-sh&quot;,&quot;theme&quot;:&quot;monokai&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;language&quot;:&quot;Shell&quot;,&quot;modeName&quot;:&quot;shell&quot;}">sudo apt install python3-certbot-nginx</pre></div>



<p>After you have installed it, to run it, execute the following commands, make sure to replace everything with your domain.</p>



<div><pre data-setting="{&quot;mode&quot;:&quot;shell&quot;,&quot;mime&quot;:&quot;text/x-sh&quot;,&quot;theme&quot;:&quot;monokai&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;language&quot;:&quot;Shell&quot;,&quot;modeName&quot;:&quot;shell&quot;}">sudo certbot --server https://acme-v02.api.letsencrypt.org/directory -d lab4iot.site -d *.lab4iot.site --manual --preferred-challenges dns-01 certonly</pre></div>



<p>It will ask you a couple of questions, answer them and the response should look like:</p>



<div><pre data-setting="{&quot;mode&quot;:&quot;shell&quot;,&quot;mime&quot;:&quot;text/x-sh&quot;,&quot;theme&quot;:&quot;monokai&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;language&quot;:&quot;Shell&quot;,&quot;modeName&quot;:&quot;shell&quot;}">/directory -d *.lab4iot.site --manual --preferred-challenges dns-01 certonly
Saving debug log to /var/log/letsencrypt/letsencrypt.log
Plugins selected: Authenticator manual, Installer None
Enter email address (used for urgent renewal and security notices) (Enter 'c' to
cancel): your_setup@email.com

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Please read the Terms of Service at
https://letsencrypt.org/documents/LE-SA-v1.2-November-15-2017.pdf. You must
agree in order to register with the ACME server at
https://acme-v02.api.letsencrypt.org/directory
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
(A)gree/(C)ancel: A

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Would you be willing to share your email address with the Electronic Frontier
Foundation, a founding partner of the Let's Encrypt project and the non-profit
organization that develops Certbot? We'd like to send you email about our work
encrypting the web, EFF news, campaigns, and ways to support digital freedom.
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
(Y)es/(N)o: N
Obtaining a new certificate
Performing the following challenges:
dns-01 challenge for lab4iot.site

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
NOTE: The IP of this machine will be publicly logged as having requested this
certificate. If you're running certbot in manual mode on a machine that is not
your server, please ensure you're okay with that.

Are you OK with your IP being logged?
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
(Y)es/(N)o: Y

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
Please deploy a DNS TXT record under the name
_acme-challenge.lab4iot.site with the following value:

2r2n6NQRRxzxF7L6kw1VFfP1MC8YdlRXGKGv8oM2ga8

Before continuing, verify the record is deployed.
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -</pre></div>



<p>Please add the <strong>TXT</strong> record in your DNS settings. </p>



<figure><img loading="lazy" width="1024" height="131" src="https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_1024,h_131/https://www.lab4iot.com/wp-content/uploads/2021/02/txt_dns_record_lets_encrypt-1024x131.jpg" alt="" srcset="https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_1024/https://www.lab4iot.com/wp-content/uploads/2021/02/txt_dns_record_lets_encrypt-1024x131.jpg 1024w, https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_300/https://www.lab4iot.com/wp-content/uploads/2021/02/txt_dns_record_lets_encrypt-300x38.jpg 300w, https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_768/https://www.lab4iot.com/wp-content/uploads/2021/02/txt_dns_record_lets_encrypt-768x98.jpg 768w, https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_1245/https://www.lab4iot.com/wp-content/uploads/2021/02/txt_dns_record_lets_encrypt.jpg 1245w" sizes="(max-width: 1024px) 100vw, 1024px"><figcaption>Added TXT DNS record.</figcaption></figure>



<p>Before you press enter, please check that the record matches the settings provided by Let’s encrypt by typing (but type this on your <strong>host machine, not server machine</strong>):</p>



<div><pre data-setting="{&quot;mode&quot;:&quot;shell&quot;,&quot;mime&quot;:&quot;text/x-sh&quot;,&quot;theme&quot;:&quot;monokai&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;language&quot;:&quot;Shell&quot;,&quot;modeName&quot;:&quot;shell&quot;}">dig _acme-challenge.lab4iot.site TXT</pre></div>



<p>You will get something as the following in the response:</p>



<div><div>
<div><div>
<div><div>
<div><pre data-setting="{&quot;mode&quot;:&quot;shell&quot;,&quot;mime&quot;:&quot;text/x-sh&quot;,&quot;theme&quot;:&quot;monokai&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;showPanel&quot;:false,&quot;language&quot;:&quot;Shell&quot;,&quot;modeName&quot;:&quot;shell&quot;}">; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.12-Ubuntu &lt;&lt;&gt;&gt; _acme-challenge.lab4iot.site TXT
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 50125
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 65494
;; QUESTION SECTION:
;_acme-challenge.lab4iot.site.	IN	TXT

;; ANSWER SECTION:
_acme-challenge.lab4iot.site. 34 IN	TXT	"2r2n6NQRRxzxF7L6kw1VFfP1MC8YdlRXGKGv8oM2ga8"

;; Query time: 0 msec
;; SERVER: 127.0.0.53#53(127.0.0.53)
;; WHEN: Sun Oct 04 16:52:14 CEST 2020
;; MSG SIZE  rcvd: 113</pre></div>



<p>Once you can see it, go to your server ssh terminal and press enter, you should get a message where it says things have been setup successfully:</p>
</div></div>



<div><pre data-setting="{&quot;mode&quot;:&quot;shell&quot;,&quot;mime&quot;:&quot;text/x-sh&quot;,&quot;theme&quot;:&quot;monokai&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;showPanel&quot;:false,&quot;language&quot;:&quot;Shell&quot;,&quot;modeName&quot;:&quot;shell&quot;}">Press Enter to Continue
Waiting for verification...
Cleaning up challenges

IMPORTANT NOTES:
 - Congratulations! Your certificate and chain have been saved at:
   /etc/letsencrypt/live/lab4iot.site/fullchain.pem
   Your key file has been saved at:
   /etc/letsencrypt/live/lab4iot.site/privkey.pem
   Your cert will expire on 2021-01-02. To obtain a new or tweaked
   version of this certificate in the future, simply run certbot
   again. To non-interactively renew *all* of your certificates, run
   "certbot renew"
 - If you like Certbot, please consider supporting our work by:

   Donating to ISRG / Let's Encrypt:   https://letsencrypt.org/donate
   Donating to EFF:                    https://eff.org/donate-le
</pre></div>



<h2>Setting up the Nginx server</h2>



<p>Now we can proceed to set up the nginx server. Go to the nginx settings folder:</p>



<div><pre data-setting="{&quot;mode&quot;:&quot;shell&quot;,&quot;mime&quot;:&quot;text/x-sh&quot;,&quot;theme&quot;:&quot;monokai&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;language&quot;:&quot;Shell&quot;,&quot;modeName&quot;:&quot;shell&quot;}">cd /etc/nginx/available-sites</pre></div>



<p>Edit the settings file:</p>
</div></div>







<p>Copy the following content over your existing <strong>“server”</strong> configuration:</p>



<div><pre data-setting="{&quot;mode&quot;:&quot;shell&quot;,&quot;mime&quot;:&quot;text/x-sh&quot;,&quot;theme&quot;:&quot;monokai&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;fileName&quot;:&quot;default&quot;,&quot;language&quot;:&quot;Shell&quot;,&quot;modeName&quot;:&quot;shell&quot;}">server {
        server_name lab4iot.site www.lab4iot.site *.lab4iot.site;

        listen 443 ssl;
        listen [::]:443 ssl;
        ssl_certificate /etc/letsencrypt/live/lab4iot.site/fullchain.pem;
        ssl_certificate_key /etc/letsencrypt/live/lab4iot.site/privkey.pem;

        root /var/www/html;

        # Add index.php to the list if you are using PHP
        index index.html index.htm index.nginx-debian.html;


        location / {
                # First attempt to serve request as file, then
                # as directory, then fall back to displaying a 404.
                try_files $uri $uri/ =404;
        }
}</pre></div>



<p>Make sure you replace lab4iot.site with your own domain name (check that certbot saved the certificates in the same path as it did for me.)<br>Save it. Then test the Nginx configuration by typing:</p>
</div></div>







<p>This command should tell you that everything in the settings file is OK:</p>



<div><div>
<div><pre data-setting="{&quot;mode&quot;:&quot;shell&quot;,&quot;mime&quot;:&quot;text/x-sh&quot;,&quot;theme&quot;:&quot;monokai&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;showPanel&quot;:false,&quot;language&quot;:&quot;Shell&quot;,&quot;modeName&quot;:&quot;shell&quot;}">nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful</pre></div>



<p>Then reload the Nginx server configuration:</p>



<div><pre data-setting="{&quot;mode&quot;:&quot;shell&quot;,&quot;mime&quot;:&quot;text/x-sh&quot;,&quot;theme&quot;:&quot;monokai&quot;,&quot;lineNumbers&quot;:true,&quot;styleActiveLine&quot;:false,&quot;lineWrapping&quot;:false,&quot;readOnly&quot;:true,&quot;language&quot;:&quot;Shell&quot;,&quot;modeName&quot;:&quot;shell&quot;}">sudo /etc/init.d/nginx reload</pre></div>



<p>You should be able to access your web site only by <strong>https://lab4iot.site</strong> as well as <strong>https://<em>www</em>.lab4iot.site</strong><br>Both links should produce a green keylock.</p>
</div></div>



<div><figure><img loading="lazy" src="https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_363,h_327/https://www.lab4iot.com/wp-content/uploads/2021/02/server_traffic_encrypted_https.jpg" alt="" width="363" height="327" srcset="https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_363/https://www.lab4iot.com/wp-content/uploads/2021/02/server_traffic_encrypted_https.jpg 363w, https://cdn.shortpixel.ai/client/q_lossless,ret_img,w_300/https://www.lab4iot.com/wp-content/uploads/2021/02/server_traffic_encrypted_https-300x270.jpg 300w" sizes="(max-width: 363px) 100vw, 363px"><figcaption>Web server traffic is encrypted now</figcaption></figure></div>



<p>Finally we are done with setting up the web server, …</p></article></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.lab4iot.com/2021/02/21/esp32-secure-firmware-update-over-the-air-ota/">https://www.lab4iot.com/2021/02/21/esp32-secure-firmware-update-over-the-air-ota/</a></em></p>]]>
            </description>
            <link>https://www.lab4iot.com/2021/02/21/esp32-secure-firmware-update-over-the-air-ota/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26223293</guid>
            <pubDate>Mon, 22 Feb 2021 11:28:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Invented, allowed, adopted. How new ideas become things in the world in 3 stages]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26223031">thread link</a>) | @amicoleo
<br/>
February 22, 2021 | https://www.orgonomyproductions.info/notes/notes/2021/02/13/InventedEnabledAdopted.html | <a href="https://web.archive.org/web/*/https://www.orgonomyproductions.info/notes/notes/2021/02/13/InventedEnabledAdopted.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        





<article itemscope="" itemtype="https://schema.org/BlogPosting">

  


  <div itemprop="articleBody">
    <p>What did a new idea have to go through before becoming a real thing and have a role in people lives? From scattered readings in recent and historical innovation examples, I picked up a pattern. To have an impact on the world, new ideas had to overcome three stages: they were first invented, then allowed, and finally adopted. Here are some examples of when they didn’t make it.</p>

<p><strong>Stage 1 - Invented</strong></p>

<p>An idea is invented when it moves from mind space to the real world. That means to consider the technical aspects necessary to realise the idea. A prototype is usually a solid sign that an idea was invented.</p>

<p>With that definition then, not invented ideas are the ones that remain only on paper. Reading Leonardo’s biography, it’s impressive to see how many interests he had. But I was even more surprised to learn how little Leonardo actually made. One good argument is that at his time, the beginning of 1500, there just wasn’t much incentive in technical entrepreneurship. What you could do with ideas was to pitch it to lords, and the things they cared about was war and prestige, and not making money with technological innovations.
Also, Leonardo was a perfectionist. He famously took years to make a painting and in all his life he painted only a few of them. No way we can expect him to have built an MVP of a flying machine.</p>

<p>More close to our times, past the Industrial Revolution, technical innovation in factories could actually make people rich. So what could kill ideas in this scenario, is that they could not be realisable. For instance, in the second half of the 1800s, Charles Babbage came up with the idea of the analytic machine, a precursor of a programmable computer, but never managed to build any of it. What would have been a fifteen-ton contraption, with over 25,000 mechanical parts, was just not ready to be created using the technology of the time.</p>

<p>For an overall picture of which ideas can be invented and which not, I like the notion of <em>adjacent possible</em>. The precondition for new ideas to be invented is that they can be made from the current technological and social conditions. No big leaps are allowed, at best an expansion of what it can be done already. But I don’t see this as a limit. As <a href="https://www.orgonomyproductions.info/notes/notes/2021/01/22/MunariWasACreativeTechnologist.html">I wrote before referring to Bruno Munari</a>, technology can be a powerful inspiration. And by getting hands-on with it as early as possible, you can make sure that a new idea can be actually produced.</p>

<p><strong>Stage 2 - Allowed</strong></p>

<p>When an idea successfully made into the realm of the possible, the first new obstacle is to be allowed. And that means that the organisation within which that invention was created enables or at least don’t stop the invention to get produced. (The exception are startups, but even then investors act as the gatekeepers with the power to enable or not a new business idea).</p>

<p>Before the industrial revolution, technical innovations were sometimes restricted by rulers. Especially when their application would affect employment. For instance in 1589 in England, Queen Elizabeth I refused a patent for a knitting machine, with the reason that would leave people without work. And a few years later, also in England, King Charles I banned the casting of buckets, to protect the craftsman that were making buckets in the traditional way.</p>

<p>But in that pre-industrial, and less economy-driven world, culture played a part too. My favourite example in the history of innovation is from Japan. After the Portugueses imported primitive weapons into the country in late 1500, Japan started improving and manufacturing their own design, and soon become a world leader in gun production. But traditionally, samurai wars involved ritualistic sword fights, and that was all ruined if guns were also used. So the samurai ruling class first restricted and finally banned their production in the whole country, and 200 years later in Japan there was almost no working guns left.</p>

<p>In recent years, this stage mostly takes place within companies. The most likely reason why Xerox failed to successfully bring to market the personal computer they invented, was that management didn’t believe enough in a product that would damage their main photocopying business. And at Kodak, they patented and built the first digital camera in 1975, and although did make money from the patent, the company never marketed the product until too late. Letting other company eventually kill their film monopoly, that they tried so hard to protect. Probably those lessons were learned. And on the other end of the spectrum, you have Amazon, which despite making the most money selling physical books, developed and in 2007 launched the Kindle e-Book reader.</p>

<p>Something can be said here on being too strict on user-centredness. Final customer as the only focus in the design work would maybe result in the best invention possible. But if that invention fails to be produced because it’s not allowed by its organisation, even the best user-centred invention is useless if people are not going to see it.</p>

<p><strong>Stage 3 - Adopted</strong></p>

<p>After an idea is invented and released into the world, the last challenge it faces is to be accepted and adopted by its final users. I don’t have historical examples of such failures, but there are some interesting recent ones to mention. The first one is of the Segway. First example of urban micro-mobility, it never took off among the public and in 2020 its production was stopped, while its heritage lives on in the rental scooter now available in many cities. Another example is of Google Glass. Impressive invention, a full backing by Google and a huge opposition from the public. For a brief period in 2013, it looked like the future of wearable augmenting technology was already among us, except that too many people could not stand to share bars and public places with enthusiastic early adopters “glass-holes”, going around with an all-recording Google camera on their face. The last example is One Laptop Per Child. MIT’s Nicholas Negroponte $100 solar-powered laptop. It was meant to bring computers to every child in the world. Estimated to sell between 5 to 15 million products, “only” 600K were sold since its launch. A too ambitious program with probably not enough consideration of the context where the computer was meant to be used.</p>

<p>Those adoption failures were due to many different causes, so it would be impossible to try explain them all with some simple reasons. In general, I believe that building iteratively, and integrating as much as possible feedback from the final adopters during development, is the best chance to make sure people will not reject a new idea when out there.  But is that what went wrong with the Segway and Google Glass? I wouldn’t know.</p>

<hr>

<p><strong>References</strong></p>

<p>Jared Diamond - Guns, Germs and Steel</p>

<p>Carl Benedikt Frey - The Technology Trap</p>

<p>Steve Johnson - Where Good Ideas Come From</p>

<p><a href="https://lens.blogs.nytimes.com/2015/08/12/kodaks-first-digital-moment/?_r=0#">Kodak’s First Digital Moment - The New York Times</a></p>

<p><a href="https://www.forbes.com/sites/tendayiviki/2017/07/01/as-xerox-parc-turns-forty-seven-the-lesson-learned-is-that-business-models-matter/">As Xerox PARC Turns 47, The  Lesson Learned Is That Business Models Matter</a></p>

<p><a href="https://www.theverge.com/2018/4/16/17233946/olpcs-100-laptop-education-where-is-it-now">OLPC’s $100 laptop was going to change the world — then it all went wrong - The Verge</a></p>


  </div>

  
</article>



      </div>


    </div></div>]]>
            </description>
            <link>https://www.orgonomyproductions.info/notes/notes/2021/02/13/InventedEnabledAdopted.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26223031</guid>
            <pubDate>Mon, 22 Feb 2021 10:51:20 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[I made an app to learn and look up VSCode's keyboard shortcuts]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26222865">thread link</a>) | @tkainrad
<br/>
February 22, 2021 | https://keycombiner.com/vscode/ | <a href="https://web.archive.org/web/*/https://keycombiner.com/vscode/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="contentdiv"><div id="contentblock"><div><div><div><p>KeyCombiner improves your VSCode workflows in several ways. Here are three concrete examples.</p></div></div><div><div><h2>1. Learn the Keyboard Shortcuts</h2><p>Increase your efficiency when working with VSCode by learning its keyboard shortcuts. KeyCombiner's typing trainer uses flashcard and spaced repetition techniques to facilitate keyboard shortcut learning. You can learn tens or even hundreds of keyboard shortcuts in minimal time. <br> We are not just saying that; read how <a href="https://tkainrad.dev/posts/how-i-learned-50-new-keyboard-shortcuts-in-42-minutes/" rel="noopener" target="_blank">How KeyCombiner's creator learned 50 new keyboard shortcuts in 42 minutes</a>.</p><p>Scroll down to try out the interactive trainer with some of VSCode's keyboard shortcuts right here on this page.</p></div></div><div><div><h2>2. Instantly Look up Shortcuts and Commands</h2><p><a href="https://keycombiner.com/desktop/">KeyCombiner Desktop</a> enables you to instantly look up all shortcuts that are in your collections <b>plus those of the currently active application</b>.</p><p>This means, whenever you are working in VSCode, you can press <kbd>Super</kbd>+<kbd>Alt</kbd>+<kbd>C</kbd> on Windows and Linux, or <kbd>Shift</kbd>+<kbd>Cmd</kbd>+<kbd>K</kbd> on macOS and KeyCombiner will show you VSCode's keyboard shortcuts. This way, you don't need to leave your current context, and can return to work immediately afterward.</p><p>If you are interested in how this works in detail, we have written a blog post about it:<br><a href="https://tkainrad.dev/posts/app-to-show-shortcuts-of-current-application-windows-linux-macos/" rel="noopener" target="_blank">An app to show the shortcuts of the current application for Windows, Linux, and macOS.</a></p></div></div><div><div><h2>3. Find the right VSCode Shortcuts for your Workflows</h2><p>KeyCombiner's collection tables can be searched, filtered, and sorted in more ways than you can imagine. In one click, you can filter by category or modifier combination, or for any key on the keyboard. Of course, there is also full text search.</p><p>We don't stop there though. KeyCombiner's collection visualizer maps all of VSCode's keyboard shortcuts onto a virtual keyboard. This helps to find relationships between key bindings, or to identify free combinations, in case you like to organize your own key bindings.</p><p>By the way, we have a blog post about that, too: <a href="https://tkainrad.dev/posts/visualize-collections-of-keyboard-shortcuts/" rel="noopener" target="_blank">An Interactive Virtual Keyboard to Visualize any Collection of Shortcuts</a></p></div></div><hr><div><div><h2>1. Select Shortcuts from KeyCombiner's public&nbsp;<img alt="VSCode logo" src="https://keycombiner.com/media/application-icons/vscode_QpYl3Cj.png">VSCode collection</h2></div></div><div><div><p>KeyCombiner's <a href="https://keycombiner.com/collections/vscode/">public VSCode collection</a> has 161 entries. Select exactly those that you need for your workflow and add them to your personal collections.</p><p>KeyCombiner will always show you which combinations of a public collection are already in your collections, so you don't lose track when gradually expanding your knowledge.</p><p>All shortcuts in your personal collections will always be available in <a href="https://keycombiner.com/desktop/">KeyCombiner Desktop</a>'s instant lookup. Next time you are trying to remember a VSCode shortcut, you don't need to suffer a context switch by searching on the Web.</p></div><p><img alt="Training Statistics" src="https://keycombiner.com/static/images/collecting.43113ea73660.gif"></p></div><div><h2>2. Go to your KeyCombiner Dashboard and click <i>Practice</i></h2><p>KeyCombiner's interactive trainer can be used with any of your collections. It comes with a flashcard inspired learning experience. Spaced repetition algorithms maximize learning efficiency. You can try out KeyCombiner's interactive trainer with a random selection of 20 VSCode shortcuts right here.</p><p>If you create an account, you can choose exactly which shortcuts you want to practice. You can combine your VSCode practice with 70+ other applications for which KeyCombiner has <a href="https://keycombiner.com/collections/">a public collection</a> or define new key combinations from scratch. KeyCombiner will save detailed statistics and use them to speed up your learning progress.</p></div><hr><div><div><p>Browsing the official documentation or printing out a list of keyboard shortcuts is great, but it does not scale well when you are searching for a specific entry among hundreds of key bindings.</p><p>KeyCombiner's collection tables offer a range of features that you won't find anywhere else.</p><ul><li>Collection visualizer that maps the 161 VSCode keyboard shortcuts onto a virtual keyboard.</li><li>Show key bindings for Windows, Linux, macOS individually, or side-by-side.</li><li>Full text search on the entire data, or limited to a description, keys, or category.</li><li>One-click filtering by category or combination of modifiers.</li><li>Click on a virtual keyboard button to filter the collection table for all combinations containing the respective key.</li><li>Sort by category, keys, description, or combination of modifiers.</li><li>Instantly pull up the current application's shortcuts, and the entirety of your personal collections with KeyCombiner Desktop's instant lookup.</li><li>Export VSCode's keyboard shortcuts to PDF, CSV, and XLSX.</li></ul></div></div></div></div></div></div>]]>
            </description>
            <link>https://keycombiner.com/vscode/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26222865</guid>
            <pubDate>Mon, 22 Feb 2021 10:26:31 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[web3 is a Stupid Idea]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 1 (<a href="https://news.ycombinator.com/item?id=26222849">thread link</a>) | @Bluestein
<br/>
February 22, 2021 | https://timdaub.github.io/2020/09/08/web3/ | <a href="https://web.archive.org/web/*/https://timdaub.github.io/2020/09/08/web3/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>A long time ago, I gave a talk in front of a full audience talking about BigchainDB, a company I worked for to create a (scalable) decentralized database. As we just had released our browser-compatible JavaScript driver, enthusiastically, I told the audience: "... and so by using our driver from within the browser, your app won't need a backend anymore"!</p>
<p>That must have been around 2017 when I first discovered Metamask, and started drinking Ethereum's web3 cool-aid. Arguably, web3 quickly became something extraordinary. All of a sudden, users could download a browser extension and directly interact with a public network. In a sense, it still is extraordinary.</p>
<p>If you have an extension like Metamask installed in your browser today, you can visit sites on the web that allow you to do the craziest things with your digital money. An excellent recent example of this are DeFi (short for "Decentralized Finance") websites. They allow a user to engage in trading cryptocurrencies, providing liquidity, and peer to peer lending. With the click of a button and no mandatory signups, you're able to pool thousands of dollars. That is super cool and confirms the viability of the web3 vision.</p>
<p>But actually, what is the web3 vision? It may be that there never was such a thing in the first place. All I know is that someone named a library "web3.js". Developers use it to talk to remote or local Ethereum nodes when working in a browser environment (JavaScript).</p>
<p>On a web3-enabled website, when a user now clicks a button to, e.g., pool ether in a smart contract, most calculations are supported by the web3.js library that periodically talks to an Ethereum node. Ultimately web3.js allows the user to send the transaction to the node to transfer the user's money.</p>
<p>Often, a key-management program, like Metamask, is running on the user's browser. It allows the user to sign transactions with the same key on different websites.</p>
<p>In a nutshell, that's web3. It's supposed to be a play on words regarding "web 2.0". Web 2.0 is the upgrade of web standards that gave us modern single-page applications and dynamic AJAX loading. And Web3? An advancement towards what exactly? Money websites?</p>
<p>Indeed, if you were capable of cleaning your mind of specific memories, specifically, let's say you could do <code>grep -l web3 brain | xargs rm</code>. And then someone asked you how you'd envision a blockchain-based and smart-contract-enabled web3; you'd likely describe an ecosystem vastly different to what it is today. You'd think about peer-2-peer networks, light clients, and renewed web standards. That's precisely not web3.</p>
<p>In today's experience it will instead be mostly shitty react websites that crash or stop working when you've neglected to install Metamask (or other key-management plugins). Opening a web3 website's network console, you'll see that it's making an excessive amount of RPC request to an Ethereum full node. Sorry, I meant to say Infura node, a hugely-popular cloud provider hosting Ethereum full nodes. That's kinda stupid.</p>
<p>And since Metamask allows developers to prompt the user for specific contract calls, what's even more stupid, is that all your money may be at the risk of continually getting stolen with the accidential click of a button. Either by someone hacking the website's server. Or by the website provider becoming corrupt themselves. Or simply because a website pretends to do X when it does Y (stealing all your money).</p>
<p>But instead of continuing to rant, I'd now like to now point out what I think should change about web3:</p>
<ul>
<li>We should stop building key-management plugins and start thinking about a standardizable web API. We must stop training our users to install shitty browser plugins!</li>
<li>We need to make light clients work as soon as possible and become independent from third-party services like thegraph and Infura.</li>
<li>We need to improve our client libraries (ethers.js and web3.js) by dramatically simplifying them and making them bug-free (god damn it!)!</li>
<li>We need to take advantage of some of the blockchain's fundamental properties. Most data is immutable so let's start caching things.</li>
</ul>
<p>And finally, I think we should stop focusing all of our attention on bumping the web's version number. Maybe we should reconsider writing more backends. We should promote more work on permissionless networks like Open Gas Station Network that allow developers to upgrade a user's experience. And, we should start thinking of a machine network of blockchains more often. In many ways, web3 was just a cool demo. But let's come up with something better. Just imagine what happens once there's a deeper integration of money into computer systems!</p>

  </div></div>]]>
            </description>
            <link>https://timdaub.github.io/2020/09/08/web3/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26222849</guid>
            <pubDate>Mon, 22 Feb 2021 10:24:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Philosophical Roots of Sweden’s Pandemic Strategy]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 1 (<a href="https://news.ycombinator.com/item?id=26222756">thread link</a>) | @imartin2k
<br/>
February 22, 2021 | https://macrinamagazine.com/issue-6-general/guest/2021/02/20/the-justice-of-gold-the-philosophical-roots-of-swedens-pandemic-strategy/ | <a href="https://web.archive.org/web/*/https://macrinamagazine.com/issue-6-general/guest/2021/02/20/the-justice-of-gold-the-philosophical-roots-of-swedens-pandemic-strategy/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-id="6d6ca2c" data-element_type="widget" data-widget_type="theme-post-content.default">
				<div>
			<p>The year is 1911. The setting, the Swedish university of Uppsala’s <em>aula magna, </em>bursting with listeners. The occasion is the installation of the Swedish philosopher Axel Hägerström as chair professor of practical philosophy.</p>
<p>As part of the ceremony Hägerström was to hold a public lecture. Not long into the address, Hägerström’s explosive philosophy managed to spark a riot in the lecture-hall: vegetables were hurled, doors slammed. At the end of the lecture, the archbishop of the Swedish Lutheran church, the Reverend Nathan Söderblom, stood up and asked how anyone who harbored such views could ever behave decently towards his wife and kids.</p>
<p>Whether or not this is an accurate portrayal of the event, this is the form the story was handed down to me by one of my professors at Uppsala University, a proud inheritor of the tradition of Hägerström. The anecdote was delivered with an air of disbelief over the naiveté of the archbishop’s question. How could anyone believe that such abstract and abstruse doctrines on the semantics of ethical sentences could have any concrete repercussions on practical issues like how one treats one’s family?</p>
<p>More than a century after Hägerström’s lecture, Sweden’s response in the global COVID-19 pandemic has cast long shadows, drawing attention from all over the globe. Unlike the vast majority of other countries, Sweden has taken scant measures to hinder the spread of the virus. No lockdown. No closure of schools or public means of transport. No obligatory face masks. The infection was left to spread as quickly and widely as possible, in an attempt to achieve “herd immunity” among the population. As a result of this, Sweden is among the countries with the highest death toll per capita, more than twenty times as much as other comparable Nordic countries, all of which have taken considerably stronger measures.</p>
<p>At bottom, the Swedish strategy rests on a set of ethical positions that the government has chosen to adopt. These positions, in turn, look surprisingly like those staked out in Hägerström’s 1911 lecture. There is a clear line of influence running from the marmoreal halls of the university’s <em>aula </em>to the halls of power today. Although the materialism of our present society may suggest otherwise, ideas leave deep marks – a good example being precisely the materialism of our present society.</p>
<p>The title of the Hägerström’s historic 1911 lecture was “On the Truth of Ethical Utterances.” In it, the philosopher laid out something of a manifesto for the ethical position known as ethical non-cognitivism. He argued that any sentence expressing an ethical stance – any statement about what is good and what is bad – is necessarily nonsense. Asking whether an action is right or wrong, Hägerström assures us, is analogous to wondering about the amount of “justice” contained in a bar of gold, or how heavy a color happens to be. Ethical opinions are simply the reflection of personal emotions, and as such cannot be subject to either truth or falsehood.</p>
<p><em>Pace</em> my professor, Hägerström did, in fact, see his non-cognitivism as the cornerstone for a radical programme for political change. In “On the Truth of Ethical Utterances,” he describes his vision of a new ethics spawning to life from the ashes of the old one. This phoenix-like morality, having shed the fetters of metaphysics and superstition, would thereafter follow a single ethical lodestar: <em>functionality</em>. The effect of this approach was a practical utilitarianism: the goal of ethics in practice became to maximize the amount of utility for the greatest possible number. The scientific veneer of utilitarian ethics made it the perfect candidate for Hägerström’s post-metaphysical ethics of functionality. While it is nonsense to measure the “justice” of a gold ingot, it certainly makes a lot of sense to ask how much “utility” that gold can buy. Functionality is measurable; measurability is functional. Hägerström thus embodies the widely celebrated Swedish value of functionality. Indeed, it is no hyperbole to say that functionality is the country’s most cherished value. It also happens, slightly more worryingly, to be the only one.</p>
<p>The shockwaves of influence of Hägerström’s double-barreled approach of theoretical non-cognitivism coupled with a practical utilitarianism are too insidious to fully map. The ideas wormed their way into the official ideology of the Social-Democratic Party, which held an iron grip on Swedish politics for the better part of a century. Several of Hägerström’s students clambered up the party hierarchy and served long terms as ministers. The minister and economist Gunnar Myrdal, a disciple of Hägerström, described his master’s influence as ripples in water, expanding indefinitely until nobody was left unaffected. Hägerström’s students also became the architects behind Sweden’s “social engineering” programmes in the 1950s – government initiatives to streamline the population in order to increase its utility and functionality. “Social engineering” – or the project of shaping the “human-material”, as it was called – was (naturally) linked to comprehensive eugenic programmes. The Swedish pandemic strategy of achieving “herd immunity” (weeding out weaker individuals for the sake of “herd’s” utility) has a venerable history.</p>
<p>With this historical background in mind the reasoning behind the Swedish strategy becomes clearer, and, if anything, more appalling. At bottom, it is founded on a utilitarian calculus. Swedish authorities made no secret of the evaluation that needed to take place: one had to choose between the economy and the elderly, the unfettered functioning of society versus the health of its citizens – either justice or gold. The outcome of the calculations was clear: Sweden would opt for the alternative that maximizes utility across the board, even if in the process – as the euphemism goes – some eggs would need to be broken.</p>
<p>No wonder, then, that utilitarian philosophers came out in force in defense of the Swedish strategy. As the Swedish ethicist Olle Torpman bluntly wrote back in April: “Can we really put a price tag on people’s lives? Can we really compare somebody’s death with another person’s happiness or lack thereof? The answer is: yes.”<sup><a href="#footnote_0_2556" id="identifier_0_2556" title="Olle Torpman, “Moralfilosofin som ger Sverige rätt” (“The Ethical Philosophy that Supports the Swedish Strategy”), Kvartal.">1</a></sup></p>
<p>Likewise, the internationally acclaimed utilitarian ethicist Torbjörn Tännsjö publicly defended the “Swedish strategy” precisely on the grounds of its palpably utilitarian texture. As he said in an interview: “It sounds as if the government is prepared to sacrifice a number of individuals – at any rate in the short term – to save as many human lives as possible on the whole, partly by indirectly saving the economy.”<sup><a href="#footnote_1_2556" id="identifier_1_2556" title="Åke Gavfelin and Lapo Lappin, “Interview with Torbjörn Tännsjö”, Metafysiskalaboratoriet.">2</a></sup> There is a more than a hint of triumphalism in Tännsjö’s defense: he cannot help but note that ethical boards across the country are spangled with his former doctoral students, who, he claims, do their best to dress up their utilitarianism enough to get away with it, while following it religiously in practice.<sup><a href="#footnote_2_2556" id="identifier_2_2556" title="Ibid.">3</a></sup></p>
<p>With all due respect to Tännsjö, if the utilitarianism is meant to be covert, his students are the least subtle players of hide-and-seek in the history of philosophy. Only a cursory glance at the ethical reports drawn up under the pandemic betrays an explicit utilitarianism. In a report on the Swedish approach, the Ethical Board of State laid out the ethical foundations to defend the strategy. This document follows through a rigorous utilitarian calculation, tallying up the greatest possible well-being for the greatest possible number. As the board writes in one official document: “To address the question [of which strategy should be chosen] we need to focus on the possible and relevant <em>consequences</em>.”<sup><a href="#footnote_3_2556" id="identifier_3_2556" title="The Swedish National Council on Medical Ethics, Etiska vägval i pandemin, 44.">4</a></sup> From the very outset, the question is formulated within a consequentialist ethical framework. The board goes on to list which such “relevant” consequences to be weighed against each other: they begin by noticing that one of these is the loss of lives, but are quick to dilute it with a much longer list, including social and psychological factors, proximate and remote economic factors, freedom, feelings of alienation. The cost of the state intervening to save lives, they suggest in one passage, must be weighed against the cost of the “support” for the government ebbing among the population.<sup><a href="#footnote_4_2556" id="identifier_4_2556" title="SMER, Etiska vägval i pandemin, 43-44.">5</a></sup></p>
<p>This calculation is, after all, perfectly in line with the pronouncements of the Ministry of Public Health; the strategy was repeatedly justified on the grounds that it allowed things to run smoothly: it was “sustainable” in the long run, as “effective” as possible.</p>
<p>A century after the Lutheran archbishop’s question to Hägerström, we are perhaps ready to suggest an answer. Whether or not we think a non-cognitivist and utilitarian father can be a decent father, it is certainly the case that a non-cognitivist and utilitarian state cannot be a decent state. We have yet to see any form of genuine remorse over the shedding of lives from those in positions of power. We may have a long wait ahead. After all, there can be no remorse for something one believes is entirely justified, even mandated, by an objective standard.</p>
<p>Perhaps the bottom line is that the cynicism of the Swedish strategy ought to raise as few eyebrows abroad as it does here in Sweden. In a society where the only value is utility, where vulnerable groups are expendable as long as the pay-off is high enough, where the values of human dignity and the holiness of life are regarded as metaphysical mumbo-jumbo, where gold will always trump justice, really – what else could one expect?<a href="#_ftnref1" name="_ftn1"></a></p>

<p><em>[Photo Attribution: Joakim Emanuelson, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons]</em></p>
<ol><li id="footnote_0_2556">Olle Torpman, “Moralfilosofin som ger Sverige rätt” (“The Ethical Philosophy that Supports the Swedish Strategy”), <em>Kvartal</em>.<span>[<a href="#identifier_0_2556">↩</a>]</span></li><li id="footnote_1_2556">Åke Gavfelin and Lapo Lappin, “Interview with Torbjörn Tännsjö”, Metafysiskalaboratoriet.<span>[<a href="#identifier_1_2556">↩</a>]</span></li><li id="footnote_2_2556">Ibid.<span>[<a href="#identifier_2_2556">↩</a>]</span></li><li id="footnote_3_2556">The Swedish National Council on Medical Ethics, <em>Etiska vägval i pandemin</em>, 44.<span>[<a href="#identifier_3_2556">↩</a>]</span></li><li id="footnote_4_2556">SMER, <em>Etiska…</em></li></ol></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://macrinamagazine.com/issue-6-general/guest/2021/02/20/the-justice-of-gold-the-philosophical-roots-of-swedens-pandemic-strategy/">https://macrinamagazine.com/issue-6-general/guest/2021/02/20/the-justice-of-gold-the-philosophical-roots-of-swedens-pandemic-strategy/</a></em></p>]]>
            </description>
            <link>https://macrinamagazine.com/issue-6-general/guest/2021/02/20/the-justice-of-gold-the-philosophical-roots-of-swedens-pandemic-strategy/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26222756</guid>
            <pubDate>Mon, 22 Feb 2021 10:10:18 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Ethereum Isn't Fun Anymore]]>
            </title>
            <description>
<![CDATA[
Score 285 | Comments 592 (<a href="https://news.ycombinator.com/item?id=26222709">thread link</a>) | @timdaub
<br/>
February 22, 2021 | https://timdaub.github.io/2021/02/22/ethereum-isnt-fun-anymore/ | <a href="https://web.archive.org/web/*/https://timdaub.github.io/2021/02/22/ethereum-isnt-fun-anymore/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p><strong>Ethereum isn't fun anymore. There I said it.</strong> And although, the last time I developed an app using it has been more than a year ago, I stand by my word. <strong>Developing dapps on Ethereum has become annoying</strong>. Here's why:</p>
<p><img src="https://timdaub.github.io/assets/images/pedro.gif"></p>
<h2 id="i-used-ethereum-before-it-was-cool.">I used Ethereum before it was cool.</h2>
<p>I know; it's such a hipster statement. But it's true. Ethereum has stopped being edgy. It has transitioned out of its niche to build a world computer. Its community has become huge, and I stopped knowing most faces. Where there was a feeling of revolution and new beginnings, now there are people in suits talking corporate. Within just a few years, it went from <a target="_blank" rel="noopener" href="https://github.com/DaddyDeFi/DaiDaddy">DAI daddys</a> and only <a target="_blank" rel="noopener" href="https://www.molochdao.com/">half-ironic satanist cults</a> to lending, insurance, and trade protocols.</p>
<p>"What's Ethereum's killer app?" we asked ourselves not long ago. Now we know. It's the world's best publicly-accessible settlement platform for financial transactions. In a way, that's exciting. The markets think so too. But for anyone else that worked with Ethereum but outside of financial applications, it's somewhat of a letdown.</p>
<p><img src="https://timdaub.github.io/assets/images/homer.gif"></p>
<h2 id="how-do-you-do-fellow-ethereans">How do you do, fellow Ethereans?</h2>
<p>I think it must have been around the time of the last big crypto bubble when Monero enthusiasts called for "Making Monero cheap again."</p>
<p>Monero, being the anonymous digital currency that had indeed just legit use cases apart from the occasional rumors that entangled it in drug trafficking, had suddenly become too expensive for everyday use. Realizing the glaring threat of becoming too valuable, its core developers went on to fix the problem by campaigning at CoinDesk's yearly industry gathering Consensus.</p>
<p>They announced the "Monero Enterprise Alliance." An inside joke, supposed to piss off other projects that had started to take themselves too seriously. Being slightly confused that day myself, I now can't recall if the effort had ever been successful. But in any case, I can't recommend buying Monero. It's useless.</p>
<h2 id="gas-prices-are-too-damn-high">Gas prices are too damn high!</h2>
<p><img src="https://timdaub.github.io/assets/images/deepfried_high_rents.jpg"></p>
<p>There was a phase in my short career as an Ethereum developer where I looked at Etherscan's "<a target="_blank" rel="noopener" href="https://etherscan.io/contractsVerified">Verified Contracts</a>" page all day long to find vulnerabilities in newly uploaded contracts. "My name's Tim and I'm an etherholic!"</p>
<p>It was addictive. I ended up calling a few of those contracts, failing to cause any havoc, sadly. But it was so much fun! Back when transaction fees were still affordable on Ethereum, building projects was great. We started up Ganache and our favorite text editor (vim). The only choice we had was Solidity. And off we went.</p>
<p>Now, building Ethereum applications has become painful. <a href="https://timdaub.github.io/2020/09/08/web3/">web3 is a stupid idea</a>. Layer 2 isn't ready. Neither is Eth 2.0. And there are still <a href="https://timdaub.github.io/2019/02/28/poa/">many reasons to NOT ship to a Proof of Authority network</a>. Finally, gas prices are too damn high!</p>
<p>How do we move on from here?</p>
<h2 id="i-need-a-hero.">I need a hero.</h2>
<p><strong>I need a hero, and by that, I mean that I need a usable methodology for building scaleable decentralized apps.</strong> Yes, you've heard that right. We don't need more "Ethereum killers" that can do 10x more tx/s than Ethereum. Those are useless.</p>
<p>Instead, we need an approach for the average Joe developer to create their idea within the Ethereum ecosystem without the need for hardcore unproven technologies. I know, you Vitalik will say: "Oh, it's not a problem, we can has '<a target="_blank" rel="noopener" href="https://vitalik.ca/general/2020/03/21/garbled.html">Garbled Circuits</a>' and zkrollups." But I'm telling you that no sane Joe will touch that shit without a serious cryptographic specialist by their side.</p>
<p>We want what we stayed for initially: Good ol smart contracts. But right now, they're too damn expensive to innovate.</p>

  </div></div>]]>
            </description>
            <link>https://timdaub.github.io/2021/02/22/ethereum-isnt-fun-anymore/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26222709</guid>
            <pubDate>Mon, 22 Feb 2021 10:05:08 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Why I sold my Raspberry Pi 4 for a Rock Pi 4]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26222704">thread link</a>) | @voxadam
<br/>
February 22, 2021 | https://ikarus.sg/why-i-sold-my-raspberry-pi-4-for-a-rock-pi-4/ | <a href="https://web.archive.org/web/*/https://ikarus.sg/why-i-sold-my-raspberry-pi-4-for-a-rock-pi-4/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="site-main">
    <div>

        <article>

            

            <figure>
                <img srcset="https://ikarus.sg/content/images/size/w300/2021/02/rpi-rockpi.png 300w,
                            https://ikarus.sg/content/images/size/w600/2021/02/rpi-rockpi.png 600w,
                            https://ikarus.sg/content/images/size/w1000/2021/02/rpi-rockpi.png 1000w,
                            https://ikarus.sg/content/images/size/w2000/2021/02/rpi-rockpi.png 2000w" sizes="(max-width: 800px) 400px,
                        (max-width: 1170px) 1170px,
                            2000px" src="https://ikarus.sg/content/images/size/w2000/2021/02/rpi-rockpi.png" alt="Why I sold my Raspberry Pi 4 for a Rock Pi 4">
            </figure>

            <section>
                <div>
                    <p>I sold my Raspberry Pi 4B 4GB recently and replaced it with a Rock Pi 4A. I had owned it for about 6 months before finally letting it go at a significant loss and honestly, that decision was not as hard as I thought. On this piece, I document my anticipations, disappointments, and epiphanies over the course of 6 months owning the Raspberry Pi 4B.</p><blockquote>Disclaimer: I'm not affiliated with nor sponsored by Radxa/Allnet, the manufacturers of Rock Pi. I'm writing this piece purely to share my experiences with both the Raspberry Pi 4B 4GB and the Rock Pi 4A 4GB.</blockquote><h2 id="i-need-more-memory-">I. Need. More. Memory.</h2><p>It was February 2020, I was running my self-hosted apps all on the <a href="https://ikarus.sg/how-i-built-kraken/">Kraken</a> cluster then. At that point in time I wanted to run a metric-monitoring stack (<em>Prometheus</em> + <em>Grafana</em>) for the cluster but after reviewing the memory requirements, I quickly realized that not even all the memory on a single Raspberry Pi 3B node was enough (although I'm cognizant that 1GB isn't much in the grander scheme of things). </p><p>Browsing around, the most obvious choice was the next model in the Raspberry Pi line that was just released a few weeks earlier then, the Raspberry Pi 4B 4GB.</p><h2 id="the-shiny-raspberry-pi-4b">The Shiny Raspberry Pi 4B</h2><!--kg-card-begin: markdown--><table>
<thead>
<tr>
<th>Aspect</th>
<th>Raspberry Pi 3B</th>
<th>Raspberry Pi 4B 4GB</th>
</tr>
</thead>
<tbody>
<tr>
<td>CPU</td>
<td>Broadcom BCM2837 (Quad core)</td>
<td>Broadcom BCM2711 (Quad core)</td>
</tr>
<tr>
<td>Cores</td>
<td>4x Cortex-A53 1.2GHz</td>
<td>4x Cortex-A72 @ 1.5GHz</td>
</tr>
<tr>
<td>Memory</td>
<td>1GB LPDDR2</td>
<td>4GB LPDDR4-3200</td>
</tr>
<tr>
<td>Ethernet</td>
<td>100Mbps</td>
<td>1000Mbps</td>
</tr>
<tr>
<td>Storage</td>
<td>Micro-SD Card</td>
<td>Micro-SD Card</td>
</tr>
<tr>
<td>USB</td>
<td>4x USB2.0</td>
<td>- 2x USB 3.0<br>- 2x USB 2.0</td>
</tr>
<tr>
<td>Power</td>
<td>5V 2A Micro-USB</td>
<td>5V via USB-C connector (&gt;=3A)</td>
</tr>
</tbody>
</table>
<!--kg-card-end: markdown--><p>There were quite a few aspects about the Raspberry Pi 4B 4GB that got me excited: 4GB Memory, Gigabit Ethernet, yada yada. But what I anticipated the most was the long-overdue elimination of the <u>painful</u> <a href="https://ikarus.sg/how-i-built-kraken/#some-caveats">USB 2.0 bus bottleneck</a> that plagued all Raspberry Pi models from Zero to 3B+. </p><p>In the Raspberry Pi 4, the ethernet is built directly into the SoC and no longer shares a bottlenecked bus with USB devices. This means we can unleash the full potential of the gigabit ethernet port without worrying about performance degradation of the micro-SD card or external drives, and vice-versa. </p><p>On top of all those changes, the Raspberry Pi 4B doubled the micro-SD card slot bandwidth from 20MB/s to 40MB/s. While this is still not cutting-edge performance, it's still a significant and welcome improvement, especially for tasks that have high demands for sequential I/O.</p><blockquote>Delay no more!</blockquote><p>With those specs, I made a trip down to Amicus @ Sim Lim Tower to snag one from the shelves.</p><h2 id="what-belies-the-shine">What Belies the Shine</h2><p>At the point of purchase and over 6 months of usage, I've uncovered many issues that really made me second-guess my purchase. </p><p>These are the pain points I've identified, each of which I will cover in detail in dedicated sections:</p><ol><li>Power concerns</li><li>Heat dissipation issues</li><li>No official cooling solutions</li><li>32-bit Operating System</li><li>Operating Systems Available and Compatibility with k8s</li><li>Storage performance</li><li>Storage longevity</li></ol><h3 id="power-concerns">Power Concerns</h3><p>Interestingly, a red flag had already surfaced prior to purchase, but I conveniently ignored it in favor of my excitement; the power supply. There were many complaints online that <a href="https://hackaday.com/2019/07/16/exploring-the-raspberry-pi-4-usb-c-issue-in-depth/">electronically-marked USB-C cables did not work</a> due to a flaw with the Raspberry Pi 4 hardware design that caused it to detect the charging cable as an audio accessory. </p><figure><img src="https://ikarus.sg/content/images/2021/02/image.png" alt=""><figcaption>Electronic market in USB-C cables (<a href="https://www.elinfor.com/market/how-to-identify-the-usb-c-cables-with-or-without-e-maker-m-27">Source</a>)</figcaption></figure><p>For context, <em>standards-compliant</em> USB-C cables that support <em>more than 3.0A </em>(Thanks <a href="https://www.reddit.com/user/ferrybig/">ferrybig</a> from <a href="https://www.reddit.com/r/selfhosted/comments/loykcd/why_i_sold_my_raspberry_pi_4_for_a_rock_pi_4_at/">/r/selfhosted</a> for correcting me with the <a href="https://www.usb.org/sites/default/files/USB%20Type-C%20Spec%20R2.0%20-%20August%202019.pdf#%5B%7B%22num%22%3A82%2C%22gen%22%3A0%7D%2C%7B%22name%22%3A%22XYZ%22%7D%2C87%2C421%2C0%5D">USB C spec</a>) current have a tiny microchip embedded within it that enables smart features such as voltage and USB protocol negotiation which provides information on supported voltages and data transfer rates to the other end. The microchip also doubles as protection for your device from over-voltage and other electrical risks. We have seen from the sacrifices of Benson Leung, full-time Google engineer and part-time USB-C vigilante, going around on Amazon testing USB-C cables and <a href="https://www.slashgear.com/beware-usb-c-cables-that-could-seriously-fry-your-device-03425324/">frying his Chromebook Pixel in the process</a>, what kind of damage non-compliant cables can potentially do to your devices.</p><figure><a href="https://www.engadget.com/2016-02-03-benson-leung-chromebook-pixel-usb-type-c-test.html"><div><p>Google engineer fries Pixel testing USB Type-C cable | Engadget</p><p>You might not remember Benson Leung, the Google engineer that tasked himself with examining USB Type-C cables. He’s been diligently doing so for months, but he’s calling his tests to a halt after one went horribly wrong. Leung bought a USB 3.1 Type-C SuperSpeed cable (it’s since been removed) from S…</p><p><img src="https://s.yimg.com/kw/assets/favicon-160x160.png"></p></div><p><img src="https://s.yimg.com/uu/api/res/1.2/.1w_EgFevrhtuAoC8AytLw--~B/aD05NDI7dz0xNDAwO2FwcGlkPXl0YWNoeW9u/https://s.yimg.com/uu/api/res/1.2/0I.gp7wz7N1ofOUfCVBLwQ--~B/aD05NDI7dz0xNDAwO2FwcGlkPXl0YWNoeW9u/https://o.aolcdn.com/hss/storage/midas/9bdc1aa766dc7bc1bc964f8a9f843dc8/203351772/chromebookpixelport.jpg.cf.jpg"></p></a></figure><p>I find it rather ridiculous that in order to power the Raspberry Pi 4B, one had to source for cables that did not have the embedded microchip and hence potentially unsafe.</p><p>Other than that issue, I also had concerns that my existing power supply, the <em>Anker PowerPort 10</em>, would not be sufficient for the Raspberry Pi 4, given that it only supports up to <em>5V 2.0A</em>. The official page states that the Raspberry Pi 4B needs <em>at least 5V 3.0A</em> to work, which means I'd need to get something like a <em>Qualcomm QuickCharge 3.0</em> brick specifically for it. </p><p>In the end, out of an abundance of caution I bought the <em>extortionately priced</em> official Raspberry Pi 4B power supply at <strong>S$18 (US$13.40)</strong>. To put things into perspective, that power brick costed<strong> 20.93% of a Raspberry Pi 4B 4GB</strong> priced at <strong>S$84.99 (US$64.03)</strong>!</p><h3 id="heat-dissipation-issues">Heat Dissipation Issues</h3><p>Heat was a real problem with the Raspberry Pi 4B. With an ambient temperature in Singapore at around <em>31°C</em>, the Raspberry Pi &nbsp;4B idles at around <strong>52°C</strong>, and quickly hits a toasty <strong>80°C </strong>on medium load, after which it thermal-throttles itself to oblivion. </p><figure><pre><code>$ sudo vcgencmd measure_temp
temp=81.2'C</code></pre><figcaption>Command to measure temperature on Raspberry Pi OS</figcaption></figure><p>I realized this when I tried running hardware-accelerated transcode of videos from HEVC to H.264 in <a href="https://jellyfin.org/">Jellyfin</a>. The first few seconds would render perfect in real-time, beyond that the CPU/GPU throttles and the video renders at <strong>0.25x</strong> speed. At this speed, I'd have to wait 4 seconds just to watch 1 second. </p><p>To demonstrate what <em>0.25x</em> means, here's an example: to render a full <em>40-minute</em> episode, I'd have to wait for a whole <strong>2h 40m</strong> for it to render. This pretty much <em>renders</em> the video unwatchable on the Raspberry Pi 4 (pun intended). Shockingly, this performance actually comparable to that of software transcode on the Raspberry Pi 3B without any cooling.</p><p>Besides video transcoding, I've tried applications that do not generate as much load, such as running <em>PostgreSQL</em> and <em>MariaDB</em>. I thought running databases on the Raspberry Pi 4 was the obvious choice since it has double the I/O bandwidth of a Raspberry Pi 3. However, even that pushed the Raspberry Pi 4 to its thermal throttling limits, and though I did not run precise database performance benchmarks, I did measure an average of <em>1.2s longer load times</em> on my Nextcloud home page, over 10 refreshes with browser caching disabled, during which the CPU utilization on the Raspberry Pi 4 would peak.</p><p>If this trend holds true, it means I've spent more money to purchase a device with poorer performance than my existing Raspberry Pi 3Bs (at least without investment on cooling solutions) 🤦‍♂️.</p><h3 id="no-official-cooling-solutions">No Official Cooling Solutions</h3><p>I searched far and wide for a solution to the heat problem. I've came across solutions on both ends of the price spectrum.</p><figure><div><div><p><img src="https://ikarus.sg/content/images/2021/02/Raspberry_Pi_4_Heat_Sinks_1_Copper_2_Aluminium_-_BC-01_1200x-1.jpg" width="1200" height="1200" alt=""></p><p><img src="https://ikarus.sg/content/images/2021/02/Raspberry_Pi_4_Heat_Sinks_1_Copper_2_Aluminium_-_BC-88_1200x.jpg" width="1200" height="1200" alt=""></p></div></div><figcaption>Affordable generic heatsinks (<a href="https://www.makersupplies.sg/products/raspberry-pi-4-heat-sinks-1-copper-2-aluminium">Source</a>)</figcaption></figure><p>The cheapest ones are those generic, colored heatsinks that cost around S$6 (US$4.61) for a set.</p><figure><div><div><p><img src="https://ikarus.sg/content/images/2021/02/ar_one_pi4_01.jpg" width="1000" height="1000" alt=""></p><p><img src="https://ikarus.sg/content/images/2021/02/ar_one_pi4_03.jpg" width="1000" height="1000" alt=""></p></div></div><figcaption>ArgonOne heatsink-case for the Raspberry Pi 4 (<a href="https://www.argon40.com/catalog/product/view/id/52/s/argon-one-raspberry-pi-4-case/">Source</a>)</figcaption></figure><p>Unsurprisingly, the pricier cases are from reputable Raspberry Pi accessory manufacturers. The most pricey one was the <a href="https://www.argon40.com/catalog/product/view/id/52/s/argon-one-raspberry-pi-4-case/"><em>ArgonOne</em> from ArgonForty</a> at S$32.50 (US$25), an intricately designed heatsink-case combo with a software-controlled PWM-fan for active-cooling. </p><p>Personally, I'm very sensitive to background noise and am easily distracted by it so I was looking for passive-cooling solutions. However, upon doing a cursory search, there were quite a number of users out there facing issues with passive cooling. </p><figure><a href="https://downey.io/blog/raspberry-pi-4-heatsinks-and-fans/#important-update"><div><p>The Great Raspberry Pi Cooling Bake-Off: Comparing Passive Heatsinks and Active Cooling for the Raspberry Pi 4|downey.io</p><p>Why is my Raspberry Pi 4 running so hot? You may know you need something to cool it down, but what? In this post we compare the performance of various Raspberry Pi coolers. All the way from the humble heatsink to a massive cooling tower complete with RGB fans.</p><p><span>Tim Downey</span></p></div><p><img src="https://images.downey.io/raspi/raspi-cooler-tower.jpg"></p></a></figure><p>Tim Downey has written a <a href="https://downey.io/blog/raspberry-pi-4-heatsinks-and-fans/#important-update">fantastic piece</a> on different cooling solutions he tested for the Raspberry Pi 4. In particular, in one of his tests, he had the <em>ArgonNEO</em> which was essentially an <em>ArgonOne</em> without a fan. On running CPU-intensive tasks, his case reached temperatures above <strong>80°C</strong>! The <em>ArgonNEO</em> is not a low-quality case by any standards, it's a pretty chunky aluminum case that can hold and dissipate quite a lot of heat! These temperatures on the surfaces of the case are not just potentially damaging for the furniture but also dangerous for kids and pets. It seems like cooling has become a matter of safety as well, and not just performance.</p><figure><div><div><p><img src="https://ikarus.sg/content/images/2021/02/dual-fan-heatsink-raspberry-pi4-A-600x600-1.jpg" width="600" height="600" alt=""></p><p><img src="https://ikarus.sg/content/images/2021/02/dual-fan-heatsink-raspberry-pi4-B-600x600.jpg" width="600" height="600" alt=""></p></div></div><figcaption>Generic dual-fan heatsink from Shopee (Source no longer exists, others available)</figcaption></figure><p>I went for an active cooling solution in the end after having doubts on the adequacy of passive cooling and bought a generic cheap dual-fan heatsink off <a href="https://shopee.sg/Cooler-Internal-Dual-Fan-With-Heat-Sink-Easy-Install-Ultimate-Durable-Accessories-Lightweight-For-Raspberry-Pi-3B-4B-i.38963929.3204455009">Shopee</a> at <strong>S$6.09 (US$4.68)</strong>.</p><p>At that point, I was rather disappointed that the Raspberry Pi 4 is <em>practically unusable out-of-the-box</em> for anything more than lightweight applications and simple shell scripts even though it has so much performance headroom. With that kind of performance impact, I had hoped that the Raspberry Pi foundation at least provided some semblance of an official add-on cooling solution instead of forcing the user to go out of his/her way to get it to work as it was designed to.</p><h3 id="32-bit-operating-system">32-bit Operating System</h3><p>The official/recommended operating system for the Raspberry Pi 4B is <em>Raspbian</em> (now known as <em>Raspberry Pi OS</em>). It's a <em>32-bit</em> Debian-based operating system, and the problem lies in this number of bits.</p><p>The Raspberry Pi 3B …</p></div></section></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://ikarus.sg/why-i-sold-my-raspberry-pi-4-for-a-rock-pi-4/">https://ikarus.sg/why-i-sold-my-raspberry-pi-4-for-a-rock-pi-4/</a></em></p>]]>
            </description>
            <link>https://ikarus.sg/why-i-sold-my-raspberry-pi-4-for-a-rock-pi-4/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26222704</guid>
            <pubDate>Mon, 22 Feb 2021 10:04:12 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[F2PY: Calling Fortran Routines from Python]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26222664">thread link</a>) | @optimalsolver
<br/>
February 22, 2021 | https://www.numfys.net/howto/F2PY/ | <a href="https://web.archive.org/web/*/https://www.numfys.net/howto/F2PY/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
		
        <p>Last edited: September 5th, 2019</p>
<hr>
<p>This tutorial gives a quick introduction to the F2PY package and how to use it as a command line tool. F2PY is a part of NumPy (<code>numpy.f2py</code>) and can also be used as a Python module. Check out <a href="https://docs.scipy.org/doc/numpy/f2py/">F2PY's user guide</a> for a more complete reference and installation procedures. 
From the documentation:</p>
<p><em>The purpose of the F2PY –Fortran to Python interface generator– is to provide a connection between Python and Fortran languages. F2PY is a part of <a href="https://numpy.org/">NumPy</a> (<code>numpy.f2py</code>) and also available as a standalone command line tool f2py when numpy is installed that facilitates creating/building Python C/API extension modules that make it possible</em></p>
<ul>
<li>
<p><em>to call Fortran 77/90/95 external subroutines and Fortran 90/95 module subroutines as well as C functions;</em></p>
</li>
<li>
<p><em>to access Fortran 77 COMMON blocks and Fortran 90/95 module data, including allocatable arrays.</em></p>
</li>
</ul>
<h2 id="how-does-f2py-work">How does F2PY work?</h2>
<p>F2PY works by creating an extension module that can be imported in Python using the <code>import</code> keyword. The module contains automatically generated wrapper functions that can be called from Python, acting as an interface between Python and the compiled Fortran routines.
First, F2PY reads the Fortran source file and creates a so-called signature file that contains all the necessary information about the Fortran routines needed to make the wrapper functions.
The signature file is then read and the source code of the extension module is generated in C, using the Python C API. In the last step, F2PY compiles all the source code and builds the extension module containing the wrappers and the compiled Fortran routines.</p>
<hr>
<h2 id="why-should-you-use-f2py">Why should you use F2PY?</h2>
<p>The choice of programming language can be challenging at times, especially when it comes to finding a balance between computational efficiency and implementation time and effort. While scripting languages like MATLAB and Python may provide intuitive code which is fast to implement, compiled languages like C/C++ and Fortran yield superior computational speed. By wrapping a compiled code for Python, we can get the best of both worlds!  Our notebook <a href="https://nbviewer.jupyter.org/urls/www.numfys.net/media/notebooks/fortran_to_python.ipynb">Calling Fortran(95) routines from a Python Script</a> shows an example of the usage and the gain in computational time.</p>
<h2 id="when-should-you-use-f2py">When should you use F2PY?</h2>
<p>This is perhaps the ultimate question, and unfortunately, there is no definite answer. A good rule of
thumb however, is to use F2PY, or compiled languages in general, when performing multiple operations/-
computations within (nested) loops. Possibly, the most typical example would be operations on elements
in multidimensional matrices. That is, linear algebra in general. Other good examples could be programs
calculating integrals or conducting Monte Carlo simulations.
At this point, you might wonder if anyone has already made F2PY-modules fitting your particular
problem. The answer is most likely yes! Most of the functions and routines found in NumPy and SciPy
are actually compiled Fortran (or C/C++) routines which provide highly efficient and fast solvers for
multiple problems. We thus advice you to always check if one of these two packages/libraries already
provide a routine in which may be suitable for your problem. If not, you should first implement your
solver in a pure Python script to investigate whether or not computational efficiency really is an issue. If
it is, then F2PY may possibly provide the best solution strategy for your problem.</p>
<h2 id="getting-started">Getting started</h2>
<p>We will be considering a simple example in which the <a href="https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes">sieve of Eratosthenes</a> algorithm is used to compute prime numbers. The Fortran code is saved in <code>primes.f95</code>. <strong>Note</strong> that the code only includes <code>subroutines</code> and that the variables are defined with a new keyword <code>intent</code>. The latter is explained in the next section.</p>
<div><pre><span></span><span>subroutine </span><span>sieve</span><span>(</span><span>is_prime</span><span>,</span> <span>n_max</span><span>)</span>
<span>! =====================================================</span>
<span>! Uses the sieve of Eratosthenes to compute a logical</span>
<span>! array of size n_max, where .true. in element i</span>
<span>! indicates that i is a prime.</span>
<span>! =====================================================</span>
    <span>integer</span><span>,</span> <span>intent</span><span>(</span><span>in</span><span>)</span>   <span>::</span> <span>n_max</span>
    <span>logical</span><span>,</span> <span>intent</span><span>(</span><span>out</span><span>)</span>  <span>::</span> <span>is_prime</span><span>(</span><span>n_max</span><span>)</span>
    <span>integer</span> <span>::</span> <span>i</span>
    <span>is_prime</span> <span>=</span> <span>.</span><span>true</span><span>.</span>
    <span>is_prime</span><span>(</span><span>1</span><span>)</span> <span>=</span> <span>.</span><span>false</span><span>.</span>
    <span>do </span><span>i</span> <span>=</span> <span>2</span><span>,</span> <span>int</span><span>(</span><span>sqrt</span><span>(</span><span>real</span><span>(</span><span>n_max</span><span>)))</span>
        <span>if</span> <span>(</span><span>is_prime</span> <span>(</span><span>i</span><span>))</span> <span>is_prime</span> <span>(</span><span>i</span> <span>*</span> <span>i</span> <span>:</span> <span>n_max</span> <span>:</span> <span>i</span><span>)</span> <span>=</span> <span>.</span><span>false</span><span>.</span>
    <span>end do</span>
<span>    return</span>
<span>end subroutine</span>

<span>subroutine </span><span>logical_to_integer</span><span>(</span><span>prime_numbers</span><span>,</span> <span>is_prime</span><span>,</span> <span>num_primes</span><span>,</span> <span>n</span><span>)</span>
<span>! =====================================================</span>
<span>! Translates the logical array from sieve to an array</span>
<span>! of size num_primes of prime numbers.</span>
<span>! =====================================================</span>
    <span>integer</span>                 <span>::</span> <span>i</span><span>,</span> <span>j</span><span>=</span><span>0</span>
    <span>integer</span><span>,</span> <span>intent</span><span>(</span><span>in</span><span>)</span>     <span>::</span> <span>n</span>
    <span>logical</span><span>,</span> <span>intent</span><span>(</span><span>in</span><span>)</span>     <span>::</span> <span>is_prime</span><span>(</span><span>n</span><span>)</span>
    <span>integer</span><span>,</span> <span>intent</span><span>(</span><span>in</span><span>)</span>     <span>::</span> <span>num_primes</span>
    <span>integer</span><span>,</span> <span>intent</span><span>(</span><span>out</span><span>)</span>    <span>::</span> <span>prime_numbers</span><span>(</span><span>num_primes</span><span>)</span>
    <span>do </span><span>i</span> <span>=</span> <span>1</span><span>,</span> <span>size</span><span>(</span><span>is_prime</span><span>)</span>
        <span>if</span> <span>(</span><span>is_prime</span><span>(</span><span>i</span><span>))</span> <span>then</span>
<span>            </span><span>j</span> <span>=</span> <span>j</span> <span>+</span> <span>1</span>
            <span>prime_numbers</span><span>(</span><span>j</span><span>)</span> <span>=</span> <span>i</span>
        <span>end if</span>
<span>    end do</span>
<span>end subroutine</span>
</pre></div>


<p>The simplest way to wrap this subroutine to python is to run </p>
<div><pre><span></span>f2py -c primes.f95 -m primes
</pre></div>


<p>Now that F2PY is a part of Numpy, an equivalent way to wrap this subroutine is to run</p>
<div><pre><span></span>python -m numpy.f2py -c primes.f95 -m primes
</pre></div>


<p><strong>Note</strong> that you might need to run <code>f2py3</code> to use Python 3! This command builds (<code>-c</code> flag) an extension module <code>primes.so</code> to the current directory. If the <code>-m</code> flag is excluded, the extension module will be named <code>untitled.so</code>. </p>
<p>We can now access these subroutines from Python:</p>
<div><pre><span></span><span>&gt;&gt;&gt; </span><span>import</span> <span>primes</span>
<span>&gt;&gt;&gt; </span><span>print</span><span>(</span><span>primes</span><span>.</span><span>__doc__</span><span>)</span>
<span>This module 'primes' is auto-generated with f2py (version:2).</span>
<span>Functions:</span>
<span>  is_prime = sieve(n_max)</span>
<span>  prime_numbers = logical_to_integer(is_prime,num_primes,n=len(is_prime))</span>
<span>.</span>
<span>&gt;&gt;&gt; </span><span>print</span><span>(</span><span>primes</span><span>.</span><span>logical_to_integer</span><span>.</span><span>__doc__</span><span>)</span>
<span>prime_numbers = logical_to_integer(is_prime,num_primes,[n])</span>

<span>Wrapper for ``logical_to_integer``.</span>

<span>Parameters</span>
<span>----------</span>
<span>is_prime : input rank-1 array('i') with bounds (n)</span>
<span>num_primes : input int</span>

<span>Other Parameters</span>
<span>----------------</span>
<span>n : input int, optional</span>
<span>    Default: len(is_prime)</span>

<span>Returns</span>
<span>-------</span>
<span>prime_numbers : rank-1 array('i') with bounds (num_primes)</span>

<span>&gt;&gt;&gt; </span><span>sieve_array</span> <span>=</span> <span>primes</span><span>.</span><span>sieve</span><span>(</span><span>100</span><span>)</span>
<span>&gt;&gt;&gt; </span><span>prime_numbers</span> <span>=</span> <span>primes</span><span>.</span><span>logical_to_integer</span><span>(</span><span>sieve_array</span><span>,</span> <span>sum</span><span>(</span><span>sieve_array</span><span>))</span>
<span>&gt;&gt;&gt; </span><span>print</span><span>(</span><span>prime_numbers</span><span>)</span>
<span>[ 2  3  5  7 11 13 17 19 23 29 31 37 41 43 47 53 59 61 67 71 73 79 83 89 97]</span>
</pre></div>


<p>Note that F2PY automatically found that the last argument (<code>n</code>) of the <code>logical_to_integer</code> subroutine was the dimension of the input array <code>is_prime</code>. F2PY concluded that <code>n</code> can be optional, with the default value <code>len(is_prime)</code>! One can use different values for the optional argument <code>n</code>. However, an exception is raised when it is incompatible with <code>is_prime</code>.</p>
<h2 id="specifying-input-and-output-arguments">Specifying input and output arguments</h2>
<p>In the example above, the different arguments of the subroutine were defined as input or output using the <code>intent()</code> attribute. The three most useful are:</p>
<ul>
<li><code>intent(in)</code> specifies that the variable is an input argument. It cannot be changed within the subroutine.</li>
<li><code>intent(out)</code> specifies that the variable is an output argument. The values stored in the variable before the routine is called is irrelevant!</li>
<li><code>intent(inout)</code> specifies that the variable is an input argument and can be changed in the subroutine.</li>
</ul>
<p>If <code>intent</code> is excluded, the arguments become input-only arguments (same as using <code>intent(inout)</code>) by default. It is considered good practice to specify all arguments using the <code>intent</code> attribute. It is also preferred to use <code>intent(out)</code> (and not <code>intent(inout)</code>) to have a returned value.</p>
<p>The intent and optionality of the arguments can also be edited manually in the signature file <code>primes.pyf</code> generated by running</p>
<div><pre><span></span>f2py primes.f95 -m primes -h primes.pyf
</pre></div>


<p>The final module is built from the signature file by running</p>
<div><pre><span></span>f2py -c primes.pyf primes.f95
</pre></div>


<p>The attributes can also be specified as comments, which is done in our <a href="https://nbviewer.jupyter.org/urls/www.numfys.net/media/notebooks/fortran_to_python.ipynb">Calling Fortran(95) routines from a Python Script</a> notebook.</p>
<p>Check out the <a href="https://docs.scipy.org/doc/numpy/f2py/signature-file.html">documentation for the signature file</a> for more options.</p>
<h2 id="pitfalls">Pitfalls</h2>
<ul>
<li>F2PY is compatible with the allocatable arrays in Fortran 90 and above. However, all output arguments must be given dimensions explicitly! In other words, output arguments cannot be of assumed size or allocatable.
For example:</li>
</ul>
<div><pre><span></span><span>integer</span><span>,</span> <span>allocatable</span><span>,</span> <span>intent</span><span>(</span><span>out</span><span>)</span> <span>::</span> <span>array1</span><span>(:)</span>  <span>! Not valid</span>
<span>integer</span><span>,</span> <span>intent</span><span>(</span><span>out</span><span>)</span>              <span>::</span> <span>array2</span><span>(:)</span>  <span>! Not valid</span>
<span>integer</span><span>,</span> <span>intent</span><span>(</span><span>out</span><span>)</span>              <span>::</span> <span>array3</span><span>(</span><span>10</span><span>)</span> <span>! Valid</span>
<span>integer</span><span>,</span> <span>intent</span><span>(</span><span>in</span><span>)</span>               <span>::</span> <span>array4</span><span>(:)</span>  <span>! Valid</span>
</pre></div>


<ul>
<li>Derived types are not supported.</li>
<li>It should be noted that it, in general, is easier to run F2PY from a UNIX based computer system. There is a lot of troubleshooting on Windows available online, but from our experience getting F2PY to work as intended was way easier using Linux or MacOS.</li>
</ul>
<h2 id="custom-docstrings">Custom docstrings</h2>
<p>As we have seen, F2PY creates a default documentation for the module and functions which can be reached using e.g. <code>help()</code> or <code>.__doc__</code>. As far as we know, there are no options in F2PY in which we can modify this documentation. However, it can be changed upon import (<code>&lt;module&gt;.__doc__=&lt;string&gt;</code>)
or one create a python function with its own (custom) docstring which calls the module. Many of <a href="https://www.scipy.org/">SciPy's</a> modules are built using F2PY, and their docstring are created using the latter method.</p>
    </div></div>]]>
            </description>
            <link>https://www.numfys.net/howto/F2PY/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26222664</guid>
            <pubDate>Mon, 22 Feb 2021 09:58:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Development of an α-synuclein knockdown peptide for Parkinson’s disease]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26222649">thread link</a>) | @JPLeRouzic
<br/>
February 22, 2021 | https://padiracinnovation.org/News/2021/02/development-of-an-a-synuclein-knockdown-peptide-and-evaluation-of-its-efficacy-in-parkinsons-disease-models | <a href="https://web.archive.org/web/*/https://padiracinnovation.org/News/2021/02/development-of-an-a-synuclein-knockdown-peptide-and-evaluation-of-its-efficacy-in-parkinsons-disease-models">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
                    						
					                    <p>
                        <span itemprop="datePublished">22 February 2021</span> - Posted in 
                        <span itemprop="articleSection"><a href="https://padiracinnovation.org/News/category/english">English</a></span> by 
                        
                    </p>
                </div><div itemprop="articleBody">                                   
                    <p>Parkinson’s disease (Parkinson’s disease) is a major neurodegenerative disorder. It currently lacks a clinically relevant treatment that can directly target the disease-causing processes. Current clinical approaches, like deep brain stimulation and pharmacological treatments with levodopa and dopamine agonists, only relieve symptoms. The efficacy of these treatments is largely limited by their undesirable complications and side effects. 
<img src="https://upload.wikimedia.org/wikipedia/commons/0/0f/Lewy_Body_alphaSynuclein.jpg" alt="enter image description here">
<em>Source: By Ajpolino via Wikipedia</em></p>

<p>Since α-synuclein is overexpressed under certain pathological conditions of PD and these upregulated proteins can interfere with many physiological processes, such as ER-to-Golgi transport, synaptic transmission, and mitochondria function and morphology, robustly knocking down the overexpressed α-synucleinmay have better neuroprotective efficacy in restoring normal cellular functions in the Parkinson’s disease brain than simply inhibiting the formation of toxic α-synuclein oligomers.</p>

<p>Knockdown of α-synuclein using genetic manipulations, such as antisense oligonucleotide and small interfering RNA (siRNA), has shown protection of dopaminergic neurons in various models of Parkinson’s disease.</p>

<p>The clinical translation of these manipulations into an efficient Parkinson’s disease therapy has however costly and uncomfortable, as it is mainly accomplished by an invasive injection or viral infection. These technologies may not be clinically practical for therapeutic use in human patients.</p>

<p><a href="https://www.nature.com/articles/s42003-021-01746-6">Here the scientists report the development of a short, BBB and plasma membrane-permeant synthetic peptide that can rapidly reduce endogenous α-synuclein via proteasomal degradation.</a></p>

<p>Using both in vitro and in vivo models of Parkinson’s disease, the scientists provide proof-of-principle evidence for using this small α-synuclein knockdown peptide as a potential Parkinson’s disease therapy.</p>

<p>The authors first demonstrated that the Tat-βsyn-degron peptide can specifically reduce the level of α-synuclein both in vitro and in vivo. The authors then showed that the peptide-induced α-synuclein knockdown is associated with protection of dopaminergic neurons against toxin-induced damage in a culture model of Parkinson’s disease.</p>

<p>Most importantly, the scientists were able to demonstrate the therapeutic potential of systemic application of the Tat-βsyn-degron peptide as an effective Parkinson’s disease treatment in two well-characterized animal models of Parkinson’s disease.</p>

<p>Their α-synuclein knockdown peptide (Tat-βsyn-degron) is innovative as the peptide directly targets one of the disease-causing processes, and can be expected to stop or slow down the progression of the disease.</p>

<p>In addition, the peptide-mediated knockdown has a clear temporal advantage over antisense or siRNA-mediated knockdown. α-synuclein is a very stable protein with a long half-life while by hijacking the endogenous proteasomal degradation system in the cell, the Tat-βsyn-degron peptide produced a rapid and robust degradation of α-synuclein protein within a few hours.</p>

<p>It is also interesting to note that α-synuclein is also expressed in tissues outside the central nervous system and the scientists found that a single intraperitoneal injection of the Tat-βsyn-degron peptide similarly reduced the α-synuclein expression in the kidney and the spleen of wild-type C57BL/6 mice .</p>

<p>A recent success in a phase 3 clinical trial has already demonstrated that a Tat-fused short peptide is not only safe, but therapeutically effective in protecting neurons against ischemic damage in humans. The authors hope this α-synuclein knockdown peptide may also have the potential to be quickly translated into the clinic as an effective disease-modifying treatment that directly targets the disease-causing process of Parkinson’s disease.</p>

<p>Due to the versatility of their peptide-mediated protein knockdown method, the scientists can theoretically target disease-causing cellular proteins by simply changing the protein-binding sequence of the targeting peptide. Since many human diseases, including some of the age-related neurodegenerative diseases such as ALS, Alzheimer’s disease and Huntington’s disease, are pathologically caused by gain of function of a protein due to its mutations and/or increased expression levels, <strong>the proposed study can be expected to spur the development of new therapeutics for human diseases beyond Parkinson’s disease</strong>.</p>

<h3><u>Advertisement</u></h3>

<p><a href="https://www.amazon.com/dp/1698147899">
<img src="https://images-na.ssl-images-amazon.com/images/I/51pNZDKvmIL._SX331_BO1,204,203,200_.jpg" width="200">
<br>
This book retraces the main achievements of ALS research over the last 30 years, presents the drugs under clinical trial, as well as ongoing research on future treatments likely to be able stop the disease in a few years and to provide a complete cure in a decade or two.<br>
</a></p>
                </div></div>]]>
            </description>
            <link>https://padiracinnovation.org/News/2021/02/development-of-an-a-synuclein-knockdown-peptide-and-evaluation-of-its-efficacy-in-parkinsons-disease-models</link>
            <guid isPermaLink="false">hacker-news-small-sites-26222649</guid>
            <pubDate>Mon, 22 Feb 2021 09:57:43 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[How to Make a Production Checklist]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26222608">thread link</a>) | @vinnyglennon
<br/>
February 22, 2021 | Https://www.blameless.com/blog/4-things-you-need-to-know-about-writing-better-production-readiness-checklists | <a href="https://web.archive.org/web/*/Https://www.blameless.com/blog/4-things-you-need-to-know-about-writing-better-production-readiness-checklists">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>When we think of reliability tools, we may overlook the humble checklist. While tools like <a href="https://www.blameless.com/blog/service-level-objectives-slos-lessons-learned">SLOs</a> represent the cutting edge of SRE, checklists have been recommended in many industries such as <a href="https://dash.harvard.edu/handle/1/38846186">surgery</a> and <a href="https://www.flightsafetyaustralia.com/2018/11/one-thing-at-a-time-a-brief-history-of-the-checklist/">aviation</a> for almost a century. But checklists owe this long and widespread adoption to their usefulness.<br></p><p>Checklists can also help limit errors when deploying code to production. In this blog post, we’ll cover:</p><ul role="list"><li>How to make a production checklist</li><li>Why production checklists are helpful</li><li>Keeping your checklist up to date</li><li>How Blameless can help integrate your checklists</li></ul><h2>How to make a production checklist</h2><p>Production checklists should be holistic. They should cover everything from launch logistics to contingency plans for failure. Let’s break down what you’ll need for a thorough checklist.</p><ol role="list"><li><strong>Determine the service level of what you’re launching</strong></li></ol><p>To determine <em>how thorough</em> your checklist should be, consider what level of reliability your customers need.. You may be tempted to be as comprehensive as possible with every checklist, but that costs time and may be unnecessary. At Mercari, <a href="https://github.com/mercari/production-readiness-checklist/blob/master/docs/references/production-readiness-level.md">the service level is determined based on the service’s SLO</a>. Services that are critical to business success are scrutinized more than niche services.</p><ol start="2" role="list"><li><strong>Map out all the checklist areas</strong></li></ol><p>List all major components of your service. These components may be under the ownership of various teams. For example, you’ll likely need to consult server management teams, testing teams, and many others. It’s important to know as soon as possible whom you’ll need to consult. Some areas to consider include:<br></p><ul role="list"><li><strong>Server-side:</strong> What machines will this service run on? If you’re cloud-based, will your plan cover the new service’s load?</li><li><strong>Client-side:</strong> Is your service usable for all potential clients?</li><li><strong>Monitoring:</strong> Do you have ways of collecting data from your new service?&nbsp;</li><li><strong>Growth:</strong> Do you have a roadmap for how you will maintain or improve the service going forward? What if usage increases? What if you need to expand functionality?</li><li><strong>Dependencies:</strong> What other in-house and third party services does your service depend on? Will they integrate smoothly?</li><li><strong>Testing:</strong> Has the new service been tested in an environment mirroring production?</li><li><strong>Security: </strong>Will your new service pass your security audits?</li><li><strong>Reliability: </strong>What level of reliability will your users expect? Do you have a plan for when you are unable to meet these expectations?</li><li><strong>Incident response:</strong> What will you do if an incident causes service interruption or degradation? Do you have runbooks to cover these incidents?</li><li><strong>Legal:</strong> Do you have an SLA that guarantees availability? Does this service deal with personal information that must be kept secure?</li><li><strong>Logistics:</strong> What is the launch schedule? What resources will you need?<br></li></ul><p>For more examples of areas to consider, check out Google’s <a href="https://sre.google/sre-book/launch-checklist/">Launch Coordination Checklist</a>, <a href="https://gruntwork.io/devops-checklist/">gruntwork.io’s AWS checklist</a>, or <a href="https://github.com/mercari/production-readiness-checklist/blob/master/docs/references/production-readiness-checklist.md">Mercari’s checklists</a>.</p><ol start="3" role="list"><li><strong>Prepare the checklist items</strong></li></ol><p>Each of these areas contains many issues, and requires data to answer. Your checklist should ask for each piece of data. Here’s an example of how certain sections could be broken down:<br></p><figure><p><img src="https://uploads-ssl.webflow.com/5ec0224560bd6a6ef89a51ae/60258a189045532048064bce_GXLhui7pRZ_R7zLCqUunWS6CCU7PBwExUev_Aayj1ioz3UDZtZHnvyMOZNAsbRp1eugg8OC6Wjyl38autUOS0tlRm_wJkM3HjmEehUg3V1jnHf5gmUg-1ZX0DM0Z2G5dNlksxcM.png" alt="example areas, issues, and corresponding checklist items for a production readiness checklist."></p></figure><p>You may also want to include information on who to consult to check off each item, and the timeframe for being able to check it. Build your checklist and check items off as development progresses. Double check to ensure that items are ready to go. Right before launch, do a final check through the whole list, just in case.</p><h3>Keeping the checklist in check</h3><p>As you develop, you’ll likely find more areas you want to vet prior to launch. To keep your checklist from becoming too long, you’ll need a system to make sure new additions are helpful. <a href="https://sre.google/sre-book/reliable-product-launches/">At Google</a>, teams have two criteria for adding an item to the checklist:<br></p><ul role="list"><li>“Every question’s importance must be substantiated, ideally by a previous launch disaster.”</li><li>“Every instruction must be concrete, practical, and reasonable for developers to accomplish.”<br></li></ul><p>You can determine criteria based on the service level you’ve assigned. It’s better to have an unnecessary item than to lack one you need. It’s okay to start with a big checklist, then remove items after each launch that proved to not be useful.</p><h2>Why are production checklists helpful?</h2><p>Production checklists can seemingly add overhead to engineers’ jobs. However, the upfront work can save teams from future problems and ensure a successful launch. Production checklists help:<br></p><ul role="list"><li>Remove the cognitive toil of having to remember everything</li><li>Identify possible problems ahead of time</li><li>Prepare resources ahead of time</li><li>Motivate development to complete necessary items</li><li>Prioritize key requirements vs unnecessary additions</li><li>Ensure contingency planning, improving reliability</li><li>Keep everyone in the loop throughout development as a centralized progress meter</li></ul><h2>How to keep your production checklist up to date</h2><p>You will need to review and revise your checklists periodically to keep them useful. Be sure to revisit them at these times:<br></p><p><strong>When development on a new service starts. </strong>When mapping out a new service, consider which production checklist to use when it launches. Based on the type of service and service level, find the closest checklist you have. Review it to make sure it follows the processes and architecture you currently use. Add any service-specific requirements as you develop.<br></p><p><strong>After a launch.</strong> Take a look at the production checklist after you launch the new service. Were there any problems with the launch? Could they have been checked for beforehand? Look for checklist items that were misunderstood and filled out incorrectly. Revise these items to ensure the checklist lines up with the reality of development.<br></p><p><strong>After an incident. </strong>If an incident impacts the new service, see if any of the contributing factors could have been addressed with the checklist If so, try to capture those items on future checklists. This task can be incorporated into your <a href="https://www.blameless.com/blog/incident-retrospective-postmortem-template">incident retrospectives</a>.<br></p><p><strong>As part of regular review cycles.</strong> Set a schedule to review tools like runbooks and production checklists. Make sure to invite all team members who will be required to use these runbooks or checklists. Each of these people can provide insight on what to improve moving forward.</p><h2>How Blameless can help integrate checklists</h2><p>To get the most from your checklists, you need to integrate them into your workflows. Here’s how Blameless can help:<br></p><ul role="list"><li><a href="https://www.blameless.com/product/incident-resolution">Blameless Incident Resolution</a> allows teams to treat each deploy like an incident and assign roles and checklists.</li><li><a href="https://www.blameless.com/product/incident-retrospectives">Blameless Incident Retrospectives</a> provide a hub of learning for future checklist development.</li><li><a href="https://www.blameless.com/blog/introducing-blameless-runbook-documentation">Blameless Runbook Documentation</a> helps richly document processes, allowing you to dive into the information behind each checklist item.<br></li></ul><p>To see more of how Blameless helps you be your most reliable, check out a <a href="https://www.blameless.com/schedule-demo">demo</a>.<br></p><p>If you enjoyed this blog post, check out these resources:</p><ul role="list"><li><a href="https://www.blameless.com/blog/how-mercari-scales-vision-culture-reliability">How Mercari Scales Vision, Culture, &amp; Reliability</a></li><li><a href="https://www.blameless.com/blog/use-blameless-power-remote-work">How We Use Blameless to Power Remote Deploys</a></li><li><a href="https://www.blameless.com/resources/webinar-how-slos-enable-fast-reliable-application-delivery">Webinar: How SLOs Enable Fast, Reliable Application Delivery</a></li></ul></div></div>]]>
            </description>
            <link>Https://www.blameless.com/blog/4-things-you-need-to-know-about-writing-better-production-readiness-checklists</link>
            <guid isPermaLink="false">hacker-news-small-sites-26222608</guid>
            <pubDate>Mon, 22 Feb 2021 09:52:55 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Avoid the Most Dangerous Word in Software Development]]>
            </title>
            <description>
<![CDATA[
Score 6 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26222529">thread link</a>) | @pawurb
<br/>
February 22, 2021 | https://pawelurbanek.com/dangerous-word-slack | <a href="https://web.archive.org/web/*/https://pawelurbanek.com/dangerous-word-slack">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

<div role="main">
<div>

<p><span>Share</span>
<a href="https://twitter.com/intent/tweet?text=How+to+Avoid+the+Most+Dangerous+Word+in+Software+Development&amp;url=https%3A%2F%2Fpawelurbanek.com%2Fdangerous-word-slack" rel="nofollow noopener noreferrer" target="_blank">
<img title="Share on Twitter" src="https://pawelurbanek.com/assets/twitter-9863fdece66492f803e15e512e1f2d16ddd1bfad52e97104245b973e55ec0343.png" srcset="https://pawelurbanek.com/assets/twitter-9863fdece66492f803e15e512e1f2d16ddd1bfad52e97104245b973e55ec0343.png 1x, https://pawelurbanek.com/assets/twitter@2x-bb4de08ef7390cb0e6bc0e4c74d50e098821cd7c55f1c6b20560a7a325d29164.png 2x">
</a>
<a href="https://facebook.com/sharer.php?u=https%3A%2F%2Fpawelurbanek.com%2Fdangerous-word-slack" rel="nofollow noopener noreferrer" target="_blank">
<img title="Share on Facebook" alt="Share on Facebook" src="https://pawelurbanek.com/assets/facebook-ebff27bfb5f4575cf52588bae9aaa3e9d638d3ea8806983038f1f40ae5fefe17.png" srcset="https://pawelurbanek.com/assets/facebook-ebff27bfb5f4575cf52588bae9aaa3e9d638d3ea8806983038f1f40ae5fefe17.png 1x, https://pawelurbanek.com/assets/facebook@2x-0d1abc87e5ffdc544fa8f0f4282d2c01706bf15a814d794080aff2f7a87a0ffb.png 2x">
</a>
<a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fpawelurbanek.com%2Fdangerous-word-slack&amp;title=How+to+Avoid+the+Most+Dangerous+Word+in+Software+Development" rel="nofollow" target="_blank">
<img title="Share on LinkedIn" alt="Share on LinkedIn" src="https://pawelurbanek.com/assets/linkedin-fa921978de5a16810ecd5894affebee63ed33b4d74cad9637ba01c6e803de9cf.png" srcset="https://pawelurbanek.com/assets/linkedin-fa921978de5a16810ecd5894affebee63ed33b4d74cad9637ba01c6e803de9cf.png 1x, https://pawelurbanek.com/assets/linkedin@2x-b89a20f8fc3d0a82f9fe54137fbbbf4029dcc189f4dec6b9a3964b9350833e1f.png 2x">
</a>
</p>
<p><span>Share</span>
<br>
<a href="https://twitter.com/intent/tweet?text=How+to+Avoid+the+Most+Dangerous+Word+in+Software+Development&amp;url=https%3A%2F%2Fpawelurbanek.com%2Fdangerous-word-slack" rel="nofollow noopener noreferrer" target="_blank">
<img title="Share on Twitter" alt="Share on Twitter" src="https://pawelurbanek.com/assets/twitter-9863fdece66492f803e15e512e1f2d16ddd1bfad52e97104245b973e55ec0343.png" srcset="https://pawelurbanek.com/assets/twitter-9863fdece66492f803e15e512e1f2d16ddd1bfad52e97104245b973e55ec0343.png 1x, https://pawelurbanek.com/assets/twitter@2x-bb4de08ef7390cb0e6bc0e4c74d50e098821cd7c55f1c6b20560a7a325d29164.png 2x">
</a>
<br>
<a href="https://facebook.com/sharer.php?u=https%3A%2F%2Fpawelurbanek.com%2Fdangerous-word-slack" rel="nofollow noopener noreferrer" target="_blank">
<img title="Share on Facebook" alt="Share on Facebook" src="https://pawelurbanek.com/assets/facebook-ebff27bfb5f4575cf52588bae9aaa3e9d638d3ea8806983038f1f40ae5fefe17.png" srcset="https://pawelurbanek.com/assets/facebook-ebff27bfb5f4575cf52588bae9aaa3e9d638d3ea8806983038f1f40ae5fefe17.png 1x, https://pawelurbanek.com/assets/facebook@2x-0d1abc87e5ffdc544fa8f0f4282d2c01706bf15a814d794080aff2f7a87a0ffb.png 2x">
</a>
<br>
<a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fpawelurbanek.com%2Fdangerous-word-slack&amp;title=How+to+Avoid+the+Most+Dangerous+Word+in+Software+Development" rel="nofollow noopener noreferrer" target="_blank">
<img title="Share on LinkedIn" alt="Share on LinkedIn" src="https://pawelurbanek.com/assets/linkedin-fa921978de5a16810ecd5894affebee63ed33b4d74cad9637ba01c6e803de9cf.png" srcset="https://pawelurbanek.com/assets/linkedin-fa921978de5a16810ecd5894affebee63ed33b4d74cad9637ba01c6e803de9cf.png 1x, https://pawelurbanek.com/assets/linkedin@2x-b89a20f8fc3d0a82f9fe54137fbbbf4029dcc189f4dec6b9a3964b9350833e1f.png 2x">
</a>
<br>
</p>



<article>
<p><img title="Unhealthy communication on Slack is represented by a mousetrap Photo by Skitterphoto from Pexels" alt="Unhealthy communication on Slack is represented by a mousetrap Photo by Skitterphoto from Pexels" data-src="https://pawelurbanek.com/assets/slack-communication-trap-eeafc54866148ef1e14021e31975e1a8cfdab1478039b1a0685c1ea63600ef22.jpg" src="https://pawelurbanek.com/assets/slack-communication-trap-thumb-c4922c24466c80938315ab6fda7ae65192a488f1a643ccc4728839c6027bee1e.jpg">
</p>
<br>


<p><em>J-U-S-T</em>. Those four characters can be significantly detrimental to a software development process. In this blog post, I’ll describe how the <em>“just keyword”</em> can affect team’s communication and how to avoid misusing it on Slack.</p>
<h2 id="lets-just-do-it">Let’s “just” do it</h2>
<p>You’ve probably been there. Your product manager shares his brand new plan on the Slack channel:</p>
<p><em>“Why don’t we</em> <strong>just</strong> <em>add this cool new feature to our application?”</em></p>
<p>or your colleague got the wrong idea about scaling after reading a HackerNews story:</p>
<p><em>“Let’s</em> <strong>just</strong> <em>migrate our infrastructure to Kubernetes…“</em></p>
<p><em>“Just”</em> is toxic and dangerous. It implicitly suggests that the proposed task is straightforward. It undermines the discussion about the issues that might pop-up during the implementation.</p>
<p>There’s no <em>“just”</em> in software development. Most of the tasks turn out to be more complex than anticipated. <em>“Just tickets”</em> tend to drag, evolve into epics, miss deadlines and hurt the team’s motivation.</p>
<p>I’ve seen this topic discussed many times before. Make sure to check out <a href="https://alistapart.com/blog/post/the-most-dangerous-word-in-software-development/" target="_blank" rel="noopener noreferrer">these two</a> <a href="https://the-pastry-box-project.net/brad-frost/2014-january-28" target="_blank" rel="noopener noreferrer">blog posts</a> for a more in-depth description of it.</p>
<h2 id="how-to-use-slack-to-get-rid-of-just-tickets">How to use Slack to get rid of “Just tickets”</h2>
<p>I want to propose a solution to the <em>“Just”</em> problem. Lexically there’s never a need to include the word <em>“just”</em> in a sentence. You can always omit it without altering the core meaning of your message.</p>
<p>You could discourage using the word <em>“just”</em> in communication. Slack offers a simple feature that will let you automate it. Introducing Slackbot triggers:</p>
<p><img alt="Slack keyword trigger in action" title="Slack keyword trigger in action" loading="lazy" src="https://pawelurbanek.com/assets/slack-keyword-trigger-40d3219d92fe2bee0932a832ff7c80608c9b99a067f704347ed564dec917bc1e.png"></p>
<p>Slack trigger in action</p>

<p>You can configure Slack to automatically send a custom message whenever a <em>trigger</em> keyword is detected. In settings, go to <strong>Customize &gt; Slackbot</strong> and enter your desired trigger and response.
<br></p>
<p><img alt="Slack trigger settings" title="Slack trigger settings" loading="lazy" src="https://pawelurbanek.com/assets/slack-trigger-settings-3456bc59b61b75a27c245690b2d0a5d57afd8e2ac535027c9d01e3a87c54cffb.png"></p>
<p>Slack trigger settings</p>

<p>It could be pretty spammy to start with, but your team should quickly adjust and stop using the <em>forbidden</em> keyword. If someone does use it, the alert message will be a fun reminder to stop and think twice if the <em>“just”</em> idea is really that simple.</p>
<p>So why won’t you just give this communication experiment a try?</p>
<p>BTW if you’re looking for more creative ways to enhance your communication on Slack, you can check out <a href="https://abot.app/" target="_blank" rel="noopener noreferrer">Abot for anonymous messaging and polls</a>. It’s highly configurable and supports various <a href="https://abot.app/scenarios" target="_blank" rel="noopener noreferrer">usage scenarios</a>.</p>
<p><img alt="Anonymous poll conducted using Abot for Slack" title="Anonymous poll conducted using Abot for Slack" loading="lazy" src="https://pawelurbanek.com/assets/slack-anonymous-poll-bda619f03336795730c69705f61eddc8f4bac6a6bcd8deb65e83bf3ff880156d.png"></p>
<p>Abot anonymous poll with private answers</p>

</article>

<p><a href="https://twitter.com/_pawurb" target="_blank" rel="nofollow">
<img loading="lazy" alt="Pawel Urbanek Twitter account" title="Pawel Urbanek Twitter account" src="https://pawelurbanek.com/assets/pawel-circle-eafe4e7f9c98c20c753dcba1f1b1a16ed8bc384cdf2da91d272dd2291d8e7a4d.jpg" srcset="https://pawelurbanek.com/assets/pawel-circle-eafe4e7f9c98c20c753dcba1f1b1a16ed8bc384cdf2da91d272dd2291d8e7a4d.jpg 1x, https://pawelurbanek.com/assets/pawel-circle@2x-352753604906054aa864cc5f3916317be7c2b7db8f8703243a0447d06187c641.jpg 2x">
</a>
</p>

<br>

<br>

<br>


<br>





</div>
</div>
</div></div>]]>
            </description>
            <link>https://pawelurbanek.com/dangerous-word-slack</link>
            <guid isPermaLink="false">hacker-news-small-sites-26222529</guid>
            <pubDate>Mon, 22 Feb 2021 09:41:45 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Welcome to the Talent Wars]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26222479">thread link</a>) | @artpi
<br/>
February 22, 2021 | https://deliber.at/2021/talent-wars/ | <a href="https://web.archive.org/web/*/https://deliber.at/2021/talent-wars/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to retrieve article]]>
            </description>
            <link>https://deliber.at/2021/talent-wars/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26222479</guid>
            <pubDate>Mon, 22 Feb 2021 09:33:24 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Start with a Niche]]>
            </title>
            <description>
<![CDATA[
Score 105 | Comments 35 (<a href="https://news.ycombinator.com/item?id=26222313">thread link</a>) | @tablet
<br/>
February 22, 2021 | https://fibery.io/blog/start-with-a-niche/ | <a href="https://web.archive.org/web/*/https://fibery.io/blog/start-with-a-niche/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><section><p>The most popular products don’t become mass popular overnight. It’s a process. Usually, their popularity is uneven, they are unknown in some niches, but very popular in other niches.</p><p>When you start a product, sometimes you know the niche already and can easily define target customers. However, in many cases, you just don’t and try to find the niche and this miraculous niche-market-fit. </p><p>One of the most common mistakes is to ignore niches and just try to attract all kinds of customers. It’s essential to find 1-2 ponds to start from and then expand to the other, larger, and more promising lakes and oceans 🚰 → 🛁 → 🌊.</p><p>Here are a couple, maybe surprising examples, that demonstrate how popular products took off. </p><h4>Electrical telegraph (1837)</h4><p>Its adoption was not easy, since it was not clear what are the benefits for commercial institutions. Stockbrokers and reporters got the benefits first. They understood that fast information transition increases efficiency and helps to get an edge. In a short term, all major news agencies and major stock markets were connected to the telegraph. </p><h4>Telephone (1876)</h4><p>Telephones were adopted by police departments and fire stations. Fast reaction to crime reports and fires was great, but the telegraph was not enough. You have to have two-ways communication to get some details that the sender maybe is not expecting to report initially. </p><h4>Phonograph (1877)</h4><p>Try to guess the first niche market for the phonograph. Rich music lovers? Nope. First phonographs were coin-machines in bars. Throw a nickel and enjoy Stephen Foster ballads.</p><h4>Car (1886)</h4><p>Cars are almost among the lucky exception to the niche rule. However, there was still one group of people in the USA that moved from horses to cars enormously fast — farmers. Cars just expanded the borders of farmers’ social life and business activities. Suddenly you can buy goods, not from a local dealer, but a dealer in a remote town (much cheaper). Suddenly you can visit a town and watch a movie. These benefits were not important for the urban population, for they were life-changers for the rural population. Nevertheless, cars were adopted in cities quite fast as well.</p><blockquote><p>In addition, the car delivered you to the door and was faster than a horse-and-buggy, thus allowing longer trips in shorter time. Farmers had traditionally felt guilty about taking such trips, even when the time was available. </p></blockquote><h4>Radio (1895)</h4><p>Radio was immediately adopted by the British Royal Navy, they thought that radio can speed up communication between ships and were right. Fun fact: in 1912 Titanic sent CQD (distress signal), however, <a href="https://www.nationalgeographic.com/history/article/why-titanic-first-call-help-not-sos-signal">radio receiver was turned off on the closest ship</a>:</p><blockquote><p>Meanwhile, the closest ship, Californian, didn’t receive Titanic’s distress calls at all. Its wireless operator had switched off his receiver and gone to bed after Phillips told him to shut up.</p></blockquote><h4>VisiCalc (Excel predecessor, 1979)</h4><p>First, it was adopted by accountants. Businessmen and analysts joined the party much later. Accountants just saw the value right away (and quite many people pirchased Apple II just to get <a href="https://thenewstack.io/how-visicalcs-spreadsheets-changed-the-world/">VisiCalc</a>):</p><blockquote><p>Like an accountant, I remember showing it to one around here and he started shaking and said, “That’s what I do all week. I could do it in an hour.” … I meet these people now, they come up to me and say, “I gotta tell you, you changed my life. You made accounting fun.”</p></blockquote><h4>Facebook (2004)</h4><p>Everybody knows that Facebook got its popularity in universities first. Everybody knows the rest of the story.</p><blockquote><p>Within 24 hours, 1,200 Harvard students had signed up, and after one month, over half of the undergraduate population had a profile. The network was promptly extended to other Boston universities, the Ivy League and eventually all US universities</p></blockquote><hr><p>Can you start without any niche in mind? Yes, you can, but this is just hard. The most problematic part is marketing. Who are your ideal customers? How to reach them? How to target your message? <strong>Product is the message</strong>, so without proper marketing startup success chances are low. </p><h4>My experience</h4><p>In Fibery we did our first release in April 2020 as a general work management tool. We were not sure in what types of companies it will work better and what use cases will be more valuable. In just a month it became clear that we had all kinds of leads from all kinds of companies. Leads demanded all kinds of improvements that just didn’t form a sane strategy. </p><p>We quickly <a href="https://fibery.io/blog/chronicles-21/">decided to select a single niche and focus on it</a>. The niche we choose was product companies from 20 to 200 people. And it made everything much simpler. Finally, we can quite accurately say what features are important and what features are not so important, what is our value proposition, who is our ideal lead (Product Ops or CPO). It took us 9 months to prepare the <a href="https://fibery.io/product-management">second release</a>, but in this niche Fibery can fly much better, I believe. </p><p>OK, niche strategy looks convincing, but how to find the niche? We used <a href="https://en.wikipedia.org/wiki/Conway%27s_law">Conway’s law</a>, and it’s just partially a joke. We’ve reviewed a couple of alternatives and selected one we knew best and were confident that our product will provide a significant value boost. There were other alternatives, like education space or digital agencies space, but our knowledge here was not deep enough. It means we should rely on some domain experts, etc. It’s not a huge problem, but we also did not feel that these niches are better.</p><p>A startup should be an experimentation facility that hypothesizes, executes, and measures the results. The faster you can do it, the faster you find your niche. Why it took us 9 months to make this niche release? In fact we spend time to create a niche-probing framework. Now we can asseble solutions for various niches in 1-2 weeks and check initial response in 1-2 months. If the first niche will not be successful, we at least have a decent experimentation framework 🧬.</p></section></div>]]>
            </description>
            <link>https://fibery.io/blog/start-with-a-niche/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26222313</guid>
            <pubDate>Mon, 22 Feb 2021 09:05:51 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[AArch64 Boards and Perception]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26222300">thread link</a>) | @pabs3
<br/>
February 22, 2021 | https://marcin.juszkiewicz.com.pl/2021/02/22/aarch64-boards-and-perception/ | <a href="https://web.archive.org/web/*/https://marcin.juszkiewicz.com.pl/2021/02/22/aarch64-boards-and-perception/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		<p>Recently I had a discussion with A13 and realized that people may have
different perception of how AArch64 boards&nbsp;work:</p>
<blockquote>
<p>Sahaj told me that you can just install generic images on&nbsp;honeycomb</p>
<p>it kinda blows my&nbsp;mind</p>
</blockquote>
<p>How did we got to that&nbsp;point?</p>
<h3>Servers are boring,&nbsp;right?</h3>
<p>I started working on AArch64 in 2012. First in fast models written by Arm
developers, then also in <span>QEMU</span>. Both used direct kernel boot method without any
firmware or&nbsp;bootloaders.</p>
<p>In 2013 I moved from Canonical/Linaro to Red Hat. And there we got server from
Applied Micro. I do not remember how it booted as I used it for building
software. Some time later we had Mustangs and all of them were booting <span>UEFI</span>.</p>
<p>Then <a href="https://marcin.juszkiewicz.com.pl/2014/06/10/aarch64-is-in-the-house/">I got Mustang at home</a>. 
Fedora, <span>RHEL</span> were booting fine. Then CentOS and Debian joined. All of them
used grub-efi like my x86-64 desktop or&nbsp;laptop.</p>
<p>Time passed, I got other servers to work with. HPe M400, ThunderX, ThunderX2,
Falkor, D05 etc. Each of them was running <span>UEFI</span>. Either Tianocore based or
commercial&nbsp;one.</p>
<p>And to install operating system all I needed was to boot generic install&nbsp;media.</p>
<h3><span>SBC</span>&nbsp;hell</h3>
<p>At same time <span>SBC</span> world was fighting with users. Each vendor/SoC/board had to be
treated specially as there was no way to store firmware on board (as <a href="https://marcin.juszkiewicz.com.pl/2020/01/29/the-most-expensive-chip-in-the-arm-world/"><span>SPI</span> flash is
very expensive</a>).</p>
<p>So depending on <span>SBC</span> your firmware could be written&nbsp;either:</p>
<ul>
<li>at some special offset from start of microSD&nbsp;card</li>
<li>at the beginning of a partition of special&nbsp;type</li>
<li>in a file on vfat partition of any&nbsp;type</li>
<li>in a file on <span>EFI</span> System Partition (also using&nbsp;vfat)</li>
</ul>
<p>Some offsets forced the use of “obsolete” <span>MBR</span> partitioning as there was no space
for <span>GPT</span> information. While <span>UEFI</span> systems require <span>GPT</span> not <span>MBR</span>.</p>
<p>It also generated lot of wrong information like “this file needs to be named in
<span>UPPERCASE</span> (on case insensitive filesystem)” or “needs to be first file written
to a partition”. Some kind of “<span>SBC</span> boot&nbsp;voodoo”.</p>
<p>So each <span>SBC</span> required its own boot media — you could not take it to a board with
some other SoC and expect it to start. Or you spend some time to create some
kind of hybrid image which had a few bootloaders written. Easier way was to
prepare a separate boot media images per <span>SBC</span>.</p>
<p>From time to time there was <span>SBC</span> with onboard flash available for storing
firmware. Some people made use of it, others continued doing offset crap as they
were used to&nbsp;it.</p>
<h3><span>SBBR</span>, <span>EBBR</span>&nbsp;came</h3>
<p>Last years brought us several specifications from Arm. First was <span>SBBR</span> which
stands for Server Base Boot Requirements. It said which features should be
present in firmware (you can read more in <a href="https://marcin.juszkiewicz.com.pl/2020/10/12/standards-in-arm-space-part-i/">my previous post about Arm
standards</a>).</p>
<p>As SBCs are not servers, a new specification was created for them: <span>EBBR</span> (E means
Embedded). It basically says “try to follow what server does” and has some
requirements either dropped or&nbsp;relaxed.</p>
<p>Both were designed to make distribution’s life easier. Never mind is it <span>BSD</span>,
Linux or Microsoft Windows — they have to put <span>EFI</span> bootloader (like Grub-efi) in
<span>EFI</span> System Partition and system will boot on any supported <span>SBBR</span>/<span>EBBR</span>&nbsp;hardware.</p>
<p>For example I have a <span>USB</span> pendrive with Debian “bullseye” installed. It boots
fine on RockPro64 and Espressobin SBCs (both have <span>EBBR</span> compliant U-Boot stored
in on-board flash) and on Mustang and HoneyComb (both with <span>SBBR</span> compliant <span>UEFI</span>
in on-board&nbsp;flash).</p>
<h3>Habits. Good, bad,&nbsp;forced.</h3>
<p>So it looks like the way how AArch64 system should boot depends on what your
habits&nbsp;are.</p>
<p>When you started from servers then <span>SBBR</span>/<span>EBBR</span> way is your way and you look weird
at most of <span>SBC</span> systems with their offsets and “other mumbo&nbsp;jumbo”.</p>
<p>If all you used were <span>SBC</span> then going into <span>SBBR</span>/<span>EBBR</span> world can be “zOMG, it just
magically&nbsp;works!”.</p>
<h3>Note to <span>SBC</span>&nbsp;vendors</h3>
<p>Most SBCs already follow the <span>EBBR</span> standard or can easily be made compliant.
Never mind you are using mainline U-Boot or some own fork (and then consider
upstreaming as board’s life may be longer than you&nbsp;expect).</p>
<p>Enable the CONFIG_DISTRO_DEFAULTS option in the config. Build U-Boot, store it
to the board and boot. Then erase whatever environment you used before with “env
default -a”&nbsp;command.</p>
<p>On next reboot your <span>SBC</span> will iterate over “boot_targets” variable and check
for few standard boot&nbsp;files:</p>
<ul>
<li>extlinux/extlinux.conf</li>
<li>boot.scr.uimg</li>
<li>boot.scr</li>
<li>/efi/boot/bootaa64.efi</li>
</ul>
<p>When it gets something then it handles that and boots. If not then goes to
another boot&nbsp;target.</p>
<p>This allows to handle basically every operating system used on Arm systems.
And allows to boot generic install <span>ISO</span> (as long as <span>OS</span> on it supports the&nbsp;device).</p>
<p>Bonus points if your <span>SBC</span> has some on board flash or eMMC it can boot from. Then
firmware can be stored there so user does not even have to worry about&nbsp;it.</p>
	</div></div>]]>
            </description>
            <link>https://marcin.juszkiewicz.com.pl/2021/02/22/aarch64-boards-and-perception/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26222300</guid>
            <pubDate>Mon, 22 Feb 2021 09:03:46 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[The Internet is made of plastic]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26222197">thread link</a>) | @jaspax
<br/>
February 22, 2021 | https://jsbangs.com/2011/01/09/the-internet-is-made-of-plastic/ | <a href="https://web.archive.org/web/*/https://jsbangs.com/2011/01/09/the-internet-is-made-of-plastic/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><article id="post-518" itemscope="" itemtype="http://schema.org/BlogPosting" itemprop="blogPost">
	

	<div itemprop="articleBody">
				<p>We’ve all seen <i>The Graduate</i>, right? And we’ve all seen that famous scene where the old guy tells the young depressed guy that he should get into plastics, because there’s a great future in plastics? If you have been living in an underground bunker waiting for the end of the world for the past forty years and haven’t had time to brush up on your pop culture, here’s a refresher:</p>
<p><span><iframe width="800" height="450" src="https://www.youtube.com/embed/DHGCvJjat1E?version=3&amp;rel=1&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;fs=1&amp;hl=en&amp;autohide=2&amp;wmode=transparent" allowfullscreen="true" sandbox="allow-scripts allow-same-origin allow-popups allow-presentation"></iframe></span></p>
<p>We all know why this scene is funny. It’s not because the old guy’s advice is wrong–on the contrary, plastics would be an excellent industry for a young college graduate to make money and build a solid career. Rather, it’s because it’s <i>plastic</i>, man. Plastic, the symbol of artificiality and artifice, of soulless corporate industrialism and estrangement from wilderness, the destruction of the environment and the victory of consumer conformity, the opposite of nature and freedom and apple pie and having sex with your girlfriend’s mother and all of the other good and wholesome things that <i>The Graduate</i> stands for.</p>
<p>However, not too many years before <i>The Graduate</i>, plastic had an entirely different connotation. In the 30’s and 40’s bakelite plastic was used in luxury items from jewelry to handrails–heck, vintage bakelite jewelry from those eras 30’s and the 40’s can <i>still</i> fetch a decent price on eBay. Disney World had <a href="http://www.retrofuture.com/index.php/2009/01/21/people-who-live-in-plastic-houses-can-throw-stones-2/">a plastic demonstration “Home of the Future”</a> in which almost everything was made of plastic (and which proved to be nearly indestructible, as described in the linked article). Plastic was touted as the material of the future for everything from furniture to footwear. But most importantly, plastic was <i>new</i>, plastic was The Future, and there is nothing more good, wholesome, and American than The Future.</p>
<figure><a href="http://www.retrofuture.com/wp-content/uploads/waterprooffurniture.gif"><img alt="A future housewife cleans her couch with a hose" src="https://i1.wp.com/www.retrofuture.com/wp-content/uploads/waterprooffurniture.gif" title="Hosing down the furniture" width="400" height="227"></a><figcaption>The couch is made of plastic. At one point people thought this was a good idea.</figcaption></figure>
<p><i>The Graduate</i> came out in 1967, putting it near the beginning of the backlash against plastic that grew throughout the 60’s and the 70’s, and which by now has simply become part of our cultural background. Today, we regard the old enthusiasm for plastic as quaint and naïve, or at worst slightly evil. (Plastic is bad for the environment, after all.) It took approximately 30 years for plastic to go from being The Future to being something a crufty old man tells you to get into at a depressing cocktail party. In 1952 being against plastic was to be a hidebound reactionary. (Are you <i>against The Future</i>?) But in 1967, being against plastic was to be a progressive, a man of good taste, and on the vanguard of things to come.</p>
<p>But I didn’t really come here to talk about plastic. Instead I want to talk about the internet. Because unlike plastic, the internet is <i>new</i>. The Intenet is The Future.</p>
<p>If you want to know why the internet is The Future, you should just read <a href="http://www.shirky.com/">Clay Shirky</a>. If you don’t have time to read all of Shirky’s articles, you could just read <a href="http://www.codinghorror.com/blog/2008/05/its-clay-shirkys-internet-we-just-live-in-it.html">Jeff Atwood on Clay Shirky</a> or <a href="http://boingboing.net/2010/06/10/clay-shirkys-cogniti.html">Cory Doctorow on Clay Shirky</a>, as they all say pretty much the same thing. Basically, things used to be terrible, because there wasn’t an internet. People had to consume mass-market media and professional journalism and had no place to put pictures of their cats. Now, however, we have an internet, so <i>everyone</i> can make videos on YouTube and be a blog journalist and amuse us with cat pictures, hilariously captioned.</p>
<p>And in reality, this <i>is</i> pretty cool. YouTube amuses me at least as often as network TV used to, Wikipedia is far more useful than any dead-tree encyclopedia, Facebook keeps me in touch with family that I would otherwise rarely talk to, I wouldn’t be writing a blog without the internet, and I’ve even been known to LOL at the odd cat every now and again. I don’t dispute the massive utility of the internet, and the advantages it offers over older means of communication. However, when I read the glowing, ecstatic pronouncements of the internet evangelists (and Shirky is only one, and not even the most hyperbolic), I get this queasy feeling of deja-vu. See, we’ve been promised The Future before.</p>
<p>So here’s my prediction: in the future, the internet enthusiasm of the 90’s and 00’s will seem as quaint and misplaced as the plastic enthusiasm of the 50’s.</p>
<p>Note what this does <i>not</i> predict. I am not predicting that the internet will go away or become less important. The people who predicted that plastic would be everywhere turned out to be correct: at least half of the things on my desk right now are made out of plastic, and I suspect that it’s literally impossible to go a day in America without using a plastic product of some kind. In the same way, the future internet will probably be more ubiquitous, more limitless, and more inescapable than it is now. But this very inescapability may destroy our earlier enthusiasm for it. Once the internet has ceased to be The Future and become the present, we’ll become keenly aware of its limitations and downsides, and attuned to the laments over what we’ve lost by giving in to a world of total connectivity.</p>
<p>The problem the present-day internet evangelists is that they believe too fully in the myth of progress, which is what makes them prone to believe that the internet-enabled future will be utopian, or at least a vast improvement over the present. You may find Shirky admitting that the change from the pre-internet to the post-internet age involves some painful transitions (this is his favorite line when it comes to the newspaper industry), but this admission does not cop to the possibility that the pain will simply go on forever, that the post-internet age will simply be objectively worse than the pre-internet age in some important ways. This is abundantly clear in <a href="http://www.shirky.com/weblog/2009/03/newspapers-and-thinking-the-unthinkable/">Shirky’s “Thinking the Unthinkable”</a>, which is refreshingly blunt about the way that the internet has vitiated the old model of journalism, but nonetheless optimistic that things will eventually settle down to a newer, better status quo. Little thought is spared for the possibility that journalism in The Future may just be more sporadic, more partisan, less reliable, and less influential than the journalism of the present–that the models which the internet breaks may never be put back together, and a suitable replacement may never be found.</p>
<p>This is part of the myth of progress: any amount of destruction in the name of progress is acceptible, as all is justified as a necessary step towards The Future. And because the myth of progress is so powerful in our society, almost every new technological advancement is greeted with this same starry-eyed adoration, and all criticism of the role and nature of that technology is powerfully marginalized, at least for a while. Only once a technological change is complete, once the handmaid of The Future has proven once more to be merely the whore of the present, do the forces of criticism, reflection, and conservation begin to come into balance with the forces of progress.</p>
<figure><a href="http://xkcd.com/262/"><img alt="Hey, at least I ran out of staples" src="https://i1.wp.com/imgs.xkcd.com/comics/in_ur_reality.png" title="IN UR REALITY" width="300" height="324"></a><figcaption>IN UR REALITY</figcaption></figure><p>
This is already starting to happen with the internet. Just in the past few months I’ve read <a href="http://www.nybooks.com/articles/archives/2010/nov/25/generation-why/">an excellent, astute discussion of the social damage of Facebook, disguised as a movie review</a>, <a href="http://hnn.us/articles/133910.html">a cranky reminder that the changes wrought by the internet are not as massive as we’d like to think</a>, and <a href="http://www.theatlantic.com/technology/print/2010/12/the-hazards-of-nerd-supremacy-the-case-of-wikileaks/68217/">an impassioned defence of secrecy in the face of Wikileaks</a>. If the internet is plastic, then we’re in the mid 1960’s, when the backlash against plastic was beginning to enter the mainstream but hadn’t yet displaced the previous narrative of plastic triumphalism. And in these critiques it’s easy to see the outlines of a new consensus that may emerge once the internet has fallen out of The Future and into the present: a preference for intimacy and privacy over openness and publicity, a higher, nostalgic value given to face-to-face interations, and a distrust of the culture of the technologists that enable and promote this structure. The geeks will be the new suits, and the creepy guy telling you to go into plastics at a cocktail party will be <a href="http://www.paulgraham.com/start.html">Paul Graham</a>.</p>
<p>It will be some time before the internet completely loses the sheen of The Future. Give it at least a decade. It is nonetheless inevitable—nothing can remain in The Future forever (except maybe the vaporous Singularity), and once the internet becomes firmly rooted in the present, criticism will become fair game. More importantly, once the internet becomes part of the status quo, the myth of progress will begin to work <i>against</i> it rather than <i>for</i> it, as the status quo is by definition not progressive. The open question, it seems to me, is whether internet enthusiasm will come to be seen as merely naïve, or actually evil.</p>
<p>Quaint is the best bet. The current crop of tech-lovers are certainly not evil themselves, and we haven’t yet seen anything that is both clearly evil and fundamentally tied to internet-enabled communication. Wikileaks, though, gives us an interesting glimpse at what may be to come. If a ponce like Julian Assange can embarrass the most powerful country in the world and get away with it, then it’s possible that someone who’s smart, ambitious, and evil could do something similar in a way that would be really disastrous. Our e-Hitler could easily get the sympathy of most of the world’s hackers and geeks, who would gladly participate in an open-source world-domination project written in Python if it were framed in the right way. And if that happened, you’d better believe that the rest of the world would turn against the tech-lovers right quick.</p>
<p>That’s pretty unlikely, though. Plus, who am I kidding? I’m a geek myself, I work for a tech company and I spend all day on the internet. I’m just part of the problem.</p>
			
			
								</div>

			
	</article></div>]]>
            </description>
            <link>https://jsbangs.com/2011/01/09/the-internet-is-made-of-plastic/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26222197</guid>
            <pubDate>Mon, 22 Feb 2021 08:43:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Practiced Humility in Retrospectives]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26222018">thread link</a>) | @kiyanwang
<br/>
February 22, 2021 | https://willgallego.com/2021/02/15/practiced-humility-in-retrospectives/ | <a href="https://web.archive.org/web/*/https://willgallego.com/2021/02/15/practiced-humility-in-retrospectives/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		    
<p>One of the fallacies about our collective approach to retrospectives, incident reviews, and post mortems is the belief that the entire process is a rational machine. Pour in a curated series of events, turn the handle, and out pop all of the action items that need completing to fix the world. I can’t speak to every industry that practices Resilience Engineering, but as for Software Engineering it stems strongly from our belief that we’re fully in control of our environment. We’ve built our tooling, architected our systems, and we’re running the retro. Why wouldn’t we be able to simply apply the calculus to our knowledge and change things for the better?</p>



<p>This all speaks to a distinct lack of humility in what we do as a practice. If we want to better understand the risks we undertake every day and to learn from failures in that work, we need to first accept that failures are in part due to our incomplete understanding, great and small, of our socio-technical systems. Even with a complete working knowledge of everything, we would be unable to act on everything needed to perfect our system, and that underlying system will change despite these efforts. Being comfortable with being wrong means we can change.</p>



<p>This reluctance to accept that things continually fail despite our best efforts, is a common reaction. It’s hard to assume that our systems are continuously in need of tweaks because it’s also hard to accept that they will always run in some form of a degraded state<sup><a rel="noreferrer noopener" href="#complex-systems-run-in-degraded-mode" target="_blank">1</a></sup>. That said, we can fall into the adjacent trap with people being the adaptable element in the system<sup><a rel="noreferrer noopener" href="#human-practitioners-are-the-adaptable-element" target="_blank">2</a></sup> that we are then in the best position to understand the entirety of our system and how best to course correct. The sharp end can be a powerful, if not perilous, position to sit in but it doesn’t guarantee omniscience in the scope of understanding an incident. This is why I frequently suggest that practitioners of retrospectives be folks who weren’t involved in the incident, to help mitigate this failing.</p>



<h2>Hubris as Facilitator</h2>



<p>It’s easy to understand the desire to sit in the facilitator chair. You’re taking the reins of the situation and you’re going to get to the bottom of things. You ask the questions, you drive the conversation and schedule the meeting, but most importantly you’re going to be there to get answers. That would be true if you held a made up title like investigation commander or retrospective captain, but you’re don’t. A facilitator is less the spike and more the bump/set. You’re there to position other folks to learn, not wear the badge.</p>



<p>Retros also come in various shapes and sizes, which makes for another tempting place to be in control. If I’m running the retro, then it can follow my guidelines and my preferred flow. This lesson I learned the hard way, having felt as though I knew “the one true way” to run it. I was there at Etsy watching John Allspaw, Morgan Evans, and Daniel Schauenberg develop and put ink to paper with the <a rel="noreferrer noopener" href="https://extfiles.etsy.com/DebriefingFacilitationGuide.pdf" target="_blank">Etsy Debriefing Guide</a>. In doing so, though, I failed to recognize the microcosm that was Etsy, that what worked for us there didn’t apply universally. Maybe folks had other tools worth surfacing and we should continually look to that to see how we can improve the production of our retros.</p>



<p>Facilitators should instead be the support for everyone else to do the talking and ask questions of their own. We can only share that deep empathy with one another when we put ourselves in one another’s shoes and that can only be done with the understanding of our own fallibility. We too know how awful it feels to be at the center of an incident, that it could have easily been us, which allows us to help recreate the scene and ditch concepts like “human error” as an easy solution to a complex problem.</p>



<p>A singular view of the problem, from that up on high as facilitator, will produce a singular set of answers constrained by our myopic vantage point.</p>



<h2>Top Down Misunderstanding of Retrospectives</h2>



<p>Another failure in our work running retrospectives is senior leadership (individual contributors and management both) using them to impart the illusion of work being done. You’ll see this often in email chains that include a CC list that races its way up the reporting structure. This incident was unacceptable, but don’t worry, <a rel="noreferrer noopener" href="https://www.youtube.com/watch?v=9AKTBHuRv9U" target="_blank">we have top men working on it right now</a>. Similarly, you’ll see public facing reports sent out by companies to reassure customers, the board of directors, and investors that all is well. It’s worth noting that there is value in shared perception being a useful tool for a business to leverage, but it still doesn’t impart learning to folks on either side of the boundaries of a company. The learning review is downgraded for the sake of making an org, at the macro or micro level, appear to be invulnerable to failure. The company cannot tolerate failure because vulnerability must be made an impossibility.</p>



<p>It’s also why so many existing tools are marketed with a primary focus on action items as the work. Going into a retro with the principal desire to create a ToDo list is problematic because the learning becomes a secondary function by nature of prioritization. It has roots in humility such that we’re going into our discussions and interviews believing we can simply solve all the problems in tech and then everything will be perfect from here on, the deeper understanding a “nice to have”.</p>



<p>This is not to give individual contributors in senior positions a pass. How often do we rely on “This is the way we’ve always done it” and our use of best practices<sup><a href="#can-we-trust-best-practices">4</a></sup> as a crutch for decision making rather than challenge established methods? We give folks in senior positions more time for questions during discussions and put them at the front to answer for the sake of expedience. Equally, holding a blameless post mortem can fail if an org values engineers who prioritize their place in the pecking order rather than risk losing face in front of others. Giving less experienced folks time to explore ideas tests the validity of our mental models.</p>



<p>Most importantly, for us to build and revise adaptive capacity<a href="#building-and-revising-adaptive-capacity"><sup>3</sup></a> we have to first acknowledge that the map of our system is potentially inaccurate, a map that is heavily influenced top down. Until we can move towards an acceptance of inaccuracies in our understanding, our assumption stands that we must be right and the view should not change. All of these concepts, and our own journey to them, are themselves a <em>work in progress</em>. There are soft boundaries and holes in the middle where our language and understanding fails us. This does not inherently diminish our work, but can in fact enhance it.</p>



<h2>Humility In Practice</h2>



<p>It’s a fairly given criticism that a lot of our work in applying Resilience Engineering and Human Factors concepts to Software Engineering fail to give concrete examples of putting theory to practice, often leaving it as an exercise to the reader. With that in mind, what does humility first in a retro look like?</p>



<ul><li><strong>A retrospective is a safe place to say “I don’t know”.</strong> A facilitator can and should actively say just that while encouraging others who exhibit similar misgivings about what they can safely hold true. By doing so, it establishes a pattern of being ok with the discomfort of uncertainty.</li><li><strong>Retros should prioritize learning before fixing.</strong> This is not infrequently stated, but bears repeating. As said elsewhere, it also doesn’t exclude action items. Rather, allow folks to freely express what they don’t understand without shame and for improvements to extend from these learning experiences.</li><li><strong>A generosity of spirit is key.</strong> Participants should hold a respect to time shared for other folks to learn, with a particular emphasis on the facilitator. As invaluable as your time is, the up front cost of interviewing folks, organizing meetings, and gathering information is paramount. Put in the extra effort to interview before a retro meeting and follow up after to tie up loose ends.</li><li><strong>There should be a reduction (not an absence) on our use of hindsight.</strong> Acknowledging that we’re all fallible means we can resist the inclination to “fix” an error with counterfactuals when we review past events. Look backwards not as a way to save face but to explore why ideas previously made sense.</li><li><strong>The malleable nature of a retrospective is to review what is assumed to be true.</strong> Confirm or refute assumptions on the narrative as it is assumed to exist regardless of who shares it. Some folks may not be in a position to share, internal pressures against them. Insights often comes from the sharp end, which isn’t always the most tenured engineer.</li><li><strong>All participants should be on equal footing.</strong> Retros are akin to a round table discussion where folks come to share events and ask questions, rather than seniority or management directing the events as to how it may best serve their own interests or those assumed to be of the organization. Don’t let titles dictate who gets to speak.</li><li><strong>Our work in retrospectives is ongoing and adaptable.</strong> Before practitioners get too set in their ways, we should remember that Resilience is a verb<sup><a rel="noreferrer noopener" href="https://willgallego.com/wp-admin/post.php?post=529&amp;action=edit#resilience-is-a-verb" target="_blank">5</a></sup>. Templates are more rigid and predefined, but allowing ourselves the chance to break out of molds, to make mistakes, and explore the boundaries with the assurance that failure is ok, we can practice new ways of pulling out sources of information from our incidents. Our meta discussions surrounding incidents should themselves be challenged.</li></ul>



<p id="complex-systems-run-in-degraded-mode">1. Cook (2002) – <a rel="noreferrer noopener" href="https://how.complexsystems.fail/#5" target="_blank"><em>How Complex Systems Fail: Complex systems run in degraded mode</em></a></p>



<p id="human-practitioners-are-the-adaptable-element">2. Cook (2002) – <a rel="noreferrer noopener" href="https://how.complexsystems.fail/#12" target="_blank"><em>How Complex Systems Fail: Human practitioners are the adaptable element of complex systems</em></a></p>



<p id="building-and-revising-adaptive-capacity">3. Cook, Long (2020) – <a rel="noreferrer noopener" href="https://www.sciencedirect.com/science/article/pii/S0003687020301903" target="_blank"><em>Building and revising adaptive capacity sharing for technical incident response: A case of resilience engineering</em></a></p>



<p id="can-we-trust-best-practices">4. Klein et al (2016) – <a href="https://www.researchgate.net/publication/300343833_Can_We_Trust_Best_Practices_Six_Cognitive_Challenges_of_Evidence-Based_Approaches" target="_blank" rel="noreferrer noopener"><em>Can We Trust Best Practices? Six Cognitive Challenges of Evidence-Based Approaches</em></a></p>



<p id="resilience-is-a-verb">5. Woods (2018) – <a rel="noreferrer noopener" href="https://www.researchgate.net/publication/329035477_Resilience_is_a_Verb" target="_blank"><em>Resilience is a Verb</em></a></p>



<p><em>Photo: <a href="https://www.flickr.com/photos/eyesplash/5307049124" target="_blank" rel="noreferrer noopener">https://www.flickr.com/photos/eyesplash/53070…</a></em></p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://willgallego.com/2021/02/15/practiced-humility-in-retrospectives/">https://willgallego.com/2021/02/15/practiced-humility-in-retrospectives/</a></em></p>]]>
            </description>
            <link>https://willgallego.com/2021/02/15/practiced-humility-in-retrospectives/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26222018</guid>
            <pubDate>Mon, 22 Feb 2021 08:09:21 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Parler Is Back Online]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26221640">thread link</a>) | @jhabdas
<br/>
February 21, 2021 | https://www.ptnewsnetwork.com/parler-is-back-online/ | <a href="https://web.archive.org/web/*/https://www.ptnewsnetwork.com/parler-is-back-online/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div data-td-block-uid="tdi_83_6e0">

<div>
<p>After big-tech censorship silenced social media company Parler on January 11, 2020, they are back online with new servers as well as new leadership.</p>



<p>Over one month ago Amazon Web Services removed Parler from their servers, taking them offline.&nbsp; Google and Apple also removed them from their app stores. &nbsp; They claimed that the platform was used to “incite, organize, and coordinate the January 6 attack on the US Capitol.”</p>



<p>Defense and National Guard officials, including now former Army Secretary Ryan McCarthy, have stated in interviews that federal law enforcement authorities indicated there was activity relating to the organizing of the Capitol attack on Twitter also.</p>



<p>Parler execs said it was a war on free speech, and Amazon stated that requests to remove violent content, including death threats against public figures were ignored.</p>



<p>In addition, their co-founder and CEO, John Matze was terminated by their board on January 29 over differences in company visions.&nbsp;</p>



<p>Parler’s interim CEO Mark Meckler told<a href="https://justthenews.com/nation/culture/welcome-back-parler-resumes-social-media-app-after-securing-new-computer-servers?utm_source=breaking-newsletter&amp;utm_medium=email&amp;utm_campaign=newsletter#article"> Just the News</a> that “20 million users who were already using the app can begin logging back in on Monday, and new users should be able to sign up in approximately one week.”</p>



<p>“He also said the platform is using artificial intelligence and human editors to police for illegal speech that violates its service agreement but otherwise is remaining true to its free speech, no censorship roots.”</p>



<p>According to his<a href="https://twitter.com/MarkMeckler?s=20"> Twitter</a> account, Meckler is the President of the Convention of States Project, Co-Founder and former National Coordinator of Tea Party Patriots, Constitutional Revolutionary, Husband, Father, and Son.</p>



<p>“Parler is being run by an experienced team and is here to stay,” Meckler said in a statement. “We will thrive as the premier social media platform dedicated to free speech, privacy and civil dialogue.”</p>



<p>There are still some issues with the platform as many people are reporting the site is still not accessible, but it seems their return is imminent.&nbsp;&nbsp;</p>



</div></div></div>]]>
            </description>
            <link>https://www.ptnewsnetwork.com/parler-is-back-online/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26221640</guid>
            <pubDate>Mon, 22 Feb 2021 07:04:38 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Two Concepts of Constitutive Rules (2018) [pdf]]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 1 (<a href="https://news.ycombinator.com/item?id=26221628">thread link</a>) | @chordalkeyboard
<br/>
February 21, 2021 | https://www.argumenta.org/wp-content/uploads/2018/11/2-Argumenta-41-Jaap-Hage-Two-Concepts-of-Constitutive-Rules.pdf | <a href="https://web.archive.org/web/*/https://www.argumenta.org/wp-content/uploads/2018/11/2-Argumenta-41-Jaap-Hage-Two-Concepts-of-Constitutive-Rules.pdf">archive.org</a>
<br/><!-- hnss:readable-content --><hr/>Unable to extract article]]>
            </description>
            <link>https://www.argumenta.org/wp-content/uploads/2018/11/2-Argumenta-41-Jaap-Hage-Two-Concepts-of-Constitutive-Rules.pdf</link>
            <guid isPermaLink="false">hacker-news-small-sites-26221628</guid>
            <pubDate>Mon, 22 Feb 2021 07:01:59 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Indian Government Breached, Massive Amount of Critical Vulnerabilities]]>
            </title>
            <description>
<![CDATA[
Score 293 | Comments 65 (<a href="https://news.ycombinator.com/item?id=26221607">thread link</a>) | @astroanax
<br/>
February 21, 2021 | https://johnjhacking.com/blog/indian-government-breached-massive-amount-of-critical-vulnerabilities/ | <a href="https://web.archive.org/web/*/https://johnjhacking.com/blog/indian-government-breached-massive-amount-of-critical-vulnerabilities/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><p>A writeup detailing the vulnerability reporting process that took place after Sakura Samurai had breached the Indian Government</p><p>Reading time: 6 minutes.</p><div>
      

<p>Sakura Samurai knew that the Indian Government operated an RVDP (Responsible Vulnerability Disclosure Program). <a href="https://twitter.com/JacksonHHax" title="https://twitter.com/JacksonHHax">Jackson Henry</a> put a list together of initial assets in scope for Sakura Samurai to legally test. <a href="https://twitter.com/rej_ex" title="https://twitter.com/rej_ex">Robert Willis</a> reported that he had found sensitive data and was able to breach police assets. <a href="https://twitter.com/JacksonHHax" title="https://twitter.com/JacksonHHax">Jackson Henry</a> was working in the enumeration processes with his friend, <a href="https://twitter.com/orpheus9001" title="https://twitter.com/orpheus9001">Zultan Holder</a> [not an active Sakura Samurai member] and identified a slew of various attack vectors, immediately resulting in the exposure of many pairs of credentials for databases and other pertinent applications.</p>
<p>The team was informed of the initial enumeration results as they continued to work on the list of assets within scope, while also further jumping into the research and began performing analysis on the sensitive data, identifying additional vectors of attack, exposed PII, and even more credentials.</p>
<p>Sakura Samurai team members included <a href="https://twitter.com/JacksonHHax" title="https://twitter.com/JacksonHHax">Jackson Henry</a>, <a href="https://twitter.com/rej_ex" title="https://twitter.com/rej_ex">Robert Willis</a>, <a href="https://twitter.com/Kirtaner" title="https://twitter.com/Kirtaner">Aubrey Cottle</a>, and<a href="https://twitter.com/johnjhacking" title="https://twitter.com/johnjhacking"> John Jackson</a></p>
<p>In total, the following vulnerabilities were identified, in no specific order:</p>
<ul>
<li>35 Separate Instances of Exposed Credential Pairs (Servers, Important Applications, etc)</li>
<li>3 Instances of Sensitive File Disclosure</li>
<li>5 Exposed private-key pairs for servers</li>
<li>13K+ PII Records [and those are only the records that we were inadvertently exposed to]</li>
<li>Dozens of Exposed Sensitive Police Reports</li>
<li>Session Hijacking Chained via Multiple Vulnerabilities, resulting in the compromise of extremely sensitive government systems</li>
<li>Remote Code Execution on a sensitive financial server; a server that contained large backups of Financial Records</li>
</ul>

<p>First and foremost, it is important to note that so many Critical findings had been identified during our testing that we cannot possibly include all of the vulnerabilities without making this writeup unnecessarily heavy. Therefore, we have opted to include small snippets of repetitive findings in this section. Many variations of application and server credentials also were obtained but the point has already been made.</p>
<p><strong>Exposed Database Credentials</strong></p>
<p><img src="https://johnjhacking.com/uploads/db-creds.png" alt=""><br>
<strong>Private SSH Keys</strong></p>
<p><img src="https://johnjhacking.com/uploads/priv-ssh.png" alt=""></p>
<p><strong>Sensitive File Exposure</strong></p>
<p><img src="https://johnjhacking.com/uploads/sens-file-exp.png" alt=""><br>
<strong>Exposed PHP Mailer Credentials</strong></p>
<p><img src="https://johnjhacking.com/uploads/mailer.png" alt=""></p>

<p><a href="https://twitter.com/rej_ex" title="https://twitter.com/rej_ex">Robert Willis</a> identified an application that resulted in a vulnerability that allowed him to access Sensitive Police Records, containing PII of individuals listed on the report. In addition, sample forensic reports and forensic tooling that is used by the police department was identified by Willis. The exposure of citizen’s sensitive information, some being victims, is a sensitive subject within itself and highly alarming.</p>
<p><img src="https://johnjhacking.com/uploads/police.png" alt=""><br>
<img src="https://johnjhacking.com/uploads/police2.png" alt=""><br>
Shortly after, <a href="https://twitter.com/JacksonHHax" title="https://twitter.com/JacksonHHax">Jackson Henry</a> found a vulnerability that resulted in the exposure of 14,000+ user records. The records included a wide range of sensitive information, including full name, contact info, employee’s department, date of birth, etc. These exposed records along with other various SQL server dumps and Rob’s Police Record Exposure is enough to constitute a data breach without even logging into any of the servers.</p>
<p>Henry identified many credential pairs which could have resulted in even more exploitation of many other people. The PII identified is a small sample of a much larger issue.</p>
<p><img src="https://johnjhacking.com/uploads/14k-records.png" alt=""><br>
<a href="https://twitter.com/johnjhacking" title="https://twitter.com/johnjhacking">John Jackson</a> was able to identify a relevant Remote Code Execution Vulnerability, affecting an out-of-date application residing on one of the government servers. The remote code execution vulnerability allowed for complete access to sensitive files on the server, including the ability to exfiltrate complete backups of financial records [although data exfiltration wasn’t performed to avoid unnecessary action]</p>
<p><img src="https://johnjhacking.com/uploads/rce1.png" alt=""><br>
<img src="https://johnjhacking.com/uploads/rce2.png" alt=""><br>
Finally, <a href="https://www.twitter.com/Kirtaner" title="https://www.twitter.com/Kirtaner">Aubrey Cottle</a> identified the presence of what appeared to be an extremely important application being hosted by the same server that John had achieved successful Remote Code Execution on. Cottle then chained together multiple vulnerabilities in conjunction with the Remote Code Execution vulnerability, resulting in the ability to hijack any user’s session on the web application. The application contained troves of sensitive government data and could have given a threat actor the ability to perform highly-critical, admin-based government actions.</p>
<p><img src="https://johnjhacking.com/uploads/session-chained.png" alt=""></p>

<p>Even though the Indian Government has a RVDP in place, we didn’t feel comfortable disclosing the vulnerabilities right away. The hacking process was far from the standard situation of business-as-usual security research. In total, our report compounded to a massive 34 page report worth of vulnerabilities. We knew that our intent was good, but we wanted to ensure that the US Government had eyes on the situation. Sakura Samurai coordinated with the <a href="https://twitter.com/DC3VDP" title="https://twitter.com/DC3VDP">U.S. DoD Vulnerability Disclosure Program (VDP)</a> to assist in facilitating initial conversations of disclosure. <a href="https://twitter.com/johnjhacking" title="https://twitter.com/johnjhacking">John Jackson</a> spoke with DC3’s Program Manager via email and coordinated on a plan of action.</p>
<p><img src="https://johnjhacking.com/uploads/dc3-1.png" alt=""><br>
Roughly 4 days later, after further communication with the DC3, we felt safe to begin our initial reveal of research on the NCIIPC’s RVDP program.</p>
<p><img src="https://johnjhacking.com/uploads/dc3-2.png" alt=""></p>
<p>In addition, the DC3 also commended the hacking that we did in support of making the cyberspace a better place for everyone.</p>
<p><img src="https://johnjhacking.com/uploads/dc3-3.png" alt=""></p>
<p>Unfortunately, what seemed like a done deal turned out to be quite the unprofessional ride. Any organization knows that fixing breach-worthy vulnerabilities is extremely time sensitive. Once threat actors catch wind of major vulnerabilities against an organization they begin poking on their own, looking for more vectors of attack. Immediately upon revealing that Sakura Samurai was the group responsible for hacking the Indian Government, we followed up with them via Email.</p>
<p><strong>Timeline</strong></p>
<p><strong>2021/02/04</strong> - DC3 begins initial contact with the Indian Government.<br>
<strong>2021/02/08</strong> - Sakura Samurai informs the public that they breached the Government.<br>
<strong>2021/02/08</strong> - Sakura Samurai makes contact with the NCIIPC, noting that the report that they received was a result of their research.<br>
<strong>2021/02/09</strong> - The NCIIPC responds, with a basic acknowledgement and thank you for the research.<br>
<strong>2021/02/09</strong> - Sakura Samurai asks for clarification on patching and the responsibility of breach disclosure to the public.<br>
<strong>2021/02/10</strong> - Sakura Samurai, having received no response, asks for an update on the involved remediation and breach notification processes.<br>
<strong>2021/02/16</strong> - Sakura Samurai once again asks for NCIIPC’s plans for remediation and disclosure.<br>
<strong>2021/02/17</strong> - The NCIIPC makes contact, 7-days later, stating that they will follow up in a short time. Again, we ask about plans of anticipated patching and breach notification to the affected citizens.<br>
<strong>2021/02/19</strong> - In the morning, we ask again about patching and disclosure, 8-hours later and still no response on the matter.<br>
<strong>2021/02/19</strong> - Sakura Samurai reviews the submitted vulnerability report and notes that only about an eighth or less of the submitted Critical Vulnerabilities have been resolved within a two-week period. No notification of breach has occurred even though Government Employees and Indian Citizens are at risk of exploitation from threat actors.</p>

<p>Governments have an obligation to protect the private data of its employees and citizens. In addition, the exposure of proprietary government data can be used for great means of manipulation and for other destructive purposes. While the NCIIPC operates a Responsible Vulnerability Disclosure Program, the recklessness and avoidance of communication represents the complete opposite of a responsible program. A failure to release notification of breach to affected citizens and to patch highly-critical vulnerabilities in a timely manner reflects poorly on the state of their Information Security posture. The clock to patch vulnerabilities began immediately when the DC3 contacted the NCIIPC via Twitter, as it is a highly visible space - one which threat actors avidly monitor.</p>
<p>Sakura Samurai urge the NCIIPC to patch the remainder of the vulnerabilities. The criticality of some of the issues cannot wait weeks or months for adequate resolution.</p>
<hr>
<p><strong>Check out our website</strong><br>
<a href="https://sakurasamurai.org/" title="https://sakurasamurai.org">https://sakurasamurai.org</a></p>
<p><strong><em>Twitter Links:</em></strong><br>
Main Page<br>
<a href="https://twitter.com/SakuraSamuraii" title="https://twitter.com/SakuraSamuraii">https://twitter.com/SakuraSamuraii</a><br>
Founders<br>
<a href="https://twitter.com/johnjhacking" title="https://twitter.com/johnjhacking">https://twitter.com/johnjhacking</a><br>
<a href="https://twitter.com/nicksahler" title="https://twitter.com/nicksahler">https://twitter.com/nicksahler</a><br>
Members<br>
<a href="https://twitter.com/JacksonHHax" title="https://twitter.com/JacksonHHax">https://twitter.com/JacksonHHax</a><br>
<a href="https://twitter.com/Kirtaner" title="https://twitter.com/Kirtaner">https://twitter.com/Kirtaner</a><br>
<a href="https://twitter.com/rej_ex" title="https://twitter.com/rej_ex">https://twitter.com/rej_ex</a><br>
<a href="https://twitter.com/endingwithali" title="https://twitter.com/endingwithali">https://twitter.com/endingwithali</a><br>
Collaborator<br>
<a href="https://twitter.com/orpheus9001" title="https://twitter.com/orpheus9001">https://twitter.com/orpheus9001</a></p>

    </div></div>]]>
            </description>
            <link>https://johnjhacking.com/blog/indian-government-breached-massive-amount-of-critical-vulnerabilities/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26221607</guid>
            <pubDate>Mon, 22 Feb 2021 06:57:33 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Data Immutability, Verifiability and Integrity Without the Blockchain Overhead]]>
            </title>
            <description>
<![CDATA[
Score 38 | Comments 9 (<a href="https://news.ycombinator.com/item?id=26221324">thread link</a>) | @sidcool
<br/>
February 21, 2021 | https://techtake.info/2021/02/16/data-immutability-verifiability-and-integrity-without-the-blockchain-overhead/ | <a href="https://web.archive.org/web/*/https://techtake.info/2021/02/16/data-immutability-verifiability-and-integrity-without-the-blockchain-overhead/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
		
<p>Whenever a software project needs to implement immutable records, people often start thinking of Blockchain or <strong>D</strong>istributed <strong>L</strong>edger <strong>T</strong>echnology like Hyperledger. Blockchain and DLT make use of cryptographic techniques that enable immutability, verifiability and integrity checks. However, Blockchain and DLT need much more than just these checks. It needs to prevent all the attempts of a double-spend by potential malicious users. In a trust-less environment it needs implementation of complex protocols which leads to consumption of huge amount of electricity. This makes writing data on Blockchain costly. DLTs like Hyperledger make use of simpler consensus algorithms between a “set of trusted nodes”, which reduces the cost. However, it still incurs significant costs if nothing more than immutability, verifiability and integrity were the concern.</p>



<p>This post is about how to implement data immutability, verifiability and integrity without using a Blockchain or a DLT in industrial strength software applications.</p>



<ul><li><a href="#foundation">Foundational technique</a><ul><li><a href="#hash-function">Cryptographic hash function</a></li></ul></li><li><a href="#verifiability">Verifiability</a><ul><li><a href="#verify-id">Deterministic and Verifiable IDs</a></li><li><a href="#data-format">Data format and ID Generation</a></li></ul></li><li><a href="#immutability">Immutability and Verifiability</a><ul><li><a href="#merkel-dag">Merkel DAG and immutable data structures</a></li><li><a href="#mutation">Mutation</a></li></ul></li><li><a href="#integrity">Immutability, Verifiability and Integrity</a><ul><li><a href="#application">More applications</a></li></ul></li><li><a href="#future-proof">Future-proofing</a><ul><li><a href="#multi-hash">Multihash</a></li></ul></li></ul>



<h2 id="foundation">Foundational technique – Cryptographic Hashing</h2>



<p>To understand the solution, some foundational techniques must be understood. This section describes what is cryptographic hashing. Those who are already aware of cryptographic hashing, they may skip to the next section.</p>



<h4 id="hash-function">Cryptographic hash function</h4>



<p>If you provide a stream of bytes to a cryptographic hash function, it generates a number called hash (also referred to as digest). The following properties make it a very useful tool:</p>



<ol><li>It is impossible to guess the generated hash value for a stream of bytes. To get the hash value, one has to run the algorithm, there is no shortcut.</li><li>For a given input it always generates the same hash value.</li><li>It is infeasible to deduce the input based on the hash value. That means it is an irreversible mathematical function.</li><li>No two different stream of bytes result in the same hash value. Even a small change in the input stream generates a totally different number.&nbsp;<em>(When two different stream of bytes produce the same hash value, we say the cryptographic hash function is broken. It is also referred to as there is a collision in the cryptographic hash function.)</em></li><li>Any size of input stream will always result in the same size of hash value. Some hash functions generate hash values that are 256 bits long. If those functions are used, the result will always be 256 bits long.</li></ol>



<p>Examples of commonly used cryptographic hash functions include:</p>



<ul><li>SHA-256 (returns 256 bit&nbsp;unsigned integers)</li><li>RIPEMD-160 (returns 160 bit unsigned integers)</li></ul>



<p>Sample hash values:</p>



<figure><table><tbody><tr><td><strong>Input</strong></td><td><strong>SHA-256</strong></td><td><strong>RIPEMD-160</strong></td></tr><tr><td>Hello world</td><td>0x64ec88ca00b268e5ba1a35678a1b5316d212f4f366b2477232534a8aeca37f3c</td><td>0xdbea7bd24eef40a2e79387542e36dd408b77b21a</td></tr><tr><td>Hello world.</td><td>0xaa3ec16e6acc809d8b2818662276256abfd2f1b441cb51574933f3d4bd115d11</td><td>0x6ad34a17d22d67a7ab02710ae9eb6f282cb1d787</td></tr><tr><td>Unrelated, totally.</td><td>0x2bd72f5c4300444890325b3363ef2027f30ed38797c3133dbc62a90564976458</td><td>0x51cb1844d22a00d5f659795e0b1c339c6fa1a8bc</td></tr></tbody></table><figcaption>Hex representation of SHA-256 and RIPEMD-160 hash values for different inputs</figcaption></figure>



<p>There are two things we observe from the table above:</p>



<ol><li>With a slight change in input, the hash values change dramatically and by looking only at the hash values, one cannot conclude&nbsp;that&nbsp;the first and second are&nbsp;even closely related. This is also referred to as <a href="https://en.wikipedia.org/wiki/Avalanche_effect" target="_blank" rel="noreferrer noopener">avalanche effect</a>. It is one of requirements of a cryptographic algorithms to have the avalanche effect.</li><li>The values mentioned are actually text strings and do not look like numbers, although we expected them to be numbers.</li></ol>



<p>They are actually numbers, represented this way to reduce the size of the presented text. For example, binary representation of the number 255&nbsp;is ‘11111111’. Decimal representation is ‘255’. Hexadecimal representation is ‘ff’ or ‘FF’. The representations in the table above are hexadecimal representations of 32 byte and 20 byte numbers. <a rel="noreferrer noopener" href="https://en.wikipedia.org/wiki/Base64" target="_blank">base64encoding</a> is commonly used to represent the hash values in cryptographic applications.</p>



<h2 id="verifiability">Verifiability</h2>



<p>For quite some time, open source software has been distributed through various mirror sites so that downloads are sped up. Any user could download the software package quickly from a nearby mirror site. These nearby sites could be malicious and could provide compromised open source software packages. In order to overcome this problem, open source software builds would publish a checksum file on their website. This checksum file is used to verify that the downloaded package from a nearby mirror site is authentic. The checksum file actually contains a cryptographic hash of the software package.</p>



<p>For example: <a rel="noreferrer noopener" href="https://www.openoffice.org/download/checksums/3.4.1_checksums.html" target="_blank">Apache OpenOffice – Download checksum files</a>.</p>



<pre>e08f9c8acecba1ee0046f820b0abed97dfe90511bd733a65936fdf0ea9c22540  Apache_OpenOffice_incubating-SDK_3.4.1_Linux_x86-64_install-rpm_en-US.tar.gz</pre>



<p>This is the content from SHA256 checksum file for <a href="http://archive.apache.org/dist/incubator/ooo/files/stable/3.4.1/Apache_OpenOffice_incubating-SDK_3.4.1_Linux_x86-64_install-rpm_en-US.tar.gz.sha256" target="_blank" rel="noreferrer noopener">Apache Open Office SDK for Linux x86-64</a> build.</p>



<p>It would not matter which mirror site the SDK is downloaded from. A user can easily verify the authenticity of the download using a simple command like:</p>



<pre><code>$ sha256sum Apache_OpenOffice_incubating-SDK_3.4.1_Linux_x86-64_install-rpm_en-US.tar.gz
e08f9c8acecba1ee0046f820b0abed97dfe90511bd733a65936fdf0ea9c22540  Apache_OpenOffice_incubating-SDK_3.4.1_Linux_x86-64_install-rpm_en-US.tar.gz</code></pre>



<p>If the generated SHA256 does not match with the checksum file, the user knows that the software package has been tampered.</p>



<p>We will use the same technique to verify data objects. The data object we are interested in would have an ID. Any application or service asks for objects using IDs. If the data that the application gets does not generate the same hash value as the ID, then the application can easily conclude that the data is not what was asked for or the data has mutated. On a typical hardware the cryptographic hash function would take a few microseconds to generate the hash value. So, this is not costly and offers good verifiability.</p>



<p>In the context of this post, the verifiability that we are seeking is not because we operate in a trust-less or adversarial environment like Blockchains, but mainly because data can become corrupt or can be deliberately changed by hackers / attackers. This is for companies to verify that the data they hold has not been modified undesirably.</p>



<h4 id="verify-id">Application: Deterministic and Verifiable IDs</h4>



<p>In a typical RESTful request, a client posts a request to a service, and the service returns back an ID. The service uses the ID to index the object that got created due to the request. However, with a predetermined ID generation technique, it is possible for the client to know the ID even before the service receives the request. This is done even in blockchains. The transactionID (also referred as <a href="https://wiki.bitcoinsv.io/index.php/TXID" target="_blank" rel="noreferrer noopener">TXID</a>) is a hash computed from certain fields of a transaction request.</p>



<p>To be able to know the ID of an object that will be returned by a service even before the object is created in a service has significant benefits. </p>



<ol><li>Just by computing the ID from the fields of an object, and comparing it with the ID provided in the object, one can determine if the object is the right object. Housekeeping processes can easily determine data corruption or software bugs or potential attacks.</li><li>Request need not be processed synchronously.</li><li>The client can fire a batch of requests in just one call. Each individual request can easily be identified by the request id which will be deterministic for both the client and the service. This eliminates the need to create IDs on the client side, and map them to the server side IDs.</li><li>Non-idempotent requests such as HTTP POST requests can achieve deterministic behaviour. Multiple POST requests (which could be because of software bug or infrastructural replays) will not cause harm, as the request ID is predetermined. The server can easily identify a duplicate request.</li></ol>



<p>Example: To generate an OrderID, one could take the sha256 of a series of bytes of the quantity, price, dateTime of the order, clientID, and the assetID or assetSymbol.</p>



<pre><code>OrderID = sha256(bytes(qty)||bytes(price)||bytes(dateTime)||bytes(clientID)||bytes(assetID))</code></pre>



<h4 id="data-format">Data format and ID Generation</h4>



<p>While the scheme above for ID generation works, it has a certain drawback. For every type of objects, a developer would have to write an ID generator. This is not desirable. For a majority of the types of objects, the id generator should just be available easily. For this, the object itself can be serialised and the serialised stream of bytes can be hashed.</p>



<p>It is important to note that text based data structures like JSON, XML are not very well suited for this. The main reason behind this is that adding a space or TAB within the document will not alter the data for JSON or XML, however will yield a totally different hash value and therefore a totally different object ID. Hence, it is better to use serialisation formats designed for cross platform, multiple language environments and are deterministic. Compact data serialisation like <a rel="noreferrer noopener" href="https://developers.google.com/protocol-buffers" target="_blank">protocol buffers</a>, <a rel="noreferrer noopener" href="https://google.github.io/flatbuffers/" target="_blank">FlatBuffers</a>, <a rel="noreferrer noopener" href="https://avro.apache.org/" target="_blank">Apache Avro</a> and even <a rel="noreferrer noopener" href="https://tools.ietf.org/html/rfc7049" target="_blank">CBOR</a> are much better suited for this.</p>



<h2 id="immutability">Immutability and Verifiability</h2>



<p>Software professionals often jump to Blockchain to achieve immutability even in a non-adversarial environment like most business applications. Any party explicitly trying to cheat would face the court, and fraudulent transactions can be reverted in the most common business applications seen throughout the world. Therefore, there is no need for all the complexity and consensus algorithms like proof-of-work or proof-of-stake. Even on DLTs there are algorithms like Raft which are used to achieve consensus. Although much lesser in the power consumption, raft could still be an overkill for some …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://techtake.info/2021/02/16/data-immutability-verifiability-and-integrity-without-the-blockchain-overhead/">https://techtake.info/2021/02/16/data-immutability-verifiability-and-integrity-without-the-blockchain-overhead/</a></em></p>]]>
            </description>
            <link>https://techtake.info/2021/02/16/data-immutability-verifiability-and-integrity-without-the-blockchain-overhead/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26221324</guid>
            <pubDate>Mon, 22 Feb 2021 05:55:49 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Worlds Beyond Ours: Extending human habitability to outer space]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26221303">thread link</a>) | @brandonlc
<br/>
February 21, 2021 | https://www.noemamag.com/worlds-beyond-ours/ | <a href="https://web.archive.org/web/*/https://www.noemamag.com/worlds-beyond-ours/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

				<div>
  <p>Credits</p>
  <p>Claire Isabel Webb is a historian and anthropologist of science, and a 2020-21 Berggruen Institute fellow.</p>
</div>


<p>Consider a trio of moments of entangled spacetime:</p>



<p>Jan. 7, 2021, cyberspace and Washington, D.C.: A staggering 4,112 people <a href="https://www.nytimes.com/2021/01/18/us/coronavirus-deaths.html" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">die</a> of the coronavirus in America, a new record. Elon Musk becomes the richest person in the world and reiterates his plan to leave Earth and start a colony on Mars.</p>



<p>Dec. 7, 1972, near-Earth orbit: The crew of <a href="https://svs.gsfc.nasa.gov/vis/a000000/a002600/a002680/" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">Apollo 17</a>, 18,000 miles from home and on their way to the moon, snap a photograph of Earth. Illuminated by the sun and nestled in the sable sea of outer space, the “Blue Marble” image becomes a resonant icon of humans’ dear and fragile life-filled planet. Earth’s denizens wonder: Are there other worlds beyond? Or is this the only example of life in the universe?</p>



<p>Nov. 7, 1957, Calcutta, India: Two prominent biologists, Joshua Lederberg and J.B.S. Haldane, meet for dinner. A month earlier, the Soviet Union had launched Sputnik I, the first artificial satellite to orbit Earth. Lederberg and Haldane <a href="https://profiles.nlm.nih.gov/spotlight/bb/catalog/nlm:nlmuid-101584906X13253-doc" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">determine</a> that a thermonuclear bomb detonated on the moon would be visible to Earthlings, and it would be so destructive it would spoil the possibility of finding traces of lunar life.</p>



<p>Lifted from three interspaced epochs of the ongoing Space Age — the COVID-19 pandemic, the environmental movement and the Cold War — those moments reveal how terrestrial troubles are entwined with hopes of discovering life, and of living, beyond Earth. As dreams to explore the cosmos curl skyward, fears and anxieties particular to each moment raise doubts not only about humans’ longevity on our home planet, but also about how we might inhabit and sustain life on other worlds as space-faring explorers. If humans self-destruct through nuclear war, poison the planet by churning out carbon into the atmosphere or fail to control a deadly virus, such events would preclude us from existing on Earth, living long enough to communicate with possible extraterrestrial beings and venturing to other worlds we might discover to be habitable.</p>



<p>Thus, fears of terrestrial apocalypse animate pursuits for life and living beyond Earth. But conversely, imagining how life (including human life) might exist in an extraterrestrial context, and seeing the planet from outer space, has driven imaginations of Earth’s possible futures — both hopeful, course-correcting pathways, but also escapist fantasies of extraplanetary colonization.</p>



<p>Anticipations of worlds <em>beyond</em> Earth — places that might be (or might be made to be) habitable — are made possible by conceiving <em>of</em> Earth as both threatened and interconnected: The coronavirus’s march across the world reveals the viruses’ disregard for political borders, the environmental movement highlighted the fragility of the planet’s entangled life and the Cold War ushered in the concept of global nuclear disaster.</p>



<p>These threats have, in different ways, revealed how actions are never self-contained in global, networked systems. Each moment’s particular planetary anxieties — pathogenic, climate, nuclear — have animated and informed scientists’ pursuit of extraterrestrial life.</p>


<!-- Quote Block Template -->

<div>

  <div>

    <p>
      “Information on whether a complete lifecycle can occur in space would also have obvious implications for the feasibility of eventual colonization of space.”    </p>

          
    
    
  </div>
</div>




<h5><strong>Annihilation</strong></h5>



<p>On Oct. 4, 1957, Sputnik I streaked across the sky. Touching off the “space race” between the United States and the Soviet Union, the satellite represented the opposition between democracy and communism. “Artificial earth satellites will pave the way to interplanetary travel,” the Communist Party’s official newspaper <a href="https://history.nasa.gov/sputnik/14.html" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">announced</a> the following day, and “our contemporaries will witness how the freed and conscientious labor of the people of the new socialist society makes the most daring dreams of mankind a reality.”</p>



<p>Sputnik I was particularly visible from the southern hemisphere, where Nobel Prize-winning microbiologist Joshua Lederberg happened to be traveling. A month later, on his way back to Stanford University, where he taught and researched, Lederberg passed through Calcutta to visit his friend and collaborator J.B.S. Haldane. Haldane had formulated the “<a href="https://www.uv.es/~orilife/textos/Haldane.pdf" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">primordial soup</a>” model — how life could have originated from <span data-note="Abiogenic means not produced by the activity of living organisms; abiogenesis is the process by which life could have evolved from non-living materials.">abiogenic</span> materials on an ancient Earth. Both scientists looked forward to that night’s lunar eclipse.</p>



<p>Over dinner, Haldane — a “confirmed Communist” and “radical alternativist,” according to Lederberg — “gloated” that it was also the 40<sup>th</sup> anniversary of the October Revolution, the event that had precipitated the formation of the Soviet Union. As a thought experiment, the two scientists wondered: What if the Soviets leveraged the symbolic occasion to plant a “red star” — a nuclear bomb — on the moon? Their back-of-the-napkin calculation revealed that it would be visible from Earth.</p>



<p>Of course, there was no “red star” that evening, and the U.S. astronauts of the Apollo 11 mission, not Soviet cosmonauts, would be the first to land on the moon twelve years later, in 1969. But the conversation with Haldane about the possibility for off-Earth atomic destruction spurred Lederberg toward the study of “exobiology,” the search to detect and preserve life beyond Earth. As the U.S. and the Soviet Union’s space race accelerated during the Cold War, the National Academy of Sciences established the Space Science Board (SSB) to research outer space and to recommend policies to NASA. That group, which included Lederberg and other prominent scientists (among them a young Carl Sagan), worked to protect the moon and other extraterrestrial sites as scientific laboratories.</p>



<p>Looking ahead to possible NASA missions that would explore Mars and Venus for traces of life, a 1959 SSB report that Lederberg chaired transported Cold War fears of nuclear war on Earth to celestial bodies beyond. It warned that “the effect of introducing radioactivity on another planet where there may be entirely different levels of background radiation from those found on Earth could greatly influence any form of life found there.” Planetary concerns of atomic fallout migrated to unexplored sites beyond our planet.</p>



<p>In addition to nuclear radiation on other planets, the possibility of microbial contamination presented risks in the search for life beyond Earth. A spacecraft landing on Mars, for example, might bring terrestrial hitchhikers, risking a false detection of organic biochemistry that would muddle attempts to theorize the origin of life in the solar system and possibly the cosmos beyond. Throughout the late 1950s and the 60s, exobiologists’ reports urged sterilization protocols be taken so as to preserve possible “planetary biota” on Mars.</p>



<p>At the same time, exobiologists worried that possible Martian microbes might infect Earth; through incautious activity by either the U.S. or the Soviet Union, a “dramatic hazard would be the introduction of a new disease, imperiling human health,” as Lederberg wrote in 1960. This particular threat took center stage in Michael Crichton’s 1969 science fiction book (and subsequent film) “The Andromeda Strain,” in which a mysterious and fatal extraterrestrial microorganism appears in Arizona and threatens to end life on Earth. Merging apocalypses, the characters consider annihilating the infected laboratory with a nuclear bomb.</p>



<p>Civilian scientists’ goals to detect extraterrestrials were often <a href="https://www.jstor.org/stable/10.1086/344962?seq=1" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">at odds</a> with those of the national agencies they answered to. While John F. Kennedy’s 1962 “<a href="https://er.jsc.nasa.gov/seh/ricetalk.htm" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">We choose to go to the moon</a>” speech mandated the priority of manned missions to outer space that would showcase national prestige, exobiologists advocated for international efforts to preserve (extra)terrestrial life forms. A 1961 SSB report suggested that the U.S. and the Soviet Union work together on sterilization protocols to “simplify the problem of protection against possible contamination of the planets and of the Earth.” The planetary struggle for political dominance, which threatened to plunge Earth into a nuclear apocalypse, was thus shaping extraplanetary pursuits.</p>



<p>As they considered Earth and extraterrestrial sites of possible life (Mars’s subsurface, Venus’s atmosphere and even, possibly, the moon’s dust) in tandem, exobiologists began to imagine interconnected, but distinct, planetary wholes. Linking Earth to planets beyond, two exobiologists wrote in a 1961 <a href="https://www.nap.edu/read/12425/chapter/1" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">report</a>, “The planets of the solar system are part of a whole — in their origins, in their present states and in their futures.” Such exercises that forecasted other worlds soon came to intersect with growing concerns about the fragility of our own planet.</p>


<!-- Quote Block Template -->

<div>

  <div>

    <p>
      “Musk has reshuffled space exploration: individualism over nationalism, money power over patriotism, the adventure or even salvation of the few over the many.”    </p>

    
    
  </div>
</div>




<h5><strong>Interconnection</strong></h5>



<p>Amid the persistent threat of nuclear apocalypse that defined the Cold War era, exobiologists began to call for planetary protection protocols for both Earth and extraterrestrial sites — concerns that became increasingly aligned with a burgeoning consciousness about humans’ harmful activities on Earth. Rachel Carson’s 1962 book “Silent Spring” introduced the idea that synthetic chemicals, especially pesticides — which she <a href="https://archive.nytimes.com/www.nytimes.com/books/97/10/05/reviews/carson-obit.html?_r=2" data-wpel-link="external" target="_blank" rel="external noopener noreferrer">argued</a> should be called “biocides” — were fundamentally altering life on Earth. The ensuing environmental movement of the 1960s and 70s culminated in the creation of the U.S. Environmental Protection Agency and made “the environment” a widespread public concern, fortifying the concept that life systems were interconnected, malleable and fragile.</p>



<p>Stewart Brand’s “Whole Earth Catalog” often advocated for ecological issues and <a href="https://www.noemamag.com/the-origins-of-planetary-realism-and-whole-earth-thinking/" data-wpel-link="internal">featured</a> images of Earth from space on its early covers, from a mosaic made of satellite photos to Apollo 8’s “Earthrise.” Images of Earth from outer space cast it as a planetary …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.noemamag.com/worlds-beyond-ours/">https://www.noemamag.com/worlds-beyond-ours/</a></em></p>]]>
            </description>
            <link>https://www.noemamag.com/worlds-beyond-ours/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26221303</guid>
            <pubDate>Mon, 22 Feb 2021 05:50:47 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Europe looks to go it alone on microchips amid US-China clash]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26221255">thread link</a>) | @nl
<br/>
February 21, 2021 | https://www.politico.eu/article/europe-seeks-to-decouple-from-us-china-chip-war/ | <a href="https://web.archive.org/web/*/https://www.politico.eu/article/europe-seeks-to-decouple-from-us-china-chip-war/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
									
							<div id="amazon-polly-audio-table">
				<p>Press play to listen to this article</p>
				
		</div>
<p>Europe is caught in the middle of an increasingly political showdown over microchips between the U.S. and China and is scrambling to get out of the firing line.</p>



<p>While Europe is a heavyweight at making planes and cars, it is a minnow when it comes to the chips that are vital to swaths of high-end manufacturing. </p>



<p>Europe accounts for only about <a href="https://ec.europa.eu/commission/presscorner/detail/en/SPEECH_20_1362" target="_blank">10 percent of the world's chip industry</a>, and the Continent is poorly prepared for supply shocks. In the past few weeks, politicians and businessmen in Brussels, Paris and Berlin were caught off guard by how quickly supply disruptions in the semiconductor industry reduced output at the crucial <a href="https://europe.autonews.com/automakers/global-microchip-shortage-hits-honda-nissan-german-automakers" target="_blank">car industry</a>. </p>



<p>The calculus for Europe on this vulnerability is as much political as it is economic, and has laid bare Europe's dependence on America's top-end chipmakers. </p>



<p>The U.S. has already restricted the supply of its premium semiconductor products to Chinese companies such as Huawei, sparking fears among European businesses about how far Washington will go to keep key U.S. chip technology out of China. For EU companies trading with China and manufacturing there, the chief concern is that they could be caught up in this fight and be frozen out of irreplaceable U.S. semiconductor supply markets by export controls.</p>



<p>A separate shock to the car sector’s supply of chips has heightened alarm about Europe’s reliance on foreign players in past weeks, with semiconductor manufacturers, especially in Asia, failing to keep up with demand. </p>



<p>This shortage of chips has caused disruption at Volkswagen's headquarters in Wolfsburg — one of the largest car plants in the world. It's also being felt at factories across Europe, and carmaker CEOs expect that the problem will continue through the first half of this year.</p>



<p>“The semiconductor supply bottleneck resulting from the rapid recovery of automotive markets is causing significant disruptions in global vehicle production for various manufacturers,” VW said on Friday.  </p>



<p>Semiconductor makers have been racing to satisfy demand from the lucrative consumer electronics industry in the coronavirus pandemic as the auto market sagged, creating a supply crunch.</p>



<p>This week, European policymakers identified the chip shortages as a key strategic concern and presented plans to deal with it.  </p>



<p>"There is currently a game underway between the United States and China … and it is likely to continue to get tougher," the EU's Internal Market Commissioner Thierry Breton told reporters this week. "We in Europe intend to play our full part in this new geostrategic game of chess."</p>



<p>"I say it clearly, in the coming years we will see a certain number of tensions ... in the field of semiconductors, that can have implications, including geopolitical ones," Breton added. "In fact, we can already see that ... We see it in particular with the large Chinese companies that today suffer from the lack of these components, so let's not be naïve."</p>



<p>France's Economy Minister Bruno Le Maire, speaking alongside Breton, <a href="https://www.vie-publique.fr/discours/278594-bruno-le-maire-15022021-strategie-industrielle-de-lunion-europeenne" target="_blank">said</a> Europe "wants to be an industry power. We want to be an independent Continent when it comes to technology." He added Europe's reliance on foreign suppliers "is excessive and unacceptable. It makes us vulnerable. It weakens our production chains today [since] tens of thousands of cars are not produced for lack of electronic components." </p>



<p>On Tuesday, Germany and France <a rel="noreferrer noopener" href="https://minefi.hosting.augure.com/Augure_Minefi/r/ContenuEnLigne/Download?id=C7D8444A-FD4B-4BFE-A6FF-E9B68E8625B3&amp;filename=Position%20commune%20FR-ALL%20-%20Industrial%20Strategy.pdf" target="_blank">published a paper calling</a> for "a first set of measures" to "reduce, where relevant, strategic dependencies." It follows <a href="https://www.politico.eu/article/germany-huawei-telecoms-plan/">earlier support</a> from Berlin to set up a joint European industrial project, and diplomatic efforts by the European Commission to <a href="https://www.politico.eu/article/europe-microchip-technology-autonomy-production-china-semiconductors/">launch an "alliance"</a> of companies and governments to pour money into the semiconductor industry. </p>



<p>German tech lobby Bitkom said its members feared they were too dependent on foreign suppliers and universally backed initiatives for greater digital sovereignty. Some 94 percent wanted Germany to push for the <a href="https://www.bitkom.org/Presse/Presseinformation/Deutsche-Wirtschaft-strebt-nach-mehr-digitaler-Souveraenitaet" target="_blank">EU to be on a level with China and the U.S.</a>, according to a poll of 1,100 medium and large companies released on Thursday.</p>



<p>The Commission even has dreams of setting up a leading factory for the most sophisticated chips — though industry officials have greeted that idea with skepticism. </p>



<h3>All out of chips</h3>



<p>The chip supply chain ran into a storm last year. </p>



<p>Under the Trump administration,<strong> </strong>China hawks in Washington identified the chip sector as an Achilles' heel in China's rise. While Beijing has proven successful in its strategies to overtake rivals on technologies like smartphones, solar panels, consumer tech, artificial intelligence technologies and more, the country has struggled to replicate or acquire some of the cutting-edge technologies needed to produce the most advanced microchips.</p>



<p>The Americans moved in on the weak spot. U.S. officials slapped <a href="https://www.commerce.gov/news/press-releases/2020/05/commerce-addresses-huaweis-efforts-undermine-entity-list-restricts" target="_blank">new restrictions</a> on chipmakers doing business with China's telecoms giant Huawei in May 2020. In December, Washington <a rel="noreferrer noopener" href="https://www.ft.com/content/7dcc105e-986b-4768-9239-9f8fa9073b53" target="_blank">barred U.S. chip designers from doing business with China's state-owned manufacturer SMIC</a>.</p>



<p>These measures took place in a world thrown into turmoil by the coronavirus pandemic. Demand for microchips for consumer products like computer screens, headphones, laptops and smartphones soared while car sales collapsed, prompting carmakers to cancel chip orders.</p>



<p>But Europe's car factories quickly found themselves short of chips with no capacity to produce them. </p>



<p>“Volkswagen has to make sure that wafer and semiconductor manufacturers also know our needs," the company said.</p>



<p>The car industry's worries extend beyond the immediate impact of the pandemic on their supply chains. Consumer electronics and telecoms products are expected to boom in coming years and the small, cutting-edge chips that power these devices are more profitable for chip manufacturers to make. </p>



<p>The crisis has pointed to the reliance on U.S. chip designers and Taiwanese manufacturers to keep up with global demand.</p>



<h2>Breton's clash with industry</h2>



<p>Buffeted by the U.S. and China trade war more generally, EU countries and officials in Brussels are cooking up wide-reaching plans for EU "strategic autonomy" and to reshore everything from masks and vaccines to lithium batteries. </p>



<p>Now chip factories are also part of those plans, spurred by the supply shortage.</p>



<p>Breton said that both French President Emmanuel Macron and German Chancellor Angela Merkel support his work to set up an "alliance for semiconductors" that would support local chip firms and would also funnel public cash into building up production capacity in Europe. The alliance would be launched as soon as April, officials involved in the work <a href="https://www.politico.eu/?p=1572339">said earlier</a>.</p>



<p>In a presentation given to national diplomats by the Commission earlier this month, and seen by POLITICO, officials promised funding from its Recovery and Resilience Facility to rebuild the economy after the pandemic. It also sought support from national capitals to set up an Important Project of Common European Interest (IPCEI) on microchips, a special funding scheme to allow state aid to critical technologies and industries. </p>



<p>That IPCEI already&nbsp;<a href="https://www.politico.eu/article/germany-huawei-telecoms-plan/#:~:text=German%20draft%20proposal%20would%20subsidize%20smaller%20firms%20to%20enter%205G%20market.&amp;text=The%20German%20government%20is%20preparing,dominant%20suppliers%20like%20China's%20Huawei.">won support from the German government</a>&nbsp;earlier this month. “We want Germany and Europe to become more sovereign and independent of imports when it comes to microelectronics and communication technologies," German Economy Minister Peter Altmaier said when announcing the government's intention to join the scheme.</p>



<p>Europe does have some leaders in niche parts of the supply chain. Dutch chip printing equipment-maker ASML holds a global monopoly on the machines that enable foundries to print the latest generations of microchips. And firms like the Dutch-American NXP and German Infineon lead in designing chips for sectors including automotive.</p>



<p>But for Breton, Europe's autonomy will depend on having a leading-edge factory too.<strong> </strong></p>



<p>"We must give ourselves the means to be autonomous on this chain," he said, with the ambition to manufacture the tiny chips used in smartphones and other high tech.</p>



<p>But that's where the European commissioner could lose support of its leading industry players, insiders warn.</p>



<p>Breton’s idea of a foundry that manufactures the most sophisticated generations of chips “is a bridge too far," said one industry official who is involved in discussions with European governments. "The gap is pretty wide between what Breton has in mind and what the industry can deliver without committing financial suicide."</p>



<p>Instead of the smallest-scale chips, Europe's car industry and other key sectors instead could use a factory that produces slightly larger semiconductors, industry experts said.  </p>



<p>"The European ambition gets bigger and bigger by the quarter. It started as the project of the decade, now it's become the project of the century and soon it'll be the project of the millennium," the official said. "Meanwhile, we are forgetting to take the first step."</p>



<p><em>Mark Scott, Joshua Posaner and Stuart Lau contributed reporting.</em></p>









<p><em>Want more analysis from </em><span>POLITICO</span><em>? </em><span>POLITICO</span><em> Pro is our premium intelligence service for professionals. From financial services to trade, technology, cybersecurity and more, Pro delivers real time intelligence, deep insight and breaking scoops you need to keep one step ahead. Email <a href="https://www.politico.eu/cdn-cgi/l/email-protection#f585879ab5859a999c819c969adb9080" target="_blank"><span data-cfemail="f7878598b787989b9e839e9498d99282">[email&nbsp;protected]</span></a> to request a complimentary trial.</em></p>								</div></div>]]>
            </description>
            <link>https://www.politico.eu/article/europe-seeks-to-decouple-from-us-china-chip-war/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26221255</guid>
            <pubDate>Mon, 22 Feb 2021 05:39:42 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Applications in a Cloud Native World]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26221189">thread link</a>) | @Ballu
<br/>
February 21, 2021 | https://ereslibre.github.io/applications-in-a-cloud-native-world/welcome.html | <a href="https://web.archive.org/web/*/https://ereslibre.github.io/applications-in-a-cloud-native-world/welcome.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page">
        <!-- Provide site root to javascript -->
        

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        

        <!-- Set the theme before any content is loaded, prevents flash -->
        

        <!-- Hide / unhide sidebar before it is displayed -->
        

        <nav id="sidebar" aria-label="Table of contents">
            <div id="sidebar-scrollbox">
                <ol><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/welcome.html"><strong aria-hidden="true">1.</strong> Welcome</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/basics.html"><strong aria-hidden="true">2.</strong> Basics</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/monolith.html"><strong aria-hidden="true">3.</strong> Monolith</a></li><li><ol><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/monolith/traits.html"><strong aria-hidden="true">3.1.</strong> Traits</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/monolith/scalability.html"><strong aria-hidden="true">3.2.</strong> Scalability</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/monolith/boundaries.html"><strong aria-hidden="true">3.3.</strong> Boundaries</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/monolith/deployment.html"><strong aria-hidden="true">3.4.</strong> Deployment</a></li><li><ol><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/monolith/deployment/infrastructure.html"><strong aria-hidden="true">3.4.1.</strong> Infrastructure</a></li><li><ol><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/monolith/deployment/infrastructure/metal.html"><strong aria-hidden="true">3.4.1.1.</strong> Metal</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/monolith/deployment/infrastructure/virtualization.html"><strong aria-hidden="true">3.4.1.2.</strong> Virtualization</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/monolith/deployment/infrastructure/containerization.html"><strong aria-hidden="true">3.4.1.3.</strong> Containerization</a></li></ol></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/monolith/deployment/techniques.html"><strong aria-hidden="true">3.4.2.</strong> Techniques</a></li><li><ol><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/monolith/deployment/techniques/manual.html"><strong aria-hidden="true">3.4.2.1.</strong> Manual</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/monolith/deployment/techniques/automated.html"><strong aria-hidden="true">3.4.2.2.</strong> Automated</a></li></ol></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/monolith/deployment/targets.html"><strong aria-hidden="true">3.4.3.</strong> Targets</a></li></ol></li></ol></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/soa.html"><strong aria-hidden="true">4.</strong> Service Oriented Architecture</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/microservice.html"><strong aria-hidden="true">5.</strong> Microservice</a></li><li><ol><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/microservice/traits.html"><strong aria-hidden="true">5.1.</strong> Traits</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/microservice/scalability.html"><strong aria-hidden="true">5.2.</strong> Scalability</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/microservice/boundaries.html"><strong aria-hidden="true">5.3.</strong> Boundaries</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/microservice/deployment.html"><strong aria-hidden="true">5.4.</strong> Deployment</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/microservice/cloud-native.html"><strong aria-hidden="true">5.5.</strong> Cloud Native</a></li></ol></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/service-communication.html"><strong aria-hidden="true">6.</strong> Service communication</a></li><li><ol><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/service-communication/protocols.html"><strong aria-hidden="true">6.1.</strong> Protocols</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/service-communication/traffic.html"><strong aria-hidden="true">6.2.</strong> Traffic</a></li></ol></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/kubernetes.html"><strong aria-hidden="true">7.</strong> Kubernetes</a></li><li><ol><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/kubernetes/concepts.html"><strong aria-hidden="true">7.1.</strong> Concepts</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/kubernetes/components.html"><strong aria-hidden="true">7.2.</strong> Components</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/kubernetes/service-scaling.html"><strong aria-hidden="true">7.3.</strong> Service Scaling</a></li><li><ol><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/kubernetes/service-scaling/manual-scaling.html"><strong aria-hidden="true">7.3.1.</strong> Manual scaling</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/kubernetes/service-scaling/vertical-pod-autoscaler.html"><strong aria-hidden="true">7.3.2.</strong> Vertical Pod Autoscaler</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/kubernetes/service-scaling/horizontal-pod-autoscaler.html"><strong aria-hidden="true">7.3.3.</strong> Horizontal Pod Autoscaler</a></li></ol></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/kubernetes/cluster-autoscaler.html"><strong aria-hidden="true">7.4.</strong> Cluster autoscaler</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/kubernetes/extensibility.html"><strong aria-hidden="true">7.5.</strong> Extensibility</a></li><li><ol><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/kubernetes/extensibility/webhooks.html"><strong aria-hidden="true">7.5.1.</strong> Webhooks</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/kubernetes/extensibility/controllers.html"><strong aria-hidden="true">7.5.2.</strong> Controllers</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/kubernetes/extensibility/crds.html"><strong aria-hidden="true">7.5.3.</strong> Custom Resource Definitions</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/kubernetes/extensibility/operators.html"><strong aria-hidden="true">7.5.4.</strong> Operators</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/kubernetes/extensibility/api-server-aggregation.html"><strong aria-hidden="true">7.5.5.</strong> API Server aggregation</a></li></ol></li></ol></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/continuous-integration.html"><strong aria-hidden="true">8.</strong> Continuous integration</a></li><li><ol><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/continuous-integration/testing.html"><strong aria-hidden="true">8.1.</strong> Testing</a></li><li><ol><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/continuous-integration/testing/unit-tests.html"><strong aria-hidden="true">8.1.1.</strong> Unit tests</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/continuous-integration/testing/integration-tests.html"><strong aria-hidden="true">8.1.2.</strong> Integration tests</a></li><li><ol><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/continuous-integration/testing/integration-tests/narrow-integration-tests.html"><strong aria-hidden="true">8.1.2.1.</strong> Narrow integration tests</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/continuous-integration/testing/integration-tests/broad-integration-tests.html"><strong aria-hidden="true">8.1.2.2.</strong> Broad integration tests</a></li></ol></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/continuous-integration/testing/functional-tests.html"><strong aria-hidden="true">8.1.3.</strong> Functional tests</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/continuous-integration/testing/end-to-end-tests.html"><strong aria-hidden="true">8.1.4.</strong> End to end tests</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/continuous-integration/testing/acceptance-tests.html"><strong aria-hidden="true">8.1.5.</strong> Acceptance tests</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/continuous-integration/testing/performance-tests.html"><strong aria-hidden="true">8.1.6.</strong> Performance tests</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/continuous-integration/testing/smoke-tests.html"><strong aria-hidden="true">8.1.7.</strong> Smoke tests</a></li></ol></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/continuous-integration/build-artifacts.html"><strong aria-hidden="true">8.2.</strong> Build artifacts</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/continuous-integration/container-image-registry.html"><strong aria-hidden="true">8.3.</strong> Container image registry</a></li></ol></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/continuous-delivery.html"><strong aria-hidden="true">9.</strong> Continuous delivery</a></li><li><ol><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/continuous-delivery/ci-driven.html"><strong aria-hidden="true">9.1.</strong> CI driven</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/continuous-delivery/gitops.html"><strong aria-hidden="true">9.2.</strong> GitOps</a></li></ol></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/observability.html"><strong aria-hidden="true">10.</strong> Observability</a></li><li><ol><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/observability/monitoring.html"><strong aria-hidden="true">10.1.</strong> Monitoring</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/observability/logging.html"><strong aria-hidden="true">10.2.</strong> Logging</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/observability/tracing.html"><strong aria-hidden="true">10.3.</strong> Tracing</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/observability/metrics.html"><strong aria-hidden="true">10.4.</strong> Metrics</a></li></ol></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/patterns.html"><strong aria-hidden="true">11.</strong> Patterns</a></li><li><ol><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/patterns/api-gateway.html"><strong aria-hidden="true">11.1.</strong> API Gateway</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/patterns/service-mesh.html"><strong aria-hidden="true">11.2.</strong> Service Mesh</a></li></ol></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/security.html"><strong aria-hidden="true">12.</strong> Security</a></li><li><ol><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/security/rbac.html"><strong aria-hidden="true">12.1.</strong> RBAC</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/security/container-runtime.html"><strong aria-hidden="true">12.2.</strong> Container runtime</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/security/network-isolation.html"><strong aria-hidden="true">12.3.</strong> Network isolation</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/security/container-image-scanners.html"><strong aria-hidden="true">12.4.</strong> Container image scanners</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/security/api-server-audit.html"><strong aria-hidden="true">12.5.</strong> API Server Audit</a></li></ol></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/debugging.html"><strong aria-hidden="true">13.</strong> Debugging</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/advanced-concepts.html"><strong aria-hidden="true">14.</strong> Advanced Concepts</a></li><li><ol><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/advanced-concepts/virtual-kubelet.html"><strong aria-hidden="true">14.1.</strong> Virtual Kubelet</a></li><li><a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/advanced-concepts/deprecation-policy.html"><strong aria-hidden="true">14.2.</strong> Deprecation Policy</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper">

            <div class="page">
                
                <div id="menu-bar">
                    <div id="menu-bar-sticky-container">
                        <div>
                            
                            
                            <ul id="theme-list" aria-label="Themes" role="menu">
                                <li role="none"></li>
                                <li role="none"></li>
                                <li role="none"></li>
                                <li role="none"></li>
                                <li role="none"></li>
                            </ul>
                            
                            
                            
                        </div>

                        

                        <div>
                            <a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/print.html" title="Print this book" aria-label="Print this book">
                                <i id="print-button"></i>
                            </a>
                            
                        </div>
                    </div>
                </div>

                
                <div id="search-wrapper">
                    <form id="searchbar-outer">
                        
                    </form>
                    <div id="searchresults-outer">
                        <div id="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                

                <div id="content">
                    <main>
                        
<blockquote>
<p><strong>Note:</strong> This book is a collaborative effort and it is expected to be in
continuous evolution. It is open for contributions at any time. Make
it better, make it yours.</p>
</blockquote>
<p>Welcome to the <em>Applications in a cloud native world</em> book. This book
will try to outline best practices when developing containerized and
cloud native services.</p>
<p>In order to lay out the foundations we will focus on, we first need to
consider how services have historically been managed.</p>
<p>Let's then jump straight to the <a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/basics.html">Basics</a>.</p>

                    </main>

                    <nav aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        
                            <a rel="next" href="https://ereslibre.github.io/applications-in-a-cloud-native-world/basics.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i></i>
                            </a>
                        

                        <div></div>
                    </nav>
                </div>
            </div>

            <nav aria-label="Page navigation">
                

                
                    <a href="https://ereslibre.github.io/applications-in-a-cloud-native-world/basics.html" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i></i>
                    </a>
                
            </nav>

        </div>

        

        

        
        
        
        
        

        

        
        
        
        
        

        
        
        

        <!-- Custom JS scripts -->
        

        

    

</div>]]>
            </description>
            <link>https://ereslibre.github.io/applications-in-a-cloud-native-world/welcome.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26221189</guid>
            <pubDate>Mon, 22 Feb 2021 05:26:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Multiplexing Multipath P2P Mobile Transports]]>
            </title>
            <description>
<![CDATA[
Score 7 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26220942">thread link</a>) | @bigfish24
<br/>
February 21, 2021 | https://www.ditto.live/blog/posts/the-new-network-multiplexer | <a href="https://web.archive.org/web/*/https://www.ditto.live/blog/posts/the-new-network-multiplexer">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><div><nav aria-label="breadcrumb"><ol><li><a href="https://www.ditto.live/">Home</a></li><li><a href="https://www.ditto.live/blog/posts">Blog</a></li><li><a href="https://www.ditto.live/blog/posts/the-new-network-multiplexer">The New Network Multiplexer </a></li></ol></nav><p>Since the first versions of Ditto, devices always made multiple connections to other peers. For example, two iOS devices will try to connect simultaneously over WiFi, Apple Wireless Direct Link (AWDL), and Bluetooth Low Energy (BLE). We make multiple connections because each transport has different characteristics such as throughput and distance. For example, Bluetooth Low Energy works over long distances but has little bandwidth. AWDL has much more bandwidth but the devices need to be close together.</p>
<p>In the example below we see two iPhones syncing over AWDL and BLE. As one device gets further from the other, the AWDL connection will degrade and disconnect while the BLE connection sustains longer distances.</p>

<h2 id="pre-version-1.0.0">Pre-Version 1.0.0</h2>
<p>Before version 1.0.0, Ditto created a unique replication session for every transport. This is the software component which tracks queries and data changes and ensures that every Ditto device stays in sync. These separate replication pathways can appear or disappear as connections come and go, without disrupting sibling sessions.</p>
<p>When there is new data to sync, a session packs that data into an update file. With multiple concurrent sessions, all of them would then race against each other to transmit that update file as quickly as possible, regardless of duplication between transports. Transactional locking on the internal database made sure that only one session at a time could modify the update file, which prevented race conditions. Since any session is able to maintain the update file, any individual session can fail and replication will always continue, providing a high level of reliability.</p>
<p><img src="https://www.ditto.live/assets/blog/posts/the-new-network-multiplexer/old-way.svg" alt="old-way-sessions"></p>
<p>In this benchmark we see the consequences of each session sending data eagerly over every transport. In an attempt to send a document with about 2 megabytes of data, all five connected modes of transport aggressively sent data as fast as they could. We can see that the highly efficient AWDL transports could send all bytes first, and the remote peer quickly notified the slower connections that they could stop. However, the slower transports had already sent duplicate bytes. In the end, the peer had sent 5.4 megabytes even though the document size was about 2 megabytes. This was 2.6 times larger than the initial payload. Furthermore, this fully occupied the BLE radio, consuming bandwidth that could have been better used by a Bluetooth-only peer.</p>
<div>
  <table>
    <thead>
      <tr>
        <th>Transport</th>
        <th>Bytes Sent</th>
        <th>Packets Sent</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>AWDL Client</td>
        <td>2097944</td>
        <td>33</td>
      </tr>
      <tr>
        <td>AWDL Server</td>
        <td>2097944</td>
        <td>33</td>
      </tr>
      <tr>
        <td>BLE Client</td>
        <td>12288</td>
        <td>16</td>
      </tr>
      <tr>
        <td>BLE Server</td>
        <td>12288</td>
        <td>16</td>
      </tr>
      <tr>
        <td>TCP Server</td>
        <td>1170432</td>
        <td>36</td>
      </tr>
      <tr>
        <td>Grand Total</td>
        <td>5390896</td>
        <td>134</td>
      </tr>
    </tbody>
  </table>
</div>


<p>In version 1.0.0, we’ve introduced a completely new system for creating sync sessions between peers we call the multiplexer. Our first order of business was to reduce the duplication of sessions to the same peer over multiple transport types. We’ve introduced the concept of a virtual connection between two peers. No matter how many transport connections are active, there will only be one virtual connection and only one session. Now incoming data is buffered and intermediated from the transport layer to a single virtual connection.</p>
<p><img src="https://www.ditto.live/assets/blog/posts/the-new-network-multiplexer/new-way.svg" alt="new-way-multiplexer"></p>
<p>This new architecture allows each virtual connection to intelligently send data over multiple physical transports with fine-grained control. For example, the multiplexer can switch active transports on the fly without unnecessary duplication.</p>
<p>In this example:</p>
<ol>
<li>The multiplexer on the left device deemed that TCP (WiFi) was the best transport to start sending data.</li>
<li>Suddenly, the infastructure WiFi goes out, and the multiplexer switches to AWDL.</li>
<li>As the device moves away from its peer, AWDL is lost and the multiplexer switches to BLE.</li>
<li>The devices move closer together and the multiplexer finishes the rest of the transmission over AWDL.</li>
</ol>

<p>Now the total bytes sent is equal to the size of the update file.</p>
<div>
  <table>
    <thead>
      <tr>
        <th>Transport</th>
        <th>Bytes Sent</th>
        <th>Packets Sent</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>AWDL Client</td>
        <td>524288</td>
        <td>8</td>
      </tr>
      <tr>
        <td>AWDL Server</td>
        <td>234264</td>
        <td>4</td>
      </tr>
      <tr>
        <td>BLE Client</td>
        <td>0</td>
        <td>0</td>
      </tr>
      <tr>
        <td>BLE Server</td>
        <td>421888</td>
        <td>206</td>
      </tr>
      <tr>
        <td>TCP Server</td>
        <td>917504</td>
        <td>28</td>
      </tr>
      <tr>
        <td>Grand Total</td>
        <td>2097944</td>
        <td>246</td>
      </tr>
    </tbody>
  </table>
</div>

<p>The introduction of the multiplexer is a gigantic step forward for Ditto's networking capabilities. Today, it focuses on using one transport at a time but this new foundation allows us to build even more powerful, dynamic and flexible replication techniques such as using multiple transports at a time over unreliable connections, streaming use cases, and decentralized data sync techniques reminiscent of BitTorrent.</p>
</div></div></div>]]>
            </description>
            <link>https://www.ditto.live/blog/posts/the-new-network-multiplexer</link>
            <guid isPermaLink="false">hacker-news-small-sites-26220942</guid>
            <pubDate>Mon, 22 Feb 2021 04:43:40 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[API vs. SDK explained in restaurant terms]]>
            </title>
            <description>
<![CDATA[
Score 5 | Comments 2 (<a href="https://news.ycombinator.com/item?id=26220862">thread link</a>) | @alexander_kir
<br/>
February 21, 2021 | https://www.amity.co/blog/api-vs-sdk-which-is-which | <a href="https://web.archive.org/web/*/https://www.amity.co/blog/api-vs-sdk-which-is-which">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div><p>Software Development Kits (SDK) and Application Programming Interface (API) are critical components of your app’s development. However, these two terms sometimes overlap, often leading to confusion.&nbsp;<br></p><p>Knowing their differences and how you can take advantage of them can significantly help you improve your application. This piece tackles their distinct features and outlines how you can utilize them to supercharge your app.&nbsp;<br></p><h3><strong>What is an API?&nbsp;&nbsp;</strong></h3><p>By definition, APIs are sets of instructions and protocols used to integrate specific functionalities into an application. An API can help connect your apps or projects to external services, enabling seamless data transfer and adding a new feature altogether.&nbsp;<br></p><p>Let’s take a look at this example from software company <a href="https://www.mulesoft.com/resources/api/what-is-an-api">Mulesoft</a> explaining the function of an API:&nbsp;&nbsp;</p><blockquote><em>Imagine you’re sitting at a table in a restaurant with a menu of choices to order from. The kitchen is the part of the “system” that will prepare your order. What is missing is the critical link to communicate your order to the kitchen and deliver your food back to your table. That’s where the waiter or API comes in. The waiter is the messenger – or API – that takes your request or order and tells the kitchen – the system – what to do. Then the waiter delivers the response back to you; in this case, it is the food.</em><br></blockquote><p>With an API, developers don’t have to worry about creating lots of custom code to enable functionalities, as various APIs exist to fulfill a specific function. As <a href="https://www.ibm.com/cloud/learn/api?utm_medium=OSocial&amp;utm_source=Youtube&amp;utm_content=000023UA&amp;utm_term=10010608&amp;utm_id=YTDescription-101-API-vs-SDK-LH-API-Guide&amp;cm_mmc=OSocial_Youtube-_-Cloud+and+Data+Platform_SFT+Cloud+Platform+Digital-_-WW_WW-_-YTDescription-101-API-vs-SDK-LH-API-Guide&amp;cm_mmca1=000023UA&amp;cm_mmca2=10010608">IBM</a> mentioned, APIs allow companies to open up their applications’ data and functionality for third-party developers to use. So if you have a food delivery app and want to verify your user’s number, provide the location, and enable payment without leaving the platform, there’s an available phone, maps, and payment API to perform these actions.&nbsp;<br></p><h3><strong>What is an SDK?&nbsp;</strong></h3><p>On the other hand, SDKs are a set of tools used to develop applications for a specific platform. <a href="https://www.redhat.com/en/topics/cloud-native-apps/what-is-SDK">Red Hat</a> mentioned that a typical SDK contains a compiler, debugger, as well as APIs, and any of the following:<br></p><ul role="list"><li>Documentation</li><li>Libraries</li><li>Editors</li><li>Runtime/development environments</li><li>Testing/analysis tools</li><li>Drivers</li><li>Network protocols<br></li></ul><p>Let’s take the restaurant scenario again. For example, you are a chef. When you’re cooking a dish, you will need ingredients for your recipe; you need the kitchen utensils so you can cook, you need a copy of a recipe to put the meal together, and so on. In the same way, SDK provides all the things you need to create your intended application.&nbsp;<br></p><p>SDKs are crucial when developing an app for a specific platform. For instance, Apple provides iOS SDKs to developers so they can create applications specifically for iOS. An SDK should add value to a developer. Hence it should be easy to use, provides a thorough explanation of the code used, and adds functionality to an existing app.&nbsp;<br></p><h4><strong>Things to remember&nbsp;</strong></h4><p>Now that we defined both, let us recap:&nbsp;<br></p><ul role="list"><li>APIs facilitates the communication and integration of software</li></ul><ul role="list"><li>SDK provides the foundation to build an application specifically for a platform</li></ul><ul role="list"><li>SDKs contain APIs; APIs don’t contain SDKs<br></li></ul><p>API, as a part of an SDK, is lightweight and specialized based on the function intended. Meanwhile, SDKs have a collection of utilities to create a new application or add new functionalities.&nbsp;<br></p><h3><strong>API and SDK can elevate your app&nbsp;</strong></h3><p>Now that we know the differences, how can you take advantage of both to improve your app?&nbsp;<br></p><p>Utilizing SDKs with the APIs that meet your needs can significantly enhance your application’s functionality. According to <a href="https://marketfinder.thinkwithgoogle.com/intl/en/guide/improve-ux-ui-of-app/#overview">Google</a>, mobile users spend nine out of ten minutes using only their top five favorite apps. So how can your app be one of their top five?&nbsp;&nbsp;<br></p><p>SDKs can enable powerful in-app features with the corresponding APIs that will substantially affect your app’s user experience. With so many apps out there, you would want yours to stand out in the app market.&nbsp;<br></p><p>And of course, you just don’t want users to download your app; you would like them to keep and share it with their peers, creating a loyal fanbase for your application.&nbsp;<br></p><p>If you are a brand and you aim to engage and retain your users, adding <a href="https://www.amity.co/blog/remain-competitive-by-adding-social-features-to-your-app">social features with an SDK </a>to your app can help raise user engagement through <a href="https://www.amity.co/blog/building-your-in-app-community-why-it-matters">in-app groups.</a> Meanwhile, integrating chat SDK into your application can facilitate 1-on-1 conversations or group interactions, allowing you to host online communities.&nbsp;<br></p><p><a href="https://www.amity.co/products/amity-video">Video SDKs</a> can help you integrate in-app live streaming and stories to your product if you have an entertainment application.&nbsp; So whether it’s a sports event or a concert broadcasted in your app, this solution can help bring throngs of fans to use your application. On the other hand, if your SDK has a <a href="https://www.amity.co/products/amity-bots">chatbot</a> API that can collect user data, you can use the information you have to create a more personalized user experience, push tailored notifications, and deliver the right content to your users.<br></p><p>In conclusion, API and SDK, regardless of their differences, can both be beneficial for your application. Using SDK with the right APIs can create numerous possibilities to improve your application. Now is the time to find the best SDKs to enhance your in-app user experience, engagement, and retention.&nbsp;<br></p></div></div>]]>
            </description>
            <link>https://www.amity.co/blog/api-vs-sdk-which-is-which</link>
            <guid isPermaLink="false">hacker-news-small-sites-26220862</guid>
            <pubDate>Mon, 22 Feb 2021 04:28:03 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Turning a wireless keyboard into a wired keyboard]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26220556">thread link</a>) | @todsacerdoti
<br/>
February 21, 2021 | https://chadaustin.me/2021/02/wired-sculpt/ | <a href="https://web.archive.org/web/*/https://chadaustin.me/2021/02/wired-sculpt/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div itemprop="articleBody">
    <p>I made a control board for the Microsoft Sculpt wireless keyboard that converts it to wired USB, and now my favorite keyboard is even better.</p>

<figure>
<a href="https://chadaustin.me/images/sculpt/finished-board.jpeg"><img src="https://chadaustin.me/images/sculpt/finished-board.jpeg" alt="The finished and installed board."></a>
<figcaption>The finished and installed board.</figcaption>
</figure>

<figure>
<a href="https://chadaustin.me/images/sculpt/messy-desk.jpeg"><img src="https://chadaustin.me/images/sculpt/messy-desk.jpeg" alt="Wired keyboard and the resulting project mess!"></a>
<figcaption>Wired keyboard and the resulting project mess!</figcaption>
</figure>

<figure>
<a href="https://chadaustin.me/images/sculpt/underside.jpeg"><img src="https://chadaustin.me/images/sculpt/underside.jpeg" alt="USB cable and reset button."></a>
<figcaption>USB cable and reset button.</figcaption>
</figure>

<p>The QMK config is available at <a href="https://github.com/chadaustin/qmk_firmware">@chadaustin/qmk_firmware</a> (<a href="https://github.com/chadaustin/qmk_firmware/tree/master/keyboards/handwired/sculpt">keyboards/handwired/sculpt/</a>), and the PCB design files at <a href="https://github.com/chadaustin/wired-sculpt-pcb">@chadaustin/wired-sculpt-pcb</a>.</p>

<p>I’m planning on making at least one more, so if you’d like one, maybe I can help.</p>

<p>It’s a huge improvement. Latency is reduced by about 13 milliseconds, and with full control over the microcontroller’s firmware, you can customize keymaps and layers, and actually use the keyboard’s built-in LEDs.</p>

<h2 id="why">Why?</h2>

<p>Feel free to stop reading here — I am going to tell the sequence of events that led to this project. Besides some exposure to basic voltage and resistance circuits in college, I have very little electronics background. But, in a short time, I went from only barely knowing what a capacitor was to having a working PCB manufactured and assembled, and maybe this will inspire someone else to give it a try.</p>

<p>Since developing RSI in college, I’ve exclusively used Microsoft’s ergonomic keyboards. And when I first tried the Sculpt, I instantly knew it was the best yet. The soft actuation, short key travel, and rigid frame are perfect for my hands. And because the number pad is a separate device, the distance to my mouse is shortened.</p>

<p>My brother went out and bought one too. Not much later, he gave it to me, saying the latency was inconsistent and high, and it was unacceptable for gaming. I thought he was being uniquely sensitive, since I had no problem in either Linux, Windows 7, or macOS. But then I updated to Windows 10 and saw exactly what he meant.</p>

<p>It was like the keyboard would go to sleep if a key wasn’t pressed for a few seconds, and the first keypress after a wake would be delayed or, worse, dropped.</p>

<p>And heaven forbid I use my USB 3 hub, whose EMI would disrupt the 2.4 GHz signal, and <em>every other</em> keypress would be unreliable. I’d gone as far as mounting the wireless transceiver directly under my keyboard, on the underside of my desk, and keys were still dropped.</p>

<p>So, best keyboard ever. But wireless sucks. (But mostly in Windows 10? No idea about that.)</p>

<h2 id="over-the-hump">Over the Hump</h2>

<p>What started this whole thing is that the <a href="https://github.com/facebookexperimental/eden/#edenfs">EdenFS</a> team was a bunch of keyboard enthusiasts. During the pandemic, as we’re all at home burning out and missing each other, we were trying to think of some virtual team offsites. Wez offered to walk everyone through building a <a href="https://www.1upkeyboards.com/instructions-downloads/sweet-16-instructions/">Sweet 16 Macro Pad</a>.</p>

<figure>
<a href="https://chadaustin.me/images/sculpt/sweet-16.jpeg"><img src="https://chadaustin.me/images/sculpt/sweet-16.jpeg" alt="Assembled Sweet 16 underside"></a>
<figcaption>Assembled Sweet 16 underside. This is take two, after resoldering and cleaning the whole thing. Take one was a bit of a mess.</figcaption>
</figure>

<p>So, okay, a keyboard is a matrix, with some diodes used to disambiguate the signalling, and a microcontroller that rapidly polls the matrix and reports events over USB…</p>

<p>So maybe I could fix the Sculpt! I bought a transceiver-less Sculpt off eBay for cheap and <a href="http://emmanuelcontreras.com/how-to/how-to-disassemble-microsoft-sculpt-ergonomic-keyboard-and-make-it-wired/">popped it open (thanks Emmanuel Contreras!)</a>, thinking maybe its controller could be flashed with new firmware that speaks USB. The Sculpt uses a <a href="https://infocenter.nordicsemi.com/pdf/nRF24LE1_PS_v1.6.pdf">Nordic Semiconductor nRF24LE1</a>, but I was nowhere near capable of making use of that information at the time, though it did point me to Samy Kamkar’s horrifying guide on <a href="https://samy.pl/keysweeper/">surreptitiously sniffing keystrokes from nearby (older) Microsoft wireless keyboards</a>.</p>

<p>I almost gave up here, but Per Vognsen <a href="https://twitter.com/pervognsen/status/1322422385174220800">suggested I scan the matrix myself</a> and it turns out Michael Fincham had already <a href="https://www.reddit.com/r/MechanicalKeyboards/comments/bhkgnp/modification_photos_qmk_wired_microsoft_sculpt/">mapped out the matrix and soldered a Teensy 2.0++ board onto the Sculpt’s test pads</a>, showing this was doable!</p>

<p>So I ordered my own microcontroller to try the same thing.</p>

<p>First, I bought an Arduino Pro Micro, like the Sweet 16 uses. Oh hey, 18 GPIO pins isn’t enough to drive the Sculpt’s 26-pin matrix. I looked at using an I2C GPIO expander, but it felt like taking on too much.</p>

<figure>
<a href="https://chadaustin.me/images/sculpt/pro-micro.jpeg"><img src="https://chadaustin.me/images/sculpt/pro-micro.jpeg" alt="Arduino Pro Micro"></a>
<figcaption>Arduino Pro Micro. Wait, you need pins to scan a matrix?</figcaption>
</figure>

<p>More pins? QMK’s Proton C has more pins! So I carefully soldered onto the test pads as Michael had shown was possible… and it worked!</p>

<figure>
<a href="https://chadaustin.me/images/sculpt/proton-c.jpeg"><img src="https://chadaustin.me/images/sculpt/proton-c.jpeg" alt="QMK Proton C"></a>
<figcaption>QMK Proton C. It's a beautiful board.</figcaption>
</figure>

<figure>
<a href="https://chadaustin.me/images/sculpt/test-pads.jpeg"><img src="https://chadaustin.me/images/sculpt/test-pads.jpeg" alt="Soldering test pads to Proton C."></a>
<figcaption>Soldering test pads to Proton C.</figcaption>
</figure>

<figure>
<a href="https://chadaustin.me/images/sculpt/all-test-pads.jpeg"><img src="https://chadaustin.me/images/sculpt/all-test-pads.jpeg" alt="All test pads connected to Proton C. It works!"></a>
<figcaption>All test pads connected to Proton C. It works!</figcaption>
</figure>

<p>Getting those wires to stick to the pads without shorting was tricky. (I hadn’t yet discovered how magical flux is.)</p>

<p>The keyboard worked, but I couldn’t fit the board, its wires, and the new microcontroller into the case, and I wasn’t <em>really</em> happy leaving it in this state, even if I could pack it in somehow.</p>

<p>I thought, all I <em>really</em> need is the ribbon cable connector, so I ordered a 30 pin, 1.0 mm pitch ribbon breakout and the pricier (but tons of pins!) <a href="https://www.pjrc.com/store/teensypp.html">Teensy 2.0++</a>. Looking back, it’s cute that I was trying to save $10 on the microcontroller… You just have to get used to spending money on whatever saves you time.</p>

<figure>
<a href="https://chadaustin.me/images/sculpt/breakout-and-teensy.jpeg"><img src="https://chadaustin.me/images/sculpt/breakout-and-teensy.jpeg" alt="Ribbon cable breakout and Teensy 2.0++"></a>
<figcaption>Ribbon cable breakout and Teensy 2.0++</figcaption>
</figure>

<p>Well, it was almost as annoying to solder, and still didn’t fit. So much for saving money on microcontrollers.</p>

<p>I thought about giving up. Is it really that bad that my keys don’t always register in games? Can I just tolerate some flakiness and latency?</p>

<p>But Jon Watte offered to spend an entire day showing me how to use KiCad, design circuits, layout PCBs, select components on Digi-Key, scan datasheets for the important information, and how to work with a PCB manufacturing house. Of course you never turn down opportunities like that.</p>

<h2 id="designing-the-final-board---schematic">Designing the Final Board - Schematic</h2>

<p>Assuming, like me, you’ve never done this, I’ll summarize the steps.</p>

<p>First you sketch out the circuit schematic.</p>

<figure>
<a href="https://chadaustin.me/images/sculpt/schematic.png"><img src="https://chadaustin.me/images/sculpt/schematic.png" alt="Schematic"></a>
<figcaption>Schematic in KiCad. Most of this was informed by the datasheet and Atmel's design guides.</figcaption>
</figure>

<p>Jon showed me several tricks in KiCad, like global labels, and starting with some standard resistor and capacitor values, but it’s very important that you go through the datasheets, because details can matter a ton.</p>

<p>I knew I wanted the main processor to be the AT90USB1286 controller, and fortunately KiCad already had a symbol for it. Atmel has a comprehensive and accessible data sheet, which showed me I needed some 22 Ω resistors on the USB data lines, which of the ISP programmer lines needed resistors (and appropriate values), and that I needed to either pull HWB low, or provide a physical switch that pulls it low, in order to allow rebooting the device into USB firmware update mode.</p>

<p>There are a bunch of things that are implicitly known to electrical engineers but that were new to me. You want:</p>

<ul>
  <li>a ground plane under the data lines and most of the microcontroller if possible.</li>
  <li>an electrolytic or tantalum bypass capacitor on the main 5V power from USB.</li>
  <li>ceramic filter capacitors on each power pin.</li>
  <li>appropriate values for the resonance capacitors on your crystal.</li>
  <li>electrostatic discharge protection! Turns out transients are common and it’s easy to fry a chip just by plugging it in.</li>
</ul>

<p>And then when you get into concerns like EMI and high-frequency signal integrity, the rabbit hole goes deep.</p>

<p>I kept having to tell myself “it’s just a keyboard”, but it also helped that there are a great number of high-quality resources on these topics just a click away. I spent lots of time on <a href="https://www.eevblog.com/">EEVBlog</a>.</p>

<p>Before finishing the circuit design, Jon had me do a couple smart things. In case the factory-supplied USB bootloader didn’t work out, he suggested I add the footprint (but not a connector!) for an ISP programmer and a debug LED to prove code would work at all.</p>

<h2 id="designing-the-final-board---physical-layout">Designing the Final Board - Physical Layout</h2>

<p>After arranging the schematic and ensuring it passed the electrical rules check, it was time to pick specific components. That is, the reference to a 220 Ω resistor is replaced with the Panasonic ERJ-3EKF2200V, 0603 surface mount.</p>

<p>There are a couple things to keep in mind. For common components, like resistors and ceramic capacitors, there is a huge amount of choice. For example, I see over 1400 surface-mount 220 Ω resistors on digikey. I tried to just stick with one high-quality brand like Panasonic or Samsung for all of that stuff.</p>

<p>The important thing is the physical form factor, which determines the footprint on the board. Once you pick a part, it has a size, and you need to tell KiCad which physical footprint should be assigned to that component. I used 0603 resistors, so I assigned each resistor in the schematic the “Resistor_SMD:R_0603_1608Metric” footprint.</p>

<p>Same for everything else. Jon showed me how to draw my own footprints, but to avoid complexity, I was able to find appropriate footprints in KiCad’s standard libraries for every component I needed.</p>

<p>When you import the schematic into Pcbnew, it’s time to figure out where things go. Where are the edges of the board? Make careful measurements here. Where do the mounting holes go? Where do you want 
the microcontroller? Where do you want the USB port?</p>

<figure>
<a href="https://chadaustin.me/images/sculpt/dimensions.jpeg"><img src="https://chadaustin.me/images/sculpt/dimensions.jpeg" alt="Measuring dimensions and mounting holes"></a>
<figcaption>Measuring dimensions and mounting holes</figcaption>
</figure>

<p>Also, you have to pick through-hole sizes and trace widths. Jon had me use .250 mm for the narrow traces and .500 mm for the wider ones, presumably from experience. I used the narrow traces for signalling and wide traces for power, though I’ve since heard it’s a good idea to use narrow traces between filter capacitors and VBUS.</p>

<figure>
<a href="https://chadaustin.me/images/sculpt/pcb-layout.svg"><img src="https://chadaustin.me/images/sculpt/pcb-layout.svg" alt="Schematic"></a>
<figcaption>PCB layout in KiCad</figcaption>
</figure>

<p>Of course, there’s some iteration between the schematic and the PCB. After physically placing the ribbon cable connector and MCU, the traces all crossed over each other, so I had to reassign all the pins so it made sense physically.</p>

<p>There are also physical constraints about how USB data lines are run, and how the electrostatic protection chip wants to be placed for the most protection.</p>

<p>So, as simple as this board is, I spent a fair amount of time getting all of that right.</p>

<p>I found myself getting lost in the abstractness of holes and traces and footprints, so it was helpful to ground myself by occasionally loading the PCB in KiCad’s 3D viewer.</p>

<figure>
<a href="https://chadaustin.me/images/sculpt/3d-view.png"><img src="https://chadaustin.me/images/sculpt/3d-view.png" alt="Schematic"></a>
<figcaption>3D View</figcaption>
</figure>

<h2 id="designing-the-final-board---manufacturing-and-testing-physical-fit">Designing the Final Board - Manufacturing and Testing Physical Fit</h2>

<p>I tried to …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://chadaustin.me/2021/02/wired-sculpt/">https://chadaustin.me/2021/02/wired-sculpt/</a></em></p>]]>
            </description>
            <link>https://chadaustin.me/2021/02/wired-sculpt/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26220556</guid>
            <pubDate>Mon, 22 Feb 2021 03:30:58 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Postgres regex search over 10k GitHub repositories (using only a MacBook)]]>
            </title>
            <description>
<![CDATA[
Score 4 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26220446">thread link</a>) | @randomdrake
<br/>
February 21, 2021 | https://devlog.hexops.com/2021/postgres-regex-search-over-10000-github-repositories | <a href="https://web.archive.org/web/*/https://devlog.hexops.com/2021/postgres-regex-search-over-10000-github-repositories">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div aria-label="Content">
      <div>
        <article itemscope="" itemtype="http://schema.org/BlogPosting">

  

  <div itemprop="articleBody">
    <p>In this article, we share empirical measurements from our experiments in using Postgres to index and search over 10,000 top GitHub repositories using <code>pg_trgm</code> on only a Macbook.</p>

<p>This is a follow up to <a href="https://devlog.hexops.com/2021/postgres-trigram-search-learnings">“Postgres Trigram search learnings”</a>, in which we shared several learnings and beliefs about trying to use Postgres Trigram indexes as an alterative to Google’s <a href="https://github.com/google/zoekt">Zoekt</a> (“Fast trigram based code search”).</p>

<p>We share our results, as well as <a href="https://github.com/hexops/pgtrgm_emperical_measurements">the exact steps we performed, scripts, and lists of the top 20,000 repositories by stars/language on GitHub</a> so you can reproduce the results yourself should you desire.</p>

<h2 id="tldr">TL;DR</h2>

<p><strong>This article is extensive and more akin to a research paper than a blog post.</strong> If you’re interested in our conclusions, see <a href="#conclusions">conclusions</a> instead.</p>

<h2 id="goals">Goals</h2>

<p>We wanted to get empirical measurements for how suitable Postgres is in providing regexp search over documents, e.g. as an alterative to Google’s <a href="https://github.com/google/zoekt">Zoekt</a> (“Fast trigram based code search”). In specific:</p>

<ul>
  <li>How many repositories can we index on just a 2019 Macbook Pro?</li>
  <li>How fast are different regexp searches over the corpus?</li>
  <li>What Postgres 13 configuration gives best results?</li>
  <li>What other operational effects need consideration if seriously attempting to use Postgres as the backend for a regexp search engine?</li>
  <li>What is the best database schema to use?</li>
</ul>

<h2 id="hardware">Hardware</h2>

<p>We ran all tests on a 2019 Macbook Pro with:</p>

<ul>
  <li>2.3 GHz 8-Core Intel Core i9</li>
  <li>16 GB 2667 MHz DDR4</li>
</ul>

<p>During test execution, few other Mac applications were in use such that effectively all CPU/memory was available to Postgres.</p>

<h2 id="corpus">Corpus</h2>

<p>We scraped <a href="https://github.com/hexops/pgtrgm_emperical_measurements/tree/main/top_repos">lists of the top 1,000 repositories from the GitHub search API</a> ranked by stars for each of the following languages (~20.5k repositories in total):</p>

<ul>
  <li>C++, C#, CSS, Go, HTML, Java, JavaScript, MatLab, ObjC, Perl, PHP, Python, Ruby, Rust, Shell, Solidity, Swift, TypeScript, VB .NET, and Zig.</li>
</ul>

<p>Cloning all ~20.5k repositories in parallel took ~14 hours with a fast ~100 Mbps connection to GitHub’s servers.</p>

<h3 id="dataset-reduction">Dataset reduction</h3>

<p>We found the amount of disk space required by <code>git clone --depth 1</code> on these repositories to be a sizable ~412G for just 12,148 repositories - and so we put in place several processes for further reduce the dataset size by about 66%:</p>

<ul>
  <li>Removing <code>.git</code> directories resulted in a 30% reduction (412G -&gt; 290G, for 12,148 repositories)</li>
  <li>Removing files &gt; 1 MiB resulted in another 51% reduction (290G -&gt; 142G, for 12,148 repositories - note GitHub does not index files &gt; 384 KiB in their search engine)</li>
</ul>

<h2 id="database-insertion">Database insertion</h2>

<p>We <a href="https://github.com/hexops/pgtrgm_emperical_measurements/blob/main/cmd/corpusindex/main.go">concurrently inserted</a> the entire corpus into Postgres, with the following DB schema:</p>

<div><div><pre><code><span>CREATE</span> <span>EXTENSION</span> <span>IF</span> <span>NOT</span> <span>EXISTS</span> <span>pg_trgm</span><span>;</span>
<span>CREATE</span> <span>TABLE</span> <span>IF</span> <span>NOT</span> <span>EXISTS</span> <span>files</span> <span>(</span>
    <span>id</span> <span>bigserial</span> <span>PRIMARY</span> <span>KEY</span><span>,</span>
    <span>contents</span> <span>text</span> <span>NOT</span> <span>NULL</span><span>,</span>
    <span>filepath</span> <span>text</span> <span>NOT</span> <span>NULL</span>
<span>);</span>
</code></pre></div></div>

<p>In total, this took around ~8 hours to complete and Postgres’s entire on-disk utilization was 101G.</p>

<h2 id="creating-the-trigram-index">Creating the Trigram index</h2>

<p>We tried three separate times to index the dataset using the following GIN Trigram index:</p>

<div><div><pre><code>CREATE INDEX IF NOT EXISTS files_contents_trgm_idx ON files USING GIN (contents gin_trgm_ops);
</code></pre></div></div>

<ul>
  <li><strong>In the first attempt, we hit an OOM after 11 hours and 34 minutes.</strong> This was due to a rapid spike in memory usage at the very end of indexing. We used a <a href="https://github.com/hexops/pgtrgm_emperical_measurements#configuration-attempt-1-indexing-failure-oom">fairly aggressive</a> Postgres configuration with a very large max WAL size, so it was not entirely unexpected.</li>
  <li><strong>In the second attempt, we ran out of SSD disk space after ~27 hours</strong>. Notable is that the disk space largely grew towards the end of indexing, similar to when we faced an OOM - it was not a gradual increase over time. For this attempt, we used the excellent <a href="https://pgtune.leopard.in.ua/#/">pgtune</a> tool to reduce our first Postgres configuration as follows:</li>
</ul>

<div><div><pre><code>shared_buffers = 4GB → 2560MB
effective_cache_size = 12GB → 7680MB
maintenance_work_mem = 16GB → 1280MB
default_statistics_target = 100 → 500
work_mem = 5242kB → 16MB
min_wal_size = 50GB → 4GB
max_wal_size = 4GB → 16GB
max_parallel_workers_per_gather = 8 → 4
max_parallel_maintenance_workers = 8 → 4
</code></pre></div></div>
<ul>
  <li><strong>In our third and final attempt, we cut the dataset in half and indexing succeeded after 22 hours.</strong> In specific, we deleted half of the files in the database (from 19,441,820 files / 178GiB of data to 9,720,910 files / 82 GiB of data.) The Postgres configuration used was the same as in attempt 2.</li>
</ul>

<h2 id="indexing-performance-memory-usage">Indexing performance: Memory usage</h2>

<p>In our first attempt, we see the reported <code>docker stats</code> memory usage of the container grow up to 12 GiB (chart shows MiB of memory used over time):</p>

<p><img width="981" alt="image" src="https://user-images.githubusercontent.com/3173176/107313722-56bbac80-6a50-11eb-94c7-8e13ea095053.png"></p>

<p>In our second and third attempts, we see far less memory usage (~1.6 GiB consistently):</p>

<p><img width="980" alt="image" src="https://user-images.githubusercontent.com/3173176/107314104-350ef500-6a51-11eb-909f-2f1b524d29b2.png"></p>

<p><img width="980" alt="image" src="https://user-images.githubusercontent.com/3173176/107315387-ce3f0b00-6a53-11eb-886c-410f000f73bd.png"></p>

<h2 id="indexing-performance-cpu-usage">Indexing performance: CPU usage</h2>

<p>Postgres’ Trigram indexing appears to be mostly single-threaded (at least when indexing <em>a single table</em>, we test multiple tables later.)</p>

<p>In our first attempt, CPU usage for the container did not rise above 156% (one and a half virtual CPU cores):</p>

<p><img width="982" alt="image" src="https://user-images.githubusercontent.com/3173176/107313915-cc277d00-6a50-11eb-9282-62159a127966.png"></p>

<p>Our second attempt was around 150-200% CPU usage on average:</p>

<p><img width="980" alt="image" src="https://user-images.githubusercontent.com/3173176/107314168-507a0000-6a51-11eb-8a18-ec18752f7f16.png"></p>

<p>Our third attempt similarly saw an average of 150-200%, but with a brief spike towards the end to ~350% CPU:</p>

<p><img width="980" alt="image" src="https://user-images.githubusercontent.com/3173176/107315239-8324f800-6a53-11eb-9a5b-fcc61d1a7b59.png"></p>

<h2 id="indexing-performance-disk-io">Indexing performance: Disk IO</h2>

<p>Disk reads/writes during indexing averaged about ~250 MB/s for reads (blue) and writes (red). Native in-software tests show the same Macbook able to achieve read/write speeds of ~860 MB/s with &lt;5% affect on CPU utilization.</p>

<p><small>Addition made Feb 20, 2021:</small> We ran tests using native Postgres as well (instead of in Docker with a bind mount) and found better indexing and query performance, more on this below.</p>

<p><img width="599" alt="image" src="https://user-images.githubusercontent.com/3173176/106507903-ec6f9e80-6488-11eb-88a8-78e5b7aacfd6.png"></p>

<h2 id="indexing-performance-disk-space">Indexing performance: Disk space</h2>

<p>The database contains 9,720,910 files totalling 82.07 GiB:</p>

<div><div><pre><code>postgres=# select count(filepath) from files;
  count  
---------
 9720910
(1 row)

postgres=# select SUM(octet_length(contents)) from files;
     sum     
-------------
 88123563320
(1 row)
</code></pre></div></div>

<p><strong>Before indexing</strong>, we find that all of Postgres is consuming 54G:</p>

<div><div><pre><code>$ du -sh .postgres/
 54G	.postgres/
</code></pre></div></div>

<p>After <code>CREATE INDEX</code>, Postgres uses:</p>

<div><div><pre><code>$ du -sh .postgres/
 73G	.postgres/
</code></pre></div></div>

<p>Thus, the index size for 82 GiB of text is 19 GiB (or 23% of the data size.)</p>

<h2 id="database-startup-times">Database startup times</h2>

<p>From an operational standpoint, it is worth noting that if Postgres is starting clean (i.e. previous shutdown was graceful) then startup time is almost instantaneous: it begins accepting connections immediately and loads the index as needed.</p>

<p>However, if Postgres experienced a non-graceful termination during e.g. startup, it can take a hefty ~10 minutes with this dataset to start as it goes through an automated recovery process.</p>

<h2 id="queries-executed">Queries executed</h2>

<p>In total, we executed 19,936 search queries against the index. We chose queries which we expect give reasonably varying amounts of coverage over the trigram index (that is, queries whose trigrams are more or less likely to occur in many files):</p>

<table>
  <thead>
    <tr>
      <th>Regexp query</th>
      <th>Matching # files in entire dataset</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>var</code></td>
      <td>unknown (2m+ suspected)</td>
    </tr>
    <tr>
      <td><code>error</code></td>
      <td>1,479,452</td>
    </tr>
    <tr>
      <td><code>123456789</code></td>
      <td>59,841</td>
    </tr>
    <tr>
      <td><code>fmt\.Error</code></td>
      <td>127,895</td>
    </tr>
    <tr>
      <td><code>fmt\.Println</code></td>
      <td>22,876</td>
    </tr>
    <tr>
      <td><code>bytes.Buffer</code></td>
      <td>34,554</td>
    </tr>
    <tr>
      <td><code>fmt\.Print.*</code></td>
      <td>37,319</td>
    </tr>
    <tr>
      <td><code>ac8ac5d63b66b83b90ce41a2d4061635</code></td>
      <td>0</td>
    </tr>
    <tr>
      <td><code>d97f1d3ff91543[e-f]49.8b07517548877</code></td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<details>
<summary>Detailed breakdown</summary>
<div>

    <table>
      <thead>
        <tr>
          <th>Query</th>
          <th>Result Limit</th>
          <th>Times executed</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><code>var</code></td>
          <td>10</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>var</code></td>
          <td>100</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>var</code></td>
          <td>1000</td>
          <td>100</td>
        </tr>
        <tr>
          <td><code>var</code></td>
          <td>unlimited</td>
          <td>4</td>
        </tr>
        <tr>
          <td><code>error'</code></td>
          <td>10</td>
          <td>2000</td>
        </tr>
        <tr>
          <td><code>error'</code></td>
          <td>100</td>
          <td>2000</td>
        </tr>
        <tr>
          <td><code>error'</code></td>
          <td>1000</td>
          <td>200</td>
        </tr>
        <tr>
          <td><code>error'</code></td>
          <td>unlimited</td>
          <td>18</td>
        </tr>
        <tr>
          <td><code>123456789</code></td>
          <td>10</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>123456789</code></td>
          <td>100</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>123456789</code></td>
          <td>1000</td>
          <td>100</td>
        </tr>
        <tr>
          <td><code>123456789</code></td>
          <td>unlimited</td>
          <td>2</td>
        </tr>
        <tr>
          <td><code>fmt\.Error</code></td>
          <td>10</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>fmt\.Error</code></td>
          <td>100</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>fmt\.Error</code></td>
          <td>1000</td>
          <td>100</td>
        </tr>
        <tr>
          <td><code>fmt\.Error</code></td>
          <td>unlimited</td>
          <td>2</td>
        </tr>
        <tr>
          <td><code>fmt\.Println</code></td>
          <td>10</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>fmt\.Println</code></td>
          <td>100</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>fmt\.Println</code></td>
          <td>1000</td>
          <td>100</td>
        </tr>
        <tr>
          <td><code>fmt\.Println</code></td>
          <td>unlimited</td>
          <td>2</td>
        </tr>
        <tr>
          <td><code>bytes.Buffer</code></td>
          <td>10</td>
          <td>4</td>
        </tr>
        <tr>
          <td><code>bytes.Buffer</code></td>
          <td>100</td>
          <td>4</td>
        </tr>
        <tr>
          <td><code>bytes.Buffer</code></td>
          <td>1000</td>
          <td>4</td>
        </tr>
        <tr>
          <td><code>bytes.Buffer</code></td>
          <td>unlimited</td>
          <td>2</td>
        </tr>
        <tr>
          <td><code>fmt\.Print.*</code></td>
          <td>10</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>fmt\.Print.*</code></td>
          <td>100</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>fmt\.Print.*</code></td>
          <td>1000</td>
          <td>100</td>
        </tr>
        <tr>
          <td><code>fmt\.Print.*</code></td>
          <td>unlimited</td>
          <td>2</td>
        </tr>
        <tr>
          <td><code>ac8ac5d63b66b83b90ce41a2d4061635</code></td>
          <td>10</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>ac8ac5d63b66b83b90ce41a2d4061635</code></td>
          <td>100</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>ac8ac5d63b66b83b90ce41a2d4061635</code></td>
          <td>1000</td>
          <td>100</td>
        </tr>
        <tr>
          <td><code>ac8ac5d63b66b83b90ce41a2d4061635</code></td>
          <td>unlimited</td>
          <td>2</td>
        </tr>
        <tr>
          <td><code>d97f1d3ff91543[e-f]49.8b07517548877</code></td>
          <td>10</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>d97f1d3ff91543[e-f]49.8b07517548877</code></td>
          <td>100</td>
          <td>1000</td>
        </tr>
        <tr>
          <td><code>d97f1d3ff91543[e-f]49.8b07517548877</code></td>
          <td>1000</td>
          <td>100</td>
        </tr>
        <tr>
          <td><code>d97f1d3ff91543[e-f]49.8b07517548877</code></td>
          <td>unlimited</td>
          <td>2</td>
        </tr>
      </tbody>
    </table>

  </div>
</details>

<h2 id="query-performance">Query performance</h2>

<p>In total, we executed 19,936 search queries against the database (linearly, not in parallel) which completed in the following times:</p>

<table>
  <thead>
    <tr>
      <th>Time bucket</th>
      <th>Percentage of queries</th>
      <th>Number of queries</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Under 50ms</td>
      <td>30%</td>
      <td>5,933</td>
    </tr>
    <tr>
      <td>Under 250ms</td>
      <td>41%</td>
      <td>8,088</td>
    </tr>
    <tr>
      <td>Under 500ms</td>
      <td>52%</td>
      <td>10,275</td>
    </tr>
    <tr>
      <td>Under 750ms</td>
      <td>63%</td>
      <td>12,473</td>
    </tr>
    <tr>
      <td>Under 1s</td>
      <td>68%</td>
      <td>13,481</td>
    </tr>
    <tr>
      <td>Under 1.5s</td>
      <td>74%</td>
      <td>14,697</td>
    </tr>
    <tr>
      <td>Under 3s</td>
      <td>79%</td>
      <td>15,706</td>
    </tr>
    <tr>
      <td>Under 25s</td>
      <td>79%</td>
      <td>15,708</td>
    </tr>
    <tr>
      <td>Under 30s</td>
      <td>99%</td>
      <td>19,788</td>
    </tr>
  </tbody>
</table>

<h2 id="query-performance-vs-planning-time">Query performance vs. planning time</h2>

<p>The following scatter plot shows how 79% of queries executed in under 3s (Y axis, in ms), while Postgres’s query planner had planned them for execution in under 100-250ms generally (X axis, in ms):</p>

<p><img width="1252" alt="image" src="https://user-images.githubusercontent.com/3173176/107848471-ef379100-6db0-11eb-8396-4d156a179aae.png"></p>

<p>If we expand the view to include all queries, we start to get a picture of just how outlier these 21% of queries are (note that the small block of dots in the bottom left represents the same diagram shown above):</p>

<p><img width="1250" alt="image" src="https://user-images.githubusercontent.com/3173176/107848517-3cb3fe00-6db1-11eb-9652-e65d7d88fe36.png"></p>

<h2 id="query-time-vs-cpu--memory-usage">Query time vs. CPU &amp; Memory usage</h2>

<p>The following image shows:</p>

<ul>
  <li>(top) Query time in milliseconds</li>
  <li>(middle) CPU usage percentage (e.g. 801% refers to 8 out of 16 virtual CPU cores being consumed)</li>
  <li>(bottom) Memory usage in MiB.</li>
</ul>

<p><img width="1255" alt="image" src="https://user-images.githubusercontent.com/3173176/107848716-efd12700-6db2-11eb-8e8b-a8141a6bdb0b.png"></p>

<p>Notable insights from this are:</p>

<ul>
  <li>The large increase in resource usage towards the end is when we began executing queries with no <code>LIMIT</code>.</li>
  <li>CPU usage does not exceed 138%, until the spike at the end.</li>
  <li>Memory usage does not exceed 42 MiB, until the spike at the end.</li>
</ul>

<p>We suspect <code>pg_trgm</code> is single-threaded within the scope of a single table, but with <a href="https://www.postgresql.org/docs/10/ddl-partitioning.html">table data partitioning</a> (or splitting data into multiple tables with subsets of the data), we suspect better parallelism could be achieved.</p>

<h2 id="investigating-slow-queries">Investigating slow queries</h2>

<p>If we plot the number of index rechecks (X axis) vs. execution time (Y axis), we can clearly see one of the most significant aspects of slow queries is that they have many more index rechecks:</p>

<p><img width="1036" alt="image" src="https://user-images.githubusercontent.com/3173176/107849660-fc0cb280-6db9-11eb-9c10-cb7e74366ab7.png"></p>

<p>And if we look at <a href="https://github.com/hexops/pgtrgm_emperical_measurements/blob/main/query_logs/query-run-3.log#L3-L24">the <code>EXPLAIN ANALYZE</code> output for one of these queries</a> we can also confirm <code>Parallel Bitmap Heap Scan</code> is slow due to <code>Rows Removed by Index Recheck</code>.</p>

<h2 id="table-splitting">Table splitting</h2>

<p>Splitting up the search index into multiple smaller tables seems like an obvious approach to getting <code>pg_trgm</code> to use multiple CPU cores. We tried this by taking the same exact data set and splitting it into 200 tables, and found …</p></div></article></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://devlog.hexops.com/2021/postgres-regex-search-over-10000-github-repositories">https://devlog.hexops.com/2021/postgres-regex-search-over-10000-github-repositories</a></em></p>]]>
            </description>
            <link>https://devlog.hexops.com/2021/postgres-regex-search-over-10000-github-repositories</link>
            <guid isPermaLink="false">hacker-news-small-sites-26220446</guid>
            <pubDate>Mon, 22 Feb 2021 03:15:30 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Is pulsar better than kafka?]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26220404">thread link</a>) | @fsynced
<br/>
February 21, 2021 | https://www.kai-waehner.de/blog/2020/06/09/apache-kafka-versus-apache-pulsar-event-streaming-comparison-features-myths-explored/ | <a href="https://web.archive.org/web/*/https://www.kai-waehner.de/blog/2020/06/09/apache-kafka-versus-apache-pulsar-event-streaming-comparison-features-myths-explored/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>

			<p><b>Pulsar vs Kafka</b> – which one is better? This blog post explores <b>pros and cons, popular myths, and non-technical criteria to find the best tool for your business problem</b>.</p>
<p>My discussions are usually around Apache Kafka and its ecosystem as I work for Confluent. <b>The only questions I got about Pulsar in the last years came from Pulsar committers and contributors</b>. They asked me deep technical questions so as to be able to explain where Kafka sucks and why Pulsar is the much better option. Discussions about this topic on platforms like Reddit are typically very opinionated, often inaccurate, and brutal. The following is my point of view based on years of experience with open source streaming platforms.</p>
<h2 id="tech-comparisons-are-the-new-black-kafka-vs-middleware-event-streaming-and-api-platforms">Tech comparisons are the new black: Kafka vs. Middleware, Event Streaming and API Platforms</h2>
<p>Tech comparisons are meant to <b>guide people to choose the right solution and architecture for their business problem</b>. There is no all-rounder, and there should be no bias. Choose the right tool for the problem.</p>
<p>However, <strong>technical comparisons are almost always biased</strong>. Even if the author does not work for a vendor and is an “independent” consultant, he or she is still likely to have a biased opinion from past experiences and knowledge, whether purposely or unknowingly. Still, comparisons from different perspectives are useful, and we’ve seen Apache Pulsar discussed in a few places on the internet, so I wanted to share my personal views of how Kafka and Pulsar compare. <strong>I work for Confluent, the leading experts behind Apache Kafka and its ecosystem</strong>, so keep that in mind, but the aim of this post is not to provide opinion, it’s to <b>weigh up facts rather than myths</b>.</p>
<p>Technical comparisons of open source frameworks and commercial software products happen all the time. I did several comparisons in the past on my blog or other platforms like InfoQ, including a <a href="https://www.kai-waehner.de/blog/2012/01/10/spoilt-for-choice-which-integration-framework-to-use-spring-integration-mule-esb-or-apache-camel/">Comparison of integration frameworks</a>, <a href="https://www.infoq.com/articles/ESB-Integration">Choosing the right ESB for your integration needs</a>, <a href="https://www.kai-waehner.de/blog/2019/03/07/apache-kafka-middleware-mq-etl-esb-comparison/">Kafka vs. ETL / ESB / MQ</a>, <a href="https://www.kai-waehner.de/blog/2020/04/24/mainframe-offloading-replacement-apache-kafka-connect-ibm-db2-mq-cdc-cobol/">Kafka vs. Mainframe</a> and <a href="https://www.kai-waehner.de/blog/2020/05/25/api-management-gateway-apache-kafka-comparison-mulesoft-kong-apigee/">Apache Kafka and API Management / API Gateway</a>. All these comparisons were done because customers wanted to understand when to use which tool.</p>
<p>For Pulsar vs. Kafka, the situation is a little bit different.</p>
<h2 id="why-compare-pulsar-and-kafka">Why compare Pulsar and Kafka?</h2>
<p><b>Talking to prospects or customers, I rarely get asked about Pulsar.</b> To be fair, this increased slightly in the last months. I guess the question comes up in every ~15th or ~20th meeting due to the overlapping feature set and use cases. However, this seems to be mostly due to a few posts on the internet that claim Pulsar is in some ways better than Kafka. There is no fact-checking and very little material, if any, for the opposing view.</p>
<p><b>I have not talked to a single organization that seriously considered deploying Pulsar in production, </b>although I know there are a large number of users out there in the world who need a distributed messaging technology like Kafka or Pulsar. But I also think that Pulsar’s alleged reference users are not particularly accurate.</p>
<p>For example, their flagship user is Tencent, a large Chinese tech company, but Tencent is a huge Kafka user, whereas Pulsar’s use is limited to just one project. <strong>Tencent processes trillion messages per day (in digits: 10,000,000,000,000) with Kafka</strong>. As it turns out, <strong>Tencent uses Kafka 1000x more than Pulsar </strong>(ten trillion msg/day vs. tens of billion msg/day)<strong>.</strong> The Tencent team discussed their Kafka deployment in more detail: <a href="https://www.confluent.io/blog/tencent-kafka-process-10-trillion-messages-per-day/" target="_blank" rel="noopener noreferrer">How Tencent PCG Uses Apache Kafka to Handle 10 Trillion+ Messages Per Day</a>.</p>
<h4 id="comparison-of-two-competitive-open-source-frameworks">Comparison of two competitive open source frameworks</h4>
<p><b>Apache Kafka and Apache Pulsar are two exciting and competing technologies</b>. Therefore, it makes a lot of sense to compare them. Period.</p>
<p><strong>Both Apache Kafka and Apache Pulsar have very similar feature sets</strong>. I recommend that you evaluate both frameworks for available features, maturity, market adoption, open source tools and projects, training material, availability of local meetups, videos, blog posts, etc. Reference use cases from your industry or business problems help making the right decision.</p>
<p>Confluent published such a comparison of “<a href="https://www.confluent.io/kafka-vs-pulsar/">Kafka vs. Pulsar vs. RabbitMQ: Performance, Architecture, and Features Compared</a>“. I was involved in creating this comparison. So we have that comparison already…</p>
<p>What is this blog post here about then?</p>
<p>I want to <b>explore the myths from some ‘Kafka vs. Pulsar’ arguments</b> which I see regularly in blog posts and forum discussions. Afterwards, I will give a more <b>comprehensive comparison beyond just technical aspects</b> because most Pulsar discussions focus purely on tech features.</p>
<p><img src="https://www.kai-waehner.de/wp-content/uploads/2020/05/Apache-Kafka-vs-Apache-Pulsar-Comparison.jpg" data-src="https://www.kai-waehner.de/wp-content/uploads/2020/05/Apache-Kafka-vs-Apache-Pulsar-Comparison.jpg" alt="Apache Kafka vs Apache Pulsar Comparison and Myths Explored" width="760" height="572" data-srcset="https://www.kai-waehner.de/wp-content/uploads/2020/05/Apache-Kafka-vs-Apache-Pulsar-Comparison.jpg 760w, https://www.kai-waehner.de/wp-content/uploads/2020/05/Apache-Kafka-vs-Apache-Pulsar-Comparison-300x225.jpg 300w, https://www.kai-waehner.de/wp-content/uploads/2020/05/Apache-Kafka-vs-Apache-Pulsar-Comparison-200x150.jpg 200w, https://www.kai-waehner.de/wp-content/uploads/2020/05/Apache-Kafka-vs-Apache-Pulsar-Comparison-380x286.jpg 380w" data-sizes="(max-width: 760px) 100vw, 760px" srcset="https://www.kai-waehner.de/wp-content/uploads/2020/05/Apache-Kafka-vs-Apache-Pulsar-Comparison.jpg 760w, https://www.kai-waehner.de/wp-content/uploads/2020/05/Apache-Kafka-vs-Apache-Pulsar-Comparison-300x225.jpg 300w, https://www.kai-waehner.de/wp-content/uploads/2020/05/Apache-Kafka-vs-Apache-Pulsar-Comparison-200x150.jpg 200w, https://www.kai-waehner.de/wp-content/uploads/2020/05/Apache-Kafka-vs-Apache-Pulsar-Comparison-380x286.jpg 380w"></p>
<h2 id="kafka-vs-pulsar-technology-myths-explored">Kafka vs Pulsar – Technology myths explored<b><br>
</b></h2>
<p><b>The following discusses some myths I have come across. I agree with some of them, but also counter some others with hard facts</b>. Of course, different opinions can exist for some of these statements. Again, this is totally fine. The following is my point of view.</p>
<h3 id="myth-1-pulsar-has-differentiating-built-in-features-compared-to-kafka">Myth 1: “Pulsar has differentiating built-in features compared to Kafka”?</h3>
<p><b>True.</b></p>
<p>If you compare Apache Kafka to Apache Pulsar, features like its tiered architecture, queuing, and multi-tenancy are mentioned as differentiators.</p>
<p><b>But:</b></p>
<p>Kafka has many differentiating features, too:</p>
<ul>
<li>Half as many servers to run</li>
<li>Data saved to disk only once</li>
<li>Data cached in memory only once</li>
<li>Battle-tested replication protocol</li>
<li>Zero copy performance</li>
<li>Transactions</li>
<li>Built-in stream processing</li>
<li>Long term storage</li>
<li>In the works: ZooKeeper removal (<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+ZooKeeper+with+a+Self-Managed+Metadata+Quorum">KIP-500</a>), which makes Kafka even more simple to operate and deploy than Pulsar (which has a four-component architecture of Pulsar, ZooKeeper, BookKeeper, and RocksDB), apart from making Kafka more scalable, more resilient, etc. etc..)</li>
<li>In the works: Tiered Storage (<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-405%3A+Kafka+Tiered+Storage">KIP-405</a>), which makes Kafka more elastic and cost-efficient.</li>
</ul>
<p>Also ask yourself: Should you really compare just the open source frameworks or products and vendors with their complete offering?</p>
<p>It is <b>easy to add new features if you don’t have to provide mission-critical support for it</b>. Don’t just evaluate features in a checklist, but also evaluate how they are battle-tested in production scenarios. <b>How many “differentiating features” are low-quality and implemented quickly vs. high-quality implementations?</b></p>
<p>For instance: <b>It took a few years to implement and battle-test Kafka Streams as Kafka-native stream processing engine. Do you really want to compare this to Pulsar Functions?</b> The latter is a feature to add user-defined functions (UDF); without any relation to “real stream processing”. Or is this more like Single Message Transformations (SMT), a core feature of Kafka Connect? Just be sure to a) compare apples to apples (instead of apples to oranges) and b) don’t forget to think about the maturity of a feature. The more powerful and critical, the more mature it should be…</p>
<p>The Kafka community spends a large amount of efforts to improve the core project and its ecosystem. <b>Confluent alone has over 200 full time engineers</b> working on the Kafka project, additional community components, commercial products and the SaaS offering on major cloud providers.</p>
<h3 id="myth-2-pulsar-has-a-few-very-big-users-like-tencent-in-china">Myth 2: “Pulsar has a few very big users like Tencent in China”?</h3>
<p><b>True.</b></p>
<p><b>But: Tencent actually uses Kafka more than Pulsar</b>. The billing department, which uses Pulsar, is only a small fraction at Tencent, whereas a large portion of the core business is using Kafka, and they have a Global-Kafka like architecture that combines 1000+ brokers into a single logical cluster.</p>
<p>Always be cautious with open source projects. Check out the success at “normal companies”. Just because a tech giant uses it, does not mean it will work for your company well. How many Fortune 2000 companies shared their success stories around Pulsar in the past?</p>
<h4 id="look-for-proof-points-beyond-tech-giants">Look for proof points beyond tech giants!</h4>
<p><b>Proof points beyond the tech giants are helpful to get insights and lessons learned from other people</b>. Not from the software vendors. The Kafka website gives many <a href="https://kafka.apache.org/powered-by">examples about mission-critical deployments</a>. Even more impressive: At the past<a href="https://kafka-summit.org/"> Kafka Summit</a> conferences in San Francisco, New York and London, every year various enterprises from different industries present their use cases and success stories. Including fortune 2000 companies, mid-size enterprises and startups.</p>
<p>Just to give you <b>one specific example in the Kafka world</b>: Various different implementations exist for replication of data in real time between separate Kafka clusters, including MirrorMaker 1 (part of the Apache Kafka project), MirrorMaker 2 (part of the Apache Kafka project), Confluent Replicator (built by Confluent and only available as part of Confluent Platform or Confluent Cloud), uReplicator (open sourced by Uber), Mirus (open sourced by Salesforce), Brooklin (open sourced by LinkedIn).</p>
<p>In practice, only two options are reasonable if you don’t want to maintain and improve the code by yourself: MirrorMaker 2 (very new, not mature yet, but a great option mid and long term) and Confluent Replicator (battle-tested in many mission-critical deployments, but not open source). All the other options work, too. But who maintains the projects? Who solves bugs and security issues? Who do you call when you have a problem in production? Deployment in production for mission-critical deployments is different from evaluating and trying out an open source project.</p>
<h3 id="myth-3-pulsar-provides-message-queuing-and-event-streaming-in-a-single-solution">Myth 3: “Pulsar provides message queuing and event streaming in a single solution”?</h3>
<p><b>Partly.</b></p>
<p>Message queues are used for point-to-point communication. They provide an asynchronous communications protocol, meaning that the sender and receiver of the message do not need to interact with the message queue at the same time.d</p>
<p><strong>Pulsar has only</strong> <b data-stringify-type="bold">limited support for message queuing, and limited support for event streaming</b>. If it wants to compete in either area, it still has a long way to go for two reasons:</p>
<p><b data-stringify-type="bold">1) Pulsar has only limited support for message queuing</b> because it misses popular messaging features like message XA transactions, routing, message filtering, etc. that are commonly used with messaging systems like IBM MQ, RabbitMQ, and ActiveMQ. …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.kai-waehner.de/blog/2020/06/09/apache-kafka-versus-apache-pulsar-event-streaming-comparison-features-myths-explored/">https://www.kai-waehner.de/blog/2020/06/09/apache-kafka-versus-apache-pulsar-event-streaming-comparison-features-myths-explored/</a></em></p>]]>
            </description>
            <link>https://www.kai-waehner.de/blog/2020/06/09/apache-kafka-versus-apache-pulsar-event-streaming-comparison-features-myths-explored/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26220404</guid>
            <pubDate>Mon, 22 Feb 2021 03:09:39 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Against Packaging Rust Crates]]>
            </title>
            <description>
<![CDATA[
Score 2 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26220147">thread link</a>) | @lifthrasiir
<br/>
February 21, 2021 | https://fy.blackhats.net.au/blog/html/2021/02/16/against_packaging_rust_crates.html | <a href="https://web.archive.org/web/*/https://fy.blackhats.net.au/blog/html/2021/02/16/against_packaging_rust_crates.html">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div id="against-packaging-rust-crates">

<p>Recently the discussion has once again come up around the notion of packaging Rust crates as
libraries in distributions. For example, taking a library like <cite>serde</cite> and packaging it to
an RPM. While I use RPM as the examples here it applies equally to other formats.</p>
<p>Proponents of crate packaging want all Rust applications to use the “distributions” versions of a crate.
This is to prevent “vendoring” or “bundling”. This is where an
application (such as 389 Directory Server) ships all of it’s sources, as well as the sources of
it’s Rust dependencies in a single archive. These sources may differ in version from the bundled
sources of other applications.</p>
<div id="packaging-crates-is-not-reinventing-cargo">
<h2>“Packaging crates is not reinventing Cargo”</h2>
<p>This is a common claim by advocates of crate packaging. However it is easily disproved:</p>
<p><em>If packaging is not reinventing cargo, I am free to use all of Cargo’s features without conflicts to distribution packaging.</em></p>
<p>The reality is that packaging crates <em>is</em> reinventing Cargo - but without all it’s features. Common
limitations are that Cargo’s exact version/less than requirements can not be used safely, or Cargo’s ability to
apply patches or uses sources from specific git revisions can not be used at all.</p>
<p>As a result, this hinders upstreams from using all the rich features within Cargo to comply with
distribution packaging limitations, or it will cause the package to hit exceptions in policy and
necesitate vendoring anyway.</p>
</div>
<div id="you-can-vendor-only-in-these-exceptional-cases">
<h2>“You can vendor only in these exceptional cases …”</h2>
<p>As noted, since packaging is reinventing Cargo, if you use features of Cargo that are unsupported
then you may be allowed to vendor depending on the distributions policy. However, this raises some
interesting issues itself.</p>
<p>Assume I have been using distribution crates for a period of time - then the upstream adds an exact version
or git revision requirement to a project or a dependency in my project. I now need to change my spec file and tooling to use vendoring
and all of the benefits of distribution crates no longer exists (because you can not have any dependency
in your tree that has an exact version rule).</p>
<p>If the upstream ‘un-does’ that change, then I need to roll back to distribution crates since
the project would no longer be covered by the exemption.</p>
<p>This will create review delays and large amounts of administrative overhead. It means pointless effort to swap between
vendored and distribution crates based on small upstream changes. This may cause packagers to avoid
certain versions or updates so that they do not need to swap between distribution methods.</p>
<p>It’s very likely that these “exceptional” cases will be very common, meaning that vendoring will be occuring.
This necesitates supporting vendored applications in distribution packages.</p>
</div>
<div id="you-don-t-need-to-package-the-universe">
<h2>“You don’t need to package the universe”</h2>
<p>Many proponents say that they have “already packaged most things”. For example in 389 Directory Server
of our 60 dependencies, only 2 were missing in Fedora (2021-02). However this overlooks the fact
that I do not want to package those 2 other crates just to move forward. I want to support 389 Directory Server
the <em>application</em> not all of it’s dependencies in a distribution.</p>
<p>This is also before we come to larger rust projects, such as Kanidm that has nearly 400 dependencies. The
likelihood that many of them are missing is high.</p>
<p>So you will need to package the universe. Maybe not all of it. But still a lot of it. It’s already
hard enough to contribute packages to a distribution. It becomes even harder when I need to submit 3, 10, or 100
more packages. It could be months before enough approvals were in place. It’s a staggering
amount of administration and work, which will discourage many contributors.</p>
<p>People have already contacted me to say that if they had to package crates to distribution packages to
contribute, they would give up and walk away. We’ve already lost future contributors.</p>
<p>Further to this Ruby, Python and many other languages today all recommend language native tools
such as rvm or virtualenv to avoid using distribution packaged libraries.</p>
<p>Packages in distributions should exist as a vehicle to ship bundled applications that are created
from their language native tools.</p>
</div>
<div id="we-will-update-your-dependencies-for-you">
<h2>“We will update your dependencies for you”</h2>
<p>A supposed benefit is that versions of crates in distributions will be updated in the background
according to semver rules.</p>
<p>If we had an exact version requirement (that was satisfiable), a silent background update will cause
this to no longer work - and will break the application from building. This would necesitate one of:</p>
<ul>
<li>A change to the Cargo.toml to remove the equality requirement - a requirement that may exist for good reason.</li>
<li>It will force the application to temporarily swap to vendoring instead.</li>
<li>The application will remain broken and unable to be updated until upstream resolves the need for the equality requirement.</li>
</ul>
<p>Background updates also ignore the state of your Cargo.lock file by removing it. A Cargo.lock file
is recommended to be checked in with binary applications in Rust, as evidence that shows “here is
an exact set of dependencies that upstream has tested and verified as building and working”.</p>
<p>To remove and ignore this file, means to remove the guarantees of quality from an upstream.</p>
<p>It is unlikely that packagers will run the entire test suite of an application to regain this
confidence. They will “apply the patch and pray” method - as they already do with other languages.</p>
<p>We can already see how background updates can have significant negative consequences on application stability. FreeIPA
has hundreds of dependencies, and it’s common that if any of them changes in small ways, it can cause
FreeIPA to fall over. This is not the fault of FreeIPA - it’s the fault of relying on so many small
moving parts that can change underneath your feet without warning. FreeIPA would strongly benefit from
vendoring to improve it’s stability and quality.</p>
<p>Inversely, it can cause hesitation to updating libraries - since there is now a risk of breaking
other applications that depend on them. We do not want people to be afraid of updates.</p>
</div>
<div id="we-can-respond-to-security-issues">
<h2>“We can respond to security issues”</h2>
<p>On the surface this is a strong argument, but in reality it does not hold up. The security issues
that face Rust are significantly different to that which affect C. In C it may be viable to patch
and update a dynamic library to fix an issue. It saves time because you only need to update and change
one library to fix everything.</p>
<p>Security issues are much rarer in Rust. When they occur, you will have to update and re-build all
applications depending on the affected library.</p>
<p>Since this rebuilding work has to occur, where the security fix is applied is irrelevant. This frees
us to apply the fixes in a different way to how we approach C.</p>
<p>It is better to apply the fixes in a consistent and universal manner. There <em>will</em> be applications
that are vendored due to vendoring exceptions, there is now duplicated work and different
processes to respond to both distribution crates, and vendored applications.</p>
<p>Instead all applications could be vendored, and tooling exists that would examine the Cargo.toml to
check for insecure versions (RustSec/cargo-audit does this for example). The Cargo.toml’s can be
patched, and applications tested and re-vendored. Even better is these changes could easily then be forwarded to
upstreams, allowing every distribution and platform to benefit from the work.</p>
<p>In the cases that the upstream can not fix the issue, then Cargo’s native patching tooling can
be used to supply fixes directly into vendored sources for rare situations requiring it.</p>
</div>
<div id="patching-20-vulnerable-crates-doesn-t-scale-we-need-to-patch-in-one-place">
<h2>“Patching 20 vulnerable crates doesn’t scale, we need to patch in one place!”</h2>
<p>A common response to the previous section is that the above process won’t scale as we need to find
and patch 20 locations compared to just one. It will take “more human effort”.</p>
<p>Today, when a security fix comes out, every distribution’s security teams will have to be made aware of
this. That means - OpenSUSE, Fedora, Debian, Ubuntu, Gentoo, Arch, and many more groups all have to
become aware and respond. Then each of these projects security teams will work with their maintainers
to build and update these libraries. In the case of SUSE and Red Hat this means that multiple developers
may be involved, quality engineering will be engaged to test these changes. Consumers of that library
will re-test their applications in some cases to ensure there are no faults of the components they
rely upon. This is all before we approach the fact that each of these distributions have many supported
and released versions they likely need to maintain so this process may be repeated for patching and
testing multiple versions in parallel.</p>
<p>In this process there are a few things to note:</p>
<ul>
<li>There is a huge amount of human effort today to keep on top of security issues in our distributions.</li>
<li>Distributions tend to be isolated and can’t share the work to resolve these - the changes to the rpm specs in SUSE won’t help Debian for example.</li>
<li>Human error occurs in all of these layers causing security issues to go un-fixed or breaking a released application.</li>
</ul>
<p>To suggest that rust and vendoring somehow makes this harder or more time consuming is discounting
the huge amount of time, skill, and effort already put in by people to keep our C based distributions functioning
today.</p>
<p>Vendored Rust won’t make this process easier or harder - it just changes the nature of the effort
we have to apply as maintainers and distributions. It shifts our focus from “how do we ensure this
library is secure” to “how do we ensure this <em>application</em> made from many libraries is secure”. It
allows further collaboration with upstreams to be involved in the security update process, which ends up
benefiting <em>all</em> distributions.</p>
</div>
<div id="it-doesn-t-duplicate-effort">
<h2>“It doesn’t duplicate effort”</h2>
<p>It does. By the very nature of both distribution libraries and vendored applications needing to
exist in a distribution, there will become duplicated but seperate processes and …</p></div></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://fy.blackhats.net.au/blog/html/2021/02/16/against_packaging_rust_crates.html">https://fy.blackhats.net.au/blog/html/2021/02/16/against_packaging_rust_crates.html</a></em></p>]]>
            </description>
            <link>https://fy.blackhats.net.au/blog/html/2021/02/16/against_packaging_rust_crates.html</link>
            <guid isPermaLink="false">hacker-news-small-sites-26220147</guid>
            <pubDate>Mon, 22 Feb 2021 02:38:26 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Show HN: Building a Binary Counter]]>
            </title>
            <description>
<![CDATA[
Score 3 | Comments 0 (<a href="https://news.ycombinator.com/item?id=26219601">thread link</a>) | @lowdanie
<br/>
February 21, 2021 | https://www.daniellowengrub.com/blog/2021/02/08/binary-counter | <a href="https://web.archive.org/web/*/https://www.daniellowengrub.com/blog/2021/02/08/binary-counter">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
  <p>A binary counter is an electronic component that records the number of times it has received a pulse. It is called <em>binary</em> because it stores the number in its binary representation. Counters are absolutely ubiquitous in electronics and can be used to make circuits ranging from memory chips to FM radio decoders.</p>

<p>Since counters are so useful, I thought it would be fun to implement one using only basic logic gates like NAND and OR. As a practical application, I hooked up a 1-bit counter to a clock and a pair of LEDs to create a random bit generator:</p>

<p><img src="https://www.daniellowengrub.com/assets/binary_counter/random_breadboard.jpeg" alt="Random Generator"></p>

<p>The output of the counter is connected to two LEDs. One of them lights up when the counter is in the 0 state and the other lights up in the 1 state. When the button is pressed down, the clock starts sending out around 100 pulses per second causing the counter to oscillate between 0 and 1. When the button is released the timer stops and the the counter remains in its most recent state as indicated by the corresponding LED. The state is maintained until the next time the button is pressed.</p>

<p>Since the oscillations are very fast relative to human reflexes, the LED that remains lit after each button release appears to be chosen randomly with each LED appearing with equal probability. In other words, one can think of this circuit as simulating a coin flip where one of the LEDs represents heads and the other tails. At the end of the post I will provide some evidence that two outcomes are indeed equally likely.</p>

<p>The primary goal of this post is to explain how to make a 1 bit counter out of basic logic gates. After that weâ€™ll get into the details of the complete random bit generator circuit shown above. Weâ€™ll conclude by showing how multiple 1 bit counters can easily be chained together to produce larger counters such as the useful 8 bit counter.</p>


<p>Here is a high level schematic of the one bit binary counter we want to build:</p>

<p><img src="https://www.daniellowengrub.com/assets/binary_counter/one_bit_counter.png" alt="One Bit Counter"></p>

<p>We now turn to the design requirements.</p>

<p>The counter has a single input which is labelled â€œClockâ€� and two outputs labelled $Q$ and $\overline{Q}$. The input and output wires can either have a â€œhighâ€� voltage or a â€œlowâ€� voltage. In our case, we will be using a 5 volt power supply so a â€œhighâ€� wire will be close to 5 volts and a â€œlowâ€� wire will be close to 0 volts.</p>

<p>The overline on the bottom output represents the fact that its state is always opposite to $Q$. I.e, if $Q$ is high then $\overline{Q}$ is low and vice versa. It may seem redundant to include both $Q$ and $\overline{Q}$ but later we will see why it is useful. We summarize the state of the counter by writing $Q=1$ if $Q$ is high and $Q=0$ if $Q$ is low.</p>

<p>As long as the clock input is low the output state should remain fixed. But each time the the clock receives a pulse (explained below) the values of the outputs should flip.</p>

<p>In more detail, a â€œpulseâ€� means that the clock input rises from low to high, and then quickly falls back to low. Since the output should only change one time per pulse, we would like the outputs to flip whenever the clock input <em>rises</em> from low to high. This special behavior is indicated by the triangle next to the clock input.</p>

<p>You may be wondering why we need this fancy pulse behavior. Why canâ€™t we simply demand that the outputs flip whenever the input is high? The issue with that design is that every pulse has some non-zero duration. So even a short pulse would cause the outputs to start rapidly flipping back and forth from the moment the clock input went high until the end of the pulse when it went low again. This would make the final state undefined!</p>

<p>In contrast, the input voltage rises from low to high exactly once per pulse so our design guarantees that the outputs will flip exactly once whenever the input receives a pulse.</p>

<p>Now that weâ€™ve specified the counterâ€™s behavior we turn to the implementation.</p>

<p>There are two main implementation challenges. First, how does the counter maintain its state between pulses? Second of all, how is it possible to detect the low to high transition exactly once per pulse?</p>

<p>In the next section we focus on the first issue by considering a simpler type of component that has two separate â€œonâ€� and â€œoffâ€� inputs rather than the complex clock input. We will then solve the pulse problem by chaining two of these simpler components to each other!</p>

<h2 id="the-sr-flip-flop">The SR Flip-flop</h2>
<p>In this section weâ€™ll build a component called the <a href="https://en.wikipedia.org/wiki/Flip-flop_(electronics)#Simple_set-reset_latches">Set Reset Flip-flop</a> or <em>SR Flip-flop</em> for short.</p>

<p>Here is a diagram of the SR flip-flop:
<img src="https://www.daniellowengrub.com/assets/binary_counter/sr_flipflop.png" alt="SR Flip-flop"></p>

<p>This flip-flop has two inputs: $S$ (â€œsetâ€�) and $R$ (â€œresetâ€�) and two outputs: $Q$ and $\overline{Q}$. As before, $\overline{Q}$ always has the opposite value of $Q$. So if $Q$ is high then $\overline{Q}$ is low and vice versa.</p>

<p>In its default state, both inputs $S$ and $R$ are high and the output $Q$ is low. If $S$ is pulled low (i.e $S=0$) this <em>sets</em> the gate and causes the output $Q$ to be high (and therefore $\overline{Q}$ to be low). The output will stay in this state even when $S$ goes high again. On the other hand, pulling $R$ low <em>resets</em> the gate which means that $Q$ will go low. As before, $Q$ will remain low even when $R$ goes back to being high.</p>

<p>In summary, we can flip between the two possible output states by lowering either $S$ or $R$. Furthermore, the SR flip-flop maintains its state until the next set or reset operation.</p>

<p>It turns out that it is possible to build an SR latch out of just two <a href="https://en.wikipedia.org/wiki/NAND_gate">NAND gates</a> via an ingenious mechanism called the <a href="https://en.wikipedia.org/wiki/Flip-flop_(electronics)#SR_NAND_latch">NAND latch</a> as shown in the following diagram:</p>

<p><img src="https://www.daniellowengrub.com/assets/binary_counter/nand_latch_circuit.png" alt="NAND Latch"></p>

<p>We are using the convention that an â€œXâ€� intersection of wires means that the wires do not touch but rather cross over each other.</p>

<p>How does this circuit work? Lets see what happens if we start in the defaults state ($S=R=\overline{Q}=1$, $Q=0$) and perform a <em>set</em> operation:</p>

<p><img src="https://www.daniellowengrub.com/assets/binary_counter/nand_latch_states.png" alt="NAND Latch"></p>

<p>In the initial state the input to the top NAND is $(1, 1)$ and so its output is $Q=0$. The input to the bottom NAND is $(0, 1)$ so its output is $\overline{Q}=1$. Since everything is consistent, the gates will stay in this configuration until we change one of the the inputs.</p>

<p>Now lets perform a â€œsetâ€� operation by pulling down $S$ to a low state to get $S=0$. Since one of the inputs to the top NAND is $0$, its output will be $Q=1$ regardless of the other input. This means that the input to the bottom NAND is $(1, 1)$ causing its output to be $\overline{Q}=0$.</p>

<p>Finally, lets see what happens if we release $S$ and let it go back to the default high state $S=1$. The input to the top NAND is now $(1, 0)$ which means that its output is still $Q=1$. Therefore the output to the bottom NAND is still $(1, 1)$ causing its output to stay at $\overline{Q}=0$.</p>

<p>In summary, we can see that one cycle of $S=0 \Rightarrow S=1$ <em>sets</em> the output to $Q=1,\,\overline{Q}=0$. A similar analysis shows that a cycle of $R=0 \Rightarrow R=1$ <em>resets</em> the output to $Q=0,\,\overline{Q}=1$.</p>

<h2 id="a-1-bit-counter-implementation">A 1-Bit Counter Implementation</h2>
<p>We now return to the problem of building a one bit counter. The SR flip-flop from the last section gets us pretty close: It has the outputs $Q$ and $\overline{Q}$ and allows us to toggle between them by pulling down $S$ or $R$. To turn this into a counter we need to replace the set/reset inputs with a single <em>clock</em> input.</p>

<p>The general logic should be:</p>
<ul>
  <li>If the clock is low then then the SR inputs should be in their default state of $S=R=1$. This will result in $Q$ maintaining its current state.</li>
  <li>If the clock is high and $Q=1$, <em>reset</em> the SR flip-flop by setting $R=0$. This will result in $Q=0$.</li>
  <li>If the clock is high and $Q=0$, <em>set</em> the SR flip-flop by setting $S=0$. This will result in $Q=1$.</li>
</ul>

<p>We can implement this logic by wiring up an SR flip-flop with two NAND gates like so:</p>

<p><img src="https://www.daniellowengrub.com/assets/binary_counter/one_bit_circuit_simple.png" alt="One Bit Counter Circuit"></p>

<p>As before, wires that meet in an X intersection do not touch each other. Weâ€™ve connected the $Q$ output to the bottom NAND and the $\overline{Q}$ output to the top NAND.</p>

<p>Letâ€™s verify that this circuit follows the logic outlined above. Indeed, if $\mathrm{Clock}=0$ then both of the NANDs will have a $0$ input and so their outputs will always both be $1$. This means that the two inputs to the SR flip-flop will be in their default $1$ state as desired.</p>

<p>What happens when the clock goes high ($\mathrm{Clock}=1$)? Suppose that $Q=1,\,\overline{Q}=0$. Then the inputs to the top NAND will be $(\overline{Q}=0, \mathrm{Clock}=1)$ and so its output will be $1$. On the other hand, the inputs to the bottom NAND will be $(\mathrm{Clock}=1, Q=1)$ so its output will be $0$. Together this means that the input to the SR flip-flop will be $S=1$, $R=0$ which by definition will <em>reset</em> the flip-flop to $Q=0,\,\overline{Q}=1$.</p>

<p>If the clock goes high again it is not hard to see that the SR inputs will now be $(S=0,\,R=1)$ causing it to <em>set</em> the output back to $Q=0,\,\overline{Q}=1$.</p>

<p>The only problem with this setup is that the outputs will keep flipping as long as the clock is high! Since each clock pulse has some non-zero duration, this version of the counter will flip many times per pulse rather than just once.</p>

<p>The solution is to use <em>two</em> SR flip-flops. One will record the current output and the other will record the output for the next pulse. The trick is that the â€œcurrentâ€� flip-flop is activated when the clock goes high as above, but the â€œnextâ€� flip-flop will be activated when the clock goes low. The effect is that the counter is only updated after a complete cycle $\mathrm{Clock}=0 \Rightarrow \mathrm{Clock}=1$, preventing the oscillations in our first version.</p>

<p>Here is an implementation of this idea:</p>

<p><img src="https://www.daniellowengrub.com/assets/binary_counter/one_bit_circuit.png" alt="One Bit Counter Circuit"></p>

<p>When the clock is <em>low</em> it is easy to see that $S_{cur}=R_{cur}=1$ meaning that the â€œcurrentâ€� flip-flop will not be updated. In contrast, when the clock is <em>high</em> $S_{next}=R_{next}=1$ and so the â€œnextâ€� clock will not be updated. It is not hard to verify that with this version the outputs $Q,\,\overline{Q}$ flip exactly once when the clock receives a pulse.</p>


<p>In this section we will use a 1 bit counter to build the random bit generator we described in …</p></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://www.daniellowengrub.com/blog/2021/02/08/binary-counter">https://www.daniellowengrub.com/blog/2021/02/08/binary-counter</a></em></p>]]>
            </description>
            <link>https://www.daniellowengrub.com/blog/2021/02/08/binary-counter</link>
            <guid isPermaLink="false">hacker-news-small-sites-26219601</guid>
            <pubDate>Mon, 22 Feb 2021 01:33:10 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[Frustrated with Parler deplatforming, I am building a service no one can silence]]>
            </title>
            <description>
<![CDATA[
Score 110 | Comments 190 (<a href="https://news.ycombinator.com/item?id=26218900">thread link</a>) | @anon20190221
<br/>
February 21, 2021 | https://1b677b8f8bb20100.github.io/introduction/ | <a href="https://web.archive.org/web/*/https://1b677b8f8bb20100.github.io/introduction/">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
      
      
      

      

<p>This is the first post in this blog, it was published on February 19, 2021.</p>

<h2 id="motivation">Motivation</h2>

<h3 id="censorship">Censorship</h3>

<p>The year of 2020 will be remembered for the pandemic, the BLM movement, and the U.S. elections among other billion of things around the globe. It is funny I even mention the third one considering how little I give a damn about U.S. politics, yet this whole story begins with <a href="https://en.wikipedia.org/wiki/Parler">Parler</a> deplatforming that happened about a month ago. Let me remind you: Apple, Google, Amazon, and a few other companies terminated their service to the free speech social network for insufficient moderation effectively destroying the platform in a matter of just a couple of days. What the fuck?</p>

<p>OK, let me be clear with my position: I believe every private company has a right to refuse service to anyone, whether an individual or a business, but I also have my own right to despise them for exercising that. What they did was probably legal, but screw them anyway, they failed us. Regardless what these psychopathic corporations like to tell the public, they are only concerned with maximizing shareholder value, and if there is anything  even remotely resembling an image liability (through pressure by political radicals, cancel culture SJWs, you name it), they will not think twice. What disgusts me the most here is neither greed nor hypocrisy but their unwillingness to grow a pair of balls and stand up for freedom of speech.</p>

<p>You see, freedom of speech and expression must be absolute. You cannot have censorship-resistance with exceptions; otherwise, these exceptions could be used to remove or block anything unwanted, not only offensive. This way, the Chinese cannot access Wikipedia because of what originally started as a counter-terrorism measure, and the Russians cannot access LinkedIn because of what originally started as a children protection measure. We cannot deprive humanity of their freedom just because some small fraction of users might, unfortunately, use that freedom to spread offensive content. In the same way, you do not ban electricity because people get electrocuted.</p>

<p>Let us now switch from corporates to governments. Ooh, wee! Do not even get me started on that. And I am not even talking about cases like Google happily not letting people disable <a href="https://en.wikipedia.org/wiki/SafeSearch">SafeSearch</a> in Indonesia because its government knows best, that is just the tip of the iceberg. I am talking about political censorship which includes silencing people with torture, gulags, and bullets. Here is the world map of the freedom of the press status:</p>

<p><img src="https://1b677b8f8bb20100.github.io/introduction/press-freedom.png" alt="2020 Press Freedom Index"></p>

<p>Just look at this mess. Blue tones mean OK-ish, others not so much. This map is <a href="https://de.wikipedia.org/wiki/User:NordNordWest">NordNordWest</a>’s work based on the <a href="https://rsf.org/en/ranking">2020 Press Freedom Index</a> and is distributed under <a href="https://creativecommons.org/licenses/by-sa/3.0/de/legalcode">CC BY-SA 3.0 de</a>. Keep in mind population densities, e.g. there are about 90 times more people per unit area living in Vietnam than Australia. I am actually surprised the U.S. did so well in 2020 considering how badly they wanted <a href="https://en.wikipedia.org/wiki/Julian_Assange">Mr. Assange</a> to be extradited and executed.</p>

<p>What would you answer your children if they asked you how in the world North Korea still exists in its current form with 25 million Koreans suffering for over 70 years and no one is doing anything about that? Or how about 28 million people in Venezuela? Or 82 million people in Iran? Giving voice to all whistleblowers and activists, especially the ones risking their lives and freedom in hostile environments is the fundamental goal of Pepe.</p>

<h3 id="darknets">Darknets</h3>

<p>There is already <a href="https://en.wikipedia.org/wiki/Tor_(anonymity_network)">Tor</a>, <a href="https://en.wikipedia.org/wiki/I2P">I2P</a>, <a href="https://en.wikipedia.org/wiki/Freenet">Freenet</a>, <a href="https://en.wikipedia.org/wiki/GNUnet">GNUnet</a> etc. We can run emails, message boards, <a href="https://en.wikipedia.org/wiki/BitTorrent">BitTorrent</a>, <a href="https://en.wikipedia.org/wiki/Kad_network">Kad</a>, and <a href="https://en.wikipedia.org/wiki/InterPlanetary_File_System">IPFS</a> on top of them, maybe even use <a href="https://en.wikipedia.org/wiki/Ethereum">Ethereum</a> smart contracts for decentralized computing. All the technology is there, why bother with something new? Well, first of all, these are all amazing projects, there is nothing wrong with them. The peculiar thing, however, is none of them except BitTorrent (and perhaps Tor) gained much popularity, neither do we see any readily available censorship-resistant communication platforms. Why is that?</p>

<p>I claim there are 2 main reasons for that:</p>

<ul>
  <li>
    <p>They are hard to use. The “Unix is user-friendly, it is just picky about who its friends are.” aphorism still lives in most them: you may need to install a bunch of additional software (such as <a href="https://en.wikipedia.org/wiki/Java_virtual_machine">JVM</a> or shared libraries) potentially dealing with a dependency hell on some platforms; read through sparse documentation and dead forums on optimal network, security, and sharing settings; carefully configure your router, computer, and client; install, study, and configure applications running on top of the darknet, i.e. repeat the steps. The reason why Tor became popular outside of research was not because it was first, but because of the hacky all-in-one Tor Browser Bundle with sane defaults.</p>
  </li>
  <li>
    <p>They prefer purity to practicality. Instead of concentrating manpower on few specific use cases, most existing tools try to conquer the world: a new internet, interplanetary, infrastructure, an application framework, APIs, a Turing-complete language on the blockchain etc. This is great and all, it is general, conceptual, modular, extensible, and stackable—everything we like—but sometimes overengineering is just overengineering given the goal. And our goal here is not to make a technical revolution, but to help as many people as we can communicate without fear of retribution.</p>
  </li>
</ul>

<p>BitTorrent evolved into something that is used by 150 million people worldwide, it seamlessly adopted <a href="https://en.wikipedia.org/wiki/Distributed_hash_table">DHT</a>, <a href="https://en.wikipedia.org/wiki/Peer_exchange">PEX</a>, <a href="https://en.wikipedia.org/wiki/Micro_Transport_Protocol">µTP</a>, trackerless <a href="https://en.wikipedia.org/wiki/Magnet_URI_scheme">magnet links</a>, and people do not even know what the hell it all means. Even though proprietary, <a href="https://en.wikipedia.org/wiki/Skype">Skype</a> thrived very similarly (at least before it was crippled by Microsoft), millions of its users did not even know what peer-to-peer meant, not to mention how it worked under the hood, it just did. These two systems succeeded not because of luck but rather as a result of some excellent product decisions. We need to learn from that and reiterate.</p>

<h2 id="pepe-overview">Pepe overview</h2>

<h3 id="user-level">User level</h3>

<p>For the messaging platform, I chose to use an <a href="https://en.wikipedia.org/wiki/Imageboard">imageboard</a> similar to <a href="https://en.wikipedia.org/wiki/4chan">4chan</a> or <a href="https://en.wikipedia.org/wiki/Futaba_Channel">Futaba Channel</a>. While not the most popular type of forum, imageboards are extremely flexible and free of junk like authentication or karma, they promote anonymity in a very practical way, and over 30 million people are already familiar with them. Perhaps, I am not a big fan of their crowded old-school design, but the initial user traction is more important than my sense of beauty, we will refine the looks through time.</p>

<p>That is, the Pepe imageboard is going to be the only application running on top the Pepe darknet, they are in fact inseparable. This way, we can design the network specifically for this one use case. This brings both security and performance benefits. Joining the darknet can be as simple as double clicking the application, and users do not need to install or configure any third-party browsers or proxy servers, they can just go to <a href="http://localhost:8666/">localhost:8666</a> using Chrome, Safari, or whatever they like, and it is going to be safe without any third-party extensions.</p>

<p>Once online, users may browse existing or create new message boards about various topics in any language such as <code>/en/food/</code> or <code>/ja/math/</code>. A board is a collection of threads about something more specific, would it be an idea or a question. A thread has a collection of posts that people send replying to each other. Each post may have one or multiple attachments such as photos, videos, you name it. So that you have an idea of what it looks like, here is a screenshot of a random thread on the 4chan DIY board:</p>

<p><img src="https://1b677b8f8bb20100.github.io/introduction/4chan-thread.png" alt="4chan thread"></p>

<p>What is fundamentally different with Pepe is moderation. Instead of relying on a centralized entity with a banhammer, each board and thread owner may anonymously moderate their spaces on their own. However, nothing can actually be deleted, it can only be shadowed, and each user decides whether they want to see the light or the full version of the page at any moment in time. People can still reply to shadowed posts inside their own shadowed posts, so no one cannot silence anyone, only maintain order on the light side.</p>

<p>If people are no longer interested in particular threads, they will eventually become forgotten by the network and naturally disappear from their board. But if there is at least one person who is subscribed to or has archived some thread, no one in the world (even Pepe creators) can censor or somehow shut it down without hurting most of the network Pepe is running on top of.</p>

<h3 id="network-level">Network level</h3>

<p>The three biggest problems with 4chan and similar communication platforms are:</p>

<ul>
  <li>They use closed source software so no one can tell how secure everything is and what is really going on there.</li>
  <li>They are centralized, i.e. some individual or business owns the servers and fully controls the whole infrastructure.</li>
  <li>They collect lots of metadata including but not limited to “someone with this <a href="https://en.wikipedia.org/wiki/IP_address">IP address</a> posted this at this moment in time”.</li>
</ul>

<p>Mitigating the first problem is the easiest: just use open-source software whenever possible. Regarding the centralization issue, we could switch to a decentralized solution like BitTorrent (imagine each torrent containing a thread with its posts and attachments), but that itself does not help with privacy, people can still see what others are doing. Similarly, we could tackle the privacy issue with a <a href="https://en.wikipedia.org/wiki/Virtual_private_network">VPN</a> or a darknet like Tor or I2P, but that, contrary to popular belief, does not solve the centralization issue in any way. Clearly, we need the best of the two worlds. Let us fuse them together!</p>

<p><img src="https://1b677b8f8bb20100.github.io/introduction/pepe-routing.png" alt="Pepe routing"></p>

<p>Here is an very simplified walk through how the network works. Imagine Bob is an undercover journalist who wants to anonymously share his report and Alice is a political activist who is interested in the investigation Bob had been doing. It all starts with Bob announcing he has the report:</p>

<ol>
  <li>
    <p>Bob joins the network and gathers information about random peers on it through the <a href="https://en.wikipedia.org/wiki/Distributed_hash_table">DHT</a>. This way, Bob discovers hundreds of participants including X and Y. Similarly, Bob registers himself on the network through the DHT so that others …</p></li></ol></div></div>
<br/><p><em>Article truncated for RSS feed. Read the full article at <a href="https://1b677b8f8bb20100.github.io/introduction/">https://1b677b8f8bb20100.github.io/introduction/</a></em></p>]]>
            </description>
            <link>https://1b677b8f8bb20100.github.io/introduction/</link>
            <guid isPermaLink="false">hacker-news-small-sites-26218900</guid>
            <pubDate>Mon, 22 Feb 2021 00:28:01 GMT</pubDate>
        </item>
        <item>
            <title>
<![CDATA[TransferWise changes name to Wise]]>
            </title>
            <description>
<![CDATA[
Score 237 | Comments 225 (<a href="https://news.ycombinator.com/item?id=26218693">thread link</a>) | @watbe
<br/>
February 21, 2021 | https://wise.com/gb/blog/world-meet-wise | <a href="https://web.archive.org/web/*/https://wise.com/gb/blog/world-meet-wise">archive.org</a>
<br/><!-- hnss:readable-content --><hr/><div id="readability-page-1" class="page"><div>
<div>
<div>
<p>Today, we’re changing our name from TransferWise to Wise.</p>
<p>Our customers now need us for more than money transfers. Sending, spending, and receiving money internationally is too expensive, slow, and inconvenient. We’re fixing that for people and businesses.</p>
<p>You can <a href="https://transferwise.com/gb/blog/world-meet-wise#transferwise-is-now-wise">skip ahead to see what changes for you</a> (spoiler: not much, right away), but first, let’s go back a bit.</p>
<h2><a href="#a-decade-into-our-mission" id="a-decade-into-our-mission"></a>A decade into our mission</h2>
<p>Ten years ago, Taavet and I set out to fix international money transfers for all of us who’d been overcharged and underserved by banks. We named our idea ‘TransferWise’ — because our early customers were ‘wise’ to know their banks were charging hidden fees in exchange rate markups.</p>
<p>We set ourselves a mission to make money work without borders — to make money move instantly, transparently, conveniently, and — eventually — for free.</p>
<p>Now, we’re a community of 10 million like-minded people and businesses managing money all over the world, saving billions and fighting as hard as ever against hidden fees.</p>
<p>Our multi-currency account and the clever debit card is replacing international banking for many of you. By building this infrastructure for you, we’ve created a platform that more than a dozen banks use today.</p>
<p>You’ve told us for years the problem is bigger than money transfers. Any time money moves into another currency, it’s still a maze of hidden exchange rate markups, high fees, delays, and small print.</p>
<h2><a href="#well-fix-international-banking-together" id="well-fix-international-banking-together"></a>We’ll fix international banking together</h2>
<p>Sending, spending, receiving, and holding money internationally doesn’t work like it should, because the international banking system was built for the past.</p>
<p>For generations, banks have been defined by borders. Traditional bank accounts trap our money in one country, making international lives more difficult and expensive than they need to be. We shouldn’t have to accept this status quo.</p>
<p>Today, we don’t. We’ll fix it with Wise — the world’s most international account. It makes your money borderless — with instant, super-cheap money transfers, a debit card to spend in any currency, account details to get paid in 30+ countries, balances to hold your money safely in 50+ different currencies, multi-currency direct debits, and other revolutionary features.</p>
<h2><a href="#transferwise-is-now-wise" id="transferwise-is-now-wise"></a>TransferWise is now Wise</h2>
<p>Today our name catches up with who we’re already building for — a community of people and businesses with multi-currency lives. Wise is for all of us who live, work, travel, or support family around the world. It’s for those of us who want to cut out the middlemen that hold us back from being truly borderless.</p>
<p>For customers, not too much will change right away. We become “Wise” or “Wise Business” — depending how you use us. You can access your exact same account via <a href="http://wise.com/">wise.com</a>, using your current email and password. You won't need a new account. In a few weeks we will start to redirect transferwise.com to wise.com.</p>
<p>Our logo has changed, and our apps will be renamed. But our icon — the fast flag — remains as a symbol for money without borders. Beyond that, you’ll notice some new colours, words, and designs.</p>
<p>The core experience of using Wise will remain faster, cheaper, and more convenient than anything else. Our mission remains the same. We’re still making — and always will be making — money work without borders.</p>
<p>We’re humbled that 10 million of you already rely on us to help you lead your international lives. We can’t wait to bring the next 100 million of you with us as we continue to build a new, fair, and transparent world of money.</p>
<p>Onwards.</p>
</div>
</div>
</div></div>]]>
            </description>
            <link>https://wise.com/gb/blog/world-meet-wise</link>
            <guid isPermaLink="false">hacker-news-small-sites-26218693</guid>
            <pubDate>Mon, 22 Feb 2021 00:02:25 GMT</pubDate>
        </item>
    </channel>
</rss>
